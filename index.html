<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-21T01:30:00Z">09-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">When a Computer Cracks a Joke: Automated Generation of Humorous Headlines. (arXiv:2109.08702v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08702">
<div class="article-summary-box-inner">
<span><p>Automated news generation has become a major interest for new agencies in the
past. Oftentimes headlines for such automatically generated news articles are
unimaginative as they have been generated with ready-made templates. We present
a computationally creative approach for headline generation that can generate
humorous versions of existing headlines. We evaluate our system with human
judges and compare the results to human authored humorous titles. The headlines
produced by the system are considered funny 36\% of the time by human
evaluators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relating Neural Text Degeneration to Exposure Bias. (arXiv:2109.08705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08705">
<div class="article-summary-box-inner">
<span><p>This work focuses on relating two mysteries in neural-based text generation:
exposure bias, and text degeneration. Despite the long time since exposure bias
was mentioned and the numerous studies for its remedy, to our knowledge, its
impact on text generation has not yet been verified. Text degeneration is a
problem that the widely-used pre-trained language model GPT-2 was recently
found to suffer from (Holtzman et al., 2020). Motivated by the unknown
causation of the text degeneration, in this paper we attempt to relate these
two mysteries. Specifically, we first qualitatively quantitatively identify
mistakes made before text degeneration occurs. Then we investigate the
significance of the mistakes by inspecting the hidden states in GPT-2. Our
results show that text degeneration is likely to be partly caused by exposure
bias. We also study the self-reinforcing mechanism of text degeneration,
explaining why the mistakes amplify. In sum, our study provides a more concrete
foundation for further investigation on exposure bias and text degeneration
problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-device neural speech synthesis. (arXiv:2109.08710v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08710">
<div class="article-summary-box-inner">
<span><p>Recent advances in text-to-speech (TTS) synthesis, such as Tacotron and
WaveRNN, have made it possible to construct a fully neural network based TTS
system, by coupling the two components together. Such a system is conceptually
simple as it only takes grapheme or phoneme input, uses Mel-spectrogram as an
intermediate feature, and directly generates speech samples. The system
achieves quality equal or close to natural speech. However, the high
computational cost of the system and issues with robustness have limited their
usage in real-world speech synthesis applications and products. In this paper,
we present key modeling improvements and optimization strategies that enable
deploying these models, not only on GPU servers, but also on mobile devices.
The proposed system can generate high-quality 24 kHz speech at 5x faster than
real time on server and 3x faster than real time on mobile devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back-translation for Large-Scale Multilingual Machine Translation. (arXiv:2109.08712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08712">
<div class="article-summary-box-inner">
<span><p>This paper illustrates our approach to the shared task on large-scale
multilingual machine translation in the sixth conference on machine translation
(WMT-21). This work aims to build a single multilingual translation system with
a hypothesis that a universal cross-language representation leads to better
multilingual translation performance. We extend the exploration of different
back-translation methods from bilingual translation to multilingual
translation. Better performance is obtained by the constrained sampling method,
which is different from the finding of the bilingual translation. Besides, we
also explore the effect of vocabularies and the amount of synthetic data.
Surprisingly, the smaller size of vocabularies perform better, and the
extensive monolingual English data offers a modest improvement. We submitted to
both the small tasks and achieved the second place.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08722">
<div class="article-summary-box-inner">
<span><p>Prerequisite chain learning helps people acquire new knowledge efficiently.
While people may quickly determine learning paths over concepts in a domain,
finding such paths in other domains can be challenging. We introduce
Domain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this
cross-domain prerequisite chain learning task efficiently. Our novel model
consists of a variational graph autoencoder (VGAE) and a domain discriminator.
The VGAE is trained to predict concept relations through link prediction, while
the domain discriminator takes both source and target domain data as input and
is trained to predict domain labels. Most importantly, this method only needs
simple homogeneous graphs as input, compared with the current state-of-the-art
model. We evaluate our model on the LectureBankCD dataset, and results show
that our model outperforms recent graph-based benchmarks while using only 1/10
of graph scale and 1/3 computation time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The JHU-Microsoft Submission for WMT21 Quality Estimation Shared Task. (arXiv:2109.08724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08724">
<div class="article-summary-box-inner">
<span><p>This paper presents the JHU-Microsoft joint submission for WMT 2021 quality
estimation shared task. We only participate in Task 2 (post-editing effort
estimation) of the shared task, focusing on the target-side word-level quality
estimation. The techniques we experimented with include Levenshtein Transformer
training and data augmentation with a combination of forward, backward,
round-trip translation, and pseudo post-editing of the MT output. We
demonstrate the competitiveness of our system compared to the widely adopted
OpenKiwi-XLM baseline. Our system is also the top-ranking system on the MT MCC
metric for the English-German language pair.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Few-Shot Intent Classification and Slot Filling. (arXiv:2109.08754v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08754">
<div class="article-summary-box-inner">
<span><p>Intent classification (IC) and slot filling (SF) are two fundamental tasks in
modern Natural Language Understanding (NLU) systems. Collecting and annotating
large amounts of data to train deep learning models for such systems is not
scalable. This problem can be addressed by learning from few examples using
fast supervised meta-learning techniques such as prototypical networks. In this
work, we systematically investigate how contrastive learning and unsupervised
data augmentation methods can benefit these existing supervised meta-learning
pipelines for jointly modelled IC/SF tasks. Through extensive experiments
across standard IC/SF benchmarks (SNIPS and ATIS), we show that our proposed
semi-supervised approaches outperform standard supervised meta-learning
methods: contrastive losses in conjunction with prototypical networks
consistently outperform the existing state-of-the-art for both IC and SF tasks,
while data augmentation strategies primarily improve few-shot IC by a
significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solar cell patent classification method based on keyword extraction and deep neural network. (arXiv:2109.08796v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08796">
<div class="article-summary-box-inner">
<span><p>With the growing impact of ESG on businesses, research related to renewable
energy is receiving great attention. Solar cells are one of them, and
accordingly, it can be said that the research value of solar cell patent
analysis is very high. Patent documents have high research value. Being able to
accurately analyze and classify patent documents can reveal several important
technical relationships. It can also describe the business trends in that
technology. And when it comes to investment, new industrial solutions will also
be inspired and proposed to make important decisions. Therefore, we must
carefully analyze patent documents and utilize the value of patents. To solve
the solar cell patent classification problem, we propose a keyword extraction
method and a deep neural network-based solar cell patent classification method.
First, solar cell patents are analyzed for pretreatment. It then uses the
KeyBERT algorithm to extract keywords and key phrases from the patent abstract
to construct a lexical dictionary. We then build a solar cell patent
classification model according to the deep neural network. Finally, we use a
deep neural network-based solar cell patent classification model to classify
power patents, and the training accuracy is greater than 95%. Also, the
validation accuracy is about 87.5%. It can be seen that the deep neural network
method can not only realize the classification of complex and difficult solar
cell patents, but also have a good classification effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-Beta: A Proactive Probabilistic Approach to Text Moderation. (arXiv:2109.08805v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08805">
<div class="article-summary-box-inner">
<span><p>Text moderation for user generated content, which helps to promote healthy
interaction among users, has been widely studied and many machine learning
models have been proposed. In this work, we explore an alternative perspective
by augmenting reactive reviews with proactive forecasting. Specifically, we
propose a new concept {\it text toxicity propensity} to characterize the extent
to which a text tends to attract toxic comments. Beta regression is then
introduced to do the probabilistic modeling, which is demonstrated to function
well in comprehensive experiments. We also propose an explanation method to
communicate the model decision clearly. Both propensity scoring and
interpretation benefit text moderation in a novel manner. Finally, the proposed
scaling mechanism for the linear model offers useful insights beyond this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Pattern Pruning Using Regularization. (arXiv:2109.08814v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08814">
<div class="article-summary-box-inner">
<span><p>Iterative Magnitude Pruning (IMP) is a network pruning method that repeats
the process of removing weights with the least magnitudes and retraining the
model. When visualizing the weight matrices of language models pruned by IMP,
previous research has shown that a structured pattern emerges, wherein the
resulting surviving weights tend to prominently cluster in a select few rows
and columns of the matrix. Though the need for further research in utilizing
these structured patterns for potential performance gains has previously been
indicated, it has yet to be thoroughly studied. We propose SPUR (Structured
Pattern pruning Using Regularization), a novel pruning mechanism that
preemptively induces structured patterns in compression by adding a
regularization term to the objective function in the IMP. Our results show that
SPUR can significantly preserve model performance under high sparsity settings
regardless of the language or the task. Our contributions are as follows: (i)
We propose SPUR, a network pruning mechanism that improves upon IMP regardless
of the language or the task. (ii) We are the first to empirically verify the
efficacy of "structured patterns" observed previously in pruning research.
(iii) SPUR is a resource-efficient mechanism in that it does not require
significant additional computations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyLex: Incoporating Dynamic Lexicons into BERT for Sequence Labeling. (arXiv:2109.08818v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08818">
<div class="article-summary-box-inner">
<span><p>Incorporating lexical knowledge into deep learning models has been proved to
be very effective for sequence labeling tasks. However, previous works commonly
have difficulty dealing with large-scale dynamic lexicons which often cause
excessive matching noise and problems of frequent updates. In this paper, we
propose DyLex, a plug-in lexicon incorporation approach for BERT based sequence
labeling tasks. Instead of leveraging embeddings of words in the lexicon as in
conventional methods, we adopt word-agnostic tag embeddings to avoid
re-training the representation while updating the lexicon. Moreover, we employ
an effective supervised lexical knowledge denoising method to smooth out
matching noise. Finally, we introduce a col-wise attention based knowledge
fusion mechanism to guarantee the pluggability of the proposed framework.
Experiments on ten datasets of three tasks show that the proposed framework
achieves new SOTA, even with very large scale lexicons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Zero and Few-shot Knowledge-seeking Turn Detection in Task-orientated Dialogue Systems. (arXiv:2109.08820v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08820">
<div class="article-summary-box-inner">
<span><p>Most prior work on task-oriented dialogue systems is restricted to supporting
domain APIs. However, users may have requests that are out of the scope of
these APIs. This work focuses on identifying such user requests. Existing
methods for this task mainly rely on fine-tuning pre-trained models on large
annotated data. We propose a novel method, REDE, based on adaptive
representation learning and density estimation. REDE can be applied to
zero-shot cases, and quickly learns a high-performing detector with only a few
shots by updating less than 3K parameters. We demonstrate REDE's competitive
performance on DSTC9 data and our newly collected test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes. (arXiv:2109.08828v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08828">
<div class="article-summary-box-inner">
<span><p>Empathy is a complex cognitive ability based on the reasoning of others'
affective states. In order to better understand others and express stronger
empathy in dialogues, we argue that two issues must be tackled at the same
time: (i) identifying which word is the cause for the other's emotion from his
or her utterance and (ii) reflecting those specific words in the response
generation. However, previous approaches for recognizing emotion cause words in
text require sub-utterance level annotations, which can be demanding. Taking
inspiration from social cognition, we leverage a generative estimator to infer
emotion cause words from utterances with no word-level label. Also, we
introduce a novel method based on pragmatics to make dialogue models focus on
targeted words in the input during generation. Our method is applicable to any
dialogue models with no additional training on the fly. We show our approach
improves multiple best-performing dialogue agents on generating more focused
empathetic responses in terms of both automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MM-Deacon: Multimodal molecular domain embedding analysis via contrastive learning. (arXiv:2109.08830v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08830">
<div class="article-summary-box-inner">
<span><p>Molecular representation learning plays an essential role in cheminformatics.
Recently, language model-based approaches have been popular as an alternative
to traditional expert-designed features to encode molecules. However, these
approaches only utilize a single modality for representing molecules. Driven by
the fact that a given molecule can be described through different modalities
such as Simplified Molecular Line Entry System (SMILES), The International
Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International
Chemical Identifier (InChI), we propose a multimodal molecular embedding
generation approach called MM-Deacon (multimodal molecular domain embedding
analysis via contrastive learning). MM-Deacon is trained using SMILES and IUPAC
molecule representations as two different modalities. First, SMILES and IUPAC
strings are encoded by using two different transformer-based language models
independently, then the contrastive loss is utilized to bring these encoded
representations from different modalities closer to each other if they belong
to the same molecule, and to push embeddings farther from each other if they
belong to different molecules. We evaluate the robustness of our molecule
embeddings on molecule clustering, cross-modal molecule search, drug similarity
assessment and drug-drug interaction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TVRecap: A Dataset for Generating Stories with Character Descriptions. (arXiv:2109.08833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08833">
<div class="article-summary-box-inner">
<span><p>We introduce TVRecap, a story generation dataset that requires generating
detailed TV show episode recaps from a brief summary and a set of documents
describing the characters involved. Unlike other story generation datasets,
TVRecap contains stories that are authored by professional screenwriters and
that feature complex interactions among multiple characters. Generating stories
in TVRecap requires drawing relevant information from the lengthy provided
documents about characters based on the brief summary. In addition, by swapping
the input and output, TVRecap can serve as a challenging testbed for
abstractive summarization. We create TVRecap from fan-contributed websites,
which allows us to collect 26k episode recaps with 1868.7 tokens on average.
Empirically, we take a hierarchical story generation approach and find that the
neural model that uses oracle content selectors for character descriptions
demonstrates the best performance on automatic metrics, showing the potential
of our dataset to inspire future research on story generation with constraints.
Qualitative analysis shows that the best-performing model sometimes generates
content that is unfaithful to the short summaries, suggesting promising
directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechNAS: Towards Better Trade-off between Latency and Accuracy for Large-Scale Speaker Verification. (arXiv:2109.08839v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08839">
<div class="article-summary-box-inner">
<span><p>Recently, x-vector has been a successful and popular approach for speaker
verification, which employs a time delay neural network (TDNN) and statistics
pooling to extract speaker characterizing embedding from variable-length
utterances. Improvement upon the x-vector has been an active research area, and
enormous neural networks have been elaborately designed based on the x-vector,
eg, extended TDNN (E-TDNN), factorized TDNN (F-TDNN), and densely connected
TDNN (D-TDNN). In this work, we try to identify the optimal architectures from
a TDNN based search space employing neural architecture search (NAS), named
SpeechNAS. Leveraging the recent advances in the speaker recognition, such as
high-order statistics pooling, multi-branch mechanism, D-TDNN and angular
additive margin softmax (AAM) loss with a minimum hyper-spherical energy (MHE),
SpeechNAS automatically discovers five network architectures, from SpeechNAS-1
to SpeechNAS-5, of various numbers of parameters and GFLOPs on the large-scale
text-independent speaker recognition dataset VoxCeleb1. Our derived best neural
network achieves an equal error rate (EER) of 1.02% on the standard test set of
VoxCeleb1, which surpasses previous TDNN based state-of-the-art approaches by a
large margin. Code and trained weights are in
https://github.com/wentaozhu/speechnas.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Engineering for US State Legislative Hearings: Stance, Affiliation, Engagement and Absentees. (arXiv:2109.08855v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08855">
<div class="article-summary-box-inner">
<span><p>In US State government legislatures, most of the activity occurs in
committees made up of lawmakers discussing bills. When analyzing, classifying
or summarizing these committee proceedings, some important features become
broadly interesting. In this paper, we engineer four useful features, two
applying to lawmakers (engagement and absence), and two to non-lawmakers
(stance and affiliation). We propose a system to automatically track the
affiliation of organizations in public comments and whether the organizational
representative supports or opposes the bill. The model tracking affiliation
achieves an F1 of 0.872 while the support determination has an F1 of 0.979.
Additionally, a metric to compute legislator engagement and absenteeism is also
proposed and as proof-of-concept, a list of the most and least engaged
legislators over one full California legislative session is presented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emily: Developing An Emotion-affective Open-Domain Chatbot with Knowledge Graph-based Persona. (arXiv:2109.08875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08875">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe approaches for developing Emily, an
emotion-affective open-domain chatbot. Emily can perceive a user's negative
emotion state and offer supports by positively converting the user's emotion
states. This is done by finetuning a pretrained dialogue model upon data
capturing dialogue contexts and desirable emotion states transition across
turns. Emily can differentiate a general open-domain dialogue utterance with
questions relating to personal information. By leveraging a question-answering
approach based on knowledge graphs to handle personal information, Emily
maintains personality consistency. We evaluate Emily against a few
state-of-the-art open-domain chatbots and show the effects of the proposed
approaches in emotion affecting and addressing personality inconsistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation. (arXiv:2109.08877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08877">
<div class="article-summary-box-inner">
<span><p>In this paper, we provide a bilingual parallel human-to-human recommendation
dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging
task of multilingual and cross-lingual conversational recommendation. The
difference between DuRecDial 2.0 and existing conversational recommendation
datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in
DuRecDial 2.0 is annotated in two languages, both English and Chinese, while
other datasets are built with the setting of a single language. We collect 8.2k
dialogs aligned across English and Chinese languages (16.5k dialogs and 255k
utterances in total) that are annotated by crowdsourced workers with strict
quality control procedure. We then build monolingual, multilingual, and
cross-lingual conversational recommendation baselines on DuRecDial 2.0.
Experiment results show that the use of additional English data can bring
performance improvement for Chinese conversational recommendation, indicating
the benefits of DuRecDial 2.0. Finally, this dataset provides a challenging
testbed for future studies of monolingual, multilingual, and cross-lingual
conversational recommendation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Joint Intent Detection and Slot Filling via Higher-order Attention. (arXiv:2109.08890v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08890">
<div class="article-summary-box-inner">
<span><p>Intent detection (ID) and Slot filling (SF) are two major tasks in spoken
language understanding (SLU). Recently, attention mechanism has been shown to
be effective in jointly optimizing these two tasks in an interactive manner.
However, latest attention-based works concentrated only on the first-order
attention design, while ignoring the exploration of higher-order attention
mechanisms. In this paper, we propose a BiLinear attention block, which
leverages bilinear pooling to simultaneously exploit both the contextual and
channel-wise bilinear attention distributions to capture the second-order
interactions between the input intent or slot features. Higher and even
infinity order interactions are built by stacking numerous blocks and assigning
Exponential Linear Unit (ELU) to blocks. Before the decoding stage, we
introduce the Dynamic Feature Fusion Layer to implicitly fuse intent and slot
information in a more effective way. Technically, instead of simply
concatenating intent and slot features, we first compute two correlation
matrices to weight on two features. Furthermore, we present Higher-order
Attention Network for the SLU tasks. Experiments on two benchmark datasets show
that our approach yields improvements compared with the state-of-the-art
approach. We also provide discussion to demonstrate the effectiveness of the
proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dependency distance minimization predicts compression. (arXiv:2109.08900v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08900">
<div class="article-summary-box-inner">
<span><p>Dependency distance minimization (DDm) is a well-established principle of
word order. It has been predicted theoretically that DDm implies compression,
namely the minimization of word lengths. This is a second order prediction
because it links a principle with another principle, rather than a principle
and a manifestation as in a first order prediction. Here we test that second
order prediction with a parallel collection of treebanks controlling for
annotation style with Universal Dependencies and Surface-Syntactic Universal
Dependencies. To test it, we use a recently introduced score that has many
mathematical and statistical advantages with respect to the widely used sum of
dependency distances. We find that the prediction is confirmed by the new score
when word lengths are measured in phonemes, independently of the annotation
style, but not when word lengths are measured in syllables. In contrast, one of
the most widely used scores, i.e. the sum of dependency distances, fails to
confirm that prediction, showing the weakness of raw dependency distances for
research on word order. Finally, our findings expand the theory of natural
communication by linking two distinct levels of organization, namely syntax
(word order) and word internal structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Detoxification using Large Pre-trained Neural Models. (arXiv:2109.08914v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08914">
<div class="article-summary-box-inner">
<span><p>We present two novel unsupervised methods for eliminating toxicity in text.
Our first method combines two recent ideas: (1) guidance of the generation
process with small style-conditional language models and (2) use of
paraphrasing models to perform style transfer. We use a well-performing
paraphraser guided by style-trained language models to keep the text content
and remove toxicity. Our second method uses BERT to replace toxic words with
their non-offensive synonyms. We make the method more flexible by enabling BERT
to replace mask tokens with a variable number of words. Finally, we present the
first large-scale comparative study of style transfer models on the task of
toxicity removal. We compare our models with a number of methods for style
transfer. The models are evaluated in a reference-free way using a combination
of unsupervised style transfer metrics. Both methods we suggest yield new SOTA
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking the Combinatorial Generalizability of Complex Query Answering on Knowledge Graphs. (arXiv:2109.08925v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08925">
<div class="article-summary-box-inner">
<span><p>Complex Query Answering (CQA) is an important reasoning task on knowledge
graphs. Current CQA learning models have been shown to be able to generalize
from atomic operators to more complex formulas, which can be regarded as the
combinatorial generalizability. In this paper, we present EFO-1-QA, a new
dataset to benchmark the combinatorial generalizability of CQA models by
including 301 different queries types, which is 20 times larger than existing
datasets. Besides, our work, for the first time, provides a benchmark to
evaluate and analyze the impact of different operators and normal forms by
using (a) 7 choices of the operator systems and (b) 9 forms of complex queries.
Specifically, we provide the detailed study of the combinatorial
generalizability of two commonly used operators, i.e., projection and
intersection, and justify the impact of the forms of queries given the
canonical choice of operators. Our code and data can provide an effective
pipeline to benchmark CQA models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic. (arXiv:2109.08927v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08927">
<div class="article-summary-box-inner">
<span><p>Natural language inference (NLI) aims to determine the logical relationship
between two sentences among the target labels Entailment, Contradiction, and
Neutral. In recent years, deep learning models have become a prevailing
approach to NLI, but they lack interpretability and explainability. In this
work, we address the explainability for NLI by weakly supervised logical
reasoning, and propose an Explainable Phrasal Reasoning (EPR) approach. Our
model first detects phrases as the semantic unit and aligns corresponding
phrases. Then, the model predicts the NLI label for the aligned phrases, and
induces the sentence label by fuzzy logic formulas. Our EPR is almost
everywhere differentiable and thus the system can be trained end-to-end in a
weakly supervised manner. We annotated a corpus and developed a set of metrics
to evaluate phrasal reasoning. Results show that our EPR yields much more
meaningful explanations in terms of F scores than previous studies. To the best
of our knowledge, we are the first to develop a weakly supervised phrasal
reasoning model for the NLI task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex Temporal Question Answering on Knowledge Graphs. (arXiv:2109.08935v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08935">
<div class="article-summary-box-inner">
<span><p>Question answering over knowledge graphs (KG-QA) is a vital topic in IR.
Questions with temporal intent are a special class of practical importance, but
have not received much attention in research. This work presents EXAQT, the
first end-to-end system for answering complex temporal questions that have
multiple entities and predicates, and associated temporal conditions. EXAQT
answers natural language questions over KGs in two stages, one geared towards
high recall, the other towards precision at top ranks. The first step computes
question-relevant compact subgraphs within the KG, and judiciously enhances
them with pertinent temporal facts, using Group Steiner Trees and fine-tuned
BERT models. The second step constructs relational graph convolutional networks
(R-GCNs) from the first step's output, and enhances the R-GCNs with time-aware
entity embeddings and attention over temporal relations. We evaluate EXAQT on
TimeQuestions, a large dataset of 16k temporal questions we compiled from a
variety of general purpose KG-QA benchmarks. Results show that EXAQT
outperforms three state-of-the-art systems for answering complex questions over
KGs, thereby justifying specialized treatment of temporal QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReaSCAN: Compositional Reasoning in Language Grounding. (arXiv:2109.08994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08994">
<div class="article-summary-box-inner">
<span><p>The ability to compositionally map language to referents, relations, and
actions is an essential component of language understanding. The recent gSCAN
dataset (Ruis et al. 2020, NeurIPS) is an inspiring attempt to assess the
capacity of models to learn this kind of grounding in scenarios involving
navigational instructions. However, we show that gSCAN's highly constrained
design means that it does not require compositional interpretation and that
many details of its instructions and scenarios are not required for task
success. To address these limitations, we propose ReaSCAN, a benchmark dataset
that builds off gSCAN but requires compositional language interpretation and
reasoning about entities and relations. We assess two models on ReaSCAN: a
multi-modal baseline and a state-of-the-art graph convolutional neural model.
These experiments show that ReaSCAN is substantially harder than gSCAN for both
neural architectures. This suggests that ReaSCAN can serve as a valuable
benchmark for advancing our understanding of models' compositional
generalization and reasoning capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting semantic lexicons using word embeddings and transfer learning. (arXiv:2109.09010v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09010">
<div class="article-summary-box-inner">
<span><p>Sentiment-aware intelligent systems are essential to a wide array of
applications including marketing, political campaigns, recommender systems,
behavioral economics, social psychology, and national security. These
sentiment-aware intelligent systems are driven by language models which broadly
fall into two paradigms: 1. Lexicon-based and 2. Contextual. Although recent
contextual models are increasingly dominant, we still see demand for
lexicon-based models because of their interpretability and ease of use. For
example, lexicon-based models allow researchers to readily determine which
words and phrases contribute most to a change in measured sentiment. A
challenge for any lexicon-based approach is that the lexicon needs to be
routinely expanded with new words and expressions. Crowdsourcing annotations
for semantic dictionaries may be an expensive and time-consuming task. Here, we
propose two models for predicting sentiment scores to augment semantic lexicons
at a relatively low cost using word embeddings and transfer learning. Our first
model establishes a baseline employing a simple and shallow neural network
initialized with pre-trained word embeddings using a non-contextual approach.
Our second model improves upon our baseline, featuring a deep Transformer-based
network that brings to bear word definitions to estimate their lexical
polarity. Our evaluation shows that both models are able to score new words
with a similar accuracy to reviewers from Amazon Mechanical Turk, but at a
fraction of the cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Machine Learning Pipeline to Examine Political Bias with Congressional Speeches. (arXiv:2109.09014v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09014">
<div class="article-summary-box-inner">
<span><p>Computational methods to model political bias in social media involve several
challenges due to heterogeneity, high-dimensional, multiple modalities, and the
scale of the data. Political bias in social media has been studied in multiple
viewpoints like media bias, political ideology, echo chambers, and
controversies using machine learning pipelines. Most of the current methods
rely heavily on the manually-labeled ground-truth data for the underlying
political bias prediction tasks. Limitations of such methods include
human-intensive labeling, labels related to only a specific problem, and the
inability to determine the near future bias state of a social media
conversation. In this work, we address such problems and give machine learning
approaches to study political bias in two ideologically diverse social media
forums: Gab and Twitter without the availability of human-annotated data. Our
proposed methods exploit the use of transcripts collected from political
speeches in US congress to label the data and achieve the highest accuracy of
70.5% and 65.1% in Twitter and Gab data respectively to predict political bias.
We also present a machine learning approach that combines features from
cascades and text to forecast cascade's political bias with an accuracy of
about 85%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Relation-Guided Type-Sentence Alignment for Long-Tail Relation Extraction with Distant Supervision. (arXiv:2109.09036v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09036">
<div class="article-summary-box-inner">
<span><p>Distant supervision uses triple facts in knowledge graphs to label a corpus
for relation extraction, leading to wrong labeling and long-tail problems. Some
works use the hierarchy of relations for knowledge transfer to long-tail
relations. However, a coarse-grained relation often implies only an attribute
(e.g., domain or topic) of the distant fact, making it hard to discriminate
relations based solely on sentence semantics. One solution is resorting to
entity types, but open questions remain about how to fully leverage the
information of entity types and how to align multi-granular entity types with
sentences. In this work, we propose a novel model to enrich
distantly-supervised sentences with entity types. It consists of (1) a pairwise
type-enriched sentence encoding module injecting both context-free and -related
backgrounds to alleviate sentence-level wrong labeling, and (2) a hierarchical
type-sentence alignment module enriching a sentence with the triple fact's
basic attributes to support long-tail relations. Our model achieves new
state-of-the-art results in overall and long-tail performance on benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Enhanced Evidence Retrieval for Counterargument Generation. (arXiv:2109.09057v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09057">
<div class="article-summary-box-inner">
<span><p>Finding counterevidence to statements is key to many tasks, including
counterargument generation. We build a system that, given a statement,
retrieves counterevidence from diverse sources on the Web. At the core of this
system is a natural language inference (NLI) model that determines whether a
candidate sentence is valid counterevidence or not. Most NLI models to date,
however, lack proper reasoning abilities necessary to find counterevidence that
involves complex inference. Thus, we present a knowledge-enhanced NLI model
that aims to handle causality- and example-based inference by incorporating
knowledge graphs. Our NLI model outperforms baselines for NLI tasks, especially
for instances that require the targeted inference. In addition, this NLI model
further improves the counterevidence retrieval system, notably finding complex
counterevidence better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Training with Contrastive Learning in NLP. (arXiv:2109.09075v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09075">
<div class="article-summary-box-inner">
<span><p>For years, adversarial training has been extensively studied in natural
language processing (NLP) settings. The main goal is to make models robust so
that similar inputs derive in semantically similar outcomes, which is not a
trivial problem since there is no objective measure of semantic similarity in
language. Previous works use an external pre-trained NLP model to tackle this
challenge, introducing an extra training stage with huge memory consumption
during training. However, the recent popular approach of contrastive learning
in language processing hints a convenient way of obtaining such similarity
restrictions. The main advantage of the contrastive learning approach is that
it aims for similar data points to be mapped close to each other and further
from different ones in the representation space. In this work, we propose
adversarial training with contrastive learning (ATCL) to adversarially train a
language processing task using the benefits of contrastive learning. The core
idea is to make linear perturbations in the embedding space of the input via
fast gradient methods (FGM) and train the model to keep the original and
perturbed representations close via contrastive learning. In NLP experiments,
we applied ATCL to language modeling and neural machine translation tasks. The
results show not only an improvement in the quantitative (perplexity and BLEU)
scores when compared to the baselines, but ATCL also achieves good qualitative
results in the semantic level for both tasks without using a pre-trained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What BERT Based Language Models Learn in Spoken Transcripts: An Empirical Study. (arXiv:2109.09105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09105">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) have been ubiquitously leveraged in various tasks
including spoken language understanding (SLU). Spoken language requires careful
understanding of speaker interactions, dialog states and speech induced
multimodal behaviors to generate a meaningful representation of the
conversation.In this work, we propose to dissect SLU into three representative
properties:conversational(disfluency, pause, overtalk), channel(speaker-type,
turn-tasks) andASR(insertion, deletion,substitution). We probe BERT based
language models (BERT, RoBERTa) trained on spoken transcripts to investigate
its ability to understand multifarious properties in absence of any speech
cues. Empirical results indicate that LM is surprisingly good at capturing
conversational properties such as pause prediction and overtalk detection from
lexical tokens. On the downsides, the LM scores low on turn-tasks and ASR
errors predictions. Additionally, pre-training the LM on spoken transcripts
restrain its linguistic understanding. Finally,we establish the efficacy and
transferability of the mentioned properties on two benchmark datasets:
Switchboard Dialog Act and Disfluency datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Long-Range Language Models Actually Use Long-Range Context?. (arXiv:2109.09115v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09115">
<div class="article-summary-box-inner">
<span><p>Language models are generally trained on short, truncated input sequences,
which limits their ability to use discourse-level information present in
long-range context to improve their predictions. Recent efforts to improve the
efficiency of self-attention have led to a proliferation of long-range
Transformer language models, which can process much longer sequences than
models of the past. However, the ways in which such models take advantage of
the long-range context remain unclear. In this paper, we perform a fine-grained
analysis of two long-range Transformer language models (including the
\emph{Routing Transformer}, which achieves state-of-the-art perplexity on the
PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to
8K tokens. Our results reveal that providing long-range context (i.e., beyond
the previous 2K tokens) to these models only improves their predictions on a
small set of tokens (e.g., those that can be copied from the distant context)
and does not help at all for sentence-level prediction tasks. Finally, we
discover that PG-19 contains a variety of different document types and domains,
and that long-range context helps most for literary novels (as opposed to
textbooks or magazines).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preventing Author Profiling through Zero-Shot Multilingual Back-Translation. (arXiv:2109.09133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09133">
<div class="article-summary-box-inner">
<span><p>Documents as short as a single sentence may inadvertently reveal sensitive
information about their authors, including e.g. their gender or ethnicity.
Style transfer is an effective way of transforming texts in order to remove any
information that enables author profiling. However, for a number of current
state-of-the-art approaches the improved privacy is accompanied by an
undesirable drop in the down-stream utility of the transformed data. In this
paper, we propose a simple, zero-shot way to effectively lower the risk of
author profiling through multilingual back-translation using off-the-shelf
translation models. We compare our models with five representative text style
transfer models on three datasets across different domains. Results from both
an automatic and a human evaluation show that our approach achieves the best
overall performance while requiring no training data. We are able to lower the
adversarial prediction of gender and race by up to $22\%$ while retaining
$95\%$ of the original utility on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition. (arXiv:2109.09161v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09161">
<div class="article-summary-box-inner">
<span><p>Unifying acoustic and linguistic representation learning has become
increasingly crucial to transfer the knowledge learned on the abundance of
high-resource language data for low-resource speech recognition. Existing
approaches simply cascade pre-trained acoustic and language models to learn the
transfer from speech to text. However, how to solve the representation
discrepancy of speech and text is unexplored, which hinders the utilization of
acoustic and linguistic information. Moreover, previous works simply replace
the embedding layer of the pre-trained language model with the acoustic
features, which may cause the catastrophic forgetting problem. In this work, we
introduce Wav-BERT, a cooperative acoustic and linguistic representation
learning method to fuse and utilize the contextual information of speech and
text. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a
language model (BERT) into an end-to-end trainable framework. A Representation
Aggregation Module is designed to aggregate acoustic and linguistic
representation, and an Embedding Attention Module is introduced to incorporate
acoustic information into BERT, which can effectively facilitate the
cooperation of two pre-trained models and thus boost the representation
learning. Extensive experiments show that our Wav-BERT significantly
outperforms the existing approaches and achieves state-of-the-art performance
on low-resource speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FST Morphological Analyser and Generator for Mapud\"ungun. (arXiv:2109.09176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09176">
<div class="article-summary-box-inner">
<span><p>Following the Mapuche grammar by Smeets, this article describes the main
morphophonological aspects of Mapud\"ungun, explaining what triggers them and
the contexts where they arise. We present a computational approach producing a
finite state morphological analyser (and generator) capable of classifying and
appropriately tagging all the components (roots and suffixes) that interact in
a Mapuche word form. The bulk of the article focuses on presenting details
about the morphology of Mapud\"ungun verb and its formalisation using FOMA. A
system evaluation process and its results are also present in this article.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Dynamic based data filtering may not work for NLP datasets. (arXiv:2109.09191v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09191">
<div class="article-summary-box-inner">
<span><p>The recent increase in dataset size has brought about significant advances in
natural language understanding. These large datasets are usually collected
through automation (search engines or web crawlers) or crowdsourcing which
inherently introduces incorrectly labeled data. Training on these datasets
leads to memorization and poor generalization. Thus, it is pertinent to develop
techniques that help in the identification and isolation of mislabelled data.
In this paper, we study the applicability of the Area Under the Margin (AUM)
metric to identify and remove/rectify mislabelled examples in NLP datasets. We
find that mislabelled samples can be filtered using the AUM metric in NLP
datasets but it also removes a significant number of correctly labeled points
and leads to the loss of a large amount of relevant language information. We
show that models rely on the distributional information instead of relying on
syntactic and semantic representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Zero-Label Language Learning. (arXiv:2109.09193v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09193">
<div class="article-summary-box-inner">
<span><p>This paper explores zero-label learning in Natural Language Processing (NLP),
whereby no human-annotated data is used anywhere during training and models are
trained purely on synthetic data. At the core of our framework is a novel
approach for better leveraging the powerful pretrained language models.
Specifically, inspired by the recent success of few-shot inference on GPT-3, we
present a training data creation procedure named Unsupervised Data Generation
(UDG), which leverages few-shot prompts to synthesize high-quality training
data without real human annotations. Our method enables zero-label learning as
we train task-specific models solely on the synthetic data, yet we achieve
better or comparable results from strong baseline models trained on
human-labeled data. Furthermore, when mixed with labeled data, our approach
serves as a highly effective data augmentation procedure, achieving new
state-of-the-art results on the SuperGLUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Crowdsourcing Protocols for Evaluatingthe Factual Consistency of Summaries. (arXiv:2109.09195v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09195">
<div class="article-summary-box-inner">
<span><p>Current pre-trained models applied to summarization are prone to factual
inconsistencies which either misrepresent the source text or introduce
extraneous information. Thus, comparing the factual consistency of summaries is
necessary as we develop improved models. However, the optimal human evaluation
setup for factual consistency has not been standardized. To address this issue,
we crowdsourced evaluations for factual consistency using the rating-based
Likert scale and ranking-based Best-Worst Scaling protocols, on 100 articles
from each of the CNN-Daily Mail and XSum datasets over four state-of-the-art
models, to determine the most reliable evaluation framework. We find that
ranking-based protocols offer a more reliable measure of summary quality across
datasets, while the reliability of Likert ratings depends on the target dataset
and the evaluation design. Our crowdsourcing templates and summary evaluations
will be publicly available to facilitate future research on factual consistency
in summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization. (arXiv:2109.09209v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09209">
<div class="article-summary-box-inner">
<span><p>We study generating abstractive summaries that are faithful and factually
consistent with the given articles. A novel contrastive learning formulation is
presented, which leverages both reference summaries, as positive training data,
and automatically generated erroneous summaries, as negative training data, to
train summarization systems that are better at distinguishing between them. We
further design four types of strategies for creating negative samples, to
resemble errors made commonly by two state-of-the-art models, BART and PEGASUS,
found in our new human annotations of summary errors. Experiments on XSum and
CNN/Daily Mail show that our contrastive learning framework is robust across
datasets and models. It consistently produces more factual summaries than
strong comparisons with post error correction, entailment-based reranking, and
unlikelihood training, according to QA-based factuality evaluation. Human
judges echo the observation and find that our model summaries correct more
errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UPV at CheckThat! 2021: Mitigating Cultural Differences for Identifying Multilingual Check-worthy Claims. (arXiv:2109.09232v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09232">
<div class="article-summary-box-inner">
<span><p>Identifying check-worthy claims is often the first step of automated
fact-checking systems. Tackling this task in a multilingual setting has been
understudied. Encoding inputs with multilingual text representations could be
one approach to solve the multilingual check-worthiness detection. However,
this approach could suffer if cultural bias exists within the communities on
determining what is check-worthy.In this paper, we propose a language
identification task as an auxiliary task to mitigate unintended bias.With this
purpose, we experiment joint training by using the datasets from CLEF-2021
CheckThat!, that contain tweets in English, Arabic, Bulgarian, Spanish and
Turkish. Our results show that joint training of language identification and
check-worthy claim detection tasks can provide performance gains for some of
the selected languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified and Multilingual Author Profiling for Detecting Haters. (arXiv:2109.09233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09233">
<div class="article-summary-box-inner">
<span><p>This paper presents a unified user profiling framework to identify hate
speech spreaders by processing their tweets regardless of the language. The
framework encodes the tweets with sentence transformers and applies an
attention mechanism to select important tweets for learning user profiles.
Furthermore, the attention layer helps to explain why a user is a hate speech
spreader by producing attention weights at both token and post level. Our
proposed model outperformed the state-of-the-art multilingual transformer
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional probing: measuring usable information beyond a baseline. (arXiv:2109.09234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09234">
<div class="article-summary-box-inner">
<span><p>Probing experiments investigate the extent to which neural representations
make properties -- like part-of-speech -- predictable. One suggests that a
representation encodes a property if probing that representation produces
higher accuracy than probing a baseline representation like non-contextual word
embeddings. Instead of using baselines as a point of comparison, we're
interested in measuring information that is contained in the representation but
not in the baseline. For example, current methods can detect when a
representation is more useful than the word identity (a baseline) for
predicting part-of-speech; however, they cannot detect when the representation
is predictive of just the aspects of part-of-speech not explainable by the word
identity. In this work, we extend a theory of usable information called
$\mathcal{V}$-information and propose conditional probing, which explicitly
conditions on the information in the baseline. In a case study, we find that
after conditioning on non-contextual word embeddings, properties like
part-of-speech are accessible at deeper layers of a network than previously
thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MirrorWiC: On Eliciting Word-in-Context Representations from Pretrained Language Models. (arXiv:2109.09237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09237">
<div class="article-summary-box-inner">
<span><p>Recent work indicated that pretrained language models (PLMs) such as BERT and
RoBERTa can be transformed into effective sentence and word encoders even via
simple self-supervised techniques. Inspired by this line of work, in this paper
we propose a fully unsupervised approach to improving word-in-context (WiC)
representations in PLMs, achieved via a simple and efficient WiC-targeted
fine-tuning procedure: MirrorWiC. The proposed method leverages only raw texts
sampled from Wikipedia, assuming no sense-annotated data, and learns
context-aware word representations within a standard contrastive learning
setup. We experiment with a series of standard and comprehensive WiC benchmarks
across multiple languages. Our proposed fully unsupervised MirrorWiC models
obtain substantial gains over off-the-shelf PLMs across all monolingual,
multilingual and cross-lingual setups. Moreover, on some standard WiC
benchmarks, MirrorWiC is even on-par with supervised models fine-tuned with
in-task data and sense labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Volatility of the Political agenda in Public Opinion and News Media. (arXiv:1808.09037v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1808.09037">
<div class="article-summary-box-inner">
<span><p>Recent election surprises, regime changes, and political shocks indicate that
political agendas have become more fast-moving and volatile. The ability to
measure the complex dynamics of agenda change and capture the nature and extent
of volatility in political systems is therefore more crucial than ever before.
This study proposes a definition and operationalization of volatility that
combines insights from political science, communications, information theory,
and computational techniques. The proposed measures of fractionalization and
agenda change encompass the shifting salience of issues in the agenda as a
whole and allow the study of agendas across different domains. We evaluate
these metrics and compare them to other measures such as issue-level survival
rates and the Pedersen Index, which uses public-opinion poll data to measure
public agendas, as well as traditional media content to measure media agendas
in the UK and Germany. We show how these measures complement existing
approaches and could be employed in future agenda-setting research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NatCat: Weakly Supervised Text Classification with Naturally Annotated Resources. (arXiv:2009.14335v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14335">
<div class="article-summary-box-inner">
<span><p>We describe NatCat, a large-scale resource for text classification
constructed from three data sources: Wikipedia, Stack Exchange, and Reddit.
NatCat consists of document-category pairs derived from manual curation that
occurs naturally within online communities. To demonstrate its usefulness, we
build general purpose text classifiers by training on NatCat and evaluate them
on a suite of 11 text classification tasks (CatEval), reporting large
improvements compared to prior work. We benchmark different modeling choices
and resource combinations and show how tasks benefit from particular NatCat
data sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exemplar-Controllable Paraphrasing and Translation using Bitext. (arXiv:2010.05856v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05856">
<div class="article-summary-box-inner">
<span><p>Most prior work on exemplar-based syntactically controlled paraphrase
generation relies on automatically-constructed large-scale paraphrase datasets,
which are costly to create. We sidestep this prerequisite by adapting models
from prior work to be able to learn solely from bilingual text (bitext).
Despite only using bitext for training, and in near zero-shot conditions, our
single proposed model can perform four tasks: controlled paraphrase generation
in both languages and controlled machine translation in both language
directions. To evaluate these tasks quantitatively, we create three novel
evaluation datasets. Our experimental results show that our models achieve
competitive results on controlled paraphrase generation and strong performance
on controlled machine translation. Analysis shows that our models learn to
disentangle semantics and syntax in their latent representations, but still
suffer from semantic drift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can images help recognize entities? A study of the role of images for Multimodal NER. (arXiv:2010.12712v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12712">
<div class="article-summary-box-inner">
<span><p>Multimodal named entity recognition (MNER) requires to bridge the gap between
language understanding and visual context. While many multimodal neural
techniques have been proposed to incorporate images into the MNER task, the
model's ability to leverage multimodal interactions remains poorly understood.
In this work, we conduct in-depth analyses of existing multimodal fusion
techniques from different perspectives and describe the scenarios where adding
information from the image does not always boost performance. We also study the
use of captions as a way to enrich the context for MNER. Experiments on three
datasets from popular social platforms expose the bottleneck of existing
multimodal models and the situations where using captions is beneficial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tweet Sentiment Quantification: An Experimental Re-Evaluation. (arXiv:2011.08091v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08091">
<div class="article-summary-box-inner">
<span><p>Sentiment quantification is the task of training, by means of supervised
learning, estimators of the relative frequency (also called ``prevalence'') of
sentiment-related classes (such as \textsf{Positive}, \textsf{Neutral},
\textsf{Negative}) in a sample of unlabelled texts. This task is especially
important when these texts are tweets, since the final goal of most sentiment
classification efforts carried out on Twitter data is actually quantification
(and not the classification of individual tweets). It is well-known that
solving quantification by means of ``classify and count'' (i.e., by classifying
all unlabelled items by means of a standard classifier and counting the items
that have been assigned to a given class) is less than optimal in terms of
accuracy, and that more accurate quantification methods exist. Gao and
Sebastiani (2016) carried out a systematic comparison of quantification methods
on the task of tweet sentiment quantification. In hindsight, we observe that
the experimental protocol followed in that work was weak, and that the
reliability of the conclusions that were drawn from the results is thus
questionable. We now re-evaluate those quantification methods (plus a few more
modern ones) on exactly the same same datasets, this time following a now
consolidated and much more robust experimental protocol (which also involves
simulating the presence, in the test data, of class prevalence values very
different from those of the training set). This experimental protocol (even
without counting the newly added methods) involves a number of experiments
5,775 times larger than that of the original study. The results of our
experiments are dramatically different from those obtained by Gao and
Sebastiani, and they provide a different, much more solid understanding of the
relative strengths and weaknesses of different sentiment quantification
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons Learned. (arXiv:2101.00133v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00133">
<div class="article-summary-box-inner">
<span><p>We review the EfficientQA competition from NeurIPS 2020. The competition
focused on open-domain question answering (QA), where systems take natural
language questions as input and return natural language answers. The aim of the
competition was to build systems that can predict correct answers while also
satisfying strict on-disk memory budgets. These memory budgets were designed to
encourage contestants to explore the trade-off between storing retrieval
corpora or the parameters of learned models. In this report, we describe the
motivation and organization of the competition, review the best submissions,
and analyze system predictions to inform a discussion of evaluation for
open-domain QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Size Does Not Fit All: Finding the Optimal Subword Sizes for FastText Models across Languages. (arXiv:2102.02585v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02585">
<div class="article-summary-box-inner">
<span><p>Unsupervised representation learning of words from large multilingual corpora
is useful for downstream tasks such as word sense disambiguation, semantic text
similarity, and information retrieval. The representation precision of
log-bilinear fastText models is mostly due to their use of subword information.
In previous work, the optimization of fastText's subword sizes has not been
fully explored, and non-English fastText models were trained using subword
sizes optimized for English and German word analogy tasks. In our work, we find
the optimal subword sizes on the English, German, Czech, Italian, Spanish,
French, Hindi, Turkish, and Russian word analogy tasks. We then propose a
simple n-gram coverage model and we show that it predicts better-than-default
subword sizes on the Spanish, French, Hindi, Turkish, and Russian word analogy
tasks. We show that the optimization of fastText's subword sizes matters and
results in a 14% improvement on the Czech word analogy task. We also show that
expensive parameter optimization can be replaced by a simple n-gram coverage
model that consistently improves the accuracy of fastText models on the word
analogy tasks by up to 3% compared to the default subword sizes, and that it is
within 1% accuracy of the optimal subword sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Post-Processing Technique for Improving Readability Assessment of Texts using Word Mover's Distance. (arXiv:2103.07277v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07277">
<div class="article-summary-box-inner">
<span><p>Assessing the proper difficulty levels of reading materials or texts in
general is the first step towards effective comprehension and learning. In this
study, we improve the conventional methodology of automatic readability
assessment by incorporating the Word Mover's Distance (WMD) of ranked texts as
an additional post-processing technique to further ground the difficulty level
given by a model. Results of our experiments on three multilingual datasets in
Filipino, German, and English show that the post-processing technique
outperforms previous vanilla and ranking-based models using SVM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuning Pretrained Transformers into RNNs. (arXiv:2103.13076v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13076">
<div class="article-summary-box-inner">
<span><p>Transformers have outperformed recurrent neural networks (RNNs) in natural
language generation. But this comes with a significant computational cost, as
the attention mechanism's complexity scales quadratically with sequence length.
Efficient transformer variants have received increasing interest in recent
works. Among them, a linear-complexity recurrent variant has proven well suited
for autoregressive generation. It approximates the softmax attention with
randomized or heuristic feature maps, but can be difficult to train and may
yield suboptimal accuracy. This work aims to convert a pretrained transformer
into its efficient recurrent counterpart, improving efficiency while
maintaining accuracy. Specifically, we propose a swap-then-finetune procedure:
in an off-the-shelf pretrained transformer, we replace the softmax attention
with its linear-complexity recurrent alternative and then finetune. With a
learned feature map, our approach provides an improved tradeoff between
efficiency and accuracy over the standard transformer and other recurrent
variants. We also show that the finetuning process has lower training cost
relative to training these recurrent variants from scratch. As many models for
natural language tasks are increasingly dependent on large-scale pretrained
transformers, this work presents a viable approach to improving inference
efficiency without repeating the expensive pretraining process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Keyword is an Island: In search of covert associations. (arXiv:2103.17114v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17114">
<div class="article-summary-box-inner">
<span><p>This paper describes how corpus-assisted discourse analysis based on keyword
(KW) identification and interpretation can benefit from employing Market basket
analysis (MBA) after KW extraction. MBA is a data mining technique used
originally in marketing that can reveal consistent associations between items
in a shopping cart, but also between keywords in a corpus of many texts. By
identifying recurring associations between KWs we can compensate for the lack
of wider context which is a major issue impeding the interpretation of isolated
KWs (esp. when analyzing large data). To showcase the advantages of MBA in
"re-contextualizing" keywords within the discourse, a pilot study on the topic
of migration was conducted contrasting anti-system and center-right Czech
internet media. was conducted. The results show that MBA is useful in
identifying the dominant strategy of anti-system news portals: to weave in a
confounding ideological undercurrent and connect the concept of migrants to a
multitude of other topics (i.e., flooding the discourse).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SYSML: StYlometry with Structure and Multitask Learning: Implications for Darknet Forum Migrant Analysis. (arXiv:2104.00764v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00764">
<div class="article-summary-box-inner">
<span><p>Darknet market forums are frequently used to exchange illegal goods and
services between parties who use encryption to conceal their identities. The
Tor network is used to host these markets, which guarantees additional
anonymization from IP and location tracking, making it challenging to link
across malicious users using multiple accounts (sybils). Additionally, users
migrate to new forums when one is closed, making it difficult to link users
across multiple forums. We develop a novel stylometry-based multitask learning
approach for natural language and interaction modeling using graph embeddings
to construct low-dimensional representations of short episodes of user activity
for authorship attribution. We provide a comprehensive evaluation of our
methods across four different darknet forums demonstrating its efficacy over
the state-of-the-art, with a lift of up to 2.5X on Mean Retrieval Rank and 2X
on Recall@10.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IGA : An Intent-Guided Authoring Assistant. (arXiv:2104.07000v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07000">
<div class="article-summary-box-inner">
<span><p>While large-scale pretrained language models have significantly improved
writing assistance functionalities such as autocomplete, more complex and
controllable writing assistants have yet to be explored. We leverage advances
in language modeling to build an interactive writing assistant that generates
and rephrases text according to fine-grained author specifications. Users
provide input to our Intent-Guided Assistant (IGA) in the form of text
interspersed with tags that correspond to specific rhetorical directives (e.g.,
adding description or contrast, or rephrasing a particular sentence). We
fine-tune a language model on a dataset heuristically-labeled with author
intent, which allows IGA to fill in these tags with generated text that users
can subsequently edit to their liking. A series of automatic and crowdsourced
evaluations confirm the quality of IGA's generated outputs, while a small-scale
user study demonstrates author preference for IGA over baseline methods in a
creative writing task. We release our dataset, code, and demo to spur further
research into AI-assisted writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Alignment-Agnostic Model for Chinese Text Error Correction. (arXiv:2104.07190v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07190">
<div class="article-summary-box-inner">
<span><p>This paper investigates how to correct Chinese text errors with types of
mistaken, missing and redundant characters, which is common for Chinese native
speakers. Most existing models based on detect-correct framework can correct
mistaken characters errors, but they cannot deal with missing or redundant
characters. The reason is that lengths of sentences before and after correction
are not the same, leading to the inconsistence between model inputs and
outputs. Although the Seq2Seq-based or sequence tagging methods provide
solutions to the problem and achieved relatively good results on English
context, but they do not perform well in Chinese context according to our
experimental results. In our work, we propose a novel detect-correct framework
which is alignment-agnostic, meaning that it can handle both text aligned and
non-aligned occasions, and it can also serve as a cold start model when there
are no annotated data provided. Experimental results on three datasets
demonstrate that our method is effective and achieves the best performance
among existing published models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multivalent Entailment Graphs for Question Answering. (arXiv:2104.07846v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07846">
<div class="article-summary-box-inner">
<span><p>Drawing inferences between open-domain natural language predicates is a
necessity for true language understanding. There has been much progress in
unsupervised learning of entailment graphs for this purpose. We make three
contributions: (1) we reinterpret the Distributional Inclusion Hypothesis to
model entailment between predicates of different valencies, like DEFEAT(Biden,
Trump) entails WIN(Biden); (2) we actualize this theory by learning
unsupervised Multivalent Entailment Graphs of open-domain predicates; and (3)
we demonstrate the capabilities of these graphs on a novel question answering
task. We show that directional entailment is more helpful for inference than
bidirectional similarity on questions of fine-grained semantics. We also show
that drawing on evidence across valencies answers more questions than by using
only the same valency evidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Across Time: What Does RoBERTa Know and When?. (arXiv:2104.07885v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07885">
<div class="article-summary-box-inner">
<span><p>Models of language trained on very large corpora have been demonstrated
useful for NLP. As fixed artifacts, they have become the object of intense
study, with many researchers "probing" the extent to which linguistic
abstractions, factual and commonsense knowledge, and reasoning abilities they
acquire and readily demonstrate. Building on this line of work, we consider a
new question: for types of knowledge a language model learns, when during
(pre)training are they acquired? We plot probing performance across iterations,
using RoBERTa as a case study. Among our findings: linguistic knowledge is
acquired fast, stably, and robustly across domains. Facts and commonsense are
slower and more domain-sensitive. Reasoning abilities are, in general, not
stably acquired. As new datasets, pretraining protocols, and probes emerge, we
believe that probing-across-time analyses can help researchers understand the
complex, intermingled learning that these models undergo and guide us toward
more efficient approaches that accomplish necessary learning faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models are Few-Shot Butlers. (arXiv:2104.07972v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07972">
<div class="article-summary-box-inner">
<span><p>Pretrained language models demonstrate strong performance in most NLP tasks
when fine-tuned on small task-specific datasets. Hence, these autoregressive
models constitute ideal agents to operate in text-based environments where
language understanding and generative capabilities are essential. Nonetheless,
collecting expert demonstrations in such environments is a time-consuming
endeavour. We introduce a two-stage procedure to learn from a small set of
demonstrations and further improve by interacting with an environment. We show
that language models fine-tuned with only 1.2% of the expert demonstrations and
a simple reinforcement learning algorithm achieve a 51% absolute improvement in
success rate over existing methods in the ALFWorld environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Passage Ranking for Diverse Multi-Answer Retrieval. (arXiv:2104.08445v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08445">
<div class="article-summary-box-inner">
<span><p>We study multi-answer retrieval, an under-explored problem that requires
retrieving passages to cover multiple distinct answers for a given question.
This task requires joint modeling of retrieved passages, as models should not
repeatedly retrieve passages containing the same answer at the cost of missing
a different valid answer. In this paper, we introduce JPR, the first joint
passage retrieval model for multi-answer retrieval. JPR makes use of an
autoregressive reranker that selects a sequence of passages, each conditioned
on previously selected passages. JPR is trained to select passages that cover
new answers at each timestep and uses a tree-decoding algorithm to enable
flexibility in the degree of diversity. Compared to prior approaches, JPR
achieves significantly better answer coverage on three multi-answer datasets.
When combined with downstream question answering, the improved retrieval
enables larger answer generation models since they need to consider fewer
passages, establishing a new state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Aware Interaction Network for Question Matching. (arXiv:2104.08451v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08451">
<div class="article-summary-box-inner">
<span><p>Impressive milestones have been achieved in text matching by adopting a
cross-attention mechanism to capture pertinent semantic connections between two
sentence representations. However, regular cross-attention focuses on
word-level links between the two input sequences, neglecting the importance of
contextual information. We propose a context-aware interaction network (COIN)
to properly align two sequences and infer their semantic relationship.
Specifically, each interaction block includes (1) a context-aware
cross-attention mechanism to effectively integrate contextual information when
aligning two sequences, and (2) a gate fusion layer to flexibly interpolate
aligned representations. We apply multiple stacked interaction blocks to
produce alignments at different levels and gradually refine the attention
results. Experiments on two question matching datasets and detailed analyses
demonstrate the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AM2iCo: Evaluating Word Meaning in Context across Low-Resource Languages with Adversarial Examples. (arXiv:2104.08639v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08639">
<div class="article-summary-box-inner">
<span><p>Capturing word meaning in context and distinguishing between correspondences
and variations across languages is key to building successful multilingual and
cross-lingual text representation models. However, existing multilingual
evaluation datasets that evaluate lexical semantics "in-context" have various
limitations. In particular, 1) their language coverage is restricted to
high-resource languages and skewed in favor of only a few language families and
areas, 2) a design that makes the task solvable via superficial cues, which
results in artificially inflated (and sometimes super-human) performances of
pretrained encoders, on many target languages, which limits their usefulness
for model probing and diagnostics, and 3) little support for cross-lingual
evaluation. In order to address these gaps, we present AM2iCo (Adversarial and
Multilingual Meaning in Context), a wide-coverage cross-lingual and
multilingual evaluation set; it aims to faithfully assess the ability of
state-of-the-art (SotA) representation models to understand the identity of
word meaning in cross-lingual contexts for 14 language pairs. We conduct a
series of experiments in a wide range of setups and demonstrate the challenging
nature of AM2iCo. The results reveal that current SotA pretrained encoders
substantially lag behind human performance, and the largest gaps are observed
for low-resource languages and languages dissimilar to English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Answers with Entailment Trees. (arXiv:2104.08661v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08661">
<div class="article-summary-box-inner">
<span><p>Our goal, in the context of open-domain textual question-answering (QA), is
to explain answers by showing the line of reasoning from what is known to the
answer, rather than simply showing a fragment of textual evidence (a
"rationale'"). If this could be done, new opportunities for understanding and
debugging the system's reasoning become possible. Our approach is to generate
explanations in the form of entailment trees, namely a tree of multipremise
entailment steps from facts that are known, through intermediate conclusions,
to the hypothesis of interest (namely the question + answer). To train a model
with this skill, we created ENTAILMENTBANK, the first dataset to contain
multistep entailment trees. Given a hypothesis (question + answer), we define
three increasingly difficult explanation tasks: generate a valid entailment
tree given (a) all relevant sentences (b) all relevant and some irrelevant
sentences, or (c) a corpus. We show that a strong language model can partially
solve these tasks, in particular when the relevant sentences are included in
the input (e.g., 35% of trees for (a) are perfect), and with indications of
generalization to other domains. This work is significant as it provides a new
type of dataset (multistep entailments) and baselines, offering a new avenue
for the community to generate richer, more systematic explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting. (arXiv:2104.09691v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09691">
<div class="article-summary-box-inner">
<span><p>Since the seminal work of Mikolov et al. (2013a) and Bojanowski et al.
(2017), word representations of shallow log-bilinear language models have found
their way into many NLP applications. Mikolov et al. (2018) introduced a
positional log-bilinear language model, which has characteristics of an
attention-based language model and which has reached state-of-the-art
performance on the intrinsic word analogy task. However, the positional model
has never been evaluated on qualitative criteria or extrinsic tasks and its
speed is impractical.
</p>
<p>We outline the similarities between the attention mechanism and the
positional model, and we propose a constrained positional model, which adapts
the sparse attention mechanism of Dai et al. (2018). We evaluate the positional
and constrained positional models on three novel qualitative criteria and on
the extrinsic language modeling task of Botha and Blunsom (2014).
</p>
<p>We show that the positional and constrained positional models contain
interpretable information about word order and outperform the subword model of
Bojanowski et al. (2017) on language modeling. We also show that the
constrained positional model outperforms the positional model on language
modeling and is twice as fast.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRILL: Dynamic Representations for Imbalanced Lifelong Learning. (arXiv:2105.08445v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08445">
<div class="article-summary-box-inner">
<span><p>Continual or lifelong learning has been a long-standing challenge in machine
learning to date, especially in natural language processing (NLP). Although
state-of-the-art language models such as BERT have ushered in a new era in this
field due to their outstanding performance in multitask learning scenarios,
they suffer from forgetting when being exposed to a continuous stream of data
with shifting data distributions. In this paper, we introduce DRILL, a novel
continual learning architecture for open-domain text classification. DRILL
leverages a biologically inspired self-organizing neural architecture to
selectively gate latent language representations from BERT in a
task-incremental manner. We demonstrate in our experiments that DRILL
outperforms current methods in a realistic scenario of imbalanced,
non-stationary data without prior knowledge about task boundaries. To the best
of our knowledge, DRILL is the first of its kind to use a self-organizing
neural architecture for open-domain lifelong learning in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What if This Modified That? Syntactic Interventions via Counterfactual Embeddings. (arXiv:2105.14002v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14002">
<div class="article-summary-box-inner">
<span><p>Neural language models exhibit impressive performance on a variety of tasks,
but their internal reasoning may be difficult to understand. Prior art aims to
uncover meaningful properties within model representations via probes, but it
is unclear how faithfully such probes portray information that the models
actually use. To overcome such limitations, we propose a technique, inspired by
causal analysis, for generating counterfactual embeddings within models. In
experiments testing our technique, we produce evidence that suggests some
BERT-based models use a tree-distance-like representation of syntax in
downstream prediction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"You made me feel this way": Investigating Partners' Influence in Predicting Emotions in Couples' Conflict Interactions using Speech Data. (arXiv:2106.01526v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01526">
<div class="article-summary-box-inner">
<span><p>How romantic partners interact with each other during a conflict influences
how they feel at the end of the interaction and is predictive of whether the
partners stay together in the long term. Hence understanding the emotions of
each partner is important. Yet current approaches that are used include
self-reports which are burdensome and hence limit the frequency of this data
collection. Automatic emotion prediction could address this challenge. Insights
from psychology research indicate that partners' behaviors influence each
other's emotions in conflict interaction and hence, the behavior of both
partners could be considered to better predict each partner's emotion. However,
it is yet to be investigated how doing so compares to only using each partner's
own behavior in terms of emotion prediction performance. In this work, we used
BERT to extract linguistic features (i.e., what partners said) and openSMILE to
extract paralinguistic features (i.e., how they said it) from a data set of 368
German-speaking Swiss couples (N = 736 individuals) who were videotaped during
an 8-minutes conflict interaction in the laboratory. Based on those features,
we trained machine learning models to predict if partners feel positive or
negative after the conflict interaction. Our results show that including the
behavior of the other partner improves the prediction performance. Furthermore,
for men, considering how their female partners spoke is most important and for
women considering what their male partner said is most important in getting
better prediction performance. This work is a step towards automatically
recognizing each partners' emotion based on the behavior of both, which would
enable a better understanding of couples in research, therapy, and the real
world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT meets LIWC: Exploring State-of-the-Art Language Models for Predicting Communication Behavior in Couples' Conflict Interactions. (arXiv:2106.01536v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01536">
<div class="article-summary-box-inner">
<span><p>Many processes in psychology are complex, such as dyadic interactions between
two interacting partners (e.g. patient-therapist, intimate relationship
partners). Nevertheless, many basic questions about interactions are difficult
to investigate because dyadic processes can be within a person and between
partners, they are based on multimodal aspects of behavior and unfold rapidly.
Current analyses are mainly based on the behavioral coding method, whereby
human coders annotate behavior based on a coding schema. But coding is
labor-intensive, expensive, slow, focuses on few modalities. Current approaches
in psychology use LIWC for analyzing couples' interactions. However, advances
in natural language processing such as BERT could enable the development of
systems to potentially automate behavioral coding, which in turn could
substantially improve psychological research. In this work, we train machine
learning models to automatically predict positive and negative communication
behavioral codes of 368 German-speaking Swiss couples during an 8-minute
conflict interaction on a fine-grained scale (10-seconds sequences) using
linguistic features and paralinguistic features derived with openSMILE. Our
results show that both simpler TF-IDF features as well as more complex BERT
features performed better than LIWC, and that adding paralinguistic features
did not improve the performance. These results suggest it might be time to
consider modern alternatives to LIWC, the de facto linguistic features in
psychology, for prediction tasks in couples research. This work is a further
step towards the automated coding of couples' behavior which could enhance
couple research and therapy, and be utilized for other dyadic interactions as
well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Programming Puzzles. (arXiv:2106.05784v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05784">
<div class="article-summary-box-inner">
<span><p>We introduce a new type of programming challenge called programming puzzles,
as an objective and comprehensive evaluation of program synthesis, and release
an open-source dataset of Python Programming Puzzles (P3). Each puzzle is
defined by a short Python program $f$, and the goal is to find an input $x$
which makes $f$ output "True". The puzzles are objective in that each one is
specified entirely by the source code of its verifier $f$, so evaluating $f(x)$
is all that is needed to test a candidate solution $x$. They do not require an
answer key or input/output examples, nor do they depend on natural language
understanding. The dataset is comprehensive in that it spans problems of a
range of difficulties and domains, ranging from trivial string manipulation
problems that are immediately obvious to human programmers (but not necessarily
to AI), to classic programming puzzles (e.g., Towers of Hanoi), to
interview/competitive-programming problems (e.g., dynamic programming), to
longstanding open problems in algorithms and mathematics (e.g., factoring). The
objective nature of P3 readily supports self-supervised bootstrapping. We
develop baseline enumerative program synthesis and GPT-3 solvers that are
capable of solving easy puzzles -- even without access to any reference
solutions -- by learning from their own past solutions. Based on a small user
study, we find puzzle difficulty to correlate between human programmers and the
baseline AI solvers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised and Unsupervised Sense Annotation via Translations. (arXiv:2106.06462v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06462">
<div class="article-summary-box-inner">
<span><p>Acquisition of multilingual training data continues to be a challenge in word
sense disambiguation (WSD). To address this problem, unsupervised approaches
have been proposed to automatically generate sense annotations for training
supervised WSD systems. We present three new methods for creating
sense-annotated corpora which leverage translations, parallel bitexts, lexical
resources, as well as contextual and synset embeddings. Our semi-supervised
method applies machine translation to transfer existing sense annotations to
other languages. Our two unsupervised methods refine sense annotations produced
by a knowledge-based WSD system via lexical translations in a parallel corpus.
We obtain state-of-the-art results on standard WSD benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAS: Self-Augmented Strategy for Language Model Pre-training. (arXiv:2106.07176v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07176">
<div class="article-summary-box-inner">
<span><p>The core of a self-supervised learning method for pre-training language
models includes the design of appropriate data augmentation and corresponding
pre-training task(s). Most data augmentations in language model pre-training
are context-independent. The seminal contextualized augmentation recently
proposed by the ELECTRA requires a separate generator, which leads to extra
computation cost as well as the challenge in adjusting the capability of its
generator relative to that of the other model component(s). We propose a
self-augmented strategy (SAS) that uses a single forward pass through the model
to augment the input data for model training in the next epoch. Essentially our
strategy eliminates a separate generator network and uses only one network to
generate the data augmentation and undertake two pre-training tasks (the MLM
task and the RTD task) jointly, which naturally avoids the challenge in
adjusting the generator's capability as well as reduces the computation cost.
Additionally, our SAS is a general strategy such that it can seamlessly
incorporate many new techniques emerging recently or in the future, such as the
disentangled attention mechanism recently proposed by the DeBERTa model. Our
experiments show that our SAS is able to outperform the ELECTRA and other
state-of-the-art models in the GLUE tasks with the same or less computation
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilateral Personalized Dialogue Generation with Dynamic Persona-Aware Fusion. (arXiv:2106.07857v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07857">
<div class="article-summary-box-inner">
<span><p>Generating personalized responses is one of the major challenges in natural
human-robot interaction. Current researches in this field mainly focus on
generating responses consistent with the robot's pre-assigned persona, while
ignoring the user's persona. Such responses may be inappropriate or even
offensive, which may lead to the bad user experience. Therefore, we propose a
bilateral personalized dialogue generation (BPDG) method with dynamic
persona-aware fusion via multi-task transfer learning to generate responses
consistent with both personas. The proposed method aims to accomplish three
learning tasks: 1) an encoder is trained with dialogue utterances added with
corresponded personalized attributes and relative position (language model
task), 2) a dynamic persona-aware fusion module predicts the persona presence
to adaptively fuse the contextual and bilateral personas encodings (persona
prediction task) and 3) a decoder generates natural, fluent and personalized
responses (dialogue generation task). To make the generated responses more
personalized and bilateral persona-consistent, the Conditional Mutual
Information Maximum (CMIM) criterion is adopted to select the final response
from the generated candidates. The experimental results show that the proposed
method outperforms several state-of-the-art methods in terms of both automatic
and manual evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Modular and Joint Approaches for Speaker-Attributed ASR on Monaural Long-Form Audio. (arXiv:2107.02852v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02852">
<div class="article-summary-box-inner">
<span><p>Speaker-attributed automatic speech recognition (SA-ASR) is a task to
recognize "who spoke what" from multi-talker recordings. An SA-ASR system
usually consists of multiple modules such as speech separation, speaker
diarization and ASR. On the other hand, considering the joint optimization, an
end-to-end (E2E) SA-ASR model has recently been proposed with promising results
on simulation data. In this paper, we present our recent study on the
comparison of such modular and joint approaches towards SA-ASR on real monaural
recordings. We develop state-of-the-art SA-ASR systems for both modular and
joint approaches by leveraging large-scale training data, including 75 thousand
hours of ASR training data and the VoxCeleb corpus for speaker representation
learning. We also propose a new pipeline that performs the E2E SA-ASR model
after speaker clustering. Our evaluation on the AMI meeting corpus reveals that
after fine-tuning with a small real data, the joint system performs 8.9--29.9%
better in accuracy compared to the best modular system while the modular system
performs better before such fine-tuning. We also conduct various error analyses
to show the remaining issues for the monaural SA-ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08661">
<div class="article-summary-box-inner">
<span><p>We present Translatotron 2, a neural direct speech-to-speech translation
model that can be trained end-to-end. Translatotron 2 consists of a speech
encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention
module that connects all the previous three components. Experimental results
suggest that Translatotron 2 outperforms the original Translatotron by a large
margin in terms of translation quality and predicted speech naturalness, and
drastically improves the robustness of the predicted speech by mitigating
over-generation, such as babbling or long pause. We also propose a new method
for retaining the source speaker's voice in the translated speech. The trained
model is restricted to retain the source speaker's voice, but unlike the
original Translatotron, it is not able to generate speech in a different
speaker's voice, making the model more robust for production deployment, by
mitigating potential misuse for creating spoofing audio artifacts. When the new
method is used together with a simple concatenation-based data augmentation,
the trained Translatotron 2 model is able to retain each speaker's voice for
input with speaker turns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural News Recommendation with Collaborative News Encoding and Structural User Encoding. (arXiv:2109.00750v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00750">
<div class="article-summary-box-inner">
<span><p>Automatic news recommendation has gained much attention from the academic
community and industry. Recent studies reveal that the key to this task lies
within the effective representation learning of both news and users. Existing
works typically encode news title and content separately while neglecting their
semantic interaction, which is inadequate for news text comprehension. Besides,
previous models encode user browsing history without leveraging the structural
correlation of user browsed news to reflect user interests explicitly. In this
work, we propose a news recommendation framework consisting of collaborative
news encoding (CNE) and structural user encoding (SUE) to enhance news and user
representation learning. CNE equipped with bidirectional LSTMs encodes news
title and content collaboratively with cross-selection and cross-attention
modules to learn semantic-interactive news representations. SUE utilizes graph
convolutional networks to extract cluster-structural features of user history,
followed by intra-cluster and inter-cluster attention modules to learn
hierarchical user interest representations. Experiment results on the MIND
dataset validate the effectiveness of our model to improve the performance of
news recommendation. Our code is released at
https://github.com/Veason-silverbullet/NNR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers. (arXiv:2109.00799v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00799">
<div class="article-summary-box-inner">
<span><p>Developing automatic Math Word Problem (MWP) solvers has been an interest of
NLP researchers since the 1960s. Over the last few years, there are a growing
number of datasets and deep learning-based methods proposed for effectively
solving MWPs. However, most existing methods are benchmarked soly on one or two
datasets, varying in different configurations, which leads to a lack of
unified, standardized, fair, and comprehensive comparison between methods. This
paper presents MWPToolkit, the first open-source framework for solving MWPs. In
MWPToolkit, we decompose the procedure of existing MWP solvers into multiple
core components and decouple their models into highly reusable modules. We also
provide a hyper-parameter search function to boost the performance. In total,
we implement and compare 17 MWP solvers on 4 widely-used single equation
generation benchmarks and 2 multiple equations generation benchmarks. These
features enable our MWPToolkit to be suitable for researchers to reproduce
advanced baseline models and develop new MWP solvers quickly. Code and
documents are available at https://github.com/LYH-YF/MWPToolkit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection. (arXiv:2109.02051v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02051">
<div class="article-summary-box-inner">
<span><p>Many endeavors have sought to develop countermeasure techniques as
enhancements on Automatic Speaker Verification (ASV) systems, in order to make
them more robust against spoof attacks. As evidenced by the latest ASVspoof
2019 countermeasure challenge, models currently deployed for the task of ASV
are, at their best, devoid of suitable degrees of generalization to unseen
attacks. Upon further investigation of the proposed methods, it appears that a
broader three-tiered view of the proposed systems. comprised of the classifier,
feature extraction phase, and model loss function, may to some extent lessen
the problem. Accordingly, the present study proposes the Efficient Attention
Branch Network (EABN) modular architecture with a combined loss function to
address the generalization problem...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Language Models with Plug-and-Play Large-Scale Commonsense. (arXiv:2109.02572v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02572">
<div class="article-summary-box-inner">
<span><p>We study how to enhance language models (LMs) with textual commonsense
knowledge. Previous work (e.g., KnowBERT) has focused on the integrating entity
knowledge from knowledge graphs. In order to introduce the external entity
embeddings, they learn to jointly represent the original sentences and external
knowledge by pre-training on a large scale corpus. However, when switching to
textual commonsense, unlike the light entity embeddings, the encoding of
commonsense descriptions is heavy. Therefore, the pre-training for learning to
jointly represent the target sentence and external commonsense descriptions is
unaffordable. On the other hand, since pre-trained LMs for representing the
target sentences alone are readily available, is it feasible to introduce
commonsense knowledge in downstream tasks by fine-tuning them only? In this
paper, we propose a plug-and-play method for large-scale commonsense
integration without pre-training. Our method is inspired by the observation
that in the regular fine-tuning for downstream tasks where no external
knowledge was introduced, the variation in the parameters of the language model
was minor. Our method starts from a pre-trained LM that represents the target
sentences only (e.g., BERT). We think that the pre-training for joint
representation learning can be avoided, if the joint representation reduces the
impact of parameters on the starting LM. Previous methods such as KnowBERT
proposed complex modifications to the vanilla LM to introduce external
knowledge. Our model (Cook-Transformer, COmmOnsense Knowledge-enhanced
Transformer), on the other hand, hardly changes the vanilla LM except adding a
knowledge token in each Transformer layer. In a variety of experiments,
COOK-Transformer-based BERT/RoBERTa improve their effect without any
pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Select One Among All? An Extensive Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding. (arXiv:2109.05696v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05696">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) is a model compression algorithm that helps
transfer the knowledge of a large neural network into a smaller one. Even
though KD has shown promise on a wide range of Natural Language Processing
(NLP) applications, little is understood about how one KD algorithm compares to
another and whether these approaches can be complimentary to each other. In
this work, we evaluate various KD algorithms on in-domain, out-of-domain and
adversarial testing. We propose a framework to assess the adversarial
robustness of multiple KD algorithms. Moreover, we introduce a new KD
algorithm, Combined-KD, which takes advantage of two promising approaches
(better training scheme and more efficient data augmentation). Our extensive
experimental results show that Combined-KD achieves state-of-the-art results on
the GLUE benchmark, out-of-domain generalization, and adversarial robustness
compared to competitive methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text is NOT Enough: Integrating Visual Impressions into Open-domain Dialogue Generation. (arXiv:2109.05778v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05778">
<div class="article-summary-box-inner">
<span><p>Open-domain dialogue generation in natural language processing (NLP) is by
default a pure-language task, which aims to satisfy human need for daily
communication on open-ended topics by producing related and informative
responses. In this paper, we point out that hidden images, named as visual
impressions (VIs), can be explored from the text-only data to enhance dialogue
understanding and help generate better responses. Besides, the semantic
dependency between an dialogue post and its response is complicated, e.g., few
word alignments and some topic transitions. Therefore, the visual impressions
of them are not shared, and it is more reasonable to integrate the response
visual impressions (RVIs) into the decoder, rather than the post visual
impressions (PVIs). However, both the response and its RVIs are not given
directly in the test process. To handle the above issues, we propose a
framework to explicitly construct VIs based on pure-language dialogue datasets
and utilize them for better dialogue understanding and generation.
Specifically, we obtain a group of images (PVIs) for each post based on a
pre-trained word-image mapping model. These PVIs are used in a co-attention
encoder to get a post representation with both visual and textual information.
Since the RVIs are not provided directly during testing, we design a cascade
decoder that consists of two sub-decoders. The first sub-decoder predicts the
content words in response, and applies the word-image mapping model to get
those RVIs. Then, the second sub-decoder generates the response based on the
post and RVIs. Experimental results on two open-domain dialogue datasets show
that our proposed approach achieves superior performance over competitive
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?. (arXiv:2109.07102v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07102">
<div class="article-summary-box-inner">
<span><p>There have been many efforts to try to understand what gram-matical knowledge
(e.g., ability to understand the part of speech of a token) is encoded in large
pre-trained language models (LM). This is done through 'Edge Probing' (EP)
tests: simple ML models that predict the grammatical properties ofa span
(whether it has a particular part of speech) using only the LM's token
representations. However, most NLP applications use fine-tuned LMs. Here, we
ask: if a LM is fine-tuned, does the encoding of linguistic information in it
change, as measured by EP tests? Conducting experiments on multiple
question-answering (QA) datasets, we answer that question negatively: the EP
test results do not change significantly when the fine-tuned QA model performs
well or in adversarial situations where the model is forced to learn wrong
correlations. However, a critical analysis of the EP task datasets reveals that
EP models may rely on spurious correlations to make predictions. This indicates
even if fine-tuning changes the encoding of such knowledge, the EP tests might
fail to measure it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Modeling Aspect and Polarity for Aspect-based Sentiment Analysis in Persian Reviews. (arXiv:2109.07680v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07680">
<div class="article-summary-box-inner">
<span><p>Identification of user's opinions from natural language text has become an
exciting field of research due to its growing applications in the real world.
The research field is known as sentiment analysis and classification, where
aspect category detection (ACD) and aspect category polarity (ACP) are two
important sub-tasks of aspect-based sentiment analysis. The goal in ACD is to
specify which aspect of the entity comes up in opinion while ACP aims to
specify the polarity of each aspect category from the ACD task. The previous
works mostly propose separate solutions for these two sub-tasks. This paper
focuses on the ACD and ACP sub-tasks to solve both problems simultaneously. The
proposed method carries out multi-label classification where four different
deep models were employed and comparatively evaluated to examine their
performance. A dataset of Persian reviews was collected from CinemaTicket
website including 2200 samples from 14 categories. The developed models were
evaluated using the collected dataset in terms of example-based and label-based
metrics. The results indicate the high applicability and preference of the CNN
and GRU models in comparison to LSTM and Bi-LSTM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Emotion Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation. (arXiv:2109.07779v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07779">
<div class="article-summary-box-inner">
<span><p>Researches on dialogue empathy aim to endow an agent with the capacity of
accurate understanding and proper responding for emotions. Existing models for
empathetic dialogue generation focus on the emotion flow in one direction, that
is, from the context to response. We argue that conducting an empathetic
conversation is a bidirectional process, where empathy occurs when the emotions
of two interlocutors could converge on the same point, i.e., reaching an
emotion consensus. Besides, we also find that the empathetic dialogue corpus is
extremely limited, which further restricts the model performance. To address
the above issues, we propose a dual-generative model, Dual-Emp, to
simultaneously construct the emotion consensus and utilize some external
unpaired data. Specifically, our model integrates a forward dialogue model, a
backward dialogue model, and a discrete latent variable representing the
emotion consensus into a unified architecture. Then, to alleviate the
constraint of paired data, we extract unpaired emotional data from open-domain
conversations and employ Dual-Emp to produce pseudo paired empathetic samples,
which is more efficient and low-cost than the human annotation. Automatic and
human evaluations demonstrate that our method outperforms competitive baselines
in producing coherent and empathetic responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as a Knowledge Source for Cognitive Agents. (arXiv:2109.08270v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08270">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are sentence-completion engines trained on massive
corpora. LMs have emerged as a significant breakthrough in natural-language
processing, providing capabilities that go far beyond sentence completion
including question answering, summarization, and natural-language inference.
While many of these capabilities have potential application to cognitive
systems, exploiting language models as a source of task knowledge, especially
for task learning, offers significant, near-term benefits. We introduce
language models and the various tasks to which they have been applied and then
review methods of knowledge extraction from language models. The resulting
analysis outlines both the challenges and opportunities for using language
models as a new knowledge source for cognitive systems. It also identifies
possible ways to improve knowledge extraction from language models using the
capabilities provided by cognitive systems. Central to success will be the
ability of a cognitive agent to itself learn an abstract model of the knowledge
implicit in the LM as well as methods to extract high-quality knowledge
effectively and efficiently. To illustrate, we introduce a hypothetical robot
agent and describe how language models could extend its task knowledge and
improve its performance and the kinds of knowledge and methods the agent can
use to exploit the knowledge within a language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers. (arXiv:2109.08406v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08406">
<div class="article-summary-box-inner">
<span><p>Despite the success of fine-tuning pretrained language encoders like BERT for
downstream natural language understanding (NLU) tasks, it is still poorly
understood how neural networks change after fine-tuning. In this work, we use
centered kernel alignment (CKA), a method for comparing learned
representations, to measure the similarity of representations in task-tuned
models across layers. In experiments across twelve NLU tasks, we discover a
consistent block diagonal structure in the similarity of representations within
fine-tuned RoBERTa and ALBERT models, with strong similarity within clusters of
earlier and later layers, but not between them. The similarity of later layer
representations implies that later layers only marginally contribute to task
performance, and we verify in experiments that the top few layers of fine-tuned
Transformers can be discarded without hurting performance, even with no further
tuning.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Asymmetric 3D Context Fusion for Universal Lesion Detection. (arXiv:2109.08684v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08684">
<div class="article-summary-box-inner">
<span><p>Modeling 3D context is essential for high-performance 3D medical image
analysis. Although 2D networks benefit from large-scale 2D supervised
pretraining, it is weak in capturing 3D context. 3D networks are strong in 3D
context yet lack supervised pretraining. As an emerging technique, \emph{3D
context fusion operator}, which enables conversion from 2D pretrained networks,
leverages the advantages of both and has achieved great success. Existing 3D
context fusion operators are designed to be spatially symmetric, i.e.,
performing identical operations on each 2D slice like convolutions. However,
these operators are not truly equivariant to translation, especially when only
a few 3D slices are used as inputs. In this paper, we propose a novel
asymmetric 3D context fusion operator (A3D), which uses different weights to
fuse 3D context from different 2D slices. Notably, A3D is NOT
translation-equivariant while it significantly outperforms existing symmetric
context fusion operators without introducing large computational overhead. We
validate the effectiveness of the proposed method by extensive experiments on
DeepLesion benchmark, a large-scale public dataset for universal lesion
detection from computed tomography (CT). The proposed A3D consistently
outperforms symmetric context fusion operators by considerable margins, and
establishes a new \emph{state of the art} on DeepLesion. To facilitate open
research, our code and model in PyTorch are available at
https://github.com/M3DV/AlignShift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised learning methods and applications in medical imaging analysis: A survey. (arXiv:2109.08685v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08685">
<div class="article-summary-box-inner">
<span><p>The availability of high quality annotated medical imaging datasets is a
major problem that collides with machine learning applications in the field of
medical imaging analysis and impedes its advancement. Self-supervised learning
is a recent training paradigm that enables learning robust representations
without the need for human annotation which can be considered as an effective
solution for the scarcity in annotated medical data. This article reviews the
state-of-the-art research directions in self-supervised learning approaches for
image data with concentration on their applications in the field of medical
imaging analysis. The article covers a set of the most recent self-supervised
learning methods from the computer vision field as they are applicable to the
medical imaging analysis and categorize them as predictive, generative and
contrastive approaches. Moreover, the article covers (40) of the most recent
researches in the field of self-supervised learning in medical imaging analysis
aiming at shedding the light on the recent innovation in the field. Ultimately,
the article concludes with possible future research directions in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation of Brain MRI using an Altruistic Harris Hawks' Optimization algorithm. (arXiv:2109.08688v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08688">
<div class="article-summary-box-inner">
<span><p>Segmentation is an essential requirement in medicine when digital images are
used in illness diagnosis, especially, in posterior tasks as analysis and
disease identification. An efficient segmentation of brain Magnetic Resonance
Images (MRIs) is of prime concern to radiologists due to their poor
illumination and other conditions related to de acquisition of the images.
Thresholding is a popular method for segmentation that uses the histogram of an
image to label different homogeneous groups of pixels into different classes.
However, the computational cost increases exponentially according to the number
of thresholds. In this paper, we perform the multi-level thresholding using an
evolutionary metaheuristic. It is an improved version of the Harris Hawks
Optimization (HHO) algorithm that combines the chaotic initialization and the
concept of altruism. Further, for fitness assignment, we use a hybrid objective
function where along with the cross-entropy minimization, we apply a new
entropy function, and leverage weights to the two objective functions to form a
new hybrid approach. The HHO was originally designed to solve numerical
optimization problems. Earlier, the statistical results and comparisons have
demonstrated that the HHO provides very promising results compared with
well-established metaheuristic techniques. In this article, the altruism has
been incorporated into the HHO algorithm to enhance its exploitation
capabilities. We evaluate the proposed method over 10 benchmark images from the
WBA database of the Harvard Medical School and 8 benchmark images from the
Brainweb dataset using some standard evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChipQA: No-Reference Video Quality Prediction via Space-Time Chips. (arXiv:2109.08726v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08726">
<div class="article-summary-box-inner">
<span><p>We propose a new model for no-reference video quality assessment (VQA). Our
approach uses a new idea of highly-localized space-time (ST) slices called
Space-Time Chips (ST Chips). ST Chips are localized cuts of video data along
directions that \textit{implicitly} capture motion. We use
perceptually-motivated bandpass and normalization models to first process the
video data, and then select oriented ST Chips based on how closely they fit
parametric models of natural video statistics. We show that the parameters that
describe these statistics can be used to reliably predict the quality of
videos, without the need for a reference video. The proposed method implicitly
models ST video naturalness, and deviations from naturalness. We train and test
our model on several large VQA databases, and show that our model achieves
state-of-the-art performance at reduced cost, without requiring motion
computation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised View-Invariant Human Posture Representation. (arXiv:2109.08730v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08730">
<div class="article-summary-box-inner">
<span><p>Most recent view-invariant action recognition and performance assessment
approaches rely on a large amount of annotated 3D skeleton data to extract
view-invariant features. However, acquiring 3D skeleton data can be cumbersome,
if not impractical, in in-the-wild scenarios. To overcome this problem, we
present a novel unsupervised approach that learns to extract view-invariant 3D
human pose representation from a 2D image without using 3D joint data. Our
model is trained by exploiting the intrinsic view-invariant properties of human
pose between simultaneous frames from different viewpoints and their
equivariant properties between augmented frames from the same viewpoint. We
evaluate the learned view-invariant pose representations for two downstream
tasks. We perform comparative experiments that show improvements on the
state-of-the-art unsupervised cross-view action classification accuracy on NTU
RGB+D by a significant margin, on both RGB and depth images. We also show the
efficiency of transferring the learned representations from NTU RGB+D to obtain
the first ever unsupervised cross-view and cross-subject rank correlation
results on the multi-view human movement quality dataset, QMAR, and marginally
improve on the-state-of-the-art supervised results for this dataset. We also
carry out ablation studies to examine the contributions of the different
components of our proposed network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto White-Balance Correction for Mixed-Illuminant Scenes. (arXiv:2109.08750v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08750">
<div class="article-summary-box-inner">
<span><p>Auto white balance (AWB) is applied by camera hardware at capture time to
remove the color cast caused by the scene illumination. The vast majority of
white-balance algorithms assume a single light source illuminates the scene;
however, real scenes often have mixed lighting conditions. This paper presents
an effective AWB method to deal with such mixed-illuminant scenes. A unique
departure from conventional AWB, our method does not require illuminant
estimation, as is the case in traditional camera AWB modules. Instead, our
method proposes to render the captured scene with a small set of predefined
white-balance settings. Given this set of rendered images, our method learns to
estimate weighting maps that are used to blend the rendered images to generate
the final corrected image. Through extensive experiments, we show this proposed
method produces promising results compared to other alternatives for single-
and mixed-illuminant scene color correction. Our source code and trained models
are available at https://github.com/mahmoudnafifi/mixedillWB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WiSoSuper: Benchmarking Super-Resolution Methods on Wind and Solar Data. (arXiv:2109.08770v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08770">
<div class="article-summary-box-inner">
<span><p>The transition to green energy grids depends on detailed wind and solar
forecasts to optimize the siting and scheduling of renewable energy generation.
Operational forecasts from numerical weather prediction models, however, only
have a spatial resolution of 10 to 20-km, which leads to sub-optimal usage and
development of renewable energy farms. Weather scientists have been developing
super-resolution methods to increase the resolution, but often rely on simple
interpolation techniques or computationally expensive differential
equation-based models. Recently, machine learning-based models, specifically
the physics-informed resolution-enhancing generative adversarial network
(PhIREGAN), have outperformed traditional downscaling methods. We provide a
thorough and extensible benchmark of leading deep learning-based
super-resolution techniques, including the enhanced super-resolution generative
adversarial network (ESRGAN) and an enhanced deep super-resolution (EDSR)
network, on wind and solar data. We accompany the benchmark with a novel
public, processed, and machine learning-ready dataset for benchmarking
super-resolution methods on wind and solar data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locally Weighted Mean Phase Angle (LWMPA) Based Tone Mapping Quality Index (TMQI-3). (arXiv:2109.08774v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08774">
<div class="article-summary-box-inner">
<span><p>High Dynamic Range (HDR) images are the ones that contain a greater range of
luminosity as compared to the standard images. HDR images have a higher detail
and clarity of structure, objects, and color, which the standard images lack.
HDR images are useful in capturing scenes that pose high brightness, darker
areas, and shadows, etc. An HDR image comprises multiple narrow-range-exposure
images combined into one high-quality image. As these HDR images cannot be
displayed on standard display devices, the real challenge comes while
converting these HDR images to Low dynamic range (LDR) images. The conversion
of HDR image to LDR image is performed using Tone-mapped operators (TMOs). This
conversion results in the loss of much valuable information in structure,
color, naturalness, and exposures. The loss of information in the LDR image may
not directly be visible to the human eye. To calculate how good an LDR image is
after conversion, various metrics have been proposed previously. Some are not
noise resilient, some work on separate color channels (Red, Green, and Blue one
by one), and some lack capacity to identify the structure. To deal with this
problem, we propose a metric in this paper called the Tone Mapping Quality
Index (TMQI-3), which evaluates the quality of the LDR image based on its
objective score. TMQI-3 is noise resilient, takes account of structure and
naturalness, and works on all three color channels combined into one luminosity
component. This eliminates the need to use multiple metrics at the same time.
We compute results for several HDR and LDR images from the literature and show
that our quality index metric performs better than the baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Small Lesion Segmentation in Brain MRIs with Subpixel Embedding. (arXiv:2109.08791v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08791">
<div class="article-summary-box-inner">
<span><p>We present a method to segment MRI scans of the human brain into ischemic
stroke lesion and normal tissues. We propose a neural network architecture in
the form of a standard encoder-decoder where predictions are guided by a
spatial expansion embedding network. Our embedding network learns features that
can resolve detailed structures in the brain without the need for
high-resolution training images, which are often unavailable and expensive to
acquire. Alternatively, the encoder-decoder learns global structures by means
of striding and max pooling. Our embedding network complements the
encoder-decoder architecture by guiding the decoder with fine-grained details
lost to spatial downsampling during the encoder stage. Unlike previous works,
our decoder outputs at 2 times the input resolution, where a single pixel in
the input resolution is predicted by four neighboring subpixels in our output.
To obtain the output at the original scale, we propose a learnable downsampler
(as opposed to hand-crafted ones e.g. bilinear) that combines subpixel
predictions. Our approach improves the baseline architecture by approximately
11.7% and achieves the state of the art on the ATLAS public benchmark dataset
with a smaller memory footprint and faster runtime than the best competing
method. Our source code has been made available at:
https://github.com/alexklwong/subpixel-embedding-segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Report on China-Spain Joint Clinical Testing for Rapid COVID-19 Risk Screening by Eye-region Manifestations. (arXiv:2109.08807v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08807">
<div class="article-summary-box-inner">
<span><p>Background: The worldwide surge in coronavirus cases has led to the COVID-19
testing demand surge. Rapid, accurate, and cost-effective COVID-19 screening
tests working at a population level are in imperative demand globally.
</p>
<p>Methods: Based on the eye symptoms of COVID-19, we developed and tested a
COVID-19 rapid prescreening model using the eye-region images captured in China
and Spain with cellphone cameras. The convolutional neural networks
(CNNs)-based model was trained on these eye images to complete binary
classification task of identifying the COVID-19 cases. The performance was
measured using area under receiver-operating-characteristic curve (AUC),
sensitivity, specificity, accuracy, and F1. The application programming
interface was open access.
</p>
<p>Findings: The multicenter study included 2436 pictures corresponding to 657
subjects (155 COVID-19 infection, 23.6%) in development dataset (train and
validation) and 2138 pictures corresponding to 478 subjects (64 COVID-19
infections, 13.4%) in test dataset. The image-level performance of COVID-19
prescreening model in the China-Spain multicenter study achieved an AUC of
0.913 (95% CI, 0.898-0.927), with a sensitivity of 0.695 (95% CI, 0.643-0.748),
a specificity of 0.904 (95% CI, 0.891 -0.919), an accuracy of
0.875(0.861-0.889), and a F1 of 0.611(0.568-0.655).
</p>
<p>Interpretation: The CNN-based model for COVID-19 rapid prescreening has
reliable specificity and sensitivity. This system provides a low-cost, fully
self-performed, non-invasive, real-time feedback solution for continuous
surveillance and large-scale rapid prescreening for COVID-19.
</p>
<p>Funding: This project is supported by Aimomics (Shanghai) Intelligent
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HYouTube: Video Harmonization Dataset. (arXiv:2109.08809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08809">
<div class="article-summary-box-inner">
<span><p>Video composition aims to generate a composite video by combining the
foreground of one video with the background of another video, but the inserted
foreground may be incompatible with the background in terms of color and
illumination. Video harmonization aims to adjust the foreground of a composite
video to make it compatible with the background. So far, video harmonization
has only received limited attention and there is no public dataset for video
harmonization. In this work, we construct a new video harmonization dataset
HYouTube by adjusting the foreground of real videos to create synthetic
composite videos. Considering the domain gap between real composite videos and
synthetic composite videos, we additionally create 100 real composite videos
via copy-and-paste. Datasets are available at
https://github.com/bcmi/Video-Harmonization-Dataset-HYouTube.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Homogeneous and Heterogeneous Relational Graph for Visible-infrared Person Re-identification. (arXiv:2109.08811v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08811">
<div class="article-summary-box-inner">
<span><p>Visible-infrared person re-identification (VI Re-ID) aims to match person
images between the visible and infrared modalities. Existing VI Re-ID methods
mainly focus on extracting homogeneous structural relationships from a single
image, while ignoring the heterogeneous correlation between cross-modality
images. The homogenous and heterogeneous structured relationships are crucial
to learning effective identity representation and cross-modality matching. In
this paper, we separately model the homogenous structural relationship by a
modality-specific graph within individual modality and then mine the
heterogeneous structural correlation in these two modality-specific graphs.
First, the homogeneous structured graph (HOSG) mines one-vs.-rest relation
between an arbitrary node (local feature) and all the rest nodes within a
visible or infrared image to learn effective identity representation. Second,
to find cross-modality identity-consistent correspondence, the heterogeneous
graph alignment module (HGAM) further measures the relational edge strength by
route search between two-modality local node features. Third, we propose the
cross-modality cross-correlation (CMCC) loss to extract the modality invariance
in heterogeneous global graph representation. CMCC computes the mutual
information between modalities and expels semantic redundancy. Extensive
experiments on SYSU-MM01 and RegDB datasets demonstrate that our method
outperforms state-of-the-arts with a gain of 13.73\% and 9.45\% Rank1/mAP. The
code is available at
https://github.com/fegnyujian/Homogeneous-and-Heterogeneous-Relational-Graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Regrasp by Learning to Place. (arXiv:2109.08817v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08817">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore whether a robot can learn to regrasp a diverse set
of objects to achieve various desired grasp poses. Regrasping is needed
whenever a robot's current grasp pose fails to perform desired manipulation
tasks. Endowing robots with such an ability has applications in many domains
such as manufacturing or domestic services. Yet, it is a challenging task due
to the large diversity of geometry in everyday objects and the high
dimensionality of the state and action space. In this paper, we propose a
system for robots to take partial point clouds of an object and the supporting
environment as inputs and output a sequence of pick-and-place operations to
transform an initial object grasp pose to the desired object grasp poses. The
key technique includes a neural stable placement predictor and a regrasp graph
based solution through leveraging and changing the surrounding environment. We
introduce a new and challenging synthetic dataset for learning and evaluating
the proposed approach. In this dataset, we show that our system is able to
achieve 73.3% success rate of regrasping diverse objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Adaptive Partial Domain Adaptation. (arXiv:2109.08829v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08829">
<div class="article-summary-box-inner">
<span><p>Partial Domain adaptation (PDA) aims to solve a more practical cross-domain
learning problem that assumes target label space is a subset of source label
space. However, the mismatched label space causes significant negative
transfer. A traditional solution is using soft weights to increase weights of
source shared domain and reduce those of source outlier domain. But it still
learns features of outliers and leads to negative immigration. The other
mainstream idea is to distinguish source domain into shared and outlier parts
by hard binary weights, while it is unavailable to correct the tangled shared
and outlier classes. In this paper, we propose an end-to-end Self-Adaptive
Partial Domain Adaptation(SAPDA) Network. Class weights evaluation mechanism is
introduced to dynamically self-rectify the weights of shared, outlier and
confused classes, thus the higher confidence samples have the more sufficient
weights. Meanwhile it can eliminate the negative transfer caused by the
mismatching of label space greatly. Moreover, our strategy can efficiently
measure the transferability of samples in a broader sense, so that our method
can achieve competitive results on unsupervised DA task likewise. A large
number of experiments on multiple benchmarks have demonstrated the
effectiveness of our SAPDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechNAS: Towards Better Trade-off between Latency and Accuracy for Large-Scale Speaker Verification. (arXiv:2109.08839v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08839">
<div class="article-summary-box-inner">
<span><p>Recently, x-vector has been a successful and popular approach for speaker
verification, which employs a time delay neural network (TDNN) and statistics
pooling to extract speaker characterizing embedding from variable-length
utterances. Improvement upon the x-vector has been an active research area, and
enormous neural networks have been elaborately designed based on the x-vector,
eg, extended TDNN (E-TDNN), factorized TDNN (F-TDNN), and densely connected
TDNN (D-TDNN). In this work, we try to identify the optimal architectures from
a TDNN based search space employing neural architecture search (NAS), named
SpeechNAS. Leveraging the recent advances in the speaker recognition, such as
high-order statistics pooling, multi-branch mechanism, D-TDNN and angular
additive margin softmax (AAM) loss with a minimum hyper-spherical energy (MHE),
SpeechNAS automatically discovers five network architectures, from SpeechNAS-1
to SpeechNAS-5, of various numbers of parameters and GFLOPs on the large-scale
text-independent speaker recognition dataset VoxCeleb1. Our derived best neural
network achieves an equal error rate (EER) of 1.02% on the standard test set of
VoxCeleb1, which surpasses previous TDNN based state-of-the-art approaches by a
large margin. Code and trained weights are in
https://github.com/wentaozhu/speechnas.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory Regulation and Alignment toward Generalizer RGB-Infrared Person. (arXiv:2109.08843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08843">
<div class="article-summary-box-inner">
<span><p>The domain shift, coming from unneglectable modality gap and non-overlapped
identity classes between training and test sets, is a major issue of
RGB-Infrared person re-identification. A key to tackle the inherent issue --
domain shift -- is to enforce the data distributions of the two domains to be
similar. However, RGB-IR ReID always demands discriminative features, leading
to over-rely feature sensitivity of seen classes, \textit{e.g.}, via
attention-based feature alignment or metric learning. Therefore, predicting the
unseen query category from predefined training classes may not be accurate and
leads to a sub-optimal adversarial gradient. In this paper, we uncover it in a
more explainable way and propose a novel multi-granularity memory regulation
and alignment module (MG-MRA) to solve this issue. By explicitly incorporating
a latent variable attribute, from fine-grained to coarse semantic granularity,
into intermediate features, our method could alleviate the over-confidence of
the model about discriminative features of seen classes. Moreover, instead of
matching discriminative features by traversing nearest neighbor, sparse
attributes, \textit{i.e.}, global structural pattern, are recollected with
respect to features and assigned to measure pair-wise image similarity in
hashing. Extensive experiments on RegDB \cite{RegDB} and SYSU-MM01 \cite{SYSU}
show the superiority of the proposed method that outperforms existing
state-of-the-art methods. Our code is available in
https://github.com/Chenfeng1271/MGMRA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards High-Quality Temporal Action Detection with Sparse Proposals. (arXiv:2109.08847v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08847">
<div class="article-summary-box-inner">
<span><p>Temporal Action Detection (TAD) is an essential and challenging topic in
video understanding, aiming to localize the temporal segments containing human
action instances and predict the action categories. The previous works greatly
rely upon dense candidates either by designing varying anchors or enumerating
all the combinations of boundaries on video sequences; therefore, they are
related to complicated pipelines and sensitive hand-crafted designs. Recently,
with the resurgence of Transformer, query-based methods have tended to become
the rising solutions for their simplicity and flexibility. However, there still
exists a performance gap between query-based methods and well-established
methods. In this paper, we identify the main challenge lies in the large
variants of action duration and the ambiguous boundaries for short action
instances; nevertheless, quadratic-computational global attention prevents
query-based methods to build multi-scale feature maps. Towards high-quality
temporal action detection, we introduce Sparse Proposals to interact with the
hierarchical features. In our method, named SP-TAD, each proposal attends to a
local segment feature in the temporal feature pyramid. The local interaction
enables utilization of high-resolution features to preserve action instances
details. Extensive experiments demonstrate the effectiveness of our method,
especially under high tIoU thresholds. E.g., we achieve the state-of-the-art
performance on THUMOS14 (45.7% on mAP@0.6, 33.4% on mAP@0.7 and 53.5% on
mAP@Avg) and competitive results on ActivityNet-1.3 (32.99% on mAP@Avg). Code
will be made available at https://github.com/wjn922/SP-TAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Composition and Attention for Unseen-Domain Generalizable Medical Image Segmentation. (arXiv:2109.08852v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08852">
<div class="article-summary-box-inner">
<span><p>Domain generalizable model is attracting increasing attention in medical
image analysis since data is commonly acquired from different institutes with
various imaging protocols and scanners. To tackle this challenging domain
generalization problem, we propose a Domain Composition and Attention-based
network (DCA-Net) to improve the ability of domain representation and
generalization. First, we present a domain composition method that represents
one certain domain by a linear combination of a set of basis representations
(i.e., a representation bank). Second, a novel plug-and-play parallel domain
preceptor is proposed to learn these basis representations and we introduce a
divergence constraint function to encourage the basis representations to be as
divergent as possible. Then, a domain attention module is proposed to learn the
linear combination coefficients of the basis representations. The result of
linear combination is used to calibrate the feature maps of an input image,
which enables the model to generalize to different and even unseen domains. We
validate our method on public prostate MRI dataset acquired from six different
institutions with apparent domain shift. Experimental results show that our
proposed model can generalize well on different and even unseen domains and it
outperforms state-of-the-art methods on the multi-domain prostate segmentation
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A survey on deep learning approaches for breast cancer diagnosis. (arXiv:2109.08853v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08853">
<div class="article-summary-box-inner">
<span><p>Deep learning has introduced several learning-based methods to recognize
breast tumours and presents high applicability in breast cancer diagnostics. It
has presented itself as a practical installment in Computer-Aided Diagnostic
(CAD) systems to further assist radiologists in diagnostics for different
modalities. A deep learning network trained on images provided by hospitals or
public databases can perform classification, detection, and segmentation of
lesion types. Significant progress has been made in recognizing tumours on 2D
images but recognizing 3D images remains a frontier so far. The interconnection
of deep learning networks between different fields of study help propels
discoveries for more efficient, accurate, and robust networks. In this review
paper, the following topics will be explored: (i) theory and application of
deep learning, (ii) progress of 2D, 2.5D, and 3D CNN approaches in breast
tumour recognition from a performance metric perspective, and (iii) challenges
faced in CNN approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modern Evolution Strategies for Creativity: Fitting Concrete Images and Abstract Concepts. (arXiv:2109.08857v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08857">
<div class="article-summary-box-inner">
<span><p>Evolutionary algorithms have been used in the digital art scene since the
1970s. A popular application of genetic algorithms is to optimize the
procedural placement of vector graphic primitives to resemble a given painting.
In recent years, deep learning-based approaches have also been proposed to
generate procedural drawings, which can be optimized using gradient descent. In
this work, we revisit the use of evolutionary algorithms for computational
creativity. We find that modern evolution strategies (ES) algorithms, when
tasked with the placement of shapes, offer large improvements in both quality
and efficiency compared to traditional genetic algorithms, and even comparable
to gradient-based methods. We demonstrate that ES is also well suited at
optimizing the placement of shapes to fit the CLIP model, and can produce
diverse, distinct geometric abstractions that are aligned with human
interpretation of language. Videos and demo: https://es-clip.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V-SlowFast Network for Efficient Visual Sound Separation. (arXiv:2109.08867v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08867">
<div class="article-summary-box-inner">
<span><p>The objective of this paper is to perform visual sound separation: i) we
study visual sound separation on spectrograms of different temporal
resolutions; ii) we propose a new light yet efficient three-stream framework
V-SlowFast that operates on Visual frame, Slow spectrogram, and Fast
spectrogram. The Slow spectrogram captures the coarse temporal resolution while
the Fast spectrogram contains the fine-grained temporal resolution; iii) we
introduce two contrastive objectives to encourage the network to learn
discriminative visual features for separating sounds; iv) we propose an
audio-visual global attention module for audio and visual feature fusion; v)
the introduced V-SlowFast model outperforms previous state-of-the-art in
single-frame based visual sound separation on small- and large-scale datasets:
MUSIC-21, AVE, and VGG-Sound. We also propose a small V-SlowFast architecture
variant, which achieves 74.2% reduction in the number of model parameters and
81.4% reduction in GMACs compared to the previous multi-stage models. Project
page:
\href{https://ly-zhu.github.io/V-SlowFast}{https://ly-zhu.github.io/V-SlowFast}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clean-label Backdoor Attack against Deep Hashing based Retrieval. (arXiv:2109.08868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08868">
<div class="article-summary-box-inner">
<span><p>Deep hashing has become a popular method in large-scale image retrieval due
to its computational and storage efficiency. However, recent works raise the
security concerns of deep hashing. Although existing works focus on the
vulnerability of deep hashing in terms of adversarial perturbations, we
identify a more pressing threat, backdoor attack, when the attacker has access
to the training data. A backdoored deep hashing model behaves normally on
original query images, while returning the images with the target label when
the trigger presents, which makes the attack hard to be detected. In this
paper, we uncover this security concern by utilizing clean-label data
poisoning. To the best of our knowledge, this is the first attempt at the
backdoor attack against deep hashing models. To craft the poisoned images, we
first generate the targeted adversarial patch as the backdoor trigger.
Furthermore, we propose the confusing perturbations to disturb the hashing code
learning, such that the hashing model can learn more about the trigger. The
confusing perturbations are imperceptible and generated by dispersing the
images with the target label in the Hamming space. We have conducted extensive
experiments to verify the efficacy of our backdoor attack under various
settings. For instance, it can achieve 63% targeted mean average precision on
ImageNet under 48 bits code length with only 40 poisoned images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastHyMix: Fast and Parameter-free Hyperspectral Image Mixed Noise Removal. (arXiv:2109.08879v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08879">
<div class="article-summary-box-inner">
<span><p>Hyperspectral imaging with high spectral resolution plays an important role
in finding objects, identifying materials, or detecting processes. The decrease
of the widths of spectral bands leads to a decrease in the signal-to-noise
ratio (SNR) of measurements. The decreased SNR reduces the reliability of
measured features or information extracted from HSIs. Furthermore, the image
degradations linked with various mechanisms also result in different types of
noise, such as Gaussian noise, impulse noise, deadlines, and stripes. This
paper introduces a fast and parameter-free hyperspectral image mixed noise
removal method (termed FastHyMix), which characterizes the complex distribution
of mixed noise by using a Gaussian mixture model and exploits two main
characteristics of hyperspectral data, namely low-rankness in the spectral
domain and high correlation in the spatial domain. The Gaussian mixture model
enables us to make a good estimation of Gaussian noise intensity and the
location of sparse noise. The proposed method takes advantage of the
low-rankness using subspace representation and the spatial correlation of HSIs
by adding a powerful deep image prior, which is extracted from a neural
denoising network. An exhaustive array of experiments and comparisons with
state-of-the-art denoisers were carried out. The experimental results show
significant improvement in both synthetic and real datasets. A MATLAB demo of
this work will be available at https://github.com/LinaZhuang for the sake of
reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computational Imaging and Artificial Intelligence: The Next Revolution of Mobile Vision. (arXiv:2109.08880v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08880">
<div class="article-summary-box-inner">
<span><p>Signal capture stands in the forefront to perceive and understand the
environment and thus imaging plays the pivotal role in mobile vision. Recent
explosive progresses in Artificial Intelligence (AI) have shown great potential
to develop advanced mobile platforms with new imaging devices. Traditional
imaging systems based on the "capturing images first and processing afterwards"
mechanism cannot meet this unprecedented demand. Differently, Computational
Imaging (CI) systems are designed to capture high-dimensional data in an
encoded manner to provide more information for mobile vision systems.Thanks to
AI, CI can now be used in real systems by integrating deep learning algorithms
into the mobile vision platform to achieve the closed loop of intelligent
acquisition, processing and decision making, thus leading to the next
revolution of mobile vision.Starting from the history of mobile vision using
digital cameras, this work first introduces the advances of CI in diverse
applications and then conducts a comprehensive review of current research
topics combining CI and AI. Motivated by the fact that most existing studies
only loosely connect CI and AI (usually using AI to improve the performance of
CI and only limited works have deeply connected them), in this work, we propose
a framework to deeply integrate CI and AI by using the example of self-driving
vehicles with high-speed communication, edge computing and traffic planning.
Finally, we outlook the future of CI plus AI by investigating new materials,
brain science and new computing techniques to shed light on new directions of
mobile vision systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S$^3$VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation. (arXiv:2109.08901v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08901">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (DA) methods have focused on achieving maximal
performance through aligning features from source and target domains without
using labeled data in the target domain. Whereas, in the real-world scenario's
it might be feasible to get labels for a small proportion of target data. In
these scenarios, it is important to select maximally-informative samples to
label and find an effective way to combine them with the existing knowledge
from source data. Towards achieving this, we propose S$^3$VAADA which i)
introduces a novel submodular criterion to select a maximally informative
subset to label and ii) enhances a cluster-based DA procedure through novel
improvements to effectively utilize all the available data for improving
generalization on target. Our approach consistently outperforms the competing
state-of-the-art approaches on datasets with varying degrees of domain shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the rogue wave pattern triggered from Gaussian perturbations by deep learning. (arXiv:2109.08909v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08909">
<div class="article-summary-box-inner">
<span><p>Weak Gaussian perturbations on a plane wave background could trigger lots of
rogue waves, due to modulational instability. Numerical simulations showed that
these rogue waves seemed to have similar unit structure. However, to the best
of our knowledge, there is no relative result to prove that these rogue waves
have the similar patterns for different perturbations, partly due to that it is
hard to measure the rogue wave pattern automatically. In this work, we address
these problems from the perspective of computer vision via using deep neural
networks. We propose a Rogue Wave Detection Network (RWD-Net) model to
automatically and accurately detect RWs on the images, which directly indicates
they have the similar computer vision patterns. For this purpose, we herein
meanwhile have designed the related dataset, termed as Rogue Wave Dataset-$10$K
(RWD-$10$K), which has $10,191$ RW images with bounding box annotations for
each RW unit. In our detection experiments, we get $99.29\%$ average precision
on the test splits of the RWD-$10$K dataset. Finally, we derive our novel
metric, the density of RW units (DRW), to characterize the evolution of
Gaussian perturbations and obtain the statistical results on them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Semantic Segmentation via Low-level Edge Information Transfer. (arXiv:2109.08912v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08912">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation for semantic segmentation aims to make models
trained on synthetic data (source domain) adapt to real images (target domain).
Previous feature-level adversarial learning methods only consider adapting
models on the high-level semantic features. However, the large domain gap
between source and target domains in the high-level semantic features makes
accurate adaptation difficult. In this paper, we present the first attempt at
explicitly using low-level edge information, which has a small inter-domain
gap, to guide the transfer of semantic information. To this end, a
semantic-edge domain adaptation architecture is proposed, which uses an
independent edge stream to process edge information, thereby generating
high-quality semantic boundaries over the target domain. Then, an edge
consistency loss is presented to align target semantic predictions with
produced semantic boundaries. Moreover, we further propose two entropy
reweighting methods for semantic adversarial learning and self-supervised
learning, respectively, which can further enhance the adaptation performance of
our architecture. Comprehensive experiments on two UDA benchmark datasets
demonstrate the superiority of our architecture compared with state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edge Prior Augmented Networks for Motion Deblurring on Naturally Blurry Images. (arXiv:2109.08915v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08915">
<div class="article-summary-box-inner">
<span><p>Motion deblurring has witnessed rapid development in recent years, and most
of the recent methods address it by using deep learning techniques, with the
help of different kinds of prior knowledge. Concerning that deblurring is
essentially expected to improve the image sharpness, edge information can serve
as an important prior. However, the edge has not yet been seriously taken into
consideration in previous methods when designing deep models. To this end, we
present a novel framework that incorporates edge prior knowledge into deep
models, termed Edge Prior Augmented Networks (EPAN). EPAN has a content-based
main branch and an edge-based auxiliary branch, which are constructed as a
Content Deblurring Net (CDN) and an Edge Enhancement Net (EEN), respectively.
EEN is designed to augment CDN in the deblurring process via an attentive
fusion mechanism, where edge features are mapped as spatial masks to guide
content features in a feature-based hierarchical manner. An edge-guided loss
function is proposed to further regulate the optimization of EPAN by enforcing
the focus on edge areas. Besides, we design a dual-camera-based image capturing
setting to build a new dataset, Real Object Motion Blur (ROMB), with paired
sharp and naturally blurry images of fast-moving cars, so as to better train
motion deblurring models and benchmark the capability of motion deblurring
algorithms in practice. Extensive experiments on the proposed ROMB and other
existing datasets demonstrate that EPAN outperforms state-of-the-art approaches
qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Underwater Image Enhancement Using Convolutional Neural Network. (arXiv:2109.08916v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08916">
<div class="article-summary-box-inner">
<span><p>This work proposes a method for underwater image enhancement using the
principle of histogram equalization. Since underwater images have a global
strong dominant colour, their colourfulness and contrast are often degraded.
Before applying the histogram equalisation technique on the image, the image is
converted from coloured image to a gray scale image for further operations.
Histogram equalization is a technique for adjusting image intensities to
enhance contrast. The colours of the image are retained using a convolutional
neural network model which is trained by the datasets of underwater images to
give better results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Studious Approach to Semi-Supervised Learning. (arXiv:2109.08924v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08924">
<div class="article-summary-box-inner">
<span><p>The problem of learning from few labeled examples while using large amounts
of unlabeled data has been approached by various semi-supervised methods.
Although these methods can achieve superior performance, the models are often
not deployable due to the large number of parameters. This paper is an ablation
study of distillation in a semi-supervised setting, which not just reduces the
number of parameters of the model but can achieve this while improving the
performance over the baseline supervised model and making it better at
generalizing. After the supervised pretraining, the network is used as a
teacher model, and a student network is trained over the soft labels that the
teacher model generates over the entire unlabeled data. We find that the fewer
the labels, the more this approach benefits from a smaller student network.
This brings forward the potential of distillation as an effective solution to
enhance performance in semi-supervised computer vision tasks while maintaining
deployability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Hybrid Transformer: Learning Global-local Context for Urban Sence Segmentation. (arXiv:2109.08937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08937">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of fine-resolution urban scene images plays a vital
role in extensive practical applications, such as land cover mapping, urban
change detection, environmental protection and economic assessment. Driven by
rapid developments in deep learning technologies, convolutional neural networks
(CNNs) have dominated the semantic segmentation task for many years.
Convolutional neural networks adopt hierarchical feature representation and
have strong local context extraction. However, the local property of the
convolution layer limits the network from capturing global information that is
crucial for improving fine-resolution image segmentation. Recently, Transformer
comprise a hot topic in the computer vision domain. Vision Transformer
demonstrates the great capability of global information modelling, boosting
many vision tasks, such as image classification, object detection and
especially semantic segmentation. In this paper, we propose an efficient hybrid
Transformer (EHT) for semantic segmentation of urban scene images. EHT takes
advantage of CNNs and Transformer, learning global-local context to strengthen
the feature representation. Extensive experiments demonstrate that EHT has
higher efficiency with competitive accuracy compared with state-of-the-art
benchmark methods. Specifically, the proposed EHT achieves a 67.0% mIoU on the
UAVid test set and outperforms other lightweight models significantly. The code
will be available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Violence Detection in Videos. (arXiv:2109.08941v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08941">
<div class="article-summary-box-inner">
<span><p>In the recent years, there has been a tremendous increase in the amount of
video content uploaded to social networking and video sharing websites like
Facebook and Youtube. As of result of this, the risk of children getting
exposed to adult and violent content on the web also increased. To address this
issue, an approach to automatically detect violent content in videos is
proposed in this work. Here, a novel attempt is made also to detect the
category of violence present in a video. A system which can automatically
detect violence from both Hollywood movies and videos from the web is extremely
useful not only in parental control but also for applications related to movie
ratings, video surveillance, genre classification and so on.
</p>
<p>Here, both audio and visual features are used to detect violence. MFCC
features are used as audio cues. Blood, Motion, and SentiBank features are used
as visual cues. Binary SVM classifiers are trained on each of these features to
detect violence. Late fusion using a weighted sum of classification scores is
performed to get final classification scores for each of the violence class
target by the system. To determine optimal weights for each of the violence
classes an approach based on grid search is employed. Publicly available
datasets, mainly Violent Scene Detection (VSD), are used for classifier
training, weight calculation, and testing. The performance of the system is
evaluated on two classification tasks, Multi-Class classification, and Binary
Classification. The results obtained for Binary Classification are better than
the baseline results from MediaEval-2014.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iWave3D: End-to-end Brain Image Compression with Trainable 3-D Wavelet Transform. (arXiv:2109.08942v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08942">
<div class="article-summary-box-inner">
<span><p>With the rapid development of whole brain imaging technology, a large number
of brain images have been produced, which puts forward a great demand for
efficient brain image compression methods. At present, the most commonly used
compression methods are all based on 3-D wavelet transform, such as JP3D.
However, traditional 3-D wavelet transforms are designed manually with certain
assumptions on the signal, but brain images are not as ideal as assumed. What's
more, they are not directly optimized for compression task. In order to solve
these problems, we propose a trainable 3-D wavelet transform based on the
lifting scheme, in which the predict and update steps are replaced by 3-D
convolutional neural networks. Then the proposed transform is embedded into an
end-to-end compression scheme called iWave3D, which is trained with a large
amount of brain images to directly minimize the rate-distortion loss.
Experimental results demonstrate that our method outperforms JP3D significantly
by 2.012 dB in terms of average BD-PSNR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Manifold-preserved GANs. (arXiv:2109.08955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08955">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) have been widely adopted in various
fields. However, existing GANs generally are not able to preserve the manifold
of data space, mainly due to the simple representation of discriminator for the
real/generated data. To address such open challenges, this paper proposes
Manifold-preserved GANs (MaF-GANs), which generalize Wasserstein GANs into
high-dimensional form. Specifically, to improve the representation of data, the
discriminator in MaF-GANs is designed to map data into a high-dimensional
manifold. Furthermore, to stabilize the training of MaF-GANs, an operation with
precise and universal solution for any K-Lipschitz continuity, called
Topological Consistency is proposed. The effectiveness of the proposed method
is justified by both theoretical analysis and empirical results. When adopting
DCGAN as the backbone on CelebA (256*256), the proposed method achieved 12.43
FID, which outperforms the state-of-the-art model like Realness GAN (23.51 FID)
by a large margin. Code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDTP: Semantic-aware Decoupled Transformer Pyramid for Dense Image Prediction. (arXiv:2109.08963v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08963">
<div class="article-summary-box-inner">
<span><p>Although transformer has achieved great progress on computer vision tasks,
the scale variation in dense image prediction is still the key challenge. Few
effective multi-scale techniques are applied in transformer and there are two
main limitations in the current methods. On one hand, self-attention module in
vanilla transformer fails to sufficiently exploit the diversity of semantic
information because of its rigid mechanism. On the other hand, it is hard to
build attention and interaction among different levels due to the heavy
computational burden. To alleviate this problem, we first revisit multi-scale
problem in dense prediction, verifying the significance of diverse semantic
representation and multi-scale interaction, and exploring the adaptation of
transformer to pyramidal structure. Inspired by these findings, we propose a
novel Semantic-aware Decoupled Transformer Pyramid (SDTP) for dense image
prediction, consisting of Intra-level Semantic Promotion (ISP), Cross-level
Decoupled Interaction (CDI) and Attention Refinement Function (ARF). ISP
explores the semantic diversity in different receptive space. CDI builds the
global attention and interaction among different levels in decoupled space
which also solves the problem of heavy computation. Besides, ARF is further
added to refine the attention in transformer. Experimental results demonstrate
the validity and generality of the proposed method, which outperforms the
state-of-the-art by a significant margin in dense image prediction tasks.
Furthermore, the proposed components are all plug-and-play, which can be
embedded in other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Atrial Fibrillation: A Medical and Technological Review. (arXiv:2109.08974v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08974">
<div class="article-summary-box-inner">
<span><p>Atrial Fibrillation (AF) is the most common type of arrhythmia (Greek a-,
loss + rhythmos, rhythm = loss of rhythm) leading to hospitalization in the
United States. Though sometimes AF is asymptomatic, it increases the risk of
stroke and heart failure in patients, in addition to lowering the
health-related quality of life (HRQOL). AF-related care costs the healthcare
system between $6.0 to $26 billion each year. Early detection of AF and
clinical attention can help improve symptoms and HRQOL of the patient, as well
as bring down the cost of care. However, the prevalent paradigm of AF detection
depends on electrocardiogram (ECG) recorded at a single point in time and does
not shed light on the relation of the symptoms with heart rhythm or AF. In the
recent decade, due to the democratization of health monitors and the advent of
high-performing computers, Machine Learning algorithms have been proven
effective in identifying AF, from the ECG of patients. This paper provides an
overview of the symptoms of AF, its diagnosis, and future prospects for
research in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AirLoop: Lifelong Loop Closure Detection. (arXiv:2109.08975v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08975">
<div class="article-summary-box-inner">
<span><p>Loop closure detection is an important building block that ensures the
accuracy and robustness of simultaneous localization and mapping (SLAM)
systems. Due to their generalization ability, CNN-based approaches have
received increasing attention. Although they normally benefit from training on
datasets that are diverse and reflective of the environments, new environments
often emerge after the model is deployed. It is therefore desirable to
incorporate the data newly collected during operation for incremental learning.
Nevertheless, simply finetuning the model on new data is infeasible since it
may cause the model's performance on previously learned data to degrade over
time, which is also known as the problem of catastrophic forgetting. In this
paper, we present AirLoop, a method that leverages techniques from lifelong
learning to minimize forgetting when training loop closure detection models
incrementally. We experimentally demonstrate the effectiveness of AirLoop on
TartanAir, Nordland, and RobotCar datasets. To the best of our knowledge,
AirLoop is one of the first works to achieve lifelong learning of deep loop
closure detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Recognition based on Retinal Bifurcations and Modified Correlation Function. (arXiv:2109.08977v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08977">
<div class="article-summary-box-inner">
<span><p>Nowadays high security is an important issue for most of the secure places
and recent advances increase the needs of high-security systems. Therefore,
needs to high security for controlling and permitting the allowable people to
enter the high secure places, increases and extends the use of conventional
recognition methods. Therefore, a novel identification method using retinal
images is proposed in this paper. For this purpose, new mathematical functions
are applied on corners and bifurcations. To evaluate the proposed method we use
40 retinal images from the DRIVE database, 20 normal retinal image from STARE
database and 140 normal retinal images from local collected database and the
accuracy rate is 99.34 percent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Random Multi-Channel Image Synthesis for Multiplexed Immunofluorescence Imaging. (arXiv:2109.09004v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09004">
<div class="article-summary-box-inner">
<span><p>Multiplex immunofluorescence (MxIF) is an emerging imaging technique that
produces the high sensitivity and specificity of single-cell mapping. With a
tenet of 'seeing is believing', MxIF enables iterative staining and imaging
extensive antibodies, which provides comprehensive biomarkers to segment and
group different cells on a single tissue section. However, considerable
depletion of the scarce tissue is inevitable from extensive rounds of staining
and bleaching ('missing tissue'). Moreover, the immunofluorescence (IF) imaging
can globally fail for particular rounds ('missing stain''). In this work, we
focus on the 'missing stain' issue. It would be appealing to develop digital
image synthesis approaches to restore missing stain images without losing more
tissue physically. Herein, we aim to develop image synthesis approaches for
eleven MxIF structural molecular markers (i.e., epithelial and stromal) on real
samples. We propose a novel multi-channel high-resolution image synthesis
approach, called pixN2N-HD, to tackle possible missing stain scenarios via a
high-resolution generative adversarial network (GAN). Our contribution is
three-fold: (1) a single deep network framework is proposed to tackle missing
stain in MxIF; (2) the proposed 'N-to-N' strategy reduces theoretical four
years of computational time to 20 hours when covering all possible missing
stains scenarios, with up to five missing stains (e.g., '(N-1)-to-1',
'(N-2)-to-2'); and (3) this work is the first comprehensive experimental study
of investigating cross-stain synthesis in MxIF. Our results elucidate a
promising direction of advancing MxIF imaging with deep image synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Unreasonable Effectiveness of the Final Batch Normalization Layer. (arXiv:2109.09016v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09016">
<div class="article-summary-box-inner">
<span><p>Early-stage disease indications are rarely recorded in real-world domains,
such as Agriculture and Healthcare, and yet, their accurate identification is
critical in that point of time. In this type of highly imbalanced
classification problems, which encompass complex features, deep learning (DL)
is much needed because of its strong detection capabilities. At the same time,
DL is observed in practice to favor majority over minority classes and
consequently suffer from inaccurate detection of the targeted early-stage
indications. In this work, we extend the study done by Kocaman et al., 2020,
showing that the final BN layer, when placed before the softmax output layer,
has a considerable impact in highly imbalanced image classification problems as
well as undermines the role of the softmax outputs as an uncertainty measure.
This current study addresses additional hypotheses and reports on the following
findings: (i) the performance gain after adding the final BN layer in highly
imbalanced settings could still be achieved after removing this additional BN
layer in inference; (ii) there is a certain threshold for the imbalance ratio
upon which the progress gained by the final BN layer reaches its peak; (iii)
the batch size also plays a role and affects the outcome of the final BN
application; (iv) the impact of the BN application is also reproducible on
other datasets and when utilizing much simpler neural architectures; (v) the
reported BN effect occurs only per a single majority class and multiple
minority classes i.e., no improvements are evident when there are two majority
classes; and finally, (vi) utilizing this BN layer with sigmoid activation has
almost no impact when dealing with a strongly imbalanced image classification
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Distribution Alignment via Adversarial Learning for Domain Adaptive Object Detection. (arXiv:2109.09033v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09033">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptive object detection aims to adapt a well-trained
detector from its original source domain with rich labeled data to a new target
domain with unlabeled data. Recently, mainstream approaches perform this task
through adversarial learning, yet still suffer from two limitations. First,
they mainly align marginal distribution by unsupervised cross-domain feature
matching, and ignore each feature's categorical and positional information that
can be exploited for conditional alignment; Second, they treat all classes as
equally important for transferring cross-domain knowledge and ignore that
different classes usually have different transferability. In this paper, we
propose a joint adaptive detection framework (JADF) to address the above
challenges. First, an end-to-end joint adversarial adaptation framework for
object detection is proposed, which aligns both marginal and conditional
distributions between domains without introducing any extra hyperparameter.
Next, to consider the transferability of each object class, a metric for
class-wise transferability assessment is proposed, which is incorporated into
the JADF objective for domain adaptation. Further, an extended study from
unsupervised domain adaptation (UDA) to unsupervised few-shot domain adaptation
(UFDA) is conducted, where only a few unlabeled training images are available
in unlabeled target domain. Extensive experiments validate that JADF is
effective in both the UDA and UFDA settings, achieving significant performance
gains over existing state-of-the-art cross-domain detection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Tracking by Jointly Exploiting Frame and Event Domain. (arXiv:2109.09052v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09052">
<div class="article-summary-box-inner">
<span><p>Inspired by the complementarity between conventional frame-based and
bio-inspired event-based cameras, we propose a multi-modal based approach to
fuse visual cues from the frame- and event-domain to enhance the single object
tracking performance, especially in degraded conditions (e.g., scenes with high
dynamic range, low light, and fast-motion objects). The proposed approach can
effectively and adaptively combine meaningful information from both domains.
Our approach's effectiveness is enforced by a novel designed cross-domain
attention schemes, which can effectively enhance features based on self- and
cross-domain attention schemes; The adaptiveness is guarded by a specially
designed weighting scheme, which can adaptively balance the contribution of the
two domains. To exploit event-based visual cues in single-object tracking, we
construct a large-scale frame-event-based dataset, which we subsequently employ
to train a novel frame-event fusion based model. Extensive experiments show
that the proposed approach outperforms state-of-the-art frame-based tracking
methods by at least 10.4% and 11.9% in terms of representative success rate and
precision rate, respectively. Besides, the effectiveness of each key component
of our approach is evidenced by our thorough ablation study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ontology-based n-ball Concept Embeddings Informing Few-shot Image Classification. (arXiv:2109.09063v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09063">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework named ViOCE that integrates ontology-based
background knowledge in the form of $n$-ball concept embeddings into a neural
network based vision architecture. The approach consists of two components -
converting symbolic knowledge of an ontology into continuous space by learning
n-ball embeddings that capture properties of subsumption and disjointness, and
guiding the training and inference of a vision model using the learnt
embeddings. We evaluate ViOCE using the task of few-shot image classification,
where it demonstrates superior performance on two standard benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Efficient Unpaired Real-world Super-Resolution using Image Statistics. (arXiv:2109.09071v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09071">
<div class="article-summary-box-inner">
<span><p>Learning super-resolution (SR) network without the paired low resolution (LR)
and high resolution (HR) image is difficult because direct supervision through
the corresponding HR counterpart is unavailable. Recently, many real-world SR
researches take advantage of the unpaired image-to-image translation technique.
That is, they used two or more generative adversarial networks (GANs), each of
which translates images from one domain to another domain, \eg, translates
images from the HR domain to the LR domain. However, it is not easy to stably
learn such a translation with GANs using unpaired data. In this study, we
present a simple and efficient method of training of real-world SR network. To
stably train the network, we use statistics of an image patch, such as means
and variances. Our real-world SR framework consists of two GANs, one for
translating HR images to LR images (degradation task) and the other for
translating LR to HR (SR task). We argue that the unpaired image translation
using GANs can be learned efficiently with our proposed data sampling strategy,
namely, variance matching. We test our method on the NTIRE 2020 real-world SR
dataset. Our method outperforms the current state-of-the-art method in terms of
the SSIM metric as well as produces comparable results on the LPIPS metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Urban-scale Point Clouds Segmentation with BEV Projection. (arXiv:2109.09074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09074">
<div class="article-summary-box-inner">
<span><p>Point clouds analysis has grasped researchers' eyes in recent years, while 3D
semantic segmentation remains a problem. Most deep point clouds models directly
conduct learning on 3D point clouds, which will suffer from the severe sparsity
and extreme data processing load in urban-scale data. To tackle the challenge,
we propose to transfer the 3D point clouds to dense bird's-eye-view projection.
In this case, the segmentation task is simplified because of class unbalance
reduction and the feasibility of leveraging various 2D segmentation methods. We
further design an attention-based fusion network that can conduct multi-modal
learning on the projected images. Finally, the 2D out are remapped to generate
3D semantic segmentation results. To demonstrate the benefits of our method, we
conduct various experiments on the SensatUrban dataset, in which our model
presents competitive evaluation results (61.17% mIoU and 91.37%
OverallAccuracy). We hope our work can inspire further exploration in point
cloud analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DECORAS: detection and characterization of radio-astronomical sources using deep learning. (arXiv:2109.09077v1 [astro-ph.IM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09077">
<div class="article-summary-box-inner">
<span><p>We present DECORAS, a deep learning based approach to detect both point and
extended sources from Very Long Baseline Interferometry (VLBI) observations.
Our approach is based on an encoder-decoder neural network architecture that
uses a low number of convolutional layers to provide a scalable solution for
source detection. In addition, DECORAS performs source characterization in
terms of the position, effective radius and peak brightness of the detected
sources. We have trained and tested the network with images that are based on
realistic Very Long Baseline Array (VLBA) observations at 20 cm. Also, these
images have not gone through any prior de-convolution step and are directly
related to the visibility data via a Fourier transform. We find that the source
catalog generated by DECORAS has a better overall completeness and purity, when
compared to a traditional source detection algorithm. DECORAS is complete at
the 7.5$\sigma$ level, and has an almost factor of two improvement in
reliability at 5.5$\sigma$. We find that DECORAS can recover the position of
the detected sources to within 0.61 $\pm$ 0.69 mas, and the effective radius
and peak surface brightness are recovered to within 20 per cent for 98 and 94
per cent of the sources, respectively. Overall, we find that DECORAS provides a
reliable source detection and characterization solution for future wide-field
VLBI surveys.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards robustness under occlusion for face recognition. (arXiv:2109.09083v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09083">
<div class="article-summary-box-inner">
<span><p>In this paper, we evaluate the effects of occlusions in the performance of a
face recognition pipeline that uses a ResNet backbone. The classifier was
trained on a subset of the CelebA-HQ dataset containing 5,478 images from 307
classes, to achieve top-1 error rate of 17.91%. We designed 8 different
occlusion masks which were applied to the input images. This caused a
significant drop in the classifier performance: its error rate for each mask
became at least two times worse than before. In order to increase robustness
under occlusions, we followed two approaches. The first is image inpainting
using the pre-trained pluralistic image completion network. The second is
Cutmix, a regularization strategy consisting of mixing training images and
their labels using rectangular patches, making the classifier more robust
against input corruptions. Both strategies revealed effective and interesting
results were observed. In particular, the Cutmix approach makes the network
more robust without requiring additional steps at the application time, though
its training time is considerably longer. Our datasets containing the different
occlusion masks as well as their inpainted counterparts are made publicly
available to promote research on the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-resolution Human Pose Estimation. (arXiv:2109.09090v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09090">
<div class="article-summary-box-inner">
<span><p>Human pose estimation has achieved significant progress on images with high
imaging resolution. However, low-resolution imagery data bring nontrivial
challenges which are still under-studied. To fill this gap, we start with
investigating existing methods and reveal that the most dominant heatmap-based
methods would suffer more severe model performance degradation from
low-resolution, and offset learning is an effective strategy. Established on
this observation, in this work we propose a novel Confidence-Aware Learning
(CAL) method which further addresses two fundamental limitations of existing
offset learning methods: inconsistent training and testing, decoupled heatmap
and offset learning. Specifically, CAL selectively weighs the learning of
heatmap and offset with respect to ground-truth and most confident prediction,
whilst capturing the statistical importance of model output in mini-batch
learning manner. Extensive experiments conducted on the COCO benchmark show
that our method outperforms significantly the state-of-the-art methods for
low-resolution human pose estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HPTQ: Hardware-Friendly Post Training Quantization. (arXiv:2109.09113v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09113">
<div class="article-summary-box-inner">
<span><p>Neural network quantization enables the deployment of models on edge devices.
An essential requirement for their hardware efficiency is that the quantizers
are hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To
the best of our knowledge, current post-training quantization methods do not
support all of these constraints simultaneously. In this work, we introduce a
hardware-friendly post training quantization (HPTQ) framework, which addresses
this problem by synergistically combining several known quantization methods.
We perform a large-scale study on four tasks: classification, object detection,
semantic segmentation and pose estimation over a wide variety of network
architectures. Our extensive experiments show that competitive results can be
obtained under hardware-friendly constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ComicGAN: Text-to-Comic Generative Adversarial Network. (arXiv:2109.09120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09120">
<div class="article-summary-box-inner">
<span><p>Drawing and annotating comic illustrations is a complex and difficult
process. No existing machine learning algorithms have been developed to create
comic illustrations based on descriptions of illustrations, or the dialogue in
comics. Moreover, it is not known if a generative adversarial network (GAN) can
generate original comics that correspond to the dialogue and/or descriptions.
GANs are successful in producing photo-realistic images, but this technology
does not necessarily translate to generation of flawless comics. What is more,
comic evaluation is a prominent challenge as common metrics such as Inception
Score will not perform comparably, as they are designed to work on photos. In
this paper: 1. We implement ComicGAN, a novel text-to-comic pipeline based on a
text-to-image GAN that synthesizes comics according to text descriptions. 2. We
describe an in-depth empirical study of the technical difficulties of comic
generation using GAN's. ComicGAN has two novel features: (i) text description
creation from labels via permutation and augmentation, and (ii) custom image
encoding with Convolutional Neural Networks. We extensively evaluate the
proposed ComicGAN in two scenarios, namely image generation from descriptions,
and image generation from dialogue. Our results on 1000 Dilbert comic panels
and 6000 descriptions show synthetic comic panels from text inputs resemble
original Dilbert panels. Novel methods for text description creation and custom
image encoding brought improvements to Frechet Inception Distance, detail, and
overall image quality over baseline algorithms. Generating illustrations from
descriptions provided clear comics including characters and colours that were
specified in the descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Autism Spectrum Disorder Based on Individual-Aware Down-Sampling and Multi-Modal Learning. (arXiv:2109.09129v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09129">
<div class="article-summary-box-inner">
<span><p>Autism Spectrum Disorder(ASD) is a set of neurodevelopmental conditions that
affect patients' social abilities. In recent years, deep learning methods have
been employed to detect ASD through functional MRI (fMRI). However, existing
approaches solely concentrated on the abnormal brain functional connections but
ignored the importance of regional activities. Due to this biased prior
knowledge, previous diagnosis models suffered from inter-site heterogeneity and
inter-individual phenotypical differences. To address this issue, we propose a
novel feature extraction method for fMRI that can learn a personalized
lowe-resolution representation of the entire brain networking regarding both
the functional connections and regional activities. First, we abstract the
brain imaging as a graph structure, where nodes represent brain areas and edges
denote functional connections, and downsample it to a sparse network by
hierarchical graph pooling. Subsequently, by assigning each subject with the
extracted features and building edges through inter-individual non-imaging
characteristics, we build a population graph. The non-identically distributed
node features are further recalibrated to node embeddings learned by graph
convolutional networks. By these means, our framework can extract features
directly and efficiently from the entire fMRI and be aware of implicit
inter-individual differences. We have evaluated our framework on the ABIDE-I
dataset with 10-fold cross-validation. The present model has achieved a mean
classification accuracy of 85.95\% and a mean AUC of 0.92, which is better than
the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RSI-Net: Two-Stream Deep Neural Network Integrating GCN and Atrous CNN for Semantic Segmentation of High-resolution Remote Sensing Images. (arXiv:2109.09148v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09148">
<div class="article-summary-box-inner">
<span><p>For semantic segmentation of remote sensing images (RSI), trade-off between
representation power and location accuracy is quite important. How to get the
trade-off effectively is an open question, where current approaches of
utilizing attention schemes or very deep models result in complex models with
large memory consumption. Compared with the popularly-used convolutional neural
network (CNN) with fixed square kernels, graph convolutional network (GCN) can
explicitly utilize correlations between adjacent land covers and conduct
flexible convolution on arbitrarily irregular image regions. However, the
problems of large variations of target scales and blurred boundary cannot be
easily solved by GCN, while densely connected atrous convolution network
(DenseAtrousCNet) with multi-scale atrous convolution can expand the receptive
fields and obtain image global information. Inspired by the advantages of both
GCN and Atrous CNN, a two-stream deep neural network for semantic segmentation
of RSI (RSI-Net) is proposed in this paper to obtain improved performance
through modeling and propagating spatial contextual structure effectively and a
novel decoding scheme with image-level and graph-level combination. Extensive
experiments are implemented on the Vaihingen, Potsdam and Gaofen RSI datasets,
where the comparison results demonstrate the superior performance of RSI-Net in
terms of overall accuracy, F1 score and kappa coefficient when compared with
six state-of-the-art RSI semantic segmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LODE: Deep Local Deblurring and A New Benchmark. (arXiv:2109.09149v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09149">
<div class="article-summary-box-inner">
<span><p>While recent deep deblurring algorithms have achieved remarkable progress,
most existing methods focus on the global deblurring problem, where the image
blur mostly arises from severe camera shake. We argue that the local blur,
which is mostly derived from moving objects with a relatively static
background, is prevalent but remains under-explored. In this paper, we first
lay the data foundation for local deblurring by constructing, for the first
time, a LOcal-DEblur (LODE) dataset consisting of 3,700 real-world captured
locally blurred images and their corresponding ground-truth. Then, we propose a
novel framework, termed BLur-Aware DEblurring network (BladeNet), which
contains three components: the Local Blur Synthesis module generates locally
blurred training pairs, the Local Blur Perception module automatically captures
the locally blurred region and the Blur-guided Spatial Attention module guides
the deblurring network with spatial attention. This framework is flexible such
that it can be combined with many existing SotA algorithms. We carry out
extensive experiments on REDS and LODE datasets showing that BladeNet improves
PSNR by 2.5dB over SotAs for local deblurring while keeping comparable
performance for global deblurring. We will publish the dataset and codes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study of the Generalizability of Self-Supervised Representations. (arXiv:2109.09150v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09150">
<div class="article-summary-box-inner">
<span><p>Recent advancements in self-supervised learning (SSL) made it possible to
learn generalizable visual representations from unlabeled data. The performance
of Deep Learning models fine-tuned on pretrained SSL representations is on par
with models fine-tuned on the state-of-the-art supervised learning (SL)
representations. Irrespective of the progress made in SSL, its generalizability
has not been studied extensively. In this article, we perform a deeper analysis
of the generalizability of pretrained SSL and SL representations by conducting
a domain-based study for transfer learning classification tasks. The
representations are learned from the ImageNet source data, which are then
fine-tuned using two types of target datasets: similar to the source dataset,
and significantly different from the source dataset. We study generalizability
of the SSL and SL-based models via their prediction accuracy as well as
prediction confidence. In addition to this, we analyze the attribution of the
final convolutional layer of these models to understand how they reason about
the semantic identity of the data. We show that the SSL representations are
more generalizable as compared to the SL representations. We explain the
generalizability of the SSL representations by investigating its invariance
property, which is shown to be better than that observed in the SL
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation. (arXiv:2109.09163v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09163">
<div class="article-summary-box-inner">
<span><p>Task-relevant grasping is critical for industrial assembly, where downstream
manipulation tasks constrain the set of valid grasps. Learning how to perform
this task, however, is challenging, since task-relevant grasp labels are hard
to define and annotate. There is also yet no consensus on proper
representations for modeling or off-the-shelf tools for performing
task-relevant grasps. This work proposes a framework to learn task-relevant
grasping for industrial objects without the need of time-consuming real-world
data collection or manual annotation. To achieve this, the entire framework is
trained solely in simulation, including supervised training with synthetic
label generation and self-supervised, hand-object interaction. In the context
of this framework, this paper proposes a novel, object-centric canonical
representation at the category level, which allows establishing dense
correspondence across object instances and transferring task-relevant grasps to
novel instances. Extensive experiments on task-relevant grasping of
densely-cluttered industrial objects are conducted in both simulation and
real-world setups, demonstrating the effectiveness of the proposed framework.
Code and data will be released upon acceptance at
https://sites.google.com/view/catgrasp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Traffic-Net: 3D Traffic Monitoring Using a Single Camera. (arXiv:2109.09165v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09165">
<div class="article-summary-box-inner">
<span><p>Computer Vision has played a major role in Intelligent Transportation Systems
(ITS) and traffic surveillance. Along with the rapidly growing automated
vehicles and crowded cities, the automated and advanced traffic management
systems (ATMS) using video surveillance infrastructures have been evolved by
the implementation of Deep Neural Networks. In this research, we provide a
practical platform for real-time traffic monitoring, including 3D
vehicle/pedestrian detection, speed detection, trajectory estimation,
congestion detection, as well as monitoring the interaction of vehicles and
pedestrians, all using a single CCTV traffic camera. We adapt a custom YOLOv5
deep neural network model for vehicle/pedestrian detection and an enhanced SORT
tracking algorithm. For the first time, a hybrid satellite-ground based inverse
perspective mapping (SG-IPM) method for camera auto-calibration is also
developed which leads to an accurate 3D object detection and visualisation. We
also develop a hierarchical traffic modelling solution based on short- and
long-term temporal video data stream to understand the traffic flow,
bottlenecks, and risky spots for vulnerable road users. Several experiments on
real-world scenarios and comparisons with state-of-the-art are conducted using
various traffic monitoring datasets, including MIO-TCD, UA-DETRAC and GRAM-RTM
collected from highways, intersections, and urban areas under different
lighting and weather conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised 3D Pose Estimation for Hierarchical Dance Video Recognition. (arXiv:2109.09166v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09166">
<div class="article-summary-box-inner">
<span><p>Dance experts often view dance as a hierarchy of information, spanning
low-level (raw images, image sequences), mid-levels (human poses and bodypart
movements), and high-level (dance genre). We propose a Hierarchical Dance Video
Recognition framework (HDVR). HDVR estimates 2D pose sequences, tracks dancers,
and then simultaneously estimates corresponding 3D poses and 3D-to-2D imaging
parameters, without requiring ground truth for 3D poses. Unlike most methods
that work on a single person, our tracking works on multiple dancers, under
occlusions. From the estimated 3D pose sequence, HDVR extracts body part
movements, and therefrom dance genre. The resulting hierarchical dance
representation is explainable to experts. To overcome noise and interframe
correspondence ambiguities, we enforce spatial and temporal motion smoothness
and photometric continuity over time. We use an LSTM network to extract 3D
movement subsequences from which we recognize the dance genre. For experiments,
we have identified 154 movement types, of 16 body parts, and assembled a new
University of Illinois Dance (UID) Dataset, containing 1143 video clips of 9
genres covering 30 hours, annotated with movement and genre labels. Our
experimental results demonstrate that our algorithms outperform the
state-of-the-art 3D pose estimation methods, which also enhances our dance
recognition performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPoint: A Deep Learning Model for 3D Reconstruction in Point Clouds via mmWave Radar. (arXiv:2109.09188v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09188">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that mmWave radar sensing is effective for object
detection in low visibility environments, which makes it an ideal technique in
autonomous navigation systems such as autonomous vehicles. However, due to the
characteristics of radar signals such as sparsity, low resolution, specularity,
and high noise, it is still quite challenging to reconstruct 3D object shapes
via mmWave radar sensing. Built on our recent proposed 3DRIMR (3D
Reconstruction and Imaging via mmWave Radar), we introduce in this paper
DeepPoint, a deep learning model that generates 3D objects in point cloud
format that significantly outperforms the original 3DRIMR design. The model
adopts a conditional Generative Adversarial Network (GAN) based deep neural
network architecture. It takes as input the 2D depth images of an object
generated by 3DRIMR's Stage 1, and outputs smooth and dense 3D point clouds of
the object. The model consists of a novel generator network that utilizes a
sequence of DeepPoint blocks or layers to extract essential features of the
union of multiple rough and sparse input point clouds of an object when
observed from various viewpoints, given that those input point clouds may
contain many incorrect points due to the imperfect generation process of
3DRIMR's Stage 1. The design of DeepPoint adopts a deep structure to capture
the global features of input point clouds, and it relies on an optimally chosen
number of DeepPoint blocks and skip connections to achieve performance
improvement over the original 3DRIMR design. Our experiments have demonstrated
that this model significantly outperforms the original 3DRIMR and other
standard techniques in reconstructing 3D objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capsule networks with non-iterative cluster routing. (arXiv:2109.09213v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09213">
<div class="article-summary-box-inner">
<span><p>Capsule networks use routing algorithms to flow information between
consecutive layers. In the existing routing procedures, capsules produce
predictions (termed votes) for capsules of the next layer. In a nutshell, the
next-layer capsule's input is a weighted sum over all the votes it receives. In
this paper, we propose non-iterative cluster routing for capsule networks. In
the proposed cluster routing, capsules produce vote clusters instead of
individual votes for next-layer capsules, and each vote cluster sends its
centroid to a next-layer capsule. Generally speaking, the next-layer capsule's
input is a weighted sum over the centroid of each vote cluster it receives. The
centroid that comes from a cluster with a smaller variance is assigned a larger
weight in the weighted sum process. Compared with the state-of-the-art capsule
networks, the proposed capsule networks achieve the best accuracy on the
Fashion-MNIST and SVHN datasets with fewer parameters, and achieve the best
accuracy on the smallNORB and CIFAR-10 datasets with a moderate number of
parameters. The proposed capsule networks also produce capsules with
disentangled representation and generalize well to images captured at novel
viewpoints. The proposed capsule networks also preserve 2D spatial information
of an input image in the capsule channels: if the capsule channels are rotated,
the object reconstructed from these channels will be rotated by the same
transformation. Codes are available at
https://github.com/ZHAOZHIHAO/ClusterRouting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Baby Robot: Improving the Motor Skills of Toddlers. (arXiv:2109.09223v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09223">
<div class="article-summary-box-inner">
<span><p>This article introduces "Baby Robot", a robot aiming to improve motor skills
of babies and toddlers. Authors developed a car-like toy that moves
autonomously using reinforcement learning and computer vision techniques. The
robot behaviour is to escape from a target baby that has been previously
recognized, or at least detected, while avoiding obstacles, so that the
security of the baby is not compromised. A myriad of commercial toys with a
similar mobility improvement purpose are into the market; however, there is no
one that bets for an intelligent autonomous movement, as they perform simple
yet repetitive trajectories in the best of the cases. Two crawling toys -- one
in representation of "Baby Robot" -- were tested in a real environment with
respect to regular toys in order to check how they improved the toddlers
mobility. These real-life experiments were conducted with our proposed robot in
a kindergarten, where a group of children interacted with the toys. Significant
improvement in the motion skills of participants were detected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Automated Framework for COVID-19 Disease Identification from a Multicenter Dataset of Chest CT Scans. (arXiv:2109.09241v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09241">
<div class="article-summary-box-inner">
<span><p>The objective of this study is to develop a robust deep learning-based
framework to distinguish COVID-19, Community-Acquired Pneumonia (CAP), and
Normal cases based on chest CT scans acquired in different imaging centers
using various protocols, and radiation doses. We showed that while our proposed
model is trained on a relatively small dataset acquired from only one imaging
center using a specific scanning protocol, the model performs well on
heterogeneous test sets obtained by multiple scanners using different technical
parameters. We also showed that the model can be updated via an unsupervised
approach to cope with the data shift between the train and test sets and
enhance the robustness of the model upon receiving a new external dataset from
a different center. We adopted an ensemble architecture to aggregate the
predictions from multiple versions of the model. For initial training and
development purposes, an in-house dataset of 171 COVID-19, 60 CAP, and 76
Normal cases was used, which contained volumetric CT scans acquired from one
imaging center using a constant standard radiation dose scanning protocol. To
evaluate the model, we collected four different test sets retrospectively to
investigate the effects of the shifts in the data characteristics on the
model's performance. Among the test cases, there were CT scans with similar
characteristics as the train set as well as noisy low-dose and ultra-low dose
CT scans. In addition, some test CT scans were obtained from patients with a
history of cardiovascular diseases or surgeries. The entire test dataset used
in this study contained 51 COVID-19, 28 CAP, and 51 Normal cases. Experimental
results indicate that our proposed framework performs well on all test sets
achieving total accuracy of 96.15% (95%CI: [91.25-98.74]), COVID-19 sensitivity
of 96.08% (95%CI: [86.54-99.5]), CAP sensitivity of 92.86% (95%CI:
[76.50-99.19]).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories. (arXiv:2002.06478v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.06478">
<div class="article-summary-box-inner">
<span><p>We address the problem of discovering 3D parts for objects in unseen
categories. Being able to learn the geometry prior of parts and transfer this
prior to unseen categories pose fundamental challenges on data-driven shape
segmentation approaches. Formulated as a contextual bandit problem, we propose
a learning-based agglomerative clustering framework which learns a grouping
policy to progressively group small part proposals into bigger ones in a
bottom-up fashion. At the core of our approach is to restrict the local context
for extracting part-level features, which encourages the generalizability to
unseen categories. On the large-scale fine-grained 3D part dataset, PartNet, we
demonstrate that our method can transfer knowledge of parts learned from 3
training categories to 21 unseen testing categories without seeing any
annotated samples. Quantitative comparisons against four shape segmentation
baselines shows that our approach achieve the state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Fusion of Deep Multitasking Representations for Robust Visual Tracking. (arXiv:2004.01382v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.01382">
<div class="article-summary-box-inner">
<span><p>Visual object tracking remains an active research field in computer vision
due to persisting challenges with various problem-specific factors in
real-world scenes. Many existing tracking methods based on discriminative
correlation filters (DCFs) employ feature extraction networks (FENs) to model
the target appearance during the learning process. However, using deep feature
maps extracted from FENs based on different residual neural networks (ResNets)
has not previously been investigated. This paper aims to evaluate the
performance of twelve state-of-the-art ResNet-based FENs in a DCF-based
framework to determine the best for visual tracking purposes. First, it ranks
their best feature maps and explores the generalized adoption of the best
ResNet-based FEN into another DCF-based method. Then, the proposed method
extracts deep semantic information from a fully convolutional FEN and fuses it
with the best ResNet-based feature maps to strengthen the target representation
in the learning process of continuous convolution filters. Finally, it
introduces a new and efficient semantic weighting method (using semantic
segmentation feature maps on each video frame) to reduce the drift problem.
Extensive experimental results on the well-known OTB-2013, OTB-2015, TC-128 and
VOT-2018 visual tracking datasets demonstrate that the proposed method
effectively outperforms state-of-the-art methods in terms of precision and
robustness of visual tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The shape and simplicity biases of adversarially robust ImageNet-trained CNNs. (arXiv:2006.09373v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.09373">
<div class="article-summary-box-inner">
<span><p>Adversarial training has been the topic of dozens of studies and a leading
method for defending against adversarial attacks. Yet, it remains largely
unknown (a) how adversarially-robust ImageNet classifiers (R classifiers)
generalize to out-of-distribution examples; and (b) how their generalization
capability relates to their hidden representations. In this paper, we perform a
thorough, systematic study to answer these two questions across AlexNet,
GoogLeNet, and ResNet-50 architectures. We found that while standard ImageNet
classifiers have a strong texture bias, their R counterparts rely heavily on
shapes. Remarkably, adversarial training induces three simplicity biases into
hidden neurons in the process of 'robustifying' the network. That is, each
convolutional neuron in R networks often changes to detecting (1) pixel-wise
smoother patterns i.e. a mechanism that blocks high-frequency noise from
passing through the network; (2) more lower-level features i.e. textures and
colors (instead of objects); and (3) fewer types of inputs. Our findings reveal
the interesting mechanisms that made networks more adversarially robust and
also explain some recent findings e.g. why R networks benefit from much larger
capacity (Xie and Yuille, 2020) and can act as a strong image prior in image
synthesis (Santurkar et al., 2019).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Loss re-scaling VQA: Revisiting the LanguagePrior Problem from a Class-imbalance View. (arXiv:2010.16010v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16010">
<div class="article-summary-box-inner">
<span><p>Recent studies have pointed out that many well-developed Visual Question
Answering (VQA) models are heavily affected by the language prior problem,
which refers to making predictions based on the co-occurrence pattern between
textual questions and answers instead of reasoning visual contents. To tackle
it, most existing methods focus on enhancing visual feature learning to reduce
this superficial textual shortcut influence on VQA model decisions. However,
limited effort has been devoted to providing an explicit interpretation for its
inherent cause. It thus lacks a good guidance for the research community to
move forward in a purposeful way, resulting in model construction perplexity in
overcoming this non-trivial problem. In this paper, we propose to interpret the
language prior problem in VQA from a class-imbalance view. Concretely, we
design a novel interpretation scheme whereby the loss of mis-predicted frequent
and sparse answers of the same question type is distinctly exhibited during the
late training phase. It explicitly reveals why the VQA model tends to produce a
frequent yet obviously wrong answer, to a given question whose right answer is
sparse in the training set. Based upon this observation, we further develop a
novel loss re-scaling approach to assign different weights to each answer based
on the training data statistics for computing the final loss. We apply our
approach into three baselines and the experimental results on two VQA-CP
benchmark datasets evidently demonstrate its effectiveness. In addition, we
also justify the validity of the class imbalance interpretation scheme on other
computer vision tasks, such as face recognition and image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUSE: Textual Attributes Guided Portrait Painting Generation. (arXiv:2011.04761v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.04761">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach, MUSE, to illustrate textual attributes visually
via portrait generation. MUSE takes a set of attributes written in text, in
addition to facial features extracted from a photo of the subject as input. We
propose 11 attribute types to represent inspirations from a subject's profile,
emotion, story, and environment. We propose a novel stacked neural network
architecture by extending an image-to-image generative model to accept textual
attributes. Experiments show that our approach significantly outperforms
several state-of-the-art methods without using textual attributes, with
Inception Score score increased by 6% and Fr\'echet Inception Distance (FID)
score decreased by 11%, respectively. We also propose a new attribute
reconstruction metric to evaluate whether the generated portraits preserve the
subject's attributes. Experiments show that our approach can accurately
illustrate 78% textual attributes, which also help MUSE capture the subject in
a more creative and expressive way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing and Mitigating JPEG Compression Defects in Deep Learning. (arXiv:2011.08932v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08932">
<div class="article-summary-box-inner">
<span><p>With the proliferation of deep learning methods, many computer vision
problems which were considered academic are now viable in the consumer setting.
One drawback of consumer applications is lossy compression, which is necessary
from an engineering standpoint to efficiently and cheaply store and transmit
user images. Despite this, there has been little study of the effect of
compression on deep neural networks and benchmark datasets are often losslessly
compressed or compressed at high quality. Here we present a unified study of
the effects of JPEG compression on a range of common tasks and datasets. We
show that there is a significant penalty on common performance metrics for high
compression. We test several methods for mitigating this penalty, including a
novel method based on artifact correction which requires no labels to train.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Residual Bi-Fusion Feature Pyramid Network for Accurate Single-Shot Object Detection. (arXiv:2012.01724v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01724">
<div class="article-summary-box-inner">
<span><p>We propose the Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN)
for fast and accurate single-shot object detection. Feature Pyramid (FP) is
widely used in recent visual detection, however the top-down pathway of FP
cannot preserve accurate localization due to pooling shifting. The advantage of
FP is weaken as deeper backbones with more layers are used. To address this
issue, we propose a new parallel FP structure with bi-directional (top-down and
bottom-up) fusion and associated improvements to retain high-quality features
for accurate localization. Our method is particularly suitable for detecting
small objects. We provide the following design improvements: (1) A parallel
bifusion FP structure with a Bottom-up Fusion Module (BFM) to detect both small
and large objects at once with high accuracy. (2) A COncatenation and
RE-organization (CORE) module provides a bottom-up pathway for feature fusion,
which leads to the bi-directional fusion FP that can recover lost information
from lower-layer feature maps. (3) The CORE feature is further purified to
retain richer contextual information. Such purification is performed with CORE
in a few iterations in both top-down and bottom-up pathways. (4) The adding of
a residual design to CORE leads to a new Re-CORE module that enables easy
training and integration with a wide range of (deeper or lighter) backbones.
The proposed network achieves state-of-the-art performance on UAVDT17 and MS
COCO datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TFPnP: Tuning-free Plug-and-Play Proximal Algorithm with Applications to Inverse Imaging Problems. (arXiv:2012.05703v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05703">
<div class="article-summary-box-inner">
<span><p>Plug-and-Play (PnP) is a non-convex optimization framework that combines
proximal algorithms, for example, the alternating direction method of
multipliers (ADMM), with advanced denoising priors. Over the past few years,
great empirical success has been obtained by PnP algorithms, especially for the
ones that integrate deep learning-based denoisers. However, a key challenge of
PnP approaches is the need for manual parameter tweaking as it is essential to
obtain high-quality results across the high discrepancy in imaging conditions
and varying scene content. In this work, we present a class of tuning-free PnP
proximal algorithms that can determine parameters such as denoising strength,
termination time, and other optimization-specific parameters automatically. A
core part of our approach is a policy network for automated parameter search
which can be effectively learned via a mixture of model-free and model-based
deep reinforcement learning strategies. We demonstrate, through rigorous
numerical and visual experiments, that the learned policy can customize
parameters to different settings, and is often more efficient and effective
than existing handcrafted criteria. Moreover, we discuss several practical
considerations of PnP denoisers, which together with our learned policy yield
state-of-the-art results. This advanced performance is prevalent on both linear
and nonlinear exemplar inverse imaging problems, and in particular shows
promising results on compressed sensing MRI, sparse-view CT, single-photon
imaging, and phase retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MRDet: A Multi-Head Network for Accurate Oriented Object Detection in Aerial Images. (arXiv:2012.13135v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13135">
<div class="article-summary-box-inner">
<span><p>Objects in aerial images usually have arbitrary orientations and are densely
located over the ground, making them extremely challenge to be detected. Many
recently developed methods attempt to solve these issues by estimating an extra
orientation parameter and placing dense anchors, which will result in high
model complexity and computational costs. In this paper, we propose an
arbitrary-oriented region proposal network (AO-RPN) to generate oriented
proposals transformed from horizontal anchors. The AO-RPN is very efficient
with only a few amounts of parameters increase than the original RPN.
Furthermore, to obtain accurate bounding boxes, we decouple the detection task
into multiple subtasks and propose a multi-head network to accomplish them.
Each head is specially designed to learn the features optimal for the
corresponding task, which allows our network to detect objects accurately. We
name it MRDet short for Multi-head Rotated object Detector for convenience. We
test the proposed MRDet on two challenging benchmarks, i.e., DOTA and HRSC2016,
and compare it with several state-of-the-art methods. Our method achieves very
promising results which clearly demonstrate its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classic versus deep learning approaches to address computer vision challenges. (arXiv:2101.09744v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09744">
<div class="article-summary-box-inner">
<span><p>Computer vision and image processing address many challenging applications.
While the last decade has seen deep neural network architectures
revolutionizing those fields, early methods relied on 'classic', i.e.,
non-learned approaches. In this study, we explore the differences between
classic and deep learning (DL) algorithms to gain new insight regarding which
is more suitable for a given application. The focus is on two challenging
ill-posed problems, namely faint edge detection and multispectral image
registration, studying recent state-of-the-art DL and classic solutions. While
those DL algorithms outperform classic methods in terms of accuracy and
development time, they tend to have higher resource requirements and are unable
to perform outside their training space. Moreover, classic algorithms are more
transparent, which facilitates their adoption for real-life applications. As
both classes of approaches have unique strengths and limitations, the choice of
a solution is clearly application dependent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Pose-only Solution to Visual Reconstruction and Navigation. (arXiv:2103.01530v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01530">
<div class="article-summary-box-inner">
<span><p>Visual navigation and three-dimensional (3D) scene reconstruction are
essential for robotics to interact with the surrounding environment.
Large-scale scenes and critical camera motions are great challenges facing the
research community to achieve this goal. We raised a pose-only imaging geometry
framework and algorithms that can help solve these challenges. The
representation is a linear function of camera global translations, which allows
for efficient and robust camera motion estimation. As a result, the spatial
feature coordinates can be analytically reconstructed and do not require
nonlinear optimization. Experiments demonstrate that the computational
efficiency of recovering the scene and associated camera poses is significantly
improved by 2-4 orders of magnitude. This solution might be promising to unlock
real-time 3D visual computing in many forefront applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unmasking Face Embeddings by Self-restrained Triplet Loss for Accurate Masked Face Recognition. (arXiv:2103.01716v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01716">
<div class="article-summary-box-inner">
<span><p>Using the face as a biometric identity trait is motivated by the contactless
nature of the capture process and the high accuracy of the recognition
algorithms. After the current COVID-19 pandemic, wearing a face mask has been
imposed in public places to keep the pandemic under control. However, face
occlusion due to wearing a mask presents an emerging challenge for face
recognition systems. In this paper, we present a solution to improve the masked
face recognition performance. Specifically, we propose the Embedding Unmasking
Model (EUM) operated on top of existing face recognition models. We also
propose a novel loss function, the Self-restrained Triplet (SRT), which enabled
the EUM to produce embeddings similar to these of unmasked faces of the same
identities. The achieved evaluation results on three face recognition models,
two real masked datasets, and two synthetically generated masked face datasets
proved that our proposed approach significantly improves the performance in
most experimental settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Pairwise Neuroimage Analysis using the Soft Jaccard Index and 3D Keypoint Sets. (arXiv:2103.06966v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06966">
<div class="article-summary-box-inner">
<span><p>We propose a novel pairwise distance measure between image keypoint sets, for
the purpose of large-scale medical image indexing. Our measure generalizes the
Jaccard index to account for soft set equivalence (SSE) between keypoint
elements, via an adaptive kernel framework modeling uncertainty in keypoint
appearance and geometry. A new kernel is proposed to quantify the variability
of keypoint geometry in location and scale. Our distance measure may be
estimated between $O(N^2)$ image pairs in $O(N~\log~N)$ operations via keypoint
indexing. Experiments report the first results for the task of predicting
family relationships from medical images, using 1010 T1-weighted MRI brain
volumes of 434 families including monozygotic and dizygotic twins, siblings and
half-siblings sharing 100%-25% of their polymorphic genes. Soft set equivalence
and the keypoint geometry kernel improve upon standard hard set equivalence
(HSE) and appearance kernels alone in predicting family relationships.
Monozygotic twin identification is near 100%, and three subjects with uncertain
genotyping are automatically paired with their self-reported families, the
first reported practical application of image-based family identification. Our
distance measure can also be used to predict group categories, sex is predicted
with an AUC=0.97. Software is provided for efficient fine-grained curation of
large, generic image datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-DETR: Image-Level Few-Shot Object Detection with Inter-Class Correlation Exploitation. (arXiv:2103.11731v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11731">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection has been extensively investigated by incorporating
meta-learning into region-based detection frameworks. Despite its success, the
said paradigm is constrained by several factors, such as (i) low-quality region
proposals for novel classes and (ii) negligence of the inter-class correlation
among different classes. Such limitations hinder the generalization of
base-class knowledge for the detection of novel-class objects. In this work, we
design Meta-DETR, a novel few-shot detection framework that incorporates
correlational aggregation for meta-learning into DETR detection frameworks.
Meta-DETR works entirely at image level without any region proposals, which
circumvents the constraint of inaccurate proposals in prevalent few-shot
detection frameworks. Besides, Meta-DETR can simultaneously attend to multiple
support classes within a single feed-forward. This unique design allows
capturing the inter-class correlation among different classes, which
significantly reduces the misclassification of similar classes and enhances
knowledge generalization to novel classes. Experiments over multiple few-shot
object detection benchmarks show that the proposed Meta-DETR outperforms
state-of-the-art methods by large margins. The implementation codes will be
released at https://github.com/ZhangGongjie/Meta-DETR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Virtual Examples for Long-tailed Recognition. (arXiv:2103.15042v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15042">
<div class="article-summary-box-inner">
<span><p>We tackle the long-tailed visual recognition problem from the knowledge
distillation perspective by proposing a Distill the Virtual Examples (DiVE)
method. Specifically, by treating the predictions of a teacher model as virtual
examples, we prove that distilling from these virtual examples is equivalent to
label distribution learning under certain constraints. We show that when the
virtual example distribution becomes flatter than the original input
distribution, the under-represented tail classes will receive significant
improvements, which is crucial in long-tailed recognition. The proposed DiVE
method can explicitly tune the virtual example distribution to become flat.
Extensive experiments on three benchmark datasets, including the large-scale
iNaturalist ones, justify that the proposed DiVE method can significantly
outperform state-of-the-art methods. Furthermore, additional analyses and
experiments verify the virtual example interpretation, and demonstrate the
effectiveness of tailored designs in DiVE for long-tailed problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing. (arXiv:2104.01375v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01375">
<div class="article-summary-box-inner">
<span><p>Although deep neural networks hold the state-of-the-art in several remote
sensing tasks, their black-box operation hinders the understanding of their
decisions, concealing any bias and other shortcomings in datasets and model
performance. To this end, we have applied explainable artificial intelligence
(XAI) methods in remote sensing multi-label classification tasks towards
producing human-interpretable explanations and improve transparency. In
particular, we utilized and trained deep learning models with state-of-the-art
performance in the benchmark BigEarthNet and SEN12MS datasets. Ten XAI methods
were employed towards understanding and interpreting models' predictions, along
with quantitative metrics to assess and compare their performance. Numerous
experiments were performed to assess the overall performance of XAI methods for
straightforward prediction cases, competing multiple labels, as well as
misclassification cases. According to our findings, Occlusion, Grad-CAM and
Lime were the most interpretable and reliable XAI methods. However, none
delivers high-resolution outputs, while apart from Grad-CAM, both Lime and
Occlusion are computationally expensive. We also highlight different aspects of
XAI performance and elaborate with insights on black-box decisions in order to
improve transparency, understand their behavior and reveal, as well, datasets'
particularities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Action-Conditioned 3D Human Motion Synthesis with Transformer VAE. (arXiv:2104.05670v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05670">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of action-conditioned generation of realistic and
diverse human motion sequences. In contrast to methods that complete, or
extend, motion sequences, this task does not require an initial pose or
sequence. Here we learn an action-aware latent representation for human motions
by training a generative variational autoencoder (VAE). By sampling from this
latent space and querying a certain duration through a series of positional
encodings, we synthesize variable-length motion sequences conditioned on a
categorical action. Specifically, we design a Transformer-based architecture,
ACTOR, for encoding and decoding a sequence of parametric SMPL human body
models estimated from action recognition datasets. We evaluate our approach on
the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the
state of the art. Furthermore, we present two use cases: improving action
recognition through adding our synthesized data to training, and motion
denoising. Code and models are available on our project page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Higher Order Recurrent Space-Time Transformer. (arXiv:2104.08665v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08665">
<div class="article-summary-box-inner">
<span><p>Endowing visual agents with predictive capability is a key step towards video
intelligence at scale. The predominant modeling paradigm for this is sequence
learning, mostly implemented through LSTMs. Feed-forward Transformer
architectures have replaced recurrent model designs in ML applications of
language processing and also partly in computer vision. In this paper we
investigate on the competitiveness of Transformer-style architectures for video
predictive tasks. To do so we propose HORST, a novel higher order recurrent
layer design whose core element is a spatial-temporal decomposition of
self-attention for video. HORST achieves state of the art competitive
performance on Something-Something-V2 early action recognition and
EPIC-Kitchens-55 action anticipation, without exploiting a task specific
design. We believe this is promising evidence of causal predictive capability
that we attribute to our recurrent higher order design of self-attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Carrying out CNN Channel Pruning in a White Box. (arXiv:2104.11883v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11883">
<div class="article-summary-box-inner">
<span><p>Channel Pruning has been long studied to compress CNNs, which significantly
reduces the overall computation. Prior works implement channel pruning in an
unexplainable manner, which tends to reduce the final classification errors
while failing to consider the internal influence of each channel. In this
paper, we conduct channel pruning in a white box. Through deep visualization of
feature maps activated by different channels, we observe that different
channels have a varying contribution to different categories in image
classification. Inspired by this, we choose to preserve channels contributing
to most categories. Specifically, to model the contribution of each channel to
differentiating categories, we develop a class-wise mask for each channel,
implemented in a dynamic training manner w.r.t. the input image's category. On
the basis of the learned class-wise mask, we perform a global voting mechanism
to remove channels with less category discrimination. Lastly, a fine-tuning
process is conducted to recover the performance of the pruned model. To our
best knowledge, it is the first time that CNN interpretability theory is
considered to guide channel pruning. Extensive experiments on representative
image classification tasks demonstrate the superiority of our White-Box over
many state-of-the-arts. For instance, on CIFAR-10, it reduces 65.23% FLOPs with
even 0.62% accuracy improvement for ResNet-110. On ILSVRC-2012, White-Box
achieves a 45.6% FLOPs reduction with only a small loss of 0.83% in the top-1
accuracy for ResNet-50.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame. (arXiv:2105.06340v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06340">
<div class="article-summary-box-inner">
<span><p>Facial expression spotting is the preliminary step for micro- and
macro-expression analysis. The task of reliably spotting such expressions in
video sequences is currently unsolved. The current best systems depend upon
optical flow methods to extract regional motion features, before categorisation
of that motion into a specific class of facial movement. Optical flow is
susceptible to drift error, which introduces a serious problem for motions with
long-term dependencies, such as high frame-rate macro-expression. We propose a
purely deep learning solution which, rather than tracking frame differential
motion, compares via a convolutional model, each frame with two temporally
local reference frames. Reference frames are sampled according to calculated
micro- and macro-expression durations. We show that our solution achieves
state-of-the-art performance (F1-score of 0.105) in a dataset of high
frame-rate (200 fps) long video sequences (SAMM-LV) and is competitive in a low
frame-rate (30 fps) dataset (CAS(ME)2). In this paper, we document our deep
learning model and parameters, including how we use local contrast
normalisation, which we show is critical for optimal results. We surpass a
limitation in existing methods, and advance the state of deep learning in the
domain of facial expression spotting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing bikeability with street view imagery and computer vision. (arXiv:2105.08499v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08499">
<div class="article-summary-box-inner">
<span><p>Studies evaluating bikeability usually compute spatial indicators shaping
cycling conditions and conflate them in a quantitative index. Much research
involves site visits or conventional geospatial approaches, and few studies
have leveraged street view imagery (SVI) for conducting virtual audits. These
have assessed a limited range of aspects, and not all have been automated using
computer vision (CV). Furthermore, studies have not yet zeroed in on gauging
the usability of these technologies thoroughly. We investigate, with
experiments at a fine spatial scale and across multiple geographies (Singapore
and Tokyo), whether we can use SVI and CV to assess bikeability
comprehensively. Extending related work, we develop an exhaustive index of
bikeability composed of 34 indicators. The results suggest that SVI and CV are
adequate to evaluate bikeability in cities comprehensively. As they
outperformed non-SVI counterparts by a wide margin, SVI indicators are also
found to be superior in assessing urban bikeability, and potentially can be
used independently, replacing traditional techniques. However, the paper
exposes some limitations, suggesting that the best way forward is combining
both SVI and non-SVI approaches. The new bikeability index presents a
contribution in transportation and urban analytics, and it is scalable to
assess cycling appeal widely.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepGaze IIE: Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling. (arXiv:2105.12441v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12441">
<div class="article-summary-box-inner">
<span><p>Since 2014 transfer learning has become the key driver for the improvement of
spatial saliency prediction; however, with stagnant progress in the last 3-5
years. We conduct a large-scale transfer learning study which tests different
ImageNet backbones, always using the same read out architecture and learning
protocol adopted from DeepGaze II. By replacing the VGG19 backbone of DeepGaze
II with ResNet50 features we improve the performance on saliency prediction
from 78% to 85%. However, as we continue to test better ImageNet models as
backbones (such as EfficientNetB5) we observe no additional improvement on
saliency prediction. By analyzing the backbones further, we find that
generalization to other datasets differs substantially, with models being
consistently overconfident in their fixation predictions. We show that by
combining multiple backbones in a principled manner a good confidence
calibration on unseen datasets can be achieved. This new model, "DeepGaze IIE",
yields a significant leap in benchmark performance in and out-of-domain with a
15 percent point improvement over DeepGaze II to 93% on MIT1003, marking a new
state of the art on the MIT/Tuebingen Saliency Benchmark in all available
metrics (AUC: 88.3%, sAUC: 79.4%, CC: 82.4%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransVOS: Video Object Segmentation with Transformers. (arXiv:2106.00588v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00588">
<div class="article-summary-box-inner">
<span><p>Recently, Space-Time Memory Network (STM) based methods have achieved
state-of-the-art performance in semi-supervised video object segmentation
(VOS). A crucial problem in this task is how to model the dependency both among
different frames and inside every frame. However, most of these methods neglect
the spatial relationships (inside each frame) and do not make full use of the
temporal relationships (among different frames). In this paper, we propose a
new transformer-based framework, termed TransVOS, introducing a vision
transformer to fully exploit and model both the temporal and spatial
relationships. Moreover, most STM-based approaches employ two separate encoders
to extract features of two significant inputs, i.e., reference sets (history
frames with predicted masks) and query frame (current frame), respectively,
increasing the models' parameters and complexity. To slim the popular
two-encoder pipeline while keeping the effectiveness, we design a single
two-path feature extractor to encode the above two inputs in a unified way.
Extensive experiments demonstrate the superiority of our TransVOS over
state-of-the-art methods on both DAVIS and YouTube-VOS datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reveal of Vision Transformers Robustness against Adversarial Attacks. (arXiv:2106.03734v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03734">
<div class="article-summary-box-inner">
<span><p>The major part of the vanilla vision transformer (ViT) is the attention block
that brings the power of mimicking the global context of the input image. For
better performance, ViT needs large-scale training data. To overcome this data
hunger limitation, many ViT-based networks, or hybrid-ViT, have been proposed
to include local context during the training. The robustness of ViTs and its
variants against adversarial attacks has not been widely investigated in the
literature like CNNs. This work studies the robustness of ViT variants 1)
against different Lp-based adversarial attacks in comparison with CNNs, 2)
under adversarial examples (AEs) after applying preprocessing defense methods
and 3) under the adaptive attacks using expectation over transformation (EOT)
framework. To that end, we run a set of experiments on 1000 images from
ImageNet-1k and then provide an analysis that reveals that vanilla ViT or
hybrid-ViT are more robust than CNNs. For instance, we found that 1) Vanilla
ViTs or hybrid-ViTs are more robust than CNNs under Lp-based attacks and under
adaptive attacks. 2) Unlike hybrid-ViTs, Vanilla ViTs are not responding to
preprocessing defenses that mainly reduce the high frequency components.
Furthermore, feature maps, attention maps, and Grad-CAM visualization jointly
with image quality measures, and perturbations' energy spectrum are provided
for an insight understanding of attention-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XIRL: Cross-embodiment Inverse Reinforcement Learning. (arXiv:2106.03911v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03911">
<div class="article-summary-box-inner">
<span><p>We investigate the visual cross-embodiment imitation setting, in which agents
learn policies from videos of other agents (such as humans) demonstrating the
same task, but with stark differences in their embodiments -- shape, actions,
end-effector dynamics, etc. In this work, we demonstrate that it is possible to
automatically discover and learn vision-based reward functions from
cross-embodiment demonstration videos that are robust to these differences.
Specifically, we present a self-supervised method for Cross-embodiment Inverse
Reinforcement Learning (XIRL) that leverages temporal cycle-consistency
constraints to learn deep visual embeddings that capture task progression from
offline videos of demonstrations across multiple expert agents, each performing
the same task differently due to embodiment differences. Prior to our work,
producing rewards from self-supervised embeddings typically required alignment
with a reference trajectory, which may be difficult to acquire under stark
embodiment differences. We show empirically that if the embeddings are aware of
task progress, simply taking the negative distance between the current state
and goal state in the learned embedding space is useful as a reward for
training policies with reinforcement learning. We find our learned reward
function not only works for embodiments seen during training, but also
generalizes to entirely new embodiments. Additionally, when transferring
real-world human demonstrations to a simulated robot, we find that XIRL is more
sample efficient than current best methods. Qualitative results, code, and
datasets are available at https://x-irl.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surgical data science for safe cholecystectomy: a protocol for segmentation of hepatocystic anatomy and assessment of the critical view of safety. (arXiv:2106.10916v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10916">
<div class="article-summary-box-inner">
<span><p>Minimally invasive image-guided surgery heavily relies on vision. Deep
learning models for surgical video analysis could therefore support visual
tasks such as assessing the critical view of safety (CVS) in laparoscopic
cholecystectomy (LC), potentially contributing to surgical safety and
efficiency. However, the performance, reliability and reproducibility of such
models are deeply dependent on the quality of data and annotations used in
their development. Here, we present a protocol, checklists, and visual examples
to promote consistent annotation of hepatocystic anatomy and CVS criteria. We
believe that sharing annotation guidelines can help build trustworthy
multicentric datasets for assessing generalizability of performance, thus
accelerating the clinical translation of deep learning models for surgical
video analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocFormer: End-to-End Transformer for Document Understanding. (arXiv:2106.11539v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11539">
<div class="article-summary-box-inner">
<span><p>We present DocFormer -- a multi-modal transformer based architecture for the
task of Visual Document Understanding (VDU). VDU is a challenging problem which
aims to understand documents in their varied formats (forms, receipts etc.) and
layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using
carefully designed tasks which encourage multi-modal interaction. DocFormer
uses text, vision and spatial features and combines them using a novel
multi-modal self-attention layer. DocFormer also shares learned spatial
embeddings across modalities which makes it easy for the model to correlate
text to visual tokens and vice versa. DocFormer is evaluated on 4 different
datasets each with strong baselines. DocFormer achieves state-of-the-art
results on all of them, sometimes beating models 4x its size (in no. of
parameters).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Counterfactual Visual Explanations With Overdetermination. (arXiv:2106.14556v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14556">
<div class="article-summary-box-inner">
<span><p>A novel explainable AI method called CLEAR Image is introduced in this paper.
CLEAR Image is based on the view that a satisfactory explanation should be
contrastive, counterfactual and measurable. CLEAR Image explains an image's
classification probability by contrasting the image with a corresponding image
generated automatically via adversarial learning. This enables both salient
segmentation and perturbations that faithfully determine each segment's
importance. CLEAR Image was successfully applied to a medical imaging case
study where it outperformed methods such as Grad-CAM and LIME by an average of
27% using a novel pointing game metric. CLEAR Image excels in identifying cases
of "causal overdetermination" where there are multiple patches in an image, any
one of which is sufficient by itself to cause the classification probability to
be close to one.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Cognitive Fatigue from fMRI Scans with Self-supervised Learning. (arXiv:2106.15009v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15009">
<div class="article-summary-box-inner">
<span><p>Functional magnetic resonance imaging (fMRI) is a neuroimaging technique that
records neural activations in the brain by capturing the blood oxygen level in
different regions based on the task performed by a subject. Given fMRI data,
the problem of predicting the state of cognitive fatigue in a person has not
been investigated to its full extent. This paper proposes tackling this issue
as a multi-class classification problem by dividing the state of cognitive
fatigue into six different levels, ranging from no-fatigue to extreme fatigue
conditions. We built a spatio-temporal model that uses convolutional neural
networks (CNN) for spatial feature extraction and a long short-term memory
(LSTM) network for temporal modeling of 4D fMRI scans. We also applied a
self-supervised method called MoCo (Momentum Contrast) to pre-train our model
on a public dataset BOLD5000 and fine-tuned it on our labeled dataset to
predict cognitive fatigue. Our novel dataset contains fMRI scans from Traumatic
Brain Injury (TBI) patients and healthy controls (HCs) while performing a
series of N-back cognitive tasks. This method establishes a state-of-the-art
technique to analyze cognitive fatigue from fMRI data and beats previous
approaches to solve this problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in Lymphoid Neoplasms. (arXiv:2106.16174v2 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.16174">
<div class="article-summary-box-inner">
<span><p>The cells and their spatial patterns in the tumor microenvironment (TME) play
a key role in tumor evolution, and yet the latter remains an understudied topic
in computational pathology. This study, to the best of our knowledge, is among
the first to hybridize local and global graph methods to profile orchestration
and interaction of cellular components. To address the challenge in
hematolymphoid cancers, where the cell classes in TME may be unclear, we first
implemented cell-level unsupervised learning and identified two new cell
subtypes. Local cell graphs or supercells were built for each image by
considering the individual cell's geospatial location and classes. Then, we
applied supercell level clustering and identified two new cell communities. In
the end, we built global graphs to abstract spatial interaction patterns and
extract features for disease diagnosis. We evaluate the proposed algorithm on
H&amp;E slides of 60 hematolymphoid neoplasms and further compared it with three
cell level graph-based algorithms, including the global cell graph, cluster
cell graph, and FLocK. The proposed algorithm achieved a mean diagnosis
accuracy of 0.703 with the repeated 5-fold cross-validation scheme. In
conclusion, our algorithm shows superior performance over the existing methods
and can be potentially applied to other cancer types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YinYang-Net: Complementing Face and Body Information for Wild Gender Recognition. (arXiv:2107.06847v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06847">
<div class="article-summary-box-inner">
<span><p>Soft biometrics inference in surveillance scenarios is a topic of interest
for various applications, particularly in security-related areas. However, soft
biometric analysis is not extensively reported in wild conditions. In
particular, previous works on gender recognition report their results in face
datasets, with relatively good image quality and frontal poses. Given the
uncertainty of the availability of the facial region in wild conditions, we
consider that these methods are not adequate for surveillance settings. To
overcome these limitations, we: 1) present frontal and wild face versions of
three well-known surveillance datasets; and 2) propose YinYang-Net (YY-Net), a
model that effectively and dynamically complements facial and body information,
which makes it suitable for gender recognition in wild conditions. The frontal
and wild face datasets derive from widely used Pedestrian Attribute Recognition
(PAR) sets (PETA, PA-100K, and RAP), using a pose-based approach to filter the
frontal samples and facial regions. This approach retrieves the facial region
of images with varying image/subject conditions, where the state-of-the-art
face detectors often fail. YY-Net combines facial and body information through
a learnable fusion matrix and a channel-attention sub-network, focusing on the
most influential body parts according to the specific image/subject features.
We compare it with five PAR methods, consistently obtaining state-of-the-art
results on gender recognition, and reducing the prediction errors by up to 24%
in frontal samples. The announced PAR datasets versions and YY-Net serve as the
basis for wild soft biometrics classification and are available in
https://github.com/Tiago-Roxo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wasserstein Distances, Geodesics and Barycenters of Merge Trees. (arXiv:2107.07789v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07789">
<div class="article-summary-box-inner">
<span><p>This paper presents a unified computational framework for the estimation of
distances, geodesics and barycenters of merge trees. We extend recent work on
the edit distance [106] and introduce a new metric, called the Wasserstein
distance between merge trees, which is purposely designed to enable efficient
computations of geodesics and barycenters. Specifically, our new distance is
strictly equivalent to the L2-Wasserstein distance between extremum persistence
diagrams, but it is restricted to a smaller solution space, namely, the space
of rooted partial isomorphisms between branch decomposition trees. This enables
a simple extension of existing optimization frameworks [112] for geodesics and
barycenters from persistence diagrams to merge trees. We introduce a task-based
algorithm which can be generically applied to distance, geodesic, barycenter or
cluster computation. The task-based nature of our approach enables further
accelerations with shared-memory parallelism. Extensive experiments on public
ensembles and SciVis contest benchmarks demonstrate the efficiency of our
approach -- with barycenter computations in the orders of minutes for the
largest examples -- as well as its qualitative ability to generate
representative barycenter merge trees, visually summarizing the features of
interest found in the ensemble. We show the utility of our contributions with
dedicated visualization applications: feature tracking, temporal reduction and
ensemble clustering. We provide a lightweight C++ implementation that can be
used to reproduce our results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SNE-RoadSeg+: Rethinking Depth-Normal Translation and Deep Supervision for Freespace Detection. (arXiv:2107.14599v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14599">
<div class="article-summary-box-inner">
<span><p>Freespace detection is a fundamental component of autonomous driving
perception. Recently, deep convolutional neural networks (DCNNs) have achieved
impressive performance for this task. In particular, SNE-RoadSeg, our
previously proposed method based on a surface normal estimator (SNE) and a
data-fusion DCNN (RoadSeg), has achieved impressive performance in freespace
detection. However, SNE-RoadSeg is computationally intensive, and it is
difficult to execute in real time. To address this problem, we introduce
SNE-RoadSeg+, an upgraded version of SNE-RoadSeg. SNE-RoadSeg+ consists of 1)
SNE+, a module for more accurate surface normal estimation, and 2) RoadSeg+, a
data-fusion DCNN that can greatly minimize the trade-off between accuracy and
efficiency with the use of deep supervision. Extensive experimental results
have demonstrated the effectiveness of our SNE+ for surface normal estimation
and the superior performance of our SNE-RoadSeg+ over all other freespace
detection approaches. Specifically, our SNE-RoadSeg+ runs in real time, and
meanwhile, achieves the state-of-the-art performance on the KITTI road
benchmark. Our project page is at
https://www.sne-roadseg.site/sne-roadseg-plus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Free Lunch for Co-Saliency Detection: Context Adjustment. (arXiv:2108.02093v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02093">
<div class="article-summary-box-inner">
<span><p>We unveil a long-standing problem in the prevailing co-saliency detection
systems: there is indeed inconsistency between training and testing.
Constructing a high-quality co-saliency detection dataset involves
time-consuming and labor-intensive pixel-level labeling, which has forced most
recent works to rely instead on semantic segmentation or saliency detection
datasets for training. However, the lack of proper co-saliency and the absence
of multiple foreground objects in these datasets can lead to spurious
variations and inherent biases learned by models. To tackle this, we introduce
the idea of counterfactual training through context adjustment and propose a
"cost-free" group-cut-paste (GCP) procedure to leverage off-the-shelf images
and synthesize new samples. Following GCP, we collect a novel dataset called
Context Adjustment Training (CAT). CAT consists of 33,500 images, which is four
times larger than the current co-saliency detection datasets. All samples are
automatically annotated with high-quality mask annotations, object categories,
and edge maps. Extensive experiments on recent benchmarks are conducted, show
that CAT can improve various state-of-the-art models by a large margin (5% ~
25%). We hope that the scale, diversity, and quality of our dataset can benefit
researchers in this area and beyond. Our dataset will be publicly accessible
through our project page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sketch Your Own GAN. (arXiv:2108.02774v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02774">
<div class="article-summary-box-inner">
<span><p>Can a user create a deep generative model by sketching a single example?
Traditionally, creating a GAN model has required the collection of a
large-scale dataset of exemplars and specialized knowledge in deep learning. In
contrast, sketching is possibly the most universally accessible way to convey a
visual concept. In this work, we present a method, GAN Sketching, for rewriting
GANs with one or more sketches, to make GANs training easier for novice users.
In particular, we change the weights of an original GAN model according to user
sketches. We encourage the model's output to match the user sketches through a
cross-domain adversarial loss. Furthermore, we explore different regularization
methods to preserve the original model's diversity and image quality.
Experiments have shown that our method can mold GANs to match shapes and poses
specified by sketches while maintaining realism and diversity. Finally, we
demonstrate a few applications of the resulting GAN, including latent space
interpolation and image editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RECALL: Replay-based Continual Learning in Semantic Segmentation. (arXiv:2108.03673v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03673">
<div class="article-summary-box-inner">
<span><p>Deep networks allow to obtain outstanding results in semantic segmentation,
however they need to be trained in a single shot with a large amount of data.
Continual learning settings where new classes are learned in incremental steps
and previous training data is no longer available are challenging due to the
catastrophic forgetting phenomenon. Existing approaches typically fail when
several incremental steps are performed or in presence of a distribution shift
of the background class. We tackle these issues by recreating no longer
available data for the old classes and outlining a content inpainting scheme on
the background class. We propose two sources for replay data. The first resorts
to a generative adversarial network to sample from the class space of past
learning steps. The second relies on web-crawled data to retrieve images
containing examples of old classes from online databases. In both scenarios no
samples of past steps are stored, thus avoiding privacy concerns. Replay data
are then blended with new samples during the incremental steps. Our approach,
RECALL, outperforms state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-WAE: Generalized Patch-Wasserstein Autoencoder for Anomaly Screening. (arXiv:2108.03815v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03815">
<div class="article-summary-box-inner">
<span><p>Anomaly detection plays a pivotal role in numerous real-world scenarios, such
as industrial automation and manufacturing intelligence. Recently, variational
inference-based anomaly analysis has attracted researchers' and developers'
attention. It aims to model the defect-free distribution so that anomalies can
be classified as out-of-distribution samples. Nevertheless, there are two
disturbing factors that need us to prioritize: (i) the simplistic prior latent
distribution inducing limited expressive capability; (ii) the strong
probability distance notion results in collapsed features. In this paper, we
propose a novel Patch-wise Wasserstein AutoEncoder (P-WAE) architecture to
alleviate those challenges. In particular, a patch-wise variational inference
model coupled with solving the jigsaw puzzle is designed, which is a simple yet
effective way to increase the expressiveness of the latent manifold. This makes
using the model on high-dimensional practical data possible. In addition, we
leverage a weaker measure, sliced-Wasserstein distance, to achieve the
equilibrium between the reconstruction fidelity and generalized
representations. Comprehensive experiments, conducted on the MVTec AD dataset,
demonstrate the superior performance of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03823">
<div class="article-summary-box-inner">
<span><p>To mitigate the radiologist's workload, computer-aided diagnosis with the
capability to review and analyze medical images is gradually deployed. Deep
learning-based region of interest segmentation is among the most exciting use
cases. However, this paradigm is restricted in real-world clinical applications
due to poor robustness and generalization. The issue is more sinister with a
lack of training data. In this paper, we address the challenge from the
representation learning point of view. We investigate that the collapsed
representations, as one of the main reasons which caused poor robustness and
generalization, could be avoided through transfer learning. Therefore, we
propose a novel two-stage framework for robust generalized segmentation. In
particular, an unsupervised Tile-wise AutoEncoder (T-AE) pretraining
architecture is coined to learn meaningful representation for improving the
generalization and robustness of the downstream tasks. Furthermore, the learned
knowledge is transferred to the segmentation benchmark. Coupled with an image
reconstruction network, the representation keeps to be decoded, encouraging the
model to capture more semantic features. Experiments of lung segmentation on
multi chest X-ray datasets are conducted. Empirically, the related experimental
results demonstrate the superior generalization capability of the proposed
framework on unseen domains in terms of high performance and robustness to
corruption, especially under the scenario of the limited training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Cut by Watching Movies. (arXiv:2108.04294v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04294">
<div class="article-summary-box-inner">
<span><p>Video content creation keeps growing at an incredible pace; yet, creating
engaging stories remains challenging and requires non-trivial video editing
expertise. Many video editing components are astonishingly hard to automate
primarily due to the lack of raw video materials. This paper focuses on a new
task for computational video editing, namely the task of raking cut
plausibility. Our key idea is to leverage content that has already been edited
to learn fine-grained audiovisual patterns that trigger cuts. To do this, we
first collected a data source of more than 10K videos, from which we extract
more than 255K cuts. We devise a model that learns to discriminate between real
and artificial cuts via contrastive learning. We set up a new task and a set of
baselines to benchmark video cut generation. We observe that our proposed model
outperforms the baselines by large margins. To demonstrate our model in
real-world applications, we conduct human studies in a collection of unedited
videos. The results show that our model does a better job at cutting than
random and alternative baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Cascaded Zoom-In Network for Patterned Fabric Defect Detection. (arXiv:2108.06760v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06760">
<div class="article-summary-box-inner">
<span><p>Nowadays, Deep Convolutional Neural Networks (DCNNs) are widely used in
fabric defect detection, which come with the cost of expensive training and
complex model parameters. With the observation that most fabrics are defect
free in practice, a two-step Cascaded Zoom-In Network (CZI-Net) is proposed for
patterned fabric defect detection. In the CZI-Net, the Aggregated HOG (A-HOG)
and SIFT features are used to instead of simple convolution filters for feature
extraction. Moreover, in order to extract more distinctive features, the
feature representation layer and full connection layer are included in the
CZI-Net. In practice, Most defect-free fabrics only involve in the first step
of our method and avoid a costive computation in the second step, which makes
very fast fabric detection. More importantly, we propose the
Locality-constrained Reconstruction Error (LCRE) in the first step and
Restrictive Locality-constrained Coding (RLC), Bag-of-Indexes (BoI) methods in
the second step. We also analyse the connections between different coding
methods and conclude that the index of visual words plays an essential role in
the coding methods. In conclusion, experiments based on real-world datasets are
implemented and demonstrate that our proposed method is not only
computationally simple but also with high detection accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Image Coding for Machines: A Content-Adaptive Approach. (arXiv:2108.09992v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09992">
<div class="article-summary-box-inner">
<span><p>Today, according to the Cisco Annual Internet Report (2018-2023), the
fastest-growing category of Internet traffic is machine-to-machine
communication. In particular, machine-to-machine communication of images and
videos represents a new challenge and opens up new perspectives in the context
of data compression. One possible solution approach consists of adapting
current human-targeted image and video coding standards to the use case of
machine consumption. Another approach consists of developing completely new
compression paradigms and architectures for machine-to-machine communications.
In this paper, we focus on image compression and present an inference-time
content-adaptive finetuning scheme that optimizes the latent representation of
an end-to-end learned image codec, aimed at improving the compression
efficiency for machine-consumption. The conducted experiments show that our
online finetuning brings an average bitrate saving (BD-rate) of -3.66% with
respect to our pretrained image codec. In particular, at low bitrate points,
our proposed method results in a significant bitrate saving of -9.85%. Overall,
our pretrained-and-then-finetuned system achieves -30.54% BD-rate over the
state-of-the-art image/video codec Versatile Video Coding (VVC).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sk-Unet Model with Fourier Domain for Mitosis Detection. (arXiv:2109.00957v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00957">
<div class="article-summary-box-inner">
<span><p>Mitotic count is the most important morphological feature of breast cancer
grading. Many deep learning-based methods have been proposed but suffer from
domain shift. In this work, we construct a Fourier-based segmentation model for
mitosis detection to address the problem. Swapping the low-frequency spectrum
of source and target images is shown effective to alleviate the discrepancy
between different scanners. Our Fourier-based segmentation method can achieve
F1 with 0.7456 on the preliminary test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GOHOME: Graph-Oriented Heatmap Output for future Motion Estimation. (arXiv:2109.01827v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01827">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose GOHOME, a method leveraging graph representations
of the High Definition Map and sparse projections to generate a heatmap output
representing the future position probability distribution for a given agent in
a traffic scene. This heatmap output yields an unconstrained 2D grid
representation of agent future possible locations, allowing inherent
multimodality and a measure of the uncertainty of the prediction. Our
graph-oriented model avoids the high computation burden of representing the
surrounding context as squared images and processing it with classical CNNs,
but focuses instead only on the most probable lanes where the agent could end
up in the immediate future. GOHOME reaches 2$nd$ on Argoverse Motion
Forecasting Benchmark on the MissRate$_6$ metric while achieving significant
speed-up and memory burden diminution compared to Argoverse 1$^{st}$ place
method HOME. We also highlight that heatmap output enables multimodal
ensembling and improve 1$^{st}$ place MissRate$_6$ by more than 15$\%$ with our
best ensemble on Argoverse. Finally, we evaluate and reach state-of-the-art
performance on the other trajectory prediction datasets nuScenes and
Interaction, demonstrating the generalizability of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum-Classical Hybrid Machine Learning for Image Classification (ICCAD Special Session Paper). (arXiv:2109.02862v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02862">
<div class="article-summary-box-inner">
<span><p>Image classification is a major application domain for conventional deep
learning (DL). Quantum machine learning (QML) has the potential to
revolutionize image classification. In any typical DL-based image
classification, we use convolutional neural network (CNN) to extract features
from the image and multi-layer perceptron network (MLP) to create the actual
decision boundaries. On one hand, QML models can be useful in both of these
tasks. Convolution with parameterized quantum circuits (Quanvolution) can
extract rich features from the images. On the other hand, quantum neural
network (QNN) models can create complex decision boundaries. Therefore,
Quanvolution and QNN can be used to create an end-to-end QML model for image
classification. Alternatively, we can extract image features separately using
classical dimension reduction techniques such as, Principal Components Analysis
(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to
train a QNN. We review two proposals on quantum-classical hybrid ML models for
image classification namely, Quanvolutional Neural Network and dimension
reduction using a classical algorithm followed by QNN. Particularly, we make a
case for trainable filters in Quanvolution and CAE-based feature extraction for
image datasets (instead of dimension reduction using linear transformations
such as, PCA). We discuss various design choices, potential opportunities, and
drawbacks of these models. We also release a Python-based framework to create
and explore these hybrid models with a variety of design choices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Melatect: A Machine Learning Model Approach For Identifying Malignant Melanoma in Skin Growths. (arXiv:2109.03310v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03310">
<div class="article-summary-box-inner">
<span><p>Malignant melanoma is a common skin cancer that is mostly curable before
metastasis -when growths spawn in organs away from the original site. Melanoma
is the most dangerous type of skin cancer if left untreated due to the high
risk of metastasis. This paper presents Melatect, a machine learning (ML) model
embedded in an iOS app that identifies potential malignant melanoma. Melatect
accurately classifies lesions as malignant or benign over 96.6% of the time
with no apparent bias or overfitting. Using the Melatect app, users have the
ability to take pictures of skin lesions (moles) and subsequently receive a
mole classification. The Melatect app provides a convenient way to get free
advice on lesions and track these lesions over time. A recursive computer image
analysis algorithm and modified MLOps pipeline was developed to create a model
that performs at a higher accuracy than existing models. Our training dataset
included 18,400 images of benign and malignant lesions, including 18,000 from
the International Skin Imaging Collaboration (ISIC) archive, as well as 400
images gathered from local dermatologists; these images were augmented using
DeepAugment, an AutoML tool, to 54,054 images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Tensor Network Representation for High-Order Tensor Completion. (arXiv:2109.04022v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04022">
<div class="article-summary-box-inner">
<span><p>This work studies the problem of high-dimensional data (referred to tensors)
completion from partially observed samplings. We consider that a tensor is a
superposition of multiple low-rank components. In particular, each component
can be represented as multilinear connections over several latent factors and
naturally mapped to a specific tensor network (TN) topology. In this paper, we
propose a fundamental tensor decomposition (TD) framework: Multi-Tensor Network
Representation (MTNR), which can be regarded as a linear combination of a range
of TD models, e.g., CANDECOMP/PARAFAC (CP) decomposition, Tensor Train (TT),
and Tensor Ring (TR). Specifically, MTNR represents a high-order tensor as the
addition of multiple TN models, and the topology of each TN is automatically
generated instead of manually pre-designed. For the optimization phase, an
adaptive topology learning (ATL) algorithm is presented to obtain latent
factors of each TN based on a rank incremental strategy and a projection error
measurement strategy. In addition, we theoretically establish the fundamental
multilinear operations for the tensors with TN representation, and reveal the
structural transformation of MTNR to a single TN. Finally, MTNR is applied to a
typical task, tensor completion, and two effective algorithms are proposed for
the exact recovery of incomplete data based on the Alternating Least Squares
(ALS) scheme and Alternating Direction Method of Multiplier (ADMM) framework.
Extensive numerical experiments on synthetic data and real-world datasets
demonstrate the effectiveness of MTNR compared with the start-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Transferable Adversarial Attacks on Vision Transformers. (arXiv:2109.04176v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04176">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have demonstrated impressive performance on a
series of computer vision tasks, yet they still suffer from adversarial
examples. In this paper, we posit that adversarial attacks on transformers
should be specially tailored for their architecture, jointly considering both
patches and self-attention, in order to achieve high transferability. More
specifically, we introduce a dual attack framework, which contains a Pay No
Attention (PNA) attack and a PatchOut attack, to improve the transferability of
adversarial samples across different ViTs. We show that skipping the gradients
of attention during backpropagation can generate adversarial examples with high
transferability. In addition, adversarial perturbations generated by optimizing
randomly sampled subsets of patches at each iteration achieve higher attack
success rates than attacks using all patches. We evaluate the transferability
of attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The
results of these experiments demonstrate that the proposed dual attack can
greatly boost transferability between ViTs and from ViTs to CNNs. In addition,
the proposed method can easily be combined with existing transfer methods to
boost performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ErfAct and PSerf: Non-monotonic smooth trainable Activation Functions. (arXiv:2109.04386v3 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04386">
<div class="article-summary-box-inner">
<span><p>An activation function is a crucial component of a neural network that
introduces non-linearity in the network. The state-of-the-art performance of a
neural network depends on the perfect choice of an activation function. We
propose two novel non-monotonic smooth trainable activation functions, called
ErfAct and PSerf. Experiments suggest that the proposed functions improve the
network performance significantly compared to the widely used activations like
ReLU, Swish, and Mish. Replacing ReLU by ErfAct and PSerf, we have 5.21% and
5.04% improvement for top-1 accuracy on PreactResNet-34 network in CIFAR100
dataset, 2.58% and 2.76% improvement for top-1 accuracy on PreactResNet-34
network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean average
precision (mAP) on SSD300 model in Pascal VOC dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvMLP: Hierarchical Convolutional MLPs for Vision. (arXiv:2109.04454v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04454">
<div class="article-summary-box-inner">
<span><p>MLP-based architectures, which consist of a sequence of consecutive
multi-layer perceptron blocks, have recently been found to reach comparable
results to convolutional and transformer-based methods. However, most adopt
spatial MLPs which take fixed dimension inputs, therefore making it difficult
to apply them to downstream tasks, such as object detection and semantic
segmentation. Moreover, single-stage designs further limit performance in other
computer vision tasks and fully connected layers bear heavy computation. To
tackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for
visual recognition, which is a light-weight, stage-wise, co-design of
convolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1
accuracy on ImageNet-1k with 9M parameters and 2.4G MACs (15% and 19% of
MLP-Mixer-B/16, respectively). Experiments on object detection and semantic
segmentation further show that visual representation learned by ConvMLP can be
seamlessly transferred and achieve competitive results with fewer parameters.
Our code and pre-trained models are publicly available at
https://github.com/SHI-Labs/Convolutional-MLPs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time multimodal image registration with partial intraoperative point-set data. (arXiv:2109.05023v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05023">
<div class="article-summary-box-inner">
<span><p>We present Free Point Transformer (FPT) - a deep neural network architecture
for non-rigid point-set registration. Consisting of two modules, a global
feature extraction module and a point transformation module, FPT does not
assume explicit constraints based on point vicinity, thereby overcoming a
common requirement of previous learning-based point-set registration methods.
FPT is designed to accept unordered and unstructured point-sets with a variable
number of points and uses a "model-free" approach without heuristic
constraints. Training FPT is flexible and involves minimizing an intuitive
unsupervised loss function, but supervised, semi-supervised, and partially- or
weakly-supervised training are also supported. This flexibility makes FPT
amenable to multimodal image registration problems where the ground-truth
deformations are difficult or impossible to measure. In this paper, we
demonstrate the application of FPT to non-rigid registration of prostate
magnetic resonance (MR) imaging and sparsely-sampled transrectal ultrasound
(TRUS) images. The registration errors were 4.71 mm and 4.81 mm for complete
TRUS imaging and sparsely-sampled TRUS imaging, respectively. The results
indicate superior accuracy to the alternative rigid and non-rigid registration
algorithms tested and substantially lower computation time. The rapid inference
possible with FPT makes it particularly suitable for applications where
real-time registration is beneficial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Team NeuroPoly: Description of the Pipelines for the MICCAI 2021 MS New Lesions Segmentation Challenge. (arXiv:2109.05409v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05409">
<div class="article-summary-box-inner">
<span><p>This paper gives a detailed description of the pipelines used for the 2nd
edition of the MICCAI 2021 Challenge on Multiple Sclerosis Lesion Segmentation.
An overview of the data preprocessing steps applied is provided along with a
brief description of the pipelines used, in terms of the architecture and the
hyperparameters. Our code for this work can be found at:
https://github.com/ivadomed/ms-challenge-2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MovieCuts: A New Dataset and Benchmark for Cut Type Recognition. (arXiv:2109.05569v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05569">
<div class="article-summary-box-inner">
<span><p>Understanding movies and their structural patterns is a crucial task to
decode the craft of video editing. While previous works have developed tools
for general analysis such as detecting characters or recognizing cinematography
properties at the shot level, less effort has been devoted to understanding the
most basic video edit, the Cut. This paper introduces the cut type recognition
task, which requires modeling of multi-modal information. To ignite research in
the new task, we construct a large-scale dataset called MovieCuts, which
contains more than 170K videoclips labeled among ten cut types. We benchmark a
series of audio-visual approaches, including some that deal with the problem's
multi-modal and multi-label nature. Our best model achieves 45.7% mAP, which
suggests that the task is challenging and that attaining highly accurate cut
type recognition is an open research problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised domain adaptation for cross-modality liver segmentation via joint adversarial learning and self-learning. (arXiv:2109.05664v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05664">
<div class="article-summary-box-inner">
<span><p>Liver segmentation on images acquired using computed tomography (CT) and
magnetic resonance imaging (MRI) plays an important role in clinical management
of liver diseases. Compared to MRI, CT images of liver are more abundant and
readily available. However, MRI can provide richer quantitative information of
the liver compared to CT. Thus, it is desirable to achieve unsupervised domain
adaptation for transferring the learned knowledge from the source domain
containing labeled CT images to the target domain containing unlabeled MR
images. In this work, we report a novel unsupervised domain adaptation
framework for cross-modality liver segmentation via joint adversarial learning
and self-learning. We propose joint semantic-aware and shape-entropy-aware
adversarial learning with post-situ identification manner to implicitly align
the distribution of task-related features extracted from the target domain with
those from the source domain. In proposed framework, a network is trained with
the above two adversarial losses in an unsupervised manner, and then a mean
completer of pseudo-label generation is employed to produce pseudo-labels to
train the next network (desired model). Additionally, semantic-aware
adversarial learning and two self-learning methods, including pixel-adaptive
mask refinement and student-to-partner learning, are proposed to train the
desired model. To improve the robustness of the desired model, a low-signal
augmentation function is proposed to transform MRI images as the input of the
desired model to handle hard samples. Using the public data sets, our
experiments demonstrated the proposed unsupervised domain adaptation framework
outperformed four supervised learning methods with a Dice score 0.912 plus or
minus 0.037 (mean plus or minus standard deviation).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UMPNet: Universal Manipulation Policy Network for Articulated Objects. (arXiv:2109.05668v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05668">
<div class="article-summary-box-inner">
<span><p>We introduce the Universal Manipulation Policy Network (UMPNet) -- a single
image-based policy network that infers closed-loop action sequences for
manipulating arbitrary articulated objects. To infer a wide range of action
trajectories, the policy supports 6DoF action representation and varying
trajectory length. To handle a diverse set of objects, the policy learns from
objects with different articulation structures and generalizes to unseen
objects or categories. The policy is trained with self-guided exploration
without any human demonstrations, scripted policy, or pre-defined goal
conditions. To support effective multi-step interaction, we introduce a novel
Arrow-of-Time action attribute that indicates whether an action will change the
object state back to the past or forward into the future. With the
Arrow-of-Time inference at each interaction step, the learned policy is able to
select actions that consistently lead towards or away from a given state,
thereby, enabling both effective state exploration and goal-conditioned
manipulation. Video is available at https://youtu.be/KqlvcL9RqKM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-Separated Curve Rendering Network for Efficient and High-Resolution Image Harmonization. (arXiv:2109.05750v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05750">
<div class="article-summary-box-inner">
<span><p>Image harmonization aims to modify the color of the composited region with
respect to the specific background. Previous works model this task as a
pixel-wise image-to-image translation using UNet family structures. However,
the model size and computational cost limit the performability of their models
on edge devices and higher-resolution images. To this end, we propose a novel
spatial-separated curve rendering network (S$^2$CRNet) for efficient and
high-resolution image harmonization for the first time. In S$^2$CRNet, we
firstly extract the spatial-separated embeddings from the thumbnails of the
masked foreground and background individually. Then, we design a curve
rendering module (CRM), which learns and combines the spatial-specific
knowledge using linear layers to generate the parameters of the pixel-wise
curve mapping in the foreground region. Finally, we directly render the
original high-resolution images using the learned color curve. Besides, we also
make two extensions of the proposed framework via the Cascaded-CRM and
Semantic-CRM for cascaded refinement and semantic guidance, respectively.
Experiments show that the proposed method reduces more than 90% parameters
compared with previous methods but still achieves the state-of-the-art
performance on both synthesized iHarmony4 and real-world DIH test set.
Moreover, our method can work smoothly on higher resolution images in real-time
which is more than 10$\times$ faster than the existing methods. The code and
pre-trained models will be made available and released at
https://github.com/stefanLeong/S2CRNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation. (arXiv:2109.06274v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06274">
<div class="article-summary-box-inner">
<span><p>Automatic methods to segment the vestibular schwannoma (VS) tumors and the
cochlea from magnetic resonance imaging (MRI) are critical to VS treatment
planning. Although supervised methods have achieved satisfactory performance in
VS segmentation, they require full annotations by experts, which is laborious
and time-consuming. In this work, we aim to tackle the VS and cochlea
segmentation problem in an unsupervised domain adaptation setting. Our proposed
method leverages both the image-level domain alignment to minimize the domain
divergence and semi-supervised training to further boost the performance.
Furthermore, we propose to fuse the labels predicted from multiple models via
noisy label correction. Our results on the challenge validation leaderboard
showed that our unsupervised method has achieved promising VS and cochlea
segmentation performance with mean dice score of 0.8261 $\pm$ 0.0416; The mean
dice value for the tumor is 0.8302 $\pm$ 0.0772. This is comparable to the
weakly-supervised based method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Local-Global Transformer for Image Dehazing. (arXiv:2109.07100v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07100">
<div class="article-summary-box-inner">
<span><p>Recently, the Vision Transformer (ViT) has shown impressive performance on
high-level and low-level vision tasks. In this paper, we propose a new ViT
architecture, named Hybrid Local-Global Vision Transformer (HyLoG-ViT), for
single image dehazing. The HyLoG-ViT block consists of two paths, the local ViT
path and the global ViT path, which are used to capture local and global
dependencies. The hybrid features are fused via convolution layers. As a
result, the HyLoG-ViT reduces the computational complexity and introduces
locality in the networks. Then, the HyLoG-ViT blocks are incorporated within
our dehazing networks, which jointly learn the intrinsic image decomposition
and image dehazing. Specifically, the network consists of one shared encoder
and three decoders for reflectance prediction, shading prediction, and
haze-free image generation. The tasks of reflectance and shading prediction can
produce meaningful intermediate features that can serve as complementary
features for haze-free image generation. To effectively aggregate the
complementary features, we propose a complementary features selection module
(CFSM) to select the useful ones for image dehazing. Extensive experiments on
homogeneous, non-homogeneous, and nighttime dehazing tasks reveal that our
proposed Transformer-based dehazing network can achieve comparable or even
better performance than CNNs-based dehazing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">New Perspective on Progressive GANs Distillation for One-class Novelty Detection. (arXiv:2109.07295v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07295">
<div class="article-summary-box-inner">
<span><p>One-class novelty detection is conducted to identify anomalous instances,
with different distributions from the expected normal instances. In this paper,
the Generative Adversarial Network based on the Encoder-Decoder-Encoder scheme
(EDE-GAN) achieves state-of-the-art performance. The two factors bellow serve
the above purpose: 1) The EDE-GAN calculates the distance between two latent
vectors as the anomaly score, which is unlike the previous methods by utilizing
the reconstruction error between images. 2) The model obtains best results when
the batch size is set to 1. To illustrate their superiority, we design a new
GAN architecture, and compare performances according to different batch sizes.
Moreover, with experimentation leads to discovery, our result implies there is
also evidence of just how beneficial constraint on the latent space are when
engaging in model training. In an attempt to learn compact and fast models, we
present a new technology, Progressive Knowledge Distillation with GANs
(P-KDGAN), which connects two standard GANs through the designed distillation
loss. Two-step progressive learning continuously augments the performance of
student GANs with improved results over single-step approach. Our experimental
results on CIFAR-10, MNIST, and FMNIST datasets illustrate that P-KDGAN
improves the performance of the student GAN by 2.44%, 1.77%, and 1.73% when
compressing the computationat ratios of 24.45:1, 311.11:1, and 700:1,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Assignment Distillation for Object Detection. (arXiv:2109.07843v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07843">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation methods are proved to be promising in improving the
performance of neural networks and no additional computational expenses are
required during the inference time. For the sake of boosting the accuracy of
object detection, a great number of knowledge distillation methods have been
proposed particularly designed for object detection. However, most of these
methods only focus on feature-level distillation and label-level distillation,
leaving the label assignment step, a unique and paramount procedure for object
detection, by the wayside. In this work, we come up with a simple but effective
knowledge distillation approach focusing on label assignment in object
detection, in which the positive and negative samples of student network are
selected in accordance with the predictions of teacher network. Our method
shows encouraging results on the MSCOCO2017 benchmark, and can not only be
applied to both one-stage detectors and two-stage detectors but also be
utilized orthogonally with other knowledge distillation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations. (arXiv:2109.07991v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07991">
<div class="article-summary-box-inner">
<span><p>Multisensory object-centric perception, reasoning, and interaction have been
a key research topic in recent years. However, the progress in these directions
is limited by the small set of objects available -- synthetic objects are not
realistic enough and are mostly centered around geometry, while real object
datasets such as YCB are often practically challenging and unstable to acquire
due to international shipping, inventory, and financial cost. We present
ObjectFolder, a dataset of 100 virtualized objects that addresses both
challenges with two key innovations. First, ObjectFolder encodes the visual,
auditory, and tactile sensory data for all objects, enabling a number of
multisensory object recognition tasks, beyond existing datasets that focus
purely on object geometry. Second, ObjectFolder employs a uniform,
object-centric, and implicit representation for each object's visual textures,
acoustic simulations, and tactile readings, making the dataset flexible to use
and easy to share. We demonstrate the usefulness of our dataset as a testbed
for multisensory perception and control by evaluating it on a variety of
benchmark tasks, including instance recognition, cross-sensory retrieval, 3D
reconstruction, and robotic grasping.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Hierarchical Dual Consistency for Semi-Supervised Left Atrium Segmentation on Cross-Domain Data. (arXiv:2109.08311v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08311">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning provides great significance in left atrium (LA)
segmentation model learning with insufficient labelled data. Generalising
semi-supervised learning to cross-domain data is of high importance to further
improve model robustness. However, the widely existing distribution difference
and sample mismatch between different data domains hinder the generalisation of
semi-supervised learning. In this study, we alleviate these problems by
proposing an Adaptive Hierarchical Dual Consistency (AHDC) for the
semi-supervised LA segmentation on cross-domain data. The AHDC mainly consists
of a Bidirectional Adversarial Inference module (BAI) and a Hierarchical Dual
Consistency learning module (HDC). The BAI overcomes the difference of
distributions and the sample mismatch between two different domains. It mainly
learns two mapping networks adversarially to obtain two matched domains through
mutual adaptation. The HDC investigates a hierarchical dual learning paradigm
for cross-domain semi-supervised segmentation based on the obtained matched
domains. It mainly builds two dual-modelling networks for mining the
complementary information in both intra-domain and inter-domain. For the
intra-domain learning, a consistency constraint is applied to the
dual-modelling targets to exploit the complementary modelling information. For
the inter-domain learning, a consistency constraint is applied to the LAs
modelled by two dual-modelling networks to exploit the complementary knowledge
among different data domains. We demonstrated the performance of our proposed
AHDC on four 3D late gadolinium enhancement cardiac MR (LGE-CMR) datasets from
different centres and a 3D CT dataset. Compared to other state-of-the-art
methods, our proposed AHDC achieved higher segmentation accuracy, which
indicated its capability in the cross-domain semi-supervised LA segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Neural Architecture Search for Imbalanced Datasets. (arXiv:2109.08580v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08580">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) provides state-of-the-art results when
trained on well-curated datasets with annotated labels. However, annotating
data or even having balanced number of samples can be a luxury for
practitioners from different scientific fields, e.g., in the medical domain. To
that end, we propose a NAS-based framework that bears the threefold
contributions: (a) we focus on the self-supervised scenario, i.e., where no
labels are required to determine the architecture, and (b) we assume the
datasets are imbalanced, (c) we design each component to be able to run on a
resource constrained setup, i.e., on a single GPU (e.g. Google Colab). Our
components build on top of recent developments in self-supervised
learning~\citep{zbontar2021barlow}, self-supervised NAS~\citep{kaplan2020self}
and extend them for the case of imbalanced datasets. We conduct experiments on
an (artificially) imbalanced version of CIFAR-10 and we demonstrate our
proposed method outperforms standard neural networks, while using $27\times$
less parameters. To validate our assumption on a naturally imbalanced dataset,
we also conduct experiments on ChestMNIST and COVID-19 X-ray. The results
demonstrate how the proposed method can be used in imbalanced datasets, while
it can be fully run on a single GPU. Code is available
\href{https://github.com/TimofeevAlex/ssnas_imbalanced}{here}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hebbian Semi-Supervised Learning in a Sample Efficiency Setting. (arXiv:2103.09002v2 [cs.NE] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09002">
<div class="article-summary-box-inner">
<span><p>We propose to address the issue of sample efficiency, in Deep Convolutional
Neural Networks (DCNN), with a semi-supervised training strategy that combines
Hebbian learning with gradient descent: all internal layers (both convolutional
and fully connected) are pre-trained using an unsupervised approach based on
Hebbian learning, and the last fully connected layer (the classification layer)
is trained using Stochastic Gradient Descent (SGD). In fact, as Hebbian
learning is an unsupervised learning method, its potential lies in the
possibility of training the internal layers of a DCNN without labels. Only the
final fully connected layer has to be trained with labeled examples.
</p>
<p>We performed experiments on various object recognition datasets, in different
regimes of sample efficiency, comparing our semi-supervised (Hebbian for
internal layers + SGD for the final fully connected layer) approach with
end-to-end supervised backprop training, and with semi-supervised learning
based on Variational Auto-Encoder (VAE). The results show that, in regimes
where the number of available labeled samples is low, our semi-supervised
approach outperforms the other approaches in almost all the cases.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-21 23:02:28.779381420 UTC">2021-09-21 23:02:28 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>