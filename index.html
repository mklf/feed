<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-28T01:30:00Z">01-28</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v1 [q-bio.BM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11147">
<div class="article-summary-box-inner">
<span><p>Self-supervised protein language models have proved their effectiveness in
learning the proteins representations. With the increasing computational power,
current protein language models pre-trained with millions of diverse sequences
can advance the parameter scale from million-level to billion-level and achieve
remarkable improvement. However, those prevailing approaches rarely consider
incorporating knowledge graphs (KGs), which can provide rich structured
knowledge facts for better protein representations. We argue that informative
biology knowledge in KGs can enhance protein representation with external
knowledge. In this work, we propose OntoProtein, the first general framework
that makes use of structure in GO (Gene Ontology) into protein pre-training
models. We construct a novel large-scale knowledge graph that consists of GO
and its related proteins, and gene annotation texts or protein sequences
describe all nodes in the graph. We propose novel contrastive learning with
knowledge-aware negative sampling to jointly optimize the knowledge graph and
protein embedding during pre-training. Experimental results show that
OntoProtein can surpass state-of-the-art methods with pre-trained protein
language models in TAPE benchmark and yield better performance compared with
baselines in protein-protein interaction and protein function prediction. Code
and datasets are available in https://github.com/zjunlp/OntoProtein.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Addressing Issues of Cross-Linguality in Open-Retrieval Question Answering Systems For Emergent Domains. (arXiv:2201.11153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11153">
<div class="article-summary-box-inner">
<span><p>Open-retrieval question answering systems are generally trained and tested on
large datasets in well-established domains. However, low-resource settings such
as new and emerging domains would especially benefit from reliable question
answering systems. Furthermore, multilingual and cross-lingual resources in
emergent domains are scarce, leading to few or no such systems. In this paper,
we demonstrate a cross-lingual open-retrieval question answering system for the
emergent domain of COVID-19. Our system adopts a corpus of scientific articles
to ensure that retrieved documents are reliable. To address the scarcity of
cross-lingual training data in emergent domains, we present a method utilizing
automatic translation, alignment, and filtering to produce English-to-all
datasets. We show that a deep semantic retriever greatly benefits from training
on our English-to-all data and significantly outperforms a BM25 baseline in the
cross-lingual setting. We illustrate the capabilities of our system with
examples and release all code necessary to train and deploy such a system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Patterns for Distinction and Prediction of Moral Judgement on Reddit. (arXiv:2201.11155v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11155">
<div class="article-summary-box-inner">
<span><p>The forum r/AmITheAsshole in Reddit hosts discussion on moral issues based on
concrete narratives presented by users. Existing analysis of the forum focuses
on its comments, and does not make the underlying data publicly available. In
this paper we build a new dataset of comments and also investigate the
classification of the posts in the forum. Further, we identify textual patterns
associated with the provocation of moral judgement by posts, with the
expression of moral stance in comments, and with the decisions of trained
classifiers of posts and comments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling data scarcity in speech translation using zero-shot multilingual machine translation techniques. (arXiv:2201.11172v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11172">
<div class="article-summary-box-inner">
<span><p>Recently, end-to-end speech translation (ST) has gained significant attention
as it avoids error propagation. However, the approach suffers from data
scarcity. It heavily depends on direct ST data and is less efficient in making
use of speech transcription and text translation data, which is often more
easily available. In the related field of multilingual text translation,
several techniques have been proposed for zero-shot translation. A main idea is
to increase the similarity of semantically similar sentences in different
languages. We investigate whether these ideas can be applied to speech
translation, by building ST models trained on speech transcription and text
translation data. We investigate the effects of data augmentation and auxiliary
loss function. The techniques were successfully applied to few-shot ST using
limited ST data, with improvements of up to +12.9 BLEU points compared to
direct end-to-end ST and +3.1 BLEU points compared to ST models fine-tuned from
ASR model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence. (arXiv:2201.11176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11176">
<div class="article-summary-box-inner">
<span><p>Recently has there been a growing interest in the creation of text generation
systems from a discourse coherence perspective, e.g., modeling the
interdependence between sentences. Still, recent BERT-based evaluation metrics
cannot recognize coherence and fail to punish incoherent elements in system
outputs. In this work, we introduce DiscoScore, a discourse metric with
multiple variants, which uses BERT to model discourse coherence from different
perspectives, driven by Centering theory. Our experiments encompass 16
non-discourse and discourse metrics, including DiscoScore and popular coherence
models, evaluated on summarization and document-level machine translation (MT).
We find that (i) the majority of BERT-based metrics correlate much worse with
human rated coherence than early discourse metrics, invented a decade ago; (ii)
the recent state-of-the-art BARTScore is weak when operated at system level --
which is particularly problematic as systems are typically compared in this
manner. DiscoScore, in contrast, achieves strong system-level correlation with
human ratings, not only in coherence but also in factual consistency and other
aspects, and surpasses BARTScore by over 10 correlation points on average.
Further, aiming to understand DiscoScore, we provide justifications to the
importance of discourse coherence for evaluation metrics, and explain the
superiority of one variant over another. Our code is available at
\url{https://github.com/AIPHES/DiscoScore}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Phonetic Inventories with Crosslingual Automatic Speech Recognition. (arXiv:2201.11207v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11207">
<div class="article-summary-box-inner">
<span><p>The high cost of data acquisition makes Automatic Speech Recognition (ASR)
model training problematic for most existing languages, including languages
that do not even have a written script, or for which the phone inventories
remain unknown. Past works explored multilingual training, transfer learning,
as well as zero-shot learning in order to build ASR systems for these
low-resource languages. While it has been shown that the pooling of resources
from multiple languages is helpful, we have not yet seen a successful
application of an ASR model to a language unseen during training. A crucial
step in the adaptation of ASR from seen to unseen languages is the creation of
the phone inventory of the unseen language. The ultimate goal of our work is to
build the phone inventory of a language unseen during training in an
unsupervised way without any knowledge about the language. In this paper, we 1)
investigate the influence of different factors (i.e., model architecture,
phonotactic model, type of speech representation) on phone recognition in an
unknown language; 2) provide an analysis of which phones transfer well across
languages and which do not in order to understand the limitations of and areas
for further improvement for automatic phone inventory creation; and 3) present
different methods to build a phone inventory of an unseen language in an
unsupervised way. To that end, we conducted mono-, multi-, and crosslingual
experiments on a set of 13 phonetically diverse languages and several in-depth
analyses. We found a number of universal phone tokens (IPA symbols) that are
well-recognized cross-linguistically. Through a detailed analysis of results,
we conclude that unique sounds, similar sounds, and tone languages remain a
major challenge for phonetic inventory discovery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Learning Knowledge Embedding and Neighborhood Consensus with Relational Knowledge Distillation for Entity Alignment. (arXiv:2201.11249v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11249">
<div class="article-summary-box-inner">
<span><p>Entity alignment aims at integrating heterogeneous knowledge from different
knowledge graphs. Recent studies employ embedding-based methods by first
learning the representation of Knowledge Graphs and then performing entity
alignment via measuring the similarity between entity embeddings. However, they
failed to make good use of the relation semantic information due to the
trade-off problem caused by the different objectives of learning knowledge
embedding and neighborhood consensus. To address this problem, we propose
Relational Knowledge Distillation for Entity Alignment (RKDEA), a Graph
Convolutional Network (GCN) based model equipped with knowledge distillation
for entity alignment. We adopt GCN-based models to learn the representation of
entities by considering the graph structure and incorporating the relation
semantic information into GCN via knowledge distillation. Then, we introduce a
novel adaptive mechanism to transfer relational knowledge so as to jointly
learn entity embedding and neighborhood consensus. Experimental results on
several benchmarking datasets demonstrate the effectiveness of our proposed
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning How to Translate North Korean through South Korean. (arXiv:2201.11258v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11258">
<div class="article-summary-box-inner">
<span><p>South and North Korea both use the Korean language. However, Korean NLP
research has focused on South Korean only, and existing NLP systems of the
Korean language, such as neural machine translation (NMT) models, cannot
properly handle North Korean inputs. Training a model using North Korean data
is the most straightforward approach to solving this problem, but there is
insufficient data to train NMT models. In this study, we create data for North
Korean NMT models using a comparable corpus. First, we manually create
evaluation data for automatic alignment and machine translation. Then, we
investigate automatic alignment methods suitable for North Korean. Finally, we
verify that a model trained by North Korean bilingual data without human
annotation can significantly boost North Korean translation accuracy compared
to existing South Korean models in zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highly Generalizable Models for Multilingual Hate Speech Detection. (arXiv:2201.11294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11294">
<div class="article-summary-box-inner">
<span><p>Hate speech detection has become an important research topic within the past
decade. More private corporations are needing to regulate user generated
content on different platforms across the globe. In this paper, we introduce a
study of multilingual hate speech classification. We compile a dataset of 11
languages and resolve different taxonomies by analyzing the combined data with
binary labels: hate speech or not hate speech. Defining hate speech in a single
way across different languages and datasets may erase cultural nuances to the
definition, therefore, we utilize language agnostic embeddings provided by
LASER and MUSE in order to develop models that can use a generalized definition
of hate speech across datasets. Furthermore, we evaluate prior state of the art
methodologies for hate speech detection under our expanded dataset. We conduct
three types of experiments for a binary hate speech classification task:
Multilingual-Train Monolingual-Test, MonolingualTrain Monolingual-Test and
Language-Family-Train Monolingual Test scenarios to see if performance
increases for each language due to learning more from other language data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Higher-Order Semantic Dependency Parser. (arXiv:2201.11312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11312">
<div class="article-summary-box-inner">
<span><p>Higher-order features bring significant accuracy gains in semantic dependency
parsing. However, modeling higher-order features with exact inference is
NP-hard. Graph neural networks (GNNs) have been demonstrated to be an effective
tool for solving NP-hard problems with approximate inference in many graph
learning tasks. Inspired by the success of GNNs, we investigate building a
higher-order semantic dependency parser by applying GNNs. Instead of explicitly
extracting higher-order features from intermediate parsing graphs, GNNs
aggregate higher-order information concisely by stacking multiple GNN layers.
Experimental results show that our model outperforms the previous
state-of-the-art parser on the SemEval 2015 Task 18 English datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Deep Semantic Model for Code Search using CodeSearchNet Corpus. (arXiv:2201.11313v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11313">
<div class="article-summary-box-inner">
<span><p>Semantic code search is the task of retrieving relevant code snippet given a
natural language query. Different from typical information retrieval tasks,
code search requires to bridge the semantic gap between the programming
language and natural language, for better describing intrinsic concepts and
semantics. Recently, deep neural network for code search has been a hot
research topic. Typical methods for neural code search first represent the code
snippet and query text as separate embeddings, and then use vector distance
(e.g. dot-product or cosine) to calculate the semantic similarity between them.
There exist many different ways for aggregating the variable length of code or
query tokens into a learnable embedding, including bi-encoder, cross-encoder,
and poly-encoder. The goal of the query encoder and code encoder is to produce
embeddings that are close with each other for a related pair of query and the
corresponding desired code snippet, in which the choice and design of encoder
is very significant.
</p>
<p>In this paper, we propose a novel deep semantic model which makes use of the
utilities of not only the multi-modal sources, but also feature extractors such
as self-attention, the aggregated vectors, combination of the intermediate
representations. We apply the proposed model to tackle the CodeSearchNet
challenge about semantic code search. We align cross-lingual embedding for
multi-modality learning with large batches and hard example mining, and combine
different learned representations for better enhancing the representation
learning. Our model is trained on CodeSearchNet corpus and evaluated on the
held-out data, the final model achieves 0.384 NDCG and won the first place in
this benchmark. Models and code are available at
https://github.com/overwindows/SemanticCodeSearch.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ontology-enhanced Prompt-tuning for Few-shot Learning. (arXiv:2201.11332v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11332">
<div class="article-summary-box-inner">
<span><p>Few-shot Learning (FSL) is aimed to make predictions based on a limited
number of samples. Structured data such as knowledge graphs and ontology
libraries has been leveraged to benefit the few-shot setting in various tasks.
However, the priors adopted by the existing methods suffer from challenging
knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder
the performance for few-shot learning. In this study, we explore knowledge
injection for FSL with pre-trained language models and propose
ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the
ontology transformation based on the external knowledge graph to address the
knowledge missing issue, which fulfills and converts structure knowledge to
text. We further introduce span-sensitive knowledge injection via a visible
matrix to select informative knowledge to handle the knowledge noise issue. To
bridge the gap between knowledge and text, we propose a collective training
algorithm to optimize representations jointly. We evaluate our proposed
OntoPrompt in three tasks, including relation extraction, event extraction, and
knowledge graph completion, with eight datasets. Experimental results
demonstrate that our approach can obtain better few-shot performance than
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pan More Gold from the Sand: Refining Open-domain Dialogue Training with Noisy Self-Retrieval Generation. (arXiv:2201.11367v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11367">
<div class="article-summary-box-inner">
<span><p>Real human conversation data are complicated, heterogeneous, and noisy, from
whom building open-domain dialogue systems remains a challenging task. In fact,
such dialogue data can still contain a wealth of information and knowledge,
however, they are not fully explored. In this paper, we show existing
open-domain dialogue generation methods by memorizing context-response paired
data with causal or encode-decode language models underutilize the training
data. Different from current approaches, using external knowledge, we explore a
retrieval-generation training framework that can increase the usage of training
data by directly considering the heterogeneous and noisy training data as the
"evidence". Experiments over publicly available datasets demonstrate that our
method can help models generate better responses, even such training data are
usually impressed as low-quality data. Such performance gain is comparable with
those improved by enlarging the training set, even better. We also found that
the model performance has a positive correlation with the relevance of the
retrieved evidence. Moreover, our method performed well on zero-shot
experiments, which indicates that our method can be more robust to real-world
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Systematic Investigation of Strategies Tailored for Low-Resource Settings for Sanskrit Dependency Parsing. (arXiv:2201.11374v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11374">
<div class="article-summary-box-inner">
<span><p>Existing state of the art approaches for Sanskrit Dependency Parsing (SDP),
are hybrid in nature, and rely on a lexicon-driven shallow parser for
linguistically motivated feature engineering. However, these methods fail to
handle out of vocabulary (OOV) words, which limits their applicability in
realistic scenarios. On the other hand, purely data-driven approaches do not
match the performance of hybrid approaches due to the labelled data sparsity.
Thus, in this work, we investigate the following question: How far can we push
a purely data-driven approach using recently proposed strategies for
low-resource settings? We experiment with five strategies, namely, data
augmentation, sequential transfer learning, cross-lingual/mono-lingual
pretraining, multi-task learning and self-training. Our proposed ensembled
system outperforms the purely data-driven state of the art system by 2.8/3.9
points (Unlabelled Attachment Score (UAS)/Labelled Attachment Score (LAS))
absolute gain. Interestingly, it also supersedes the performance of the state
of the art hybrid system by 1.2 points (UAS) absolute gain and shows comparable
performance in terms of LAS. Code and data will be publicly available at:
\url{https://github.com/Jivnesh/SanDP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prabhupadavani: A Code-mixed Speech Translation Data for 25 Languages. (arXiv:2201.11391v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11391">
<div class="article-summary-box-inner">
<span><p>Nowadays, code-mixing has become ubiquitous in Natural Language Processing
(NLP); however, no efforts have been made to address this phenomenon for Speech
Translation (ST) task. This can be solely attributed to the lack of code-mixed
ST task labelled data. Thus, we introduce Prabhupadavani, a multilingual
code-mixed ST dataset for 25 languages, covering ten language families,
containing 94 hours of speech by 130+ speakers, manually aligned with
corresponding text in the target language. Prabhupadvani is the first
code-mixed ST dataset available in the ST literature to the best of our
knowledge. This data also can be used for a code-mixed machine translation
task. All the dataset and code can be accessed at:
\url{https://github.com/frozentoad9/CMST}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Yes-Yes-Yes: Donation-based Peer Reviewing Data Collection for ACL Rolling Review and Beyond. (arXiv:2201.11443v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11443">
<div class="article-summary-box-inner">
<span><p>Peer review is the primary gatekeeper of scientific merit and quality, yet it
is prone to bias and suffers from low efficiency. This demands
cross-disciplinary scrutiny of the processes that underlie peer reviewing;
however, quantitative research is limited by the data availability, as most of
the peer reviewing data across research disciplines is never made public.
Existing data collection efforts focus on few scientific domains and do not
address a range of ethical, license- and confidentiality-related issues
associated with peer reviewing data, preventing wide-scale research and
application development. While recent methods for peer review analysis and
processing show promise, a solid data foundation for computational research in
peer review is still missing. To address this, we present an in-depth
discussion of peer reviewing data, outline the ethical and legal desiderata for
peer reviewing data collection, and propose the first continuous,
donation-based data collection workflow that meets these requirements. We
report on the ongoing implementation of this workflow at the ACL Rolling Review
and deliver the first insights obtained with the newly collected data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Like Program Executors. (arXiv:2201.11473v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11473">
<div class="article-summary-box-inner">
<span><p>Reasoning over natural language is a long-standing goal for the research
community. However, studies have shown that existing language models are
inadequate in reasoning. To address the issue, we present POET, a new
pre-training paradigm. Through pre-training language models with programs and
their execution results, POET empowers language models to harvest the reasoning
knowledge possessed in program executors via a data-driven approach. POET is
conceptually simple and can be instantiated by different kinds of programs. In
this paper, we show three empirically powerful instances, i.e., POET-Math,
POET-Logic, and POET-SQL. Experimental results on six benchmarks demonstrate
that POET can significantly boost model performance on natural language
reasoning, such as numerical reasoning, logical reasoning, and multi-hop
reasoning. Taking the DROP benchmark as a representative example, POET improves
the F1 metric of BART from 69.2% to 80.6%. Furthermore, POET shines in giant
language models, pushing the F1 metric of T5-11B to 87.6% and achieving a new
state-of-the-art performance on DROP. POET opens a new gate on
reasoning-enhancement pre-training and we hope our analysis would shed light on
the future research of reasoning like program executors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Interpretation of Saliency-based Explanation Over Text. (arXiv:2201.11569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11569">
<div class="article-summary-box-inner">
<span><p>While a lot of research in explainable AI focuses on producing effective
explanations, less work is devoted to the question of how people understand and
interpret the explanation. In this work, we focus on this question through a
study of saliency-based explanations over textual data. Feature-attribution
explanations of text models aim to communicate which parts of the input text
were more influential than others towards the model decision. Many current
explanation methods, such as gradient-based or Shapley value-based methods,
provide measures of importance which are well-understood mathematically. But
how does a person receiving the explanation (the explainee) comprehend it? And
does their understanding match what the explanation attempted to communicate?
We empirically investigate the effect of various factors of the input, the
feature-attribution explanation, and visualization procedure, on laypeople's
interpretation of the explanation. We query crowdworkers for their
interpretation on tasks in English and German, and fit a GAMM model to their
responses considering the factors of interest. We find that people often
mis-interpret the explanations: superficial and unrelated factors, such as word
length, influence the explainees' importance assignment despite the explanation
communicating importance directly. We then show that some of this distortion
can be attenuated: we propose a method to adjust saliencies based on model
estimates of over- and under-perception, and explore bar charts as an
alternative to heatmap saliency visualization. We find that both approaches can
attenuate the distorting effect of specific factors, leading to
better-calibrated understanding of the explanation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesizing Dysarthric Speech Using Multi-talker TTS for Dysarthric Speech Recognition. (arXiv:2201.11571v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11571">
<div class="article-summary-box-inner">
<span><p>Dysarthria is a motor speech disorder often characterized by reduced speech
intelligibility through slow, uncoordinated control of speech production
muscles. Automatic Speech recognition (ASR) systems may help dysarthric talkers
communicate more effectively. To have robust dysarthria-specific ASR,
sufficient training speech is required, which is not readily available. Recent
advances in Text-To-Speech (TTS) synthesis multi-speaker end-to-end TTS systems
suggest the possibility of using synthesis for data augmentation. In this
paper, we aim to improve multi-speaker end-to-end TTS systems to synthesize
dysarthric speech for improved training of a dysarthria-specific DNN-HMM ASR.
In the synthesized speech, we add dysarthria severity level and pause insertion
mechanisms to other control parameters such as pitch, energy, and duration.
Results show that a DNN-HMM model trained on additional synthetic dysarthric
speech achieves WER improvement of 12.2% compared to the baseline, the addition
of the severity level and pause insertion controls decrease WER by 6.5%,
showing the effectiveness of adding these parameters. Audio samples are
available at
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation. (arXiv:2201.11576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11576">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models (LMs) like BERT have improved performance in
many disparate natural language processing (NLP) tasks. However, fine tuning
such models requires a large number of training examples for each target task.
Simultaneously, many realistic NLP problems are "few shot", without a
sufficiently large training set. In this work, we propose a novel conditional
neural process-based approach for few-shot text classification that learns to
transfer from other diverse tasks with rich annotation. Our key idea is to
represent each task using gradient information from a base model and to train
an adaptation network that modulates a text classifier conditioned on the task
representation. While previous task-aware few-shot learners represent tasks by
input encoding, our novel task representation is more powerful, as the gradient
captures input-output relationships of a task. Experimental results show that
our approach outperforms traditional fine-tuning, sequential transfer learning,
and state-of-the-art meta learning approaches on a collection of diverse
few-shot tasks. We further conducted analysis and ablations to justify our
design choices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GUDN A novel guide network for extreme multi-label text classification. (arXiv:2201.11582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11582">
<div class="article-summary-box-inner">
<span><p>The problem of extreme multi-label text classification (XMTC) is to recall
some most relevant labels for a text from an extremely large label set. Though
the methods based on deep pre-trained models have reached significant
achievement, the pre-trained models are still not fully utilized. Label
semantics has not attracted much attention so far, and the latent space between
texts and labels has not been effectively explored. This paper constructs a
novel guide network (GUDN) to help fine-tune the pre-trained model to instruct
classification later. Also, we use the raw label semantics to effectively
explore the latent space between texts and labels, which can further improve
predicted accuracy. Experimental results demonstrate that GUDN outperforms
state-of-the-art methods on several popular datasets. Our source code is
released at https://github.com/wq2581/GUDN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages. (arXiv:2201.11732v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11732">
<div class="article-summary-box-inner">
<span><p>Reliable evaluation benchmarks designed for replicability and
comprehensiveness have driven progress in machine learning. Due to the lack of
a multilingual benchmark, however, vision-and-language research has mostly
focused on English language tasks. To fill this gap, we introduce the
Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings
together - by both aggregating pre-existing datasets and creating new ones -
visual question answering, cross-modal retrieval, grounded reasoning, and
grounded entailment tasks across 20 diverse languages. Our benchmark enables
the evaluation of multilingual multimodal models for transfer learning, not
only in a zero-shot setting, but also in newly defined few-shot learning
setups. Based on the evaluation of the available state-of-the-art models, we
find that translate-test transfer is superior to zero-shot transfer and that
few-shot learning is hard to harness for many tasks. Moreover, downstream
performance is partially explained by the amount of available unlabelled
textual data for pretraining, and only weakly by the typological distance of
target-source languages. We hope to encourage future research efforts in this
area by releasing the benchmark to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Named Entity Recognition and Relation Extraction using Enhanced Table Filling by Contextualized Representations. (arXiv:2010.07522v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07522">
<div class="article-summary-box-inner">
<span><p>In this study, a novel method for extracting named entities and relations
from unstructured text based on the table representation is presented. By using
contextualized word embeddings, the proposed method computes representations
for entity mentions and long-range dependencies without complicated
hand-crafted features or neural-network architectures. We also adapt a tensor
dot-product to predict relation labels all at once without resorting to
history-based predictions or search strategies. These advances significantly
simplify the model and algorithm for the extraction of named entities and
relations. Despite its simplicity, the experimental results demonstrate that
the proposed method outperforms the state-of-the-art methods on the CoNLL04 and
ACE05 English datasets. We also confirm that the proposed method achieves a
comparable performance with the state-of-the-art NER models on the ACE05
datasets when multiple sentences are provided for context aggregation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains. (arXiv:2102.12206v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12206">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing algorithms have made incredible progress, but
they still struggle when applied to out-of-distribution examples. We address a
challenging and underexplored version of this domain adaptation problem, where
an algorithm is trained on several source domains, and then applied to examples
from unseen domains that are unknown at training time. Particularly, no
examples, labeled or unlabeled, or any other knowledge about the target domain
are available to the algorithm at training time. We present PADA: An
example-based autoregressive Prompt learning algorithm for on-the-fly
Any-Domain Adaptation, based on the T5 language model. Given a test example,
PADA first generates a unique prompt for it and then, conditioned on this
prompt, labels the example with respect to the NLP prediction task. PADA is
trained to generate a prompt which is a token sequence of unrestricted length,
consisting of Domain Related Features (DRFs) that characterize each of the
source domains. Intuitively, the generated prompt is a unique signature that
maps the test example to a semantic space spanned by the source domains. In
experiments with 3 tasks (text classification and sequence tagging), for a
total of 14 multi-source adaptation scenarios, PADA substantially outperforms
strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Neighbourhood Framework for Resource-Lean Content Flagging. (arXiv:2103.17055v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17055">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework for cross-lingual content flagging with limited
target-language data, which significantly outperforms prior work in terms of
predictive performance. The framework is based on a nearest-neighbour
architecture. It is a modern instantiation of the vanilla k-nearest neighbour
model, as we use Transformer representations in all its components. Our
framework can adapt to new source-language instances, without the need to be
retrained from scratch. Unlike prior work on neighbourhood-based approaches, we
encode the neighbourhood information based on query--neighbour interactions. We
propose two encoding schemes and we show their effectiveness using both
qualitative and quantitative analysis. Our evaluation results on eight
languages from two different datasets for abusive language detection show
sizable improvements of up to 9.5 F1 points absolute (for Italian) over strong
baselines. On average, we achieve 3.6 absolute F1 points of improvement for the
three languages in the Jigsaw Multilingual dataset and 2.14 points for the WUL
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grover's Algorithm for Question Answering. (arXiv:2106.05299v3 [quant-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05299">
<div class="article-summary-box-inner">
<span><p>Grover's algorithm, a well-know quantum search algorithm, allows one to find
the correct item in a database, with quadratic speedup. In this paper we adapt
Grover's algorithm to the problem of finding a correct answer to a natural
language question in English, thus contributing to the growing field of Quantum
Natural Language Processing. Using a grammar that can be interpreted as tensor
contractions, each word is represented as a quantum state that serves as input
to the quantum circuit. We here introduce a quantum measurement to contract the
representations of words, resulting in the representation of larger text
fragments. Using this framework, a representation for the question is found
that contains all the possible answers in equal quantum superposition, and
allows for the building of an oracle that can detect a correct answer, being
agnostic to the specific question. Furthermore, we show that our construction
can deal with certain types of ambiguous phrases by keeping the various
different meanings in quantum superposition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13161">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have contributed significantly to
natural language processing by demonstrating remarkable abilities as few-shot
learners. However, their effectiveness depends mainly on scaling the model
parameters and prompt design, hindering their implementation in most real-world
applications. This study proposes a novel pluggable, extensible, and efficient
approach named DifferentiAble pRompT (DART), which can convert small language
models into better few-shot learners without any prompt engineering. The main
principle behind this approach involves reformulating potential natural
language processing tasks into the task of a pre-trained language model and
differentially optimizing the prompt template as well as the target label with
backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any
pre-trained language models; (ii) Extended to widespread classification tasks.
A comprehensive evaluation of standard NLP tasks demonstrates that the proposed
approach achieves a better few-shot performance. Code is available in
https://github.com/zjunlp/DART.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Audio Captions Be Evaluated with Image Caption Metrics?. (arXiv:2110.04684v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04684">
<div class="article-summary-box-inner">
<span><p>Automated audio captioning aims at generating textual descriptions for an
audio clip. To evaluate the quality of generated audio captions, previous works
directly adopt image captioning metrics like SPICE and CIDEr, without
justifying their suitability in this new domain, which may mislead the
development of advanced models. This problem is still unstudied due to the lack
of human judgment datasets on caption quality. Therefore, we firstly construct
two evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established
with pairwise comparison instead of absolute rating to achieve better
inter-annotator agreement. Current metrics are found in poor correlation with
human annotations on these datasets. To overcome their limitations, we propose
a metric named FENSE, where we combine the strength of Sentence-BERT in
capturing similarity, and a novel Error Detector to penalize erroneous
sentences for robustness. On the newly established benchmarks, FENSE
outperforms current metrics by 14-25% accuracy. Code, data and web demo
available at: https://github.com/blmoistawinde/fense
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Multi-hop Question Answering over Knowledge Base. (arXiv:2112.11909v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11909">
<div class="article-summary-box-inner">
<span><p>KBQA is a task that requires to answer questions by using semantic structured
information in knowledge base. Previous work in this area has been restricted
due to the lack of large semantic parsing dataset and the exponential growth of
searching space with the increasing hops of relation paths. In this paper, we
propose an efficient pipeline method equipped with a pre-trained language
model. By adopting Beam Search algorithm, the searching space will not be
restricted in subgraph of 3 hops. Besides, we propose a data generation
strategy, which enables our model to generalize well from few training samples.
We evaluate our model on an open-domain complex Chinese Question Answering task
CCKS2019 and achieve F1-score of 62.55% on the test dataset. In addition, in
order to test the few-shot learning capability of our model, we ramdomly select
10% of the primary data to train our model, the result shows that our model can
still achieves F1-score of 58.54%, which verifies the capability of our model
to process KBQA task and the advantage in few-shot Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling. (arXiv:2201.01337v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01337">
<div class="article-summary-box-inner">
<span><p>Traditional text classification approaches often require a good amount of
labeled data, which is difficult to obtain, especially in restricted domains or
less widespread languages. This lack of labeled data has led to the rise of
low-resource methods, that assume low data availability in natural language
processing. Among them, zero-shot learning stands out, which consists of
learning a classifier without any previously labeled data. The best results
reported with this approach use language models such as Transformers, but fall
into two problems: high execution time and inability to handle long texts as
input. This paper proposes a new model, ZeroBERTo, which leverages an
unsupervised clustering step to obtain a compressed data representation before
the classification task. We show that ZeroBERTo has better performance for long
inputs and shorter execution time, outperforming XLM-R by about 12% in the F1
score in the FolhaUOL dataset. Keywords: Low-Resource NLP, Unlabeled data,
Zero-Shot Learning, Topic Modeling, Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks. (arXiv:2201.09745v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09745">
<div class="article-summary-box-inner">
<span><p>Since a vast number of tables can be easily collected from web pages,
spreadsheets, PDFs, and various other document types, a flurry of table
pre-training frameworks have been proposed following the success of text and
images, and they have achieved new state-of-the-arts on various tasks such as
table question answering, table type recognition, column relation
classification, table search, formula prediction, etc. To fully use the
supervision signals in unlabeled tables, a variety of pre-training objectives
have been designed and evaluated, for example, denoising cell values,
predicting numerical relationships, and implicitly executing SQLs. And to best
leverage the characteristics of (semi-)structured tables, various tabular
language models, particularly with specially-designed attention mechanisms,
have been explored. Since tables usually appear and interact with free-form
text, table pre-training usually takes the form of table-text joint
pre-training, which attracts significant research interests from multiple
domains. This survey aims to provide a comprehensive review of different model
designs, pre-training objectives, and downstream tasks for table pre-training,
and we further share our thoughts and vision on existing challenges and future
opportunities.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DIREG3D: DIrectly REGress 3D Hands from Multiple Cameras. (arXiv:2201.11187v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11187">
<div class="article-summary-box-inner">
<span><p>In this paper, we present DIREG3D, a holistic framework for 3D Hand Tracking.
The proposed framework is capable of utilizing camera intrinsic parameters, 3D
geometry, intermediate 2D cues, and visual information to regress parameters
for accurately representing a Hand Mesh model. Our experiments show that
information like the size of the 2D hand, its distance from the optical center,
and radial distortion is useful for deriving highly reliable 3D poses in camera
space from just monocular information. Furthermore, we extend these results to
a multi-view camera setup by fusing features from different viewpoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReforesTree: A Dataset for Estimating Tropical Forest Carbon Stock with Deep Learning and Aerial Imagery. (arXiv:2201.11192v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11192">
<div class="article-summary-box-inner">
<span><p>Forest biomass is a key influence for future climate, and the world urgently
needs highly scalable financing schemes, such as carbon offsetting
certifications, to protect and restore forests. Current manual forest carbon
stock inventory methods of measuring single trees by hand are time, labour, and
cost-intensive and have been shown to be subjective. They can lead to
substantial overestimation of the carbon stock and ultimately distrust in
forest financing. The potential for impact and scale of leveraging advancements
in machine learning and remote sensing technologies is promising but needs to
be of high quality in order to replace the current forest stock protocols for
certifications.
</p>
<p>In this paper, we present ReforesTree, a benchmark dataset of forest carbon
stock in six agro-forestry carbon offsetting sites in Ecuador. Furthermore, we
show that a deep learning-based end-to-end model using individual tree
detection from low cost RGB-only drone imagery is accurately estimating forest
carbon stock within official carbon offsetting certification standards.
Additionally, our baseline CNN model outperforms state-of-the-art
satellite-based forest biomass and carbon stock estimates for this type of
small-scale, tropical agro-forestry sites. We present this dataset to encourage
machine learning research in this area to increase accountability and
transparency of monitoring, verification and reporting (MVR) in carbon
offsetting projects, as well as scaling global reforestation financing through
accurate remote sensing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges and Opportunities for Machine Learning Classification of Behavior and Mental State from Images. (arXiv:2201.11197v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11197">
<div class="article-summary-box-inner">
<span><p>Computer Vision (CV) classifiers which distinguish and detect nonverbal
social human behavior and mental state can aid digital diagnostics and
therapeutics for psychiatry and the behavioral sciences. While CV classifiers
for traditional and structured classification tasks can be developed with
standard machine learning pipelines for supervised learning consisting of data
labeling, preprocessing, and training a convolutional neural network, there are
several pain points which arise when attempting this process for behavioral
phenotyping. Here, we discuss the challenges and corresponding opportunities in
this space, including handling heterogeneous data, avoiding biased models,
labeling massive and repetitive data sets, working with ambiguous or compound
class labels, managing privacy concerns, creating appropriate representations,
and personalizing models. We discuss current state-of-the-art research
endeavors in CV such as data curation, data augmentation, crowdsourced
labeling, active learning, reinforcement learning, generative models,
representation learning, federated learning, and meta-learning. We highlight at
least some of the machine learning advancements needed for imaging classifiers
to detect human social cues successfully and reliably.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Examination by Automatic Quiz Assessment Using Spiral Codes and Image Processing. (arXiv:2201.11228v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11228">
<div class="article-summary-box-inner">
<span><p>We describe a technical solution implemented at Halmstad University to
automatise assessment and reporting of results of paper-based quiz exams. Paper
quizzes are affordable and within reach of campus education in classrooms.
Offering and taking them is accepted as they cause fewer issues with
reliability and democratic access, e.g. a large number of students can take
them without a trusted mobile device, internet, or battery. By contrast,
correction of the quiz is a considerable obstacle. We suggest mitigating the
issue by a novel image processing technique using harmonic spirals that aligns
answer sheets in sub-pixel accuracy to read student identity and answers and to
email results within minutes, all fully automatically. Using the described
method, we carry out regular weekly examinations in two master courses at the
mentioned centre without a significant workload increase. The employed solution
also enables us to assign a unique identifier to each quiz (e.g. week 1, week
2. . . ) while allowing us to have an individualised quiz for each student.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HistoKT: Cross Knowledge Transfer in Computational Pathology. (arXiv:2201.11246v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11246">
<div class="article-summary-box-inner">
<span><p>The lack of well-annotated datasets in computational pathology (CPath)
obstructs the application of deep learning techniques for classifying medical
images. %Since pathologist time is expensive, dataset curation is intrinsically
difficult. Many CPath workflows involve transferring learned knowledge between
various image domains through transfer learning. Currently, most transfer
learning research follows a model-centric approach, tuning network parameters
to improve transfer results over few datasets. In this paper, we take a
data-centric approach to the transfer learning problem and examine the
existence of generalizable knowledge between histopathological datasets. First,
we create a standardization workflow for aggregating existing histopathological
data. We then measure inter-domain knowledge by training ResNet18 models across
multiple histopathological datasets, and cross-transferring between them to
determine the quantity and quality of innate shared knowledge. Additionally, we
use weight distillation to share knowledge between models without additional
training. We find that hard to learn, multi-class datasets benefit most from
pretraining, and a two stage learning framework incorporating a large source
domain such as ImageNet allows for better utilization of smaller datasets.
Furthermore, we find that weight distillation enables models trained on purely
histopathological features to outperform models using external natural image
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlling Directions Orthogonal to a Classifier. (arXiv:2201.11259v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11259">
<div class="article-summary-box-inner">
<span><p>We propose to identify directions invariant to a given classifier so that
these directions can be controlled in tasks such as style transfer. While
orthogonal decomposition is directly identifiable when the given classifier is
linear, we formally define a notion of orthogonality in the non-linear case. We
also provide a surprisingly simple method for constructing the orthogonal
classifier (a classifier utilizing directions other than those of the given
classifier). Empirically, we present three use cases where controlling
orthogonal variation is important: style transfer, domain adaptation, and
fairness. The orthogonal classifier enables desired style transfer when domains
vary in multiple aspects, improves domain adaptation with label shifts and
mitigates the unfairness as a predictor. The code is available at
<a href="http://github.com/Newbeeer/orthogonal_classifier">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting RCAN: Improved Training for Image Super-Resolution. (arXiv:2201.11279v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11279">
<div class="article-summary-box-inner">
<span><p>Image super-resolution (SR) is a fast-moving field with novel architectures
attracting the spotlight. However, most SR models were optimized with dated
training strategies. In this work, we revisit the popular RCAN model and
examine the effect of different training options in SR. Surprisingly (or
perhaps as expected), we show that RCAN can outperform or match nearly all the
CNN-based SR architectures published after RCAN on standard benchmarks with a
proper training strategy and minimal architecture change. Besides, although
RCAN is a very large SR architecture with more than four hundred convolutional
layers, we draw a notable conclusion that underfitting is still the main
problem restricting the model capability instead of overfitting. We observe
supportive evidence that increasing training iterations clearly improves the
model performance while applying regularization techniques generally degrades
the predictions. We denote our simply revised RCAN as RCAN-it and recommend
practitioners to use it as baselines for future research. Code is publicly
available at https://github.com/zudi-lin/rcan-it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive 3D Character Modeling from 2D Orthogonal Drawings with Annotations. (arXiv:2201.11284v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11284">
<div class="article-summary-box-inner">
<span><p>We propose an interactive 3D character modeling approach from orthographic
drawings (e.g., front and side views) based on 2D-space annotations. First, the
system builds partial correspondences between the input drawings and generates
a base mesh with sweeping splines according to edge information in 2D images.
Next, users annotates the desired parts on the input drawings (e.g., the eyes
and mouth) by using two type of strokes, called addition and erosion, and the
system re-optimizes the shape of the base mesh. By repeating the 2D-space
operations (i.e., revising and modifying the annotations), users can design a
desired character model. To validate the efficiency and quality of our system,
we verified the generated results with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient divide-and-conquer registration of UAV and ground LiDAR point clouds through canopy shape context. (arXiv:2201.11296v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11296">
<div class="article-summary-box-inner">
<span><p>Registration of unmanned aerial vehicle laser scanning (ULS) and ground light
detection and ranging (LiDAR) point clouds in forests is critical to create a
detailed representation of a forest structure and an accurate inversion of
forest parameters. However, forest occlusion poses challenges for marker-based
registration methods, and some marker-free automated registration methods have
low efficiency due to the process of object (e.g., tree, crown) segmentation.
Therefore, we use a divide-and-conquer strategy and propose an automated and
efficient method to register ULS and ground LiDAR point clouds in forests.
Registration involves coarse alignment and fine registration, where the coarse
alignment of point clouds is divided into vertical and horizontal alignment.
The vertical alignment is achieved by ground alignment, which is achieved by
the transformation relationship between normal vectors of the ground point
cloud and the horizontal plane, and the horizontal alignment is achieved by
canopy projection image matching. During image matching, vegetation points are
first distinguished by the ground filtering algorithm, and then, vegetation
points are projected onto the horizontal plane to obtain two binary images. To
match the two images, a matching strategy is used based on canopy shape context
features, which are described by a two-point congruent set and canopy overlap.
Finally, we implement coarse alignment of ULS and ground LiDAR datasets by
combining the results of ground alignment and image matching and finish fine
registration. Also, the effectiveness, accuracy, and efficiency of the proposed
method are demonstrated by field measurements of forest plots. Experimental
results show that the ULS and ground LiDAR data in different plots are
registered, of which the horizontal alignment errors are less than 0.02 m, and
the average runtime of the proposed method is less than 1 second.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dissecting the impact of different loss functions with gradient surgery. (arXiv:2201.11307v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11307">
<div class="article-summary-box-inner">
<span><p>Pair-wise loss is an approach to metric learning that learns a semantic
embedding by optimizing a loss function that encourages images from the same
semantic class to be mapped closer than images from different classes. The
literature reports a large and growing set of variations of the pair-wise loss
strategies. Here we decompose the gradient of these loss functions into
components that relate to how they push the relative feature positions of the
anchor-positive and anchor-negative pairs. This decomposition allows the
unification of a large collection of current pair-wise loss functions.
Additionally, explicitly constructing pair-wise gradient updates to separate
out these effects gives insights into which have the biggest impact, and leads
to a simple algorithm that beats the state of the art for image retrieval on
the CAR, CUB and Stanford Online products datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Module Networks for Systematic Generalization in Visual Question Answering. (arXiv:2201.11316v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11316">
<div class="article-summary-box-inner">
<span><p>Transformer-based models achieve great performance on Visual Question
Answering (VQA). However, when we evaluate them on systematic generalization,
i.e., handling novel combinations of known concepts, their performance
degrades. Neural Module Networks (NMNs) are a promising approach for systematic
generalization that consists on composing modules, i.e., neural networks that
tackle a sub-task. Inspired by Transformers and NMNs, we propose Transformer
Module Network (TMN), a novel Transformer-based model for VQA that dynamically
composes modules into a question-specific Transformer network. TMNs achieve
state-of-the-art systematic generalization performance in three VQA datasets,
namely, CLEVR-CoGenT, CLOSURE and GQA-SGL, in some cases improving more than
30% over standard Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Rectification Knowledge Distillation. (arXiv:2201.11319v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11319">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation is a technique which aims to utilize dark knowledge to
compress and transfer information from a vast, well-trained neural network
(teacher model) to a smaller, less capable neural network (student model) with
improved inference efficiency. This approach of distilling knowledge has gained
popularity as a result of the prohibitively complicated nature of such
cumbersome models for deployment on edge computing devices. Generally, the
teacher models used to teach smaller student models are cumbersome in nature
and expensive to train. To eliminate the necessity for a cumbersome teacher
model completely, we propose a simple yet effective knowledge distillation
framework that we termed Dynamic Rectification Knowledge Distillation (DR-KD).
Our method transforms the student into its own teacher, and if the self-teacher
makes wrong predictions while distilling information, the error is rectified
prior to the knowledge being distilled. Specifically, the teacher targets are
dynamically tweaked by the agency of ground-truth while distilling the
knowledge gained from traditional training. Our proposed DR-KD performs
remarkably well in the absence of a sophisticated cumbersome teacher model and
achieves comparable performance to existing state-of-the-art teacher-free
knowledge distillation frameworks when implemented by a low-cost dynamic
mannered teacher. Our approach is all-encompassing and can be utilized for any
deep neural network training that requires categorization or object
recognition. DR-KD enhances the test accuracy on Tiny ImageNet by 2.65% over
prominent baseline models, which is significantly better than any other
knowledge distillation approach while requiring no additional training costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Transfer Learning for Holographic Image Reconstruction using a Recurrent Neural Network. (arXiv:2201.11333v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11333">
<div class="article-summary-box-inner">
<span><p>Deep learning-based methods in computational microscopy have been shown to be
powerful but in general face some challenges due to limited generalization to
new types of samples and requirements for large and diverse training data.
Here, we demonstrate a few-shot transfer learning method that helps a
holographic image reconstruction deep neural network rapidly generalize to new
types of samples using small datasets. We pre-trained a convolutional recurrent
neural network on a large dataset with diverse types of samples, which serves
as the backbone model. By fixing the recurrent blocks and transferring the rest
of the convolutional blocks of the pre-trained model, we reduced the number of
trainable parameters by ~90% compared with standard transfer learning, while
achieving equivalent generalization. We validated the effectiveness of this
approach by successfully generalizing to new types of samples using small
holographic datasets for training, and achieved (i) ~2.5-fold convergence speed
acceleration, (ii) ~20% computation time reduction per epoch, and (iii)
improved reconstruction performance over baseline network models trained from
scratch. This few-shot transfer learning approach can potentially be applied in
other microscopic imaging methods, helping to generalize to new types of
samples without the need for extensive training time and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Global Diversity and Local Context for Video Summarization. (arXiv:2201.11345v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11345">
<div class="article-summary-box-inner">
<span><p>Video summarization aims to automatically generate a diverse and concise
summary which is useful in large-scale video processing. Most of methods tend
to adopt self attention mechanism across video frames, which fails to model the
diversity of video frames. To alleviate this problem, we revisit the pairwise
similarity measurement in self attention mechanism and find that the existing
inner-product affinity leads to discriminative features rather than diversified
features. In light of this phenomenon, we propose global diverse attention by
using the squared Euclidean distance instead to compute the affinities.
Moreover, we model the local contextual information by proposing local
contextual attention to remove the redundancy in the video. By combining these
two attention mechanism, a video \textbf{SUM}marization model with Diversified
Contextual Attention scheme is developed and named as SUM-DCA. Extensive
experiments are conducted on benchmark data sets to verify the effectiveness
and the superiority of SUM-DCA in terms of F-score and rank-based evaluation
without any bells and whistles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Shortcut Technique for GAN. (arXiv:2201.11351v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11351">
<div class="article-summary-box-inner">
<span><p>In recent years, generative adversarial network (GAN)-based image generation
techniques design their generators by stacking up multiple residual blocks. The
residual block generally contains a shortcut, \ie skip connection, which
effectively supports information propagation in the network. In this paper, we
propose a novel shortcut method, called the gated shortcut, which not only
embraces the strength point of the residual block but also further boosts the
GAN performance. More specifically, based on the gating mechanism, the proposed
method leads the residual block to keep (or remove) information that is
relevant (or irrelevant) to the image being generated. To demonstrate that the
proposed method brings significant improvements in the GAN performance, this
paper provides extensive experimental results on the various standard datasets
such as CIFAR-10, CIFAR-100, LSUN, and tiny-ImageNet. Quantitative evaluations
show that the gated shortcut achieves the impressive GAN performance in terms
of Frechet inception distance (FID) and Inception score (IS). For instance, the
proposed method improves the FID and IS scores on the tiny-ImageNet dataset
from 35.13 to 27.90 and 20.23 to 23.42, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Confidence Guided Distance for 3D Partial Shape Registration. (arXiv:2201.11379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11379">
<div class="article-summary-box-inner">
<span><p>We present a novel non-iterative learnable method for partial-to-partial 3D
shape registration. The partial alignment task is extremely complex, as it
jointly tries to match between points and identify which points do not appear
in the corresponding shape, causing the solution to be non-unique and ill-posed
in most cases.
</p>
<p>Until now, two principal methodologies have been suggested to solve this
problem: sample a subset of points that are likely to have correspondences or
perform soft alignment between the point clouds and try to avoid a match to an
occluded part. These heuristics work when the partiality is mild or when the
transformation is small but fails for severe occlusions or when outliers are
present. We present a unique approach named Confidence Guided Distance Network
(CGD-net), where we fuse learnable similarity between point embeddings and
spatial distance between point clouds, inducing an optimized solution for the
overlapping points while ignoring parts that only appear in one of the shapes.
The point feature generation is done by a self-supervised architecture that
repels far points to have different embeddings, therefore succeeds to align
partial views of shapes, even with excessive internal symmetries or acute
rotations. We compare our network to recently presented learning-based and
axiomatic methods and report a fundamental boost in performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Embedding Distribution Refinement and Entropy-Aware Attention for 3D Point Cloud Classification. (arXiv:2201.11388v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11388">
<div class="article-summary-box-inner">
<span><p>Learning a powerful representation from point clouds is a fundamental and
challenging problem in the field of computer vision. Different from images
where RGB pixels are stored in the regular grid, for point clouds, the
underlying semantic and structural information of point clouds is the spatial
layout of the points. Moreover, the properties of challenging in-context and
background noise pose more challenges to point cloud analysis. One assumption
is that the poor performance of the classification model can be attributed to
the indistinguishable embedding feature that impedes the search for the optimal
classifier. This work offers a new strategy for learning powerful
representations via a contrastive learning approach that can be embedded into
any point cloud classification network. First, we propose a supervised
contrastive classification method to implement embedding feature distribution
refinement by improving the intra-class compactness and inter-class
separability. Second, to solve the confusion problem caused by small
inter-class compactness and inter-class separability. Second, to solve the
confusion problem caused by small inter-class variations between some
similar-looking categories, we propose a confusion-prone class mining strategy
to alleviate the confusion effect. Finally, considering that outliers of the
sample clusters in the embedding space may cause performance degradation, we
design an entropy-aware attention module with information entropy theory to
identify the outlier cases and the unstable samples by measuring the
uncertainty of predicted probability. The results of extensive experiments
demonstrate that our method outperforms the state-of-the-art approaches by
achieving 82.9% accuracy on the real-world ScanObjectNN dataset and substantial
performance gains up to 2.9% in DCGNN, 3.1% in PointNet++, and 2.4% in GBNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Frame Quality Enhancement On Compressed Video Using Quantised Data of Deep Belief Networks. (arXiv:2201.11389v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11389">
<div class="article-summary-box-inner">
<span><p>In the age of streaming and surveillance compressed video enhancement has
become a problem in need of constant improvement. Here, we investigate a way of
improving the Multi-Frame Quality Enhancement approach. This approach consists
of making use of the frames that have the peak quality in the region to improve
those that have a lower quality in that region. This approach consists of
obtaining quantized data from the videos using a deep belief network. The
quantized data is then fed into the MF-CNN architecture to improve the
compressed video. We further investigate the impact of using a Bi-LSTM for
detecting the peak quality frames. Our approach obtains better results than the
first approach of the MFQE which uses an SVM for PQF detection. On the other
hand, our MFQE approach does not outperform the latest version of the MQFE
approach that uses a Bi-LSTM for PQF detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalised Image Outpainting with U-Transformer. (arXiv:2201.11403v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11403">
<div class="article-summary-box-inner">
<span><p>While most present image outpainting conducts horizontal extrapolation, we
study the generalised image outpainting problem that extrapolates visual
context all-side around a given image. To this end, we develop a novel
transformer-based generative adversarial network called U-Transformer able to
extend image borders with plausible structure and details even for complicated
scenery images. Specifically, we design a generator as an encoder-to-decoder
structure embedded with the popular Swin Transformer blocks. As such, our novel
framework can better cope with image long-range dependencies which are
crucially important for generalised image outpainting. We propose additionally
a U-shaped structure and multi-view Temporal Spatial Predictor network to
reinforce image self-reconstruction as well as unknown-part prediction smoothly
and realistically. We experimentally demonstrate that our proposed method could
produce visually appealing results for generalized image outpainting against
the state-of-the-art image outpainting approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-linear Motion Estimation for Video Frame Interpolation using Space-time Convolutions. (arXiv:2201.11407v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11407">
<div class="article-summary-box-inner">
<span><p>Video frame interpolation aims to synthesize one or multiple frames between
two consecutive frames in a video. It has a wide range of applications
including slow-motion video generation, frame-rate up-scaling and developing
video codecs. Some older works tackled this problem by assuming per-pixel
linear motion between video frames. However, objects often follow a non-linear
motion pattern in the real domain and some recent methods attempt to model
per-pixel motion by non-linear models (e.g., quadratic). A quadratic model can
also be inaccurate, especially in the case of motion discontinuities over time
(i.e. sudden jerks) and occlusions, where some of the flow information may be
invalid or inaccurate.
</p>
<p>In our paper, we propose to approximate the per-pixel motion using a
space-time convolution network that is able to adaptively select the motion
model to be used. Specifically, we are able to softly switch between a linear
and a quadratic model. Towards this end, we use an end-to-end 3D CNN
encoder-decoder architecture over bidirectional optical flows and occlusion
maps to estimate the non-linear motion model of each pixel. Further, a motion
refinement module is employed to refine the non-linear motion and the
interpolated frames are estimated by a simple warping of the neighboring frames
with the estimated per-pixel motion. Through a set of comprehensive
experiments, we validate the effectiveness of our model and show that our
method outperforms state-of-the-art algorithms on four datasets (Vimeo, DAVIS,
HD and GoPro).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocSegTr: An Instance-Level End-to-End Document Image Segmentation Transformer. (arXiv:2201.11438v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11438">
<div class="article-summary-box-inner">
<span><p>Understanding documents with rich layouts is an essential step towards
information extraction. Business intelligence processes often require the
extraction of useful semantic content from documents at a large scale for
subsequent decision-making tasks. In this context, instance-level segmentation
of different document objects(title, sections, figures, tables and so on) has
emerged as an interesting problem for the document layout analysis community.
To advance the research in this direction, we present a transformer-based model
for end-to-end segmentation of complex layouts in document images. To our
knowledge, this is the first work on transformer-based document segmentation.
Extensive experimentation on the PubLayNet dataset shows that our model
achieved comparable or better segmentation performance than the existing
state-of-the-art approaches. We hope our simple and flexible framework could
serve as a promising baseline for instance-level recognition tasks in document
images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks. (arXiv:2201.11440v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11440">
<div class="article-summary-box-inner">
<span><p>Novel and high-performance medical image classification pipelines are heavily
utilizing ensemble learning strategies. The idea of ensemble learning is to
assemble diverse models or multiple predictions and, thus, boost prediction
performance. However, it is still an open question to what extent as well as
which ensemble learning strategies are beneficial in deep learning based
medical image classification pipelines. In this work, we proposed a
reproducible medical image classification pipeline for analyzing the
performance impact of the following ensemble learning techniques: Augmenting,
Stacking, and Bagging. The pipeline consists of state-of-the-art preprocessing
and image augmentation methods as well as 9 deep convolution neural network
architectures. It was applied on four popular medical imaging datasets with
varying complexity. Furthermore, 12 pooling functions for combining multiple
predictions were analyzed, ranging from simple statistical functions like
unweighted averaging up to more complex learning-based functions like support
vector machines. Our results revealed that Stacking achieved the largest
performance gain of up to 13% F1-score increase. Augmenting showed consistent
improvement capabilities by up to 4% and is also applicable to single model
based pipelines. Cross-validation based Bagging demonstrated to be the most
complex ensemble learning method, which resulted in an F1-score decrease in all
analyzed datasets (up to -10%). Furthermore, we demonstrated that simple
statistical pooling functions are equal or often even better than more complex
pooling functions. We concluded that the integration of Stacking and
Augmentation ensemble learning techniques is a powerful method for any medical
image classification pipeline to improve robustness and boost performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pan-Tumor CAnine cuTaneous Cancer Histology (CATCH) Dataset. (arXiv:2201.11446v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11446">
<div class="article-summary-box-inner">
<span><p>Due to morphological similarities, the differentiation of histologic sections
of cutaneous tumors into individual subtypes can be challenging. Recently, deep
learning-based approaches have proven their potential for supporting
pathologists in this regard. However, many of these supervised algorithms
require a large amount of annotated data for robust development. We present a
publicly available dataset consisting of 350 whole slide images of seven
different canine cutaneous tumors complemented by 12,424 polygon annotations
for 13 histologic classes including seven cutaneous tumor subtypes. Regarding
sample size and annotation extent, this exceeds most publicly available
datasets which are oftentimes limited to the tumor area or merely provide
patch-level annotations. We validated our model for tissue segmentation,
achieving a class-averaged Jaccard coefficient of 0.7047, and 0.9044 for tumor
in particular. For tumor subtype classification, we achieve a slide-level
accuracy of 0.9857. Since canine cutaneous tumors possess various histologic
homologies to human tumors, we believe that the added value of this dataset is
not limited to veterinary pathology but extends to more general fields of
application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In Defense of Kalman Filtering for Polyp Tracking from Colonoscopy Videos. (arXiv:2201.11450v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11450">
<div class="article-summary-box-inner">
<span><p>Real-time and robust automatic detection of polyps from colonoscopy videos
are essential tasks to help improve the performance of doctors during this
exam. The current focus of the field is on the development of accurate but
inefficient detectors that will not enable a real-time application. We advocate
that the field should instead focus on the development of simple and efficient
detectors that an be combined with effective trackers to allow the
implementation of real-time polyp detectors. In this paper, we propose a Kalman
filtering tracker that can work together with powerful, but efficient
detectors, enabling the implementation of real-time polyp detectors. In
particular, we show that the combination of our Kalman filtering with the
detector PP-YOLO shows state-of-the-art (SOTA) detection accuracy and real-time
processing. More specifically, our approach has SOTA results on the
CVC-ClinicDB dataset, with a recall of 0.740, precision of 0.869, $F_1$ score
of 0.799, an average precision (AP) of 0.837, and can run in real time (i.e.,
30 frames per second). We also evaluate our method on a subset of the
Hyper-Kvasir annotated by our clinical collaborators, resulting in SOTA
results, with a recall of 0.956, precision of 0.875, $F_1$ score of 0.914, AP
of 0.952, and can run in real time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RelTR: Relation Transformer for Scene Graph Generation. (arXiv:2201.11460v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11460">
<div class="article-summary-box-inner">
<span><p>Different objects in the same scene are more or less related to each other,
but only a limited number of these relationships are noteworthy. Inspired by
DETR, which excels in object detection, we view scene graph generation as a set
prediction problem and propose an end-to-end scene graph generation model RelTR
which has an encoder-decoder architecture. The encoder reasons about the visual
feature context while the decoder infers a fixed-size set of triplets
subject-predicate-object using different types of attention mechanisms with
coupled subject and object queries. We design a set prediction loss performing
the matching between the ground truth and predicted triplets for the end-to-end
training. In contrast to most existing scene graph generation methods, RelTR is
a one-stage method that predicts a set of relationships directly only using
visual appearance without combining entities and labeling all possible
predicates. Extensive experiments on the Visual Genome and Open Images V6
datasets demonstrate the superior performance and fast inference of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eye-focused Detection of Bell's Palsy in Videos. (arXiv:2201.11479v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11479">
<div class="article-summary-box-inner">
<span><p>In this paper, we present how Bell's Palsy, a neurological disorder, can be
detected just from a subject's eyes in a video. We notice that Bell's Palsy
patients often struggle to blink their eyes on the affected side. As a result,
we can observe a clear contrast between the blinking patterns of the two eyes.
Although previous works did utilize images/videos to detect this disorder, none
have explicitly focused on the eyes. Most of them require the entire face. One
obvious advantage of having an eye-focused detection system is that subjects'
anonymity is not at risk. Also, our AI decisions based on simple blinking
patterns make them explainable and straightforward. Specifically, we develop a
novel feature called blink similarity, which measures the similarity between
the two blinking patterns. Our extensive experiments demonstrate that the
proposed feature is quite robust, for it helps in Bell's Palsy detection even
with very few labels. Our proposed eye-focused detection system is not only
cheaper but also more convenient than several existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Head and eye egocentric gesture recognition for human-robot interaction using eyewear cameras. (arXiv:2201.11500v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11500">
<div class="article-summary-box-inner">
<span><p>Non-verbal communication plays a particularly important role in a wide range
of scenarios in Human-Robot Interaction (HRI). Accordingly, this work addresses
the problem of human gesture recognition. In particular, we focus on head and
eye gestures, and adopt an egocentric (first-person) perspective using eyewear
cameras. We argue that this egocentric view offers a number of conceptual and
technical benefits over scene- or robot-centric perspectives.
</p>
<p>A motion-based recognition approach is proposed, which operates at two
temporal granularities. Locally, frame-to-frame homographies are estimated with
a convolutional neural network (CNN). The output of this CNN is input to a long
short-term memory (LSTM) to capture longer-term temporal visual relationships,
which are relevant to characterize gestures.
</p>
<p>Regarding the configuration of the network architecture, one particularly
interesting finding is that using the output of an internal layer of the
homography CNN increases the recognition rate with respect to using the
homography matrix itself. While this work focuses on action recognition, and no
robot or user study has been conducted yet, the system has been de signed to
meet real-time constraints. The encouraging results suggest that the proposed
egocentric perspective is viable, and this proof-of-concept work provides novel
and useful contributions to the exciting area of HRI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection in Retinal Images using Multi-Scale Deep Feature Sparse Coding. (arXiv:2201.11506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11506">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Network models have successfully detected retinal
illness from optical coherence tomography (OCT) and fundus images. These CNN
models frequently rely on vast amounts of labeled data for training, difficult
to obtain, especially for rare diseases. Furthermore, a deep learning system
trained on a data set with only one or a few diseases cannot detect other
diseases, limiting the system's practical use in disease identification. We
have introduced an unsupervised approach for detecting anomalies in retinal
images to overcome this issue. We have proposed a simple, memory efficient,
easy to train method which followed a multi-step training technique that
incorporated autoencoder training and Multi-Scale Deep Feature Sparse Coding
(MDFSC), an extended version of normal sparse coding, to accommodate diverse
types of retinal datasets. We achieve relative AUC score improvement of 7.8\%,
6.7\% and 12.1\% over state-of-the-art SPADE on Eye-Q, IDRiD and OCTID datasets
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Density-Aware Hyper-Graph Neural Networks for Graph-based Semi-supervised Node Classification. (arXiv:2201.11511v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11511">
<div class="article-summary-box-inner">
<span><p>Graph-based semi-supervised learning, which can exploit the connectivity
relationship between labeled and unlabeled data, has been shown to outperform
the state-of-the-art in many artificial intelligence applications. One of the
most challenging problems for graph-based semi-supervised node classification
is how to use the implicit information among various data to improve the
performance of classifying. Traditional studies on graph-based semi-supervised
learning have focused on the pairwise connections among data. However, the data
correlation in real applications could be beyond pairwise and more complicated.
The density information has been demonstrated to be an important clue, but it
is rarely explored in depth among existing graph-based semi-supervised node
classification methods. To develop a flexible and effective model for
graph-based semi-supervised node classification, we propose a novel
Density-Aware Hyper-Graph Neural Networks (DA-HGNN). In our proposed approach,
hyper-graph is provided to explore the high-order semantic correlation among
data, and a density-aware hyper-graph attention network is presented to explore
the high-order connection relationship. Extensive experiments are conducted in
various benchmark datasets, and the results demonstrate the effectiveness of
the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResiDualGAN: Resize-Residual DualGAN for Cross-Domain Remote Sensing Images Semantic Segmentation. (arXiv:2201.11523v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11523">
<div class="article-summary-box-inner">
<span><p>The performance of a semantic segmentation model for remote sensing (RS)
images pretrained on an annotated dataset would greatly decrease when testing
on another unannotated dataset because of the domain gap. Adversarial
generative methods, e.g., DualGAN, are utilized for unpaired image-to-image
translation to minimize the pixel-level domain gap, which is one of the common
approaches for unsupervised domain adaptation (UDA). However, existing image
translation methods are facing two problems when performing RS images
translation: 1) ignoring the scale discrepancy between two RS datasets which
greatly affect the accuracy performance of scale-invariant objects, 2) ignoring
the characteristic of real-to-real translation of RS images which brings an
unstable factor for the training of the models. In this paper, ResiDualGAN is
proposed for RS images translation, where a resizer module is used for
addressing the scale discrepancy of RS datasets, and a residual connection is
used for strengthening the stability of real-to-real images translation and
improving the performance in cross-domain semantic segmentation tasks.
Combining with an output space adaptation method, the proposed method greatly
improves the accuracy performance on common benchmarks, which demonstrates the
superiority and reliability of ResiDuanGAN. At the end of the paper, a thorough
discussion is also conducted to give a reasonable explanation for the
improvement of ResiDualGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains. (arXiv:2201.11528v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11528">
<div class="article-summary-box-inner">
<span><p>Adversarial examples have posed a severe threat to deep neural networks due
to their transferable nature. Currently, various works have paid great efforts
to enhance the cross-model transferability, which mostly assume the substitute
model is trained in the same domain as the target model. However, in reality,
the relevant information of the deployed model is unlikely to leak. Hence, it
is vital to build a more practical black-box threat model to overcome this
limitation and evaluate the vulnerability of deployed models. In this paper,
with only the knowledge of the ImageNet domain, we propose a Beyond ImageNet
Attack (BIA) to investigate the transferability towards black-box domains
(unknown classification tasks). Specifically, we leverage a generative model to
learn the adversarial function for disrupting low-level features of input
images. Based on this framework, we further propose two variants to narrow the
gap between the source and target domains from the data and model perspectives,
respectively. Extensive experiments on coarse-grained and fine-grained domains
demonstrate the effectiveness of our proposed methods. Notably, our methods
outperform state-of-the-art approaches by up to 7.71\% (towards coarse-grained
domains) and 25.91\% (towards fine-grained domains) on average. Our code is
available at \url{https://github.com/qilong-zhang/Beyond-ImageNet-Attack}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASOC: Adaptive Self-aware Object Co-localization. (arXiv:2201.11547v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11547">
<div class="article-summary-box-inner">
<span><p>The primary goal of this paper is to localize objects in a group of
semantically similar images jointly, also known as the object co-localization
problem. Most related existing works are essentially weakly-supervised, relying
prominently on the neighboring images' weak-supervision. Although weak
supervision is beneficial, it is not entirely reliable, for the results are
quite sensitive to the neighboring images considered. In this paper, we combine
it with a self-awareness phenomenon to mitigate this issue. By self-awareness
here, we refer to the solution derived from the image itself in the form of
saliency cue, which can also be unreliable if applied alone. Nevertheless,
combining these two paradigms together can lead to a better co-localization
ability. Specifically, we introduce a dynamic mediator that adaptively strikes
a proper balance between the two static solutions to provide an optimal
solution. Therefore, we call this method \textit{ASOC}: Adaptive Self-aware
Object Co-localization. We perform exhaustive experiments on several benchmark
datasets and validate that weak-supervision supplemented with self-awareness
has superior performance outperforming several compared competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Probabilistic Framework for Dynamic Object Recognition in 3D Environment With A Novel Continuous Ground Estimation Method. (arXiv:2201.11608v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11608">
<div class="article-summary-box-inner">
<span><p>In this thesis a probabilistic framework is developed and proposed for
Dynamic Object Recognition in 3D Environments. A software package is developed
using C++ and Python in ROS that performs the detection and tracking task.
Furthermore, a novel Gaussian Process Regression (GPR) based method is
developed to detect ground points in different urban scenarios of regular,
sloped and rough. The ground surface behavior is assumed to only demonstrate
local input-dependent smoothness. kernel's length-scales are obtained. Bayesian
inference is implemented sing \textit{Maximum a Posteriori} criterion. The
log-marginal likelihood function is assumed to be a multi-task objective
function, to represent a whole-frame unbiased view of the ground at each frame
because adjacent segments may not have similar ground structure in an uneven
scene while having shared hyper-parameter values. Simulation results shows the
effectiveness of the proposed method in uneven and rough scenes which
outperforms similar Gaussian process based ground segmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain generalization in deep learning-based mass detection in mammography: A large-scale multi-center study. (arXiv:2201.11620v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11620">
<div class="article-summary-box-inner">
<span><p>Computer-aided detection systems based on deep learning have shown great
potential in breast cancer detection. However, the lack of domain
generalization of artificial neural networks is an important obstacle to their
deployment in changing clinical environments. In this work, we explore the
domain generalization of deep learning methods for mass detection in digital
mammography and analyze in-depth the sources of domain shift in a large-scale
multi-center setting. To this end, we compare the performance of eight
state-of-the-art detection methods, including Transformer-based models, trained
in a single domain and tested in five unseen domains. Moreover, a single-source
mass detection training pipeline is designed to improve the domain
generalization without requiring images from the new domain. The results show
that our workflow generalizes better than state-of-the-art transfer
learning-based approaches in four out of five domains while reducing the domain
shift caused by the different acquisition protocols and scanner manufacturers.
Subsequently, an extensive analysis is performed to identify the covariate
shifts with bigger effects on the detection performance, such as due to
differences in patient age, breast density, mass size, and mass malignancy.
Ultimately, this comprehensive study provides key insights and best practices
for future research on domain generalization in deep learning-based breast
cancer detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Classification of Neuromuscular Diseases in Children Using Photoacoustic Imaging. (arXiv:2201.11630v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11630">
<div class="article-summary-box-inner">
<span><p>Neuromuscular diseases (NMDs) cause a significant burden for both healthcare
systems and society. They can lead to severe progressive muscle weakness,
muscle degeneration, contracture, deformity and progressive disability. The
NMDs evaluated in this study often manifest in early childhood. As subtypes of
disease, e.g. Duchenne Muscular Dystropy (DMD) and Spinal Muscular Atrophy
(SMA), are difficult to differentiate at the beginning and worsen quickly, fast
and reliable differential diagnosis is crucial. Photoacoustic and ultrasound
imaging has shown great potential to visualize and quantify the extent of
different diseases. The addition of automatic classification of such image data
could further improve standard diagnostic procedures. We compare deep
learning-based 2-class and 3-class classifiers based on VGG16 for
differentiating healthy from diseased muscular tissue. This work shows
promising results with high accuracies above 0.86 for the 3-class problem and
can be used as a proof of concept for future approaches for earlier diagnosis
and therapeutic monitoring of NMDs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Video Prior for Video Consistency and Propagation. (arXiv:2201.11632v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11632">
<div class="article-summary-box-inner">
<span><p>Applying an image processing algorithm independently to each video frame
often leads to temporal inconsistency in the resulting video. To address this
issue, we present a novel and general approach for blind video temporal
consistency. Our method is only trained on a pair of original and processed
videos directly instead of a large dataset. Unlike most previous methods that
enforce temporal consistency with optical flow, we show that temporal
consistency can be achieved by training a convolutional neural network on a
video with Deep Video Prior (DVP). Moreover, a carefully designed iteratively
reweighted training strategy is proposed to address the challenging multimodal
inconsistency problem. We demonstrate the effectiveness of our approach on 7
computer vision tasks on videos. Extensive quantitative and perceptual
experiments show that our approach obtains superior performance than
state-of-the-art methods on blind video temporal consistency. We further extend
DVP to video propagation and demonstrate its effectiveness in propagating three
different types of information (color, artistic style, and object
segmentation). A progressive propagation strategy with pseudo labels is also
proposed to enhance DVP's performance on video propagation. Our source codes
are publicly available at https://github.com/ChenyangLEI/deep-video-prior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Team Yao at Factify 2022: Utilizing Pre-trained Models and Co-attention Networks for Multi-Modal Fact Verification. (arXiv:2201.11664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11664">
<div class="article-summary-box-inner">
<span><p>In recent years, social media has enabled users to get exposed to a myriad of
misinformation and disinformation; thus, misinformation has attracted a great
deal of attention in research fields and as a social issue. To address the
problem, we propose a framework, Pre-CoFact, composed of two pre-trained models
for extracting features from text and images, and multiple co-attention
networks for fusing the same modality but different sources and different
modalities. Besides, we adopt the ensemble method by using different
pre-trained models in Pre-CoFact to achieve better performance. We further
illustrate the effectiveness from the ablation study and examine different
pre-trained models for comparison. Our team, Yao, won the fifth prize
(F1-score: 74.585\%) in the Factify challenge hosted by De-Factify @ AAAI 2022,
which demonstrates that our model achieved competitive performance without
using auxiliary tasks or extra information. The source code of our work is
publicly available at
https://github.com/wywyWang/Multi-Modal-Fact-Verification-2021
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Checklist: Towards Testable Error Analysis of Image Models to Help System Designers Interrogate Model Capabilities. (arXiv:2201.11674v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11674">
<div class="article-summary-box-inner">
<span><p>Using large pre-trained models for image recognition tasks is becoming
increasingly common owing to the well acknowledged success of recent models
like vision transformers and other CNN-based models like VGG and Resnet. The
high accuracy of these models on benchmark tasks has translated into their
practical use across many domains including safety-critical applications like
autonomous driving and medical diagnostics. Despite their widespread use, image
models have been shown to be fragile to changes in the operating environment,
bringing their robustness into question. There is an urgent need for methods
that systematically characterise and quantify the capabilities of these models
to help designers understand and provide guarantees about their safety and
robustness. In this paper, we propose Vision Checklist, a framework aimed at
interrogating the capabilities of a model in order to produce a report that can
be used by a system designer for robustness evaluations. This framework
proposes a set of perturbation operations that can be applied on the underlying
data to generate test samples of different types. The perturbations reflect
potential changes in operating environments, and interrogate various properties
ranging from the strictly quantitative to more qualitative. Our framework is
evaluated on multiple datasets like Tinyimagenet, CIFAR10, CIFAR100 and
Camelyon17 and for models like ViT and Resnet. Our Vision Checklist proposes a
specific set of evaluations that can be integrated into the previously proposed
concept of a model card. Robustness evaluations like our checklist will be
crucial in future safety evaluations of visual perception modules, and be
useful for a wide range of stakeholders including designers, deployers, and
regulators involved in the certification of these systems. Source code of
Vision Checklist would be open for public use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Change Detection using DRE-CUSUM. (arXiv:2201.11678v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11678">
<div class="article-summary-box-inner">
<span><p>This paper presents DRE-CUSUM, an unsupervised density-ratio estimation (DRE)
based approach to determine statistical changes in time-series data when no
knowledge of the pre-and post-change distributions are available. The core idea
behind the proposed approach is to split the time-series at an arbitrary point
and estimate the ratio of densities of distribution (using a parametric model
such as a neural network) before and after the split point. The DRE-CUSUM
change detection statistic is then derived from the cumulative sum (CUSUM) of
the logarithm of the estimated density ratio. We present a theoretical
justification as well as accuracy guarantees which show that the proposed
statistic can reliably detect statistical changes, irrespective of the split
point. While there have been prior works on using density ratio based methods
for change detection, to the best of our knowledge, this is the first
unsupervised change detection approach with a theoretical justification and
accuracy guarantees. The simplicity of the proposed framework makes it readily
applicable in various practical settings (including high-dimensional
time-series data); we also discuss generalizations for online change detection.
We experimentally show the superiority of DRE-CUSUM using both synthetic and
real-world datasets over existing state-of-the-art unsupervised algorithms
(such as Bayesian online change detection, its variants as well as several
other heuristic methods).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DropNAS: Grouped Operation Dropout for Differentiable Architecture Search. (arXiv:2201.11679v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11679">
<div class="article-summary-box-inner">
<span><p>Neural architecture search (NAS) has shown encouraging results in automating
the architecture design. Recently, DARTS relaxes the search process with a
differentiable formulation that leverages weight-sharing and SGD where all
candidate operations are trained simultaneously. Our empirical results show
that such procedure results in the co-adaption problem and Matthew Effect:
operations with fewer parameters would be trained maturely earlier. This causes
two problems: firstly, the operations with more parameters may never have the
chance to express the desired function since those with less have already done
the job; secondly, the system will punish those underperforming operations by
lowering their architecture parameter, and they will get smaller loss
gradients, which causes the Matthew Effect. In this paper, we systematically
study these problems and propose a novel grouped operation dropout algorithm
named DropNAS to fix the problems with DARTS. Extensive experiments demonstrate
that DropNAS solves the above issues and achieves promising performance.
Specifically, DropNAS achieves 2.26% test error on CIFAR-10, 16.39% on
CIFAR-100 and 23.4% on ImageNet (with the same training hyperparameters as
DARTS for a fair comparison). It is also observed that DropNAS is robust across
variants of the DARTS search space. Code is available at
https://github.com/wiljohnhong/DropNAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained Structure Learning for Scene Graph Generation. (arXiv:2201.11697v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11697">
<div class="article-summary-box-inner">
<span><p>As a structured prediction task, scene graph generation aims to build a
visually-grounded scene graph to explicitly model objects and their
relationships in an input image. Currently, the mean field variational Bayesian
framework is the de facto methodology used by the existing methods, in which
the unconstrained inference step is often implemented by a message passing
neural network. However, such formulation fails to explore other inference
strategies, and largely ignores the more general constrained optimization
models. In this paper, we present a constrained structure learning method, for
which an explicit constrained variational inference objective is proposed.
Instead of applying the ubiquitous message-passing strategy, a generic
constrained optimization method - entropic mirror descent - is utilized to
solve the constrained variational inference step. We validate the proposed
generic model on various popular scene graph generation benchmarks and show
that it outperforms the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matched Illumination. (arXiv:2201.11700v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11700">
<div class="article-summary-box-inner">
<span><p>In previous work, it was shown that a camera can theoretically be made more
colorimetric - its RGBs become more linearly related to XYZ tristimuli - by
placing a specially designed color filter in the optical path. While the prior
art demonstrated the principle, the optimal color-correction filters were not
actually manufactured. In this paper, we provide a novel way of creating the
color filtering effect without making a physical filter: we modulate the
spectrum of the light source by using a spectrally tunable lighting system to
recast the prefiltering effect from a lighting perspective. According to our
method, if we wish to measure color under a D65 light, we relight the scene
with a modulated D65 spectrum where the light modulation mimics the effect of
color prefiltering in the prior art. We call our optimally modulated light, the
matched illumination. In the experiments, using synthetic and real
measurements, we show that color measurement errors can be reduced by about 50%
or more on simulated data and 25% or more on real images when the matched
illumination is used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Study of Bias Amplification. (arXiv:2201.11706v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11706">
<div class="article-summary-box-inner">
<span><p>Recent research suggests that predictions made by machine-learning models can
amplify biases present in the training data. When a model amplifies bias, it
makes certain predictions at a higher rate for some groups than expected based
on training-data statistics. Mitigating such bias amplification requires a deep
understanding of the mechanics in modern machine learning that give rise to
that amplification. We perform the first systematic, controlled study into when
and how bias amplification occurs. To enable this study, we design a simple
image-classification problem in which we can tightly control (synthetic)
biases. Our study of this problem reveals that the strength of bias
amplification is correlated to measures such as model accuracy, model capacity,
model overconfidence, and amount of training data. We also find that bias
amplification can vary greatly during training. Finally, we find that bias
amplification may depend on the difficulty of the classification task relative
to the difficulty of recognizing group membership: bias amplification appears
to occur primarily when it is easier to recognize group membership than class
membership. Our results suggest best practices for training machine-learning
models that we hope will help pave the way for the development of better
mitigation strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages. (arXiv:2201.11732v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11732">
<div class="article-summary-box-inner">
<span><p>Reliable evaluation benchmarks designed for replicability and
comprehensiveness have driven progress in machine learning. Due to the lack of
a multilingual benchmark, however, vision-and-language research has mostly
focused on English language tasks. To fill this gap, we introduce the
Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings
together - by both aggregating pre-existing datasets and creating new ones -
visual question answering, cross-modal retrieval, grounded reasoning, and
grounded entailment tasks across 20 diverse languages. Our benchmark enables
the evaluation of multilingual multimodal models for transfer learning, not
only in a zero-shot setting, but also in newly defined few-shot learning
setups. Based on the evaluation of the available state-of-the-art models, we
find that translate-test transfer is superior to zero-shot transfer and that
few-shot learning is hard to harness for many tasks. Moreover, downstream
performance is partially explained by the amount of available unlabelled
textual data for pretraining, and only weakly by the typological distance of
target-source languages. We hope to encourage future research efforts in this
area by releasing the benchmark to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives. (arXiv:2201.11736v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11736">
<div class="article-summary-box-inner">
<span><p>This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a
new member in the family of InfoNCE losses that preserves a ranked ordering of
positive samples. In contrast to the standard InfoNCE loss, which requires a
strict binary separation of the training pairs into similar and dissimilar
samples, RINCE can exploit information about a similarity ranking for learning
a corresponding embedding space. We show that the proposed loss function learns
favorable embeddings compared to the standard InfoNCE whenever at least noisy
ranking information can be obtained or when the definition of positives and
negatives is blurry. We demonstrate this for a supervised classification task
with additional superclass labels and noisy similarity scores. Furthermore, we
show that RINCE can also be applied to unsupervised training with experiments
on unsupervised representation learning from videos. In particular, the
embedding yields higher classification accuracy, retrieval rates and performs
better in out-of-distribution detection than the standard InfoNCE loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PRNU Based Source Camera Identification for Webcam and Smartphone Videos. (arXiv:2201.11737v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11737">
<div class="article-summary-box-inner">
<span><p>This communication is about an application of image forensics where we use
camera sensor fingerprints to identify source camera (SCI: Source Camera
Identification) in webcam/smartphone videos. Sensor or camera fingerprints are
based on computing the intrinsic noise that is always present in this kind of
sensors due to manufacturing imperfections. This is an unavoidable
characteristic that links each sensor with its noise pattern. PRNU (Photo
Response Non-Uniformity) has become the default technique to compute a camera
fingerprint. There are many applications nowadays dealing with PRNU patterns
for camera identification using still images. In this work we focus on video,
first on webcam video and afterwards on smartphone video. Webcams and
smartphones are the most used video cameras nowadays. Three possible methods
for SCI are implemented and assessed in this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Agents in Networks: Estimation and Targeting. (arXiv:1808.04878v3 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1808.04878">
<div class="article-summary-box-inner">
<span><p>We consider a network of agents. Associated with each agent are her covariate
and outcome. Agents influence each other's outcomes according to a certain
connection/influence structure. A subset of the agents participate on a
platform, and hence, are observable to it. The rest are not observable to the
platform and are called the latent agents. The platform does not know the
influence structure of the observable or the latent parts of the network. It
only observes the data on past covariates and decisions of the observable
agents. Observable agents influence each other both directly and indirectly
through the influence they exert on the latent agents.
</p>
<p>We investigate how the platform can estimate the dependence of the observable
agents' outcomes on their covariates, taking the latent agents into account.
First, we show that this relationship can be succinctly captured by a matrix
and provide an algorithm for estimating it under a suitable approximate
sparsity condition using historical data of covariates and outcomes for the
observable agents. We also obtain convergence rates for the proposed estimator
despite the high dimensionality that allows more agents than observations.
Second, we show that the approximate sparsity condition holds under the
standard conditions used in the literature. Hence, our results apply to a large
class of networks. Finally, we apply our results to two practical settings:
targeted advertising and promotional pricing. We show that by using the
available historical data with our estimator, it is possible to obtain
asymptotically optimal advertising/pricing decisions, despite the presence of
latent agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis and algorithms for $\ell_p$-based semi-supervised learning on graphs. (arXiv:1901.05031v4 [math.NA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1901.05031">
<div class="article-summary-box-inner">
<span><p>This paper addresses theory and applications of $\ell_p$-based Laplacian
regularization in semi-supervised learning. The graph $p$-Laplacian for $p&gt;2$
has been proposed recently as a replacement for the standard ($p=2$) graph
Laplacian in semi-supervised learning problems with very few labels, where
Laplacian learning is degenerate.
</p>
<p>In the first part of the paper we prove new discrete to continuum convergence
results for $p$-Laplace problems on $k$-nearest neighbor ($k$-NN) graphs, which
are more commonly used in practice than random geometric graphs. Our analysis
shows that, on $k$-NN graphs, the $p$-Laplacian retains information about the
data distribution as $p\to \infty$ and Lipschitz learning ($p=\infty$) is
sensitive to the data distribution. This situation can be contrasted with
random geometric graphs, where the $p$-Laplacian forgets the data distribution
as $p\to \infty$. We also present a general framework for proving discrete to
continuum convergence results in graph-based learning that only requires
pointwise consistency and monotonicity.
</p>
<p>In the second part of the paper, we develop fast algorithms for solving the
variational and game-theoretic $p$-Laplace equations on weighted graphs for
$p&gt;2$. We present several efficient and scalable algorithms for both
formulations, and present numerical results on synthetic data indicating their
convergence properties. Finally, we conduct extensive numerical experiments on
the MNIST, FashionMNIST and EMNIST datasets that illustrate the effectiveness
of the $p$-Laplacian formulation for semi-supervised learning with few labels.
In particular, we find that Lipschitz learning ($p=\infty$) performs well with
very few labels on $k$-NN graphs, which experimentally validates our
theoretical findings that Lipschitz learning retains information about the data
distribution (the unlabeled data) on $k$-NN graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Adjacency Matrix for Video Relocalization. (arXiv:2008.08977v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.08977">
<div class="article-summary-box-inner">
<span><p>In this paper, we continue our work on video relocalization task. Based on
using graph convolution to extract intra-video and inter-video frame features,
we improve the method by using similarity-metric based graph convolution, whose
weighted adjacency matrix is achieved by calculating similarity metric between
features of any two different time steps in the graph. Experiments on
ActivityNet v1.2 and Thumos14 dataset show the effectiveness of this
improvement, and it outperforms the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Colorization: A Survey and Dataset. (arXiv:2008.10774v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.10774">
<div class="article-summary-box-inner">
<span><p>Image colorization is the process of estimating RGB colors for grayscale
images or video frames to improve their aesthetic and perceptual quality. Deep
learning techniques for image colorization have progressed notably over the
last decade, calling the need for a systematic survey and benchmarking of these
techniques. This article presents a comprehensive survey of recent
state-of-the-art deep learning-based image colorization techniques, describing
their fundamental block architectures, inputs, optimizers, loss functions,
training protocols, and training data \textit{etc.} It categorizes the existing
colorization techniques into seven classes and discusses important factors
governing their performance, such as benchmark datasets and evaluation metrics.
We highlight the limitations of existing datasets and introduce a new dataset
specific to colorization. Using the existing datasets and our new one, we
perform an extensive experimental evaluation of existing image colorization
methods. Finally, we discuss the limitations of existing methods and recommend
possible solutions as well as future research directions for this rapidly
evolving topic of deep image colorization. Dataset and codes for evaluation are
publicly available at https://github.com/saeed-anwar/ColorSurvey
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can we Generalize and Distribute Private Representation Learning?. (arXiv:2010.01792v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01792">
<div class="article-summary-box-inner">
<span><p>We study the problem of learning representations that are private yet
informative i.e., provide information about intended "ally" targets while
hiding sensitive "adversary" attributes). We propose Exclusion-Inclusion
Generative Adversarial Network (EIGAN), a generalized private representation
learning (PRL) architecture that accounts for multiple ally and adversary
attributes unlike existing PRL solutions. While centrally-aggregated dataset is
a prerequisite for most PRL techniques, data in real-world is often siloed
across multiple distributed nodes unwilling to share the raw data because of
privacy concerns. We address this practical constraint by developing D-EIGAN,
the first distributed PRL method that learns representations at each node
without transmitting the source data. We theoretically analyze the behavior of
adversaries under the optimal EIGAN and D-EIGAN encoders and the impact of
dependencies among ally and adversary tasks on the optimization objective. Our
experiments on various datasets demonstrate the advantages of EIGAN in terms of
performance, robustness, and scalability. In particular, EIGAN outperforms the
previous state-of-the-art by a significant accuracy margin (47% improvement),
and D-EIGAN's performance is consistently on par with EIGAN under different
network settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing. (arXiv:2011.09899v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09899">
<div class="article-summary-box-inner">
<span><p>User data confidentiality protection is becoming a rising challenge in the
present deep learning research. Without access to data, conventional
data-driven model compression faces a higher risk of performance degradation.
Recently, some works propose to generate images from a specific pretrained
model to serve as training data. However, the inversion process only utilizes
biased feature statistics stored in one model and is from low-dimension to
high-dimension. As a consequence, it inevitably encounters the difficulties of
generalizability and inexact inversion, which leads to unsatisfactory
performance. To address these problems, we propose MixMix based on two simple
yet effective techniques: (1) Feature Mixing: utilizes various models to
construct a universal feature space for generalized inversion; (2) Data Mixing:
mixes the synthesized images and labels to generate exact label information. We
prove the effectiveness of MixMix from both theoretical and empirical
perspectives. Extensive experiments show that MixMix outperforms existing
methods on the mainstream compression tasks, including quantization, knowledge
distillation, and pruning. Specifically, MixMix achieves up to 4% and 20%
accuracy uplift on quantization and pruning, respectively, compared to existing
data-free compression work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition. (arXiv:2011.11961v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11961">
<div class="article-summary-box-inner">
<span><p>Existing portrait matting methods either require auxiliary inputs that are
costly to obtain or involve multiple stages that are computationally expensive,
making them less suitable for real-time applications. In this work, we present
a light-weight matting objective decomposition network (MODNet) for portrait
matting in real-time with a single input image. The key idea behind our
efficient design is by optimizing a series of sub-objectives simultaneously via
explicit constraints. In addition, MODNet includes two novel techniques for
improving model efficiency and robustness. First, an Efficient Atrous Spatial
Pyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for
semantic estimation. Second, a self-supervised sub-objectives consistency (SOC)
strategy is proposed to adapt MODNet to real-world data to address the domain
shift problem common to trimap-free methods. MODNet is easy to be trained in an
end-to-end manner. It is much faster than contemporaneous methods and runs at
67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms
prior trimap-free methods by a large margin on both Adobe Matting Dataset and a
carefully designed photographic portrait matting (PPM-100) benchmark proposed
by us. Further, MODNet achieves remarkable results on daily photos and videos.
Our code and models are available at https://github.com/ZHKKKe/MODNet, and the
PPM-100 benchmark is released at https://github.com/ZHKKKe/PPM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image Classifiers. (arXiv:2012.05858v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05858">
<div class="article-summary-box-inner">
<span><p>Light-based adversarial attacks use spatial augmented reality (SAR)
techniques to fool image classifiers by altering the physical light condition
with a controllable light source, e.g., a projector. Compared with physical
attacks that place hand-crafted adversarial objects, projector-based ones
obviate modifying the physical entities, and can be performed transiently and
dynamically by altering the projection pattern. However, subtle light
perturbations are insufficient to fool image classifiers, due to the complex
environment and project-and-capture process. Thus, existing approaches focus on
projecting clearly perceptible adversarial patterns, while the more interesting
yet challenging goal, stealthy projector-based attack, remains open. In this
paper, for the first time, we formulate this problem as an end-to-end
differentiable process and propose a Stealthy Projector-based Adversarial
Attack (SPAA) solution. In SPAA, we approximate the real Project-and-Capture
process using a deep neural network named PCNet, then we include PCNet in the
optimization of projector-based attacks such that the generated adversarial
projection is physically plausible. Finally, to generate both robust and
stealthy adversarial projections, we propose an algorithm that uses minimum
perturbation and adversarial confidence thresholds to alternate between the
adversarial loss and stealthiness loss optimization. Our experimental
evaluations show that SPAA clearly outperforms other methods by achieving
higher attack success rates and meanwhile being stealthier, for both targeted
and untargeted attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Non-linear Wavelet Transformation via Normalizing Flow. (arXiv:2101.11306v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11306">
<div class="article-summary-box-inner">
<span><p>Wavelet transformation stands as a cornerstone in modern data analysis and
signal processing. Its mathematical essence is an invertible transformation
that discerns slow patterns from fast ones in the frequency domain. Such an
invertible transformation can be learned by a designed normalizing flow model.
With a generalized lifting scheme as coupling layers, a factor-out layer
resembling the downsampling, and parameter sharing at different levels of the
model, one can train the normalizing flow to filter high-frequency elements at
different levels, thus extending traditional linear wavelet transformations to
learnable non-linear deep learning models. In this paper, a way of building
such flow is proposed, along with a numerical analysis of the learned
transformation. Then, we demonstrate the model's ability in image lossless
compression, show it can achieve SOTA compression scores while achieving a
small model size, substantial generalization ability, and the ability to handle
high-dimensional data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking Beyond Two Frames: End-to-End Multi-Object Tracking Using Spatial and Temporal Transformers. (arXiv:2103.14829v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14829">
<div class="article-summary-box-inner">
<span><p>Tracking a time-varying indefinite number of objects in a video sequence over
time remains a challenge despite recent advances in the field. Ignoring
long-term temporal information, most existing approaches are not able to
properly handle multi-object tracking challenges such as occlusion. To address
these shortcomings, we present MO3TR: a truly end-to-end Transformer-based
online multi-object tracking (MOT) framework that learns to handle occlusions,
track initiation and termination without the need for an explicit data
association module or any heuristics/post-processing. MO3TR encodes object
interactions into long-term temporal embeddings using a combination of spatial
and temporal Transformers, and recursively uses the information jointly with
the input data to estimate the states of all tracked objects over time. The
spatial attention mechanism enables our framework to learn implicit
representations between all the objects and the objects to the measurements,
while the temporal attention mechanism focuses on specific parts of past
information, allowing our approach to resolve occlusions over multiple frames.
Our experiments demonstrate the potential of this new approach, reaching new
state-of-the-art results on multiple MOT metrics for two popular multi-object
tracking benchmarks. Our code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning from Semantically Imprecise Data. (arXiv:2104.10901v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10901">
<div class="article-summary-box-inner">
<span><p>Learning from imprecise labels such as "animal" or "bird", but making precise
predictions like "snow bunting" at inference time is an important capability
for any classifier when expertly labeled training data is scarce. Contributions
by volunteers or results of web crawling lack precision in this manner, but are
still valuable. And crucially, these weakly labeled examples are available in
larger quantities for lower cost than high-quality bespoke training data.
CHILLAX, a recently proposed method to tackle this task, leverages a
hierarchical classifier to learn from imprecise labels. However, it has two
major limitations. First, it does not learn from examples labeled as the root
of the hierarchy, e.g., "object". Second, an extrapolation of annotations to
precise labels is only performed at test time, where confident extrapolations
could be already used as training data. In this work, we extend CHILLAX with a
self-supervised scheme using constrained semantic extrapolation to generate
pseudo-labels. This addresses the second concern, which in turn solves the
first problem, enabling an even weaker supervision requirement than CHILLAX. We
evaluate our approach empirically, showing that our method allows for a
consistent accuracy improvement of 0.84 to 1.19 percent points over CHILLAX and
is suitable as a drop-in replacement without any negative consequences such as
longer training times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Prediction and Evaluation of Brassica Growth in the Field using Conditional Generative Adversarial Networks. (arXiv:2105.07789v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07789">
<div class="article-summary-box-inner">
<span><p>Farmers frequently assess plant growth and performance as basis for making
decisions when to take action in the field, such as fertilization, weed
control, or harvesting. The prediction of plant growth is a major challenge, as
it is affected by numerous and highly variable environmental factors. This
paper proposes a novel monitoring approach that comprises high-throughput
imaging sensor measurements and their automatic analysis to predict future
plant growth. Our approach's core is a novel machine learning-based generative
growth model based on conditional generative adversarial networks, which is
able to predict the future appearance of individual plants. In experiments with
RGB time-series images of laboratory-grown Arabidopsis thaliana and field-grown
cauliflower plants, we show that our approach produces realistic, reliable, and
reasonable images of future growth stages. The automatic interpretation of the
generated images through neural network-based instance segmentation allows the
derivation of various phenotypic traits that describe plant growth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLO5Face: Why Reinventing a Face Detector. (arXiv:2105.12931v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12931">
<div class="article-summary-box-inner">
<span><p>Tremendous progress has been made on face detection in recent years using
convolutional neural networks. While many face detectors use designs designated
for detecting faces, we treat face detection as a generic object detection
task. We implement a face detector based on the YOLOv5 object detector and call
it YOLO5Face. We make a few key modifications to the YOLOv5 and optimize it for
face detection. These modifications include adding a five-point landmark
regression head, using a stem block at the input of the backbone, using
smaller-size kernels in the SPP, and adding a P6 output in the PAN block. We
design detectors of different model sizes, from an extra-large model to achieve
the best performance to a super small model for real-time detection on an
embedded or mobile device. Experiment results on the WiderFace dataset show
that on VGA images, our face detectors can achieve state-of-the-art performance
in almost all the Easy, Medium, and Hard subsets, exceeding the more complex
designated face detectors. The code is available at
\url{https://github.com/deepcam-cn/yolov5-face}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14944">
<div class="article-summary-box-inner">
<span><p>Explaining the decisions of an Artificial Intelligence (AI) model is
increasingly critical in many real-world, high-stake applications. Hundreds of
papers have either proposed new feature attribution methods, discussed or
harnessed these tools in their work. However, despite humans being the target
end-users, most attribution methods were only evaluated on proxy
automatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et
al. 2018). In this paper, we conduct the first user study to measure
attribution map effectiveness in assisting humans in ImageNet classification
and Stanford Dogs fine-grained classification, and when an image is natural or
adversarial (i.e., contains adversarial perturbations). Overall, feature
attribution is surprisingly not more effective than showing humans nearest
training-set examples. On a harder task of fine-grained dog categorization,
presenting attribution maps to humans does not help, but instead hurts the
performance of human-AI teams compared to AI alone. Importantly, we found
automatic attribution-map evaluation measures to correlate poorly with the
actual human-AI team performance. Our findings encourage the community to
rigorously test their methods on the downstream human-in-the-loop applications
and to rethink the existing evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Distillation of Confident Knowledge. (arXiv:2106.01489v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01489">
<div class="article-summary-box-inner">
<span><p>Mutual knowledge distillation (MKD) improves a model by distilling knowledge
from another model. However, \textit{not all knowledge is certain and correct},
especially under adverse conditions. For example, label noise usually leads to
less reliable models due to undesired memorization
\cite{zhang2017understanding,arpit2017closer}. Wrong knowledge misleads the
learning rather than helps. This problem can be handled by two aspects: (i)
improving the reliability of a model where the knowledge is from (i.e.,
knowledge source's reliability); (ii) selecting reliable knowledge for
distillation. In the literature, making a model more reliable is widely studied
while selective MKD receives little attention. Therefore, we focus on studying
selective MKD. Concretely, a generic MKD framework, \underline{C}onfident
knowledge selection followed by \underline{M}utual \underline{D}istillation
(CMD), is designed. The key component of CMD is a generic knowledge selection
formulation, making the selection threshold either static (CMD-S) or
progressive (CMD-P). Additionally, CMD covers two special cases: zero-knowledge
and all knowledge, leading to a unified MKD framework. Extensive experiments
are present to demonstrate the effectiveness of CMD and thoroughly justify the
design of CMD. For example, CMD-P obtains new state-of-the-art results in
robustness against label noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-Based Associative Joint Location for Human Pose Estimation in Videos. (arXiv:2107.03591v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03591">
<div class="article-summary-box-inner">
<span><p>Video-based human pose estimation (VHPE) is a vital yet challenging task.
While deep learning methods have made significant progress for the VHPE, most
approaches to this task implicitly model the long-range interaction between
joints by enlarging the receptive field of the convolution. Unlike prior
methods, we design a lightweight and plug-and-play joint relation extractor
(JRE) to model the associative relationship between joints explicitly and
automatically. The JRE takes the pseudo heatmaps of joints as input and
calculates the similarity between pseudo heatmaps. In this way, the JRE
flexibly learns the relationship between any two joints, allowing it to learn
the rich spatial configuration of human poses. Moreover, the JRE can infer
invisible joints according to the relationship between joints, which is
beneficial for the model to locate occluded joints. Then, combined with
temporal semantic continuity modeling, we propose a Relation-based Pose
Semantics Transfer Network (RPSTN) for video-based human pose estimation.
Specifically, to capture the temporal dynamics of poses, the pose semantic
information of the current frame is transferred to the next with a joint
relation guided pose semantics propagator (JRPSP). The proposed model can
transfer the pose semantic features from the non-occluded frame to the occluded
frame, making our method robust to the occlusion. Furthermore, the proposed JRE
module is also suitable for image-based human pose estimation. The proposed
RPSTN achieves state-of-the-art results on the video-based Penn Action dataset,
Sub-JHMDB dataset, and PoseTrack2018 dataset. Moreover, the proposed JRE
improves the performance of backbones on the image-based COCO2017 dataset. Code
is available at https://github.com/YHDang/pose-estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modality specific U-Net variants for biomedical image segmentation: A survey. (arXiv:2107.04537v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04537">
<div class="article-summary-box-inner">
<span><p>With the advent of advancements in deep learning approaches, such as deep
convolution neural network, residual neural network, adversarial network; U-Net
architectures are most widely utilized in biomedical image segmentation to
address the automation in identification and detection of the target regions or
sub-regions. In recent studies, U-Net based approaches have illustrated
state-of-the-art performance in different applications for the development of
computer-aided diagnosis systems for early diagnosis and treatment of diseases
such as brain tumor, lung cancer, alzheimer, breast cancer, etc., using various
modalities. This article contributes in presenting the success of these
approaches by describing the U-Net framework, followed by the comprehensive
analysis of the U-Net variants by performing 1) inter-modality, and 2)
intra-modality categorization to establish better insights into the associated
challenges and solutions. Besides, this article also highlights the
contribution of U-Net based frameworks in the ongoing pandemic, severe acute
respiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19.
Finally, the strengths and similarities of these U-Net variants are analysed
along with the challenges involved in biomedical image segmentation to uncover
promising future research directions in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Network (CNN) vs Vision Transformer (ViT) for Digital Holography. (arXiv:2108.09147v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09147">
<div class="article-summary-box-inner">
<span><p>In Digital Holography (DH), it is crucial to extract the object distance from
a hologram in order to reconstruct its amplitude and phase. This step is called
auto-focusing and it is conventionally solved by first reconstructing a stack
of images and then by sharpening each reconstructed image using a focus metric
such as entropy or variance. The distance corresponding to the sharpest image
is considered the focal position. This approach, while effective, is
computationally demanding and time-consuming. In this paper, the determination
of the distance is performed by Deep Learning (DL). Two deep learning (DL)
architectures are compared: Convolutional Neural Network (CNN) and Vision
Transformer (ViT). ViT and CNN are used to cope with the problem of
auto-focusing as a classification problem. Compared to a first attempt [11] in
which the distance between two consecutive classes was 100$\mu$m, our proposal
allows us to drastically reduce this distance to 1$\mu$m. Moreover, ViT reaches
similar accuracy and is more robust than CNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11845">
<div class="article-summary-box-inner">
<span><p>This letter is concerned with image classification with deep convolutional
neural networks (CNNs). The focus is on the following question: given a set of
candidate CNN models, how to select the right one with the best generalization
property for the current task? Present model selection methods require access
to a batch of labeled data for computing a pre-specified performance metric,
such as the cross-entropy loss, the classification error rate, the negative
log-likelihood. In many practical cases, labels are not available in time as
labeling itself is a time-consuming and expensive task. To this end, this
letter presents an approach to CNN model selection using only unlabeled data.
This method is developed based on a principle termed consistent relative
confidence. The effectiveness and efficiency of the proposed method are
demonstrated by experiments using benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Aggregate and Refine Noisy Labels for Visual Sentiment Analysis. (arXiv:2109.07509v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07509">
<div class="article-summary-box-inner">
<span><p>Visual sentiment analysis has received increasing attention in recent years.
However, the dataset's quality is a concern because the sentiment labels are
crowd-sourcing, subjective, and prone to mistakes, and poses a severe threat to
the data-driven models, especially the deep neural networks. The deep models
would generalize poorly on the testing cases when trained to over-fit the
training samples with noisy sentiment labels. Inspired by the recent progress
on learning with noisy labels, we propose a robust learning method to perform
robust visual sentiment analysis. Our method relies on external memory to
aggregate and filters noisy labels during training. The memory is composed of
the prototypes with corresponding labels, which can be updated online. The
learned prototypes and their labels can be regarded as denoising features and
labels for the local regions and can guide the training process to prevent the
model from overfitting the noisy cases. We establish a benchmark for visual
sentiment analysis with label noise using publicly available datasets. The
experiment results of the proposed benchmark settings comprehensively show the
effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Protecting Face Embeddings in Mobile Face Verification Scenarios. (arXiv:2110.00434v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00434">
<div class="article-summary-box-inner">
<span><p>This paper proposes PolyProtect, a method for protecting the sensitive face
embeddings that are used to represent people's faces in neural-network-based
face verification systems. PolyProtect transforms a face embedding to a more
secure template, using a mapping based on multivariate polynomials
parameterised by user-specific coefficients and exponents. In this work,
PolyProtect is evaluated on two open-source face recognition systems in a
cooperative-user mobile face verification context, under the toughest threat
model that assumes a fully-informed attacker with complete knowledge of the
system and all its parameters. Results indicate that PolyProtect can be tuned
to achieve a satisfactory trade-off between the recognition accuracy of the
PolyProtected face verification system and the irreversibility of the
PolyProtected templates. Furthermore, PolyProtected templates are shown to be
effectively unlinkable, especially if the user-specific parameters employed in
the PolyProtect mapping are selected in a non-naive manner. The evaluation is
conducted using practical methodologies with tangible results, to present
realistic insight into the method's robustness as a face embedding protection
scheme in practice. This work is fully reproducible using the publicly
available code at: https://gitlab.idiap.ch/bob/bob.paper.polyprotect_2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FOD-A: A Dataset for Foreign Object Debris in Airports. (arXiv:2110.03072v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03072">
<div class="article-summary-box-inner">
<span><p>Foreign Object Debris (FOD) detection has attracted increased attention in
the area of machine learning and computer vision. However, a robust and
publicly available image dataset for FOD has not been initialized. To this end,
this paper introduces an image dataset of FOD, named FOD in Airports (FOD-A).
FOD-A object categories have been selected based on guidance from prior
documentation and related research by the Federal Aviation Administration
(FAA). In addition to the primary annotations of bounding boxes for object
detection, FOD-A provides labeled environmental conditions. As such, each
annotation instance is further categorized into three light level categories
(bright, dim, and dark) and two weather categories (dry and wet). Currently,
FOD-A has released 31 object categories and over 30,000 annotation instances.
This paper presents the creation methodology, discusses the publicly available
dataset extension process, and demonstrates the practicality of FOD-A with
widely used machine learning models for object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biometric Template Protection for Neural-Network-based Face Recognition Systems: A Survey of Methods and Evaluation Techniques. (arXiv:2110.05044v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05044">
<div class="article-summary-box-inner">
<span><p>This paper presents a survey of biometric template protection (BTP) methods
for securing face templates in neural-network-based face recognition systems.
The BTP methods are categorised into two types: Non-NN and NN-learned. Non-NN
methods use a neural network (NN) as a feature extractor, but the BTP part is
based on a non-NN algorithm applied at image-level or feature-level. In
contrast, NN-learned methods specifically employ a NN to learn a protected
template from the unprotected face image/features. We present examples of
Non-NN and NN-learned face BTP methods from the literature, along with a
discussion of the two categories' comparative strengths and weaknesses. We also
investigate the techniques used to evaluate these BTP methods, in terms of the
three most common criteria: recognition accuracy, irreversibility, and
renewability/unlinkability. As expected, the recognition accuracy of protected
face recognition systems is generally evaluated using the same (empirical)
techniques employed for evaluating standard (unprotected) biometric systems. On
the contrary, most irreversibility and renewability/unlinkability evaluations
are based on theoretical assumptions/estimates or verbal implications, with no
empirical validation in a practical face recognition context. So, we recommend
a greater focus on empirical evaluation strategies, to provide more concrete
insights into the irreversibility and renewability/unlinkability of face BTP
methods in practice. An exploration of the reproducibility of the studied BTP
works, in terms of the public availability of their implementation code and
evaluation datasets/procedures, suggests that it would currently be difficult
for the BTP community to faithfully replicate (and thus validate) most of the
reported findings. So, we advocate for a push towards reproducibility, in the
hope of furthering our understanding of the face BTP research field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CVAD: A generic medical anomaly detector based on Cascade VAE. (arXiv:2110.15811v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15811">
<div class="article-summary-box-inner">
<span><p>Detecting out-of-distribution (OOD) samples in medical imaging plays an
important role for downstream medical diagnosis. However, existing OOD
detectors are demonstrated on natural images composed of inter-classes and have
difficulty generalizing to medical images. The key issue is the granularity of
OOD data in the medical domain, where intra-class OOD samples are predominant.
We focus on the generalizability of OOD detection for medical images and
propose a self-supervised Cascade Variational autoencoder-based Anomaly
Detector (CVAD). We use a variational autoencoders' cascade architecture, which
combines latent representation at multiple scales, before being fed to a
discriminator to distinguish the OOD data from the in-distribution (ID) data.
Finally, both the reconstruction error and the OOD probability predicted by the
binary discriminator are used to determine the anomalies. We compare the
performance with the state-of-the-art deep learning models to demonstrate our
model's efficacy on various open-access medical imaging datasets for both
intra- and inter-class OOD. Further extensive results on datasets including
common natural datasets show our model's effectiveness and generalizability.
The code is available at https://github.com/XiaoyuanGuo/CVAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Radiograph Representation Learning via Cross-supervision between Images and Free-text Radiology Reports. (arXiv:2111.03452v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03452">
<div class="article-summary-box-inner">
<span><p>Pre-training lays the foundation for recent successes in radiograph analysis
supported by deep learning. It learns transferable image representations by
conducting large-scale fully-supervised or self-supervised learning on a source
domain. However, supervised pre-training requires a complex and labor intensive
two-stage human-assisted annotation process while self-supervised learning
cannot compete with the supervised paradigm. To tackle these issues, we propose
a cross-supervised methodology named REviewing FreE-text Reports for
Supervision (REFERS), which acquires free supervision signals from original
radiology reports accompanying the radiographs. The proposed approach employs a
vision transformer and is designed to learn joint representations from multiple
views within every patient study. REFERS outperforms its transfer learning and
self-supervised learning counterparts on 4 well-known X-ray datasets under
extremely limited supervision. Moreover, REFERS even surpasses methods based on
a source domain of radiographs with human-assisted structured labels. Thus
REFERS has the potential to replace canonical pre-training methodologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iBOT: Image BERT Pre-Training with Online Tokenizer. (arXiv:2111.07832v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07832">
<div class="article-summary-box-inner">
<span><p>The success of language Transformers is primarily attributed to the pretext
task of masked language modeling (MLM), where texts are first tokenized into
semantically meaningful pieces. In this work, we study masked image modeling
(MIM) and indicate the advantages and challenges of using a semantically
meaningful visual tokenizer. We present a self-supervised framework iBOT that
can perform masked prediction with an online tokenizer. Specifically, we
perform self-distillation on masked patch tokens and take the teacher network
as the online tokenizer, along with self-distillation on the class token to
acquire visual semantics. The online tokenizer is jointly learnable with the
MIM objective and dispenses with a multi-stage training pipeline where the
tokenizer needs to be pre-trained beforehand. We show the prominence of iBOT by
achieving an 82.3% linear probing accuracy and an 87.8% fine-tuning accuracy
evaluated on ImageNet-1K. Beyond the state-of-the-art image classification
results, we underline emerging local semantic patterns, which helps the models
to obtain strong robustness against common corruptions and achieve leading
results on dense downstream tasks, eg., object detection, instance
segmentation, and semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient deep learning models for land cover image classification. (arXiv:2111.09451v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09451">
<div class="article-summary-box-inner">
<span><p>The availability of the sheer volume of Copernicus Sentinel-2 imagery has
created new opportunities for exploiting deep learning methods for land use
land cover (LULC) mapping at large scales. However, an extensive set of
benchmark experiments is currently lacking, i.e. deep learning models tested on
the same dataset, with a common and consistent set of metrics, and in the same
hardware. In this work, we use the BigEarthNet Sentinel-2 multispectral dataset
to benchmark for the first time different state-of-the-art deep learning models
for the multi-label, multi-class LULC classification problem, contributing with
an exhaustive zoo of 56 trained models. Our benchmark includes standard
Convolution Neural Network architectures, but we also test non-convolutional
methods, such as Multi-Layer Perceptrons and Vision Transformers. We put to the
test EfficientNets and Wide Residual Networks (WRN) architectures, and leverage
classification accuracy, training time and inference rate. Furthermore, we
propose to use the EfficientNet framework for the compound scaling of a
lightweight WRN, by varying network depth, width, and input data resolution.
Enhanced with an Efficient Channel Attention mechanism, our scaled lightweight
model emerged as the new state-of-the-art. It achieves 4.5% higher averaged
F-Score classification accuracy for all 19 LULC classes compared to a standard
ResNet50 baseline model, with an order of magnitude less trainable parameters.
We provide access to all trained models, along with our code for distributed
training on multiple GPU nodes. This model zoo of pre-trained encoders can be
used for transfer learning and rapid prototyping in different remote sensing
tasks that use Sentinel-2 data, instead of exploiting backbone models trained
with data from a different domain, e.g., from ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D High-Quality Magnetic Resonance Image Restoration in Clinics Using Deep Learning. (arXiv:2111.14259v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14259">
<div class="article-summary-box-inner">
<span><p>Shortening acquisition time and reducing the motion artifacts are two of the
most essential concerns in magnetic resonance imaging. As a promising solution,
deep learning-based high-quality MR image restoration has been investigated to
generate higher resolution and motion artifact-free MR images from lower
resolution images acquired with shortened acquisition time, without costing
additional acquisition time or modifying the pulse sequences. However, numerous
problems still exist to prevent deep learning approaches from becoming
practical in the clinic environment. Specifically, most of the prior works
focus solely on the network model but ignore the impact of various downsampling
strategies on the acquisition time. Besides, the long inference time and high
GPU consumption are also the bottlenecks to deploy most of the prior works in
clinics. Furthermore, prior studies employ random movement in retrospective
motion artifact generation, resulting in uncontrollable severity of motion
artifact. More importantly, doctors are unsure whether the generated MR images
are trustworthy, making diagnosis difficult. To overcome all these problems, we
employed a unified 2D deep learning neural network for both 3D MRI super
resolution and motion artifact reduction, demonstrating such a framework can
achieve better performance in 3D MRI restoration tasks compared to other states
of the art methods and remains the GPU consumption and inference time
significantly low, thus easier to deploy. We also analyzed several downsampling
strategies based on the acceleration factor, including multiple combinations of
in-plane and through-plane downsampling, and developed a controllable and
quantifiable motion artifact generation method. At last, the pixel-wise
uncertainty was calculated and used to estimate the accuracy of the generated
image, providing additional information for reliable diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Prediction of MGMT Promoter Methylation from MRI Scans using Adversarial Learning. (arXiv:2201.04416v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04416">
<div class="article-summary-box-inner">
<span><p>Glioblastoma Multiforme (GBM) is a malignant brain cancer forming around 48%
of al brain and Central Nervous System (CNS) cancers. It is estimated that
annually over 13,000 deaths occur in the US due to GBM, making it crucial to
have early diagnosis systems that can lead to predictable and effective
treatment. The most common treatment after GBM diagnosis is chemotherapy, which
works by sending rapidly dividing cells to apoptosis. However, this form of
treatment is not effective when the MGMT promoter sequence is methylated, and
instead leads to severe side effects decreasing patient survivability.
Therefore, it is important to be able to identify the MGMT promoter methylation
status through non-invasive magnetic resonance imaging (MRI) based machine
learning (ML) models. This is accomplished using the Brain Tumor Segmentation
(BraTS) 2021 dataset, which was recently used for an international Kaggle
competition. We developed four primary models - two radiomic models and two CNN
models - each solving the binary classification task with progressive
improvements. We built a novel ML model termed as the Intermediate State
Generator which was used to normalize the slice thicknesses of all MRI scans.
With further improvements, our best model was able to achieve performance
significantly ($p &lt; 0.05$) better than the best performing Kaggle model with a
6% increase in average cross-validation accuracy. This improvement could
potentially lead to a more informed choice of chemotherapy as a treatment
option, prolonging lives of thousands of patients with GBM each year.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Robust are Discriminatively Trained Zero-Shot Learning Models?. (arXiv:2201.10972v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10972">
<div class="article-summary-box-inner">
<span><p>Data shift robustness has been primarily investigated from a fully supervised
perspective, and robustness of zero-shot learning (ZSL) models have been
largely neglected. In this paper, we present novel analyses on the robustness
of discriminative ZSL to image corruptions. We subject several ZSL models to a
large set of common corruptions and defenses. In order to realize the
corruption analysis, we curate and release the first ZSL corruption robustness
datasets SUN-C, CUB-C and AWA2-C. We analyse our results by taking into account
the dataset characteristics, class imbalance, class transitions between seen
and unseen classes and the discrepancies between ZSL and GZSL performances. Our
results show that discriminative ZSL suffers from corruptions and this trend is
further exacerbated by the severe class imbalance and model weakness inherent
in ZSL methods. We then combine our findings with those based on adversarial
attacks in ZSL, and highlight the different effects of corruptions and
adversarial examples, such as the pseudo-robustness effect present under
adversarial attacks. We also obtain new strong baselines for both models with
the defense methods. Finally, our experiments show that although existing
methods to improve robustness somewhat work for ZSL models, they do not produce
a tangible effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Attribute Balanced Sampling for Disentangled GAN Controls. (arXiv:2111.00909v2 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00909">
<div class="article-summary-box-inner">
<span><p>Various controls over the generated data can be extracted from the latent
space of a pre-trained GAN, as it implicitly encodes the semantics of the
training data. The discovered controls allow to vary semantic attributes in the
generated images but usually lead to entangled edits that affect multiple
attributes at the same time. Supervised approaches typically sample and
annotate a collection of latent codes, then train classifiers in the latent
space to identify the controls. Since the data generated by GANs reflects the
biases of the original dataset, so do the resulting semantic controls. We
propose to address disentanglement by subsampling the generated data to remove
over-represented co-occuring attributes thus balancing the semantics of the
dataset before training the classifiers. We demonstrate the effectiveness of
this approach by extracting disentangled linear directions for face
manipulation on two popular GAN architectures, PGGAN and StyleGAN, and two
datasets, CelebAHQ and FFHQ. We show that this approach outperforms
state-of-the-art classifier-based methods while avoiding the need for
disentanglement-enforcing post-processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jacobian Computation for Cumulative B-splines on SE(3) and Application to Continuous-Time Object Tracking. (arXiv:2201.10602v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10602">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a method that estimates the $SE(3)$ continuous
trajectories (orientation and translation) of the dynamic rigid objects present
in a scene, from multiple RGB-D views. Specifically, we fit the object
trajectories to cumulative B-Splines curves, which allow us to interpolate, at
any intermediate time stamp, not only their poses but also their linear and
angular velocities and accelerations. Additionally, we derive in this work the
analytical $SE(3)$ Jacobians needed by the optimization, being applicable to
any other approach that uses this type of curves. To the best of our knowledge
this is the first work that proposes 6-DoF continuous-time object tracking,
which we endorse with significant computational cost reduction thanks to our
analytical derivations. We evaluate our proposal in synthetic data and in a
public benchmark, showing competitive results in localization and significant
improvements in velocity estimation in comparison to discrete-time approaches.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-28 23:07:06.110928204 UTC">2022-01-28 23:07:06 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>