<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-07T01:30:00Z">03-07</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages. (arXiv:2203.01976v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01976">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language models such as mBERT and XLM-R have
demonstrated great potential for zero-shot cross-lingual transfer to low
web-resource languages (LRL). However, due to limited model capacity, the large
difference in the sizes of available monolingual corpora between high
web-resource languages (HRL) and LRLs does not provide enough scope of
co-embedding the LRL with the HRL, thereby affecting downstream task
performance of LRLs. In this paper, we argue that relatedness among languages
in a language family along the dimension of lexical overlap may be leveraged to
overcome some of the corpora limitations of LRLs. We propose Overlap BPE
(OBPE), a simple yet effective modification to the BPE vocabulary generation
algorithm which enhances overlap across related languages. Through extensive
experiments on multiple NLP tasks and datasets, we observe that OBPE generates
a vocabulary that increases the representation of LRLs via tokens shared with
HRLs. This results in improved zero-shot transfer from related HRLs to LRLs
without reducing HRL representation and accuracy. Unlike previous studies that
dismissed the importance of token-overlap, we show that in the low-resource
related language setting, token overlap matters. Synthetically reducing the
overlap to zero can cause as much as a four-fold drop in zero-shot transfer
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations. (arXiv:2203.02013v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02013">
<div class="article-summary-box-inner">
<span><p>The ability for a human to understand an Artificial Intelligence (AI) model's
decision-making process is critical in enabling stakeholders to visualize model
behavior, perform model debugging, promote trust in AI models, and assist in
collaborative human-AI decision-making. As a result, the research fields of
interpretable and explainable AI have gained traction within AI communities as
well as interdisciplinary scientists seeking to apply AI in their subject
areas. In this paper, we focus on advancing the state-of-the-art in
interpreting multimodal models - a class of machine learning methods that
tackle core challenges in representing and capturing interactions between
heterogeneous data sources such as images, text, audio, and time-series data.
Multimodal models have proliferated numerous real-world applications across
healthcare, robotics, multimedia, affective computing, and human-computer
interaction. By performing model disentanglement into unimodal contributions
(UC) and multimodal interactions (MI), our proposed approach, DIME, enables
accurate and fine-grained analysis of multimodal models while maintaining
generality across arbitrary modalities, model architectures, and tasks. Through
a comprehensive suite of experiments on both synthetic and real-world
multimodal tasks, we show that DIME generates accurate disentangled
explanations, helps users of multimodal models gain a deeper understanding of
model behavior, and presents a step towards debugging and improving these
models for real-world deployment. Code for our experiments can be found at
https://github.com/lvyiwei1/DIME.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning. (arXiv:2203.02053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02053">
<div class="article-summary-box-inner">
<span><p>We present modality gap, an intriguing geometric phenomenon of the
representation space of multi-modal models. Specifically, we show that
different data modalities (e.g. images and text) are embedded at arm's length
in their shared representation in multi-modal models such as CLIP. Our
systematic analysis demonstrates that this gap is caused by a combination of
model initialization and contrastive learning optimization. In model
initialization, we show empirically and theoretically that the representation
of a common deep neural network is restricted to a narrow cone. As a
consequence, in a multi-modal model with two encoders, the representations of
the two modalities are clearly apart when the model is initialized. During
optimization, contrastive learning keeps the different modalities separate by a
certain distance, which is influenced by the temperature parameter in the loss
function. Our experiments further demonstrate that varying the modality gap
distance has a significant impact in improving the model's downstream zero-shot
classification performance and fairness. Our code and data are available at
https://modalitygap.readthedocs.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Latent-Variable Models for Text Generation. (arXiv:2203.02055v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02055">
<div class="article-summary-box-inner">
<span><p>Text generation aims to produce human-like natural language output for
down-stream tasks. It covers a wide range of applications like machine
translation, document summarization, dialogue generation and so on. Recently
deep neural network-based end-to-end architectures have been widely adopted.
The end-to-end approach conflates all sub-modules, which used to be designed by
complex handcrafted rules, into a holistic encode-decode architecture. Given
enough training data, it is able to achieve state-of-the-art performance yet
avoiding the need of language/domain-dependent knowledge. Nonetheless, deep
learning models are known to be extremely data-hungry, and text generated from
them usually suffer from low diversity, interpretability and controllability.
As a result, it is difficult to trust the output from them in real-life
applications. Deep latent-variable models, by specifying the probabilistic
distribution over an intermediate latent process, provide a potential way of
addressing these problems while maintaining the expressive power of deep neural
networks. This dissertation presents how deep latent-variable models can
improve over the standard encoder-decoder model for text generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Lexical Hypothesis: Identifying personality structure in natural language. (arXiv:2203.02092v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02092">
<div class="article-summary-box-inner">
<span><p>Recent advances in natural language processing (NLP) have produced general
models that can perform complex tasks such as summarizing long passages and
translating across languages. Here, we introduce a method to extract adjective
similarities from language models as done with survey-based ratings in
traditional psycholexical studies but using millions of times more text in a
natural setting. The correlational structure produced through this method is
highly similar to that of self- and other-ratings of 435 terms reported by
Saucier and Goldberg (1996a). The first three unrotated factors produced using
NLP are congruent with those in survey data, with coefficients of 0.89, 0.79,
and 0.79. This structure is robust to many modeling decisions: adjective set,
including those with 1,710 terms (Goldberg, 1982) and 18,000 terms (Allport &amp;
Odbert, 1936); the query used to extract correlations; and language model.
Notably, Neuroticism and Openness are only weakly and inconsistently recovered.
This is a new source of signal that is closer to the original (semantic) vision
of the Lexical Hypothesis. The method can be applied where surveys cannot: in
dozens of languages simultaneously, with tens of thousands of items, on
historical text, and at extremely large scale for little cost. The code is made
public to facilitate reproduction and fast iteration in new directions of
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiteTransformerSearch: Training-free On-device Search for Efficient Autoregressive Language Models. (arXiv:2203.02094v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02094">
<div class="article-summary-box-inner">
<span><p>The transformer architecture is ubiquitously used as the building block of
most large-scale language models. However, it remains a painstaking guessing
game of trial and error to set its myriad of architectural hyperparameters,
e.g., number of layers, number of attention heads, and inner size of the feed
forward network, and find architectures with the optimal trade-off between task
performance like perplexity and compute constraints like memory and latency.
This challenge is further exacerbated by the proliferation of various hardware.
In this work, we leverage the somewhat surprising empirical observation that
the number of non-embedding parameters in autoregressive transformers has a
high rank correlation with task performance, irrespective of the architectural
hyperparameters. Since architectural hyperparameters affect the latency and
memory footprint in a hardware-dependent manner, the above observation
organically induces a simple search algorithm that can be directly run on
target devices. We rigorously show that the latency and perplexity
pareto-frontier can be found without need for any model training, using
non-embedding parameters as a proxy for perplexity. We evaluate our method,
dubbed Lightweight Transformer Search (LTS), on diverse devices from ARM CPUs
to Nvidia GPUs and show that the perplexity of Transformer-XL can be achieved
with up to 2x lower latency. LTS extracts the pareto-frontier in less than 3
hours while running on a commodity laptop. We effectively remove the carbon
footprint of training for hundreds of GPU hours, offering a strong simple
baseline for future NAS methods in autoregressive language modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In the Service of Online Order: Tackling Cyber-Bullying with Machine Learning and Affect Analysis. (arXiv:2203.02116v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02116">
<div class="article-summary-box-inner">
<span><p>One of the burning problems lately in Japan has been cyber-bullying, or
slandering and bullying people online. The problem has been especially noticed
on unofficial Web sites of Japanese schools. Volunteers consisting of school
personnel and PTA (Parent-Teacher Association) members have started Online
Patrol to spot malicious contents within Web forums and blogs. In practise,
Online Patrol assumes reading through the whole Web contents, which is a task
difficult to perform manually. With this paper we introduce a research intended
to help PTA members perform Online Patrol more efficiently. We aim to develop a
set of tools that can automatically detect malicious entries and report them to
PTA members. First, we collected cyber-bullying data from unofficial school Web
sites. Then we performed analysis of this data in two ways. Firstly, we
analysed the entries with a multifaceted affect analysis system in order to
find distinctive features for cyber-bullying and apply them to a machine
learning classifier. Secondly, we applied a SVM based machine learning method
to train a classifier for detection of cyber-bullying. The system was able to
classify cyber-bullying entries with 88.2% of balanced F-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation. (arXiv:2203.02135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02135">
<div class="article-summary-box-inner">
<span><p>Existing continual relation learning (CRL) methods rely on plenty of labeled
training data for learning a new task, which can be hard to acquire in real
scenario as getting large and representative labeled data is often expensive
and time-consuming. It is therefore necessary for the model to learn novel
relational patterns with very few labeled data while avoiding catastrophic
forgetting of previous task knowledge. In this paper, we formulate this
challenging yet practical problem as continual few-shot relation learning
(CFRL). Based on the finding that learning for new emerging few-shot tasks
often results in feature distributions that are incompatible with previous
tasks' learned distributions, we propose a novel method based on embedding
space regularization and data augmentation. Our method generalizes to new
few-shot tasks and avoids catastrophic forgetting of previous tasks by
enforcing extra constraints on the relational embeddings and by adding extra
{relevant} data in a self-supervised manner. With extensive experiments we
demonstrate that our method can significantly outperform previous
state-of-the-art methods in CFRL task settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training language models to follow instructions with human feedback. (arXiv:2203.02155v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02155">
<div class="article-summary-box-inner">
<span><p>Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models. (arXiv:2203.02167v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02167">
<div class="article-summary-box-inner">
<span><p>Knowledge graph completion (KGC) aims to reason over known facts and infer
the missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn
entity representations from natural language descriptions, and have the
potential for inductive KGC. However, the performance of text-based methods
still largely lag behind graph embedding-based methods like TransE (Bordes et
al., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the
key issue is efficient contrastive learning. To improve the learning
efficiency, we introduce three types of negatives: in-batch negatives,
pre-batch negatives, and self-negatives which act as a simple form of hard
negatives. Combined with InfoNCE loss, our proposed model SimKGC can
substantially outperform embedding-based methods on several benchmark datasets.
In terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19%
on WN18RR, +6.8% on the Wikidata5M transductive setting, and +22% on the
Wikidata5M inductive setting. Thorough analyses are conducted to gain insights
into each component. Our code is available at
https://github.com/intfloat/SimKGC .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCNet: Graph Completion Network for Incomplete Multimodal Learning in Conversation. (arXiv:2203.02177v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02177">
<div class="article-summary-box-inner">
<span><p>Conversations have become a critical data format on social media platforms.
Understanding conversation from emotion, content, and other aspects also
attracts increasing attention from researchers due to its widespread
application in human-computer interaction. In real-world environments, we often
encounter the problem of incomplete modalities, which has become a core issue
of conversation understanding. To address this problem, researchers propose
various methods. However, existing approaches are mainly designed for
individual utterances or medical images rather than conversational data, which
cannot exploit temporal and speaker information in conversations. To this end,
we propose a novel framework for incomplete multimodal learning in
conversations, called "Graph Complete Network (GCNet)", filling the gap of
existing works. Our GCNet contains two well-designed graph neural network-based
modules, "Speaker GNN" and "Temporal GNN", to capture temporal and speaker
information in conversations. To make full use of complete and incomplete data
in feature learning, we jointly optimize classification and reconstruction in
an end-to-end manner. To verify the effectiveness of our method, we conduct
experiments on three benchmark conversational datasets. Experimental results
demonstrate that our GCNet is superior to existing state-of-the-art approaches
in incomplete multimodal learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EAG: Extract and Generate Multi-way Aligned Corpus for Complete Multi-lingual Neural Machine Translation. (arXiv:2203.02180v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02180">
<div class="article-summary-box-inner">
<span><p>Complete Multi-lingual Neural Machine Translation (C-MNMT) achieves superior
performance against the conventional MNMT by constructing multi-way aligned
corpus, i.e., aligning bilingual training examples from different language
pairs when either their source or target sides are identical. However, since
exactly identical sentences from different language pairs are scarce, the power
of the multi-way aligned corpus is limited by its scale. To handle this
problem, this paper proposes "Extract and Generate" (EAG), a two-step approach
to construct large-scale and high-quality multi-way aligned corpus from
bilingual data. Specifically, we first extract candidate aligned examples by
pairing the bilingual examples from different language pairs with highly
similar source or target sentences; and then generate the final aligned
examples from the candidates with a well-trained generation model. With this
two-step pipeline, EAG can construct a large-scale and multi-way aligned corpus
whose diversity is almost identical to the original bilingual corpus.
Experiments on two publicly available datasets i.e., WMT-5 and OPUS-100, show
that the proposed method achieves significant improvements over strong
baselines, with +1.1 and +1.4 BLEU points improvements on the two datasets
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification. (arXiv:2203.02225v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02225">
<div class="article-summary-box-inner">
<span><p>Generating new events given context with correlated ones plays a crucial role
in many event-centric reasoning tasks. Existing works either limit their scope
to specific scenarios or overlook event-level correlations. In this paper, we
propose to pre-train a general Correlation-aware context-to-Event Transformer
(ClarET) for event-centric reasoning. To achieve this, we propose three novel
event-centric objectives, i.e., whole event recovering, contrastive
event-correlation encoding and prompt-based event locating, which highlight
event-level correlations with effective training. The proposed ClarET is
applicable to a wide range of event-centric reasoning scenarios, considering
its versatility of (i) event-correlation types (e.g., causal, temporal,
contrast), (ii) application formulations (i.e., generation and classification),
and (iii) reasoning types (e.g., abductive, counterfactual and ending
reasoning). Empirical fine-tuning results, as well as zero- and few-shot
learning, on 9 benchmarks (5 generation and 4 classification tasks covering 4
reasoning types with diverse event correlations), verify its effectiveness and
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IISERB Brains at SemEval 2022 Task 6: A Deep-learning Framework to Identify Intended Sarcasm in English. (arXiv:2203.02244v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02244">
<div class="article-summary-box-inner">
<span><p>This paper describes the system architectures and the models submitted by our
team "IISERBBrains" to SemEval 2022 Task 6 competition. We contested for all
three sub-tasks floated for the English dataset. On the leader-board, wegot19th
rank out of43 teams for sub-taskA, the 8th rank out of22 teams for sub-task
B,and13th rank out of 16 teams for sub-taskC. Apart from the submitted results
and models, we also report the other models and results that we obtained
through our experiments after organizers published the gold labels of their
evaluation data
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Discounting of Implicit Language Models in RNN-Transducers. (arXiv:2203.02317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02317">
<div class="article-summary-box-inner">
<span><p>RNN-Transducer (RNN-T) models have become synonymous with streaming
end-to-end ASR systems. While they perform competitively on a number of
evaluation categories, rare words pose a serious challenge to RNN-T models. One
main reason for the degradation in performance on rare words is that the
language model (LM) internal to RNN-Ts can become overconfident and lead to
hallucinated predictions that are acoustically inconsistent with the underlying
speech. To address this issue, we propose a lightweight adaptive LM discounting
technique AdaptLMD, that can be used with any RNN-T architecture without
requiring any external resources or additional parameters. AdaptLMD uses a
two-pronged approach: 1) Randomly mask the prediction network output to
encourage the RNN-T to not be overly reliant on it's outputs. 2) Dynamically
choose when to discount the implicit LM (ILM) based on rarity of recently
predicted tokens and divergence between ILM and implicit acoustic model (IAM)
scores. Comparing AdaptLMD to a competitive RNN-T baseline, we obtain up to 4%
and 14% relative reductions in overall WER and rare word PER, respectively, on
a conversational, code-mixed Hindi-English ASR task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations. (arXiv:2203.02385v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02385">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition in Conversations (ERC) has considerable prospects for
developing empathetic machines. For multimodal ERC, it is vital to understand
context and fuse modality information in conversations. Recent graph-based
fusion methods generally aggregate multimodal information by exploring unimodal
and cross-modal interactions in a graph. However, they accumulate redundant
information at each layer, limiting the context understanding between
modalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network
(MM-DFN) to recognize emotions by fully understanding multimodal conversational
context. Specifically, we design a new graph-based dynamic fusion module to
fuse multimodal contextual features in a conversation. The module reduces
redundancy and enhances complementarity between modalities by capturing the
dynamics of contextual information in different semantic spaces. Extensive
experiments on two public benchmark datasets demonstrate the effectiveness and
superiority of MM-DFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Plain Toxic: Detection of Inappropriate Statements on Flammable Topics for the Russian Language. (arXiv:2203.02392v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02392">
<div class="article-summary-box-inner">
<span><p>Toxicity on the Internet, such as hate speech, offenses towards particular
users or groups of people, or the use of obscene words, is an acknowledged
problem. However, there also exist other types of inappropriate messages which
are usually not viewed as toxic, e.g. as they do not contain explicit offences.
Such messages can contain covered toxicity or generalizations, incite harmful
actions (crime, suicide, drug use), provoke "heated" discussions. Such messages
are often related to particular sensitive topics, e.g. on politics, sexual
minorities, social injustice which more often than other topics, e.g. cars or
computing, yield toxic emotional reactions. At the same time, clearly not all
messages within such flammable topics are inappropriate.
</p>
<p>Towards this end, in this work, we present two text collections labelled
according to binary notion of inapropriateness and a multinomial notion of
sensitive topic. Assuming that the notion of inappropriateness is common among
people of the same culture, we base our approach on human intuitive
understanding of what is not acceptable and harmful. To objectivise the notion
of inappropriateness, we define it in a data-driven way though crowdsourcing.
Namely we run a large-scale annotation study asking workers if a given chatbot
textual statement could harm reputation of a company created it. Acceptably
high values of inter-annotator agreement suggest that the notion of
inappropriateness exists and can be uniformly understood by different people.
To define the notion of sensitive topics in an objective way we use on
guidelines suggested commonly by specialists of legal and PR department of a
large public company as potentially harmful.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comprehension of Subtitles from Re-Translating Simultaneous Speech Translation. (arXiv:2203.02458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02458">
<div class="article-summary-box-inner">
<span><p>In simultaneous speech translation, one can vary the size of the output
window, system latency and sometimes the allowed level of rewriting. The effect
of these properties on readability and comprehensibility has not been tested
with modern neural translation systems. In this work, we propose an evaluation
method and investigate the effects on comprehension and user preferences. It is
a pilot study with 14 users on 2 hours of German documentaries or speeches with
online translations into Czech. We collect continuous feedback and answers on
factual questions. Our results show that the subtitling layout or flicker have
a little effect on comprehension, in contrast to machine translation itself and
individual competence. Other results show that users with a limited knowledge
of the source language have different preferences to stability and latency than
the users with zero knowledge. The results are statistically insignificant,
however, we show that our method works and can be reproduced in larger volume.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Simultaneous to Streaming Machine Translation by Leveraging Streaming History. (arXiv:2203.02459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02459">
<div class="article-summary-box-inner">
<span><p>Simultaneous Machine Translation is the task of incrementally translating an
input sentence before it is fully available. Currently, simultaneous
translation is carried out by translating each sentence independently of the
previously translated text. More generally, Streaming MT can be understood as
an extension of Simultaneous MT to the incremental translation of a continuous
input text stream. In this work, a state-of-the-art simultaneous sentence-level
MT system is extended to the streaming setup by leveraging the streaming
history. Extensive empirical results are reported on IWSLT Translation Tasks,
showing that leveraging the streaming history leads to significant quality
gains. In particular, the proposed system proves to compare favorably to the
best performing systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding Comparator: Visualizing Differences in Global Structure and Local Neighborhoods via Small Multiples. (arXiv:1912.04853v3 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.04853">
<div class="article-summary-box-inner">
<span><p>Embeddings mapping high-dimensional discrete input to lower-dimensional
continuous vector spaces have been widely adopted in machine learning
applications as a way to capture domain semantics. Interviewing 13 embedding
users across disciplines, we find comparing embeddings is a key task for
deployment or downstream analysis but unfolds in a tedious fashion that poorly
supports systematic exploration. In response, we present the Embedding
Comparator, an interactive system that presents a global comparison of
embedding spaces alongside fine-grained inspection of local neighborhoods. It
systematically surfaces points of comparison by computing the similarity of the
$k$-nearest neighbors of every embedded object between a pair of spaces.
Through case studies across multiple modalities, we demonstrate our system
rapidly reveals insights, such as semantic changes following fine-tuning,
language changes over time, and differences between seemingly similar models.
In evaluations with 15 participants, we find our system accelerates comparisons
by shifting from laborious manual specification to browsing and manipulating
visualizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Classifying Continuous Constraint Satisfaction Problems. (arXiv:2106.02397v3 [cs.CC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02397">
<div class="article-summary-box-inner">
<span><p>A continuous constraint satisfaction problem (CCSP) is a constraint
satisfaction problem (CSP) with an interval domain $U \subset \mathbb{R}$. We
engage in a systematic study to classify CCSPs that are complete of the
Existential Theory of the Reals, i.e., ER-complete. To define this class, we
first consider the problem ETR, which also stands for Existential Theory of the
Reals. In an instance of this problem we are given some sentence of the form
$\exists x_1, \ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$, where
$\Phi$ is a well-formed quantifier-free formula consisting of the symbols $\{0,
1, +, \cdot, \geq, &gt;, \wedge, \vee, \neg\}$, the goal is to check whether this
sentence is true. Now the class ER is the family of all problems that admit a
polynomial-time many-one reduction to ETR. It is known that NP $\subseteq$ ER
$\subseteq$ PSPACE.
</p>
<p>We restrict our attention on CCSPs with addition constraints ($x + y = z$)
and some other mild technical condition. Previously, it was shown that
multiplication constraints ($x \cdot y = z$), squaring constraints ($x^2 = y$),
or inversion constraints ($x\cdot y = 1$) are sufficient to establish
ER-completeness. We extend this in the strongest possible sense for equality
constraints as follows. We show that CCSPs (with addition constraints and some
other mild technical condition) that have any one well-behaved curved equality
constraint ($f(x,y) = 0$) are ER-complete. We further extend our results to
inequality constraints. We show that any well-behaved convexly curved and any
well-behaved concavely curved inequality constraint ($f(x,y) \geq 0$ and
$g(x,y) \geq 0$) imply ER-completeness on the class of such CCSPs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear-time calculation of the expected sum of edge lengths in random projective linearizations of trees. (arXiv:2107.03277v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03277">
<div class="article-summary-box-inner">
<span><p>The syntactic structure of a sentence is often represented using syntactic
dependency trees. The sum of the distances between syntactically related words
has been in the limelight for the past decades. Research on dependency
distances led to the formulation of the principle of dependency distance
minimization whereby words in sentences are ordered so as to minimize that sum.
Numerous random baselines have been defined to carry out related quantitative
studies on languages. The simplest random baseline is the expected value of the
sum in unconstrained random permutations of the words in the sentence, namely
when all the shufflings of the words of a sentence are allowed and equally
likely. Here we focus on a popular baseline: random projective permutations of
the words of the sentence, that is, permutations where the syntactic dependency
structure is projective, a formal constraint that sentences satisfy often in
languages. Thus far, the expectation of the sum of dependency distances in
random projective shufflings of a sentence has been estimated approximately
with a Monte Carlo procedure whose cost is of the order of $Rn$, where $n$ is
the number of words of the sentence and $R$ is the number of samples; it is
well known that the larger $R$, the lower the error of the estimation but the
larger the time cost. Here we present formulae to compute that expectation
without error in time of the order of $n$. Furthermore, we show that star trees
maximize it, and give an algorithm to retrieve the trees that minimize it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Sentence Classification: Detecting Sustainability Initiatives in Company Reports. (arXiv:2110.03727v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03727">
<div class="article-summary-box-inner">
<span><p>We introduce the novel task of detecting sustainability initiatives in
company reports. Given a full report, the aim is to automatically identify
mentions of practical activities that a company has performed in order to
tackle specific societal issues. New methods for identifying continuous
sentence spans need to be developed for capturing the multi-sentence structure
of individual sustainability initiatives. We release a new dataset of company
reports in which the text has been manually annotated with sustainability
initiatives. We also evaluate different models for initiative detection,
introducing a novel aggregation and evaluation methodology. Our proposed
architecture uses sequences of consecutive sentences to account for contextual
information when making classification decisions at the individual sentence
level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPstyler: Image Style Transfer with a Single Text Condition. (arXiv:2112.00374v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00374">
<div class="article-summary-box-inner">
<span><p>Existing neural style transfer methods require reference style images to
transfer texture information of style images to content images. However, in
many practical situations, users may not have reference style images but still
be interested in transferring styles by just imagining them. In order to deal
with such applications, we propose a new framework that enables a style
transfer `without' a style image, but only with a text description of the
desired style. Using the pre-trained text-image embedding model of CLIP, we
demonstrate the modulation of the style of content images only with a single
text condition. Specifically, we propose a patch-wise text-image matching loss
with multiview augmentations for realistic texture transfer. Extensive
experimental results confirmed the successful image style transfer with
realistic textures that reflect semantic query texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An unsupervised extractive summarization method based on multi-round computation. (arXiv:2112.03203v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03203">
<div class="article-summary-box-inner">
<span><p>Text summarization methods have attracted much attention all the time. In
recent years, deep learning has been applied to text summarization, and it
turned out to be pretty effective. However, most of the current text
summarization methods based on deep learning need large-scale datasets, which
is difficult to achieve in practical applications. In this paper, an
unsupervised extractive text summarization method based on multi-round
calculation is proposed. Based on the directed graph algorithm, we change the
traditional method of calculating the sentence ranking at one time to
multi-round calculation, and the summary sentences are dynamically optimized
after each round of calculation to better match the characteristics of the
text. In this paper, experiments are carried out on four data sets, each
separately containing Chinese, English, long and short texts. The experiment
results show that our method has better performance than both baseline methods
and other unsupervised methods and is robust on different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViNMT: Neural Machine Translation Toolkit. (arXiv:2112.15272v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15272">
<div class="article-summary-box-inner">
<span><p>We present an open-source toolkit for neural machine translation (NMT). The
new toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along
with many other improvements detailed below, in order to create a
self-contained, simple to use, consistent and comprehensive framework for
Machine Translation tasks of various domains. It is tooled to support both
bilingual and multilingual translation tasks, starting from building the model
from respective corpora, to inferring new predictions or packaging the model to
serving-capable JIT format.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Planck Radiation and Quantization Scheme for Human Cognition and Language. (arXiv:2201.03306v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03306">
<div class="article-summary-box-inner">
<span><p>As a result of the identification of 'identity' and 'indistinguishability'
and strong experimental evidence for the presence of the associated
Bose-Einstein statistics in human cognition and language, we argued in previous
work for an extension of the research domain of quantum cognition. In addition
to quantum complex vector spaces and quantum probability models, we showed that
quantization itself, with words as quanta, is relevant and potentially
important to human cognition. In the present work, we build on this result, and
introduce a powerful radiation quantization scheme for human cognition. We show
that the lack of independence of the Bose-Einstein statistics compared to the
Maxwell-Boltzmann statistics can be explained by the presence of a 'meaning
dynamics', which causes words to be attracted to the same words. And so words
clump together in the same states, a phenomenon well known for photons in the
early years of quantum mechanics, leading to fierce disagreements between
Planck and Einstein. Using a simple example, we introduce all the elements to
get a better and detailed view of this 'meaning dynamics', such as micro and
macro states, and Maxwell-Boltzmann, Bose-Einstein and Fermi-Dirac numbers and
weights, and compare this example and its graphs, with the radiation
quantization scheme of a Winnie the Pooh story, also with its graphs. By
connecting a concept directly to human experience, we show that entanglement is
a necessity for preserving the 'meaning dynamics' we identified, and it becomes
clear in what way Fermi-Dirac addresses human memory. Within the human mind, as
a crucial aspect of memory, in spaces with internal parameters, identical words
can nevertheless be assigned different states and hence realize locally and
contextually the necessary distinctiveness, structured by a Pauli exclusion
principle, for human thought to thrive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-assisted prompt editing to improve GPT-3 after deployment. (arXiv:2201.06009v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06009">
<div class="article-summary-box-inner">
<span><p>Large LMs such as GPT-3 are powerful, but can commit mistakes that are
obvious to humans. For example, GPT-3 would mistakenly interpret "What word is
similar to good?" to mean a homonym, while the user intended a synonym. Our
goal is to effectively correct such errors via user interactions with the
system but without retraining, which will be prohibitively costly. We pair
GPT-3 with a growing memory of recorded cases where the model misunderstood the
user's intents, along with user feedback for clarification. Such a memory
allows our system to produce enhanced prompts for any new query based on the
user feedback for error correction on similar cases in the past. On four tasks
(two lexical tasks, two advanced ethical reasoning tasks), we show how a
(simulated) user can interactively teach a deployed GPT-3, substantially
increasing its accuracy over the queries with different kinds of
misunderstandings by the GPT-3. Our approach is a step towards the low-cost
utility enhancement for very large pre-trained LMs. All the code and data is
available at https://github.com/madaan/memprompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TourBERT: A pretrained language model for the tourism industry. (arXiv:2201.07449v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07449">
<div class="article-summary-box-inner">
<span><p>The Bidirectional Encoder Representations from Transformers (BERT) is
currently one of the most important and state-of-the-art models for natural
language. However, it has also been shown that for domain-specific tasks it is
helpful to pretrain BERT on a domain-specific corpus. In this paper, we present
TourBERT, a pretrained language model for tourism. We describe how TourBERT was
developed and evaluated. The evaluations show that TourBERT is outperforming
BERT in all tourism-specific tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure Extraction in Task-Oriented Dialogues with Slot Clustering. (arXiv:2203.00073v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00073">
<div class="article-summary-box-inner">
<span><p>Extracting structure information from dialogue data can help us better
understand user and system behaviors. In task-oriented dialogues, dialogue
structure has often been considered as transition graphs among dialogue states.
However, annotating dialogue states manually is expensive and time-consuming.
In this paper, we propose a simple yet effective approach for structure
extraction in task-oriented dialogues. We first detect and cluster possible
slot tokens with a pre-trained model to approximate dialogue ontology for a
target domain. Then we track the status of each identified token group and
derive a state transition structure. Empirical results show that our approach
outperforms unsupervised baseline models by far in dialogue structure
extraction. In addition, we show that data augmentation based on extracted
structures enriches the surface formats of training data and can achieve a
significant performance boost in dialogue response generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. (arXiv:2203.01311v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01311">
<div class="article-summary-box-inner">
<span><p>Learning multimodal representations involves discovering correspondences and
integrating information from multiple heterogeneous sources of data. While
recent research has begun to explore the design of more general-purpose
multimodal models (contrary to prior focus on domain and modality-specific
architectures), these methods are still largely focused on a small set of
modalities in the language, vision, and audio space. In order to accelerate
generalization towards diverse and understudied modalities, we investigate
methods for high-modality (a large set of diverse modalities) and
partially-observable (each task only defined on a small subset of modalities)
scenarios. To tackle these challenges, we design a general multimodal model
that enables multitask and transfer learning: multitask learning with shared
parameters enables stable parameter counts (addressing scalability), and
cross-modal transfer learning enables information sharing across modalities and
tasks (addressing partial observability). Our resulting model generalizes
across text, image, video, audio, time-series, sensors, tables, and set
modalities from different research areas, improves the tradeoff between
performance and efficiency, transfers to new modalities and tasks, and reveals
surprising insights on the nature of information sharing in multitask models.
We release our code and benchmarks which we hope will present a unified
platform for subsequent theoretical and empirical analysis:
https://github.com/pliang279/HighMMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives. (arXiv:2203.01445v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01445">
<div class="article-summary-box-inner">
<span><p>The volume of available data has grown dramatically in recent years in many
applications. Furthermore, the age of networks that used multiple modalities
separately has practically ended. Therefore, enabling bidirectional
cross-modality data retrieval capable of processing has become a requirement
for many domains and disciplines of research. This is especially true in the
medical field, as data comes in a multitude of types, including various types
of images and reports as well as molecular data. Most contemporary works apply
cross attention to highlight the essential elements of an image or text in
relation to the other modalities and try to match them together. However,
regardless of their importance in their own modality, these approaches usually
consider features of each modality equally. In this study, self-attention as an
additional loss term will be proposed to enrich the internal representation
provided into the cross attention module. This work suggests a novel
architecture with a new loss term to help represent images and texts in the
joint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO
and ARCH, show the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QaNER: Prompting Question Answering Models for Few-shot Named Entity Recognition. (arXiv:2203.01543v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01543">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-based learning for pre-trained language models has succeeded
in few-shot Named Entity Recognition (NER) by exploiting prompts as task
guidance to increase label efficiency. However, previous prompt-based methods
for few-shot NER have limitations such as a higher computational complexity,
poor zero-shot ability, requiring manual prompt engineering, or lack of prompt
robustness. In this work, we address these shortcomings by proposing a new
prompt-based learning NER method with Question Answering (QA), called QaNER.
Our approach includes 1) a refined strategy for converting NER problems into
the QA formulation; 2) NER prompt generation for QA models; 3) prompt-based
tuning with QA models on a few annotated NER examples; 4) zero-shot NER by
prompting the QA model. Comparing the proposed approach with previous methods,
QaNER is faster at inference, insensitive to the prompt quality, and robust to
hyper-parameters, as well as demonstrating significantly better low-resource
performance and zero-shot capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Neural Networks on SVD Boosted Latent Spaces for Semantic Classification. (arXiv:2101.00563v1 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00563">
<div class="article-summary-box-inner">
<span><p>The availability of large amounts of data and compelling computation power
have made deep learning models much popular for text classification and
sentiment analysis. Deep neural networks have achieved competitive performance
on the above tasks when trained on naive text representations such as word
count, term frequency, and binary matrix embeddings. However, many of the above
representations result in the input space having a dimension of the order of
the vocabulary size, which is enormous. This leads to a blow-up in the number
of parameters to be learned, and the computational cost becomes infeasible when
scaling to domains that require retaining a colossal vocabulary. This work
proposes using singular value decomposition to transform the high dimensional
input space to a lower-dimensional latent space. We show that neural networks
trained on this lower-dimensional space are not only able to retain performance
while savoring significant reduction in the computational complexity but, in
many situations, also outperforms the classical neural networks trained on the
native input space.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Attention Network: Transformer Meets U-Net. (arXiv:2203.01932v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01932">
<div class="article-summary-box-inner">
<span><p>Currently, convolutional neural networks (CNN) (e.g., U-Net) have become the
de facto standard and attained immense success in medical image segmentation.
However, as a downside, CNN based methods are a double-edged sword as they fail
to build long-range dependencies and global context connections due to the
limited receptive field that stems from the intrinsic characteristics of the
convolution operation. Hence, recent articles have exploited Transformer
variants for medical image segmentation tasks which open up great opportunities
due to their innate capability of capturing long-range correlations through the
attention mechanism. Although being feasibly designed, most of the cohort
studies incur prohibitive performance in capturing local information, thereby
resulting in less lucidness of boundary areas. In this paper, we propose a
contextual attention network to tackle the aforementioned limitations. The
proposed method uses the strength of the Transformer module to model the
long-range contextual dependency. Simultaneously, it utilizes the CNN encoder
to capture local semantic information. In addition, an object-level
representation is included to model the regional interaction map. The extracted
hierarchical features are then fed to the contextual attention module to
adaptively recalibrate the representation space using the local information.
Then, they emphasize the informative regions while taking into account the
long-range contextual dependency derived by the Transformer module. We validate
our method on several large-scale public medical image segmentation datasets
and achieve state-of-the-art performance. We have provided the implementation
code in https://github.com/rezazad68/TMUnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Context Matters: Enhancing Single Image Prediction with Disease Progression Representations. (arXiv:2203.01933v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01933">
<div class="article-summary-box-inner">
<span><p>Clinical outcome or severity prediction from medical images has largely
focused on learning representations from single-timepoint or snapshot scans. It
has been shown that disease progression can be better characterized by temporal
imaging. We therefore hypothesized that outcome predictions can be improved by
utilizing the disease progression information from sequential images. We
present a deep learning approach that leverages temporal progression
information to improve clinical outcome predictions from single-timepoint
images. In our method, a self-attention based Temporal Convolutional Network
(TCN) is used to learn a representation that is most reflective of the disease
trajectory. Meanwhile, a Vision Transformer is pretrained in a self-supervised
fashion to extract features from single-timepoint images. The key contribution
is to design a recalibration module that employs maximum mean discrepancy loss
(MMD) to align distributions of the above two contextual representations. We
train our system to predict clinical outcomes and severity grades from
single-timepoint images. Experiments on chest and osteoarthritis radiography
datasets demonstrate that our approach outperforms other state-of-the-art
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality or Quantity: Toward a Unified Approach for Multi-organ Segmentation in Body CT. (arXiv:2203.01934v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01934">
<div class="article-summary-box-inner">
<span><p>Organ segmentation of medical images is a key step in virtual imaging trials.
However, organ segmentation datasets are limited in terms of quality (because
labels cover only a few organs) and quantity (since case numbers are limited).
In this study, we explored the tradeoffs between quality and quantity. Our goal
is to create a unified approach for multi-organ segmentation of body CT, which
will facilitate the creation of large numbers of accurate virtual phantoms.
Initially, we compared two segmentation architectures, 3D-Unet and DenseVNet,
which were trained using XCAT data that is fully labeled with 22 organs, and
chose the 3D-Unet as the better performing model. We used the XCAT-trained
model to generate pseudo-labels for the CT-ORG dataset that has only 7 organs
segmented. We performed two experiments: First, we trained 3D-UNet model on the
XCAT dataset, representing quality data, and tested it on both XCAT and CT-ORG
datasets. Second, we trained 3D-UNet after including the CT-ORG dataset into
the training set to have more quantity. Performance improved for segmentation
in the organs where we have true labels in both datasets and degraded when
relying on pseudo-labels. When organs were labeled in both datasets, Exp-2
improved Average DSC in XCAT and CT-ORG by 1. This demonstrates that quality
data is the key to improving the model's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E-CIR: Event-Enhanced Continuous Intensity Recovery. (arXiv:2203.01935v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01935">
<div class="article-summary-box-inner">
<span><p>A camera begins to sense light the moment we press the shutter button. During
the exposure interval, relative motion between the scene and the camera causes
motion blur, a common undesirable visual artifact. This paper presents E-CIR,
which converts a blurry image into a sharp video represented as a parametric
function from time to intensity. E-CIR leverages events as an auxiliary input.
We discuss how to exploit the temporal event structure to construct the
parametric bases. We demonstrate how to train a deep learning model to predict
the function coefficients. To improve the appearance consistency, we further
introduce a refinement module to propagate visual features among consecutive
frames. Compared to state-of-the-art event-enhanced deblurring approaches,
E-CIR generates smoother and more realistic results. The implementation of
E-CIR is available at https://github.com/chensong1995/E-CIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-guided Image Virtual Attribute Learning for Noisy Multi-label Chest X-ray Classification. (arXiv:2203.01937v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01937">
<div class="article-summary-box-inner">
<span><p>Deep learning methods have shown outstanding classification accuracy in
medical image analysis problems, which is largely attributed to the
availability of large datasets manually annotated with clean labels. However,
such manual annotation can be expensive to obtain for large datasets, so we may
rely on machine-generated noisy labels. Many Chest X-ray (CXR) classifiers are
modelled from datasets with machine-generated labels, but their training
procedure is in general not robust to the presence of noisy-label samples and
can overfit those samples to produce sub-optimal solutions. Furthermore, CXR
datasets are mostly multi-label, so current noisy-label learning methods
designed for multi-class problems cannot be easily adapted. To address such
noisy multi-label CXR learning problem, we propose a new learning method based
on estimating image virtual attributes using semantic information from the
label to assist in the identification and correction of noisy multi-labels from
training samples. Our experiments on diverse noisy multi-label training sets
and clean testing sets show that our model has state-of-the-art accuracy and
robustness across all datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Color Space-based HoVer-Net for Nuclei Instance Segmentation and Classification. (arXiv:2203.01940v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01940">
<div class="article-summary-box-inner">
<span><p>Nuclei segmentation and classification is the first and most crucial step
that is utilized for many different microscopy medical analysis applications.
However, it suffers from many issues such as the segmentation of small objects,
imbalance, and fine-grained differences between types of nuclei. In this paper,
multiple different contributions were done tackling these problems present.
Firstly, the recently released "ConvNeXt" was used as the encoder for HoVer-Net
model since it leverages the key components of transformers that make them
perform well. Secondly, to enhance the visual differences between nuclei, a
multi-channel color space-based approach is used to aid the model in extracting
distinguishing features. Thirdly, Unified Focal loss (UFL) was used to tackle
the background-foreground imbalance. Finally, Sharpness-Aware Minimization
(SAM) was used to ensure generalizability of the model. Overall, we were able
to outperform the current state-of-the-art (SOTA), HoVer-Net, on the
preliminary test set of the CoNiC Challenge 2022 by 12.489% mPQ+.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoregressive Image Generation using Residual Quantization. (arXiv:2203.01941v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01941">
<div class="article-summary-box-inner">
<span><p>For autoregressive (AR) modeling of high-resolution images, vector
quantization (VQ) represents an image as a sequence of discrete codes. A short
sequence length is important for an AR model to reduce its computational costs
to consider long-range interactions of codes. However, we postulate that
previous VQ cannot shorten the code sequence and generate high-fidelity images
together in terms of the rate-distortion trade-off. In this study, we propose
the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and
RQ-Transformer, to effectively generate high-resolution images. Given a fixed
codebook size, RQ-VAE can precisely approximate a feature map of an image and
represent the image as a stacked map of discrete codes. Then, RQ-Transformer
learns to predict the quantized feature vector at the next position by
predicting the next stack of codes. Thanks to the precise approximation of
RQ-VAE, we can represent a 256$\times$256 image as 8$\times$8 resolution of the
feature map, and RQ-Transformer can efficiently reduce the computational costs.
Consequently, our framework outperforms the existing AR models on various
benchmarks of unconditional and conditional image generation. Our approach also
has a significantly faster sampling speed than previous AR models to generate
high-quality images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A multi-stream convolutional neural network for classification of progressive MCI in Alzheimer's disease using structural MRI images. (arXiv:2203.01944v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01944">
<div class="article-summary-box-inner">
<span><p>Early diagnosis of Alzheimer's disease and its prodromal stage, also known as
mild cognitive impairment (MCI), is critical since some patients with
progressive MCI will develop the disease. We propose a multi-stream deep
convolutional neural network fed with patch-based imaging data to classify
stable MCI and progressive MCI. First, we compare MRI images of Alzheimer's
disease with cognitively normal subjects to identify distinct anatomical
landmarks using a multivariate statistical test. These landmarks are then used
to extract patches that are fed into the proposed multi-stream convolutional
neural network to classify MRI images. Next, we train the architecture in a
separate scenario using samples from Alzheimer's disease images, which are
anatomically similar to the progressive MCI ones and cognitively normal images
to compensate for the lack of progressive MCI training data. Finally, we
transfer the trained model weights to the proposed architecture in order to
fine-tune the model using progressive MCI and stable MCI data. Experimental
results on the ADNI-1 dataset indicate that our method outperforms existing
methods for MCI classification, with an F1-score of 85.96%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining. (arXiv:2203.01969v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01969">
<div class="article-summary-box-inner">
<span><p>Retrospective analysis of brain MRI scans acquired in the clinic has the
potential to enable neuroimaging studies with sample sizes much larger than
those found in research datasets. However, analysing such clinical images "in
the wild" is challenging, since subjects are scanned with highly variable
protocols (MR contrast, resolution, orientation, etc.). Nevertheless, recent
advances in convolutional neural networks (CNNs) and domain randomisation for
image segmentation, best represented by the publicly available method SynthSeg,
may enable morphometry of clinical MRI at scale. In this work, we first
evaluate SynthSeg on an uncurated, heterogeneous dataset of more than 10,000
scans acquired at Massachusetts General Hospital. We show that SynthSeg is
generally robust, but frequently falters on scans with low signal-to-noise
ratio or poor tissue contrast. Next, we propose SynthSeg+, a novel method that
greatly mitigates these problems using a hierarchy of conditional segmentation
and denoising CNNs. We show that this method is considerably more robust than
SynthSeg, while also outperforming cascaded networks and state-of-the-art
segmentation denoising methods. Finally, we apply our approach to a
proof-of-concept volumetric study of ageing, where it closely replicates
atrophy patterns observed in research studies conducted on high-quality, 1mm,
T1-weighted scans. The code and trained model are publicly available at
https://github.com/BBillot/SynthSeg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Rich, Portable, and Large-Scale Pedestrian Data Collection. (arXiv:2203.01974v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01974">
<div class="article-summary-box-inner">
<span><p>Recently, pedestrian behavior research has shifted towards machine learning
based methods and converged on the topic of modeling pedestrian interactions.
For this, a large-scale dataset that contains rich information is needed. We
propose a data collection system that is portable, which facilitates accessible
large-scale data collection in diverse environments. We also couple the system
with a semi-autonomous labeling pipeline for fast trajectory label production.
We demonstrate the effectiveness of our system by further introducing a dataset
we have collected -- the TBD pedestrian dataset. Compared with existing
pedestrian datasets, our dataset contains three components: human verified
labels grounded in the metric space, a combination of top-down and perspective
views, and naturalistic human behavior in the presence of a socially
appropriate "robot". In addition, the TBD pedestrian dataset is larger in
quantity compared to similar existing datasets and contains unique pedestrian
behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-Visual Object Classification for Human-Robot Collaboration. (arXiv:2203.01977v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01977">
<div class="article-summary-box-inner">
<span><p>Human-robot collaboration requires the contactless estimation of the physical
properties of containers manipulated by a person, for example while pouring
content in a cup or moving a food box. Acoustic and visual signals can be used
to estimate the physical properties of such objects, which may vary
substantially in shape, material and size, and also be occluded by the hands of
the person. To facilitate comparisons and stimulate progress in solving this
problem, we present the CORSMAL challenge and a dataset to assess the
performance of the algorithms through a set of well-defined performance scores.
The tasks of the challenge are the estimation of the mass, capacity, and
dimensions of the object (container), and the classification of the type and
amount of its content. A novel feature of the challenge is our
real-to-simulation framework for visualising and assessing the impact of
estimation errors in human-to-robot handovers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region-of-Interest Based Neural Video Compression. (arXiv:2203.01978v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01978">
<div class="article-summary-box-inner">
<span><p>Humans do not perceive all parts of a scene with the same resolution, but
rather focus on few regions of interest (ROIs). Traditional Object-Based codecs
take advantage of this biological intuition, and are capable of non-uniform
allocation of bits in favor of salient regions, at the expense of increased
distortion the remaining areas: such a strategy allows a boost in perceptual
quality under low rate constraints. Recently, several neural codecs have been
introduced for video compression, yet they operate uniformly over all spatial
locations, lacking the capability of ROI-based processing. In this paper, we
introduce two models for ROI-based neural video coding. First, we propose an
implicit model that is fed with a binary ROI mask and it is trained by
de-emphasizing the distortion of the background. Secondly, we design an
explicit latent scaling method, that allows control over the quantization
binwidth for different spatial regions of latent variables, conditioned on the
ROI mask. By extensive experiments, we show that our methods outperform all our
baselines in terms of Rate-Distortion (R-D) performance in the ROI. Moreover,
they can generalize to different datasets and to any arbitrary ROI at inference
time. Finally, they do not require expensive pixel-level annotations during
training, as synthetic ROI masks can be used with little to no degradation in
performance. To the best of our knowledge, our proposals are the first
solutions that integrate ROI-based capabilities into neural video compression
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polarity Sampling: Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values. (arXiv:2203.01993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01993">
<div class="article-summary-box-inner">
<span><p>We present Polarity Sampling, a theoretically justified plug-and-play method
for controlling the generation quality and diversity of pre-trained deep
generative networks DGNs). Leveraging the fact that DGNs are, or can be
approximated by, continuous piecewise affine splines, we derive the analytical
DGN output space distribution as a function of the product of the DGN's
Jacobian singular values raised to a power $\rho$. We dub $\rho$ the
$\textbf{polarity}$ parameter and prove that $\rho$ focuses the DGN sampling on
the modes ($\rho &lt; 0$) or anti-modes ($\rho &gt; 0$) of the DGN output-space
distribution. We demonstrate that nonzero polarity values achieve a better
precision-recall (quality-diversity) Pareto frontier than standard methods,
such as truncation, for a number of state-of-the-art DGNs. We also present
quantitative and qualitative results on the improvement of overall generation
quality (e.g., in terms of the Frechet Inception Distance) for a number of
state-of-the-art DGNs, including StyleGAN3, BigGAN-deep, NVAE, for different
conditional and unconditional image generation tasks. In particular, Polarity
Sampling redefines the state-of-the-art for StyleGAN2 on the FFHQ Dataset to
FID 2.57, StyleGAN2 on the LSUN Car Dataset to FID 2.27 and StyleGAN3 on the
AFHQv2 Dataset to FID 3.95. Demo: bit.ly/polarity-demo-colab
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Neural Architecture Search for Lightweight Dense Prediction Networks. (arXiv:2203.01994v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01994">
<div class="article-summary-box-inner">
<span><p>Dense prediction is a class of computer vision problems aiming at mapping
every pixel of the input image with some predicted values. Depending on the
problem, the output values can be either continous or discrete. For instance,
monocular depth estimation and image super-resolution are often formulated as
regression, while semantic segmentation is a dense classification, i.e.
discrete, problem. More specifically, the monocular depth estimation problem
produces a dense depth map from a single image to be used in various
applications including robotics, scene understanding, and augmented reality.
Single image super-resolution (SISR) is a low-level vision task that generates
a high-resolution image from its low-resolution counterpart. SISR is widely
utilized in medical and surveillance imaging, where images with more precise
details can provide invaluable information. On the other hand, semantic
segmentation predicts a dense annotated map of different semantic categories
from a given image that is crucial for image understanding tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counting Molecules: Python based scheme for automated enumeration and categorization of molecules in scanning tunneling microscopy images. (arXiv:2203.01998v1 [cond-mat.mes-hall])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01998">
<div class="article-summary-box-inner">
<span><p>Scanning tunneling and atomic force microscopies (STM/nc-AFM) are rapidly
progressing to offer unprecedented spatial resolution of a diverse array of
chemical species. In particular, they are employed to characterize on-surface
chemical reactions by directly examining precursors and products. Chiral
effects and self-assembled structures can also be investigated. This open
source, modular, python based scheme automates the categorization of a variety
of molecules present in medium sized (10$\times$10 to 100$\times$100 nm)
scanned probe images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why adversarial training can hurt robust accuracy. (arXiv:2203.02006v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02006">
<div class="article-summary-box-inner">
<span><p>Machine learning classifiers with high test accuracy often perform poorly
under adversarial attacks. It is commonly believed that adversarial training
alleviates this issue. In this paper, we demonstrate that, surprisingly, the
opposite may be true -- Even though adversarial training helps when enough data
is available, it may hurt robust generalization in the small sample size
regime. We first prove this phenomenon for a high-dimensional linear
classification setting with noiseless observations. Our proof provides
explanatory insights that may also transfer to feature learning models.
Further, we observe in experiments on standard image datasets that the same
behavior occurs for perceptible attacks that effectively reduce class
information such as mask attacks and object corruptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations. (arXiv:2203.02013v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02013">
<div class="article-summary-box-inner">
<span><p>The ability for a human to understand an Artificial Intelligence (AI) model's
decision-making process is critical in enabling stakeholders to visualize model
behavior, perform model debugging, promote trust in AI models, and assist in
collaborative human-AI decision-making. As a result, the research fields of
interpretable and explainable AI have gained traction within AI communities as
well as interdisciplinary scientists seeking to apply AI in their subject
areas. In this paper, we focus on advancing the state-of-the-art in
interpreting multimodal models - a class of machine learning methods that
tackle core challenges in representing and capturing interactions between
heterogeneous data sources such as images, text, audio, and time-series data.
Multimodal models have proliferated numerous real-world applications across
healthcare, robotics, multimedia, affective computing, and human-computer
interaction. By performing model disentanglement into unimodal contributions
(UC) and multimodal interactions (MI), our proposed approach, DIME, enables
accurate and fine-grained analysis of multimodal models while maintaining
generality across arbitrary modalities, model architectures, and tasks. Through
a comprehensive suite of experiments on both synthetic and real-world
multimodal tasks, we show that DIME generates accurate disentangled
explanations, helps users of multimodal models gain a deeper understanding of
model behavior, and presents a step towards debugging and improving these
models for real-world deployment. Code for our experiments can be found at
https://github.com/lvyiwei1/DIME.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection-Inspired Few-Shot Medical Image Segmentation Through Self-Supervision With Supervoxels. (arXiv:2203.02048v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02048">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that label-efficient few-shot learning through
self-supervision can achieve promising medical image segmentation results.
However, few-shot segmentation models typically rely on prototype
representations of the semantic classes, resulting in a loss of local
information that can degrade performance. This is particularly problematic for
the typically large and highly heterogeneous background class in medical image
segmentation problems. Previous works have attempted to address this issue by
learning additional prototypes for each class, but since the prototypes are
based on a limited number of slices, we argue that this ad-hoc solution is
insufficient to capture the background properties. Motivated by this, and the
observation that the foreground class (e.g., one organ) is relatively
homogeneous, we propose a novel anomaly detection-inspired approach to few-shot
medical image segmentation in which we refrain from modeling the background
explicitly. Instead, we rely solely on a single foreground prototype to compute
anomaly scores for all query pixels. The segmentation is then performed by
thresholding these anomaly scores using a learned threshold. Assisted by a
novel self-supervision task that exploits the 3D structure of medical images
through supervoxels, our proposed anomaly detection-inspired few-shot medical
image segmentation model outperforms previous state-of-the-art approaches on
two representative MRI datasets for the tasks of abdominal organ segmentation
and cardiac segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning. (arXiv:2203.02053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02053">
<div class="article-summary-box-inner">
<span><p>We present modality gap, an intriguing geometric phenomenon of the
representation space of multi-modal models. Specifically, we show that
different data modalities (e.g. images and text) are embedded at arm's length
in their shared representation in multi-modal models such as CLIP. Our
systematic analysis demonstrates that this gap is caused by a combination of
model initialization and contrastive learning optimization. In model
initialization, we show empirically and theoretically that the representation
of a common deep neural network is restricted to a narrow cone. As a
consequence, in a multi-modal model with two encoders, the representations of
the two modalities are clearly apart when the model is initialized. During
optimization, contrastive learning keeps the different modalities separate by a
certain distance, which is influenced by the temperature parameter in the loss
function. Our experiments further demonstrate that varying the modality gap
distance has a significant impact in improving the model's downstream zero-shot
classification performance and fairness. Our code and data are available at
https://modalitygap.readthedocs.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim2Real Instance-Level Style Transfer for 6D Pose Estimation. (arXiv:2203.02069v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02069">
<div class="article-summary-box-inner">
<span><p>In recent years, synthetic data has been widely used in the training of 6D
pose estimation networks, in part because it automatically provides perfect
annotation at low cost. However, there are still non-trivial domain gaps, such
as differences in textures/materials, between synthetic and real data. These
gaps have a measurable impact on performance. To solve this problem, we
introduce a simulation to reality (sim2real) instance-level style transfer for
6D pose estimation network training. Our approach transfers the style of target
objects individually, from synthetic to real, without human intervention. This
improves the quality of synthetic data for training pose estimation networks.
We also propose a complete pipeline from data collection to the training of a
pose estimation network and conduct extensive evaluation on a real-world
robotic platform. Our evaluation shows significant improvement achieved by our
method in both pose estimation performance and the realism of images adapted by
the style transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Subpopulation-based Membership Inference Attack. (arXiv:2203.02080v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02080">
<div class="article-summary-box-inner">
<span><p>Membership inference attacks allow a malicious entity to predict whether a
sample is used during training of a victim model or not. State-of-the-art
membership inference attacks have shown to achieve good accuracy which poses a
great privacy threat. However, majority of SOTA attacks require training dozens
to hundreds of shadow models to accurately infer membership. This huge
computation cost raises questions about practicality of these attacks on deep
models. In this paper, we introduce a fundamentally different MI attack
approach which obviates the need to train hundreds of shadow models. Simply
put, we compare the victim model output on the target sample versus the samples
from the same subpopulation (i.e., semantically similar samples), instead of
comparing it with the output of hundreds of shadow models. The intuition is
that the model response should not be significantly different between the
target sample and its subpopulation if it was not a training sample. In cases
where subpopulation samples are not available to the attacker, we show that
training only a single generative model can fulfill the requirement. Hence, we
achieve the state-of-the-art membership inference accuracy while significantly
reducing the training computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WPNAS: Neural Architecture Search by jointly using Weight Sharing and Predictor. (arXiv:2203.02086v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02086">
<div class="article-summary-box-inner">
<span><p>Weight sharing based and predictor based methods are two major types of fast
neural architecture search methods. In this paper, we propose to jointly use
weight sharing and predictor in a unified framework. First, we construct a
SuperNet in a weight-sharing way and probabilisticly sample architectures from
the SuperNet. To increase the correctness of the evaluation of architectures,
besides direct evaluation using the inherited weights, we further apply a
few-shot predictor to assess the architecture on the other hand. The final
evaluation of the architecture is the combination of direct evaluation, the
prediction from the predictor and the cost of the architecture. We regard the
evaluation as a reward and apply a self-critical policy gradient approach to
update the architecture probabilities. To further reduce the side effects of
weight sharing, we propose a weakly weight sharing method by introducing
another HyperNet. We conduct experiments on datasets including CIFAR-10,
CIFAR-100 and ImageNet under NATS-Bench, DARTS and MobileNet search space. The
proposed WPNAS method achieves state-of-the-art performance on these datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Segmentation of 33 Anatomies. (arXiv:2203.02098v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02098">
<div class="article-summary-box-inner">
<span><p>In the paper, we present an approach for learning a single model that
universally segments 33 anatomical structures, including vertebrae, pelvic
bones, and abdominal organs. Our model building has to address the following
challenges. Firstly, while it is ideal to learn such a model from a
large-scale, fully-annotated dataset, it is practically hard to curate such a
dataset. Thus, we resort to learn from a union of multiple datasets, with each
dataset containing the images that are partially labeled. Secondly, along the
line of partial labelling, we contribute an open-source, large-scale vertebra
segmentation dataset for the benefit of spine analysis community, CTSpine1K,
boasting over 1,000 3D volumes and over 11K annotated vertebrae. Thirdly, in a
3D medical image segmentation task, due to the limitation of GPU memory, we
always train a model using cropped patches as inputs instead a whole 3D volume,
which limits the amount of contextual information to be learned. To this, we
propose a cross-patch transformer module to fuse more information in adjacent
patches, which enlarges the aggregated receptive field for improved
segmentation performance. This is especially important for segmenting, say, the
elongated spine. Based on 7 partially labeled datasets that collectively
contain about 2,800 3D volumes, we successfully learn such a universal model.
Finally, we evaluate the universal model on multiple open-source datasets,
proving that our model has a good generalization performance and can
potentially serve as a solid foundation for downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Incrementally to Segment Multiple Organs in a CT Image. (arXiv:2203.02100v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02100">
<div class="article-summary-box-inner">
<span><p>There exists a large number of datasets for organ segmentation, which are
partially annotated and sequentially constructed. A typical dataset is
constructed at a certain time by curating medical images and annotating the
organs of interest. In other words, new datasets with annotations of new organ
categories are built over time. To unleash the potential behind these partially
labeled, sequentially-constructed datasets, we propose to incrementally learn a
multi-organ segmentation model. In each incremental learning (IL) stage, we
lose the access to previous data and annotations, whose knowledge is assumingly
captured by the current model, and gain the access to a new dataset with
annotations of new organ categories, from which we learn to update the organ
segmentation model to include the new organs. While IL is notorious for its
`catastrophic forgetting' weakness in the context of natural image analysis, we
experimentally discover that such a weakness mostly disappears for CT
multi-organ segmentation. To further stabilize the model performance across the
IL stages, we introduce a light memory module and some loss functions to
restrain the representation of different categories in feature space,
aggregating feature representation of the same class and separating feature
representation of different classes. Extensive experiments on five open-sourced
datasets are conducted to illustrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Image Synthesis with Panoptic Layout Generation. (arXiv:2203.02104v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02104">
<div class="article-summary-box-inner">
<span><p>Interactive image synthesis from user-guided input is a challenging task when
users wish to control the scene structure of a generated image with
ease.Although remarkable progress has been made on layout-based image synthesis
approaches, in order to get realistic fake image in interactive scene, existing
methods require high-precision inputs, which probably need adjustment several
times and are unfriendly to novice users. When placement of bounding boxes is
subject to perturbation, layout-based models suffer from "missing regions" in
the constructed semantic layouts and hence undesirable artifacts in the
generated images. In this work, we propose Panoptic Layout Generative
Adversarial Networks (PLGAN) to address this challenge. The PLGAN employs
panoptic theory which distinguishes object categories between "stuff" with
amorphous boundaries and "things" with well-defined shapes, such that stuff and
instance layouts are constructed through separate branches and later fused into
panoptic layouts. In particular, the stuff layouts can take amorphous shapes
and fill up the missing regions left out by the instance layouts. We
experimentally compare our PLGAN with state-of-the-art layout-based models on
the COCO-Stuff, Visual Genome, and Landscape datasets. The advantages of PLGAN
are not only visually demonstrated but quantitatively verified in terms of
inception score, Fr\'echet inception distance, classification accuracy score,
and coverage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scribble-Supervised Medical Image Segmentation via Dual-Branch Network and Dynamically Mixed Pseudo Labels Supervision. (arXiv:2203.02106v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02106">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation plays an irreplaceable role in computer-assisted
diagnosis, treatment planning, and following-up. Collecting and annotating a
large-scale dataset is crucial to training a powerful segmentation model, but
producing high-quality segmentation masks is an expensive and time-consuming
procedure. Recently, weakly-supervised learning that uses sparse annotations
(points, scribbles, bounding boxes) for network training has achieved
encouraging performance and shown the potential for annotation cost reduction.
However, due to the limited supervision signal of sparse annotations, it is
still challenging to employ them for networks training directly. In this work,
we propose a simple yet efficient scribble-supervised image segmentation method
and apply it to cardiac MRI segmentation. Specifically, we employ a dual-branch
network with one encoder and two slightly different decoders for image
segmentation and dynamically mix the two decoders' predictions to generate
pseudo labels for auxiliary supervision. By combining the scribble supervision
and auxiliary pseudo labels supervision, the dual-branch network can
efficiently learn from scribble annotations end-to-end. Experiments on the
public ACDC dataset show that our method performs better than current
scribble-supervised segmentation methods and also outperforms several
semi-supervised segmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Category-Level Generalizable Object Manipulation Policy via Generative Adversarial Self-Imitation Learning from Demonstrations. (arXiv:2203.02107v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02107">
<div class="article-summary-box-inner">
<span><p>Generalizable object manipulation skills are critical for intelligent and
multi-functional robots to work in real-world complex scenes. Despite the
recent progress in reinforcement learning, it is still very challenging to
learn a generalizable manipulation policy that can handle a category of
geometrically diverse articulated objects. In this work, we tackle this
category-level object manipulation policy learning problem via imitation
learning in a task-agnostic manner, where we assume no handcrafted dense
rewards but only a terminal reward. Given this novel and challenging
generalizable policy learning problem, we identify several key issues that can
fail the previous imitation learning algorithms and hinder the generalization
to unseen instances. We then propose several general but critical techniques,
including generative adversarial self-imitation learning from demonstrations,
progressive growing of discriminator, and instance-balancing for expert buffer,
that accurately pinpoints and tackles these issues and can benefit
category-level manipulation policy learning regardless of the tasks. Our
experiments on ManiSkill benchmarks demonstrate a remarkable improvement on all
tasks and our ablation studies further validate the contribution of each
proposed technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis. (arXiv:2203.02110v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02110">
<div class="article-summary-box-inner">
<span><p>Many works have shown that deep learning-based medical image classification
models can exhibit bias toward certain demographic attributes like race,
gender, and age. Existing bias mitigation methods primarily focus on learning
debiased models, which may not necessarily guarantee all sensitive information
can be removed and usually comes with considerable accuracy degradation on both
privileged and unprivileged groups. To tackle this issue, we propose a method,
FairPrune, that achieves fairness by pruning. Conventionally, pruning is used
to reduce the model size for efficient inference. However, we show that pruning
can also be a powerful tool to achieve fairness. Our observation is that during
pruning, each parameter in the model has different importance for different
groups' accuracy. By pruning the parameters based on this importance
difference, we can reduce the accuracy difference between the privileged group
and the unprivileged group to improve fairness without a large accuracy drop.
To this end, we use the second derivative of the parameters of a pre-trained
model to quantify the importance of each parameter with respect to the model
accuracy for each group. Experiments on two skin lesion diagnosis datasets over
multiple sensitive attributes demonstrate that our method can greatly improve
fairness while keeping the average accuracy of both groups as high as possible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Stereo for Monocular 3D Object Detection in Autonomous Driving. (arXiv:2203.02112v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02112">
<div class="article-summary-box-inner">
<span><p>Pseudo-LiDAR 3D detectors have made remarkable progress in monocular 3D
detection by enhancing the capability of perceiving depth with depth estimation
networks, and using LiDAR-based 3D detection architectures. The advanced stereo
3D detectors can also accurately localize 3D objects. The gap in image-to-image
generation for stereo views is much smaller than that in image-to-LiDAR
generation. Motivated by this, we propose a Pseudo-Stereo 3D detection
framework with three novel virtual view generation methods, including
image-level generation, feature-level generation, and feature-clone, for
detecting 3D objects from a single image. Our analysis of depth-aware learning
shows that the depth loss is effective in only feature-level virtual view
generation and the estimated depth map is effective in both image-level and
feature-level in our framework. We propose a disparity-wise dynamic convolution
with dynamic kernels sampled from the disparity feature map to filter the
features adaptively from a single image for generating virtual image features,
which eases the feature degradation caused by the depth estimation errors. Till
submission (November 18, 2021), our Pseudo-Stereo 3D detection framework ranks
1st on car, pedestrian, and cyclist among the monocular 3D detectors with
publications on the KITTI-3D benchmark. The code is released at
https://github.com/revisitq/Pseudo-Stereo-3D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context. (arXiv:2203.02113v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02113">
<div class="article-summary-box-inner">
<span><p>We advance sketch research to scenes with the first dataset of freehand scene
sketches, FS-COCO. With practical applications in mind, we collect sketches
that convey well scene content but can be sketched within a few minutes by a
person with any sketching skills. Our dataset comprises 10,000 freehand scene
vector sketches with per point space-time information by 100 non-expert
individuals, offering both object- and scene-level abstraction. Each sketch is
augmented with its text description. Using our dataset, we study for the first
time the problem of the fine-grained image retrieval from freehand scene
sketches and sketch captions. We draw insights on (i) Scene salience encoded in
sketches with strokes temporal order; (ii) The retrieval performance accuracy
from scene sketches against image captions; (iii) Complementarity of
information in sketches and image captions, as well as the potential benefit of
combining the two modalities. In addition, we propose new solutions enabled by
our dataset (i) We adopt meta-learning to show how the retrieval model can be
fine-tuned to a new user style given just a small set of sketches, (ii) We
extend a popular vector sketch LSTM-based encoder to handle sketches with
larger complexity than was supported by previous work. Namely, we propose a
hierarchical sketch decoder, which we leverage at a sketch-specific "pretext"
task. Our dataset enables for the first time research on freehand scene sketch
understanding and its practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixCL: Pixel label matters to contrastive learning. (arXiv:2203.02114v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02114">
<div class="article-summary-box-inner">
<span><p>Contrastive learning and self-supervised techniques have gained prevalence in
computer vision for the past few years. It is essential for medical image
analysis, which is often notorious for its lack of annotations. Most existing
self-supervised methods applied in natural imaging tasks focus on designing
proxy tasks for unlabeled data. For example, contrastive learning is often
based on the fact that an image and its transformed version share the same
identity. However, pixel annotations contain much valuable information for
medical image segmentation, which is largely ignored in contrastive learning.
In this work, we propose a novel pre-training framework called Mixed
Contrastive Learning (MixCL) that leverages both image identities and pixel
labels for better modeling by maintaining identity consistency, label
consistency, and reconstruction consistency together. Consequently, thus
pre-trained model has more robust representations that characterize medical
images. Extensive experiments demonstrate the effectiveness of the proposed
method, improving the baseline by 5.28% and 14.12% in Dice coefficient when 5%
labeled data of Spleen and 15% of BTVC are used in fine-tuning, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Benchmarking and Evaluating Deepfake Detection. (arXiv:2203.02115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02115">
<div class="article-summary-box-inner">
<span><p>Deepfake detection automatically recognizes the manipulated medias through
the analysis of the difference between manipulated and non-altered videos. It
is natural to ask which are the top performers among the existing deepfake
detection approaches to identify promising research directions and provide
practical guidance. Unfortunately, it's difficult to conduct a sound
benchmarking comparison of existing detection approaches using the results in
the literature because evaluation conditions are inconsistent across studies.
Our objective is to establish a comprehensive and consistent benchmark, to
develop a repeatable evaluation procedure, and to measure the performance of a
range of detection approaches so that the results can be compared soundly. A
challenging dataset consisting of the manipulated samples generated by more
than 13 different methods has been collected, and 11 popular detection
approaches (9 algorithms) from the existing literature have been implemented
and evaluated with 6 fair-minded and practical evaluation metrics. Finally, 92
models have been trained and 644 experiments have been performed for the
evaluation. The results along with the shared data and evaluation methodology
constitute a benchmark for comparing deepfake detection approaches and
measuring progress.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D endoscopic depth estimation using 3D surface-aware constraints. (arXiv:2203.02131v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02131">
<div class="article-summary-box-inner">
<span><p>Robotic-assisted surgery allows surgeons to conduct precise surgical
operations with stereo vision and flexible motor control. However, the lack of
3D spatial perception limits situational awareness during procedures and
hinders mastering surgical skills in the narrow abdominal space. Depth
estimation, as a representative perception task, is typically defined as an
image reconstruction problem. In this work, we show that depth estimation can
be reformed from a 3D surface perspective. We propose a loss function for depth
estimation that integrates the surface-aware constraints, leading to a faster
and better convergence with the valid information from spatial information. In
addition, camera parameters are incorporated into the training pipeline to
increase the control and transparency of the depth estimation. We also
integrate a specularity removal module to recover more buried image
information. Quantitative experimental results on endoscopic datasets and user
studies with medical professionals demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Versatile Multi-View Framework for LiDAR-based 3D Object Detection with Guidance from Panoptic Segmentation. (arXiv:2203.02133v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02133">
<div class="article-summary-box-inner">
<span><p>3D object detection using LiDAR data is an indispensable component for
autonomous driving systems. Yet, only a few LiDAR-based 3D object detection
methods leverage segmentation information to further guide the detection
process. In this paper, we propose a novel multi-task framework that jointly
performs 3D object detection and panoptic segmentation. In our method, the 3D
object detection backbone in Bird's-Eye-View (BEV) plane is augmented by the
injection of Range-View (RV) feature maps from the 3D panoptic segmentation
backbone. This enables the detection backbone to leverage multi-view
information to address the shortcomings of each projection view. Furthermore,
foreground semantic information is incorporated to ease the detection task by
highlighting the locations of each object class in the feature maps. Finally, a
new center density heatmap generated based on the instance-level information
further guides the detection backbone by suggesting possible box center
locations for objects. Our method works with any BEV-based 3D object detection
method, and as shown by extensive experiments on the nuScenes dataset, it
provides significant performance gains. Notably, the proposed method based on a
single-stage CenterPoint 3D object detection network achieved state-of-the-art
performance on nuScenes 3D Detection Benchmark with 67.3 NDS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACVNet: Attention Concatenation Volume for Accurate and Efficient Stereo Matching. (arXiv:2203.02146v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02146">
<div class="article-summary-box-inner">
<span><p>Stereo matching is a fundamental building block for many vision and robotics
applications. An informative and concise cost volume representation is vital
for stereo matching of high accuracy and efficiency. In this paper, we present
a novel cost volume construction method which generates attention weights from
correlation clues to suppress redundant information and enhance
matching-related information in the concatenation volume. To generate reliable
attention weights, we propose multi-level adaptive patch matching to improve
the distinctiveness of the matching cost at different disparities even for
textureless regions. The proposed cost volume is named attention concatenation
volume (ACV) which can be seamlessly embedded into most stereo matching
networks, the resulting networks can use a more lightweight aggregation network
and meanwhile achieve higher accuracy, e.g. using only 1/25 parameters of the
aggregation network can achieve higher accuracy for GwcNet. Furthermore, we
design a highly accurate network (ACVNet) based on our ACV, which achieves
state-of-the-art performance on several benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDNet: High-resolution Dual-domain Learning for Spectral Compressive Imaging. (arXiv:2203.02149v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02149">
<div class="article-summary-box-inner">
<span><p>The rapid development of deep learning provides a better solution for the
end-to-end reconstruction of hyperspectral image (HSI). However, existing
learning-based methods have two major defects. Firstly, networks with
self-attention usually sacrifice internal resolution to balance model
performance against complexity, losing fine-grained high-resolution (HR)
features. Secondly, even if the optimization focusing on spatial-spectral
domain learning (SDL) converges to the ideal solution, there is still a
significant visual difference between the reconstructed HSI and the truth.
Therefore, we propose a high-resolution dual-domain learning network (HDNet)
for HSI reconstruction. On the one hand, the proposed HR spatial-spectral
attention module with its efficient feature fusion provides continuous and fine
pixel-level features. On the other hand, frequency domain learning (FDL) is
introduced for HSI reconstruction to narrow the frequency domain discrepancy.
Dynamic FDL supervision forces the model to reconstruct fine-grained
frequencies and compensate for excessive smoothing and distortion caused by
pixel-level losses. The HR pixel-level attention and frequency-level refinement
in our HDNet mutually promote HSI perceptual quality. Extensive quantitative
and qualitative evaluation experiments show that our method achieves SOTA
performance on simulated and real HSI datasets. Code and models will be
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PatchMVSNet: Patch-wise Unsupervised Multi-View Stereo for Weakly-Textured Surface Reconstruction. (arXiv:2203.02156v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02156">
<div class="article-summary-box-inner">
<span><p>Learning-based multi-view stereo (MVS) has gained fine reconstructions on
popular datasets. However, supervised learning methods require ground truth for
training, which is hard to be collected, especially for the large-scale
datasets. Though nowadays unsupervised learning methods have been proposed and
have gotten gratifying results, those methods still fail to reconstruct intact
results in challenging scenes, such as weakly-textured surfaces, as those
methods primarily depend on pixel-wise photometric consistency which is
subjected to various illuminations. To alleviate matching ambiguity in those
challenging scenes, this paper proposes robust loss functions leveraging
constraints beneath multi-view images: 1) Patch-wise photometric consistency
loss, which expands the receptive field of the features in multi-view
similarity measuring, 2) Robust twoview geometric consistency, which includes a
cross-view depth consistency checking with the minimum occlusion. Our
unsupervised strategy can be implemented with arbitrary depth estimation
frameworks and can be trained with arbitrary large-scale MVS datasets.
Experiments show that our method can decrease the matching ambiguity and
particularly improve the completeness of weakly-textured reconstruction.
Moreover, our method reaches the performance of the state-of-the-art methods on
popular benchmarks, like DTU, Tanks and Temples and ETH3D. The code will be
released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DetFlowTrack: 3D Multi-object Tracking based on Simultaneous Optimization of Object Detection and Scene Flow Estimation. (arXiv:2203.02157v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02157">
<div class="article-summary-box-inner">
<span><p>3D Multi-Object Tracking (MOT) is an important part of the unmanned vehicle
perception module. Most methods optimize object detection and data association
independently. These methods make the network structure complicated and limit
the improvement of MOT accuracy. we proposed a 3D MOT framework based on
simultaneous optimization of object detection and scene flow estimation. In the
framework, a detection-guidance scene flow module is proposed to relieve the
problem of incorrect inter-frame assocation. For more accurate scene flow label
especially in the case of motion with rotation, a box-transformation-based
scene flow ground truth calculation method is proposed. Experimental results on
the KITTI MOT dataset show competitive results over the state-of-the-arts and
the robustness under extreme motion with rotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformations in Learned Image Compression from a Communication Perspective. (arXiv:2203.02158v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02158">
<div class="article-summary-box-inner">
<span><p>In this paper, a unified transformation method in learned image
compression(LIC) is proposed from the perspective of communication. Firstly,
the quantization in LIC is considered as a generalized channel with additive
uniform noise. Moreover, the LIC is interpreted as a particular communication
system according to the consistency in structures and optimization objectives.
Thus, the technology of communication systems can be applied to guide the
design of modules in LIC. Furthermore, a unified transform method based on
signal modulation (TSM) is defined. In the view of TSM, the existing
transformation methods are mathematically reduced to a linear modulation. A
series of transformation methods, e.g. TPM and TJM, are obtained by extending
to nonlinear modulation. The experimental results on various datasets and
backbone architectures verify that the effectiveness and robustness of the
proposed method. More importantly, it further confirms the feasibility of
guiding LIC design from a communication perspective. For example, when backbone
architecture is hyperprior combining context model, our method achieves
3.52$\%$ BD-rate reduction over GDN on Kodak dataset without increasing
complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MF-Hovernet: An Extension of Hovernet for Colon Nuclei Identification and Counting (CoNiC) Challenge. (arXiv:2203.02161v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02161">
<div class="article-summary-box-inner">
<span><p>Nuclei Identification and Counting is the most important morphological
feature of cancers, especially in the colon. Many deep learning-based methods
have been proposed to deal with this problem. In this work, we construct an
extension of Hovernet for nuclei identification and counting to address the
problem named MF-Hovernet. Our proposed model is the combination of multiple
filer block to Hovernet architecture. The current result shows the efficiency
of multiple filter block to improve the performance of the original Hovernet
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Analysis Operator Learning by End-To-End Training of Iterative Neural Networks. (arXiv:2203.02166v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02166">
<div class="article-summary-box-inner">
<span><p>The concept of sparsity has been extensively applied for regularization in
image reconstruction. Typically, sparsifying transforms are either pre-trained
on ground-truth images or adaptively trained during the reconstruction.
Thereby, learning algorithms are designed to minimize some target function
which encodes the desired properties of the transform. However, this procedure
ignores the subsequently employed reconstruction algorithm as well as the
physical model which is responsible for the image formation process. Iterative
neural networks - which contain the physical model - can overcome these issues.
In this work, we demonstrate how convolutional sparsifying filters can be
efficiently learned by end-to-end training of iterative neural networks. We
evaluated our approach on a non-Cartesian 2D cardiac cine MRI example and show
that the obtained filters are better suitable for the corresponding
reconstruction algorithm than the ones obtained by decoupled pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels. (arXiv:2203.02172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02172">
<div class="article-summary-box-inner">
<span><p>Training the multi-label image recognition models with partial labels, in
which merely some labels are known while others are unknown for each image, is
a considerably challenging and practical task. To address this task, current
algorithms mainly depend on pre-training classification or similarity models to
generate pseudo labels for the unknown labels. However, these algorithms depend
on sufficient multi-label annotations to train the models, leading to poor
performance especially with low known label proportion. In this work, we
propose to blend category-specific representation across different images to
transfer information of known labels to complement unknown labels, which can
get rid of pre-training models and thus does not depend on sufficient
annotations. To this end, we design a unified semantic-aware representation
blending (SARB) framework that exploits instance-level and prototype-level
semantic representation to complement unknown labels by two complementary
modules: 1) an instance-level representation blending (ILRB) module blends the
representations of the known labels in an image to the representations of the
unknown labels in another image to complement these unknown labels. 2) a
prototype-level representation blending (PLRB) module learns more stable
representation prototypes for each category and blends the representation of
unknown labels with the prototypes of corresponding labels to complement these
labels. Extensive experiments on the MS-COCO, Visual Genome, Pascal VOC 2007
datasets show that the proposed SARB framework obtains superior performance
over current leading competitors on all known label proportion settings, i.e.,
with the mAP improvement of 4.6%, 4.%, 2.2% on these three datasets when the
known label proportion is 10%. Codes are available at
https://github.com/HCPLab-SYSU/HCP-MLR-PL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time-to-Label: Temporal Consistency for Self-Supervised Monocular 3D Object Detection. (arXiv:2203.02193v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02193">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection continues to attract attention due to the cost
benefits and wider availability of RGB cameras. Despite the recent advances and
the ability to acquire data at scale, annotation cost and complexity still
limit the size of 3D object detection datasets in the supervised settings.
Self-supervised methods, on the other hand, aim at training deep networks
relying on pretext tasks or various consistency constraints. Moreover, other 3D
perception tasks (such as depth estimation) have shown the benefits of temporal
priors as a self-supervision signal. In this work, we argue that the temporal
consistency on the level of object poses, provides an important supervision
signal given the strong prior on physical motion. Specifically, we propose a
self-supervised loss which uses this consistency, in addition to
render-and-compare losses, to refine noisy pose predictions and derive
high-quality pseudo labels. To assess the effectiveness of the proposed method,
we finetune a synthetically trained monocular 3D object detection model using
the pseudo-labels that we generated on real data. Evaluation on the standard
KITTI3D benchmark demonstrates that our method reaches competitive performance
compared to other monocular self-supervised and supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection. (arXiv:2203.02194v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02194">
<div class="article-summary-box-inner">
<span><p>In some scenarios, classifier requires detecting out-of-distribution samples
far from its training data. With desirable characteristics, reconstruction
autoencoder-based methods deal with this problem by using input reconstruction
error as a metric of novelty vs. normality. We formulate the essence of such
approach as a quadruplet domain translation with an intrinsic bias to only
query for a proxy of conditional data uncertainty. Accordingly, an improvement
direction is formalized as maximumly compressing the autoencoder's latent space
while ensuring its reconstructive power for acting as a described domain
translator. From it, strategies are introduced including semantic
reconstruction, data certainty decomposition and normalized L2 distance to
substantially improve original methods, which together establish
state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of
CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method
works without any additional data, hard-to-implement structure, time-consuming
pipeline, and even harming the classification accuracy of known classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voice-Face Homogeneity Tells Deepfake. (arXiv:2203.02195v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02195">
<div class="article-summary-box-inner">
<span><p>Detecting forgery videos is highly desired due to the abuse of deepfake.
Existing detection approaches contribute to exploring the specific artifacts in
deepfake videos and fit well on certain data. However, the growing technique on
these artifacts keeps challenging the robustness of traditional deepfake
detectors. As a result, the development of generalizability of these approaches
has reached a blockage. To address this issue, given the empirical results that
the identities behind voices and faces are often mismatched in deepfake videos,
and the voices and faces have homogeneity to some extent, in this paper, we
propose to perform the deepfake detection from an unexplored voice-face
matching view. To this end, a voice-face matching detection model is devised to
measure the matching degree of these two on a generic audio-visual dataset.
Thereafter, this model can be smoothly transferred to deepfake datasets without
any fine-tuning, and the generalization across datasets is accordingly
enhanced. We conduct extensive experiments over two widely exploited datasets -
DFDC and FakeAVCeleb. Our model obtains significantly improved performance as
compared to other state-of-the-art competitors and maintains favorable
generalizability. The code has been released at
https://github.com/xaCheng1996/VFD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Carbon Footprint of Selecting and Training Deep Learning Models for Medical Image Analysis. (arXiv:2203.02202v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02202">
<div class="article-summary-box-inner">
<span><p>The increasing energy consumption and carbon footprint of deep learning (DL)
due to growing compute requirements has become a cause of concern. In this
work, we focus on the carbon footprint of developing DL models for medical
image analysis (MIA), where volumetric images of high spatial resolution are
handled. In this study, we present and compare the features of four tools from
literature to quantify the carbon footprint of DL. Using one of these tools we
estimate the carbon footprint of medical image segmentation pipelines. We
choose nnU-net as the proxy for a medical image segmentation pipeline and
experiment on three common datasets. With our work we hope to inform on the
increasing energy costs incurred by MIA. We discuss simple strategies to
cut-down the environmental impact that can make model selection and training
processes more efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Safety-aware metrics for object detectors in autonomous driving. (arXiv:2203.02205v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02205">
<div class="article-summary-box-inner">
<span><p>We argue that object detectors in the safety critical domain should
prioritize detection of objects that are most likely to interfere with the
actions of the autonomous actor. Especially, this applies to objects that can
impact the actor's safety and reliability. In the context of autonomous
driving, we propose new object detection metrics that reward the correct
identification of objects that are most likely to interact with the subject
vehicle (i.e., the actor), and that may affect its driving decision. To achieve
this, we build a criticality model to reward the detection of the objects based
on proximity, orientation, and relative velocity with respect to the subject
vehicle. Then, we apply our model on the recent autonomous driving dataset
nuScenes, and we compare eight different object detectors. Results show that,
in several settings, object detectors that perform best according to the
nuScenes ranking are not the preferable ones when the focus is shifted on
safety and reliability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partial Wasserstein Adversarial Network for Non-rigid Point Set Registration. (arXiv:2203.02227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02227">
<div class="article-summary-box-inner">
<span><p>Given two point sets, the problem of registration is to recover a
transformation that matches one set to the other. This task is challenging due
to the presence of the large number of outliers, the unknown non-rigid
deformations and the large sizes of point sets. To obtain strong robustness
against outliers, we formulate the registration problem as a partial
distribution matching (PDM) problem, where the goal is to partially match the
distributions represented by point sets in a metric space. To handle large
point sets, we propose a scalable PDM algorithm by utilizing the efficient
partial Wasserstein-1 (PW) discrepancy. Specifically, we derive the
Kantorovich-Rubinstein duality for the PW discrepancy, and show its gradient
can be explicitly computed. Based on these results, we propose a partial
Wasserstein adversarial network (PWAN), which is able to approximate the PW
discrepancy by a neural network, and minimize it by gradient descent. In
addition, it also incorporates an efficient coherence regularizer for non-rigid
transformations to avoid unrealistic deformations. We evaluate PWAN on
practical point set registration tasks, and show that the proposed PWAN is
robust, scalable and performs more favorably than the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPAL: Occlusion Pattern Aware Loss for Unsupervised Light Field Disparity Estimation. (arXiv:2203.02231v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02231">
<div class="article-summary-box-inner">
<span><p>Light field disparity estimation is an essential task in computer vision with
various applications. Although supervised learning-based methods have achieved
both higher accuracy and efficiency than traditional optimization-based
methods, the dependency on ground-truth disparity for training limits the
overall generalization performance not to say for real-world scenarios where
the ground-truth disparity is hard to capture. In this paper, we argue that
unsupervised methods can achieve comparable accuracy, but, more importantly,
much higher generalization capacity and efficiency than supervised methods.
Specifically, we present the Occlusion Pattern Aware Loss, named OPAL, which
successfully extracts and encodes the general occlusion patterns inherent in
the light field for loss calculation. OPAL enables i) accurate and robust
estimation by effectively handling occlusions without using any ground-truth
information for training and ii) much efficient performance by significantly
reducing the network parameters required for accurate inference. Besides, a
transformer-based network and a refinement module are proposed for achieving
even more accurate results. Extensive experiments demonstrate our method not
only significantly improves the accuracy compared with the SOTA unsupervised
methods, but also possesses strong generalization capacity, even for real-world
data, compared with supervised methods. Our code will be made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting GAN-generated Images by Orthogonal Training of Multiple CNNs. (arXiv:2203.02246v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02246">
<div class="article-summary-box-inner">
<span><p>In the last few years, we have witnessed the rise of a series of deep
learning methods to generate synthetic images that look extremely realistic.
These techniques prove useful in the movie industry and for artistic purposes.
However, they also prove dangerous if used to spread fake news or to generate
fake online accounts. For this reason, detecting if an image is an actual
photograph or has been synthetically generated is becoming an urgent necessity.
This paper proposes a detector of synthetic images based on an ensemble of
Convolutional Neural Networks (CNNs). We consider the problem of detecting
images generated with techniques not available at training time. This is a
common scenario, given that new image generators are published more and more
frequently. To solve this issue, we leverage two main ideas: (i) CNNs should
provide orthogonal results to better contribute to the ensemble; (ii) original
images are better defined than synthetic ones, thus they should be better
trusted at testing time. Experiments show that pursuing these two ideas
improves the detector accuracy on NVIDIA's newly generated StyleGAN3 images,
never used in training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch Similarity Aware Data-Free Quantization for Vision Transformers. (arXiv:2203.02250v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02250">
<div class="article-summary-box-inner">
<span><p>Vision transformers have recently gained great success on various computer
vision tasks; nevertheless, their high model complexity makes it challenging to
deploy on resource-constrained devices. Quantization is an effective approach
to reduce model complexity, and data-free quantization, which can address data
privacy and security concerns during model deployment, has received widespread
interest. Unfortunately, all existing methods, such as BN regularization, were
designed for convolutional neural networks and cannot be applied to vision
transformers with significantly different model architectures. In this paper,
we propose PSAQ-ViT, a Patch Similarity Aware data-free Quantization framework
for Vision Transformers, to enable the generation of "realistic" samples based
on the vision transformer's unique properties for calibrating the quantization
parameters. Specifically, we analyze the self-attention module's properties and
reveal a general difference (patch similarity) in its processing of Gaussian
noise and real images. The above insights guide us to design a relative value
metric to optimize the Gaussian noise to approximate the real images, which are
then utilized to calibrate the quantization parameters. Extensive experiments
and ablation studies are conducted on various benchmarks to validate the
effectiveness of PSAQ-ViT, which can even outperform the real-data-driven
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Aware Contrastive Semi-Supervised Learning. (arXiv:2203.02261v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02261">
<div class="article-summary-box-inner">
<span><p>Pseudo-label-based semi-supervised learning (SSL) has achieved great success
on raw data utilization. However, its training procedure suffers from
confirmation bias due to the noise contained in self-generated artificial
labels. Moreover, the model's judgment becomes noisier in real-world
applications with extensive out-of-distribution data. To address this issue, we
propose a general method named Class-aware Contrastive Semi-Supervised Learning
(CCSSL), which is a drop-in helper to improve the pseudo-label quality and
enhance the model's robustness in the real-world setting. Rather than treating
real-world data as a union set, our method separately handles reliable
in-distribution data with class-wise clustering for blending into downstream
tasks and noisy out-of-distribution data with image-wise contrastive for better
generalization. Furthermore, by applying target re-weighting, we successfully
emphasize clean label learning and simultaneously reduce noisy label learning.
Despite its simplicity, our proposed CCSSL has significant performance
improvements over the state-of-the-art SSL methods on the standard datasets
CIFAR100 and STL10. On the real-world dataset Semi-iNat 2021, we improve
FixMatch by 9.80% and CoMatch by 3.18%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Explanations Explain? Model Knows Best. (arXiv:2203.02269v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02269">
<div class="article-summary-box-inner">
<span><p>It is a mystery which input features contribute to a neural network's output.
Various explanation (feature attribution) methods are proposed in the
literature to shed light on the problem. One peculiar observation is that these
explanations (attributions) point to different features as being important. The
phenomenon raises the question, which explanation to trust? We propose a
framework for evaluating the explanations using the neural network model
itself. The framework leverages the network to generate input features that
impose a particular behavior on the output. Using the generated features, we
devise controlled experimental setups to evaluate whether an explanation method
conforms to an axiom. Thus we propose an empirical framework for axiomatic
evaluation of explanation methods. We evaluate well-known and promising
explanation solutions using the proposed framework. The framework provides a
toolset to reveal properties and drawbacks within existing and future
explanation solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Transformation for Cross-domain Few-shot Remote Sensing Scene Classification. (arXiv:2203.02270v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02270">
<div class="article-summary-box-inner">
<span><p>Effectively classifying remote sensing scenes is still a challenge due to the
increasing spatial resolution of remote imaging and large variances between
remote sensing images. Existing research has greatly improved the performance
of remote sensing scene classification (RSSC). However, these methods are not
applicable to cross-domain few-shot problems where target domain is with very
limited training samples available and has a different data distribution from
source domain. To improve the model's applicability, we propose the
feature-wise transformation module (FTM) in this paper. FTM transfers the
feature distribution learned on source domain to that of target domain by a
very simple affine operation with negligible additional parameters. Moreover,
FTM can be effectively learned on target domain in the case of few training
data available and is agnostic to specific network structures. Experiments on
RSSC and land-cover mapping tasks verified its capability to handle
cross-domain few-shot problems. By comparison with directly finetuning, FTM
achieves better performance and possesses better transferability and
fine-grained discriminability. \textit{Code will be publicly available.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Review of Computer Vision in Sports: Open Issues, Future Trends and Research Directions. (arXiv:2203.02281v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02281">
<div class="article-summary-box-inner">
<span><p>Recent developments in video analysis of sports and computer vision
techniques have achieved significant improvements to enable a variety of
critical operations. To provide enhanced information, such as detailed complex
analysis in sports like soccer, basketball, cricket, badminton, etc., studies
have focused mainly on computer vision techniques employed to carry out
different tasks. This paper presents a comprehensive review of sports video
analysis for various applications high-level analysis such as detection and
classification of players, tracking player or ball in sports and predicting the
trajectories of player or ball, recognizing the teams strategies, classifying
various events in sports. The paper further discusses published works in a
variety of application-specific tasks related to sports and the present
researchers views regarding them. Since there is a wide research scope in
sports for deploying computer vision techniques in various sports, some of the
publicly available datasets related to a particular sport have been provided.
This work reviews a detailed discussion on some of the artificial
intelligence(AI)applications in sports vision, GPU-based work stations, and
embedded platforms. Finally, this review identifies the research directions,
probable challenges, and future trends in the area of visual recognition in
sports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nuclei segmentation and classification in histopathology images with StarDist for the CoNIC Challenge 2022. (arXiv:2203.02284v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02284">
<div class="article-summary-box-inner">
<span><p>Segmentation and classification of nuclei in histopathology images is an
important task in computational pathology. Here we describe how we used
StarDist, a deep learning based approach based on star-convex shape
representations, for the Colon Nuclei Identification and Counting (CoNIC)
challenge 2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-parametric Makeup Transfer via Semantic-aware Correspondence. (arXiv:2203.02286v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02286">
<div class="article-summary-box-inner">
<span><p>The large discrepancy between the source non-makeup image and the reference
makeup image is one of the key challenges in makeup transfer. Conventional
approaches for makeup transfer either learn disentangled representation or
perform pixel-wise correspondence in a parametric way between two images. We
argue that non-parametric techniques have a high potential for addressing the
pose, expression, and occlusion discrepancies. To this end, this paper proposes
a \textbf{S}emi-\textbf{p}arametric \textbf{M}akeup \textbf{T}ransfer (SpMT)
method, which combines the reciprocal strengths of non-parametric and
parametric mechanisms. The non-parametric component is a novel
\textbf{S}emantic-\textbf{a}ware \textbf{C}orrespondence (SaC) module that
explicitly reconstructs content representation with makeup representation under
the strong constraint of component semantics. The reconstructed representation
is desired to preserve the spatial and identity information of the source image
while "wearing" the makeup of the reference image. The output image is
synthesized via a parametric decoder that draws on the reconstructed
representation. Extensive experiments demonstrate the superiority of our method
in terms of visual quality, robustness, and flexibility. Code and pre-trained
model are available at \url{https://github.com/AnonymScholar/SpMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Freeform Body Motion Generation from Speech. (arXiv:2203.02291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02291">
<div class="article-summary-box-inner">
<span><p>People naturally conduct spontaneous body motions to enhance their speeches
while giving talks. Body motion generation from speech is inherently difficult
due to the non-deterministic mapping from speech to body motions. Most existing
works map speech to motion in a deterministic way by conditioning on certain
styles, leading to sub-optimal results. Motivated by studies in linguistics, we
decompose the co-speech motion into two complementary parts: pose modes and
rhythmic dynamics. Accordingly, we introduce a novel freeform motion generation
model (FreeMo) by equipping a two-stream architecture, i.e., a pose mode branch
for primary posture generation, and a rhythmic motion branch for rhythmic
dynamics synthesis. On one hand, diverse pose modes are generated by
conditional sampling in a latent space, guided by speech semantics. On the
other hand, rhythmic dynamics are synced with the speech prosody. Extensive
experiments demonstrate the superior performance against several baselines, in
terms of motion diversity, quality and syncing with speech. Code and
pre-trained models will be publicly available through
https://github.com/TheTempAccount/Co-Speech-Motion-Generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed Reality Depth Contour Occlusion Using Binocular Similarity Matching and Three-dimensional Contour Optimisation. (arXiv:2203.02300v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02300">
<div class="article-summary-box-inner">
<span><p>Mixed reality applications often require virtual objects that are partly
occluded by real objects. However, previous research and commercial products
have limitations in terms of performance and efficiency. To address these
challenges, we propose a novel depth contour occlusion (DCO) algorithm. The
proposed method is based on the sensitivity of contour occlusion and a
binocular stereoscopic vision device. In this method, a depth contour map is
combined with a sparse depth map obtained from a two-stage adaptive filter area
stereo matching algorithm and the depth contour information of the objects
extracted by a digital image stabilisation optical flow method. We also propose
a quadratic optimisation model with three constraints to generate an accurate
dense map of the depth contour for high-quality real-virtual occlusion. The
whole process is accelerated by GPU. To evaluate the effectiveness of the
algorithm, we demonstrate a time con-sumption statistical analysis for each
stage of the DCO algorithm execution. To verify the relia-bility of the
real-virtual occlusion effect, we conduct an experimental analysis on
single-sided, enclosed, and complex occlusions; subsequently, we compare it
with the occlusion method without quadratic optimisation. With our GPU
implementation for real-time DCO, the evaluation indicates that applying the
presented DCO algorithm can enhance the real-time performance and the visual
quality of real-virtual occlusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum Levenberg--Marquardt Algorithm for optimization in Bundle Adjustment. (arXiv:2203.02311v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02311">
<div class="article-summary-box-inner">
<span><p>In this paper we develop a quantum optimization algorithm and use it to solve
the bundle adjustment problem with a simulated quantum computer. Bundle
adjustment is the process of optimizing camera poses and sensor properties to
best reconstruct the three-dimensional structure and viewing parameters. This
problem is often solved using some implementation of the Levenberg--Marquardt
algorithm. In this case we implement a quantum algorithm for solving the linear
system of normal equations that calculates the optimization step in
Levenberg--Marquardt. This procedure is the current bottleneck in the
algorithmic complexity of bundle adjustment. The proposed quantum algorithm
dramatically reduces the complexity of this operation with respect to the
number of points.
</p>
<p>We investigate 9 configurations of a toy-model for bundle adjustment, limited
to 10 points and 2 cameras. This optimization problem is solved both by using
the sparse Levenberg-Marquardt algorithm and our quantum implementation. The
resulting solutions are presented, showing an improved rate of convergence,
together with an analysis of the theoretical speed up and the probability of
running the algorithm successfully on a current quantum computer.
</p>
<p>The presented quantum algorithm is a seminal implementation of using quantum
computing algorithms in order to solve complex optimization problems in
computer vision, in particular bundle adjustment, which offers several avenues
of further investigations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">F2DNet: Fast Focal Detection Network for Pedestrian Detection. (arXiv:2203.02331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02331">
<div class="article-summary-box-inner">
<span><p>Two-stage detectors are state-of-the-art in object detection as well as
pedestrian detection. However, the current two-stage detectors are inefficient
as they do bounding box regression in multiple steps i.e. in region proposal
networks and bounding box heads. Also, the anchor-based region proposal
networks are computationally expensive to train. We propose F2DNet, a novel
two-stage detection architecture which eliminates redundancy of current
two-stage detectors by replacing the region proposal network with our focal
detection network and bounding box head with our fast suppression head. We
benchmark F2DNet on top pedestrian detection datasets, thoroughly compare it
against the existing state-of-the-art detectors and conduct cross dataset
evaluation to test the generalizability of our model to unseen data. Our F2DNet
achieves 8.7%, 2.2%, and 6.1% MR-2 on City Persons, Caltech Pedestrian, and
Euro City Person datasets respectively when trained on a single dataset and
reaches 20.4% and 26.2% MR-2 in heavy occlusion setting of Caltech Pedestrian
and City Persons datasets when using progressive fine-tunning. On top of that
F2DNet have significantly lesser inference time compared to the current
state-of-the-art. Code and trained models will be available at
https://github.com/AbdulHannanKhan/F2DNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Estimation for Heatmap-based Landmark Localization. (arXiv:2203.02351v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02351">
<div class="article-summary-box-inner">
<span><p>Automatic anatomical landmark localization has made great strides by
leveraging deep learning methods in recent years. The ability to quantify the
uncertainty of these predictions is a vital ingredient needed to see these
methods adopted in clinical use, where it is imperative that erroneous
predictions are caught and corrected. We propose Quantile Binning, a
data-driven method to categorise predictions by uncertainty with estimated
error bounds. This framework can be applied to any continuous uncertainty
measure, allowing straightforward identification of the best subset of
predictions with accompanying estimated error bounds. We facilitate easy
comparison between uncertainty measures by constructing two evaluation metrics
derived from Quantile Binning. We demonstrate this framework by comparing and
contrasting three uncertainty measures (a baseline, the current gold standard,
and a proposed method combining aspects of the two), across two datasets (one
easy, one hard) and two heatmap-based landmark localization model paradigms
(U-Net and patch-based). We conclude by illustrating how filtering out gross
mispredictions caught in our Quantile Bins significantly improves the
proportion of predictions under an acceptable error threshold, and offer
recommendations on which uncertainty measure to use and how to use it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer-Aided Road Inspection: Systems and Algorithms. (arXiv:2203.02355v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02355">
<div class="article-summary-box-inner">
<span><p>Road damage is an inconvenience and a safety hazard, severely affecting
vehicle condition, driving comfort, and traffic safety. The traditional manual
visual road inspection process is pricey, dangerous, exhausting, and
cumbersome. Also, manual road inspection results are qualitative and
subjective, as they depend entirely on the inspector's personal experience.
Therefore, there is an ever-increasing need for automated road inspection
systems. This chapter first compares the five most common road damage types.
Then, 2-D/3-D road imaging systems are discussed. Finally, state-of-the-art
machine vision and intelligence-based road damage detection algorithms are
introduced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViT-P: Rethinking Data-efficient Vision Transformers from Locality. (arXiv:2203.02358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02358">
<div class="article-summary-box-inner">
<span><p>Recent advances of Transformers have brought new trust to computer vision
tasks. However, on small dataset, Transformers is hard to train and has lower
performance than convolutional neural networks. We make vision transformers as
data-efficient as convolutional neural networks by introducing multi-focal
attention bias. Inspired by the attention distance in a well-trained ViT, we
constrain the self-attention of ViT to have multi-scale localized receptive
field. The size of receptive field is adaptable during training so that optimal
configuration can be learned. We provide empirical evidence that proper
constrain of receptive field can reduce the amount of training data for vision
transformers. On Cifar100, our ViT-P Base model achieves the state-of-the-art
accuracy (83.16%) trained from scratch. We also perform analysis on ImageNet to
show our method does not lose accuracy on large data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiT: Self-supervised Pre-training for Document Image Transformer. (arXiv:2203.02378v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02378">
<div class="article-summary-box-inner">
<span><p>Image Transformer has recently achieved significant progress for natural
image understanding, either using supervised (ViT, DeiT, etc.) or
self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we
propose DiT, a self-supervised pre-trained Document Image Transformer model
using large-scale unlabeled text images for Document AI tasks, which is
essential since no supervised counterparts ever exist due to the lack of human
labeled document images. We leverage DiT as the backbone network in a variety
of vision-based Document AI tasks, including document image classification,
document layout analysis, as well as table detection. Experiment results have
illustrated that the self-supervised pre-trained DiT model achieves new
state-of-the-art results on these downstream tasks, e.g. document image
classification (91.11 $\rightarrow$ 92.69), document layout analysis (91.0
$\rightarrow$ 94.9) and table detection (94.23 $\rightarrow$ 96.55). The code
and pre-trained models are publicly available at \url{https://aka.ms/msdit}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoMO-Mixer: An automated multi-objective Mixer model for balanced, safe and robust prediction in medicine. (arXiv:2203.02384v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02384">
<div class="article-summary-box-inner">
<span><p>Accurately identifying patient's status through medical images plays an
important role in diagnosis and treatment. Artificial intelligence (AI),
especially the deep learning, has achieved great success in many fields.
However, more reliable AI model is needed in image guided diagnosis and
therapy. To achieve this goal, developing a balanced, safe and robust model
with a unified framework is desirable. In this study, a new unified model
termed as automated multi-objective Mixer (AutoMO-Mixer) model was developed,
which utilized a recent developed multiple layer perceptron Mixer (MLP-Mixer)
as base. To build a balanced model, sensitivity and specificity were considered
as the objective functions simultaneously in training stage. Meanwhile, a new
evidential reasoning based on entropy was developed to achieve a safe and
robust model in testing stage. The experiment on an optical coherence
tomography dataset demonstrated that AutoMO-Mixer can obtain safer, more
balanced, and robust results compared with MLP-Mixer and other available
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simultaneous Alignment and Surface Regression Using Hybrid 2D-3D Networks for 3D Coherent Layer Segmentation of Retina OCT Images. (arXiv:2203.02390v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02390">
<div class="article-summary-box-inner">
<span><p>Automated surface segmentation of retinal layer is important and challenging
in analyzing optical coherence tomography (OCT). Recently, many deep learning
based methods have been developed for this task and yield remarkable
performance. However, due to large spatial gap and potential mismatch between
the B-scans of OCT data, all of them are based on 2D segmentation of individual
B-scans, which may loss the continuity information across the B-scans. In
addition, 3D surface of the retina layers can provide more diagnostic
information, which is crucial in quantitative image analysis. In this study, a
novel framework based on hybrid 2D-3D convolutional neural networks (CNNs) is
proposed to obtain continuous 3D retinal layer surfaces from OCT. The 2D
features of individual B-scans are extracted by an encoder consisting of 2D
convolutions. These 2D features are then used to produce the alignment
displacement field and layer segmentation by two 3D decoders, which are coupled
via a spatial transformer module. The entire framework is trained end-to-end.
To the best of our knowledge, this is the first study that attempts 3D retinal
layer segmentation in volumetric OCT images based on CNNs. Experiments on a
publicly available dataset show that our framework achieves superior results to
state-of-the-art 2D methods in terms of both layer segmentation accuracy and
cross-B-scan 3D continuity, thus offering more clinical values than previous
works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mobile authentication of copy detection patterns. (arXiv:2203.02397v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02397">
<div class="article-summary-box-inner">
<span><p>In the recent years, the copy detection patterns (CDP) attracted a lot of
attention as a link between the physical and digital worlds, which is of great
interest for the internet of things and brand protection applications. However,
the security of CDP in terms of their reproducibility by unauthorized parties
or clonability remains largely unexplored. In this respect this paper addresses
a problem of anti-counterfeiting of physical objects and aims at investigating
the authentication aspects and the resistances to illegal copying of the modern
CDP from machine learning perspectives. A special attention is paid to a
reliable authentication under the real life verification conditions when the
codes are printed on an industrial printer and enrolled via modern mobile
phones under regular light conditions. The theoretical and empirical
investigation of authentication aspects of CDP is performed with respect to
four types of copy fakes from the point of view of (i) multi-class supervised
classification as a baseline approach and (ii) one-class classification as a
real-life application case. The obtained results show that the modern
machine-learning approaches and the technical capacities of modern mobile
phones allow to reliably authenticate CDP on end-user mobile phones under the
considered classes of fakes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Control Barrier Functions for Vision-based End-to-End Autonomous Driving. (arXiv:2203.02401v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02401">
<div class="article-summary-box-inner">
<span><p>Guaranteeing safety of perception-based learning systems is challenging due
to the absence of ground-truth state information unlike in state-aware control
scenarios. In this paper, we introduce a safety guaranteed learning framework
for vision-based end-to-end autonomous driving. To this end, we design a
learning system equipped with differentiable control barrier functions (dCBFs)
that is trained end-to-end by gradient descent. Our models are composed of
conventional neural network architectures and dCBFs. They are interpretable at
scale, achieve great test performance under limited training data, and are
safety guaranteed in a series of autonomous driving scenarios such as lane
keeping and obstacle avoidance. We evaluated our framework in a sim-to-real
environment, and tested on a real autonomous car, achieving safe lane following
and obstacle avoidance via Augmented Reality (AR) and real parked vehicles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing Renal Structures with 3D Block Aggregate Transformers. (arXiv:2203.02430v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02430">
<div class="article-summary-box-inner">
<span><p>Efficiently quantifying renal structures can provide distinct spatial context
and facilitate biomarker discovery for kidney morphology. However, the
development and evaluation of the transformer model to segment the renal
cortex, medulla, and collecting system remains challenging due to data
inefficiency. Inspired by the hierarchical structures in vision transformer, we
propose a novel method using a 3D block aggregation transformer for segmenting
kidney components on contrast-enhanced CT scans. We construct the first cohort
of renal substructures segmentation dataset with 116 subjects under
institutional review board (IRB) approval. Our method yields the
state-of-the-art performance (Dice of 0.8467) against the baseline approach of
0.8308 with the data-efficient design. The Pearson R achieves 0.9891 between
the proposed method and manual standards and indicates the strong correlation
and reproducibility for volumetric analysis. We extend the proposed method to
the public KiTS dataset, the method leads to improved accuracy compared to
transformer-based approaches. We show that the 3D block aggregation transformer
can achieve local communication between sequence representations without
modifying self-attention, and it can serve as an accurate and efficient
quantification tool for characterizing renal structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Efficient Lane Detection via Curve Modeling. (arXiv:2203.02431v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02431">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel parametric curve-based method for lane detection
in RGB images. Unlike state-of-the-art segmentation-based and point
detection-based methods that typically require heuristics to either decode
predictions or formulate a large sum of anchors, the curve-based methods can
learn holistic lane representations naturally. To handle the optimization
difficulties of existing polynomial curve methods, we propose to exploit the
parametric B\'ezier curve due to its ease of computation, stability, and high
freedom degrees of transformations. In addition, we propose the deformable
convolution-based feature flip fusion, for exploiting the symmetry properties
of lanes in driving scenes. The proposed method achieves a new state-of-the-art
performance on the popular LLAMAS benchmark. It also achieves favorable
accuracy on the TuSimple and CULane datasets, while retaining both low latency
(&gt; 150 FPS) and small model size (&lt; 10M). Our method can serve as a new
baseline, to shed the light on the parametric curves modeling for lane
detection. Codes of our model and PytorchAutoDrive: a unified framework for
self-driving perception, are available at:
https://github.com/voldemortX/pytorch-auto-drive .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SFPN: Synthetic FPN for Object Detection. (arXiv:2203.02445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02445">
<div class="article-summary-box-inner">
<span><p>FPN (Feature Pyramid Network) has become a basic component of most SoTA one
stage object detectors. Many previous studies have repeatedly proved that FPN
can caputre better multi-scale feature maps to more precisely describe objects
if they are with different sizes. However, for most backbones such VGG, ResNet,
or DenseNet, the feature maps at each layer are downsized to their quarters due
to the pooling operation or convolutions with stride 2. The gap of
down-scaling-by-2 is large and makes its FPN not fuse the features smoothly.
This paper proposes a new SFPN (Synthetic Fusion Pyramid Network) arichtecture
which creates various synthetic layers between layers of the original FPN to
enhance the accuracy of light-weight CNN backones to extract objects' visual
features more accurately. Finally, experiments prove the SFPN architecture
outperforms either the large backbone VGG16, ResNet50 or light-weight backbones
such as MobilenetV2 based on AP score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextformer: A Transformer with Spatio-Channel Attention for Context Modeling in Learned Image Compression. (arXiv:2203.02452v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02452">
<div class="article-summary-box-inner">
<span><p>Entropy modeling is a key component for high-performance image compression
algorithms. Recent developments in autoregressive context modeling helped
learning-based methods to surpass their classical counterparts. However, the
performance of those models can be further improved due to the underexploited
spatio-channel dependencies in latent space, and the suboptimal implementation
of context adaptivity. Inspired by the adaptive characteristics of the
transformers, we propose a transformer-based context model, a.k.a.
Contextformer, which generalizes the de facto standard attention mechanism to
spatio-channel attention. We replace the context model of a modern compression
framework with the Contextformer and test it on the widely used Kodak image
dataset. Our experimental results show that the proposed model provides up to
10% rate savings compared to the standard Versatile Video Coding (VVC) Test
Model (VTM) 9.1, and outperforms various learning-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Hybrid Mapping of Populated Indoor Scenes using a Low-Cost Monocular UAV. (arXiv:2203.02453v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02453">
<div class="article-summary-box-inner">
<span><p>Unmanned aerial vehicles (UAVs) have been used for many applications in
recent years, from urban search and rescue, to agricultural surveying, to
autonomous underground mine exploration. However, deploying UAVs in tight,
indoor spaces, especially close to humans, remains a challenge. One solution,
when limited payload is required, is to use micro-UAVs, which pose less risk to
humans and typically cost less to replace after a crash. However, micro-UAVs
can only carry a limited sensor suite, e.g. a monocular camera instead of a
stereo pair or LiDAR, complicating tasks like dense mapping and markerless
multi-person 3D human pose estimation, which are needed to operate in tight
environments around people. Monocular approaches to such tasks exist, and dense
monocular mapping approaches have been successfully deployed for UAV
applications. However, despite many recent works on both marker-based and
markerless multi-UAV single-person motion capture, markerless single-camera
multi-person 3D human pose estimation remains a much earlier-stage technology,
and we are not aware of existing attempts to deploy it in an aerial context. In
this paper, we present what is thus, to our knowledge, the first system to
perform simultaneous mapping and multi-person 3D human pose estimation from a
monocular camera mounted on a single UAV. In particular, we show how to loosely
couple state-of-the-art monocular depth estimation and monocular 3D human pose
estimation approaches to reconstruct a hybrid map of a populated indoor scene
in real time. We validate our component-level design choices via extensive
experiments on the large-scale ScanNet and GTA-IM datasets. To evaluate our
system-level performance, we also construct a new Oxford Hybrid Mapping dataset
of populated indoor scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Didn't see that coming: a survey on non-verbal social human behavior forecasting. (arXiv:2203.02480v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02480">
<div class="article-summary-box-inner">
<span><p>Non-verbal social human behavior forecasting has increasingly attracted the
interest of the research community in recent years. Its direct applications to
human-robot interaction and socially-aware human motion generation make it a
very attractive field. In this survey, we define the behavior forecasting
problem for multiple interactive agents in a generic way that aims at unifying
the fields of social signals prediction and human motion forecasting,
traditionally separated. We hold that both problem formulations refer to the
same conceptual problem, and identify many shared fundamental challenges:
future stochasticity, context awareness, history exploitation, etc. We also
propose a taxonomy that comprises methods published in the last 5 years in a
very informative way and describes the current main concerns of the community
with regard to this problem. In order to promote further research on this
field, we also provide a summarised and friendly overview of audiovisual
datasets featuring non-acted social interactions. Finally, we describe the most
common metrics used in this task and their particular issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set Methods. (arXiv:2203.02486v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02486">
<div class="article-summary-box-inner">
<span><p>In many object recognition applications, the set of possible categories is an
open set, and the deployed recognition system will encounter novel objects
belonging to categories unseen during training. Detecting such "novel category"
objects is usually formulated as an anomaly detection problem. Anomaly
detection algorithms for feature-vector data identify anomalies as outliers,
but outlier detection has not worked well in deep learning. Instead, methods
based on the computed logits of visual object classifiers give state-of-the-art
performance. This paper proposes the Familiarity Hypothesis that these methods
succeed because they are detecting the absence of familiar learned features
rather than the presence of novelty. The paper reviews evidence from the
literature and presents additional evidence from our own experiments that
provide strong support for this hypothesis. The paper concludes with a
discussion of whether familiarity detection is an inevitable consequence of
representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Behavioural Curves Analysis Using Near-Infrared-Iris Image Sequences. (arXiv:2203.02488v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02488">
<div class="article-summary-box-inner">
<span><p>This paper proposes a new method to estimate behavioural curves from a stream
of Near-Infra-Red (NIR) iris video frames. This method can be used in a Fitness
For Duty system (FFD). The research focuses on determining the effect of
external factors such as alcohol, drugs, and sleepiness on the Central Nervous
System (CNS). The aim is to analyse how this behaviour is represented on iris
and pupil movements and if it is possible to capture these changes with a
standard NIR camera. The behaviour analysis showed essential differences in
pupil and iris behaviour to classify the workers in "Fit" or "Unfit"
conditions. The best results can distinguish subjects robustly under alcohol,
drug consumption, and sleep conditions. The Multi-Layer-Perceptron and Gradient
Boosted Machine reached the best results in all groups with an overall accuracy
for Fit and Unfit classes of 74.0% and 75.5%, respectively. These results open
a new application for iris capture devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pedestrian Stop and Go Forecasting with Hybrid Feature Fusion. (arXiv:2203.02489v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02489">
<div class="article-summary-box-inner">
<span><p>Forecasting pedestrians' future motions is essential for autonomous driving
systems to safely navigate in urban areas. However, existing prediction
algorithms often overly rely on past observed trajectories and tend to fail
around abrupt dynamic changes, such as when pedestrians suddenly start or stop
walking. We suggest that predicting these highly non-linear transitions should
form a core component to improve the robustness of motion prediction
algorithms. In this paper, we introduce the new task of pedestrian stop and go
forecasting. Considering the lack of suitable existing datasets for it, we
release TRANS, a benchmark for explicitly studying the stop and go behaviors of
pedestrians in urban traffic. We build it from several existing datasets
annotated with pedestrians' walking motions, in order to have various scenarios
and behaviors. We also propose a novel hybrid model that leverages
pedestrian-specific and scene features from several modalities, both video
sequences and high-level attributes, and gradually fuses them to integrate
multiple levels of context. We evaluate our model and several baselines on
TRANS, and set a new benchmark for the community to work on pedestrian stop and
go forecasting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening. (arXiv:2203.02503v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02503">
<div class="article-summary-box-inner">
<span><p>Pansharpening aims to fuse a registered high-resolution panchromatic image
(PAN) with a low-resolution hyperspectral image (LR-HSI) to generate an
enhanced HSI with high spectral and spatial resolution. Existing pansharpening
approaches neglect using an attention mechanism to transfer HR texture features
from PAN to LR-HSI features, resulting in spatial and spectral distortions. In
this paper, we present a novel attention mechanism for pansharpening called
HyperTransformer, in which features of LR-HSI and PAN are formulated as queries
and keys in a transformer, respectively. HyperTransformer consists of three
main modules, namely two separate feature extractors for PAN and HSI, a
multi-head feature soft attention module, and a spatial-spectral feature fusion
module. Such a network improves both spatial and spectral quality measures of
the pansharpened HSI by learning cross-feature space dependencies and
long-range details of PAN and LR-HSI. Furthermore, HyperTransformer can be
utilized across multiple spatial scales at the backbone for obtaining improved
performance. Extensive experiments conducted on three widely used datasets
demonstrate that HyperTransformer achieves significant improvement over the
state-of-the-art methods on both spatial and spectral quality measures.
Implementation code and pre-trained weights can be accessed at
https://github.com/wgcban/HyperTransformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RGB cameras failures and their effects in autonomous driving applications. (arXiv:2008.05938v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.05938">
<div class="article-summary-box-inner">
<span><p>RGB cameras are one of the most relevant sensors for autonomous driving
applications. It is undeniable that failures of vehicle cameras may compromise
the autonomous driving task, possibly leading to unsafe behaviors when images
that are subsequently processed by the driving system are altered. To support
the definition of safe and robust vehicle architectures and intelligent
systems, in this paper we define the failure modes of a vehicle camera,
together with an analysis of effects and known mitigations. Further, we build a
software library for the generation of the corresponding failed images and we
feed them to six object detectors for mono and stereo cameras and to the
self-driving agent of an autonomous driving simulator. The resulting
misbehaviors with respect to operating with clean images allow a better
understanding of failures effects and the related safety risks in image-based
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does anatomical contextual information improve 3D U-Net based brain tumor segmentation?. (arXiv:2010.13460v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13460">
<div class="article-summary-box-inner">
<span><p>Effective, robust, and automatic tools for brain tumor segmentation are
needed for the extraction of information useful in treatment planning from
magnetic resonance (MR) images. Context-aware artificial intelligence is an
emerging concept for the development of deep learning applications for
computer-aided medical image analysis. In this work, it is investigated whether
the addition of contextual information from the brain anatomy in the form of
white matter, gray matter, and cerebrospinal fluid masks and probability maps
improves U-Net-based brain tumor segmentation. The BraTS2020 dataset was used
to train and test two standard 3D U-Net models that, in addition to the
conventional MR image modalities, used the anatomical contextual information as
extra channels in the form of binary masks (CIM) or probability maps (CIP). A
baseline model (BLM) that only used the conventional MR image modalities was
also trained. The impact of adding contextual information was investigated in
terms of overall segmentation accuracy, model training time, domain
generalization, and compensation for fewer MR modalities available for each
subject. Results show that there is no statistically significant difference
when comparing Dice scores between the baseline model and the contextual
information models, even when comparing performances for high- and low-grade
tumors independently. Only in the case of compensation for fewer MR modalities
available for each subject did the addition of anatomical contextual
information significantly improve the segmentation of the whole tumor. Overall,
there is no overall significant improvement in segmentation performance when
using anatomical contextual information in the form of either binary masks or
probability maps as extra channels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepMI: A Mutual Information Based Framework For Unsupervised Deep Learning of Tasks. (arXiv:2101.06411v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06411">
<div class="article-summary-box-inner">
<span><p>In this work, we propose an information theory based framework DeepMI to
train deep neural networks (DNN) using Mutual Information (MI). The DeepMI
framework is especially targeted but not limited to the learning of real world
tasks in an unsupervised manner. The primary motivation behind this work is the
limitation of the traditional loss functions for unsupervised learning of a
given task. Directly using MI for the training purpose is quite challenging to
deal with because of its unbounded above nature. Hence, we develop an
alternative linearized representation of MI as a part of the framework.
Contributions of this paper are three fold: i) investigation of MI to train
deep neural networks, ii) novel loss function LLMI , and iii) a fuzzy logic
based end-to-end differentiable pipeline to integrate DeepMI into deep learning
framework. Due to the unavailability of a standard benchmark, we carefully
design the experimental analysis and select three different tasks for the
experimental study. We demonstrate that L LMI alone provides better gradients
to achieve a neural network better performance over the popular loss functions,
also in the cases when multiple loss functions are used for a given task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Inverse Problems by Joint Posterior Maximization with Autoencoding Prior. (arXiv:2103.01648v3 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01648">
<div class="article-summary-box-inner">
<span><p>In this work we address the problem of solving ill-posed inverse problems in
imaging where the prior is a variational autoencoder (VAE). Specifically we
consider the decoupled case where the prior is trained once and can be reused
for many different log-concave degradation models without retraining. Whereas
previous MAP-based approaches to this problem lead to highly non-convex
optimization algorithms, our approach computes the joint (space-latent) MAP
that naturally leads to alternate optimization algorithms and to the use of a
stochastic encoder to accelerate computations. The resulting technique (JPMAP)
performs Joint Posterior Maximization using an Autoencoding Prior. We show
theoretical and experimental evidence that the proposed objective function is
quite close to bi-convex. Indeed it satisfies a weak bi-convexity property
which is sufficient to guarantee that our optimization scheme converges to a
stationary point. We also highlight the importance of correctly training the
VAE using a denoising criterion, in order to ensure that the encoder
generalizes well to out-of-distribution images, without affecting the quality
of the generative model. This simple modification is key to providing
robustness to the whole procedure. Finally we show how our joint MAP
methodology relates to more common MAP approaches, and we propose a
continuation scheme that makes use of our JPMAP algorithm to provide more
robust MAP estimates. Experimental results also show the higher quality of the
solutions obtained by our JPMAP approach with respect to other non-convex MAP
approaches which more often get stuck in spurious local optima.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransCrowd: Weakly-Supervised Crowd Counting with Transformer. (arXiv:2104.09116v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09116">
<div class="article-summary-box-inner">
<span><p>The mainstream crowd counting methods usually utilize the convolution neural
network (CNN) to regress a density map, requiring point-level annotations.
However, annotating each person with a point is an expensive and laborious
process. During the testing phase, the point-level annotations are not
considered to evaluate the counting accuracy, which means the point-level
annotations are redundant. Hence, it is desirable to develop weakly-supervised
counting methods that just rely on count-level annotations, a more economical
way of labeling. Current weakly-supervised counting methods adopt the CNN to
regress a total count of the crowd by an image-to-count paradigm. However,
having limited receptive fields for context modeling is an intrinsic limitation
of these weakly-supervised CNN-based methods. These methods thus can not
achieve satisfactory performance, with limited applications in the real-word.
The Transformer is a popular sequence-to-sequence prediction model in NLP,
which contains a global receptive field. In this paper, we propose TransCrowd,
which reformulates the weakly-supervised crowd counting problem from the
perspective of sequence-to-count based on Transformer. We observe that the
proposed TransCrowd can effectively extract the semantic crowd information by
using the self-attention mechanism of Transformer. To the best of our
knowledge, this is the first work to adopt a pure Transformer for crowd
counting research. Experiments on five benchmark datasets demonstrate that the
proposed TransCrowd achieves superior performance compared with all the
weakly-supervised CNN-based counting methods and gains highly competitive
counting performance compared with some popular fully-supervised counting
methods. An implementation of our method is available at
https://github.com/dk-liang/TransCrowd
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Model Sparsification by Scheduled Grow-and-Prune Methods. (arXiv:2106.09857v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09857">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are effective in solving many real-world
problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but
their excessive computation results in long inference time. Model
sparsification can reduce the computation and memory cost while maintaining
model quality. Most existing sparsification algorithms unidirectionally remove
weights, while others randomly or greedily explore a small subset of weights in
each layer for pruning. The limitations of these algorithms reduce the level of
achievable sparsity. In addition, many algorithms still require pre-trained
dense models and thus suffer from large memory footprint. In this paper, we
propose a novel scheduled grow-and-prune (GaP) methodology without having to
pre-train a dense model. It addresses the shortcomings of the previous works by
repeatedly growing a subset of layers to dense and then pruning them back to
sparse after some training. Experiments show that the models pruned using the
proposed methods match or beat the quality of the highly optimized dense models
at 80% sparsity on a variety of tasks, such as image classification, objective
detection, 3D object part segmentation, and translation. They also outperform
other state-of-the-art (SOTA) methods for model sparsification. As an example,
a 90% non-uniform sparse ResNet-50 model obtained via GaP achieves 77.9% top-1
accuracy on ImageNet, improving the previous SOTA results by 1.5%. Code
available at: https://github.com/boone891214/GaP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FP-Age: Leveraging Face Parsing Attention for Facial Age Estimation in the Wild. (arXiv:2106.11145v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11145">
<div class="article-summary-box-inner">
<span><p>Image-based age estimation aims to predict a person's age from facial images.
It is used in a variety of real-world applications. Although end-to-end deep
models have achieved impressive results for age estimation on benchmark
datasets, their performance in-the-wild still leaves much room for improvement
due to the challenges caused by large variations in head pose, facial
expressions, and occlusions. To address this issue, we propose a simple yet
effective method to explicitly incorporate facial semantics into age
estimation, so that the model would learn to correctly focus on the most
informative facial components from unaligned facial images regardless of head
pose and non-rigid deformation. To this end, we design a face parsing-based
network to learn semantic information at different scales and a novel face
parsing attention module to leverage these semantic features for age
estimation. To evaluate our method on in-the-wild data, we also introduce a new
challenging large-scale benchmark called IMDB-Clean. This dataset is created by
semi-automatically cleaning the noisy IMDB-WIKI dataset using a constrained
clustering method. Through comprehensive experiment on IMDB-Clean and other
benchmark datasets, under both intra-dataset and cross-dataset evaluation
protocols, we show that our method consistently outperforms all existing age
estimation methods and achieves a new state-of-the-art performance. To the best
of our knowledge, our work presents the first attempt of leveraging face
parsing attention to achieve semantic-aware age estimation, which may be
inspiring to other high level facial analysis tasks. Code and data are
available on \url{https://github.com/ibug-group/fpage}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modality Deep Restoration of Extremely Compressed Face Videos. (arXiv:2107.05548v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05548">
<div class="article-summary-box-inner">
<span><p>Arguably the most common and salient object in daily video communications is
the talking head, as encountered in social media, virtual classrooms,
teleconferences, news broadcasting, talk shows, etc. When communication
bandwidth is limited by network congestions or cost effectiveness, compression
artifacts in talking head videos are inevitable. The resulting video quality
degradation is highly visible and objectionable due to high acuity of human
visual system to faces. To solve this problem, we develop a multi-modality deep
convolutional neural network method for restoring face videos that are
aggressively compressed. The main innovation is a new DCNN architecture that
incorporates known priors of multiple modalities: the video-synchronized speech
signal and semantic elements of the compression code stream, including motion
vectors, code partition map and quantization parameters. These priors strongly
correlate with the latent video and hence they are able to enhance the
capability of deep learning to remove compression artifacts. Ample empirical
evidences are presented to validate the superior performance of the proposed
DCNN method on face videos over the existing state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Text-to-Face GAN -ST^2FG. (arXiv:2107.10756v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10756">
<div class="article-summary-box-inner">
<span><p>Faces generated using generative adversarial networks (GANs) have reached
unprecedented realism. These faces, also known as "Deep Fakes", appear as
realistic photographs with very little pixel-level distortions. While some work
has enabled the training of models that lead to the generation of specific
properties of the subject, generating a facial image based on a natural
language description has not been fully explored. For security and criminal
identification, the ability to provide a GAN-based system that works like a
sketch artist would be incredibly useful. In this paper, we present a novel
approach to generate facial images from semantic text descriptions. The learned
model is provided with a text description and an outline of the type of face,
which the model uses to sketch the features. Our models are trained using an
Affine Combination Module (ACM) mechanism to combine the text embedding from
BERT and the GAN latent space using a self-attention matrix. This avoids the
loss of features due to inadequate "attention", which may happen if text
embedding and latent vector are simply concatenated. Our approach is capable of
generating images that are very accurately aligned to the exhaustive textual
descriptions of faces with many fine detail features of the face and helps in
generating better images. The proposed method is also capable of making
incremental changes to a previously generated image if it is provided with
additional textual descriptions or sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Vision Transformers See Like Convolutional Neural Networks?. (arXiv:2108.08810v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08810">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have so far been the de-facto model for
visual data. Recent work has shown that (Vision) Transformer models (ViT) can
achieve comparable or even superior performance on image classification tasks.
This raises a central question: how are Vision Transformers solving these
tasks? Are they acting like convolutional networks, or learning entirely
different visual representations? Analyzing the internal representation
structure of ViTs and CNNs on image classification benchmarks, we find striking
differences between the two architectures, such as ViT having more uniform
representations across all layers. We explore how these differences arise,
finding crucial roles played by self-attention, which enables early aggregation
of global information, and ViT residual connections, which strongly propagate
features from lower to higher layers. We study the ramifications for spatial
localization, demonstrating ViTs successfully preserve input spatial
information, with noticeable effects from different classification methods.
Finally, we study the effect of (pretraining) dataset scale on intermediate
features and transfer learning, and conclude with a discussion on connections
to new architectures such as the MLP-Mixer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Visual Navigation under Partial Observability. (arXiv:2109.07752v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07752">
<div class="article-summary-box-inner">
<span><p>How can a robot navigate successfully in rich and diverse environments,
indoors or outdoors, along office corridors or trails on the grassland, on the
flat ground or the staircase? To this end, this work aims to address three
challenges: (i) complex visual observations, (ii) partial observability of
local visual sensing, and (iii) multimodal robot behaviors conditioned on both
the local environment and the global navigation objective. We propose to train
a neural network (NN) controller for local navigation via imitation learning.
To tackle complex visual observations, we extract multi-scale spatial
representations through CNNs. To tackle partial observability, we aggregate
multi-scale spatial information over time and encode it in LSTMs. To learn
multimodal behaviors, we use a separate memory module for each behavior mode.
Importantly, we integrate the multiple neural network modules into a unified
controller that achieves robust performance for visual navigation in complex,
partially observable environments. We implemented the controller on the
quadrupedal Spot robot and evaluated it on three challenging tasks: adversarial
pedestrian avoidance, blind-spot obstacle avoidance, and elevator riding. The
experiments show that the proposed NN architecture significantly improves
navigation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-Assemble: Leveraging Multiple Datasets with Partial Labels. (arXiv:2109.12265v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12265">
<div class="article-summary-box-inner">
<span><p>The success of deep learning relies heavily on large and diverse datasets
with extensive labels, but we often only have access to several small datasets
associated with partial labels. In this paper, we start a new initiative,
"Label-Assemble", that aims to unleash the full potential of partially labeled
data from an assembly of public datasets. Specifically, we introduce a new
dynamic adapter to encode different visual tasks, which addresses the
challenges of incomparable, heterogeneous, or even conflicting labeling
protocols. We also employ pseudo-labeling and consistency constraints to
harness data with missing labels and to mitigate the domain gap across
datasets. From rigorous evaluations on three natural imaging and six medical
imaging tasks, we discover that learning from "negative examples" facilitates
both classification and segmentation of classes of interest. This sheds new
light on the computer-aided diagnosis of rare diseases and emerging pandemics,
wherein "positive examples" are hard to collect, yet "negative examples" are
relatively easier to assemble. Apart from exceeding prior arts in the ChestXray
benchmark, our model is particularly strong in identifying diseases of minority
classes, yielding over 3-point improvement on average. Remarkably, when using
existing partial labels, our model performance is on-par with that using full
labels, eliminating the need for an additional 40% of annotation costs. Code
will be made available at https://github.com/MrGiovanni/LabelAssemble.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer. (arXiv:2110.02178v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02178">
<div class="article-summary-box-inner">
<span><p>Light-weight convolutional neural networks (CNNs) are the de-facto for mobile
vision tasks. Their spatial inductive biases allow them to learn
representations with fewer parameters across different vision tasks. However,
these networks are spatially local. To learn global representations,
self-attention-based vision trans-formers (ViTs) have been adopted. Unlike
CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is
it possible to combine the strengths of CNNs and ViTs to build a light-weight
and low latency network for mobile vision tasks? Towards this end, we introduce
MobileViT, a light-weight and general-purpose vision transformer for mobile
devices. MobileViT presents a different perspective for the global processing
of information with transformers, i.e., transformers as convolutions. Our
results show that MobileViT significantly outperforms CNN- and ViT-based
networks across different tasks and datasets. On the ImageNet-1k dataset,
MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters,
which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT
(ViT-based) for a similar number of parameters. On the MS-COCO object detection
task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of
parameters.
</p>
<p>Our source code is open-source and available at:
https://github.com/apple/ml-cvnets
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gray Matter Segmentation in Ultra High Resolution 7 Tesla ex vivo T2w MRI of Human Brain Hemispheres. (arXiv:2110.07711v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07711">
<div class="article-summary-box-inner">
<span><p>Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for
visualizing and characterizing detailed neuroanatomy. However, automated
cortical segmentation methods in ex vivo MRI are not well developed, primarily
due to limited availability of labeled datasets, and heterogeneity in scanner
hardware and acquisition protocols. In this work, we present a high resolution
7 Tesla dataset of 32 ex vivo human brain specimens. We benchmark the cortical
mantle segmentation performance of nine neural network architectures, trained
and evaluated using manually-segmented 3D patches sampled from specific
cortical regions, and show excellent generalizing capabilities across whole
brain hemispheres in different specimens, and also on unseen images acquired at
different magnetic field strength and imaging sequences. Finally, we provide
cortical thickness measurements across key regions in 3D ex vivo human brain
images. Our code and processed datasets are publicly available at
https://github.com/Pulkit-Khandelwal/picsl-ex-vivo-segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilateral-ViT for Robust Fovea Localization. (arXiv:2110.09860v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09860">
<div class="article-summary-box-inner">
<span><p>The fovea is an important anatomical landmark of the retina. Detecting the
location of the fovea is essential for the analysis of many retinal diseases.
However, robust fovea localization remains a challenging problem, as the fovea
region often appears fuzzy, and retina diseases may further obscure its
appearance. This paper proposes a novel Vision Transformer (ViT) approach that
integrates information both inside and outside the fovea region to achieve
robust fovea localization. Our proposed network, named
Bilateral-Vision-Transformer (Bilateral-ViT), consists of two network branches:
a transformer-based main network branch for integrating global context across
the entire fundus image and a vessel branch for explicitly incorporating the
structure of blood vessels. The encoded features from both network branches are
subsequently merged with a customized Multi-scale Feature Fusion (MFF) module.
Our comprehensive experiments demonstrate that the proposed approach is
significantly more robust for diseased images and establishes the new state of
the arts using the Messidor and PALM datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study of Multimodal Person Verification Using Audio-Visual-Thermal Data. (arXiv:2110.12136v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12136">
<div class="article-summary-box-inner">
<span><p>In this paper, we study an approach to multimodal person verification using
audio, visual, and thermal modalities. The combination of audio and visual
modalities has already been shown to be effective for robust person
verification. From this perspective, we investigate the impact of further
increasing the number of modalities by adding thermal images. In particular, we
implemented unimodal, bimodal, and trimodal verification systems using
state-of-the-art deep learning architectures and compared their performance
under clean and noisy conditions. We also compared two popular fusion
approaches based on simple score averaging and the soft attention mechanism.
The experiment conducted on the SpeakingFaces dataset demonstrates the superior
performance of the trimodal verification system. Specifically, on the easy test
set, the trimodal system outperforms the best unimodal and bimodal systems by
over 50% and 18% relative equal error rates, respectively, under both the clean
and noisy conditions. On the hard test set, the trimodal system outperforms the
best unimodal and bimodal systems by over 40% and 13% relative equal error
rates, respectively, under both the clean and noisy conditions. To enable
reproducibility of the experiment and facilitate research into multimodal
person verification, we made our code, pretrained models, and preprocessed
dataset freely available in our GitHub repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Texture-enhanced Light Field Super-resolution with Spatio-Angular Decomposition Kernels. (arXiv:2111.04069v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04069">
<div class="article-summary-box-inner">
<span><p>Despite the recent progress in light field super-resolution (LFSR) achieved
by convolutional neural networks, the correlation information of light field
(LF) images has not been sufficiently studied and exploited due to the
complexity of 4D LF data. To cope with such high-dimensional LF data, most of
the existing LFSR methods resorted to decomposing it into lower dimensions and
subsequently performing optimization on the decomposed sub-spaces. However,
these methods are inherently limited as they neglected the characteristics of
the decomposition operations and only utilized a limited set of LF sub-spaces
ending up failing to sufficiently extract spatio-angular features and leading
to a performance bottleneck. To overcome these limitations, in this paper, we
comprehensively discover the potentials of LF decomposition and propose a novel
concept of decomposition kernels. In particular, we systematically unify the
decomposition operations of various sub-spaces into a series of such
decomposition kernels, which are incorporated into our proposed Decomposition
Kernel Network (DKNet) for comprehensive spatio-angular feature extraction. The
proposed DKNet is experimentally verified to achieve considerable improvements
compared with the state-of-the-art methods. To further improve DKNet in
producing more visually pleasing LFSR results, based on the VGG network, we
propose a LFVGG loss to guide the Texture-Enhanced DKNet (TE-DKNet) to generate
rich authentic textures and enhance LF images' visual quality significantly. We
also propose an indirect evaluation metric by taking advantage of LF material
recognition to objectively assess the perceptual enhancement brought by the
LFVGG loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Aegis: Robust adversarial protectors for medical images. (arXiv:2111.10969v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10969">
<div class="article-summary-box-inner">
<span><p>Deep neural network based medical image systems are vulnerable to adversarial
examples. Many defense mechanisms have been proposed in the literature,
however, the existing defenses assume a passive attacker who knows little about
the defense system and does not change the attack strategy according to the
defense. Recent works have shown that a strong adaptive attack, where an
attacker is assumed to have full knowledge about the defense system, can easily
bypass the existing defenses. In this paper, we propose a novel adversarial
example defense system called Medical Aegis. To the best of our knowledge,
Medical Aegis is the first defense in the literature that successfully
addresses the strong adaptive adversarial example attacks to medical images.
Medical Aegis boasts two-tier protectors: The first tier of Cushion weakens the
adversarial manipulation capability of an attack by removing its high-frequency
components, yet posing a minimal effect on classification performance of the
original image; the second tier of Shield learns a set of per-class DNN models
to predict the logits of the protected model. Deviation from the Shield's
prediction indicates adversarial examples. Shield is inspired by the
observations in our stress tests that there exist robust trails in the shallow
layers of a DNN model, which the adaptive attacks can hardly destruct.
Experimental results show that the proposed defense accurately detects adaptive
attacks, with negligible overhead for model inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch Feature Swapping for Bodies and Faces. (arXiv:2111.12448v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12448">
<div class="article-summary-box-inner">
<span><p>Learning a disentangled, interpretable, and structured latent representation
in 3D generative models of faces and bodies is still an open problem. The
problem is particularly acute when control over identity features is required.
In this paper, we propose an intuitive yet effective self-supervised approach
to train a 3D shape variational autoencoder (VAE) which encourages a
disentangled latent representation of identity features. Curating the
mini-batch generation by swapping arbitrary features across different shapes
allows to define a loss function leveraging known differences and similarities
in the latent representations. Experimental results conducted on 3D meshes show
that state-of-the-art methods for latent disentanglement are not able to
disentangle identity features of faces and bodies. Our proposed method properly
decouples the generation of such features while maintaining good representation
and reconstruction capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity. (arXiv:2111.14330v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14330">
<div class="article-summary-box-inner">
<span><p>DETR is the first end-to-end object detector using a transformer
encoder-decoder architecture and demonstrates competitive performance but low
computational efficiency on high resolution feature maps. The subsequent work,
Deformable DETR, enhances the efficiency of DETR by replacing dense attention
with deformable attention, which achieves 10x faster convergence and improved
performance. Deformable DETR uses the multiscale feature to ameliorate
performance, however, the number of encoder tokens increases by 20x compared to
DETR, and the computation cost of the encoder attention remains a bottleneck.
In our preliminary experiment, we observe that the detection performance hardly
deteriorates even if only a part of the encoder token is updated. Inspired by
this observation, we propose Sparse DETR that selectively updates only the
tokens expected to be referenced by the decoder, thus help the model
effectively detect objects. In addition, we show that applying an auxiliary
detection loss on the selected tokens in the encoder improves the performance
while minimizing computational overhead. We validate that Sparse DETR achieves
better performance than Deformable DETR even with only 10% encoder tokens on
the COCO dataset. Albeit only the encoder tokens are sparsified, the total
computation cost decreases by 38% and the frames per second (FPS) increases by
42% compared to Deformable DETR.
</p>
<p>Code is available at https://github.com/kakaobrain/sparse-detr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPstyler: Image Style Transfer with a Single Text Condition. (arXiv:2112.00374v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00374">
<div class="article-summary-box-inner">
<span><p>Existing neural style transfer methods require reference style images to
transfer texture information of style images to content images. However, in
many practical situations, users may not have reference style images but still
be interested in transferring styles by just imagining them. In order to deal
with such applications, we propose a new framework that enables a style
transfer `without' a style image, but only with a text description of the
desired style. Using the pre-trained text-image embedding model of CLIP, we
demonstrate the modulation of the style of content images only with a single
text condition. Specifically, we propose a patch-wise text-image matching loss
with multiview augmentations for realistic texture transfer. Extensive
experimental results confirmed the successful image style transfer with
realistic textures that reflect semantic query texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Saliency Enhancement using Superpixel Similarity. (arXiv:2112.00665v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00665">
<div class="article-summary-box-inner">
<span><p>Saliency Object Detection (SOD) has several applications in image analysis.
The methods have evolved from image-intrinsic to object-inspired
(deep-learning-based) models. When a model fail, however, there is no
alternative to enhance its saliency map. We fill this gap by introducing a
hybrid approach, named \textit{Iterative Saliency Enhancement over Superpixel
Similarity} (ISESS), that iteratively generates enhanced saliency maps by
executing two operations alternately: object-based superpixel segmentation and
superpixel-based saliency estimation -- cycling operations never exploited.
ISESS estimates seeds for superpixel delineation from a given saliency map and
defines superpixel queries in the foreground and background. A new saliency map
results from color similarities between queries and superpixels at each
iteration. The process repeats and, after a given number of iterations, the
generated saliency maps are combined into one by cellular automata. Finally,
the resulting map is merged with the initial one by the maximum bewteen their
average values per superpixel. We demonstrate that our hybrid model can
consistently outperform three state-of-the-art deep-learning-based methods on
five image datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Modeling of Turbulence. (arXiv:2112.02548v2 [physics.flu-dyn] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02548">
<div class="article-summary-box-inner">
<span><p>We present a mathematically well founded approach for the synthetic modeling
of turbulent flows using generative adversarial networks (GAN). Based on the
analysis of chaotic, deterministic systems in terms of ergodicity, we outline a
mathematical proof that GAN can actually learn to sample state snapshots form
the invariant measure of the chaotic system. Based on this analysis, we study a
hierarchy of chaotic systems starting with the Lorenz attractor and then carry
on to the modeling of turbulent flows with GAN. As training data, we use fields
of velocity fluctuations obtained from large eddy simulations (LES). Two
architectures are investigated in detail: we use a deep, convolutional GAN
(DCGAN) to synthesise the turbulent flow around a cylinder. We furthermore
simulate the flow around a low pressure turbine stator using the pix2pixHD
architecture for a conditional DCGAN being conditioned on the position of a
rotating wake in front of the stator. The settings of adversarial training and
the effects of using specific GAN architectures are explained. We thereby show
that GAN are efficient in simulating turbulence in technically challenging flow
problems on the basis of a moderate amount of training data. GAN training and
inference times significantly fall short when compared with classical numerical
methods, in particular LES, while still providing turbulent flows in high
resolution. We furthermore analyse the statistical properties of the
synthesized and LES flow fields, which agree excellently. We also show the
ability of the conditional GAN to generalize over changes of geometry by
generating turbulent flow fields for positions of the wake that are not
included in the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-based Image Captioning. (arXiv:2112.06558v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06558">
<div class="article-summary-box-inner">
<span><p>Text-based image captioning (TextCap) requires simultaneous comprehension of
visual content and reading the text of images to generate a natural language
description. Although a task can teach machines to understand the complex human
environment further given that text is omnipresent in our daily surroundings,
it poses additional challenges in normal captioning. A text-based image
intuitively contains abundant and complex multimodal relational content, that
is, image details can be described diversely from multiview rather than a
single caption. Certainly, we can introduce additional paired training data to
show the diversity of images' descriptions, this process is labor-intensive and
time-consuming for TextCap pair annotations with extra texts. Based on the
insight mentioned above, we investigate how to generate diverse captions that
focus on different image parts using an unpaired training paradigm. We propose
the Multimodal relAtional Graph adversarIal inferenCe (MAGIC) framework for
diverse and unpaired TextCap. This framework can adaptively construct multiple
multimodal relational graphs of images and model complex relationships among
graphs to represent descriptive diversity. Moreover, a cascaded generative
adversarial network is developed from modeled graphs to infer the unpaired
caption generation in image-sentence feature alignment and linguistic coherence
levels. We validate the effectiveness of MAGIC in generating diverse captions
from different relational information items of an image. Experimental results
show that MAGIC can generate very promising outcomes without using any
image-caption training pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forensic Analysis of Synthetically Generated Scientific Images. (arXiv:2112.08739v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08739">
<div class="article-summary-box-inner">
<span><p>The widespread diffusion of synthetically generated content is a serious
threat that needs urgent countermeasures. The generation of synthetic content
is not restricted to multimedia data like videos, photographs, or audio
sequences, but covers a significantly vast area that can include biological
images as well, such as western-blot and microscopic images. In this paper, we
focus on the detection of synthetically generated western-blot images. These
images are largely explored in the biomedical literature and it has been
already shown how they can be easily counterfeited with few hopes to spot
manipulations by visual inspection or by standard forensics detectors. To
overcome the absence of a publicly available dataset in this area, we create a
new dataset comprising more than 14K original western-blot images and 18K
synthetic western-blot images, generated using four different state-of-the-art
generation methods. We investigate different strategies to detect synthetic
western blots, exploring binary classification methods as well as one-class
detectors. In both scenarios, we never exploit synthetic western-blot images at
training stage. The achieved results show that synthetically generated
western-blot images can be spot with good accuracy, even though the exploited
detectors are not optimized over synthetic versions of these scientific images.
We also test the robustness of the developed detectors against common
post-processing operations performed on scientific images, showing that we can
be robust to JPEG compression and that some generative models can be easily
detected, independently of the post-processing applied.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Semantic Transfer for Multi-Label Recognition with Partial Labels. (arXiv:2112.10941v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10941">
<div class="article-summary-box-inner">
<span><p>Multi-label image recognition is a fundamental yet practical task because
real-world images inherently possess multiple semantic labels. However, it is
difficult to collect large-scale multi-label annotations due to the complexity
of both the input images and output label spaces. To reduce the annotation
cost, we propose a structured semantic transfer (SST) framework that enables
training multi-label recognition models with partial labels, i.e., merely some
labels are known while other labels are missing (also called unknown labels)
per image. The framework consists of two complementary transfer modules that
explore within-image and cross-image semantic correlations to transfer
knowledge of known labels to generate pseudo labels for unknown labels.
Specifically, an intra-image semantic transfer module learns image-specific
label co-occurrence matrix and maps the known labels to complement unknown
labels based on this matrix. Meanwhile, a cross-image transfer module learns
category-specific feature similarities and helps complement unknown labels with
high similarities. Finally, both known and generated labels are used to train
the multi-label recognition models. Extensive experiments on the Microsoft
COCO, Visual Genome and Pascal VOC datasets show that the proposed SST
framework obtains superior performance over current state-of-the-art
algorithms. Codes are available at https://github.com/HCPLab-SYSU/HCP-MLR-PL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition. (arXiv:2112.12496v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12496">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art deep learning based face recognition (FR) models
require a large number of face identities for central training. However, due to
the growing privacy awareness, it is prohibited to access the face images on
user devices to continually improve face recognition models. Federated Learning
(FL) is a technique to address the privacy issue, which can collaboratively
optimize the model without sharing the data between clients. In this work, we
propose a FL based framework called FedFR to improve the generic face
representation in a privacy-aware manner. Besides, the framework jointly
optimizes personalized models for the corresponding clients via the proposed
Decoupled Feature Customization module. The client-specific personalized model
can serve the need of optimized face recognition experience for registered
identities at the local device. To the best of our knowledge, we are the first
to explore the personalized face recognition in FL setup. The proposed
framework is validated to be superior to previous approaches on several generic
and personalized face recognition benchmarks with diverse FL scenarios. The
source codes and our proposed personalized FR benchmark under FL setup are
available at https://github.com/jackie840129/FedFR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Curse of Zero Task Diversity: On the Failure of Transfer Learning to Outperform MAML and their Empirical Equivalence. (arXiv:2112.13121v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13121">
<div class="article-summary-box-inner">
<span><p>Recently, it has been observed that a transfer learning solution might be all
we need to solve many few-shot learning benchmarks -- thus raising important
questions about when and how meta-learning algorithms should be deployed. In
this paper, we seek to clarify these questions by proposing a novel metric --
the diversity coefficient -- to measure the diversity of tasks in a few-shot
learning benchmark. We hypothesize that the diversity coefficient of the
few-shot learning benchmark is predictive of whether meta-learning solutions
will succeed or not. Using the diversity coefficient, we show that the
MiniImagenet benchmark has zero diversity. This novel insight contextualizes
claims that transfer learning solutions are better than meta-learned solutions.
Specifically, we empirically find that a diversity coefficient of zero
correlates with a high similarity between transfer learning and Model-Agnostic
Meta-Learning (MAML) learned solutions in terms of meta-accuracy (at meta-test
time). Therefore, we conjecture meta-learned solutions have the same meta-test
performance as transfer learning when the diversity coefficient is zero. Our
work provides the first test of whether diversity correlates with meta-learning
success.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlertTrap: A study on object detection in remote insects trap monitoring system using on-the-edge deep learning platform. (arXiv:2112.13341v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13341">
<div class="article-summary-box-inner">
<span><p>Fruit flies are one of the most harmful insect species to fruit yields. In
AlertTrap, implementation of SSD architecture with different state-of-the-art
backbone feature extractors such as MobileNetV1 and MobileNetV2 appear to be
potential solutions for the real-time detection problem. SSD-MobileNetV1 and
SSD-MobileNetV2 perform well and result in AP@0.5 of 0.957 and 1.0
respectively. YOLOv4-tiny outperforms the SSD family with 1.0 in AP@0.5;
however, its throughput velocity is slightly slower.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ITSA: An Information-Theoretic Approach to Automatic Shortcut Avoidance and Domain Generalization in Stereo Matching Networks. (arXiv:2201.02263v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02263">
<div class="article-summary-box-inner">
<span><p>State-of-the-art stereo matching networks trained only on synthetic data
often fail to generalize to more challenging real data domains. In this paper,
we attempt to unfold an important factor that hinders the networks from
generalizing across domains: through the lens of shortcut learning. We
demonstrate that the learning of feature representations in stereo matching
networks is heavily influenced by synthetic data artefacts (shortcut
attributes). To mitigate this issue, we propose an Information-Theoretic
Shortcut Avoidance~(ITSA) approach to automatically restrict shortcut-related
information from being encoded into the feature representations. As a result,
our proposed method learns robust and shortcut-invariant features by minimizing
the sensitivity of latent features to input variations. To avoid the
prohibitive computational cost of direct input sensitivity optimization, we
propose an effective yet feasible algorithm to achieve robustness. We show that
using this method, state-of-the-art stereo matching networks that are trained
purely on synthetic data can effectively generalize to challenging and
previously unseen real data scenarios. Importantly, the proposed method
enhances the robustness of the synthetic trained networks to the point that
they outperform their fine-tuned counterparts (on real data) for challenging
out-of-domain stereo datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invariance encoding in sliced-Wasserstein space for image classification with limited training data. (arXiv:2201.02980v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02980">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks (CNNs) are broadly considered to be
state-of-the-art generic end-to-end image classification systems. However, they
are known to underperform when training data are limited and thus require data
augmentation strategies that render the method computationally expensive and
not always effective. Rather than using a data augmentation strategy to encode
invariances as typically done in machine learning, here we propose to
mathematically augment a nearest subspace classification model in
sliced-Wasserstein space by exploiting certain mathematical properties of the
Radon Cumulative Distribution Transform (R-CDT), a recently introduced image
transform. We demonstrate that for a particular type of learning problem, our
mathematical solution has advantages over data augmentation with deep CNNs in
terms of classification accuracy and computational complexity, and is
particularly effective under a limited training data setting. The method is
simple, effective, computationally efficient, non-iterative, and requires no
parameters to be tuned. Python code implementing our method is available at
https://github.com/rohdelab/mathematical_augmentation. Our method is integrated
as a part of the software package PyTransKit, which is available at
https://github.com/rohdelab/PyTransKit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical image segmentation. (arXiv:2201.08582v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08582">
<div class="article-summary-box-inner">
<span><p>Current research on deep learning for medical image segmentation exposes
their limitations in learning either global semantic information or local
contextual information. To tackle these issues, a novel network named
SegTransVAE is proposed in this paper. SegTransVAE is built upon
encoder-decoder architecture, exploiting transformer with the variational
autoencoder (VAE) branch to the network to reconstruct the input images jointly
with segmentation. To the best of our knowledge, this is the first method
combining the success of CNN, transformer, and VAE. Evaluation on various
recently introduced datasets shows that SegTransVAE outperforms previous
methods in Dice Score and $95\%$-Haudorff Distance while having comparable
inference time to a simple CNN-based architecture network. The source code is
available at: https://github.com/itruonghai/SegTransVAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-based Video Reconstruction via Potential-assisted Spiking Neural Network. (arXiv:2201.10943v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10943">
<div class="article-summary-box-inner">
<span><p>Neuromorphic vision sensor is a new bio-inspired imaging paradigm that
reports asynchronous, continuously per-pixel brightness changes called `events'
with high temporal resolution and high dynamic range. So far, the event-based
image reconstruction methods are based on artificial neural networks (ANN) or
hand-crafted spatiotemporal smoothing techniques. In this paper, we first
implement the image reconstruction work via fully spiking neural network (SNN)
architecture. As the bio-inspired neural networks, SNNs operating with
asynchronous binary spikes distributed over time, can potentially lead to
greater computational efficiency on event-driven hardware. We propose a novel
Event-based Video reconstruction framework based on a fully Spiking Neural
Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and
Membrane Potential (MP) neuron. We find that the spiking neurons have the
potential to store useful temporal information (memory) to complete such
time-dependent tasks. Furthermore, to better utilize the temporal information,
we propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane
potential of spiking neuron. The proposed neuron is referred as Adaptive
Membrane Potential (AMP) neuron, which adaptively updates the membrane
potential according to the input spikes. The experimental results demonstrate
that our models achieve comparable performance to ANN-based models on IJRR,
MVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are
19.36$\times$ and 7.75$\times$ more computationally efficient than their ANN
architectures, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLA-NeRF: Category-Level Articulated Neural Radiance Field. (arXiv:2202.00181v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00181">
<div class="article-summary-box-inner">
<span><p>We propose CLA-NeRF -- a Category-Level Articulated Neural Radiance Field
that can perform view synthesis, part segmentation, and articulated pose
estimation. CLA-NeRF is trained at the object category level using no CAD
models and no depth, but a set of RGB images with ground truth camera poses and
part segments. During inference, it only takes a few RGB views (i.e., few-shot)
of an unseen 3D object instance within the known category to infer the object
part segmentation and the neural radiance field. Given an articulated pose as
input, CLA-NeRF can perform articulation-aware volume rendering to generate the
corresponding RGB image at any camera pose. Moreover, the articulated pose of
an object can be estimated via inverse rendering. In our experiments, we
evaluate the framework across five categories on both synthetic and real-world
data. In all cases, our method shows realistic deformation results and accurate
articulated pose estimation. We believe that both few-shot articulated object
rendering and articulated pose estimation open doors for robots to perceive and
interact with unseen articulated objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality Multi-Atlas Segmentation via Deep Registration and Label Fusion. (arXiv:2202.02000v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02000">
<div class="article-summary-box-inner">
<span><p>Multi-atlas segmentation (MAS) is a promising framework for medical image
segmentation. Generally, MAS methods register multiple atlases, i.e., medical
images with corresponding labels, to a target image; and the transformed atlas
labels can be combined to generate target segmentation via label fusion
schemes. Many conventional MAS methods employed the atlases from the same
modality as the target image. However, the number of atlases with the same
modality may be limited or even missing in many clinical applications. Besides,
conventional MAS methods suffer from the computational burden of registration
or label fusion procedures. In this work, we design a novel cross-modality MAS
framework, which uses available atlases from a certain modality to segment a
target image from another modality. To boost the computational efficiency of
the framework, both the image registration and label fusion are achieved by
well-designed deep neural networks. For the atlas-to-target image registration,
we propose a bi-directional registration network (BiRegNet), which can
efficiently align images from different modalities. For the label fusion, we
design a similarity estimation network (SimNet), which estimates the fusion
weight of each atlas by measuring its similarity to the target image. SimNet
can learn multi-scale information for similarity estimation to improve the
performance of label fusion. The proposed framework was evaluated by the left
ventricle and liver segmentation tasks on the MM-WHS and CHAOS datasets,
respectively. Results have shown that the framework is effective for
cross-modality MAS in both registration and label fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical and Topological Summaries Aid Disease Detection for Segmented Retinal Vascular Images. (arXiv:2202.09708v2 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09708">
<div class="article-summary-box-inner">
<span><p>Disease complications can alter vascular network morphology and disrupt
tissue functioning. Diabetic retinopathy, for example, is a complication of
type 1 and 2 diabetus mellitus that can cause blindness. Microvascular diseases
are assessed by visual inspection of retinal images, but this can be
challenging when diseases exhibit silent symptoms or patients cannot attend
in-person meetings. We examine the performance of machine learning algorithms
in detecting microvascular disease when trained on either statistical or
topological summaries of segmented retinal vascular images. We apply our
methods to four publicly-available datasets and find that the fractal dimension
performs best for high resolution images. By contrast, we find that topological
descriptor vectors quantifying the number of loops in the data achieve the
highest accuracy for low resolution images. Further analysis, using the
topological approach, reveals that microvascular disease may alter morphology
by reducing the number of loops in the retinal vasculature. Our work provides
preliminary guidelines on which methods are most appropriate for assessing
disease in high and low resolution images. In the longer term, these methods
could be incorporated into automated disease assessment tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Multi-Object Dynamics with Compositional Neural Radiance Fields. (arXiv:2202.11855v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11855">
<div class="article-summary-box-inner">
<span><p>We present a method to learn compositional predictive models from image
observations based on implicit object encoders, Neural Radiance Fields (NeRFs),
and graph neural networks. A central question in learning dynamic models from
sensor observations is on which representations predictions should be
performed. NeRFs have become a popular choice for representing scenes due to
their strong 3D prior. However, most NeRF approaches are trained on a single
scene, representing the whole scene with a global model, making generalization
to novel scenes, containing different numbers of objects, challenging. Instead,
we present a compositional, object-centric auto-encoder framework that maps
multiple views of the scene to a \emph{set} of latent vectors representing each
object separately. The latent vectors parameterize individual NeRF models from
which the scene can be reconstructed and rendered from novel viewpoints. We
train a graph neural network dynamics model in the latent space to achieve
compositionality for dynamics prediction. A key feature of our approach is that
the learned 3D information of the scene through the NeRF model enables us to
incorporate structural priors in learning the dynamics models, making long-term
predictions more stable. The model can further be used to synthesize new scenes
from individual object observations. For planning, we utilize RRTs in the
learned latent space, where we can exploit our model and the implicit object
encoder to make sampling the latent space informative and more efficient. In
the experiments, we show that the model outperforms several baselines on a
pushing task containing many objects. Video:
https://dannydriess.github.io/compnerfdyn/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Human Observer Ability in Morphing Attack Detection -- Where Do We Stand?. (arXiv:2202.12426v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12426">
<div class="article-summary-box-inner">
<span><p>While several works have studied the vulnerability of automated FRS and have
proposed morphing attack detection (MAD) methods, very few have focused on
studying the human ability to detect morphing attacks. The examiner/observer's
face morph detection ability is based on their observation, domain knowledge,
experience, and familiarity with the problem, and no works report the detailed
findings from observers who check identity documents as a part of their
everyday professional life. This work creates a new benchmark database of
realistic morphing attacks from 48 unique subjects leading to 400 morphed
images presented to the observers in a Differential-MAD (D-MAD) setting. Unlike
the existing databases, the newly created morphed image database has been
created with careful considerations to age, gender and ethnicity to create
realistic morph attacks. Further, unlike the previous works, we also capture
ten images from Automated Border Control (ABC) gates to mimic the realistic
D-MAD setting leading to 400 probe images in border crossing scenarios. The
newly created dataset is further used to study the ability of human observers'
ability to detect morphed images. In addition, a new dataset of 180 morphed
images is also created using the FRGCv2 dataset under the Single Image-MAD
(S-MAD) setting. Further, to benchmark the human ability in detecting morphs, a
new evaluation platform is created to conduct S-MAD and D-MAD analysis. The
benchmark study employs 469 observers for D-MAD and 410 observers for S-MAD who
are primarily governmental employees from more than 40 countries. The analysis
provides interesting insights and points to expert observers' missing
competence and failure to detect a considerable amount of morphing attacks.
Human observers tend to detect morphed images to a lower accuracy as compared
to the automated MAD algorithms evaluated in this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pattern Based Multivariable Regression using Deep Learning (PBMR-DP). (arXiv:2202.13541v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13541">
<div class="article-summary-box-inner">
<span><p>We propose a deep learning methodology for multivariate regression that is
based on pattern recognition that triggers fast learning over sensor data. We
used a conversion of sensors-to-image which enables us to take advantage of
Computer Vision architectures and training processes. In addition to this data
preparation methodology, we explore the use of state-of-the-art architectures
to generate regression outputs to predict agricultural crop continuous yield
information. Finally, we compare with some of the top models reported in
MLCAS2021. We found that using a straightforward training process, we were able
to accomplish an MAE of 4.394, RMSE of 5.945, and R^2 of 0.861.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. (arXiv:2203.01311v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01311">
<div class="article-summary-box-inner">
<span><p>Learning multimodal representations involves discovering correspondences and
integrating information from multiple heterogeneous sources of data. While
recent research has begun to explore the design of more general-purpose
multimodal models (contrary to prior focus on domain and modality-specific
architectures), these methods are still largely focused on a small set of
modalities in the language, vision, and audio space. In order to accelerate
generalization towards diverse and understudied modalities, we investigate
methods for high-modality (a large set of diverse modalities) and
partially-observable (each task only defined on a small subset of modalities)
scenarios. To tackle these challenges, we design a general multimodal model
that enables multitask and transfer learning: multitask learning with shared
parameters enables stable parameter counts (addressing scalability), and
cross-modal transfer learning enables information sharing across modalities and
tasks (addressing partial observability). Our resulting model generalizes
across text, image, video, audio, time-series, sensors, tables, and set
modalities from different research areas, improves the tradeoff between
performance and efficiency, transfers to new modalities and tasks, and reveals
surprising insights on the nature of information sharing in multitask models.
We release our code and benchmarks which we hope will present a unified
platform for subsequent theoretical and empirical analysis:
https://github.com/pliang279/HighMMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives. (arXiv:2203.01445v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01445">
<div class="article-summary-box-inner">
<span><p>The volume of available data has grown dramatically in recent years in many
applications. Furthermore, the age of networks that used multiple modalities
separately has practically ended. Therefore, enabling bidirectional
cross-modality data retrieval capable of processing has become a requirement
for many domains and disciplines of research. This is especially true in the
medical field, as data comes in a multitude of types, including various types
of images and reports as well as molecular data. Most contemporary works apply
cross attention to highlight the essential elements of an image or text in
relation to the other modalities and try to match them together. However,
regardless of their importance in their own modality, these approaches usually
consider features of each modality equally. In this study, self-attention as an
additional loss term will be proposed to enrich the internal representation
provided into the cross attention module. This work suggests a novel
architecture with a new loss term to help represent images and texts in the
joint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO
and ARCH, show the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relative distance matters for one-shot landmark detection. (arXiv:2203.01687v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01687">
<div class="article-summary-box-inner">
<span><p>Contrastive learning based methods such as cascade comparing to detect (CC2D)
have shown great potential for one-shot medical landmark detection. However,
the important cue of relative distance between landmarks is ignored in CC2D. In
this paper, we upgrade CC2D to version II by incorporating a
simple-yet-effective relative distance bias in the training stage, which is
theoretically proved to encourage the encoder to project the relatively distant
landmarks to the embeddings with low similarities. As consequence, CC2Dv2 is
less possible to detect a wrong point far from the correct landmark.
Furthermore, we present an open-source, landmark-labeled dataset for the
measurement of biomechanical parameters of the lower extremity to alleviate the
burden of orthopedic surgeons. The effectiveness of CC2Dv2 is evaluated on the
public dataset from the ISBI 2015 Grand-Challenge of cephalometric radiographs
and our new dataset, which greatly outperforms the state-of-the-art one-shot
landmark detection approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Attended Meta-Learning for Few-Shot Learning. (arXiv:2106.10642v1 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10642">
<div class="article-summary-box-inner">
<span><p>Meta-learning (ML) has emerged as a promising direction in learning models
under constrained resource settings like few-shot learning. The popular
approaches for ML either learn a generalizable initial model or a generic
parametric optimizer through episodic training. The former approaches leverage
the knowledge from a batch of tasks to learn an optimal prior. In this work, we
study the importance of a batch for ML. Specifically, we first incorporate a
batch episodic training regimen to improve the learning of the generic
parametric optimizer. We also hypothesize that the common assumption in batch
episodic training that each task in a batch has an equal contribution to
learning an optimal meta-model need not be true. We propose to weight the tasks
in a batch according to their "importance" in improving the meta-model's
learning. To this end, we introduce a training curriculum motivated by
selective focus in humans, called task attended meta-training, to weight the
tasks in a batch. Task attention is a standalone module that can be integrated
with any batch episodic training regimen. The comparisons of the models with
their non-task-attended counterparts on complex datasets like miniImageNet and
tieredImageNet validate its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-07 23:07:35.911949926 UTC">2022-03-07 23:07:35 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>