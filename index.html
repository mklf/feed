<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-10T01:30:00Z">02-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Explanations and Human Understanding. (arXiv:2202.04092v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04092">
<div class="article-summary-box-inner">
<span><p>Explanations are hypothesized to improve human understanding of machine
learning models and achieve a variety of desirable outcomes, ranging from model
debugging to enhancing human decision making. However, empirical studies have
found mixed and even negative results. An open question, therefore, is under
what conditions explanations can improve human understanding and in what way.
Using adapted causal diagrams, we provide a formal characterization of the
interplay between machine explanations and human understanding, and show how
human intuitions play a central role in enabling human understanding.
Specifically, we identify three core concepts of interest that cover all
existing quantitative measures of understanding in the context of human-AI
decision making: task decision boundary, model decision boundary, and model
error. Our key result is that without assumptions about task-specific
intuitions, explanations may potentially improve human understanding of model
decision boundary, but they cannot improve human understanding of task decision
boundary or model error. To achieve complementary human-AI performance, we
articulate possible ways on how explanations need to work with human
intuitions. For instance, human intuitions about the relevance of features
(e.g., education is more important than age in predicting a person's income)
can be critical in detecting model error. We validate the importance of human
intuitions in shaping the outcome of machine explanations with empirical
human-subject studies. Overall, our work provides a general framework along
with actionable implications for future algorithmic development and empirical
experiments of machine explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logical Reasoning for Task Oriented Dialogue Systems. (arXiv:2202.04161v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04161">
<div class="article-summary-box-inner">
<span><p>In recent years, large pretrained models have been used in dialogue systems
to improve successful task completion rates. However, lack of reasoning
capabilities of dialogue platforms make it difficult to provide relevant and
fluent responses, unless the designers of a conversational experience spend a
considerable amount of time implementing these capabilities in external rule
based modules. In this work, we propose a novel method to fine-tune pretrained
transformer models such as Roberta and T5. to reason over a set of facts in a
given dialogue context. Our method includes a synthetic data generation
mechanism which helps the model learn logical relations, such as comparison
between list of numerical values, inverse relations (and negation), inclusion
and exclusion for categorical attributes, and application of a combination of
attributes over both numerical and categorical values, and spoken form for
numerical values, without need for additional training dataset. We show that
the transformer based model can perform logical reasoning to answer questions
when the dialogue context contains all the required information, otherwise it
is able to extract appropriate constraints to pass to downstream components
(e.g. a knowledge base) when partial information is available. We observe that
transformer based models such as UnifiedQA-T5 can be fine-tuned to perform
logical reasoning (such as numerical and categorical attributes' comparison)
over attributes that been seen in training time (e.g., accuracy of 90\%+ for
comparison of smaller than $k_{\max}$=5 values over heldout test dataset).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models. (arXiv:2202.04173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04173">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (LMs) are shown to easily generate toxic
language. In this work, we systematically explore domain-adaptive training to
reduce the toxicity of language models. We conduct this study on three
dimensions: training corpus, model size, and parameter efficiency. For the
training corpus, we propose to leverage the generative power of LMs and
generate nontoxic datasets for domain-adaptive training, which mitigates the
exposure bias and is shown to be more data-efficient than using a curated
pre-training corpus. We demonstrate that the self-generation method
consistently outperforms the existing baselines across various model sizes on
both automatic and human evaluations, even when it uses a 1/3 smaller training
corpus. We then comprehensively study detoxifying LMs with parameter sizes
ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never
been studied before. We find that i) large LMs have similar toxicity levels as
smaller ones given the same pre-training corpus, and ii) large LMs require more
endeavor to detoxify. We also explore parameter-efficient training methods for
detoxification. We demonstrate that adding and training adapter-only layers in
LMs not only saves a lot of parameters but also achieves a better trade-off
between toxicity and perplexity than whole model adaptation for the large-scale
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Police Text Analysis: Topic Modeling and Spatial Relative Density Estimation. (arXiv:2202.04176v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04176">
<div class="article-summary-box-inner">
<span><p>We analyze a large corpus of police incident narrative documents in
understanding the spatial distribution of the topics. The motivation for doing
this is that police narratives in each incident report contains very
fine-grained information that is richer than the category that is manually
assigned by the police. Our approach is to split the corpus into topics using
two different unsupervised machine learning algorithms - Latent Dirichlet
Allocation and Non-negative Matrix Factorization. We validate the performance
of each learned topic model using model coherence. Then, using a k-nearest
neighbors density ratio estimation (kNN-DRE) approach that we propose, we
estimate the spatial density ratio per topic and use this for data discovery
and analysis of each topic, allowing for insights into the described incidents
at scale. We provide a qualitative assessment of each topic and highlight some
key benefits for using our kNN-DRE model for estimating spatial trends.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Executable Formal Model of the VHDL in Isabelle/HOL. (arXiv:2202.04192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04192">
<div class="article-summary-box-inner">
<span><p>In the hardware design process, hardware components are usually described in
a hardware description language. Most of the hardware description languages,
such as Verilog and VHDL, do not have mathematical foundation and hence are not
fit for formal reasoning about the design. To enable formal reasoning in one of
the most commonly used description language VHDL, we define a formal model of
the VHDL language in Isabelle/HOL. Our model targets the functional part of
VHDL designs used in industry, specifically the design of the LEON3 processor's
integer unit. We cover a wide range of features in the VHDL language that are
usually not modelled in the literature and define a novel operational semantics
for it. Furthermore, our model can be exported to OCaml code for execution,
turning the formal model into a VHDL simulator. We have tested our simulator
against simple designs used in the literature, as well as the div32 module in
the LEON3 design. The Isabelle/HOL code is publicly available:
https://zhehou.github.io/apps/VHDLModel.zip
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Difference Captioning with Pre-training and Contrastive Learning. (arXiv:2202.04298v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04298">
<div class="article-summary-box-inner">
<span><p>The Image Difference Captioning (IDC) task aims to describe the visual
differences between two similar images with natural language. The major
challenges of this task lie in two aspects: 1) fine-grained visual differences
that require learning stronger vision and language association and 2) high-cost
of manual annotations that leads to limited supervised data. To address these
challenges, we propose a new modeling framework following the
pre-training-finetuning paradigm. Specifically, we design three self-supervised
tasks and contrastive learning strategies to align visual differences and text
descriptions at a fine-grained level. Moreover, we propose a data expansion
strategy to utilize extra cross-task supervision information, such as data for
fine-grained image classification, to alleviate the limitation of available
supervised IDC data. Extensive experiments on two IDC benchmark datasets,
CLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed
modeling framework. The codes and models will be released at
https://github.com/yaolinli/IDC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?. (arXiv:2202.04306v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04306">
<div class="article-summary-box-inner">
<span><p>The task of Outside Knowledge Visual Question Answering (OKVQA) requires an
automatic system to answer natural language questions about pictures and images
using external knowledge. We observe that many visual questions, which contain
deictic referential phrases referring to entities in the image, can be
rewritten as "non-grounded" questions and can be answered by existing
text-based question answering systems. This allows for the reuse of existing
text-based Open Domain Question Answering (QA) Systems for visual question
answering. In this work, we propose a potentially data-efficient approach that
reuses existing systems for (a) image analysis, (b) question rewriting, and (c)
text-based question answering to answer such visual questions. Given an image
and a question pertaining to that image (a visual question), we first extract
the entities present in the image using pre-trained object and scene
classifiers. Using these detected entities, the visual questions can be
rewritten so as to be answerable by open domain QA systems. We explore two
rewriting strategies: (1) an unsupervised method using BERT for masking and
rewriting, and (2) a weakly supervised approach that combines adaptive
rewriting and reinforcement learning techniques to use the implicit feedback
from the QA system. We test our strategies on the publicly available OKVQA
dataset and obtain a competitive performance with state-of-the-art models while
using only 10% of the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">pNLP-Mixer: an Efficient all-MLP Architecture for Language. (arXiv:2202.04350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04350">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models drastically changed the natural language
processing(NLP) landscape. Nowadays, they represent the go-to framework to
tackle diverse NLP tasks, even with a limited number of annotations. However,
using those models in production, either in the cloud or at the edge, remains a
challenge due to the memory footprint and/or inference costs. As an
alternative, recent work on efficient NLP has shown that small weight-efficient
models can reach competitive performance at a fraction of the costs. Here, we
introduce pNLP-Mixer, an embbedding-free model based on the MLP-Mixer
architecture that achieves high weight-efficiency thanks to a novel
linguistically informed projection layer. We evaluate our model on two
multi-lingual semantic parsing datasets, MTOP and multiATIS. On MTOP our
pNLP-Mixer almost matches the performance of mBERT, which has 38 times more
parameters, and outperforms the state-of-the-art of tiny models (pQRNN) with 3
times fewer parameters. On a long-sequence classification task (Hyperpartisan)
our pNLP-Mixer without pretraining outperforms RoBERTa, which has 100 times
more parameters, demonstrating the potential of this architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Merit-based Fusion of NLP Techniques for Instant Feedback on Water Quality from Twitter Text. (arXiv:2202.04462v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04462">
<div class="article-summary-box-inner">
<span><p>This paper focuses on an important environmental challenge; namely, water
quality by analyzing the potential of social media as an immediate source of
feedback. The main goal of the work is to automatically analyze and retrieve
social media posts relevant to water quality with particular attention to posts
describing different aspects of water quality, such as watercolor, smell,
taste, and related illnesses. To this aim, we propose a novel framework
incorporating different preprocessing, data augmentation, and classification
techniques. In total, three different Neural Networks (NNs) architectures,
namely (i) Bidirectional Encoder Representations from Transformers (BERT), (ii)
Robustly Optimized BERT Pre-training Approach (XLM-RoBERTa), and (iii) custom
Long short-term memory (LSTM) model, are employed in a merit-based fusion
scheme. For merit-based weight assignment to the models, several optimization
and search techniques are compared including a Particle Swarm Optimization
(PSO), a Genetic Algorithm (GA), Brute Force (BF), Nelder-Mead, and Powell's
optimization methods. We also provide an evaluation of the individual models
where the highest F1-score of 0.81 is obtained with the BERT model. In
merit-based fusion, overall better results are obtained with BF achieving an
F1-score score of 0.852.
</p>
<p>We also provide comparison against existing methods, where a significant
improvement for our proposed solutions is obtained. We believe such rigorous
analysis of this relatively new topic will provide a baseline for future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Training Data with Language Models: Towards Zero-Shot Language Understanding. (arXiv:2202.04538v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04538">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) have demonstrated remarkable performance in
various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are
well known for their superior text generation capabilities; bidirectional PLMs
(e.g., BERT) have been the prominent choice for natural language understanding
(NLU) tasks. While both types of models have achieved promising few-shot
learning performance, their potential for zero-shot learning has been
underexplored. In this paper, we present a simple approach that uses both types
of PLMs for fully zero-shot learning of NLU tasks without requiring any
task-specific data: A unidirectional PLM generates class-conditioned texts
guided by prompts, which are used as the training data for fine-tuning a
bidirectional PLM. With quality training data selected based on the generation
probability and regularization techniques (label smoothing and temporal
ensembling) applied to the fine-tuning stage for better generalization and
stability, our approach demonstrates strong performance across seven
classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and
92.8 on SST-2), significantly outperforming zero-shot prompting methods and
achieving even comparable results to strong few-shot approaches using 32
training samples per class.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations. (arXiv:2202.04582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04582">
<div class="article-summary-box-inner">
<span><p>Topic models have been the prominent tools for automatic topic discovery from
text corpora. Despite their effectiveness, topic models suffer from several
limitations including the inability of modeling word ordering information in
documents, the difficulty of incorporating external linguistic knowledge, and
the lack of both accurate and efficient inference methods for approximating the
intractable posterior. Recently, pretrained language models (PLMs) have brought
astonishing performance improvements to a wide variety of tasks due to their
superior representations of text. Interestingly, there have not been standard
approaches to deploy PLMs for topic discovery as better alternatives to topic
models. In this paper, we begin by analyzing the challenges of using PLM
representations for topic discovery, and then propose a joint latent space
learning and clustering framework built upon PLM embeddings. In the latent
space, topic-word and document-topic distributions are jointly modeled so that
the discovered topics can be interpreted by coherent and distinctive terms and
meanwhile serve as meaningful summaries of the documents. Our model effectively
leverages the strong representation power and superb linguistic features
brought by PLMs for topic discovery, and is conceptually simpler than topic
models. On two benchmark datasets in different domains, our model generates
significantly more coherent and diverse topics than strong topic models, and
offers better topic-wise document representations, based on both automatic and
human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Probabilistic Generative Grammar for Semantic Parsing. (arXiv:1606.06361v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1606.06361">
<div class="article-summary-box-inner">
<span><p>Domain-general semantic parsing is a long-standing goal in natural language
processing, where the semantic parser is capable of robustly parsing sentences
from domains outside of which it was trained. Current approaches largely rely
on additional supervision from new domains in order to generalize to those
domains. We present a generative model of natural language utterances and
logical forms and demonstrate its application to semantic parsing. Our approach
relies on domain-independent supervision to generalize to new domains. We
derive and implement efficient algorithms for training, parsing, and sentence
generation. The work relies on a novel application of hierarchical Dirichlet
processes (HDPs) for structured prediction, which we also present in this
manuscript.
</p>
<p>This manuscript is an excerpt of chapter 4 from the Ph.D. thesis of Saparov
(2022), where the model plays a central role in a larger natural language
understanding system.
</p>
<p>This manuscript provides a new simplified and more complete presentation of
the work first introduced in Saparov, Saraswat, and Mitchell (2017). The
description and proofs of correctness of the training algorithm, parsing
algorithm, and sentence generation algorithm are much simplified in this new
presentation. We also describe the novel application of hierarchical Dirichlet
processes for structured prediction. In addition, we extend the earlier work
with a new model of word morphology, which utilizes the comprehensive
morphological data from Wiktionary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled Analyses of Social Biases in Wikipedia Bios. (arXiv:2101.00078v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00078">
<div class="article-summary-box-inner">
<span><p>Social biases on Wikipedia, a widely-read global platform, could greatly
influence public opinion. While prior research has examined man/woman gender
bias in biography articles, possible influences of other demographic attributes
limit conclusions. In this work, we present a methodology for analyzing
Wikipedia pages about people that isolates dimensions of interest (e.g.,
gender), from other attributes (e.g., occupation). Given a target corpus for
analysis (e.g.~biographies about women), we present a method for constructing a
comparison corpus that matches the target corpus in as many attributes as
possible, except the target one. We develop evaluation metrics to measure how
well the comparison corpus aligns with the target corpus and then examine how
articles about gender and racial minorities (cis. women, non-binary people,
transgender women, and transgender men; African American, Asian American, and
Hispanic/Latinx American people) differ from other articles. In addition to
identifying suspect social biases, our results show that failing to control for
covariates can result in different conclusions and veil biases. Our
contributions include methodology that facilitates further analyses of bias in
Wikipedia articles, findings that can aid Wikipedia editors in reducing biases,
and a framework and evaluation metrics to guide future work in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Neural Coreference Resolution Revisited: A Simple yet Effective Baseline. (arXiv:2107.01700v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01700">
<div class="article-summary-box-inner">
<span><p>Since the first end-to-end neural coreference resolution model was
introduced, many extensions to the model have been proposed, ranging from using
higher-order inference to directly optimizing evaluation metrics using
reinforcement learning. Despite improving the coreference resolution
performance by a large margin, these extensions add substantial extra
complexity to the original model. Motivated by this observation and the recent
advances in pre-trained Transformer language models, we propose a simple yet
effective baseline for coreference resolution. Even though our model is a
simplified version of the original neural coreference resolution model, it
achieves impressive performance, outperforming all recent extended works on the
public English OntoNotes benchmark. Our work provides evidence for the
necessity of carefully justifying the complexity of existing or newly proposed
models, as introducing a conceptual or practical simplification to an existing
model can still yield competitive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acyclic and Cyclic Reversing Computations in Petri Nets. (arXiv:2108.02167v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02167">
<div class="article-summary-box-inner">
<span><p>Reversible computations constitute an unconventional form of computing where
any sequence of performed operations can be undone by executing in reverse
order at any point during a computation. It has been attracting increasing
attention as it provides opportunities for low-power computation, being at the
same time essential or eligible in various applications. In recent work, we
have proposed a structural way of translating Reversing Petri Nets (RPNs) - a
type of Petri nets that embeds reversible computation, to bounded Coloured
Petri Nets (CPNs) - an extension of traditional Petri Nets, where tokens carry
data values. Three reversing semantics are possible in RPNs: backtracking
(reversing of the lately executed action), causal reversing (action can be
reversed only when all its effects have been undone) and out of causal
reversing (any previously performed action can be reversed). In this paper, we
extend the RPN to CPN translation with formal proofs of correctness. Moreover,
the possibility of introduction of cycles to RPNs is discussed. We analyze
which type of cycles could be allowed in RPNs to ensure consistency with the
current semantics. It emerged that the most interesting case related to cycles
in RPNs occurs in causal semantics, where various interpretations of dependency
result in different net's behaviour during reversing. Three definitions of
dependence are presented and discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuned Language Models Are Zero-Shot Learners. (arXiv:2109.01652v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01652">
<div class="article-summary-box-inner">
<span><p>This paper explores a simple method for improving the zero-shot learning
abilities of language models. We show that instruction tuning -- finetuning
language models on a collection of tasks described via instructions --
substantially improves zero-shot performance on unseen tasks.
</p>
<p>We take a 137B parameter pretrained language model and instruction-tune it on
over 60 NLP tasks verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task
types. FLAN substantially improves the performance of its unmodified
counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we
evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,
BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number
of finetuning datasets, model scale, and natural language instructions are key
to the success of instruction tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calculating Question Similarity is Enough: A New Method for KBQA Tasks. (arXiv:2111.07658v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07658">
<div class="article-summary-box-inner">
<span><p>Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with the help of an external knowledge base. The core idea is to find
the link between the internal knowledge behind questions and known triples of
the knowledge base. Traditional KBQA task pipelines contain several steps,
including entity recognition, entity linking, answering selection, etc. In this
kind of pipeline methods, errors in any procedure will inevitably propagate to
the final prediction. To address this challenge, this paper proposes a Corpus
Generation - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for
the KBQA task. The major novelty lies in the design of the new method, wherein
our approach, the knowledge enhanced T5 (kT5) model aims to generate natural
language QA pairs based on Knowledge Graph triples and directly solve the QA by
retrieving the synthetic dataset. The new method can extract more information
about the entities from PLM to improve accuracy and simplify the processes. We
test our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that
our method improves the performance of KBQA and the out straight-forward method
is competitive with the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Job Titles from Job Descriptions with Multi-label Text Classification. (arXiv:2112.11052v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11052">
<div class="article-summary-box-inner">
<span><p>Finding a suitable job and hunting for eligible candidates are important to
job seeking and human resource agencies. With the vast information about job
descriptions, employees and employers need assistance to automatically detect
job titles based on job description texts. In this paper, we propose the
multi-label classification approach for predicting relevant job titles from job
description texts, and implement the Bi-GRU-LSTM-CNN with different pre-trained
language models to apply for the job titles prediction problem. The BERT with
multilingual pre-trained model obtains the highest result by F1-scores on both
development and test sets, which are 62.20% on the development set, and 47.44%
on the test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Scaling Laws for Routed Language Models. (arXiv:2202.01169v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01169">
<div class="article-summary-box-inner">
<span><p>The performance of a language model has been shown to be effectively modeled
as a power-law in its parameter count. Here we study the scaling behaviors of
Routing Networks: architectures that conditionally use only a subset of their
parameters while processing an input. For these models, parameter count and
computational requirement form two independent axes along which an increase
leads to better performance. In this work we derive and justify scaling laws
defined on these two variables which generalize those known for standard
language models and describe the performance of a wide range of routing
architectures trained via three different techniques. Afterwards we provide two
applications of these laws: first deriving an Effective Parameter Count along
which all models scale at the same rate, and then using the scaling
coefficients to give a quantitative comparison of the three routing techniques
considered. Our analysis derives from an extensive evaluation of Routing
Networks across five orders of magnitude of size, including models with
hundreds of experts and hundreds of billions of parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selecting Seed Words for Wordle using Character Statistics. (arXiv:2202.03457v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03457">
<div class="article-summary-box-inner">
<span><p>Wordle, a word guessing game rose to global popularity in the January of
2022. The goal of the game is to guess a five-letter English word within six
tries. Each try provides the player with hints by means of colour changing
tiles which inform whether or not a given character is part of the solution as
well as, in cases where it is part of the solution, whether or not it is in the
correct placement. Numerous attempts have been made to find the best starting
word and best strategy to solve the daily wordle. This study uses character
statistics of five-letter words to determine the best three starting words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Multi-Token Fairness in Text Classification. (arXiv:2202.03792v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03792">
<div class="article-summary-box-inner">
<span><p>The counterfactual token generation has been limited to perturbing only a
single token in texts that are generally short and single sentences. These
tokens are often associated with one of many sensitive attributes. With limited
counterfactuals generated, the goal to achieve invariant nature for machine
learning classification models towards any sensitive attribute gets bounded,
and the formulation of Counterfactual Fairness gets narrowed. In this paper, we
overcome these limitations by solving root problems and opening bigger domains
for understanding. We have curated a resource of sensitive tokens and their
corresponding perturbation tokens, even extending the support beyond
traditionally used sensitive attributes like Age, Gender, Race to Nationality,
Disability, and Religion. The concept of Counterfactual Generation has been
extended to multi-token support valid over all forms of texts and documents. We
define the method of generating counterfactuals by perturbing multiple
sensitive tokens as Counterfactual Multi-token Generation. The method has been
conceptualized to showcase significant performance improvement over
single-token methods and validated over multiple benchmark datasets. The
emendation in counterfactual generation propagates in achieving improved
Counterfactual Multi-token Fairness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable N-gram Objective on Abstractive Summarization. (arXiv:2202.04003v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04003">
<div class="article-summary-box-inner">
<span><p>ROUGE is a standard automatic evaluation metric based on n-grams for
sequence-to-sequence tasks, while cross-entropy loss is an essential objective
of neural network language model that optimizes at a unigram level. We present
differentiable n-gram objectives, attempting to alleviate the discrepancy
between training criterion and evaluating criterion. The objective maximizes
the probabilistic weight of matched sub-sequences, and the novelty of our work
is the objective weights the matched sub-sequences equally and does not ceil
the number of matched sub-sequences by the ground truth count of n-grams in
reference sequence. We jointly optimize cross-entropy loss and the proposed
objective, providing decent ROUGE score enhancement over abstractive
summarization dataset CNN/DM and XSum, outperforming alternative n-gram
objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InferGrad: Improving Diffusion Models for Vocoder by Considering Inference in Training. (arXiv:2202.03751v1 [eess.AS] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03751">
<div class="article-summary-box-inner">
<span><p>Denoising diffusion probabilistic models (diffusion models for short) require
a large number of iterations in inference to achieve the generation quality
that matches or surpasses the state-of-the-art generative models, which
invariably results in slow inference speed. Previous approaches aim to optimize
the choice of inference schedule over a few iterations to speed up inference.
However, this results in reduced generation quality, mainly because the
inference process is optimized separately, without jointly optimizing with the
training process. In this paper, we propose InferGrad, a diffusion model for
vocoder that incorporates inference process into training, to reduce the
inference iterations while maintaining high generation quality. More
specifically, during training, we generate data from random noise through a
reverse process under inference schedules with a few iterations, and impose a
loss to minimize the gap between the generated and ground-truth data samples.
Then, unlike existing approaches, the training of InferGrad considers the
inference process. The advantages of InferGrad are demonstrated through
experiments on the LJSpeech dataset showing that InferGrad achieves better
voice quality than the baseline WaveGrad under same conditions while
maintaining the same voice quality as the baseline but with $3$x speedup ($2$
iterations for InferGrad vs $6$ iterations for WaveGrad).
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting and Localizing Copy-Move and Image-Splicing Forgery. (arXiv:2202.04069v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04069">
<div class="article-summary-box-inner">
<span><p>In the world of fake news and deepfakes, there have been an alarmingly large
number of cases of images being tampered with and published in newspapers, used
in court, and posted on social media for defamation purposes. Detecting these
tampered images is an important task and one we try to tackle. In this paper,
we focus on the methods to detect if an image has been tampered with using both
Deep Learning and Image transformation methods and comparing the performances
and robustness of each method. We then attempt to identify the tampered area of
the image and predict the corresponding mask. Based on the results, suggestions
and approaches are provided to achieve a more robust framework to detect and
identify the forgeries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent gaze information in highly dynamic decision-tasks. (arXiv:2202.04072v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04072">
<div class="article-summary-box-inner">
<span><p>Digitization is penetrating more and more areas of life. Tasks are
increasingly being completed digitally, and are therefore not only fulfilled
faster, more efficiently but also more purposefully and successfully. The rapid
developments in the field of artificial intelligence in recent years have
played a major role in this, as they brought up many helpful approaches to
build on. At the same time, the eyes, their movements, and the meaning of these
movements are being progressively researched. The combination of these
developments has led to exciting approaches. In this dissertation, I present
some of these approaches which I worked on during my Ph.D.
</p>
<p>First, I provide insight into the development of models that use artificial
intelligence to connect eye movements with visual expertise. This is
demonstrated for two domains or rather groups of people: athletes in
decision-making actions and surgeons in arthroscopic procedures. The resulting
models can be considered as digital diagnostic models for automatic expertise
recognition. Furthermore, I show approaches that investigate the
transferability of eye movement patterns to different expertise domains and
subsequently, important aspects of techniques for generalization. Finally, I
address the temporal detection of confusion based on eye movement data. The
results suggest the use of the resulting model as a clock signal for possible
digital assistance options in the training of young professionals. An
interesting aspect of my research is that I was able to draw on very valuable
data from DFB youth elite athletes as well as on long-standing experts in
arthroscopy. In particular, the work with the DFB data attracted the interest
of radio and print media, namely DeutschlandFunk Nova and SWR DasDing. All
resulting articles presented here have been published in internationally
renowned journals or at conferences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms. (arXiv:2202.04073v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04073">
<div class="article-summary-box-inner">
<span><p>Developing and validating artificial intelligence models in medical imaging
requires datasets that are large, granular, and diverse. To date, the majority
of publicly available breast imaging datasets lack in one or more of these
areas. Models trained on these data may therefore underperform on patient
populations or pathologies that have not previously been encountered. The EMory
BrEast imaging Dataset (EMBED) addresses these gaps by providing 3650,000 2D
and DBT screening and diagnostic mammograms for 116,000 women divided equally
between White and African American patients. The dataset also contains 40,000
annotated lesions linked to structured imaging descriptors and 61 ground truth
pathologic outcomes grouped into six severity classes. Our goal is to share
this dataset with research partners to aid in development and validation of
breast AI models that will serve all patients fairly and help decrease bias in
medical AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-level Contrastive Learning and Consistency Constraint for Semi-supervised Medical Image Segmentation. (arXiv:2202.04074v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04074">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL), which aims at leveraging a few labeled images
and a large number of unlabeled images for network training, is beneficial for
relieving the burden of data annotation in medical image segmentation.
According to the experience of medical imaging experts, local attributes such
as texture, luster and smoothness are very important factors for identifying
target objects like lesions and polyps in medical images. Motivated by this, we
propose a cross-level constrastive learning scheme to enhance representation
capacity for local features in semi-supervised medical image segmentation.
Compared to existing image-wise, patch-wise and point-wise constrastive
learning algorithms, our devised method is capable of exploring more complex
similarity cues, namely the relational characteristics between global
point-wise and local patch-wise representations. Additionally, for fully making
use of cross-level semantic relations, we devise a novel consistency constraint
that compares the predictions of patches against those of the full image. With
the help of the cross-level contrastive learning and consistency constraint,
the unlabelled data can be effectively explored to improve segmentation
performance on two medical image datasets for polyp and skin lesion
segmentation respectively. Code of our approach is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint-bone Fusion Graph Convolutional Network for Semi-supervised Skeleton Action Recognition. (arXiv:2202.04075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04075">
<div class="article-summary-box-inner">
<span><p>In recent years, graph convolutional networks (GCNs) play an increasingly
critical role in skeleton-based human action recognition. However, most
GCN-based methods still have two main limitations: 1) They only consider the
motion information of the joints or process the joints and bones separately,
which are unable to fully explore the latent functional correlation between
joints and bones for action recognition. 2) Most of these works are performed
in the supervised learning way, which heavily relies on massive labeled
training data. To address these issues, we propose a semi-supervised
skeleton-based action recognition method which has been rarely exploited
before. We design a novel correlation-driven joint-bone fusion graph
convolutional network (CD-JBF-GCN) as an encoder and use a pose prediction head
as a decoder to achieve semi-supervised learning. Specifically, the CD-JBF-GC
can explore the motion transmission between the joint stream and the bone
stream, so that promoting both streams to learn more discriminative feature
representations. The pose prediction based auto-encoder in the self-supervised
training stage allows the network to learn motion representation from unlabeled
data, which is essential for action recognition. Extensive experiments on two
popular datasets, i.e. NTU-RGB+D and Kinetics-Skeleton, demonstrate that our
model achieves the state-of-the-art performance for semi-supervised
skeleton-based action recognition and is also useful for fully-supervised
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face2PPG: An unsupervised pipeline for blood volume pulse extraction from faces. (arXiv:2202.04101v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04101">
<div class="article-summary-box-inner">
<span><p>Photoplethysmography (PPG) signals have become a key technology in many
fields such as medicine, well-being, or sports. Our work proposes a set of
pipelines to extract remote PPG signals (rPPG) from the face, robustly,
reliably, and in a configurable manner. We identify and evaluate the possible
choices in the critical steps of unsupervised rPPG methodologies. We evaluate a
state-of-the-art processing pipeline in six different datasets, incorporating
important corrections in the methodology that ensure reproducible and fair
comparisons. In addition, we extend the pipeline by proposing three novel
ideas; 1) a new method to stabilize the detected face based on a rigid mesh
normalization; 2) a new method to dynamically select the different regions in
the face that provide the best raw signals, and 3) a new RGB to rPPG
transformation method called Orthogonal Matrix Image Transformation (OMIT)
based on QR decomposition, that increases robustness against compression
artifacts. We show that all three changes introduce noticeable improvements in
retrieving rPPG signals from faces, obtaining state-of-the-art results compared
with unsupervised, non-learning-based methodologies, and in some databases,
very close to supervised, learning-based methods. We perform a comparative
study to quantify the contribution of each proposed idea. In addition, we
depict a series of observations that could help in future implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangle Saliency Detection into Cascaded Detail Modeling and Body Filling. (arXiv:2202.04112v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04112">
<div class="article-summary-box-inner">
<span><p>Salient object detection has been long studied to identify the most visually
attractive objects in images/videos. Recently, a growing amount of approaches
have been proposed all of which rely on the contour/edge information to improve
detection performance. The edge labels are either put into the loss directly or
used as extra supervision. The edge and body can also be learned separately and
then fused afterward. Both methods either lead to high prediction errors near
the edge or cannot be trained in an end-to-end manner. Another problem is that
existing methods may fail to detect objects of various sizes due to the lack of
efficient and effective feature fusion mechanisms. In this work, we propose to
decompose the saliency detection task into two cascaded sub-tasks, \emph{i.e.},
detail modeling and body filling. Specifically, the detail modeling focuses on
capturing the object edges by supervision of explicitly decomposed detail label
that consists of the pixels that are nested on the edge and near the edge. Then
the body filling learns the body part which will be filled into the detail map
to generate more accurate saliency map. To effectively fuse the features and
handle objects at different scales, we have also proposed two novel multi-scale
detail attention and body attention blocks for precise detail and body
modeling. Experimental results show that our method achieves state-of-the-art
performances on six public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Untrimmed Action Anticipation. (arXiv:2202.04132v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04132">
<div class="article-summary-box-inner">
<span><p>Egocentric action anticipation consists in predicting a future action the
camera wearer will perform from egocentric video. While the task has recently
attracted the attention of the research community, current approaches assume
that the input videos are "trimmed", meaning that a short video sequence is
sampled a fixed time before the beginning of the action. We argue that, despite
the recent advances in the field, trimmed action anticipation has a limited
applicability in real-world scenarios where it is important to deal with
"untrimmed" video inputs and it cannot be assumed that the exact moment in
which the action will begin is known at test time. To overcome such
limitations, we propose an untrimmed action anticipation task, which, similarly
to temporal action detection, assumes that the input video is untrimmed at test
time, while still requiring predictions to be made before the actions actually
take place. We design an evaluation procedure for methods designed to address
this novel task, and compare several baselines on the EPIC-KITCHENS-100
dataset. Experiments show that the performance of current models designed for
trimmed action anticipation is very limited and more research on this task is
required.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning of Generative Image Priors for MRI Reconstruction. (arXiv:2202.04175v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04175">
<div class="article-summary-box-inner">
<span><p>Multi-institutional efforts can facilitate training of deep MRI
reconstruction models, albeit privacy risks arise during cross-site sharing of
imaging data. Federated learning (FL) has recently been introduced to address
privacy concerns by enabling distributed training without transfer of imaging
data. Existing FL methods for MRI reconstruction employ conditional models to
map from undersampled to fully-sampled acquisitions via explicit knowledge of
the imaging operator. Since conditional models generalize poorly across
different acceleration rates or sampling densities, imaging operators must be
fixed between training and testing, and they are typically matched across
sites. To improve generalization and flexibility in multi-institutional
collaborations, here we introduce a novel method for MRI reconstruction based
on Federated learning of Generative IMage Priors (FedGIMP). FedGIMP leverages a
two-stage approach: cross-site learning of a generative MRI prior, and
subject-specific injection of the imaging operator. The global MRI prior is
learned via an unconditional adversarial model that synthesizes high-quality MR
images based on latent variables. Specificity in the prior is preserved via a
mapper subnetwork that produces site-specific latents. During inference, the
prior is combined with subject-specific imaging operators to enable
reconstruction, and further adapted to individual test samples by minimizing
data-consistency loss. Comprehensive experiments on multi-institutional
datasets clearly demonstrate enhanced generalization performance of FedGIMP
against site-specific and federated methods based on conditional models, as
well as traditional reconstruction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransformNet: Self-supervised representation learning through predicting geometric transformations. (arXiv:2202.04181v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04181">
<div class="article-summary-box-inner">
<span><p>Deep neural networks need a big amount of training data, while in the real
world there is a scarcity of data available for training purposes. To resolve
this issue unsupervised methods are used for training with limited data. In
this report, we describe the unsupervised semantic feature learning approach
for recognition of the geometric transformation applied to the input data. The
basic concept of our approach is that if someone is unaware of the objects in
the images, he/she would not be able to quantitatively predict the geometric
transformation that was applied to them. This self supervised scheme is based
on pretext task and the downstream task. The pretext classification task to
quantify the geometric transformations should force the CNN to learn high-level
salient features of objects useful for image classification. In our baseline
model, we define image rotations by multiples of 90 degrees. The CNN trained on
this pretext task will be used for the classification of images in the CIFAR-10
dataset as a downstream task. we run the baseline method using various models,
including ResNet, DenseNet, VGG-16, and NIN with a varied number of rotations
in feature extracting and fine-tuning settings. In extension of this baseline
model we experiment with transformations other than rotation in pretext task.
We compare performance of selected models in various settings with different
transformations applied to images,various data augmentation techniques as well
as using different optimizers. This series of different type of experiments
will help us demonstrate the recognition accuracy of our self-supervised model
when applied to a downstream task of classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaskGIT: Masked Generative Image Transformer. (arXiv:2202.04200v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04200">
<div class="article-summary-box-inner">
<span><p>Generative transformers have experienced rapid popularity growth in the
computer vision community in synthesizing high-fidelity and high-resolution
images. The best generative transformer models so far, however, still treat an
image naively as a sequence of tokens, and decode an image sequentially
following the raster scan ordering (i.e. line-by-line). We find this strategy
neither optimal nor efficient. This paper proposes a novel image synthesis
paradigm using a bidirectional transformer decoder, which we term MaskGIT.
During training, MaskGIT learns to predict randomly masked tokens by attending
to tokens in all directions. At inference time, the model begins with
generating all tokens of an image simultaneously, and then refines the image
iteratively conditioned on the previous generation. Our experiments demonstrate
that MaskGIT significantly outperforms the state-of-the-art transformer model
on the ImageNet dataset, and accelerates autoregressive decoding by up to 64x.
Besides, we illustrate that MaskGIT can be easily extended to various image
editing tasks, such as inpainting, extrapolation, and image manipulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Event-Based Tracking and Detection for Maritime Environments. (arXiv:2202.04231v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04231">
<div class="article-summary-box-inner">
<span><p>Event cameras are ideal for object tracking applications due to their ability
to capture fast-moving objects while mitigating latency and data redundancy.
Existing event-based clustering and feature tracking approaches for
surveillance and object detection work well in the majority of cases, but fall
short in a maritime environment. Our application of maritime vessel detection
and tracking requires a process that can identify features and output a
confidence score representing the likelihood that the feature was produced by a
vessel, which may trigger a subsequent alert or activate a classification
system. However, the maritime environment presents unique challenges such as
the tendency of waves to produce the majority of events, demanding the majority
of computational processing and producing false positive detections. By
filtering redundant events and analyzing the movement of each event cluster, we
can identify and track vessels while ignoring shorter lived and erratic
features such as those produced by waves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations. (arXiv:2202.04235v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04235">
<div class="article-summary-box-inner">
<span><p>Model robustness against adversarial examples of single perturbation type
such as the $\ell_{p}$-norm has been widely studied, yet its generalization to
more realistic scenarios involving multiple semantic perturbations and their
composition remains largely unexplored. In this paper, we firstly propose a
novel method for generating composite adversarial examples. By utilizing
component-wise projected gradient descent and automatic attack-order
scheduling, our method can find the optimal attack composition. We then propose
\textbf{generalized adversarial training} (\textbf{GAT}) to extend model
robustness from $\ell_{p}$-norm to composite semantic perturbations, such as
the combination of Hue, Saturation, Brightness, Contrast, and Rotation. The
results on ImageNet and CIFAR-10 datasets show that GAT can be robust not only
to any single attack but also to any combination of multiple attacks. GAT also
outperforms baseline $\ell_{\infty}$-norm bounded adversarial training
approaches by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Robust Convolutional Neural Networks with Relevant Feature Focusing via Explanations. (arXiv:2202.04237v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04237">
<div class="article-summary-box-inner">
<span><p>Existing image recognition techniques based on convolutional neural networks
(CNNs) basically assume that the training and test datasets are sampled from
i.i.d distributions. However, this assumption is easily broken in the real
world because of the distribution shift that occurs when the co-occurrence
relations between objects and backgrounds in input images change. Under this
type of distribution shift, CNNs learn to focus on features that are not
task-relevant, such as backgrounds from the training data, and degrade their
accuracy on the test data. To tackle this problem, we propose relevant feature
focusing (ReFF). ReFF detects task-relevant features and regularizes CNNs via
explanation outputs (e.g., Grad-CAM). Since ReFF is composed of post-hoc
explanation modules, it can be easily applied to off-the-shelf CNNs.
Furthermore, ReFF requires no additional inference cost at test time because it
is only used for regularization while training. We demonstrate that CNNs
trained with ReFF focus on features relevant to the target task and that ReFF
improves the test-time accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A multiscale spatiotemporal approach for smallholder irrigation detection. (arXiv:2202.04239v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04239">
<div class="article-summary-box-inner">
<span><p>In presenting an irrigation detection methodology that leverages multiscale
satellite imagery of vegetation abundance, this paper introduces a process to
supplement limited ground-collected labels and ensure classifier applicability
in an area of interest. Spatiotemporal analysis of MODIS 250m Enhanced
Vegetation Index (EVI) timeseries characterizes native vegetation phenologies
at regional scale to provide the basis for a continuous phenology map that
guides supplementary label collection over irrigated and non-irrigated
agriculture. Subsequently, validated dry season greening and senescence cycles
observed in 10m Sentinel-2 imagery are used to train a suite of classifiers for
automated detection of potential smallholder irrigation. Strategies to improve
model robustness are demonstrated, including a method of data augmentation that
randomly shifts training samples; and an assessment of classifier types that
produce the best performance in withheld target regions. The methodology is
applied to detect smallholder irrigation in two states in the Ethiopian
highlands, Tigray and Amhara. Results show that a transformer-based neural
network architecture allows for the most robust prediction performance in
withheld regions, followed closely by a CatBoost random forest model. Over
withheld ground-collection survey labels, the transformer-based model achieves
96.7% accuracy over non-irrigated samples and 95.9% accuracy over irrigated
samples. Over a larger set of samples independently collected via the
introduced method of label supplementation, non-irrigated and irrigated labels
are predicted with 98.3% and 95.5% accuracy, respectively. The detection model
is then deployed over Tigray and Amhara, revealing crop rotation patterns and
year-over-year irrigated area change. Predictions suggest that irrigated area
in these two states has decreased by approximately 40% from 2020 to 2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distillation with Contrast is All You Need for Self-Supervised Point Cloud Representation Learning. (arXiv:2202.04241v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04241">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a simple and general framework for self-supervised
point cloud representation learning. Human beings understand the 3D world by
extracting two levels of information and establishing the relationship between
them. One is the global shape of an object, and the other is the local
structures of it. However, few existing studies in point cloud representation
learning explored how to learn both global shapes and local-to-global
relationships without a specified network architecture. Inspired by how human
beings understand the world, we utilize knowledge distillation to learn both
global shape information and the relationship between global shape and local
structures. At the same time, we combine contrastive learning with knowledge
distillation to make the teacher network be better updated. Our method achieves
the state-of-the-art performance on linear classification and multiple other
downstream tasks. Especially, we develop a variant of ViT for 3D point cloud
feature extraction, which also achieves comparable results with existing
backbones when combined with our framework, and visualization of the attention
maps show that our model does understand the point cloud by combining the
global shape information and multiple local structural information, which is
consistent with the inspiration of our representation learning method. Our code
will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion-Aware Transformer For Occluded Person Re-identification. (arXiv:2202.04243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04243">
<div class="article-summary-box-inner">
<span><p>Recently, occluded person re-identification(Re-ID) remains a challenging task
that people are frequently obscured by other people or obstacles, especially in
a crowd massing situation. In this paper, we propose a self-supervised deep
learning method to improve the location performance for human parts through
occluded person Re-ID. Unlike previous works, we find that motion information
derived from the photos of various human postures can help identify major human
body components. Firstly, a motion-aware transformer encoder-decoder
architecture is designed to obtain keypoints heatmaps and part-segmentation
maps. Secondly, an affine transformation module is utilized to acquire motion
information from the keypoint detection branch. Then the motion information
will support the segmentation branch to achieve refined human part segmentation
maps, and effectively divide the human body into reasonable groups. Finally,
several cases demonstrate the efficiency of the proposed model in
distinguishing different representative parts of the human body, which can
avoid the background and occlusion disturbs. Our method consistently achieves
state-of-the-art results on several popular datasets, including occluded,
partial, and holistic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GiraffeDet: A Heavy-Neck Paradigm for Object Detection. (arXiv:2202.04256v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04256">
<div class="article-summary-box-inner">
<span><p>In conventional object detection frameworks, a backbone body inherited from
image recognition models extracts deep latent features and then a neck module
fuses these latent features to capture information at different scales. As the
resolution in object detection is much larger than in image recognition, the
computational cost of the backbone often dominates the total inference cost.
This heavy-backbone design paradigm is mostly due to the historical legacy when
transferring image recognition models to object detection rather than an
end-to-end optimized design for object detection. In this work, we show that
such paradigm indeed leads to sub-optimal object detection models. To this end,
we propose a novel heavy-neck paradigm, GiraffeDet, a giraffe-like network for
efficient object detection. The GiraffeDet uses an extremely lightweight
backbone and a very deep and large neck module which encourages dense
information exchange among different spatial scales as well as different levels
of latent semantics simultaneously. This design paradigm allows detectors to
process the high-level semantic information and low-level spatial information
at the same priority even in the early stage of the network, making it more
effective in detection tasks. Numerical evaluations on multiple popular object
detection benchmarks show that GiraffeDet consistently outperforms previous
SOTA models across a wide spectrum of resource constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Detection without Model Information. (arXiv:2202.04271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04271">
<div class="article-summary-box-inner">
<span><p>Most prior state-of-the-art adversarial detection works assume that the
underlying vulnerable model is accessible, i,e., the model can be trained or
its outputs are visible. However, this is not a practical assumption due to
factors like model encryption, model information leakage and so on. In this
work, we propose a model independent adversarial detection method using a
simple energy function to distinguish between adversarial and natural inputs.
We train a standalone detector independent of the underlying model, with
sequential layer-wise training to increase the energy separation corresponding
to natural and adversarial inputs. With this, we perform energy
distribution-based adversarial detection. Our method achieves state-of-the-art
detection performance (ROC-AUC &gt; 0.9) across a wide range of gradient, score
and decision-based adversarial attacks on CIFAR10, CIFAR100 and TinyImagenet
datasets. Compared to prior approaches, our method requires ~10-100x less
number of operations and parameters for adversarial detection. Further, we show
that our detection method is transferable across different datasets and
adversarial attacks. For reproducibility, we provide code in the supplementary
material.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Amplitude Spectrum Transformation for Open Compound Domain Adaptive Semantic Segmentation. (arXiv:2202.04287v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04287">
<div class="article-summary-box-inner">
<span><p>Open compound domain adaptation (OCDA) has emerged as a practical adaptation
setting which considers a single labeled source domain against a compound of
multi-modal unlabeled target data in order to generalize better on novel unseen
domains. We hypothesize that an improved disentanglement of domain-related and
task-related factors of dense intermediate layer features can greatly aid OCDA.
Prior-arts attempt this indirectly by employing adversarial domain
discriminators on the spatial CNN output. However, we find that latent features
derived from the Fourier-based amplitude spectrum of deep CNN features hold a
more tractable mapping with domain discrimination. Motivated by this, we
propose a novel feature space Amplitude Spectrum Transformation (AST). During
adaptation, we employ the AST auto-encoder for two purposes. First, carefully
mined source-target instance pairs undergo a simulation of cross-domain feature
stylization (AST-Sim) at a particular layer by altering the AST-latent. Second,
AST operating at a later layer is tasked to normalize (AST-Norm) the domain
content by fixing its latent to a mean prototype. Our simplified adaptation
technique is not only clustering-free but also free from complex adversarial
alignment. We achieve leading performance against the prior arts on the OCDA
scene segmentation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Bootstrap for Combating Label Noise. (arXiv:2202.04291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04291">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are powerful tools for representation learning, but can
easily overfit to noisy labels which are prevalent in many real-world
scenarios. Generally, noisy supervision could stem from variation among
labelers, label corruption by adversaries, etc. To combat such label noises,
one popular line of approach is to apply customized weights to the training
instances, so that the corrupted examples contribute less to the model
learning. However, such learning mechanisms potentially erase important
information about the data distribution and therefore yield suboptimal results.
To leverage useful information from the corrupted instances, an alternative is
the bootstrapping loss, which reconstructs new training targets on-the-fly by
incorporating the network's own predictions (i.e., pseudo-labels).
</p>
<p>In this paper, we propose a more generic learnable loss objective which
enables a joint reweighting of instances and labels at once. Specifically, our
method dynamically adjusts the per-sample importance weight between the real
observed labels and pseudo-labels, where the weights are efficiently determined
in a meta process. Compared to the previous instance reweighting methods, our
approach concurrently conducts implicit relabeling, and thereby yield
substantial improvements with almost no extra cost. Extensive experimental
results demonstrated the strengths of our approach over existing methods on
multiple natural and medical image benchmark datasets, including CIFAR-10,
CIFAR-100, ISIC2019 and Clothing 1M. The code is publicly available at
https://github.com/yuyinzhou/L2B.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Difference Captioning with Pre-training and Contrastive Learning. (arXiv:2202.04298v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04298">
<div class="article-summary-box-inner">
<span><p>The Image Difference Captioning (IDC) task aims to describe the visual
differences between two similar images with natural language. The major
challenges of this task lie in two aspects: 1) fine-grained visual differences
that require learning stronger vision and language association and 2) high-cost
of manual annotations that leads to limited supervised data. To address these
challenges, we propose a new modeling framework following the
pre-training-finetuning paradigm. Specifically, we design three self-supervised
tasks and contrastive learning strategies to align visual differences and text
descriptions at a fine-grained level. Moreover, we propose a data expansion
strategy to utilize extra cross-task supervision information, such as data for
fine-grained image classification, to alleviate the limitation of available
supervised IDC data. Extensive experiments on two IDC benchmark datasets,
CLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed
modeling framework. The codes and models will be released at
https://github.com/yaolinli/IDC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?. (arXiv:2202.04306v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04306">
<div class="article-summary-box-inner">
<span><p>The task of Outside Knowledge Visual Question Answering (OKVQA) requires an
automatic system to answer natural language questions about pictures and images
using external knowledge. We observe that many visual questions, which contain
deictic referential phrases referring to entities in the image, can be
rewritten as "non-grounded" questions and can be answered by existing
text-based question answering systems. This allows for the reuse of existing
text-based Open Domain Question Answering (QA) Systems for visual question
answering. In this work, we propose a potentially data-efficient approach that
reuses existing systems for (a) image analysis, (b) question rewriting, and (c)
text-based question answering to answer such visual questions. Given an image
and a question pertaining to that image (a visual question), we first extract
the entities present in the image using pre-trained object and scene
classifiers. Using these detected entities, the visual questions can be
rewritten so as to be answerable by open domain QA systems. We explore two
rewriting strategies: (1) an unsupervised method using BERT for masking and
rewriting, and (2) a weakly supervised approach that combines adaptive
rewriting and reinforcement learning techniques to use the implicit feedback
from the QA system. We test our strategies on the publicly available OKVQA
dataset and obtain a competitive performance with state-of-the-art models while
using only 10% of the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Motion In-betweening. (arXiv:2202.04307v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04307">
<div class="article-summary-box-inner">
<span><p>Motion in-betweening (MIB) is a process of generating intermediate skeletal
movement between the given start and target poses while preserving the
naturalness of the motion, such as periodic footstep motion while walking.
Although state-of-the-art MIB methods are capable of producing plausible
motions given sparse key-poses, they often lack the controllability to generate
motions satisfying the semantic contexts required in practical applications. We
focus on the method that can handle pose or semantic conditioned MIB tasks
using a unified model. We also present a motion augmentation method to improve
the quality of pose-conditioned motion generation via defining a distribution
over smooth trajectories. Our proposed method outperforms the existing
state-of-the-art MIB method in pose prediction errors while providing
additional controllability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anchor Graph Structure Fusion Hashing for Cross-Modal Similarity Search. (arXiv:2202.04327v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04327">
<div class="article-summary-box-inner">
<span><p>Cross-modal hashing still has some challenges needed to address: (1) most
existing CMH methods take graphs as input to model data distribution. These
methods omit to consider the correlation of graph structure among multiple
modalities; (2) most existing CMH methods ignores considering the fusion
affinity among multi-modalities data; (3) most existing CMH methods relax the
discrete constraints to solve the optimization objective, significantly
degrading the retrieval performance. To solve the above limitations, we propose
a novel Anchor Graph Structure Fusion Hashing (AGSFH). AGSFH constructs the
anchor graph structure fusion matrix from different anchor graphs of multiple
modalities with the Hadamard product, which can fully exploit the geometric
property of underlying data structure. Based on the anchor graph structure
fusion matrix, AGSFH attempts to directly learn an intrinsic anchor graph,
where the structure of the intrinsic anchor graph is adaptively tuned so that
the number of components of the intrinsic graph is exactly equal to the number
of clusters. Besides, AGSFH preserves the anchor fusion affinity into the
common binary Hamming space. Furthermore, a discrete optimization framework is
designed to learn the unified binary codes. Extensive experimental results on
three public social datasets demonstrate the superiority of AGSFH.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Feature Rotation for Multimodal Image Style Transfer. (arXiv:2202.04426v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04426">
<div class="article-summary-box-inner">
<span><p>Recently, style transfer is a research area that attracts a lot of attention,
which transfers the style of an image onto a content target. Extensive research
on style transfer has aimed at speeding up processing or generating
high-quality stylized images. Most approaches only produce an output from a
content and style image pair, while a few others use complex architectures and
can only produce a certain number of outputs. In this paper, we propose a
simple method for representing style features in many ways called Deep Feature
Rotation (DFR), while not only producing diverse outputs but also still
achieving effective stylization compared to more complex methods. Our approach
is representative of the many ways of augmentation for intermediate feature
embedding without consuming too much computational expense. We also analyze our
method by visualizing output in different rotation weights. Our code is
available at https://github.com/sonnguyen129/deep-feature-rotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-Guided Day-Night Visual Localization in Urban Scenes. (arXiv:2202.04445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04445">
<div class="article-summary-box-inner">
<span><p>We introduce Object-Guided Localization (OGuL) based on a novel method of
local-feature matching. Direct matching of local features is sensitive to
significant changes in illumination. In contrast, object detection often
survives severe changes in lighting conditions. The proposed method first
detects semantic objects and establishes correspondences of those objects
between images. Object correspondences provide local coarse alignment of the
images in the form of a planar homography. These homographies are consequently
used to guide the matching of local features. Experiments on standard urban
localization datasets (Aachen, Extended-CMU-Season, RobotCar-Season) show that
OGuL significantly improves localization results with as simple local features
as SIFT, and its performance competes with the state-of-the-art CNN-based
methods trained for day-to-night localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting the intended action using internal simulation of perception. (arXiv:2202.04466v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04466">
<div class="article-summary-box-inner">
<span><p>This article proposes an architecture, which allows the prediction of
intention by internally simulating perceptual states represented by action
pattern vectors. To this end, associative self-organising neural networks
(A-SOM) is utilised to build a hierarchical cognitive architecture for
recognition and simulation of the skeleton based human actions. The abilities
of the proposed architecture in recognising and predicting actions is evaluated
in experiments using three different datasets of 3D actions. Based on the
experiments of this article, applying internally simulated perceptual states
represented by action pattern vectors improves the performance of the
recognition task in all experiments. Furthermore, internal simulation of
perception addresses the problem of having limited access to the sensory input,
and also the future prediction of the consecutive perceptual sequences. The
performance of the system is compared and discussed with similar architecture
using self-organizing neural networks (SOM).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRAT-Pred: Vehicle Trajectory Prediction with Crystal Graph Convolutional Neural Networks and Multi-Head Self-Attention. (arXiv:2202.04488v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04488">
<div class="article-summary-box-inner">
<span><p>Predicting the motion of surrounding vehicles is essential for autonomous
vehicles, as it governs their own motion plan. Current state-of-the-art vehicle
prediction models heavily rely on map information. In reality, however, this
information is not always available. We therefore propose CRAT-Pred, a
multi-modal and non-rasterization-based trajectory prediction model,
specifically designed to effectively model social interactions between
vehicles, without relying on map information. CRAT-Pred applies a graph
convolution method originating from the field of material science to vehicle
prediction, allowing to efficiently leverage edge features, and combines it
with multi-head self-attention. Compared to other map-free approaches, the
model achieves state-of-the-art performance with a significantly lower number
of model parameters. In addition to that, we quantitatively show that the
self-attention mechanism is able to learn social interactions between vehicles,
with the weights representing a measurable interaction score. The source code
is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Blind Quality Assessment for Laparoscopic Videos using Neural Networks. (arXiv:2202.04517v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04517">
<div class="article-summary-box-inner">
<span><p>Video quality assessment is a challenging problem having a critical
significance in the context of medical imaging. For instance, in laparoscopic
surgery, the acquired video data suffers from different kinds of distortion
that not only hinder surgery performance but also affect the execution of
subsequent tasks in surgical navigation and robotic surgeries. For this reason,
we propose in this paper neural network-based approaches for distortion
classification as well as quality prediction. More precisely, a Residual
Network (ResNet) based approach is firstly developed for simultaneous ranking
and classification task. Then, this architecture is extended to make it
appropriate for the quality prediction task by using an additional Fully
Connected Neural Network (FCNN). To train the overall architecture (ResNet and
FCNN models), transfer learning and end-to-end learning approaches are
investigated. Experimental results, carried out on a new laparoscopic video
quality database, have shown the efficiency of the proposed methods compared to
recent conventional and deep learning based approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NIMBLE: A Non-rigid Hand Model with Bones and Muscles. (arXiv:2202.04533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04533">
<div class="article-summary-box-inner">
<span><p>Emerging Metaverse applications demand reliable, accurate, and photorealistic
reproductions of human hands to perform sophisticated operations as if in the
physical world. While real human hand represents one of the most intricate
coordination between bones, muscle, tendon, and skin, state-of-the-art
techniques unanimously focus on modeling only the skeleton of the hand. In this
paper, we present NIMBLE, a novel parametric hand model that includes the
missing key components, bringing 3D hand model to a new level of realism. We
first annotate muscles, bones and skins on the recent Magnetic Resonance
Imaging hand (MRI-Hand) dataset and then register a volumetric template hand
onto individual poses and subjects within the dataset. NIMBLE consists of 20
bones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin
mesh. Via iterative shape registration and parameter learning, it further
produces shape blend shapes, pose blend shapes, and a joint regressor. We
demonstrate applying NIMBLE to modeling, rendering, and visual inference tasks.
By enforcing the inner bones and muscles to match anatomic and kinematic rules,
NIMBLE can animate 3D hands to new poses at unprecedented realism. To model the
appearance of skin, we further construct a photometric HandStage to acquire
high-quality textures and normal maps to model wrinkles and palm print.
Finally, NIMBLE also benefits learning-based hand pose and shape estimation by
either synthesizing rich data or acting directly as a differentiable layer in
the inference network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Structural Sparsity in Neural Image Compression. (arXiv:2202.04595v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04595">
<div class="article-summary-box-inner">
<span><p>Neural image compression have reached or out-performed traditional methods
(such as JPEG, BPG, WebP). However,their sophisticated network structures with
cascaded convolution layers bring heavy computational burden for practical
deployment. In this paper, we explore the structural sparsity in neural image
compression network to obtain real-time acceleration without any specialized
hardware design or algorithm. We propose a simple plug-in adaptive binary
channel masking(ABCM) to judge the importance of each convolution channel and
introduce sparsity during training. During inference, the unimportant channels
are pruned to obtain slimmer network and less computation. We implement our
method into three neural image compression networks with different entropy
models to verify its effectiveness and generalization, the experiment results
show that up to 7x computation reduction and 3x acceleration can be achieved
with negligible performance drop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distance Estimation and Animal Tracking for Wildlife Camera Trapping. (arXiv:2202.04613v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04613">
<div class="article-summary-box-inner">
<span><p>The ongoing biodiversity crysis calls for accurate estimation of animal
density and abundance to identify, for example, sources of biodiversity decline
and effectiveness of conservation interventions. Camera traps together with
abundance estimation methods are often employed for this purpose. The necessary
distances between camera and observed animal are traditionally derived in a
laborious, fully manual or semi-automatic process. Both approaches require
reference image material, which is both difficult to acquire and not available
for existing datasets. In this study, we propose a fully automatic approach to
estimate camera-to-animal distances, based on monocular depth estimation (MDE),
and without the need of reference image material. We leverage state-of-the-art
relative MDE and a novel alignment procedure to estimate metric distances. We
evaluate the approach on a zoo scenario dataset unseen during training. We
achieve a mean absolute distance estimation error of only 0.9864 meters at a
precision of 90.3% and recall of 63.8%, while completely eliminating the
previously required manual effort for biodiversity researchers. The code will
be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Redundancy in the Bottleneck Representation of the Autoencoders. (arXiv:2202.04629v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04629">
<div class="article-summary-box-inner">
<span><p>Autoencoders are a type of unsupervised neural networks, which can be used to
solve various tasks, e.g., dimensionality reduction, image compression, and
image denoising. An AE has two goals: (i) compress the original input to a
low-dimensional space at the bottleneck of the network topology using an
encoder, (ii) reconstruct the input from the representation at the bottleneck
using a decoder. Both encoder and decoder are optimized jointly by minimizing a
distortion-based loss which implicitly forces the model to keep only those
variations of input data that are required to reconstruct the and to reduce
redundancies. In this paper, we propose a scheme to explicitly penalize feature
redundancies in the bottleneck representation. To this end, we propose an
additional loss term, based on the pair-wise correlation of the neurons, which
complements the standard reconstruction loss forcing the encoder to learn a
more diverse and richer representation of the input. We tested our approach
across different tasks: dimensionality reduction using three different dataset,
image compression using the MNIST dataset, and image denoising using fashion
MNIST. The experimental results show that the proposed loss leads consistently
to superior performance compared to the standard AE loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-Level Region Contrast for Object Detection Pre-Training. (arXiv:2202.04639v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04639">
<div class="article-summary-box-inner">
<span><p>In this work we present point-level region contrast, a self-supervised
pre-training approach for the task of object detection. This approach is
motivated by the two key factors in detection: localization and recognition.
While accurate localization favors models that operate at the pixel- or
point-level, correct recognition typically relies on a more holistic,
region-level view of objects. Incorporating this perspective in pre-training,
our approach performs contrastive learning by directly sampling individual
point pairs from different regions. Compared to an aggregated representation
per region, our approach is more robust to the change in input region quality,
and further enables us to implicitly improve initial region assignments via
online knowledge distillation during training. Both advantages are important
when dealing with imperfect regions encountered in the unsupervised setting.
Experiments show point-level region contrast improves on state-of-the-art
pre-training methods for object detection and segmentation across multiple
tasks and datasets, and we provide extensive ablation studies and
visualizations to aid understanding. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributionally Robust Deep Learning using Hardness Weighted Sampling. (arXiv:2001.02658v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.02658">
<div class="article-summary-box-inner">
<span><p>Limiting failures of machine learning systems is of paramount importance for
safety-critical applications. In order to improve the robustness of machine
learning systems, Distributionally Robust Optimization (DRO) has been proposed
as a generalization of Empirical Risk Minimization (ERM). However, its use in
deep learning has been severely restricted due to the relative inefficiency of
the optimizers available for DRO in comparison to the wide-spread variants of
Stochastic Gradient Descent (SGD) optimizers for ERM. We propose SGD with
hardness weighted sampling, a principled and efficient optimization method for
DRO in machine learning that is particularly suited in the context of deep
learning. Similar to a hard example mining strategy in practice, the proposed
algorithm is straightforward to implement and computationally as efficient as
SGD-based optimizers used for deep learning, requiring minimal overhead
computation. In contrast to typical ad hoc hard mining approaches, we prove the
convergence of our DRO algorithm for over-parameterized deep learning networks
with ReLU activation and a finite number of layers and parameters. Our
experiments on fetal brain 3D MRI segmentation and brain tumor segmentation in
MRI demonstrate the feasibility and the usefulness of our approach. Using our
hardness weighted sampling for training a state-of-the-art deep learning
pipeline leads to improved robustness to anatomical variabilities in automatic
fetal brain 3D MRI segmentation using deep learning and to improved robustness
to the image protocol variations in brain tumor segmentation. Our code is
available at https://github.com/LucasFidon/HardnessWeightedSampler.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sampling possible reconstructions of undersampled acquisitions in MR imaging. (arXiv:2010.00042v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.00042">
<div class="article-summary-box-inner">
<span><p>Undersampling the k-space during MR acquisitions saves time, however results
in an ill-posed inversion problem, leading to an infinite set of images as
possible solutions. Traditionally, this is tackled as a reconstruction problem
by searching for a single "best" image out of this solution set according to
some chosen regularization or prior. This approach, however, misses the
possibility of other solutions and hence ignores the uncertainty in the
inversion process. In this paper, we propose a method that instead returns
multiple images which are possible under the acquisition model and the chosen
prior to capture the uncertainty in the inversion process. To this end, we
introduce a low dimensional latent space and model the posterior distribution
of the latent vectors given the acquisition data in k-space, from which we can
sample in the latent space and obtain the corresponding images. We use a
variational autoencoder for the latent model and the Metropolis adjusted
Langevin algorithm for the sampling. We evaluate our method on two datasets;
with images from the Human Connectome Project and in-house measured multi-coil
images. We compare to five alternative methods. Results indicate that the
proposed method produces images that match the measured k-space data better
than the alternatives, while showing realistic structural variability.
Furthermore, in contrast to the compared methods, the proposed method yields
higher uncertainty in the undersampled phase encoding direction, as expected.
</p>
<p>Keywords: Magnetic Resonance image reconstruction, uncertainty estimation,
inverse problems, sampling, MCMC, deep learning, unsupervised learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfVoxeLO: Self-supervised LiDAR Odometry with Voxel-based Deep Neural Networks. (arXiv:2010.09343v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09343">
<div class="article-summary-box-inner">
<span><p>Recent learning-based LiDAR odometry methods have demonstrated their
competitiveness. However, most methods still face two substantial challenges:
1) the 2D projection representation of LiDAR data cannot effectively encode 3D
structures from the point clouds; 2) the needs for a large amount of labeled
data for training limit the application scope of these methods. In this paper,
we propose a self-supervised LiDAR odometry method, dubbed SelfVoxeLO, to
tackle these two difficulties. Specifically, we propose a 3D convolution
network to process the raw LiDAR data directly, which extracts features that
better encode the 3D geometric patterns. To suit our network to self-supervised
learning, we design several novel loss functions that utilize the inherent
properties of LiDAR point clouds. Moreover, an uncertainty-aware mechanism is
incorporated in the loss functions to alleviate the interference of moving
objects/noises. We evaluate our method's performances on two large-scale
datasets, i.e., KITTI and Apollo-SouthBay. Our method outperforms
state-of-the-art unsupervised methods by 27%/32% in terms of
translational/rotational errors on the KITTI dataset and also performs well on
the Apollo-SouthBay dataset. By including more unlabelled training data, our
method can further improve performance comparable to the supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensor Composition Net for Visual Relationship Prediction. (arXiv:2012.05473v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05473">
<div class="article-summary-box-inner">
<span><p>We present a novel Tensor Composition Net (TCN) to predict visual
relationships in images. Visual Relationship Prediction (VRP) provides a more
challenging test of image understanding than conventional image tagging and is
difficult to learn due to a large label-space and incomplete annotation. The
key idea of our TCN is to exploit the low-rank property of the visual
relationship tensor, so as to leverage correlations within and across objects
and relations and make a structured prediction of all visual relationships in
an image. To show the effectiveness of our model, we first empirically compare
our model with Multi-Label Image Classification (MLIC) methods, eXtreme
Multi-label Classification (XMC) methods, and VRD methods. We then show that
thanks to our tensor (de)composition layer, our model can predict visual
relationships which have not been seen in the training dataset. We finally show
our TCN's image-level visual relationship prediction provides a simple and
efficient mechanism for relation-based image-retrieval even compared with VRD
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balanced softmax cross-entropy for incremental learning with and without memory. (arXiv:2103.12532v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12532">
<div class="article-summary-box-inner">
<span><p>When incrementally trained on new classes, deep neural networks are subject
to catastrophic forgetting which leads to an extreme deterioration of their
performance on the old classes while learning the new ones. Using a small
memory containing few samples from past classes has shown to be an effective
method to mitigate catastrophic forgetting. However, due to the limited size of
the replay memory, there is a large imbalance between the number of samples for
the new and the old classes in the training dataset resulting in bias in the
final model. To address this issue, we propose to use the Balanced Softmax
Cross-Entropy and show that it can be seamlessly combined with state-of-the-art
approaches for class-incremental learning in order to improve their accuracy
while also potentially decreasing the computational cost of the training
procedure. We further extend this approach to the more demanding
class-incremental learning without memory setting and achieve competitive
results with memory-based approaches. Experiments on the challenging ImageNet,
ImageNet-Subset and CIFAR100 benchmarks with various settings demonstrate the
benefits of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leaning Compact and Representative Features for Cross-Modality Person Re-Identification. (arXiv:2103.14210v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14210">
<div class="article-summary-box-inner">
<span><p>This paper pays close attention to the cross-modality visible-infrared person
re-identification (VI Re-ID) task, which aims to match pedestrian samples
between visible and infrared modes. In order to reduce the modality-discrepancy
between samples from different cameras, most existing works usually use
constraints based on Euclidean metric. Because of the Euclidean based distance
metric strategy cannot effectively measure the internal angles between the
embedded vectors, the existing solutions cannot learn the angularly
discriminative feature embedding. Since the most important factor affecting the
classification task based on embedding vector is whether there is an angularly
discriminative feature space, in this paper, we present a new loss function
called Enumerate Angular Triplet (EAT) loss. Also, motivated by the knowledge
distillation, to narrow down the features between different modalities before
feature embedding, we further present a novel Cross-Modality Knowledge
Distillation (CMKD) loss. Benefit from the above two considerations, the
embedded features are discriminative enough in a way to tackle
modality-discrepancy problem. The experimental results on RegDB and SYSU-MM01
datasets have demonstrated that the proposed method is superior to the other
most advanced methods in terms of impressive performance. Code is available at
https://github.com/IVIPLab/LCCRF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MODS -- A USV-oriented object detection and obstacle segmentation benchmark. (arXiv:2105.02359v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02359">
<div class="article-summary-box-inner">
<span><p>Small-sized unmanned surface vehicles (USV) are coastal water devices with a
broad range of applications such as environmental control and surveillance. A
crucial capability for autonomous operation is obstacle detection for timely
reaction and collision avoidance, which has been recently explored in the
context of camera-based visual scene interpretation. Owing to curated datasets,
substantial advances in scene interpretation have been made in a related field
of unmanned ground vehicles. However, the current maritime datasets do not
adequately capture the complexity of real-world USV scenes and the evaluation
protocols are not standardised, which makes cross-paper comparison of different
methods difficult and hinders the progress. To address these issues, we
introduce a new obstacle detection benchmark MODS, which considers two major
perception tasks: maritime object detection and the more general maritime
obstacle segmentation. We present a new diverse maritime evaluation dataset
containing approximately 81k stereo images synchronized with an on-board IMU,
with over 60k objects annotated. We propose a new obstacle segmentation
performance evaluation protocol that reflects the detection accuracy in a way
meaningful for practical USV navigation. Nineteen recent state-of-the-art
object detection and obstacle segmentation methods are evaluated using the
proposed protocol, creating a benchmark to facilitate development of the field.
The proposed dataset, as well as evaluation routines, are made publicly
available at vicos.si/resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v3 [cs.NI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11166">
<div class="article-summary-box-inner">
<span><p>State-of-the-art performance for many emerging edge applications is achieved
by deep neural networks (DNNs). Often, the employed DNNs are location- and
time-dependent, and the parameters of a specific DNN must be delivered from an
edge server to the edge device rapidly and efficiently to carry out
time-sensitive inference tasks. This can be considered as a joint
source-channel coding (JSCC) problem, in which the goal is not to recover the
DNN coefficients with the minimal distortion, but in a manner that provides the
highest accuracy in the downstream task. For this purpose we introduce AirNet,
a novel training and analog transmission method to deliver DNNs over the air.
We first train the DNN with noise injection to counter the wireless channel
noise. We also employ pruning to identify the most significant DNN parameters
that can be delivered within the available channel bandwidth, knowledge
distillation, and non-linear bandwidth expansion to provide better error
protection for the most important network parameters. We show that AirNet
achieves significantly higher test accuracy compared to the separation-based
alternative, and exhibits graceful degradation with channel quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains. (arXiv:2105.14391v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14391">
<div class="article-summary-box-inner">
<span><p>Tracking the 6D pose of objects in video sequences is important for robot
manipulation. This work presents se(3)-TrackNet, a data-driven optimization
approach for long term, 6D pose tracking. It aims to identify the optimal
relative pose given the current RGB-D observation and a synthetic image
conditioned on the previous best estimate and the object's model. The key
contribution in this context is a novel neural network architecture, which
appropriately disentangles the feature encoding to help reduce domain shift,
and an effective 3D orientation representation via Lie Algebra. Consequently,
even when the network is trained solely with synthetic data can work
effectively over real images. Comprehensive experiments over multiple
benchmarks show se(3)-TrackNet achieves consistently robust estimates and
outperforms alternatives, even though they have been trained with real images.
The approach runs in real time at 90.9Hz. Code, data and supplementary video
for this project are available at
https://github.com/wenbowen123/iros20-6d-pose-tracking
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LENAS: Learning-based Neural Architecture Search and Ensemble for 3D Radiotherapy Dose Prediction. (arXiv:2106.06733v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06733">
<div class="article-summary-box-inner">
<span><p>Radiation therapy treatment planning is a complex process, as the target dose
prescription and normal tissue sparing are conflicting objectives. In order to
reduce human planning time efforts and improve the quality of treatment
planning, knowledge-based planning (KBP) is in high demand. In this study, we
propose a novel learning-based ensemble approach, named LENAS, which integrates
neural architecture search (NAS) with knowledge distillation for 3D
radiotherapy dose prediction. Specifically, the prediction network first
exhaustively searches each block from an enormous architecture space. Then,
multiple architectures with promising performance and a large diversity are
selected. To reduce the inference time, we adopt the teacher-student paradigm
by treating the combination of diverse outputs from multiple learned networks
as supervisions to guide the student network training. In addition, we apply
adversarial learning to optimize the student network to recover the knowledge
in teacher networks. To the best of our knowledge, this is the first attempt to
investigate NAS and knowledge distillation in ensemble learning, especially in
the field of medical image analysis. The proposed method has been evaluated on
two public datasets, i.e., the OpenKBP and AIMIS dataset. Extensive
experimental results demonstrate the effectiveness of our method and its
superior performance to the state-of-the-art methods. In addition, several
in-depth analysis and empirical guidelines are derived for ensemble learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13797">
<div class="article-summary-box-inner">
<span><p>Transformer recently has presented encouraging progress in computer vision.
In this work, we present new baselines by improving the original Pyramid Vision
Transformer (PVTv1) by adding three designs, including (1) linear complexity
attention layer, (2) overlapping patch embedding, and (3) convolutional
feed-forward network. With these modifications, PVTv2 reduces the computational
complexity of PVTv1 to linear and achieves significant improvements on
fundamental vision tasks such as classification, detection, and segmentation.
Notably, the proposed PVTv2 achieves comparable or better performances than
recent works such as Swin Transformer. We hope this work will facilitate
state-of-the-art Transformer researches in computer vision. Code is available
at https://github.com/whai362/PVT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PU-Flow: a Point Cloud Upsampling Networkwith Normalizing Flows. (arXiv:2107.05893v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05893">
<div class="article-summary-box-inner">
<span><p>Point cloud upsampling aims to generate dense point clouds from given sparse
ones, which is a challenging task due to the irregular and unordered nature of
point sets. To address this issue, we present a novel deep learning-based
model, called PU-Flow, which incorporates normalizing flows and weight
prediction techniques to produce dense points uniformly distributed on the
underlying surface. Specifically, we exploit the invertible characteristics of
normalizing flows to transform points between Euclidean and latent spaces and
formulate the upsampling process as ensemble of neighbouring points in a latent
space, where the ensemble weights are adaptively learned from local geometric
context. Extensive experiments show that our method is competitive and, in most
test cases, it outperforms state-of-the-art methods in terms of reconstruction
quality, proximity-to-surface accuracy, and computation efficiency. The source
code will be publicly available at https://github.com/unknownue/pu-flow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Pose Transfer with Augmented Disentangled Feature Consistency. (arXiv:2107.10984v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10984">
<div class="article-summary-box-inner">
<span><p>Deep generative models have made great progress in synthesizing images with
arbitrary human poses and transferring poses of one person to others. Though
many different methods have been proposed to generate images with high visual
fidelity, the main challenge remains and comes from two fundamental issues:
pose ambiguity and appearance inconsistency. To alleviate the current
limitations and improve the quality of the synthesized images, we propose a
pose transfer network with augmented Disentangled Feature Consistency (DFC-Net)
to facilitate human pose transfer. Given a pair of images containing the source
and target person, DFC-Net extracts pose and static information from the source
and target respectively, then synthesizes an image of the target person with
the desired pose from the source. Moreover, DFC-Net leverages disentangled
feature consistency losses in the adversarial training to strengthen the
transfer coherence and integrates a keypoint amplifier to enhance the pose
feature extraction. With the help of the disentangled feature consistency
losses, we further propose a novel data augmentation scheme that introduces
unpaired support data with the augmented consistency constraints to improve the
generality and robustness of DFC-Net. Extensive experimental results on
Mixamo-Pose and EDN-10k have demonstrated DFC-Net achieves state-of-the-art
performance on pose transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wavelet-Based Network For High Dynamic Range Imaging. (arXiv:2108.01434v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01434">
<div class="article-summary-box-inner">
<span><p>High dynamic range (HDR) imaging from multiple low dynamic range (LDR) images
has been suffering from ghosting artifacts caused by scene and objects motion.
Existing methods, such as optical flow based and end-to-end deep learning based
solutions, are error-prone either in detail restoration or ghosting artifacts
removal. Comprehensive empirical evidence shows that ghosting artifacts caused
by large foreground motion are mainly low-frequency signals and the details are
mainly high-frequency signals. In this work, we propose a novel
frequency-guided end-to-end deep neural network (FHDRNet) to conduct HDR fusion
in the frequency domain, and Discrete Wavelet Transform (DWT) is used to
decompose inputs into different frequency bands. The low-frequency signals are
used to avoid specific ghosting artifacts, while the high-frequency signals are
used for preserving details. Using a U-Net as the backbone, we propose two
novel modules: merging module and frequency-guided upsampling module. The
merging module applies the attention mechanism to the low-frequency components
to deal with the ghost caused by large foreground motion. The frequency-guided
upsampling module reconstructs details from multiple frequency-specific
components with rich details. In addition, a new RAW dataset is created for
training and evaluating multi-frame HDR imaging algorithms in the RAW domain.
Extensive experiments are conducted on public datasets and our RAW dataset,
showing that the proposed FHDRNet achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FBSNet: A Fast Bilateral Symmetrical Network for Real-Time Semantic Segmentation. (arXiv:2109.00699v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00699">
<div class="article-summary-box-inner">
<span><p>Real-time semantic segmentation, which can be visually understood as the
pixel-level classification task on the input image, currently has broad
application prospects, especially in the fast-developing fields of autonomous
driving and drone navigation. However, the huge burden of calculation together
with redundant parameters are still the obstacles to its technological
development. In this paper, we propose a Fast Bilateral Symmetrical Network
(FBSNet) to alleviate the above challenges. Specifically, FBSNet employs a
symmetrical encoder-decoder structure with two branches, semantic information
branch, and spatial detail branch. The semantic information branch is the main
branch with deep network architecture to acquire the contextual information of
the input image and meanwhile acquire sufficient receptive field. While spatial
detail branch is a shallow and simple network used to establish local
dependencies of each pixel for preserving details, which is essential for
restoring the original resolution during the decoding phase. Meanwhile, a
feature aggregation module (FAM) is designed to effectively combine the output
features of the two branches. The experimental results of Cityscapes and CamVid
show that the proposed FBSNet can strike a good balance between accuracy and
efficiency. Specifically, it obtains 70.9\% and 68.9\% mIoU along with the
inference speed of 90 fps and 120 fps on these two test datasets, respectively,
with only 0.62 million parameters on a single RTX 2080Ti GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trident Pyramid Networks: The importance of processing at the feature pyramid level for better object detection. (arXiv:2110.04004v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04004">
<div class="article-summary-box-inner">
<span><p>Feature pyramids have become ubiquitous in multi-scale computer vision tasks
such as object detection. Based on their importance, we divide a computer
vision network into three parts: a backbone (generating a feature pyramid), a
core (refining the feature pyramid) and a head (generating the final output).
Most existing networks operating on feature pyramids, named cores, are shallow
and mostly focus on communication-based processing in the form of top-down and
bottom-up operations. We present a new core architecture called Trident Pyramid
Network (TPN), that allows for a deeper design and for a better balance between
communication-based processing and self-processing. We show consistent
improvements when using our TPN core on the COCO object detection benchmark,
outperforming the popular BiFPN baseline by 0.5 AP. Additionally, we
empirically show that it is more beneficial to put additional computation into
the TPN core, rather than into the backbone, by outperforming a ResNet-101+FPN
baseline with our ResNet-50+TPN network by 1.7 AP, while operating under
similar computation budgets. This emphasizes the importance of performing
computation at the feature pyramid level in modern-day object detection
systems. Code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v3 [cs.IT] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06986">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new deep unfolding neural network based on the
ADMM algorithm for analysis Compressed Sensing. The proposed network jointly
learns a redundant analysis operator for sparsification and reconstructs the
signal of interest. We compare our proposed network with a state-of-the-art
unfolded ISTA decoder, that also learns an orthogonal sparsifier. Moreover, we
consider not only image, but also speech datasets as test examples.
Computational experiments demonstrate that our proposed network outperforms the
state-of-the-art deep unfolding network, consistently for both real-world image
and speech datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfAnFace: Bridging the infant-adult domain gap in facial landmark estimation in the wild. (arXiv:2110.08935v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08935">
<div class="article-summary-box-inner">
<span><p>We lay the groundwork for research in the algorithmic comprehension of infant
faces, in anticipation of applications from healthcare to psychology,
especially in the early prediction of developmental disorders. Specifically, we
introduce the first-ever dataset of infant faces annotated with facial landmark
coordinates and pose attributes, demonstrate the inadequacies of existing
facial landmark estimation algorithms in the infant domain, and train new
state-of-the-art models that significantly improve upon those algorithms using
domain adaptation techniques. We touch on the closely related task of facial
detection for infants, and also on a challenging case study of infrared baby
monitor images gathered by our lab as part of in-field research into the
aforementioned developmental issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Quantity in Percentage of Factory Machines Using Computer Vision and Mathematical Methods. (arXiv:2111.05080v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05080">
<div class="article-summary-box-inner">
<span><p>Computer vision has been thriving since AI development was gaining thrust.
Using deep learning techniques has been the most popular way which computer
scientists thought the solution of. However, deep learning techniques tend to
show lower performance than manual processing. Using deep learning is not
always the answer to a problem related to computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLOSH: Set LOcality Sensitive Hashing via Sliced-Wasserstein Embeddings. (arXiv:2112.05872v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05872">
<div class="article-summary-box-inner">
<span><p>Learning from set-structured data is an essential problem with many
applications in machine learning and computer vision. This paper focuses on
non-parametric and data-independent learning from set-structured data using
approximate nearest neighbor (ANN) solutions, particularly locality-sensitive
hashing. We consider the problem of set retrieval from an input set query. Such
retrieval problem requires: 1) an efficient mechanism to calculate the
distances/dissimilarities between sets, and 2) an appropriate data structure
for fast nearest neighbor search. To that end, we propose Sliced-Wasserstein
set embedding as a computationally efficient "set-2-vector" mechanism that
enables downstream ANN, with theoretical guarantees. The set elements are
treated as samples from an unknown underlying distribution, and the
Sliced-Wasserstein distance is used to compare sets. We demonstrate the
effectiveness of our algorithm, denoted as Set-LOcality Sensitive Hashing
(SLOSH), on various set retrieval datasets and compare our proposed embedding
with standard set embedding approaches, including Generalized Mean (GeM)
embedding/pooling, Featurewise Sort Pooling (FSPool), and Covariance Pooling
and show consistent improvement in retrieval results. The code for replicating
our results is available here:
\href{https://github.com/mint-vu/SLOSH}{https://github.com/mint-vu/SLOSH}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persistent Object Identification Leveraging Non-Visual Markers. (arXiv:2112.06809v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06809">
<div class="article-summary-box-inner">
<span><p>Our objective is to locate and provide a unique identifier for each mouse in
a cluttered home-cage environment through time, as a precursor to automated
behaviour recognition for biological research. This is a very challenging
problem due to (i) the lack of distinguishing visual features for each mouse,
and (ii) the close confines of the scene with constant occlusion, making
standard visual tracking approaches unusable. However, a coarse estimate of
each mouse's location is available from a unique RFID implant, so there is the
potential to optimally combine information from (weak) tracking with coarse
information on identity. To achieve our objective, we make the following key
contributions: (a) the formulation of the object identification problem as an
assignment problem (solved using Integer Linear Programming), and (b) a novel
probabilistic model of the affinity between tracklets and RFID data. The latter
is a crucial part of the model, as it provides a principled probabilistic
treatment of object detections given coarse localisation. Our approach achieves
77% accuracy on this animal identification problem, and is able to reject
spurious detections when the animals are hidden.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional generative data-free knowledge distillation. (arXiv:2112.15358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15358">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation has made remarkable achievements in model compression.
However, most existing methods demand original training data, while real data
in practice are often unavailable due to privacy, security and transmission
limitation. To address this problem, we propose a conditional generative
data-free knowledge distillation (CGDD) framework to train efficient portable
network without any real data. In this framework, except using the knowledge
extracted from teacher model, we introduce preset labels as additional
auxiliary information to train the generator. Then, the trained generator can
produce meaningful training samples of specified category as required. In order
to promote distillation process, except using conventional distillation loss,
we treat preset label as ground truth label so that student network is directly
supervised by the category of synthetic training sample. Moreover, we force
student network to mimic the attention maps of teacher model and further
improve its performance. To verify the superiority of our method, we design a
new evaluation metric is called as relative accuracy to directly compare the
effectiveness of different distillation methods. Trained portable network
learned with proposed data-free distillation method obtains 99.63%, 99.07% and
99.84% relative accuracy on CIFAR10, CIFAR100 and Caltech101, respectively. The
experimental results demonstrate the superiority of proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04584">
<div class="article-summary-box-inner">
<span><p>Automatic segmentation of lung lesions associated with COVID-19 in CT images
requires large amount of annotated volumes. Annotations mandate expert
knowledge and are time-intensive to obtain through fully manual segmentation
methods. Additionally, lung lesions have large inter-patient variations, with
some pathologies having similar visual appearance as healthy lung tissues. This
poses a challenge when applying existing semi-automatic interactive
segmentation techniques for data labelling. To address these challenges, we
propose an efficient convolutional neural networks (CNNs) that can be learned
online while the annotator provides scribble-based interaction. To accelerate
learning from only the samples labelled through user-interactions, a
patch-based approach is used for training the network. Moreover, we use
weighted cross-entropy loss to address the class imbalance that may result from
user-interactions. During online inference, the learned network is applied to
the whole input volume using a fully convolutional approach. We compare our
proposed method with state-of-the-art using synthetic scribbles and show that
it outperforms existing methods on the task of annotating lung lesions
associated with COVID-19, achieving 16% higher Dice score while reducing
execution time by 3$\times$ and requiring 9000 lesser scribbles-based labelled
voxels. Due to the online learning aspect, our approach adapts quickly to user
input, resulting in high quality segmentation labels. Source code for ECONet is
available at: https://github.com/masadcv/ECONet-MONAILabel
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network. (arXiv:2201.08954v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08954">
<div class="article-summary-box-inner">
<span><p>Synthetic aperture radar (SAR) image change detection is a vital yet
challenging task in the field of remote sensing image analysis. Most previous
works adopt a self-supervised method which uses pseudo-labeled samples to guide
subsequent training and testing. However, deep networks commonly require many
high-quality samples for parameter optimization. The noise in pseudo-labels
inevitably affects the final change detection performance. To solve the
problem, we propose a Graph-based Knowledge Supplement Network (GKSNet). To be
more specific, we extract discriminative information from the existing labeled
dataset as additional knowledge, to suppress the adverse effects of noisy
samples to some extent. Afterwards, we design a graph transfer module to
distill contextual information attentively from the labeled dataset to the
target dataset, which bridges feature correlation between datasets. To validate
the proposed method, we conducted extensive experiments on four SAR datasets,
which demonstrated the superiority of the proposed GKSNet as compared to
several state-of-the-art baselines. Our codes are available at
https://github.com/summitgao/SAR_CD_GKSNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Background Invariant Classification on Infrared Imagery by Data Efficient Training and Reducing Bias in CNNs. (arXiv:2201.09144v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09144">
<div class="article-summary-box-inner">
<span><p>Even though convolutional neural networks can classify objects in images very
accurately, it is well known that the attention of the network may not always
be on the semantically important regions of the scene. It has been observed
that networks often learn background textures which are not relevant to the
object of interest. In turn this makes the networks susceptible to variations
and changes in the background which negatively affect their performance. We
propose a new two-step training procedure called split training to reduce this
bias in CNNs on both Infrared imagery and RGB data. Our split training
procedure has two steps: using MSE loss first train the layers of the network
on images with background to match the activations of the same network when it
is trained using images without background; then with these layers frozen,
train the rest of the network with cross-entropy loss to classify the objects.
Our training method outperforms the traditional training procedure in both a
simple CNN architecture, and deep CNNs like VGG and Densenet which use lots of
hardware resources, and learns to mimic human vision which focuses more on
shape and structure than background with higher accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02832">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks have demonstrated human-level performance in
the classification of melanoma and other skin lesions, but evident performance
disparities between differing skin tones should be addressed before widespread
deployment. In this work, we utilise a modified variational autoencoder to
uncover skin tone bias in datasets commonly used as benchmarks. We propose an
efficient yet effective algorithm for automatically labelling the skin tone of
lesion images, and use this to annotate the benchmark ISIC dataset. We
subsequently use two leading bias unlearning techniques to mitigate skin tone
bias. Our experimental results provide evidence that our skin tone detection
algorithm outperforms existing solutions and that unlearning skin tone improves
generalisation and can reduce the performance disparity between melanoma
detection in lighter and darker skin tones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning for Complex Data through Ensemble-based Self-Supervised Learning. (arXiv:2202.03126v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03126">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning deals with problems that have little or no available
labeled data. Recent work has shown impressive results when underlying classes
have significant semantic differences. One important dataset in which this
technique thrives is ImageNet, as intra-class distances are substantially lower
than inter-class distances. However, this is not the case for several critical
tasks, and general self-supervised learning methods fail to learn
discriminative features when classes have closer semantics, thus requiring more
robust strategies. We propose a strategy to tackle this problem, and to enable
learning from unlabeled data even when samples from different classes are not
prominently diverse. We approach the problem by leveraging a novel
ensemble-based clustering strategy where clusters derived from different
configurations are combined to generate a better grouping for the data samples
in a fully-unsupervised way. This strategy allows clusters with different
densities and higher variability to emerge, which in turn reduces intra-class
discrepancies, without requiring the burden of finding an optimal configuration
per dataset. We also consider different Convolutional Neural Networks to
compute distances between samples. We refine these distances by performing
context analysis and group them to capture complementary information. We
consider two applications to validate our pipeline: Person Re-Identification
and Text Authorship Verification. These are challenging applications
considering that classes are semantically close to each other and that training
and test sets have disjoint identities. Our method is robust across different
modalities and outperforms state-of-the-art results with a fully-unsupervised
solution without any labeling or human intervention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSSNet: Planarity-sensible Semantic Segmentation of Large-scale Urban Meshes. (arXiv:2202.03209v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03209">
<div class="article-summary-box-inner">
<span><p>We introduce a novel deep learning-based framework to interpret 3D urban
scenes represented as textured meshes. Based on the observation that object
boundaries typically align with the boundaries of planar regions, our framework
achieves semantic segmentation in two steps: planarity-sensible
over-segmentation followed by semantic classification. The over-segmentation
step generates an initial set of mesh segments that capture the planar and
non-planar regions of urban scenes. In the subsequent classification step, we
construct a graph that encodes geometric and photometric features of the
segments in its nodes and multi-scale contextual features in its edges. The
final semantic segmentation is obtained by classifying the segments using a
graph convolutional network. Experiments and comparisons on a large semantic
urban mesh benchmark demonstrate that our approach outperforms the
state-of-the-art methods in terms of boundary quality and mean IoU
(intersection over union). Besides, we also introduce several new metrics for
evaluating mesh over-segmentation methods dedicated for semantic segmentation,
and our proposed over-segmentation approach outperforms state-of-the-art
methods on all metrics. Our source code will be released when the paper is
accepted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fair SA: Sensitivity Analysis for Fairness in Face Recognition. (arXiv:2202.03586v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03586">
<div class="article-summary-box-inner">
<span><p>As the use of deep learning in high impact domains becomes ubiquitous, it is
increasingly important to assess the resilience of models. One such high impact
domain is that of face recognition, with real world applications involving
images affected by various degradations, such as motion blur or high exposure.
Moreover, images captured across different attributes, such as gender and race,
can also challenge the robustness of a face recognition algorithm. While
traditional summary statistics suggest that the aggregate performance of face
recognition models has continued to improve, these metrics do not directly
measure the robustness or fairness of the models. Visual Psychophysics
Sensitivity Analysis (VPSA) [1] provides a way to pinpoint the individual
causes of failure by way of introducing incremental perturbations in the data.
However, perturbations may affect subgroups differently. In this paper, we
propose a new fairness evaluation based on robustness in the form of a generic
framework that extends VPSA. With this framework, we can analyze the ability of
a model to perform fairly for different subgroups of a population affected by
perturbations, and pinpoint the exact failure modes for a subgroup by measuring
targeted robustness. With the increasing focus on the fairness of models, we
use face recognition as an example application of our framework and propose to
compactly visualize the fairness analysis of a model via AUC matrices. We
analyze the performance of common face recognition models and empirically show
that certain subgroups are at a disadvantage when images are perturbed, thereby
uncovering trends that were not visible using the model's performance on
subgroups without perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Understand Masked Autoencoders. (arXiv:2202.03670v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03670">
<div class="article-summary-box-inner">
<span><p>"Masked Autoencoders (MAE) Are Scalable Vision Learners" revolutionizes the
self-supervised learning method in that it not only achieves the
state-of-the-art for image pre-training, but is also a milestone that bridges
the gap between visual and linguistic masked autoencoding (BERT-style)
pre-trainings. However, to our knowledge, to date there are no theoretical
perspectives to explain the powerful expressivity of MAE. In this paper, we,
for the first time, propose a unified theoretical framework that provides a
mathematical understanding for MAE. Specifically, we explain the patch-based
attention approaches of MAE using an integral kernel under a non-overlapping
domain decomposition setting. To help the research community to further
comprehend the main reasons of the great success of MAE, based on our
framework, we pose five questions and answer them with mathematical rigor using
insights from operator theory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-Relational Domain Adaptation. (arXiv:2202.03628v1 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03628">
<div class="article-summary-box-inner">
<span><p>Existing domain adaptation methods tend to treat every domain equally and
align them all perfectly. Such uniform alignment ignores topological structures
among different domains; therefore it may be beneficial for nearby domains, but
not necessarily for distant domains. In this work, we relax such uniform
alignment by using a domain graph to encode domain adjacency, e.g., a graph of
states in the US with each state as a domain and each edge indicating
adjacency, thereby allowing domains to align flexibly based on the graph
structure. We generalize the existing adversarial learning framework with a
novel graph discriminator using encoding-conditioned graph embeddings.
Theoretical analysis shows that at equilibrium, our method recovers classic
domain adaptation when the graph is a clique, and achieves non-trivial
alignment for other types of graphs. Empirical results show that our approach
successfully generalizes uniform alignment, naturally incorporates domain
information represented by graphs, and improves upon existing domain adaptation
methods on both synthetic and real-world datasets. Code will soon be available
at https://github.com/Wang-ML-Lab/GRDA.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-10 23:07:29.297423062 UTC">2022-02-10 23:07:29 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>