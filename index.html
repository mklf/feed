<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-14T01:30:00Z">02-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs. (arXiv:2202.05331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05331">
<div class="article-summary-box-inner">
<span><p>Several services for people with visual disabilities have emerged recently
due to achievements in Assistive Technologies and Artificial Intelligence
areas. Despite the growth in assistive systems availability, there is a lack of
services that support specific tasks, such as understanding the image context
presented in online content, e.g., webinars. Image captioning techniques and
their variants are limited as Assistive Technologies as they do not match the
needs of visually impaired people when generating specific descriptions. We
propose an approach for generating context of webinar images combining a dense
captioning technique with a set of filters, to fit the captions in our domain,
and a language model for the abstractive summary task. The results demonstrated
that we can produce descriptions with higher interpretability and focused on
the relevant information for that group of people by combining image analysis
methods and neural language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Including Facial Expressions in Contextual Embeddings for Sign Language Generation. (arXiv:2202.05383v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05383">
<div class="article-summary-box-inner">
<span><p>State-of-the-art sign language generation frameworks lack expressivity and
naturalness which is the result of only focusing manual signs, neglecting the
affective, grammatical and semantic functions of facial expressions. The
purpose of this work is to augment semantic representation of sign language
through grounding facial expressions. We study the effect of modeling the
relationship between text, gloss, and facial expressions on the performance of
the sign generation systems. In particular, we propose a Dual Encoder
Transformer able to generate manual signs as well as facial expressions by
capturing the similarities and differences found in text and sign gloss
annotation. We take into consideration the role of facial muscle activity to
express intensities of manual signs by being the first to employ facial action
units in sign language generation. We perform a series of experiments showing
that our proposed model improves the quality of automatically generated sign
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing ASR for Stuttered Speech with Limited Data Using Detect and Pass. (arXiv:2202.05396v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05396">
<div class="article-summary-box-inner">
<span><p>It is estimated that around 70 million people worldwide are affected by a
speech disorder called stuttering. With recent advances in Automatic Speech
Recognition (ASR), voice assistants are increasingly useful in our everyday
lives. Many technologies in education, retail, telecommunication and healthcare
can now be operated through voice. Unfortunately, these benefits are not
accessible for People Who Stutter (PWS). We propose a simple but effective
method called 'Detect and Pass' to make modern ASR systems accessible for
People Who Stutter in a limited data setting. The algorithm uses a context
aware classifier trained on a limited amount of data, to detect acoustic frames
that contain stutter. To improve robustness on stuttered speech, this extra
information is passed on to the ASR model to be utilized during inference. Our
experiments show a reduction of 12.18% to 71.24% in Word Error Rate (WER)
across various state of the art ASR systems. Upon varying the threshold of the
associated posterior probability of stutter for each stacked frame used in
determining low frame rate (LFR) acoustic features, we were able to determine
an optimal setting that reduced the WER by 23.93% to 71.67% across different
ASR systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Task Framework for Debiasing Persona-grounded Dialogue Dataset. (arXiv:2202.05435v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05435">
<div class="article-summary-box-inner">
<span><p>This paper introduces a simple yet effective data-centric approach for the
task of improving persona-conditioned dialogue agents. Prior model-centric
approaches unquestioningly depend on the raw crowdsourced benchmark datasets
such as Persona-Chat. In contrast, we aim to fix annotation artifacts in
benchmarking, which is orthogonally applicable to any dialogue model.
Specifically, we augment relevant personas to improve dialogue dataset/agent,
by leveraging the primal-dual structure of the two tasks, predicting dialogue
responses and personas based on each other. Experiments on Persona-Chat show
that our approach outperforms pre-trained LMs by an 11.7 point gain in terms of
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACORT: A Compact Object Relation Transformer for Parameter Efficient Image Captioning. (arXiv:2202.05451v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05451">
<div class="article-summary-box-inner">
<span><p>Recent research that applies Transformer-based architectures to image
captioning has resulted in state-of-the-art image captioning performance,
capitalising on the success of Transformers on natural language tasks.
Unfortunately, though these models work well, one major flaw is their large
model sizes. To this end, we present three parameter reduction methods for
image captioning Transformers: Radix Encoding, cross-layer parameter sharing,
and attention parameter sharing. By combining these methods, our proposed ACORT
models have 3.7x to 21.6x fewer parameters than the baseline model without
compromising test performance. Results on the MS-COCO dataset demonstrate that
our ACORT models are competitive against baselines and SOTA approaches, with
CIDEr score &gt;=126. Finally, we present qualitative results and ablation studies
to demonstrate the efficacy of the proposed changes further. Code and
pre-trained models are publicly available at
https://github.com/jiahuei/sparse-image-captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hindi/Bengali Sentiment Analysis Using Transfer Learning and Joint Dual Input Learning with Self Attention. (arXiv:2202.05457v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05457">
<div class="article-summary-box-inner">
<span><p>Sentiment Analysis typically refers to using natural language processing,
text analysis and computational linguistics to extract affect and emotion based
information from text data. Our work explores how we can effectively use deep
neural networks in transfer learning and joint dual input learning settings to
effectively classify sentiments and detect hate speech in Hindi and Bengali
data. We start by training Word2Vec word embeddings for Hindi \textbf{HASOC
dataset} and Bengali hate speech and then train LSTM and subsequently, employ
parameter sharing based transfer learning to Bengali sentiment classifiers by
reusing and fine-tuning the trained weights of Hindi classifiers with both
classifier being used as baseline in our study. Finally, we use BiLSTM with
self attention in joint dual input learning setting where we train a single
neural network on Hindi and Bengali dataset simultaneously using their
respective embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer. (arXiv:2202.05508v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05508">
<div class="article-summary-box-inner">
<span><p>Text spotting end-to-end methods have recently gained attention in the
literature due to the benefits of jointly optimizing the text detection and
recognition components. Existing methods usually have a distinct separation
between the detection and recognition branches, requiring exact annotations for
the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach
for text spotting and the first text spotting framework which may be trained
with both fully- and weakly-supervised settings. By learning a single latent
representation per word detection, and using a novel loss function based on the
Hungarian loss, our method alleviates the need for expensive localization
annotations. Trained with only text transcription annotations on real data, our
weakly-supervised method achieves competitive performance with previous
state-of-the-art fully-supervised methods. When trained in a fully-supervised
manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks
\footnote {Our code will be publicly available upon publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Does it Mean for a Language Model to Preserve Privacy?. (arXiv:2202.05520v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05520">
<div class="article-summary-box-inner">
<span><p>Natural language reflects our private lives and identities, making its
privacy concerns as broad as those of real life. Language models lack the
ability to understand the context and sensitivity of text, and tend to memorize
phrases present in their training sets. An adversary can exploit this tendency
to extract training data. Depending on the nature of the content and the
context in which this data was collected, this could violate expectations of
privacy. Thus there is a growing interest in techniques for training language
models that preserve privacy. In this paper, we discuss the mismatch between
the narrow assumptions made by popular data protection techniques (data
sanitization and differential privacy), and the broadness of natural language
and of privacy as a social norm. We argue that existing protection methods
cannot guarantee a generic and meaningful notion of privacy for language
models. We conclude that language models should be trained on text data which
was explicitly produced for public use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization. (arXiv:2202.05599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05599">
<div class="article-summary-box-inner">
<span><p>We present ClidSum, a benchmark dataset for building cross-lingual
summarization systems on dialogue documents. It consists of 67k+ dialogue
documents from two subsets (i.e., SAMSum and MediaSum) and 112k+ annotated
summaries in different target languages. Based on the proposed ClidSum, we
introduce two benchmark settings for supervised and semi-supervised scenarios,
respectively. We then build various baseline systems in different paradigms
(pipeline and end-to-end) and conduct extensive experiments on ClidSum to
provide deeper analyses. Furthermore, we propose mDialBART which extends
mBART-50 (a multi-lingual BART) via further pre-training. The multiple
objectives used in the further pre-training stage help the pre-trained model
capture the structural characteristics as well as important content in
dialogues and the transformation from source to the target language.
Experimental results show the superiority of mDialBART, as an end-to-end model,
outperforms strong pipeline models on ClidSum. Finally, we discuss specific
challenges that current approaches faced with this task and give multiple
promising directions for future research. We have released the dataset and code
at https://github.com/krystalan/ClidSum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenderedNews: Une approche computationnelle des \'ecarts de repr\'esentation des genres dans la presse fran\c{c}aise. (arXiv:2202.05682v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05682">
<div class="article-summary-box-inner">
<span><p>In this article, we present GenderedNews (https://gendered-news.imag.fr), an
online dashboard which gives weekly measures of gender imbalance in French
online press. We use Natural Language Processing (NLP) methods to quantify
gender inequalities in the media, in the wake of global projects like the
Global Media Monitoring Project. Such projects are instrumental in highlighting
gender imbalance in the media and its very slow evolution. However, their
generalisation is limited by their sampling and cost in terms of time, data and
staff. Automation allows us to offer complementary measures to quantify
inequalities in gender representation. We understand representation as the
presence and distribution of men and women mentioned and quoted in the news --
as opposed to representation as stereotypification. In this paper, we first
review different means adopted by previous studies on gender inequality in the
media : qualitative content analysis, quantitative content analysis and
computational methods. We then detail the methods adopted by {\it GenderedNews}
and the two metrics implemented: the masculinity rate of mentions and the
proportion of men quoted in online news. We describe the data collected daily
(seven main titles of French online news media) and the methodology behind our
metrics, as well as a few visualisations. We finally propose to illustrate
possible analysis of our data by conducting an in-depth observation of a sample
of two months of our database.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HaT5: Hate Language Identification using Text-to-Text Transfer Transformer. (arXiv:2202.05690v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05690">
<div class="article-summary-box-inner">
<span><p>We investigate the performance of a state-of-the art (SoTA) architecture T5
(available on the SuperGLUE) and compare with it 3 other previous SoTA
architectures across 5 different tasks from 2 relatively diverse datasets. The
datasets are diverse in terms of the number and types of tasks they have. To
improve performance, we augment the training data by using an autoregressive
model. We achieve near-SoTA results on a couple of the tasks - macro F1 scores
of 81.66% for task A of the OLID 2019 dataset and 82.54% for task A of the hate
speech and offensive content (HASOC) 2021 dataset, where SoTA are 82.9% and
83.05%, respectively. We perform error analysis and explain why one of the
models (Bi-LSTM) makes the predictions it does by using a publicly available
algorithm: Integrated Gradient (IG). This is because explainable artificial
intelligence (XAI) is essential for earning the trust of users. The main
contributions of this work are the implementation method of T5, which is
discussed; the data augmentation using a new conversational AI model
checkpoint, which brought performance improvements; and the revelation on the
shortcomings of HASOC 2021 dataset. It reveals the difficulties of poor data
annotation by using a small set of examples where the T5 model made the correct
predictions, even when the ground truth of the test set were incorrect (in our
opinion). We also provide our model checkpoints on the HuggingFace hub1 to
foster transparency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained Optimization with Dynamic Bound-scaling for Effective NLPBackdoor Defense. (arXiv:2202.05749v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05749">
<div class="article-summary-box-inner">
<span><p>We develop a novel optimization method for NLPbackdoor inversion. We leverage
a dynamically reducing temperature coefficient in the softmax function to
provide changing loss landscapes to the optimizer such that the process
gradually focuses on the ground truth trigger, which is denoted as a one-hot
value in a convex hull. Our method also features a temperature rollback
mechanism to step away from local optimals, exploiting the observation that
local optimals can be easily deter-mined in NLP trigger inversion (while not in
general optimization). We evaluate the technique on over 1600 models (with
roughly half of them having injected backdoors) on 3 prevailing NLP tasks, with
4 different backdoor attacks and 7 architectures. Our results show that the
technique is able to effectively and efficiently detect and remove backdoors,
outperforming 4 baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Random Perturbations to Mitigate Adversarial Attacks on Sentiment Analysis Models. (arXiv:2202.05758v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05758">
<div class="article-summary-box-inner">
<span><p>Attacks on deep learning models are often difficult to identify and therefore
are difficult to protect against. This problem is exacerbated by the use of
public datasets that typically are not manually inspected before use. In this
paper, we offer a solution to this vulnerability by using, during testing,
random perturbations such as spelling correction if necessary, substitution by
random synonym, or simply dropping the word. These perturbations are applied to
random words in random sentences to defend NLP models against adversarial
attacks. Our Random Perturbations Defense and Increased Randomness Defense
methods are successful in returning attacked models to similar accuracy of
models before attacks. The original accuracy of the model used in this work is
80% for sentiment classification. After undergoing attacks, the accuracy drops
to accuracy between 0% and 44%. After applying our defense methods, the
accuracy of the model is returned to the original accuracy within statistical
significance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">White-Box Attacks on Hate-speech BERT Classifiers in German with Explicit and Implicit Character Level Defense. (arXiv:2202.05778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05778">
<div class="article-summary-box-inner">
<span><p>In this work, we evaluate the adversarial robustness of BERT models trained
on German Hate Speech datasets. We also complement our evaluation with two
novel white-box character and word level attacks thereby contributing to the
range of attacks available. Furthermore, we also perform a comparison of two
novel character-level defense strategies and evaluate their robustness with one
another.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Knowledge Graph Construction and Application: A Survey. (arXiv:2202.05786v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05786">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the resurgence of knowledge engineering which is
featured by the fast growth of knowledge graphs. However, most of existing
knowledge graphs are represented with pure symbols, which hurts the machine's
capability to understand the real world. The multi-modalization of knowledge
graphs is an inevitable key step towards the realization of human-level machine
intelligence. The results of this endeavor are Multi-modal Knowledge Graphs
(MMKGs). In this survey on MMKGs constructed by texts and images, we first give
definitions of MMKGs, followed with the preliminaries on multi-modal tasks and
techniques. We then systematically review the challenges, progresses and
opportunities on the construction and application of MMKGs respectively, with
detailed analyses of the strength and weakness of different solutions. We
finalize this survey with open research problems relevant to MMKGs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating MT Systems: A Theoretical Framework. (arXiv:2202.05806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05806">
<div class="article-summary-box-inner">
<span><p>This paper outlines a theoretical framework using which different automatic
metrics can be designed for evaluation of Machine Translation systems. It
introduces the concept of {\em cognitive ease} which depends on {\em adequacy}
and {\em lack of fluency}. Thus, cognitive ease becomes the main parameter to
be measured rather than comprehensibility. The framework allows the components
of cognitive ease to be broken up and computed based on different linguistic
levels etc. Independence of dimensions and linearly combining them provides for
a highly modular approach.
</p>
<p>The paper places the existing automatic methods in an overall framework, to
understand them better and to improve upon them in future. It can also be used
to evaluate the newer types of MT systems, such as speech to speech translation
and discourse translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-hoc Interpretability for Neural NLP: A Survey. (arXiv:2108.04840v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04840">
<div class="article-summary-box-inner">
<span><p>Neural networks for NLP are becoming increasingly complex and widespread, and
there is a growing concern if these models are responsible to use. Explaining
models helps to address the safety and ethical concerns and is essential for
accountability. Interpretability serves to provide these explanations in terms
that are understandable to humans. Additionally, post-hoc methods provide
explanations after a model is learned and are generally model-agnostic. This
survey provides a categorization of how recent post-hoc interpretability
methods communicate explanations to humans, it discusses each method in-depth,
and how they are validated, as the latter is often a common concern.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters. (arXiv:2108.08103v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08103">
<div class="article-summary-box-inner">
<span><p>The open-access dissemination of pretrained language models through online
repositories has led to a democratization of state-of-the-art natural language
processing (NLP) research. This also allows people outside of NLP to use such
models and adapt them to specific use-cases. However, a certain amount of
technical proficiency is still required which is an entry barrier for users who
want to apply these models to a certain task but lack the necessary knowledge
or resources. In this work, we aim to overcome this gap by providing a tool
which allows researchers to leverage pretrained models without writing a single
line of code. Built upon the parameter-efficient adapter modules for transfer
learning, our AdapterHub Playground provides an intuitive interface, allowing
the usage of adapters for prediction, training and analysis of textual data for
a variety of NLP tasks. We present the tool's architecture and demonstrate its
advantages with prototypical use-cases, where we show that predictive
performance can easily be increased in a few-shot learning scenario. Finally,
we evaluate its usability in a user study. We provide the code and a live
interface at https://adapter-hub.github.io/playground.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13161">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have contributed significantly to
natural language processing by demonstrating remarkable abilities as few-shot
learners. However, their effectiveness depends mainly on scaling the model
parameters and prompt design, hindering their implementation in most real-world
applications. This study proposes a novel pluggable, extensible, and efficient
approach named DifferentiAble pRompT (DART), which can convert small language
models into better few-shot learners without any prompt engineering. The main
principle behind this approach involves reformulating potential natural
language processing tasks into the task of a pre-trained language model and
differentially optimizing the prompt template as well as the target label with
backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any
pre-trained language models; (ii) Extended to widespread classification tasks.
A comprehensive evaluation of standard NLP tasks demonstrates that the proposed
approach achieves a better few-shot performance. Code is available in
https://github.com/zjunlp/DART.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Unsupervised and Text Augmented Semi-Supervised Learning for Low Resourced Autoregressive Speech Recognition. (arXiv:2110.15836v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15836">
<div class="article-summary-box-inner">
<span><p>Recent advances in unsupervised representation learning have demonstrated the
impact of pretraining on large amounts of read speech. We adapt these
techniques for domain adaptation in low-resource -- both in terms of data and
compute -- conversational and broadcast domains. Moving beyond CTC, we pretrain
state-of-the-art Conformer models in an unsupervised manner. While the
unsupervised approach outperforms traditional semi-supervised training, the
techniques are complementary. Combining the techniques is a 5% absolute
improvement in WER, averaged over all conditions, compared to semi-supervised
training alone. Additional text data is incorporated through external language
models. By using CTC-based decoding, we are better able to take advantage of
the additional text data. When used as a transcription model, it allows the
Conformer model to better incorporate the knowledge from the language model
through semi-supervised training than shallow fusion. Final performance is an
additional 2% better absolute when using CTC-based decoding for semi-supervised
training compared to shallow fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TiltedBERT: Resource Adjustable Version of BERT. (arXiv:2201.03327v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03327">
<div class="article-summary-box-inner">
<span><p>In this paper, we proposed a novel adjustable finetuning method that improves
the training and inference time of the BERT model on downstream tasks. In the
proposed method, we first detect more important word vectors in each layer by
our proposed redundancy metric and then eliminate the less important word
vectors with our proposed strategy. In our method, the word vector elimination
rate in each layer is controlled by the Tilt-Rate hyper-parameter, and the
model learns to work with a considerably lower number of Floating Point
Operations (FLOPs) than the original BERTbase model. Our proposed method does
not need any extra training steps, and also it can be generalized to other
transformer-based models. We perform extensive experiments that show the word
vectors in higher layers have an impressive amount of redundancy that can be
eliminated and decrease the training and inference time. Experimental results
on extensive sentiment analysis, classification and regression datasets, and
benchmarks like IMDB and GLUE showed that our proposed method is effective in
various datasets. By applying our method on the BERTbase model, we decrease the
inference time up to 5.3 times with less than 0.85% accuracy degradation on
average. After the fine-tuning stage, the inference time of our model can be
adjusted with our method offline-tuning property for a wide range of the
Tilt-Rate value selections. Also, we propose a mathematical speedup analysis
that can estimate the speedup of our method accurately. With the help of this
analysis, the proper Tilt-Rate value can be selected before fine-tuning or
while offline-tuning stages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Knowledge Integration in Language Models with Graph Convolutions. (arXiv:2202.00964v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00964">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (LMs) do not capture factual knowledge very well.
This has led to the development of a number of knowledge integration (KI)
methods which aim to incorporate external knowledge into pretrained LMs. Even
though KI methods show some performance gains over vanilla LMs, the
inner-workings of these methods are not well-understood. For instance, it is
unclear how and what kind of knowledge is effectively integrated into these
models and if such integration may lead to catastrophic forgetting of already
learned knowledge. This paper revisits the KI process in these models with an
information-theoretic view and shows that KI can be interpreted using a graph
convolution operation. We propose a probe model called \textit{Graph
Convolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and
exposing what kind of knowledge is integrated into these models. We conduct
experiments to verify that our GCS can indeed be used to correctly interpret
the KI process, and we use it to analyze two well-known knowledge-enhanced LMs:
ERNIE and K-Adapter, and find that only a small amount of factual knowledge is
integrated in them. We stratify knowledge in terms of various relation types
and find that ERNIE and K-Adapter integrate different kinds of knowledge to
different extent. Our analysis also shows that simply increasing the size of
the KI corpus may not lead to better KI; fundamental advances may be needed.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">On Real-time Image Reconstruction with Neural Networks for MRI-guided Radiotherapy. (arXiv:2202.05267v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05267">
<div class="article-summary-box-inner">
<span><p>MRI-guidance techniques that dynamically adapt radiation beams to follow
tumor motion in real-time will lead to more accurate cancer treatments and
reduced collateral healthy tissue damage. The gold-standard for reconstruction
of undersampled MR data is compressed sensing (CS) which is computationally
slow and limits the rate that images can be available for real-time adaptation.
Here, we demonstrate the use of automated transform by manifold approximation
(AUTOMAP), a generalized framework that maps raw MR signal to the target image
domain, to rapidly reconstruct images from undersampled radial k-space data.
The AUTOMAP neural network was trained to reconstruct images from a
golden-angle radial acquisition, a benchmark for motion-sensitive imaging, on
lung cancer patient data and generic images from ImageNet. Model training was
subsequently augmented with motion-encoded k-space data derived from videos in
the YouTube-8M dataset to encourage motion robust reconstruction. We find that
AUTOMAP-reconstructed radial k-space has equivalent accuracy to CS but with
much shorter processing times after initial fine-tuning on retrospectively
acquired lung cancer patient data. Validation of motion-trained models with a
virtual dynamic lung tumor phantom showed that the generalized motion
properties learned from YouTube lead to improved target tracking accuracy. Our
work shows that AUTOMAP can achieve real-time, accurate reconstruction of
radial data. These findings imply that neural-network-based reconstruction is
potentially superior to existing approaches for real-time image guidance
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HNF-Netv2 for Brain Tumor Segmentation using multi-modal MR Imaging. (arXiv:2202.05268v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05268">
<div class="article-summary-box-inner">
<span><p>In our previous work, $i.e.$, HNF-Net, high-resolution feature representation
and light-weight non-local self-attention mechanism are exploited for brain
tumor segmentation using multi-modal MR imaging. In this paper, we extend our
HNF-Net to HNF-Netv2 by adding inter-scale and intra-scale semantic
discrimination enhancing blocks to further exploit global semantic
discrimination for the obtained high-resolution features. We trained and
evaluated our HNF-Netv2 on the multi-modal Brain Tumor Segmentation Challenge
(BraTS) 2021 dataset. The result on the test set shows that our HNF-Netv2
achieved the average Dice scores of 0.878514, 0.872985, and 0.924919, as well
as the Hausdorff distances ($95\%$) of 8.9184, 16.2530, and 4.4895 for the
enhancing tumor, tumor core, and whole tumor, respectively. Our method won the
RSNA 2021 Brain Tumor AI Challenge Prize (Segmentation Task), which ranks 8th
out of all 1250 submitted results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Plug-and-Play Approach to Multiparametric Quantitative MRI: Image Reconstruction using Pre-Trained Deep Denoisers. (arXiv:2202.05269v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05269">
<div class="article-summary-box-inner">
<span><p>Current spatiotemporal deep learning approaches to Magnetic Resonance
Fingerprinting (MRF) build artefact-removal models customised to a particular
k-space subsampling pattern which is used for fast (compressed) acquisition.
This may not be useful when the acquisition process is unknown during training
of the deep learning model and/or changes during testing time. This paper
proposes an iterative deep learning plug-and-play reconstruction approach to
MRF which is adaptive to the forward acquisition process. Spatiotemporal image
priors are learned by an image denoiser i.e. a Convolutional Neural Network
(CNN), trained to remove generic white gaussian noise (not a particular
subsampling artefact) from data. This CNN denoiser is then used as a
data-driven shrinkage operator within the iterative reconstruction algorithm.
This algorithm with the same denoiser model is then tested on two simulated
acquisition processes with distinct subsampling patterns. The results show
consistent de-aliasing performance against both acquisition schemes and
accurate mapping of tissues' quantitative bio-properties. Software available:
https://github.com/ketanfatania/QMRI-PnP-Recon-POC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Approach for Digital ColorReconstruction of Lenticular Films. (arXiv:2202.05270v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05270">
<div class="article-summary-box-inner">
<span><p>We propose the first accurate digitization and color reconstruction process
for historical lenticular film that is robust to artifacts. Lenticular films
emerged in the 1920s and were one of the first technologies that permitted to
capture full color information in motion. The technology leverages an RGB
filter and cylindrical lenticules embossed on the film surface to encode the
color in the horizontal spatial dimension of the image. To project the pictures
the encoding process was reversed using an appropriate analog device. In this
work, we introduce an automated, fully digital pipeline to process the scan of
lenticular films and colorize the image. Our method merges deep learning with a
model-based approach in order to maximize the performance while making sure
that the reconstructed colored images truthfully match the encoded color
information. Our model employs different strategies to achieve an effective
color reconstruction, in particular (i) we use data augmentation to create a
robust lenticule segmentation network, (ii) we fit the lenticules raster
prediction to obtain a precise vectorial lenticule localization, and (iii) we
train a colorization network that predicts interpolation coefficients in order
to obtain a truthful colorization. We validate the proposed method on a
lenticular film dataset and compare it to other approaches. Since no colored
groundtruth is available as reference, we conduct a user study to validate our
method in a subjective manner. The results of the study show that the proposed
method is largely preferred with respect to other existing and baseline
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Field of Experts Prior for Adapting Neural Networks at Test Time. (arXiv:2202.05271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05271">
<div class="article-summary-box-inner">
<span><p>Performance of convolutional neural networks (CNNs) in image analysis tasks
is often marred in the presence of acquisition-related distribution shifts
between training and test images. Recently, it has been proposed to tackle this
problem by fine-tuning trained CNNs for each test image. Such
test-time-adaptation (TTA) is a promising and practical strategy for improving
robustness to distribution shifts as it requires neither data sharing between
institutions nor annotating additional data. Previous TTA methods use a helper
model to increase similarity between outputs and/or features extracted from a
test image with those of the training images. Such helpers, which are typically
modeled using CNNs, can be task-specific and themselves vulnerable to
distribution shifts in their inputs. To overcome these problems, we propose to
carry out TTA by matching the feature distributions of test and training
images, as modelled by a field-of-experts (FoE) prior. FoEs model complicated
probability distributions as products of many simpler expert distributions. We
use 1D marginal distributions of a trained task CNN's features as experts in
the FoE model. Further, we compute principal components of patches of the task
CNN's features, and consider the distributions of PCA loadings as additional
experts. We validate the method on 5 MRI segmentation tasks (healthy tissues in
4 anatomical regions and lesions in 1 one anatomy), using data from 17 clinics,
and on a MRI registration task, using data from 3 clinics. We find that the
proposed FoE-based TTA is generically applicable in multiple tasks, and
outperforms all previous TTA methods for lesion segmentation. For healthy
tissue segmentation, the proposed method outperforms other task-agnostic
methods, but a previous TTA method which is specifically designed for
segmentation performs the best for most of the tested datasets. Our code is
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Guideline for Evaluation Metrics in Medical Image Segmentation. (arXiv:2202.05273v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05273">
<div class="article-summary-box-inner">
<span><p>In the last decade, research on artificial intelligence has seen rapid growth
with deep learning models, especially in the field of medical image
segmentation. Various studies demonstrated that these models have powerful
prediction capabilities and achieved similar results as clinicians. However,
recent studies revealed that the evaluation in image segmentation studies lacks
reliable model performance assessment and showed statistical bias by incorrect
metric implementation or usage. Thus, this work provides an overview and
interpretation guide on the following metrics for medical image segmentation
evaluation in binary as well as multi-class problems: Dice similarity
coefficient, Jaccard, Sensitivity, Specificity, Rand index, ROC curves, Cohen's
Kappa, and Hausdorff distance. As a summary, we propose a guideline for
standardized medical image segmentation evaluation to improve evaluation
quality, reproducibility, and comparability in the research field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion Puzzle: Arbitrary Motion Style Transfer by Body Part. (arXiv:2202.05274v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05274">
<div class="article-summary-box-inner">
<span><p>This paper presents Motion Puzzle, a novel motion style transfer network that
advances the state-of-the-art in several important respects. The Motion Puzzle
is the first that can control the motion style of individual body parts,
allowing for local style editing and significantly increasing the range of
stylized motions. Designed to keep the human's kinematic structure, our
framework extracts style features from multiple style motions for different
body parts and transfers them locally to the target body parts. Another major
advantage is that it can transfer both global and local traits of motion style
by integrating the adaptive instance normalization and attention modules while
keeping the skeleton topology. Thus, it can capture styles exhibited by dynamic
movements, such as flapping and staggering, significantly better than previous
work. In addition, our framework allows for arbitrary motion style transfer
without datasets with style labeling or motion pairing, making many publicly
available motion datasets available for training. Our framework can be easily
integrated with motion generation frameworks to create many applications, such
as real-time motion transfer. We demonstrate the advantages of our framework
with a number of examples and comparisons with previous work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Beneath the Ink: Synthetic Data and Tattoo Removal with Application to Face Recognition. (arXiv:2202.05297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05297">
<div class="article-summary-box-inner">
<span><p>Systems that analyse faces have seen significant improvements in recent years
and are today used in numerous application scenarios. However, these systems
have been found to be negatively affected by facial alterations such as
tattoos. To better understand and mitigate the effect of facial tattoos in
facial analysis systems, large datasets of images of individuals with and
without tattoos are needed. To this end, we propose a generator for
automatically adding realistic tattoos to facial images. Moreover, we
demonstrate the feasibility of the generation by training a deep learning-based
model for removing tattoos from face images. The experimental results show that
it is possible to remove facial tattoos from real images without degrading the
quality of the image. Additionally, we show that it is possible to improve face
recognition accuracy by using the proposed deep learning-based tattoo removal
before extracting and comparing facial features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks. (arXiv:2202.05306v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05306">
<div class="article-summary-box-inner">
<span><p>We hypothesize that due to the greedy nature of learning in multi-modal deep
neural networks, these models tend to rely on just one modality while
under-fitting the other modalities. Such behavior is counter-intuitive and
hurts the models' generalization, as we observe empirically. To estimate the
model's dependence on each modality, we compute the gain on the accuracy when
the model has access to it in addition to another modality. We refer to this
gain as the conditional utilization rate. In the experiments, we consistently
observe an imbalance in conditional utilization rates between modalities,
across multiple tasks and architectures. Since conditional utilization rate
cannot be computed efficiently during training, we introduce a proxy for it
based on the pace at which the model learns from each modality, which we refer
to as the conditional learning speed. We propose an algorithm to balance the
conditional learning speeds between modalities during training and demonstrate
that it indeed addresses the issue of greedy learning. The proposed algorithm
improves the model's generalization on three datasets: Colored MNIST, Princeton
ModelNet40, and NVIDIA Dynamic Hand Gesture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining the manifolds of deep generative models for multiple data-consistent solutions of ill-posed tomographic imaging problems. (arXiv:2202.05311v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05311">
<div class="article-summary-box-inner">
<span><p>Tomographic imaging is in general an ill-posed inverse problem. Typically, a
single regularized image estimate of the sought-after object is obtained from
tomographic measurements. However, there may be multiple objects that are all
consistent with the same measurement data. The ability to generate such
alternate solutions is important because it may enable new assessments of
imaging systems. In principle, this can be achieved by means of posterior
sampling methods. In recent years, deep neural networks have been employed for
posterior sampling with promising results. However, such methods are not yet
for use with large-scale tomographic imaging applications. On the other hand,
empirical sampling methods may be computationally feasible for large-scale
imaging systems and enable uncertainty quantification for practical
applications. Empirical sampling involves solving a regularized inverse problem
within a stochastic optimization framework in order to obtain alternate
data-consistent solutions. In this work, we propose a new empirical sampling
method that computes multiple solutions of a tomographic inverse problem that
are consistent with the same acquired measurement data. The method operates by
repeatedly solving an optimization problem in the latent space of a style-based
generative adversarial network (StyleGAN), and was inspired by the Photo
Upsampling via Latent Space Exploration (PULSE) method that was developed for
super-resolution tasks. The proposed method is demonstrated and analyzed via
numerical studies that involve two stylized tomographic imaging modalities.
These studies establish the ability of the method to perform efficient
empirical sampling and uncertainty quantification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs. (arXiv:2202.05331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05331">
<div class="article-summary-box-inner">
<span><p>Several services for people with visual disabilities have emerged recently
due to achievements in Assistive Technologies and Artificial Intelligence
areas. Despite the growth in assistive systems availability, there is a lack of
services that support specific tasks, such as understanding the image context
presented in online content, e.g., webinars. Image captioning techniques and
their variants are limited as Assistive Technologies as they do not match the
needs of visually impaired people when generating specific descriptions. We
propose an approach for generating context of webinar images combining a dense
captioning technique with a set of filters, to fit the captions in our domain,
and a language model for the abstractive summary task. The results demonstrated
that we can produce descriptions with higher interpretability and focused on
the relevant information for that group of people by combining image analysis
methods and neural language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Pedestrian-Vehicle Interaction for Pedestrian Trajectory Prediction. (arXiv:2202.05334v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05334">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the interaction between pedestrians and vehicles and
propose a novel neural network structure called the Pedestrian-Vehicle
Interaction (PVI) extractor for learning the pedestrian-vehicle interaction. We
implement the proposed PVI extractor on both sequential approaches (long
short-term memory (LSTM) models) and non-sequential approaches (convolutional
models). We use the Waymo Open Dataset that contains real-world urban traffic
scenes with both pedestrian and vehicle annotations. For the LSTM-based models,
our proposed model is compared with Social-LSTM and Social-GAN, and using our
proposed PVI extractor reduces the average displacement error (ADE) and the
final displacement error (FDE) by 7.46% and 5.24%, respectively. For the
convolutional-based models, our proposed model is compared with Social-STGCNN
and Social-IWSTCNN, and using our proposed PVI extractor reduces the ADE and
FDE by 2.10% and 1.27%, respectively. The results show that the
pedestrian-vehicle interaction influences pedestrian behavior, and the models
using the proposed PVI extractor can capture the interaction between
pedestrians and vehicles, and thereby outperform the compared methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Background Subtraction by Generative Neural Networks. (arXiv:2202.05336v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05336">
<div class="article-summary-box-inner">
<span><p>Background subtraction is a significant task in computer vision and an
essential step for many real world applications. One of the challenges for
background subtraction methods is dynamic background, which constitute
stochastic movements in some parts of the background. In this paper, we have
proposed a new background subtraction method, called DBSGen, which uses two
generative neural networks, one for dynamic motion removal and another for
background generation. At the end, the foreground moving objects are obtained
by a pixel-wise distance threshold based on a dynamic entropy map. The proposed
method has a unified framework that can be optimized in an end-to-end and
unsupervised fashion. The performance of the method is evaluated over dynamic
background sequences and it outperforms most of state-of-the-art methods. Our
code is publicly available at https://github.com/FatemeBahri/DBSGen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coded ResNeXt: a network for designing disentangled information paths. (arXiv:2202.05343v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05343">
<div class="article-summary-box-inner">
<span><p>To avoid treating neural networks as highly complex black boxes, the deep
learning research community has tried to build interpretable models allowing
humans to understand the decisions taken by the model. Unfortunately, the focus
is mostly on manipulating only the very high-level features associated with the
last layers. In this work, we look at neural network architectures for
classification in a more general way and introduce an algorithm which defines
before the training the paths of the network through which the per-class
information flows. We show that using our algorithm we can extract a lighter
single-purpose binary classifier for a particular class by removing the
parameters that do not participate in the predefined information path of that
class, which is approximately 60% of the total parameters. Notably, leveraging
coding theory to design the information paths enables us to use intermediate
network layers for making early predictions without having to evaluate the full
network. We demonstrate that a slightly modified ResNeXt model, trained with
our algorithm, can achieve higher classification accuracy on CIFAR-10/100 and
ImageNet than the original ResNeXt, while having all the aforementioned
properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adversarial Training: A Game Perspective. (arXiv:2202.05352v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05352">
<div class="article-summary-box-inner">
<span><p>The dominant line of work in domain adaptation has focused on learning
invariant representations using domain-adversarial training. In this paper, we
interpret this approach from a game theoretical perspective. Defining optimal
solutions in domain-adversarial training as a local Nash equilibrium, we show
that gradient descent in domain-adversarial training can violate the asymptotic
convergence guarantees of the optimizer, oftentimes hindering the transfer
performance. Our analysis leads us to replace gradient descent with high-order
ODE solvers (i.e., Runge-Kutta), for which we derive asymptotic convergence
guarantees. This family of optimizers is significantly more stable and allows
more aggressive learning rates, leading to high performance gains when used as
a drop-in replacement over standard optimizers. Our experiments show that in
conjunction with state-of-the-art domain-adversarial methods, we achieve up to
3.5% improvement with less than of half training iterations. Our optimizers are
easy to implement, free of additional parameters, and can be plugged into any
domain-adversarial framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Transport for Super Resolution Applied to Astronomy Imaging. (arXiv:2202.05354v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05354">
<div class="article-summary-box-inner">
<span><p>Super resolution is an essential tool in optics, especially on interstellar
scales, due to physical laws restricting possible imaging resolution. We
propose using optimal transport and entropy for super resolution applications.
We prove that the reconstruction is accurate when sparsity is known and noise
or distortion is small enough. We prove that the optimizer is stable and robust
to noise and perturbations. We compare this method to a state of the art
convolutional neural network and get similar results for much less
computational cost and greater methodological flexibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The MeLa BitChute Dataset. (arXiv:2202.05364v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05364">
<div class="article-summary-box-inner">
<span><p>In this paper we present a near-complete dataset of over 3M videos from 61K
channels over 2.5 years (June 2019 to December 2021) from the social video
hosting platform BitChute, a commonly used alternative to YouTube.
Additionally, we include a variety of video-level metadata, including comments,
channel descriptions, and views for each video. The MeLa-BitChute dataset can
be found at:
https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KRD1VS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Give me a knee radiograph, I will tell you where the knee joint area is: a deep convolutional neural network adventure. (arXiv:2202.05382v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05382">
<div class="article-summary-box-inner">
<span><p>Knee pain is undoubtedly the most common musculoskeletal symptom that impairs
quality of life, confines mobility and functionality across all ages. Knee pain
is clinically evaluated by routine radiographs, where the widespread adoption
of radiographic images and their availability at low cost, make them the
principle component in the assessment of knee pain and knee pathologies, such
as arthritis, trauma, and sport injuries. However, interpretation of the knee
radiographs is still highly subjective, and overlapping structures within the
radiographs and the large volume of images needing to be analyzed on a daily
basis, make interpretation challenging for both naive and experienced
practitioners. There is thus a need to implement an artificial intelligence
strategy to objectively and automatically interpret knee radiographs,
facilitating triage of abnormal radiographs in a timely fashion. The current
work proposes an accurate and effective pipeline for autonomous detection,
localization, and classification of knee joint area in plain radiographs
combining the You Only Look Once (YOLO v3) deep convolutional neural network
with a large and fully-annotated knee radiographs dataset. The present work is
expected to stimulate more interest from the deep learning computer vision
community to this pragmatic and clinical application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Including Facial Expressions in Contextual Embeddings for Sign Language Generation. (arXiv:2202.05383v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05383">
<div class="article-summary-box-inner">
<span><p>State-of-the-art sign language generation frameworks lack expressivity and
naturalness which is the result of only focusing manual signs, neglecting the
affective, grammatical and semantic functions of facial expressions. The
purpose of this work is to augment semantic representation of sign language
through grounding facial expressions. We study the effect of modeling the
relationship between text, gloss, and facial expressions on the performance of
the sign generation systems. In particular, we propose a Dual Encoder
Transformer able to generate manual signs as well as facial expressions by
capturing the similarities and differences found in text and sign gloss
annotation. We take into consideration the role of facial muscle activity to
express intensities of manual signs by being the first to employ facial action
units in sign language generation. We perform a series of experiments showing
that our proposed model improves the quality of automatically generated sign
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Learning of Structured Memory via Closed-Loop Transcription. (arXiv:2202.05411v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05411">
<div class="article-summary-box-inner">
<span><p>This work proposes a minimal computational model for learning a structured
memory of multiple object classes in an incremental setting. Our approach is
based on establishing a closed-loop transcription between multiple classes and
their corresponding subspaces, known as a linear discriminative representation,
in a low-dimensional feature space. Our method is both simpler and more
efficient than existing approaches to incremental learning, in terms of model
size, storage, and computation: it requires only a single, fixed-capacity
autoencoding network with a feature space that is used for both discriminative
and generative purposes. All network parameters are optimized simultaneously
without architectural manipulations, by solving a constrained minimax game
between the encoding and decoding maps over a single rate reduction-based
objective. Experimental results show that our method can effectively alleviate
catastrophic forgetting, achieving significantly better performance than prior
work for both generative and discriminative purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACORT: A Compact Object Relation Transformer for Parameter Efficient Image Captioning. (arXiv:2202.05451v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05451">
<div class="article-summary-box-inner">
<span><p>Recent research that applies Transformer-based architectures to image
captioning has resulted in state-of-the-art image captioning performance,
capitalising on the success of Transformers on natural language tasks.
Unfortunately, though these models work well, one major flaw is their large
model sizes. To this end, we present three parameter reduction methods for
image captioning Transformers: Radix Encoding, cross-layer parameter sharing,
and attention parameter sharing. By combining these methods, our proposed ACORT
models have 3.7x to 21.6x fewer parameters than the baseline model without
compromising test performance. Results on the MS-COCO dataset demonstrate that
our ACORT models are competitive against baselines and SOTA approaches, with
CIDEr score &gt;=126. Finally, we present qualitative results and ablation studies
to demonstrate the efficacy of the proposed changes further. Code and
pre-trained models are publicly available at
https://github.com/jiahuei/sparse-image-captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WAD-CMSN: Wasserstein Distance based Cross-Modal Semantic Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2202.05465v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05465">
<div class="article-summary-box-inner">
<span><p>Zero-shot sketch-based image retrieval (ZSSBIR), as a popular studied branch
of computer vision, attracts wide attention recently. Unlike sketch-based image
retrieval (SBIR), the main aim of ZSSBIR is to retrieve natural images given
free hand-drawn sketches that may not appear during training. Previous
approaches used semantic aligned sketch-image pairs or utilized memory
expensive fusion layer for projecting the visual information to a low
dimensional subspace, which ignores the significant heterogeneous cross-domain
discrepancy between highly abstract sketch and relevant image. This may yield
poor performance in the training phase. To tackle this issue and overcome this
drawback, we propose a Wasserstein distance based cross-modal semantic network
(WAD-CMSN) for ZSSBIR. Specifically, it first projects the visual information
of each branch (sketch, image) to a common low dimensional semantic subspace
via Wasserstein distance in an adversarial training manner. Furthermore,
identity matching loss is employed to select useful features, which can not
only capture complete semantic knowledge, but also alleviate the over-fitting
phenomenon caused by the WAD-CMSN model. Experimental results on the
challenging Sketchy (Extended) and TU-Berlin (Extended) datasets indicate the
effectiveness of the proposed WAD-CMSN model over several competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bench-Marking And Improving Arabic Automatic Image Captioning Through The Use Of Multi-Task Learning Paradigm. (arXiv:2202.05474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05474">
<div class="article-summary-box-inner">
<span><p>The continuous increase in the use of social media and the visual content on
the internet have accelerated the research in computer vision field in general
and the image captioning task in specific. The process of generating a caption
that best describes an image is a useful task for various applications such as
it can be used in image indexing and as a hearing aid for the visually
impaired. In recent years, the image captioning task has witnessed remarkable
advances regarding both datasets and architectures, and as a result, the
captioning quality has reached an astounding performance. However, the majority
of these advances especially in datasets are targeted for English, which left
other languages such as Arabic lagging behind. Although Arabic language, being
spoken by more than 450 million people and being the most growing language on
the internet, lacks the fundamental pillars it needs to advance its image
captioning research, such as benchmarks or unified datasets. This works is an
attempt to expedite the synergy in this task by providing unified datasets and
benchmarks, while also exploring methods and techniques that could enhance the
performance of Arabic image captioning. The use of multi-task learning is
explored, alongside exploring various word representations and different
features. The results showed that the use of multi-task learning and
pre-trained word embeddings noticeably enhanced the quality of image
captioning, however the presented results shows that Arabic captioning still
lags behind when compared to the English language. The used dataset and code
are available at this link.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exemplar-free Online Continual Learning. (arXiv:2202.05491v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05491">
<div class="article-summary-box-inner">
<span><p>Targeted for real world scenarios, online continual learning aims to learn
new tasks from sequentially available data under the condition that each data
is observed only once by the learner. Though recent works have made remarkable
achievements by storing part of learned task data as exemplars for knowledge
replay, the performance is greatly relied on the size of stored exemplars while
the storage consumption is a significant constraint in continual learning. In
addition, storing exemplars may not always be feasible for certain applications
due to privacy concerns. In this work, we propose a novel exemplar-free method
by leveraging nearest-class-mean (NCM) classifier where the class mean is
estimated during training phase on all data seen so far through online mean
update criteria. We focus on image classification task and conduct extensive
experiments on benchmark datasets including CIFAR-100 and Food-1k. The results
demonstrate that our method without using any exemplar outperforms
state-of-the-art exemplar-based approaches with large margins under standard
protocol (20 exemplars per class) and is able to achieve competitive
performance even with larger exemplar size (100 exemplars per class).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entroformer: A Transformer-based Entropy Model for Learned Image Compression. (arXiv:2202.05492v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05492">
<div class="article-summary-box-inner">
<span><p>One critical component in lossy deep image compression is the entropy model,
which predicts the probability distribution of the quantized latent
representation in the encoding and decoding modules. Previous works build
entropy models upon convolutional neural networks which are inefficient in
capturing global dependencies. In this work, we propose a novel
transformer-based entropy model, termed Entroformer, to capture long-range
dependencies in probability distribution estimation effectively and
efficiently. Different from vision transformers in image classification, the
Entroformer is highly optimized for image compression, including a top-k
self-attention and a diamond relative position encoding. Meanwhile, we further
expand this architecture with a parallel bidirectional context model to speed
up the decoding process. The experiments show that the Entroformer achieves
state-of-the-art performance on image compression while being time-efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Fusion for Sensorimotor Coordination in Steering Angle Prediction. (arXiv:2202.05500v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05500">
<div class="article-summary-box-inner">
<span><p>Imitation learning is employed to learn sensorimotor coordination for
steering angle prediction in an end-to-end fashion requires expert
demonstrations. These expert demonstrations are paired with environmental
perception and vehicle control data. The conventional frame-based RGB camera is
the most common exteroceptive sensor modality used to acquire the environmental
perception data. The frame-based RGB camera has produced promising results when
used as a single modality in learning end-to-end lateral control. However, the
conventional frame-based RGB camera has limited operability in illumination
variation conditions and is affected by the motion blur. The event camera
provides complementary information to the frame-based RGB camera. This work
explores the fusion of frame-based RGB and event data for learning end-to-end
lateral control by predicting steering angle. In addition, how the
representation from event data fuse with frame-based RGB data helps to predict
the lateral control robustly for the autonomous vehicle. To this end, we
propose DRFuser, a novel convolutional encoder-decoder architecture for
learning end-to-end lateral control. The encoder module is branched between the
frame-based RGB data and event data along with the self-attention layers.
Moreover, this study has also contributed to our own collected dataset
comprised of event, frame-based RGB, and vehicle control data. The efficacy of
the proposed method is experimentally evaluated on our collected dataset, Davis
Driving dataset (DDD), and Carla Eventscape dataset. The experimental results
illustrate that the proposed method DRFuser outperforms the state-of-the-art in
terms of root-mean-square error (RMSE) and mean absolute error (MAE) used as
evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer. (arXiv:2202.05508v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05508">
<div class="article-summary-box-inner">
<span><p>Text spotting end-to-end methods have recently gained attention in the
literature due to the benefits of jointly optimizing the text detection and
recognition components. Existing methods usually have a distinct separation
between the detection and recognition branches, requiring exact annotations for
the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach
for text spotting and the first text spotting framework which may be trained
with both fully- and weakly-supervised settings. By learning a single latent
representation per word detection, and using a novel loss function based on the
Hungarian loss, our method alleviates the need for expensive localization
annotations. Trained with only text transcription annotations on real data, our
weakly-supervised method achieves competitive performance with previous
state-of-the-art fully-supervised methods. When trained in a fully-supervised
manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks
\footnote {Our code will be publicly available upon publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dilated convolutional neural network-based deep reference picture generation for video compression. (arXiv:2202.05514v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05514">
<div class="article-summary-box-inner">
<span><p>Motion estimation and motion compensation are indispensable parts of inter
prediction in video coding. Since the motion vector of objects is mostly in
fractional pixel units, original reference pictures may not accurately provide
a suitable reference for motion compensation. In this paper, we propose a deep
reference picture generator which can create a picture that is more relevant to
the current encoding frame, thereby further reducing temporal redundancy and
improving video compression efficiency. Inspired by the recent progress of
Convolutional Neural Network(CNN), this paper proposes to use a dilated CNN to
build the generator. Moreover, we insert the generated deep picture into
Versatile Video Coding(VVC) as a reference picture and perform a comprehensive
set of experiments to evaluate the effectiveness of our network on the latest
VVC Test Model VTM. The experimental results demonstrate that our proposed
method achieves on average 9.7% bit saving compared with VVC under low-delay P
configuration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised HDR Imaging: What Can Be Learned from a Single 8-bit Video?. (arXiv:2202.05522v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05522">
<div class="article-summary-box-inner">
<span><p>Recently, Deep Learning-based methods for inverse tone-mapping standard
dynamic range (SDR) images to obtain high dynamic range (HDR) images have
become very popular. These methods manage to fill over-exposed areas
convincingly both in terms of details and dynamic range. Typically, these
methods, to be effective, need to learn from large datasets and to transfer
this knowledge to the network weights. In this work, we tackle this problem
from a completely different perspective. What can we learn from a single SDR
video? With the presented zero-shot approach, we show that, in many cases, a
single SDR video is sufficient to be able to generate an HDR video of the same
quality or better than other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-driven Neural Physically-based Facial Asset for Production. (arXiv:2202.05592v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05592">
<div class="article-summary-box-inner">
<span><p>Production-level workflows for producing convincing 3D dynamic human faces
have long relied on a disarray of labor-intensive tools for geometry and
texture generation, motion capture and rigging, and expression synthesis.
Recent neural approaches automate individual components but the corresponding
latent representations cannot provide artists explicit controls as in
conventional tools. In this paper, we present a new learning-based,
video-driven approach for generating dynamic facial geometries with
high-quality physically-based assets. Two key components are well-structured
latent spaces due to dense temporal samplings from videos and explicit facial
expression controls to regulate the latent spaces. For data collection, we
construct a hybrid multiview-photometric capture stage, coupling with an
ultra-fast video camera to obtain raw 3D facial assets. We then model the
facial expression, geometry and physically-based textures using separate VAEs
with a global MLP-based expression mapping across the latent spaces, to
preserve characteristics across respective attributes while maintaining
explicit controls over geometry and texture. We also introduce to model the
delta information as wrinkle maps for physically-base textures, achieving
high-quality rendering of dynamic textures. We demonstrate our approach in
high-fidelity performer-specific facial capture and cross-identity facial
motion retargeting. In addition, our neural asset along with fast adaptation
schemes can also be deployed to handle in-the-wild videos. Besides, we motivate
the utility of our explicit facial disentangle strategy by providing promising
physically-based editing results like geometry and material editing or winkle
transfer with high realism. Comprehensive experiments show that our technique
provides higher accuracy and visual fidelity than previous video-driven facial
reconstruction and animation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Wasserstein GAN for Joint Learning of Inpainting and its Spatial Optimisation. (arXiv:2202.05623v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05623">
<div class="article-summary-box-inner">
<span><p>Classic image inpainting is a restoration method that reconstructs missing
image parts. However, a carefully selected mask of known pixels that yield a
high quality inpainting can also act as a sparse image representation. This
challenging spatial optimisation problem is essential for practical
applications such as compression. So far, it has been almost exclusively
addressed by model-based approaches. First attempts with neural networks seem
promising, but are tailored towards specific inpainting operators or require
postprocessing. To address this issue, we propose the first generative
adversarial network for spatial inpainting data optimisation. In contrast to
previous approaches, it allows joint training of an inpainting generator and a
corresponding mask optimisation network. With a Wasserstein distance, we ensure
that our inpainting results accurately reflect the statistics of natural
images. This yields significant improvements in visual quality and speed over
conventional stochastic models and also outperforms current spatial
optimisation networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artemis: Articulated Neural Pets with Appearance and Motion synthesis. (arXiv:2202.05628v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05628">
<div class="article-summary-box-inner">
<span><p>We human are entering into a virtual era, and surely want to bring animals to
virtual world as well for companion. Yet, computer-generated (CGI) furry
animals is limited by tedious off-line rendering, let alone interactive motion
control. In this paper, we present ARTEMIS, a novel neural modeling and
rendering pipeline for generating ARTiculated neural pets with appEarance and
Motion synthesIS. Our ARTEMIS enables interactive motion control, real-time
animation and photo-realistic rendering of furry animals. The core of ARTEMIS
is a neural-generated (NGI) animal engine, which adopts an efficient octree
based representation for animal animation and fur rendering. The animation then
becomes equivalent to voxel level skeleton based deformation. We further use a
fast octree indexing, an efficient volumetric rendering scheme to generate
appearance and density features maps. Finally, we propose a novel shading
network to generate high-fidelity details of appearance and opacity under novel
poses. For the motion control module in ARTEMIS, we combine state-of-the-art
animal motion capture approach with neural character control scheme. We
introduce an effective optimization scheme to reconstruct skeletal motion of
real animals captured by a multi-view RGB and Vicon camera array. We feed the
captured motion into a neural character control scheme to generate abstract
control signals with motion styles. We further integrate ARTEMIS into existing
engines that support VR headsets, providing an unprecedented immersive
experience where a user can intimately interact with a variety of virtual
animals with vivid movements and photo-realistic appearance. Extensive
experiments and showcases demonstrate the effectiveness of our ARTEMIS system
to achieve highly realistic rendering of NGI animals in real-time, providing
daily immersive and interactive experience with digital animals unseen before.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vehicle and License Plate Recognition with Novel Dataset for Toll Collection. (arXiv:2202.05631v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05631">
<div class="article-summary-box-inner">
<span><p>We propose an automatic framework for toll collection, consisting of three
steps: vehicle type recognition, license plate localization, and reading.
However, each of the three steps becomes non-trivial due to image variations
caused by several factors. The traditional vehicle decorations on the front
cause variations among vehicles of the same type. These decorations make
license plate localization and recognition difficult due to severe background
clutter and partial occlusions. Likewise, on most vehicles, specifically
trucks, the position of the license plate is not consistent. Lastly, for
license plate reading, the variations are induced by non-uniform font styles,
sizes, and partially occluded letters and numbers. Our proposed framework takes
advantage of both data availability and performance evaluation of the backbone
deep learning architectures. We gather a novel dataset, \emph{Diverse Vehicle
and License Plates Dataset (DVLPD)}, consisting of 10k images belonging to six
vehicle types. Each image is then manually annotated for vehicle type, license
plate, and its characters and digits. For each of the three tasks, we evaluate
You Only Look Once (YOLO)v2, YOLOv3, YOLOv4, and FasterRCNN. For real-time
implementation on a Raspberry Pi, we evaluate the lighter versions of YOLO
named Tiny YOLOv3 and Tiny YOLOv4. The best Mean Average Precision (mAP@0.5) of
98.8% for vehicle type recognition, 98.5% for license plate detection, and
98.3% for license plate reading is achieved by YOLOv4, while its lighter
version, i.e., Tiny YOLOv4 obtained a mAP of 97.1%, 97.4%, and 93.7% on vehicle
type recognition, license plate detection, and license plate reading,
respectively. The dataset and the training codes are available at
https://github.com/usama-x930/VT-LPR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tiny Object Tracking: A Large-scale Dataset and A Baseline. (arXiv:2202.05659v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05659">
<div class="article-summary-box-inner">
<span><p>Tiny objects, frequently appearing in practical applications, have weak
appearance and features, and receive increasing interests in meany vision
tasks, such as object detection and segmentation. To promote the research and
development of tiny object tracking, we create a large-scale video dataset,
which contains 434 sequences with a total of more than 217K frames. Each frame
is carefully annotated with a high-quality bounding box. In data creation, we
take 12 challenge attributes into account to cover a broad range of viewpoints
and scene complexities, and annotate these attributes for facilitating the
attribute-based performance analysis. To provide a strong baseline in tiny
object tracking, we propose a novel Multilevel Knowledge Distillation Network
(MKDNet), which pursues three-level knowledge distillations in a unified
framework to effectively enhance the feature representation, discrimination and
localization abilities in tracking tiny objects. Extensive experiments are
performed on the proposed dataset, and the results prove the superiority and
effectiveness of MKDNet compared with state-of-the-art methods. The dataset,
the algorithm code, and the evaluation code are available at
https://github.com/mmic-lcl/Datasets-and-benchmark-code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuperCon: Supervised Contrastive Learning for Imbalanced Skin Lesion Classification. (arXiv:2202.05685v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05685">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have achieved great success in skin
lesion classification. A balanced dataset is required to train a good model.
However, due to the appearance of different skin lesions in practice, severe or
even deadliest skin lesion types (e.g., melanoma) naturally have quite small
amount represented in a dataset. In that, classification performance
degradation occurs widely, it is significantly important to have CNNs that work
well on class imbalanced skin lesion image dataset. In this paper, we propose
SuperCon, a two-stage training strategy to overcome the class imbalance problem
on skin lesion classification. It contains two stages: (i) representation
training that tries to learn a feature representation that closely aligned
among intra-classes and distantly apart from inter-classes, and (ii) classifier
fine-tuning that aims to learn a classifier that correctly predict the label
based on the learnt representations. In the experimental evaluation, extensive
comparisons have been made among our approach and other existing approaches on
skin lesion benchmark datasets. The results show that our two-stage training
strategy effectively addresses the class imbalance classification problem, and
significantly improves existing works in terms of F1-score and AUC score,
resulting in state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Adversarially Robust Deepfake Detection: An Ensemble Approach. (arXiv:2202.05687v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05687">
<div class="article-summary-box-inner">
<span><p>Detecting deepfakes is an important problem, but recent work has shown that
DNN-based deepfake detectors are brittle against adversarial deepfakes, in
which an adversary adds imperceptible perturbations to a deepfake to evade
detection. In this work, we show that a modification to the detection strategy
in which we replace a single classifier with a carefully chosen ensemble, in
which input transformations for each model in the ensemble induces pairwise
orthogonal gradients, can significantly improve robustness beyond the de facto
solution of adversarial training. We present theoretical results to show that
such orthogonal gradients can help thwart a first-order adversary by reducing
the dimensionality of the input subspace in which adversarial deepfakes lie. We
validate the results empirically by instantiating and evaluating a randomized
version of such "orthogonal" ensembles for adversarial deepfake detection and
find that these randomized ensembles exhibit significantly higher robustness as
deepfake detectors compared to state-of-the-art deepfake detectors against
adversarial deepfakes, even those created using strong PGD-500 attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep soccer captioning with transformer: dataset, semantics-related losses, and multi-level evaluation. (arXiv:2202.05728v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05728">
<div class="article-summary-box-inner">
<span><p>This work aims at generating captions for soccer videos using deep learning.
In this context, this paper introduces a dataset, model, and triple-level
evaluation. The dataset consists of 22k caption-clip pairs and three visual
features (images, optical flow, inpainting) for ~500 hours of \emph{SoccerNet}
videos. The model is divided into three parts: a transformer learns language,
ConvNets learn vision, and a fusion of linguistic and visual features generates
captions. The paper suggests evaluating generated captions at three levels:
syntax (the commonly used evaluation metrics such as BLEU-score and CIDEr),
meaning (the quality of descriptions for a domain expert), and corpus (the
diversity of generated captions). The paper shows that the diversity of
generated captions has improved (from 0.07 reaching 0.18) with
semantics-related losses that prioritize selected words. Semantics-related
losses and the utilization of more visual features (optical flow, inpainting)
improved the normalized captioning score by 28\%. The web page of this work:
https://sites.google.com/view/soccercaptioning}{https://sites.google.com/view/soccercaptioning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-NetVLAD+: Learned patch descriptor and weighted matching strategy for place recognition. (arXiv:2202.05738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05738">
<div class="article-summary-box-inner">
<span><p>Visual Place Recognition (VPR) in areas with similar scenes such as urban or
indoor scenarios is a major challenge. Existing VPR methods using global
descriptors have difficulty capturing local specific regions (LSR) in the scene
and are therefore prone to localization confusion in such scenarios. As a
result, finding the LSR that are critical for location recognition becomes key.
To address this challenge, we introduced Patch-NetVLAD+, which was inspired by
patch-based VPR researches. Our method proposed a fine-tuning strategy with
triplet loss to make NetVLAD suitable for extracting patch-level descriptors.
Moreover, unlike existing methods that treat all patches in an image equally,
our method extracts patches of LSR, which present less frequently throughout
the dataset, and makes them play an important role in VPR by assigning proper
weights to them. Experiments on Pittsburgh30k and Tokyo247 datasets show that
our approach achieved up to 6.35\% performance improvement than existing
patch-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Borrowing from yourself: Faster future video segmentation with partial channel update. (arXiv:2202.05748v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05748">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is a well-addressed topic in the computer vision
literature, but the design of fast and accurate video processing networks
remains challenging. In addition, to run on embedded hardware, computer vision
models often have to make compromises on accuracy to run at the required speed,
so that a latency/accuracy trade-off is usually at the heart of these real-time
systems' design. For the specific case of videos, models have the additional
possibility to make use of computations made for previous frames to mitigate
the accuracy loss while being real-time.
</p>
<p>In this work, we propose to tackle the task of fast future video segmentation
prediction through the use of convolutional layers with time-dependent channel
masking. This technique only updates a chosen subset of the feature maps at
each time-step, bringing simultaneously less computation and latency, and
allowing the network to leverage previously computed features. We apply this
technique to several fast architectures and experimentally confirm its benefits
for the future prediction subtask.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Privacy Risks from Feature Vector Reconstruction Attacks. (arXiv:2202.05760v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05760">
<div class="article-summary-box-inner">
<span><p>In deep neural networks for facial recognition, feature vectors are numerical
representations that capture the unique features of a given face. While it is
known that a version of the original face can be recovered via "feature
reconstruction," we lack an understanding of the end-to-end privacy risks
produced by these attacks. In this work, we address this shortcoming by
developing metrics that meaningfully capture the threat of reconstructed face
images. Using end-to-end experiments and user studies, we show that
reconstructed face images enable re-identification by both commercial facial
recognition systems and humans, at a rate that is at worst, a factor of four
times higher than randomized baselines. Our results confirm that feature
vectors should be recognized as Personal Identifiable Information (PII) in
order to protect user privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Knowledge Graph Construction and Application: A Survey. (arXiv:2202.05786v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05786">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the resurgence of knowledge engineering which is
featured by the fast growth of knowledge graphs. However, most of existing
knowledge graphs are represented with pure symbols, which hurts the machine's
capability to understand the real world. The multi-modalization of knowledge
graphs is an inevitable key step towards the realization of human-level machine
intelligence. The results of this endeavor are Multi-modal Knowledge Graphs
(MMKGs). In this survey on MMKGs constructed by texts and images, we first give
definitions of MMKGs, followed with the preliminaries on multi-modal tasks and
techniques. We then systematically review the challenges, progresses and
opportunities on the construction and application of MMKGs respectively, with
detailed analyses of the strength and weakness of different solutions. We
finalize this survey with open research problems relevant to MMKGs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-learning with GANs for anomaly detection, with deployment in high-speed rail inspection system. (arXiv:2202.05795v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05795">
<div class="article-summary-box-inner">
<span><p>Anomaly detection has been an active research area with a wide range of
potential applications. Key challenges for anomaly detection in the AI era with
big data include lack of prior knowledge of potential anomaly types, highly
complex and noisy background in input data, scarce abnormal samples, and
imbalanced training dataset. In this work, we propose a meta-learning framework
for anomaly detection to deal with these issues. Within this framework, we
incorporate the idea of generative adversarial networks (GANs) with appropriate
choices of loss functions including structural similarity index measure (SSIM).
Experiments with limited labeled data for high-speed rail inspection
demonstrate that our meta-learning framework is sharp and robust in identifying
anomalies. Our framework has been deployed in five high-speed railways of China
since 2021: it has reduced more than 99.7% workload and saved 96.7% inspection
time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPasso: Semantically-Aware Object Sketching. (arXiv:2202.05822v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05822">
<div class="article-summary-box-inner">
<span><p>Abstraction is at the heart of sketching due to the simple and minimal nature
of line drawings. Abstraction entails identifying the essential visual
properties of an object or scene, which requires semantic understanding and
prior knowledge of high-level concepts. Abstract depictions are therefore
challenging for artists, and even more so for machines. We present an object
sketching method that can achieve different levels of abstraction, guided by
geometric and semantic simplifications. While sketch generation methods often
rely on explicit sketch datasets for training, we utilize the remarkable
ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic
concepts from sketches and images alike. We define a sketch as a set of
B\'ezier curves and use a differentiable rasterizer to optimize the parameters
of the curves directly with respect to a CLIP-based perceptual loss. The
abstraction degree is controlled by varying the number of strokes. The
generated sketches demonstrate multiple levels of abstraction while maintaining
recognizability, underlying structure, and essential visual components of the
subject drawn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SafePicking: Learning Safe Object Extraction via Object-Level Mapping. (arXiv:2202.05832v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05832">
<div class="article-summary-box-inner">
<span><p>Robots need object-level scene understanding to manipulate objects while
reasoning about contact, support, and occlusion among objects. Given a pile of
objects, object recognition and reconstruction can identify the boundary of
object instances, giving important cues as to how the objects form and support
the pile. In this work, we present a system, SafePicking, that integrates
object-level mapping and learning-based motion planning to generate a motion
that safely extracts occluded target objects from a pile. Planning is done by
learning a deep Q-network that receives observations of predicted poses and a
depth-based heightmap to output a motion trajectory, trained to maximize a
safety metric reward. Our results show that the observation fusion of poses and
depth-sensing gives both better performance and robustness to the model. We
evaluate our methods using the YCB objects in both simulation and the real
world, achieving safe object extraction from piles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Object Tracking: A Literature Review. (arXiv:1409.7618v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1409.7618">
<div class="article-summary-box-inner">
<span><p>Multiple Object Tracking (MOT) has gained increasing attention due to its
academic and commercial potential. Although different approaches have been
proposed to tackle this problem, it still remains challenging due to factors
like abrupt appearance changes and severe object occlusions. In this work, we
contribute the first comprehensive and most recent review on this problem. We
inspect the recent advances in various aspects and propose some interesting
directions for future research. To the best of our knowledge, there has not
been any extensive review on this topic in the community. We endeavor to
provide a thorough review on the development of this problem in recent decades.
The main contributions of this review are fourfold: 1) Key aspects in an MOT
system, including formulation, categorization, key principles, evaluation of
MOT are discussed; 2) Instead of enumerating individual works, we discuss
existing approaches according to various aspects, in each of which methods are
divided into different groups and each group is discussed in detail for the
principles, advances and drawbacks; 3) We examine experiments of existing
publications and summarize results on popular datasets to provide quantitative
and comprehensive comparisons. By analyzing the results from different
perspectives, we have verified some basic agreements in the field; and 4) We
provide a discussion about issues of MOT research, as well as some interesting
directions which will become potential research effort in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Filter Pruning by Switching to Neighboring CNNs with Good Attributes. (arXiv:1904.03961v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.03961">
<div class="article-summary-box-inner">
<span><p>Filter pruning is effective to reduce the computational costs of neural
networks. Existing methods show that updating the previous pruned filter would
enable large model capacity and achieve better performance. However, during the
iterative pruning process, even if the network weights are updated to new
values, the pruning criterion remains the same. In addition, when evaluating
the filter importance, only the magnitude information of the filters is
considered. However, in neural networks, filters do not work individually, but
they would affect other filters. As a result, the magnitude information of each
filter, which merely reflects the information of an individual filter itself,
is not enough to judge the filter importance. To solve the above problems, we
propose Meta-attribute-based Filter Pruning (MFP). First, to expand the
existing magnitude information based pruning criteria, we introduce a new set
of criteria to consider the geometric distance of filters. Additionally, to
explicitly assess the current state of the network, we adaptively select the
most suitable criteria for pruning via a meta-attribute, a property of the
neural network at the current state. Experiments on two image classification
benchmarks validate our method. For ResNet-50 on ILSVRC-2012, we could reduce
more than 50% FLOPs with only 0.44% top-5 accuracy loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Puzzle-AE: Novelty Detection in Images through Solving Puzzles. (arXiv:2008.12959v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.12959">
<div class="article-summary-box-inner">
<span><p>Autoencoder, as an essential part of many anomaly detection methods, is
lacking flexibility on normal data in complex datasets. U-Net is proved to be
effective for this purpose but overfits on the training data if trained by just
using reconstruction error similar to other AE-based frameworks.
Puzzle-solving, as a pretext task of self-supervised learning (SSL) methods,
has earlier proved its ability in learning semantically meaningful features. We
show that training U-Nets based on this task is an effective remedy that
prevents overfitting and facilitates learning beyond pixel-level features.
Shortcut solutions, however, are a big challenge in SSL tasks, including jigsaw
puzzles. We propose adversarial robust training as an effective automatic
shortcut removal. We achieve competitive or superior results compared to the
State of the Art (SOTA) anomaly detection methods on various toy and real-world
datasets. Unlike many competitors, the proposed framework is stable, fast,
data-efficient, and does not require unprincipled early stopping.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NITI: Training Integer Neural Networks Using Integer-only Arithmetic. (arXiv:2009.13108v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13108">
<div class="article-summary-box-inner">
<span><p>While integer arithmetic has been widely adopted for improved performance in
deep quantized neural network inference, training remains a task primarily
executed using floating point arithmetic. This is because both high dynamic
range and numerical accuracy are central to the success of most modern training
algorithms. However, due to its potential for computational, storage and energy
advantages in hardware accelerators, neural network training methods that can
be implemented with low precision integer-only arithmetic remains an active
research challenge. In this paper, we present NITI, an efficient deep neural
network training framework that stores all parameters and intermediate values
as integers, and computes exclusively with integer arithmetic. A pseudo
stochastic rounding scheme that eliminates the need for external random number
generation is proposed to facilitate conversion from wider intermediate results
to low precision storage. Furthermore, a cross-entropy loss backpropagation
scheme computed with integer-only arithmetic is proposed. A proof-of-concept
open-source software implementation of NITI that utilizes native 8-bit integer
operations in modern GPUs to achieve end-to-end training is presented. When
compared with an equivalent training setup implemented with floating point
storage and arithmetic, NITI achieves negligible accuracy degradation on the
MNIST and CIFAR10 datasets using 8-bit integer storage and computation. On
ImageNet, 16-bit integers are needed for weight accumulation with an 8-bit
datapath. This achieves training results comparable to all-floating-point
implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Camera Convolutional Color Constancy. (arXiv:2011.11890v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11890">
<div class="article-summary-box-inner">
<span><p>We present "Cross-Camera Convolutional Color Constancy" (C5), a
learning-based method, trained on images from multiple cameras, that accurately
estimates a scene's illuminant color from raw images captured by a new camera
previously unseen during training. C5 is a hypernetwork-like extension of the
convolutional color constancy (CCC) approach: C5 learns to generate the weights
of a CCC model that is then evaluated on the input image, with the CCC weights
dynamically adapted to different input content. Unlike prior cross-camera color
constancy models, which are usually designed to be agnostic to the spectral
properties of test-set images from unobserved cameras, C5 approaches this
problem through the lens of transductive inference: additional unlabeled images
are provided as input to the model at test time, which allows the model to
calibrate itself to the spectral properties of the test-set camera during
inference. C5 achieves state-of-the-art accuracy for cross-camera color
constancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on
a GPU or CPU, respectively), and requires little memory (~2 MB), and thus is a
practical solution to the problem of calibration-free automatic white balance
for mobile photography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handling Noisy Labels via One-Step Abductive Multi-Target Learning: An Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14956">
<div class="article-summary-box-inner">
<span><p>Learning from noisy labels is an important concern because of the lack of
accurate ground-truth labels in plenty of real-world scenarios. In practice,
various approaches for this concern first make some corrections corresponding
to potentially noisy-labeled instances, and then update predictive model with
information of the made corrections. However, in specific areas, such as
medical histopathology whole slide image analysis (MHWSIA), it is often
difficult or even impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. In this paper, we
focus on alleviating these two problems. For the problem 1), we present
one-step abductive multi-target learning (OSAMTL) that imposes a one-step
logical reasoning upon machine learning via a multi-target learning procedure
to constrain the predictions of the learning model to be subject to our prior
knowledge about the true target. For the problem 2), we propose a logical
assessment formula (LAF) that evaluates the logical rationality of the outputs
of an approach by estimating the consistencies between the predictions of the
learning model and the logical facts narrated from the results of the one-step
logical reasoning of OSAMTL. Applying OSAMTL and LAF to the Helicobacter pylori
(H. pylori) segmentation task in MHWSIA, we show that OSAMTL is able to enable
the machine learning model achieving logically more rational predictions, which
is beyond various state-of-the-art approaches in handling complex noisy labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Bias in Calibration Error Estimation. (arXiv:2012.08668v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08668">
<div class="article-summary-box-inner">
<span><p>For an AI system to be reliable, the confidence it expresses in its decisions
must match its accuracy. To assess the degree of match, examples are typically
binned by confidence and the per-bin mean confidence and accuracy are compared.
Most research in calibration focuses on techniques to reduce this empirical
measure of calibration error, ECE_bin. We instead focus on assessing
statistical bias in this empirical measure, and we identify better estimators.
We propose a framework through which we can compute the bias of a particular
estimator for an evaluation data set of a given size. The framework involves
synthesizing model outputs that have the same statistics as common neural
architectures on popular data sets. We find that binning-based estimators with
bins of equal mass (number of instances) have lower bias than estimators with
bins of equal width. Our results indicate two reliable calibration-error
estimators: the debiased estimator (Brocker, 2012; Ferro and Fricker, 2012) and
a method we propose, ECE_sweep, which uses equal-mass bins and chooses the
number of bins to be as large as possible while preserving monotonicity in the
calibration function. With these estimators, we observe improvements in the
effectiveness of recalibration methods and in the detection of model
miscalibration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Model Deep Learning on Imbalanced Small Datasets for Skin Lesion Classification. (arXiv:2102.01284v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01284">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural network (DCNN) models have been widely explored for
skin disease diagnosis and some of them have achieved the diagnostic outcomes
comparable or even superior to those of dermatologists. However, broad
implementation of DCNN in skin disease detection is hindered by small size and
data imbalance of the publically accessible skin lesion datasets. This paper
proposes a novel single-model based strategy for classification of skin lesions
on small and imbalanced datasets. First, various DCNNs are trained on different
small and imbalanced datasets to verify that the models with moderate
complexity outperform the larger models. Second, regularization DropOut and
DropBlock are added to reduce overfitting and a Modified RandAugment
augmentation strategy is proposed to deal with the defects of sample
underrepresentation in the small dataset. Finally, a novel Multi-Weighted New
Loss (MWNL) function and an end-to-end cumulative learning strategy (CLS) are
introduced to overcome the challenge of uneven sample size and classification
difficulty and to reduce the impact of abnormal samples on training. By
combining Modified RandAugment, MWNL and CLS, our single DCNN model method
achieved the classification accuracy comparable or superior to those of
multiple ensembling models on different dermoscopic image datasets. Our study
shows that this method is able to achieve a high classification performance at
a low cost of computational resources and inference time, potentially suitable
to implement in mobile devices for automated screening of skin lesions and many
other malignancies in low resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-based Facial Affect Analysis: A Review. (arXiv:2103.15599v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15599">
<div class="article-summary-box-inner">
<span><p>As one of the most important affective signals, facial affect analysis (FAA)
is essential for developing human-computer interaction systems. Early methods
focus on extracting appearance and geometry features associated with human
affects while ignoring the latent semantic information among individual facial
changes, leading to limited performance and generalization. Recent work
attempts to establish a graph-based representation to model these semantic
relationships and develop frameworks to leverage them for various FAA tasks.
This paper provides a comprehensive review of graph-based FAA, including the
evolution of algorithms and their applications. First, the FAA background
knowledge is introduced, especially on the role of the graph. We then discuss
approaches widely used for graph-based affective representation in literature
and show a trend towards graph construction. For the relational reasoning in
graph-based FAA, existing studies are categorized according to their non-deep
or deep learning methods, emphasizing the latest graph neural networks.
Performance comparisons of the state-of-the-art graph-based FAA methods are
also summarized. Finally, we discuss the challenges and potential directions.
As far as we know, this is the first survey of graph-based FAA methods. Our
findings can serve as a reference for future research in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Variable Sequential Set Transformers For Joint Multi-Agent Motion Prediction. (arXiv:2104.00563v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00563">
<div class="article-summary-box-inner">
<span><p>Robust multi-agent trajectory prediction is essential for the safe control of
robotic systems. A major challenge is to efficiently learn a representation
that approximates the true joint distribution of contextual, social, and
temporal information to enable planning. We propose Latent Variable Sequential
Set Transformers which are encoder-decoder architectures that generate
scene-consistent multi-agent trajectories. We refer to these architectures as
"AutoBots". The encoder is a stack of interleaved temporal and social
multi-head self-attention (MHSA) modules which alternately perform equivariant
processing across the temporal and social dimensions. The decoder employs
learnable seed parameters in combination with temporal and social MHSA modules
allowing it to perform inference over the entire future scene in a single
forward pass efficiently. AutoBots can produce either the trajectory of one
ego-agent or a distribution over the future trajectories for all agents in the
scene. For the single-agent prediction case, our model achieves top results on
the global nuScenes vehicle motion prediction leaderboard, and produces strong
results on the Argoverse vehicle prediction challenge. In the multi-agent
setting, we evaluate on the synthetic partition of TrajNet++ dataset to
showcase the model's socially-consistent predictions. We also demonstrate our
model on general sequences of sets and provide illustrative experiments
modelling the sequential structure of the multiple strokes that make up symbols
in the Omniglot data. A distinguishing feature of AutoBots is that all models
are trainable on a single desktop GPU (1080 Ti) in under 48h.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection. (arXiv:2106.01483v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01483">
<div class="article-summary-box-inner">
<span><p>The area of domain adaptation has been instrumental in addressing the domain
shift problem encountered by many applications. This problem arises due to the
difference between the distributions of source data used for training in
comparison with target data used during realistic testing scenarios. In this
paper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)
framework that employs multiple domain adaptation paths and corresponding
domain classifiers at different scales of the recently introduced YOLOv4 object
detector to generate domain-invariant features. We train and test our proposed
method using popular datasets. Our experiments show significant improvements in
object detection performance when training YOLOv4 using the proposed MS-DAYOLO
and when tested on target data representing challenging weather conditions for
autonomous driving applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB. (arXiv:2107.05287v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05287">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce a novel, end-to-end trainable CNN-based
architecture to deliver high quality results for grasp detection suitable for a
parallel-plate gripper, and semantic segmentation. Utilizing this, we propose a
novel refinement module that takes advantage of previously calculated grasp
detection and semantic segmentation and further increases grasp detection
accuracy. Our proposed network delivers state-of-the-art accuracy on two
popular grasp dataset, namely Cornell and Jacquard. As additional contribution,
we provide a novel dataset extension for the OCID dataset, making it possible
to evaluate grasp detection in highly challenging scenes. Using this dataset,
we show that semantic segmentation can additionally be used to assign grasp
candidates to object classes, which can be used to pick specific objects in the
scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PU-Flow: a Point Cloud Upsampling Network with Normalizing Flows. (arXiv:2107.05893v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05893">
<div class="article-summary-box-inner">
<span><p>Point cloud upsampling aims to generate dense point clouds from given sparse
ones, which is a challenging task due to the irregular and unordered nature of
point sets. To address this issue, we present a novel deep learning-based
model, called PU-Flow, which incorporates normalizing flows and weight
prediction techniques to produce dense points uniformly distributed on the
underlying surface. Specifically, we exploit the invertible characteristics of
normalizing flows to transform points between Euclidean and latent spaces and
formulate the upsampling process as ensemble of neighbouring points in a latent
space, where the ensemble weights are adaptively learned from local geometric
context. Extensive experiments show that our method is competitive and, in most
test cases, it outperforms state-of-the-art methods in terms of reconstruction
quality, proximity-to-surface accuracy, and computation efficiency. The source
code will be publicly available at https://github.com/unknownue/pu-flow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perception-and-Regulation Network for Salient Object Detection. (arXiv:2107.12560v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12560">
<div class="article-summary-box-inner">
<span><p>Effective fusion of different types of features is the key to salient object
detection. The majority of existing network structure design is based on the
subjective experience of scholars and the process of feature fusion does not
consider the relationship between the fused features and highest-level
features. In this paper, we focus on the feature relationship and propose a
novel global attention unit, which we term the "perception- and-regulation"
(PR) block, that adaptively regulates the feature fusion process by explicitly
modeling interdependencies between features. The perception part uses the
structure of fully-connected layers in classification networks to learn the
size and shape of objects. The regulation part selectively strengthens and
weakens the features to be fused. An imitating eye observation module (IEO) is
further employed for improving the global perception ability of the network.
The imitation of foveal vision and peripheral vision enables IEO to scrutinize
highly detailed objects and to organize the broad spatial scene to better
segment objects. Sufficient experiments conducted on SOD datasets demonstrate
that the proposed method performs favorably against 22 state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are socially-aware trajectory prediction models really socially-aware?. (arXiv:2108.10879v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10879">
<div class="article-summary-box-inner">
<span><p>Our field has recently witnessed an arms race of neural network-based
trajectory predictors. While these predictors are at the core of many
applications such as autonomous navigation or pedestrian flow simulations,
their adversarial robustness has not been carefully studied. In this paper, we
introduce a socially-attended attack to assess the social understanding of
prediction models in terms of collision avoidance. An attack is a small yet
carefully-crafted perturbations to fail predictors. Technically, we define
collision as a failure mode of the output, and propose hard- and soft-attention
mechanisms to guide our attack. Thanks to our attack, we shed light on the
limitations of the current models in terms of their social understanding. We
demonstrate the strengths of our method on the recent trajectory prediction
models. Finally, we show that our attack can be employed to increase the social
understanding of state-of-the-art models. The code is available online:
https://s-attack.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11250">
<div class="article-summary-box-inner">
<span><p>A panoptic driving perception system is an essential part of autonomous
driving. A high-precision and real-time perception system can assist the
vehicle in making the reasonable decision while driving. We present a panoptic
driving perception network (YOLOP) to perform traffic object detection,
drivable area segmentation and lane detection simultaneously. It is composed of
one encoder for feature extraction and three decoders to handle the specific
tasks. Our model performs extremely well on the challenging BDD100K dataset,
achieving state-of-the-art on all three tasks in terms of accuracy and speed.
Besides, we verify the effectiveness of our multi-task learning model for joint
training via ablative studies. To our best knowledge, this is the first work
that can process these three visual perception tasks simultaneously in
real-time on an embedded device Jetson TX2(23 FPS) and maintain excellent
accuracy. To facilitate further research, the source codes and pre-trained
models will be released at https://github.com/hustvl/YOLOP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Decidability-Based Loss Function. (arXiv:2109.05524v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05524">
<div class="article-summary-box-inner">
<span><p>Nowadays, deep learning is the standard approach for a wide range of
problems, including biometrics, such as face recognition and speech
recognition, etc. Biometric problems often use deep learning models to extract
features from images, also known as embeddings. Moreover, the loss function
used during training strongly influences the quality of the generated
embeddings. In this work, a loss function based on the decidability index is
proposed to improve the quality of embeddings for the verification routine. Our
proposal, the D-loss, avoids some Triplet-based loss disadvantages such as the
use of hard samples and tricky parameter tuning, which can lead to slow
convergence. The proposed approach is compared against the Softmax
(cross-entropy), Triplets Soft-Hard, and the Multi Similarity losses in four
different benchmarks: MNIST, Fashion-MNIST, CIFAR10 and CASIA-IrisV4. The
achieved results show the efficacy of the proposal when compared to other
popular metrics in the literature. The D-loss computation, besides being
simple, non-parametric and easy to implement, favors both the inter-class and
intra-class scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UMPNet: Universal Manipulation Policy Network for Articulated Objects. (arXiv:2109.05668v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05668">
<div class="article-summary-box-inner">
<span><p>We introduce the Universal Manipulation Policy Network (UMPNet) -- a single
image-based policy network that infers closed-loop action sequences for
manipulating arbitrary articulated objects. To infer a wide range of action
trajectories, the policy supports 6DoF action representation and varying
trajectory length. To handle a diverse set of objects, the policy learns from
objects with different articulation structures and generalizes to unseen
objects or categories. The policy is trained with self-guided exploration
without any human demonstrations, scripted policy, or pre-defined goal
conditions. To support effective multi-step interaction, we introduce a novel
Arrow-of-Time action attribute that indicates whether an action will change the
object state back to the past or forward into the future. With the
Arrow-of-Time inference at each interaction step, the learned policy is able to
select actions that consistently lead towards or away from a given state,
thereby, enabling both effective state exploration and goal-conditioned
manipulation. Video is available at https://youtu.be/KqlvcL9RqKM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Holistic Semi-Supervised Approaches for EEG Representation Learning. (arXiv:2109.11732v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11732">
<div class="article-summary-box-inner">
<span><p>Recently, supervised methods, which often require substantial amounts of
class labels, have achieved promising results for EEG representation learning.
However, labeling EEG data is a challenging task. More recently, holistic
semi-supervised learning approaches, which only require few output labels, have
shown promising results in the field of computer vision. These methods,
however, have not yet been adapted for EEG learning. In this paper, we adapt
three state-of-the-art holistic semi-supervised approaches, namely MixMatch,
FixMatch, and AdaMatch, as well as five classical semi-supervised methods for
EEG learning. We perform rigorous experiments with all 8 methods on two public
EEG-based emotion recognition datasets, namely SEED and SEED-IV. The
experiments with different amounts of limited labeled samples show that the
holistic approaches achieve strong results even when only 1 labeled sample is
used per class. Further experiments show that in most cases, AdaMatch is the
most effective method, followed by MixMatch and FixMatch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Camera Calibration through Camera Projection Loss. (arXiv:2110.03479v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03479">
<div class="article-summary-box-inner">
<span><p>Camera calibration is a necessity in various tasks including 3D
reconstruction, hand-eye coordination for a robotic interaction, autonomous
driving, etc. In this work we propose a novel method to predict extrinsic
(baseline, pitch, and translation), intrinsic (focal length and principal point
offset) parameters using an image pair. Unlike existing methods, instead of
designing an end-to-end solution, we proposed a new representation that
incorporates camera model equations as a neural network in multi-task learning
framework. We estimate the desired parameters via novel camera projection loss
(CPL) that uses the camera model neural network to reconstruct the 3D points
and uses the reconstruction loss to estimate the camera parameters. To the best
of our knowledge, ours is the first method to jointly estimate both the
intrinsic and extrinsic parameters via a multi-task learning methodology that
combines analytical equations in learning framework for the estimation of
camera parameters. We also proposed a novel dataset using CARLA Simulator.
Empirically, we demonstrate that our proposed approach achieves better
performance with respect to both deep learning-based and traditional methods on
8 out of 10 parameters evaluated using both synthetic and real data. Our code
and generated dataset are available at
https://github.com/thanif/Camera-Calibration-through-Camera-Projection-Loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EchoVPR: Echo State Networks for Visual Place Recognition. (arXiv:2110.05572v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05572">
<div class="article-summary-box-inner">
<span><p>Recognising previously visited locations is an important, but unsolved, task
in autonomous navigation. Current visual place recognition (VPR) benchmarks
typically challenge models to recover the position of a query image (or images)
from sequential datasets that include both spatial and temporal components.
Recently, Echo State Network (ESN) varieties have proven particularly powerful
at solving machine learning tasks that require spatio-temporal modelling. These
networks are simple, yet powerful neural architectures that--exhibiting memory
over multiple time-scales and non-linear high-dimensional representations--can
discover temporal relations in the data while still maintaining linearity in
the learning time. In this paper, we present a series of ESNs and analyse their
applicability to the VPR problem. We report that the addition of ESNs to
pre-processed convolutional neural networks led to a dramatic boost in
performance in comparison to non-recurrent networks in five out of six standard
benchmarks (GardensPoint, SPEDTest, ESSEX3IN1, Oxford RobotCar, and Nordland),
demonstrating that ESNs are able to capture the temporal structure inherent in
VPR problems. Moreover, we show that models that include ESNs can outperform
class-leading VPR models which also exploit the sequential dynamics of the
data. Finally, our results demonstrate that ESNs improve generalisation
abilities, robustness, and accuracy further supporting their suitability to VPR
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP. (arXiv:2110.11316v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11316">
<div class="article-summary-box-inner">
<span><p>CLIP yielded impressive results on zero-shot transfer learning tasks and is
considered as a foundation model like BERT or GPT3. CLIP vision models that
have a rich representation are pre-trained using the InfoNCE objective and
natural language supervision before they are fine-tuned on the particular
tasks. Though CLIP excels at zero-shot transfer learning, it suffers from
explaining away, that is, it focuses too much on few specific features and/or
insufficiently extracts the covariance structure in the data. The former
problem of focusing on few features only is caused by a saturation of the
InfoNCE objective, which is severe for high mutual information. The latter
problem of insufficiently exploiting the covariance structure is caused by a
deficiency in extracting feature associations and co-occurrences. We introduce
"Contrastive Leave One Out Boost" (CLOOB), which uses the InfoLOOB objective
and modern Hopfield networks. In contrast to InfoNCE, the InfoLOOB objective
(leave one out bound) does not saturate and works well for high mutual
information. Modern Hopfield networks, on the other hand, allow to use
retrieved embeddings, which have an enriched covariance structure via
co-occurrences of stored features. We compare CLOOB to CLIP after pre-training
on the Conceptual Captions and the YFCC dataset with respect to their zero-shot
transfer learning performance on other datasets. CLOOB consistently outperforms
CLIP at zero-shot transfer learning across all considered architectures and
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Longitudinal Analysis of Mask and No-Mask on Child Face Recognition. (arXiv:2111.00121v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00121">
<div class="article-summary-box-inner">
<span><p>Face is one of the most widely employed traits for person recognition, even
in many large-scale applications. Despite technological advancements in face
recognition systems, they still face obstacles caused by pose, expression,
occlusion, and aging variations. Owing to the COVID-19 pandemic, contactless
identity verification has become exceedingly vital. Recently, few studies have
been conducted on the effect of face mask on adult face recognition systems
(FRS). However, the impact of aging with face mask on child subject recognition
has not been adequately explored. Thus, the main objective of this study is
analyzing the child longitudinal impact together with face mask and other
covariates on FRS. Specifically, we performed a comparative investigation of
three top performing publicly available face matchers and a post-COVID-19
commercial-off-the-shelf (COTS) system under child cross-age verification and
identification settings using our generated synthetic mask and no-mask samples.
Furthermore, we investigated the longitudinal consequence of eyeglasses with
mask and no-mask. The study exploited no-mask longitudinal child face dataset
(i.e., extended Indian Child Longitudinal Face Dataset) that contains 26,258
face images of 7,473 subjects in the age group of [2, 18] over an average time
span of 3.35 years. Due to the combined effects of face mask and face aging,
the FaceNet, PFE, ArcFace, and COTS face verification system accuracies
decrease approximately 25%, 22%, 18%, 12%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency-Aware Physics-Inspired Degradation Model for Real-World Image Super-Resolution. (arXiv:2111.03301v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03301">
<div class="article-summary-box-inner">
<span><p>Current learning-based single image super-resolution (SISR) algorithms
underperform on real data due to the deviation in the assumed degrada-tion
process from that in the real-world scenario. Conventional degradation
processes consider applying blur, noise, and downsampling (typicallybicubic
downsampling) on high-resolution (HR) images to synthesize low-resolution (LR)
counterparts. However, few works on degradation modelling have taken the
physical aspects of the optical imaging system intoconsideration. In this
paper, we analyze the imaging system optically andexploit the characteristics
of the real-world LR-HR pairs in the spatial frequency domain. We formulate a
real-world physics-inspired degradationmodel by considering
bothopticsandsensordegradation; The physical degradation of an imaging system
is modelled as a low-pass filter, whose cut-off frequency is dictated by the
object distance, the focal length of thelens, and the pixel size of the image
sensor. In particular, we propose to use a convolutional neural network (CNN)
to learn the cutoff frequency of real-world degradation process. The learned
network is then applied to synthesize LR images from unpaired HR images. The
synthetic HR-LR image pairs are later used to train an SISR network. We
evaluatethe effectiveness and generalization capability of the proposed
degradation model on real-world images captured by different imaging systems.
Experimental results showcase that the SISR network trained by using our
synthetic data performs favorably against the network using the traditional
degradation model. Moreover, our results are comparable to that obtained by the
same network trained by using real-world LR-HR pairs, which are challenging to
obtain in real scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imagine by Reasoning: A Reasoning-Based Implicit Semantic Data Augmentation for Long-Tailed Classification. (arXiv:2112.07928v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07928">
<div class="article-summary-box-inner">
<span><p>Real-world data often follows a long-tailed distribution, which makes the
performance of existing classification algorithms degrade heavily. A key issue
is that samples in tail categories fail to depict their intra-class diversity.
Humans can imagine a sample in new poses, scenes, and view angles with their
prior knowledge even if it is the first time to see this category. Inspired by
this, we propose a novel reasoning-based implicit semantic data augmentation
method to borrow transformation directions from other classes. Since the
covariance matrix of each category represents the feature transformation
directions, we can sample new directions from similar categories to generate
definitely different instances. Specifically, the long-tailed distributed data
is first adopted to train a backbone and a classifier. Then, a covariance
matrix for each category is estimated, and a knowledge graph is constructed to
store the relations of any two categories. Finally, tail samples are adaptively
enhanced via propagating information from all the similar categories in the
knowledge graph. Experimental results on CIFAR-100-LT, ImageNet-LT, and
iNaturalist 2018 have demonstrated the effectiveness of our proposed method
compared with the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Deep Hybrid Boosted and Ensemble Learning-based Brain Tumor Analysis using MRI. (arXiv:2201.05373v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05373">
<div class="article-summary-box-inner">
<span><p>Brain tumors analysis is important in timely diagnosis and effective
treatment to cure patients. Tumor analysis is challenging because of tumor
morphology like size, location, texture, and heteromorphic appearance in the
medical images. In this regard, a novel two-phase deep learning-based framework
is proposed to detect and categorize brain tumors in magnetic resonance images
(MRIs). In the first phase, a novel deep boosted features and ensemble
classifiers (DBF-EC) scheme is proposed to detect tumor MRI images from healthy
individuals effectively. The deep boosted feature space is achieved through the
customized and well-performing deep convolutional neural networks (CNNs), and
consequently, fed into the ensemble of machine learning (ML) classifiers. While
in the second phase, a new hybrid features fusion-based brain tumor
classification approach is proposed, comprised of dynamic-static feature and ML
classifier to categorize different tumor types. The dynamic features are
extracted from the proposed BRAIN-RENet CNN, which carefully learns
heteromorphic and inconsistent behavior of various tumors, while the static
features are extracted using HOG. The effectiveness of the proposed two-phase
brain tumor analysis framework is validated on two standard benchmark datasets;
collected from Kaggle and Figshare containing different types of tumor,
including glioma, meningioma, pituitary, and normal images. Experimental
results proved that the proposed DBF-EC detection scheme outperforms and
achieved accuracy (99.56%), precision (0.9991), recall (0.9899), F1-Score
(0.9945), MCC (0.9892), and AUC-PR (0.9990). While the classification scheme,
the joint employment of the deep features fusion of proposed BRAIN-RENet and
HOG features improves performance significantly in terms of recall (0.9913),
precision (0.9906), F1-Score (0.9909), and accuracy (99.20%) on diverse
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eikonal Fields for Refractive Novel-View Synthesis. (arXiv:2202.00948v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00948">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of generating novel-view images from collections of 2D
images showing refractive and reflective objects. Current solutions assume
opaque or transparent light transport along straight paths following the
emission-absorption model. Instead, we optimize for a field of 3D-varying Index
of Refraction (IoR) and trace light through it that bends toward the spatial
gradients of said IoR according to the laws of eikonal light transport.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Nuclei Segmentation via Instance Learning. (arXiv:2202.01564v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01564">
<div class="article-summary-box-inner">
<span><p>Weakly supervised nuclei segmentation is a critical problem for pathological
image analysis and greatly benefits the community due to the significant
reduction of labeling cost. Adopting point annotations, previous methods mostly
rely on less expressive representations for nuclei instances and thus have
difficulty in handling crowded nuclei. In this paper, we propose to decouple
weakly supervised semantic and instance segmentation in order to enable more
effective subtask learning and to promote instance-aware representation
learning. To achieve this, we design a modular deep network with two branches:
a semantic proposal network and an instance encoding network, which are trained
in a two-stage manner with an instance-sensitive loss. Empirical results show
that our approach achieves the state-of-the-art performance on two public
benchmarks of pathological images from different types of organs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative study of 3D object detection frameworks based on LiDAR data and sensor fusion techniques. (arXiv:2202.02521v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02521">
<div class="article-summary-box-inner">
<span><p>Estimating and understanding the surroundings of the vehicle precisely forms
the basic and crucial step for the autonomous vehicle. The perception system
plays a significant role in providing an accurate interpretation of a vehicle's
environment in real-time. Generally, the perception system involves various
subsystems such as localization, obstacle (static and dynamic) detection, and
avoidance, mapping systems, and others. For perceiving the environment, these
vehicles will be equipped with various exteroceptive (both passive and active)
sensors in particular cameras, Radars, LiDARs, and others. These systems are
equipped with deep learning techniques that transform the huge amount of data
from the sensors into semantic information on which the object detection and
localization tasks are performed. For numerous driving tasks, to provide
accurate results, the location and depth information of a particular object is
necessary. 3D object detection methods, by utilizing the additional pose data
from the sensors such as LiDARs, stereo cameras, provides information on the
size and location of the object. Based on recent research, 3D object detection
frameworks performing object detection and localization on LiDAR data and
sensor fusion techniques show significant improvement in their performance. In
this work, a comparative study of the effect of using LiDAR data for object
detection frameworks and the performance improvement seen by using sensor
fusion techniques are performed. Along with discussing various state-of-the-art
methods in both the cases, performing experimental analysis, and providing
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrated Multiscale Domain Adaptive YOLO. (arXiv:2202.03527v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03527">
<div class="article-summary-box-inner">
<span><p>The area of domain adaptation has been instrumental in addressing the domain
shift problem encountered by many applications. This problem arises due to the
difference between the distributions of source data used for training in
comparison with target data used during realistic testing scenarios. In this
paper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)
framework that employs multiple domain adaptation paths and corresponding
domain classifiers at different scales of the recently introduced YOLOv4 object
detector. Building on our baseline multiscale DAYOLO framework, we introduce
three novel deep learning architectures for a Domain Adaptation Network (DAN)
that generates domain-invariant features. In particular, we propose a
Progressive Feature Reduction (PFR), a Unified Classifier (UC), and an
Integrated architecture. We train and test our proposed DAN architectures in
conjunction with YOLOv4 using popular datasets. Our experiments show
significant improvements in object detection performance when training YOLOv4
using the proposed MS-DAYOLO architectures and when tested on target data for
autonomous driving applications. Moreover, MS-DAYOLO framework achieves an
order of magnitude real-time speed improvement relative to Faster R-CNN
solutions while providing comparable object detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the automated large-scale reconstruction of past road networks from historical maps. (arXiv:2202.04883v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04883">
<div class="article-summary-box-inner">
<span><p>Transportation infrastructure, such as road or railroad networks, represent a
fundamental component of our civilization. For sustainable planning and
informed decision making, a thorough understanding of the long-term evolution
of transportation infrastructure such as road networks is crucial. However,
spatially explicit, multi-temporal road network data covering large spatial
extents are scarce and rarely available prior to the 2000s. Herein, we propose
a framework that employs increasingly available scanned and georeferenced
historical map series to reconstruct past road networks, by integrating
abundant, contemporary road network data and color information extracted from
historical maps. Specifically, our method uses contemporary road segments as
analytical units and extracts historical roads by inferring their existence in
historical map series based on image processing and clustering techniques. We
tested our method on over 300,000 road segments representing more than 50,000
km of the road network in the United States, extending across three study areas
that cover 53 historical topographic map sheets dated between 1890 and 1950. We
evaluated our approach by comparison to other historical datasets and against
manually created reference data, achieving F-1 scores of up to 0.95, and showed
that the extracted road network statistics are highly plausible over time,
i.e., following general growth patterns. We demonstrated that contemporary
geospatial data integrated with information extracted from historical map
series open up new avenues for the quantitative analysis of long-term
urbanization processes and landscape changes far beyond the era of operational
remote sensing and digital cartography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spherical Transformer. (arXiv:2202.04942v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04942">
<div class="article-summary-box-inner">
<span><p>Using convolutional neural networks for 360images can induce sub-optimal
performance due to distortions entailed by a planar projection. The distortion
gets deteriorated when a rotation is applied to the 360image. Thus, many
researches based on convolutions attempt to reduce the distortions to learn
accurate representation. In contrast, we leverage the transformer architecture
to solve image classification problems for 360images. Using the proposed
transformer for 360images has two advantages. First, our method does not
require the erroneous planar projection process by sampling pixels from the
sphere surface. Second, our sampling method based on regular polyhedrons makes
low rotation equivariance errors, because specific rotations can be reduced to
permutations of faces. In experiments, we validate our network on two aspects,
as follows. First, we show that using a transformer with highly uniform
sampling methods can help reduce the distortion. Second, we demonstrate that
the transformer architecture can achieve rotation equivariance on specific
rotations. We compare our method to other state-of-the-art algorithms using the
SPH-MNIST, SPH-CIFAR, and SUN360 datasets and show that our method is
competitive with other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-level augmentation to improve robustness of deep neural networks to affine transformations. (arXiv:2202.05152v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05152">
<div class="article-summary-box-inner">
<span><p>Recent studies revealed that convolutional neural networks do not generalize
well to small image transformations, e.g. rotations by a few degrees or
translations of a few pixels. To improve the robustness to such
transformations, we propose to introduce data augmentation at intermediate
layers of the neural architecture, in addition to the common data augmentation
applied on the input images. By introducing small perturbations to activation
maps (features) at various levels, we develop the capacity of the neural
network to cope with such transformations. We conduct experiments on three
image classification benchmarks (Tiny ImageNet, Caltech-256 and Food-101),
considering two different convolutional architectures (ResNet-18 and
DenseNet-121). When compared with two state-of-the-art stabilization methods,
the empirical results show that our approach consistently attains the best
trade-off between accuracy and mean flip rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Servoing for Pose Control of Soft Continuum Arm in a Structured Environment. (arXiv:2202.05200v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05200">
<div class="article-summary-box-inner">
<span><p>For soft continuum arms, visual servoing is a popular control strategy that
relies on visual feedback to close the control loop. However, robust visual
servoing is challenging as it requires reliable feature extraction from the
image, accurate control models and sensors to perceive the shape of the arm,
both of which can be hard to implement in a soft robot. This letter circumvents
these challenges by presenting a deep neural network-based method to perform
smooth and robust 3D positioning tasks on a soft arm by visual servoing using a
camera mounted at the distal end of the arm. A convolutional neural network is
trained to predict the actuations required to achieve the desired pose in a
structured environment. Integrated and modular approaches for estimating the
actuations from the image are proposed and are experimentally compared. A
proportional control law is implemented to reduce the error between the desired
and current image as seen by the camera. The model together with the
proportional feedback control makes the described approach robust to several
variations such as new targets, lighting, loads, and diminution of the soft
arm. Furthermore, the model lends itself to be transferred to a new environment
with minimal effort.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-14 23:06:59.132945671 UTC">2022-02-14 23:06:59 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>