<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-08-26T01:30:00Z">08-26</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.AI updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">The Word is Mightier than the Label Learning without Pointillistic Labels using Data Programming. (arXiv:2108.10921v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10921">
<div class="article-summary-box-inner">
<span><p>Most advanced supervised Machine Learning (ML) models rely on vast amounts of
point-by-point labelled training examples. Hand-labelling vast amounts of data
may be tedious, expensive, and error-prone. Recently, some studies have
explored the use of diverse sources of weak supervision to produce competitive
end model classifiers. In this paper, we survey recent work on weak
supervision, and in particular, we investigate the Data Programming (DP)
framework. Taking a set of potentially noisy heuristics as input, DP assigns
denoised probabilistic labels to each data point in a dataset using a
probabilistic graphical model of heuristics. We analyze the math fundamentals
behind DP and demonstrate the power of it by applying it on two real-world text
classification tasks. Furthermore, we compare DP with pointillistic active and
semi-supervised learning techniques traditionally applied in data-sparse
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning about Counterfactuals and Explanations: Problems, Results and Directions. (arXiv:2108.11004v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11004">
<div class="article-summary-box-inner">
<span><p>There are some recent approaches and results about the use of answer-set
programming for specifying counterfactual interventions on entities under
classification, and reasoning about them. These approaches are flexible and
modular in that they allow the seamless addition of domain knowledge. Reasoning
is enabled by query answering from the answer-set program. The programs can be
used to specify and compute responsibility-based numerical scores as
attributive explanations for classification results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversary agent reinforcement learning for pursuit-evasion. (arXiv:2108.11010v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11010">
<div class="article-summary-box-inner">
<span><p>A reinforcement learning environment with adversary agents is proposed in
this work for pursuit-evasion game in the presence of fog of war, which is of
both scientific significance and practical importance in aerospace
applications. One of the most popular learning environments, StarCraft, is
adopted here and the associated mini-games are analyzed to identify the current
limitation for training adversary agents. The key contribution includes the
analysis of the potential performance of an agent by incorporating control and
differential game theory into the specific reinforcement learning environment,
and the development of an adversary agents challenge (SAAC) environment by
extending the current StarCraft mini-games. The subsequent study showcases the
use of this learning environment and the effectiveness of an adversary agent
for evasion units. Overall, the proposed SAAC environment should benefit
pursuit-evasion studies with rapidly-emerging reinforcement learning
technologies. Last but not least, the corresponding tutorial code can be found
at GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree Decomposed Graph Neural Network. (arXiv:2108.11022v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11022">
<div class="article-summary-box-inner">
<span><p>Graph Neural Networks (GNNs) have achieved significant success in learning
better representations by performing feature propagation and transformation
iteratively to leverage neighborhood information. Nevertheless, iterative
propagation restricts the information of higher-layer neighborhoods to be
transported through and fused with the lower-layer neighborhoods', which
unavoidably results in feature smoothing between neighborhoods in different
layers and can thus compromise the performance, especially on heterophily
networks. Furthermore, most deep GNNs only recognize the importance of
higher-layer neighborhoods while yet to fully explore the importance of
multi-hop dependency within the context of different layer neighborhoods in
learning better representations. In this work, we first theoretically analyze
the feature smoothing between neighborhoods in different layers and empirically
demonstrate the variance of the homophily level across neighborhoods at
different layers. Motivated by these analyses, we further propose a tree
decomposition method to disentangle neighborhoods in different layers to
alleviate feature smoothing among these layers. Moreover, we characterize the
multi-hop dependency via graph diffusion within our tree decomposition
formulation to construct Tree Decomposed Graph Neural Network (TDGNN), which
can flexibly incorporate information from large receptive fields and aggregate
this information utilizing the multi-hop dependency. Comprehensive experiments
demonstrate the superior performance of TDGNN on both homophily and heterophily
networks under a variety of node classification settings. Extensive parameter
analysis highlights the ability of TDGNN to prevent over-smoothing and
incorporate features from shallow layers with deeper multi-hop dependencies,
which provides new insights towards deeper graph neural networks. Code of
TDGNN: <a href="http://github.com/YuWVandy/TDGNN">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Visual Quality of Unrestricted Adversarial Examples with Wavelet-VAE. (arXiv:2108.11032v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11032">
<div class="article-summary-box-inner">
<span><p>Traditional adversarial examples are typically generated by adding
perturbation noise to the input image within a small matrix norm. In practice,
un-restricted adversarial attack has raised great concern and presented a new
threat to the AI safety. In this paper, we propose a wavelet-VAE structure to
reconstruct an input image and generate adversarial examples by modifying the
latent code. Different from perturbation-based attack, the modifications of the
proposed method are not limited but imperceptible to human eyes. Experiments
show that our method can generate high quality adversarial examples on ImageNet
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRIM: A General, Real-Time Deep Learning Inference Framework for Mobile Devices based on Fine-Grained Structured Weight Sparsity. (arXiv:2108.11033v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11033">
<div class="article-summary-box-inner">
<span><p>It is appealing but challenging to achieve real-time deep neural network
(DNN) inference on mobile devices because even the powerful modern mobile
devices are considered as ``resource-constrained'' when executing large-scale
DNNs. It necessitates the sparse model inference via weight pruning, i.e., DNN
weight sparsity, and it is desirable to design a new DNN weight sparsity scheme
that can facilitate real-time inference on mobile devices while preserving a
high sparse model accuracy. This paper designs a novel mobile inference
acceleration framework GRIM that is General to both convolutional neural
networks (CNNs) and recurrent neural networks (RNNs) and that achieves
Real-time execution and high accuracy, leveraging fine-grained structured
sparse model Inference and compiler optimizations for Mobiles. We start by
proposing a new fine-grained structured sparsity scheme through the Block-based
Column-Row (BCR) pruning. Based on this new fine-grained structured sparsity,
our GRIM framework consists of two parts: (a) the compiler optimization and
code generation for real-time mobile inference; and (b) the BCR pruning
optimizations for determining pruning hyperparameters and performing weight
pruning. We compare GRIM with Alibaba MNN, TVM, TensorFlow-Lite, a sparse
implementation based on CSR, PatDNN, and ESE (a representative FPGA inference
acceleration framework for RNNs), and achieve up to 14.08x speedup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social Norm Bias: Residual Harms of Fairness-Aware Algorithms. (arXiv:2108.11056v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11056">
<div class="article-summary-box-inner">
<span><p>Many modern learning algorithms mitigate bias by enforcing fairness across
coarsely-defined groups related to a sensitive attribute like gender or race.
However, the same algorithms seldom account for the within-group biases that
arise due to the heterogeneity of group members. In this work, we characterize
Social Norm Bias (SNoB), a subtle but consequential type of discrimination that
may be exhibited by automated decision-making systems, even when these systems
achieve group fairness objectives. We study this issue through the lens of
gender bias in occupation classification from biographies. We quantify SNoB by
measuring how an algorithm's predictions are associated with conformity to
gender norms, which is measured using a machine learning approach. This
framework reveals that for classification tasks related to male-dominated
occupations, fairness-aware classifiers favor biographies written in ways that
align with masculine gender norms. We compare SNoB across fairness intervention
techniques and show that post-processing interventions do not mitigate this
type of bias at all.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Longitudinal Dynamics of Recommender Systems with Agent-Based Modeling and Simulation. (arXiv:2108.11068v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11068">
<div class="article-summary-box-inner">
<span><p>Today's research in recommender systems is largely based on experimental
designs that are static in a sense that they do not consider potential
longitudinal effects of providing recommendations to users. In reality,
however, various important and interesting phenomena only emerge or become
visible over time, e.g., when a recommender system continuously reinforces the
popularity of already successful artists on a music streaming site or when
recommendations that aim at profit maximization lead to a loss of consumer
trust in the long run. In this paper, we discuss how Agent-Based Modeling and
Simulation (ABM) techniques can be used to study such important longitudinal
dynamics of recommender systems. To that purpose, we provide an overview of the
ABM principles, outline a simulation framework for recommender systems based on
the literature, and discuss various practical research questions that can be
addressed with such an ABM-based simulation framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inductive Matrix Completion Using Graph Autoencoder. (arXiv:2108.11124v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11124">
<div class="article-summary-box-inner">
<span><p>Recently, the graph neural network (GNN) has shown great power in matrix
completion by formulating a rating matrix as a bipartite graph and then
predicting the link between the corresponding user and item nodes. The majority
of GNN-based matrix completion methods are based on Graph Autoencoder (GAE),
which considers the one-hot index as input, maps a user (or item) index to a
learnable embedding, applies a GNN to learn the node-specific representations
based on these learnable embeddings and finally aggregates the representations
of the target users and its corresponding item nodes to predict missing links.
However, without node content (i.e., side information) for training, the user
(or item) specific representation can not be learned in the inductive setting,
that is, a model trained on one group of users (or items) cannot adapt to new
users (or items). To this end, we propose an inductive matrix completion method
using GAE (IMC-GAE), which utilizes the GAE to learn both the user-specific (or
item-specific) representation for personalized recommendation and local graph
patterns for inductive matrix completion. Specifically, we design two
informative node features and employ a layer-wise node dropout scheme in GAE to
learn local graph patterns which can be generalized to unseen data. The main
contribution of our paper is the capability to efficiently learn local graph
patterns in GAE, with good scalability and superior expressiveness compared to
previous GNN-based matrix completion methods. Furthermore, extensive
experiments demonstrate that our model achieves state-of-the-art performance on
several matrix completion benchmarks. Our official code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YANMTT: Yet Another Neural Machine Translation Toolkit. (arXiv:2108.11126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11126">
<div class="article-summary-box-inner">
<span><p>In this paper we present our open-source neural machine translation (NMT)
toolkit called "Yet Another Neural Machine Translation Toolkit" abbreviated as
YANMTT which is built on top of the Transformers library. Despite the growing
importance of sequence to sequence pre-training there surprisingly few, if not
none, well established toolkits that allow users to easily do pre-training.
Toolkits such as Fairseq which do allow pre-training, have very large codebases
and thus they are not beginner friendly. With regards to transfer learning via
fine-tuning most toolkits do not explicitly allow the user to have control over
what parts of the pre-trained models can be transferred. YANMTT aims to address
these issues via the minimum amount of code to pre-train large scale NMT
models, selectively transfer pre-trained parameters and fine-tune them, perform
translation as well as extract representations and attentions for visualization
and analyses. Apart from these core features our toolkit also provides other
advanced functionalities such as but not limited to document/multi-source NMT,
simultaneous NMT and model compression via distillation which we believe are
relevant to the purpose behind our toolkit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridged Adversarial Training. (arXiv:2108.11135v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11135">
<div class="article-summary-box-inner">
<span><p>Adversarial robustness is considered as a required property of deep neural
networks. In this study, we discover that adversarially trained models might
have significantly different characteristics in terms of margin and smoothness,
even they show similar robustness. Inspired by the observation, we investigate
the effect of different regularizers and discover the negative effect of the
smoothness regularizer on maximizing the margin. Based on the analyses, we
propose a new method called bridged adversarial training that mitigates the
negative effect by bridging the gap between clean and adversarial examples. We
provide theoretical and empirical evidence that the proposed method provides
stable and better robustness, especially for large perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Superpixel-guided Discriminative Low-rank Representation of Hyperspectral Images for Classification. (arXiv:2108.11172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11172">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel classification scheme for the remotely
sensed hyperspectral image (HSI), namely SP-DLRR, by comprehensively exploring
its unique characteristics, including the local spatial information and
low-rankness. SP-DLRR is mainly composed of two modules, i.e., the
classification-guided superpixel segmentation and the discriminative low-rank
representation, which are iteratively conducted. Specifically, by utilizing the
local spatial information and incorporating the predictions from a typical
classifier, the first module segments pixels of an input HSI (or its
restoration generated by the second module) into superpixels. According to the
resulting superpixels, the pixels of the input HSI are then grouped into
clusters and fed into our novel discriminative low-rank representation model
with an effective numerical solution. Such a model is capable of increasing the
intra-class similarity by suppressing the spectral variations locally while
promoting the inter-class discriminability globally, leading to a restored HSI
with more discriminative pixels. Experimental results on three benchmark
datasets demonstrate the significant superiority of SP-DLRR over
state-of-the-art methods, especially for the case with an extremely limited
number of training pixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens. (arXiv:2108.11193v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11193">
<div class="article-summary-box-inner">
<span><p>Standard pretrained language models operate on sequences of subword tokens
without direct access to the characters that compose each token's string
representation. We probe the embedding layer of pretrained language models and
show that models learn the internal character composition of whole word and
subword tokens to a surprising extent, without ever seeing the characters
coupled with the tokens. Our results show that the embedding layer of RoBERTa
holds enough information to accurately spell up to a third of the vocabulary
and reach high average character ngram overlap on all token types. We further
test whether enriching subword models with additional character information can
improve language modeling, and observe that this method has a near-identical
learning curve as training without spelling-based enrichment. Overall, our
results suggest that language modeling objectives incentivize the model to
implicitly learn some notion of spelling, and that explicitly teaching the
model how to spell does not enhance its performance on such tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subgoal Search For Complex Reasoning Tasks. (arXiv:2108.11204v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11204">
<div class="article-summary-box-inner">
<span><p>Humans excel in solving complex reasoning tasks through a mental process of
moving from one idea to a related one. Inspired by this, we propose Subgoal
Search (kSubS) method. Its key component is a learned subgoal generator that
produces a diversity of subgoals that are both achievable and closer to the
solution. Using subgoals reduces the search space and induces a high-level
search graph suitable for efficient planning. In this paper, we implement kSubS
using a transformer-based subgoal module coupled with the classical best-first
search framework. We show that a simple approach of generating $k$-th step
ahead subgoals is surprisingly efficient on three challenging domains: two
popular puzzle games, Sokoban and the Rubik's Cube, and an inequality proving
benchmark INT. kSubS achieves strong results including state-of-the-art on INT
within a modest computational budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiscale Spatio-Temporal Graph Neural Networks for 3D Skeleton-Based Motion Prediction. (arXiv:2108.11244v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11244">
<div class="article-summary-box-inner">
<span><p>We propose a multiscale spatio-temporal graph neural network (MST-GNN) to
predict the future 3D skeleton-based human poses in an action-category-agnostic
manner. The core of MST-GNN is a multiscale spatio-temporal graph that
explicitly models the relations in motions at various spatial and temporal
scales. Different from many previous hierarchical structures, our multiscale
spatio-temporal graph is built in a data-adaptive fashion, which captures
nonphysical, yet motion-based relations. The key module of MST-GNN is a
multiscale spatio-temporal graph computational unit (MST-GCU) based on the
trainable graph structure. MST-GCU embeds underlying features at individual
scales and then fuses features across scales to obtain a comprehensive
representation. The overall architecture of MST-GNN follows an encoder-decoder
framework, where the encoder consists of a sequence of MST-GCUs to learn the
spatial and temporal features of motions, and the decoder uses a graph-based
attention gate recurrent unit (GA-GRU) to generate future poses. Extensive
experiments are conducted to show that the proposed MST-GNN outperforms
state-of-the-art methods in both short and long-term motion prediction on the
datasets of Human 3.6M, CMU Mocap and 3DPW, where MST-GNN outperforms previous
works by 5.33% and 3.67% of mean angle errors in average for short-term and
long-term prediction on Human 3.6M, and by 11.84% and 4.71% of mean angle
errors for short-term and long-term prediction on CMU Mocap, and by 1.13% of
mean angle errors on 3DPW in average, respectively. We further investigate the
learned multiscale graphs for interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-term, Short-term and Sudden Event: Trading Volume Movement Prediction with Graph-based Multi-view Modeling. (arXiv:2108.11318v1 [q-fin.ST])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11318">
<div class="article-summary-box-inner">
<span><p>Trading volume movement prediction is the key in a variety of financial
applications. Despite its importance, there is few research on this topic
because of its requirement for comprehensive understanding of information from
different sources. For instance, the relation between multiple stocks, recent
transaction data and suddenly released events are all essential for
understanding trading market. However, most of the previous methods only take
the fluctuation information of the past few weeks into consideration, thus
yielding poor performance. To handle this issue, we propose a graphbased
approach that can incorporate multi-view information, i.e., long-term stock
trend, short-term fluctuation and sudden events information jointly into a
temporal heterogeneous graph. Besides, our method is equipped with deep
canonical analysis to highlight the correlations between different perspectives
of fluctuation for better prediction. Experiment results show that our method
outperforms strong baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Ladder of Causal Distances. (arXiv:2005.02480v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.02480">
<div class="article-summary-box-inner">
<span><p>Causal discovery, the task of automatically constructing a causal model from
data, is of major significance across the sciences. Evaluating the performance
of causal discovery algorithms should ideally involve comparing the inferred
models to ground-truth models available for benchmark datasets, which in turn
requires a notion of distance between causal models. While such distances have
been proposed previously, they are limited by focusing on graphical properties
of the causal models being compared. Here, we overcome this limitation by
defining distances derived from the causal distributions induced by the models,
rather than exclusively from their graphical structure. Pearl and Mackenzie
(2018) have arranged the properties of causal models in a hierarchy called the
"ladder of causation" spanning three rungs: observational, interventional, and
counterfactual. Following this organization, we introduce a hierarchy of three
distances, one for each rung of the ladder. Our definitions are intuitively
appealing as well as efficient to compute approximately. We put our causal
distances to use by benchmarking standard causal discovery systems on both
synthetic and real-world datasets for which ground-truth causal models are
available. Finally, we highlight the usefulness of our causal distances by
briefly discussing further applications beyond the evaluation of causal
discovery techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Workflow Provenance in the Lifecycle of Scientific Machine Learning. (arXiv:2010.00330v3 [cs.DB] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.00330">
<div class="article-summary-box-inner">
<span><p>Machine Learning (ML) has already fundamentally changed several businesses.
More recently, it has also been profoundly impacting the computational science
and engineering domains, like geoscience, climate science, and health science.
In these domains, users need to perform comprehensive data analyses combining
scientific data and ML models to provide for critical requirements, such as
reproducibility, model explainability, and experiment data understanding.
However, scientific ML is multidisciplinary, heterogeneous, and affected by the
physical constraints of the domain, making such analyses even more challenging.
In this work, we leverage workflow provenance techniques to build a holistic
view to support the lifecycle of scientific ML. We contribute with (i)
characterization of the lifecycle and taxonomy for data analyses; (ii) design
principles to build this view, with a W3C PROV compliant data representation
and a reference system architecture; and (iii) lessons learned after an
evaluation in an Oil &amp; Gas case using an HPC cluster with 393 nodes and 946
GPUs. The experiments show that the principles enable queries that integrate
domain semantics with ML models while keeping low overhead (&lt;1%), high
scalability, and an order of magnitude of query acceleration under certain
workloads against without our representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Discriminative Feature Learning for Accent Recognition. (arXiv:2011.12461v4 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12461">
<div class="article-summary-box-inner">
<span><p>Accent recognition with deep learning framework is a similar work to deep
speaker identification, they're both expected to give the input speech an
identifiable representation.
</p>
<p>Compared with the individual-level features learned by speaker identification
network, the deep accent recognition work throws a more challenging point that
forging group-level accent features for speakers.
</p>
<p>In this paper, we borrow and improve the deep speaker identification
framework to recognize accents, in detail, we adopt Convolutional Recurrent
Neural Network as front-end encoder and integrate local features using
Recurrent Neural Network to make an utterance-level accent representation.
</p>
<p>Novelly, to address overfitting, we simply add Connectionist Temporal
Classification based speech recognition auxiliary task during training, and for
ambiguous accent discrimination, we introduce some powerful discriminative loss
functions in face recognition works to enhance the discriminative power of
accent features.
</p>
<p>We show that our proposed network with discriminative training method
(without data-augment) is significantly ahead of the baseline system on the
accent classification track in the Accented English Speech Recognition
Challenge 2020, where the loss function Circle-Loss has achieved the best
discriminative optimization for accent representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07650">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-tuning has achieved promising results on some few-class
classification tasks. The core idea of prompt-tuning is to insert text pieces,
i.e., template, to the input and transform a classification task into a masked
language modeling problem. However, as for relation extraction, determining the
appropriate prompt template requires domain expertise, and single label word
handcrafted or auto-searched is cumbersome and time-consuming to verify their
effectiveness in non-few-shot scenarios, which also fails to leverage the
abundant semantic knowledge in the entities and relation labels. To this end,
we focus on incorporating knowledge into prompt-tuning for relation extraction
and propose a knowledge-aware prompt-tuning with synergistic optimization
(KNIGHT) approach. Specifically, we inject entity and relation knowledge into
prompt construction with learnable virtual template words and answer words and
jointly optimize their representation with knowledge constraints. Extensive
experimental results on 5 datasets with standard and low-resource settings
demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locate Who You Are: Matching Geo-location to Text for User Identity Linkage. (arXiv:2104.09119v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09119">
<div class="article-summary-box-inner">
<span><p>Nowadays, users are encouraged to activate across multiple online social
networks simultaneously. Anchor link prediction, which aims to reveal the
correspondence among different accounts of the same user across networks, has
been regarded as a fundamental problem for user profiling, marketing,
cybersecurity, and recommendation. Existing methods mainly address the
prediction problem by utilizing profile, content, or structural features of
users in symmetric ways. However, encouraged by online services, users would
also post asymmetric information across networks, such as geo-locations and
texts. It leads to an emerged challenge in aligning users with asymmetric
information across networks. Instead of similarity evaluation applied in
previous works, we formalize correlation between geo-locations and texts and
propose a novel anchor link prediction framework for matching users across
networks. Moreover, our model can alleviate the label scarcity problem by
introducing external data. Experimental results on real-world datasets show
that our approach outperforms existing methods and achieves state-of-the-art
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Agent Routing and Scheduling Through Coalition Formation. (arXiv:2105.00451v2 [cs.MA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00451">
<div class="article-summary-box-inner">
<span><p>In task allocation for real-time domains, such as disaster response, a
limited number of agents is deployed across a large area to carry out numerous
tasks, each with its prerequisites, profit, time window and workload. To
maximize profits while minimizing time penalties, agents need to cooperate by
forming, disbanding and reforming coalitions. In this paper, we name this
problem Multi-Agent Routing and Scheduling through Coalition formation (MARSC)
and show that it generalizes the important Team Orienteering Problem with Time
Windows. We propose a binary integer program and an anytime and scalable
heuristic to solve it. Using public London Fire Brigade records, we create a
dataset with 347588 tasks and a test framework that simulates the mobilization
of firefighters. In problems with up to 150 agents and 3000 tasks, our
heuristic finds solutions up to 3.25 times better than the Earliest Deadline
First approach commonly used in real-time systems. Our results constitute the
first large-scale benchmark for the MARSC problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Feature Decorrelation in Self-Supervised Learning. (arXiv:2105.00470v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00470">
<div class="article-summary-box-inner">
<span><p>In self-supervised representation learning, a common idea behind most of the
state-of-the-art approaches is to enforce the robustness of the representations
to predefined augmentations. A potential issue of this idea is the existence of
completely collapsed solutions (i.e., constant features), which are typically
avoided implicitly by carefully chosen implementation details. In this work, we
study a relatively concise framework containing the most common components from
recent approaches. We verify the existence of complete collapse and discover
another reachable collapse pattern that is usually overlooked, namely
dimensional collapse. We connect dimensional collapse with strong correlations
between axes and consider such connection as a strong motivation for feature
decorrelation (i.e., standardizing the covariance matrix). The gains from
feature decorrelation are verified empirically to highlight the importance and
the potential of this insight.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Episodic Transformer for Vision-and-Language Navigation. (arXiv:2105.06453v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06453">
<div class="article-summary-box-inner">
<span><p>Interaction and navigation defined by natural language instructions in
dynamic environments pose significant challenges for neural agents. This paper
focuses on addressing two challenges: handling long sequence of subtasks, and
understanding complex human instructions. We propose Episodic Transformer
(E.T.), a multimodal transformer that encodes language inputs and the full
episode history of visual observations and actions. To improve training, we
leverage synthetic instructions as an intermediate representation that
decouples understanding the visual appearance of an environment from the
variations of natural language instructions. We demonstrate that encoding the
history with a transformer is critical to solve compositional tasks, and that
pretraining and joint training with synthetic instructions further improve the
performance. Our approach sets a new state of the art on the challenging ALFRED
benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test
splits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v5 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07160">
<div class="article-summary-box-inner">
<span><p>We study automated intrusion prevention using reinforcement learning. In a
novel approach, we formulate the problem of intrusion prevention as an optimal
stopping problem. This formulation allows us insight into the structure of the
optimal policies, which turn out to be threshold based. Since the computation
of the optimal defender policy using dynamic programming is not feasible for
practical cases, we approximate the optimal policy through reinforcement
learning in a simulation environment. To define the dynamics of the simulation,
we emulate the target infrastructure and collect measurements. Our evaluations
show that the learned policies are close to optimal and that they indeed can be
expressed using thresholds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Domain Adaptation in Ordinal Regression. (arXiv:2106.11576v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11576">
<div class="article-summary-box-inner">
<span><p>We address the problem of universal domain adaptation (UDA) in ordinal
regression (OR), which attempts to solve classification problems in which
labels are not independent, but follow a natural order. We show that the UDA
techniques developed for classification and based on the clustering assumption,
under-perform in OR settings. We propose a method that complements the OR
classifier with an auxiliary task of order learning, which plays the double
role of discriminating between common and private instances, and expanding
class labels to the private target images via ranking. Combined with
adversarial domain discrimination, our model is able to address the closed set,
partial and open set configurations. We evaluate our method on three face age
estimation datasets, and show that it outperforms the baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning Characterization of Cancer Patients-Derived Extracellular Vesicles using Vibrational Spectroscopies. (arXiv:2107.10332v3 [q-bio.OT] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10332">
<div class="article-summary-box-inner">
<span><p>The early detection of cancer is a challenging problem in medicine. The blood
sera of cancer patients are enriched with heterogeneous secretory lipid bound
extracellular vesicles (EVs), which present a complex repertoire of information
and biomarkers, representing their cell of origin, that are being currently
studied in the field of liquid biopsy and cancer screening. Vibrational
spectroscopies provide non-invasive approaches for the assessment of structural
and biophysical properties in complex biological samples. In this study,
multiple Raman spectroscopy measurements were performed on the EVs extracted
from the blood sera of 9 patients consisting of four different cancer subtypes
(colorectal cancer, hepatocellular carcinoma, breast cancer and pancreatic
cancer) and five healthy patients (controls). FTIR(Fourier Transform Infrared)
spectroscopy measurements were performed as a complementary approach to Raman
analysis, on two of the four cancer subtypes.
</p>
<p>The AdaBoost Random Forest Classifier, Decision Trees, and Support Vector
Machines (SVM) distinguished the baseline corrected Raman spectra of cancer EVs
from those of healthy controls (18 spectra) with a classification accuracy of
greater than 90% when reduced to a spectral frequency range of 1800 to 1940
inverse cm, and subjected to a 0.5 training/testing split. FTIR classification
accuracy on 14 spectra showed an 80% classification accuracy. Our findings
demonstrate that basic machine learning algorithms are powerful tools to
distinguish the complex vibrational spectra of cancer patient EVs from those of
healthy patients. These experimental methods hold promise as valid and
efficient liquid biopsy for machine intelligence-assisted early cancer
screening.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An ASP-based Solution to the Chemotherapy Treatment Scheduling problem. (arXiv:2108.02637v3 [cs.LO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02637">
<div class="article-summary-box-inner">
<span><p>The problem of scheduling chemotherapy treatments in oncology clinics is a
complex problem, given that the solution has to satisfy (as much as possible)
several requirements such as the cyclic nature of chemotherapy treatment plans,
maintaining a constant number of patients, and the availability of resources,
e.g., treatment time, nurses, and drugs. At the same time, realizing a
satisfying schedule is of upmost importance for obtaining the best health
outcomes. In this paper we first consider a specific instance of the problem
which is employed in the San Martino Hospital in Genova, Italy, and present a
solution to the problem based on Answer Set Programming (ASP). Then, we enrich
the problem and the related ASP encoding considering further features often
employed in other hospitals, desirable also in S. Martino, and/or considered in
related papers. Results of an experimental analysis, conducted on the real data
provided by the San Martino Hospital, show that ASP is an effective solving
methodology also for this important scheduling problem. Under consideration for
acceptance in TPLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SERF: Towards better training of deep neural networks using log-Softplus ERror activation Function. (arXiv:2108.09598v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09598">
<div class="article-summary-box-inner">
<span><p>Activation functions play a pivotal role in determining the training dynamics
and neural network performance. The widely adopted activation function ReLU
despite being simple and effective has few disadvantages including the Dying
ReLU problem. In order to tackle such problems, we propose a novel activation
function called Serf which is self-regularized and nonmonotonic in nature. Like
Mish, Serf also belongs to the Swish family of functions. Based on several
experiments on computer vision (image classification and object detection) and
natural language processing (machine translation, sentiment classification and
multimodal entailment) tasks with different state-of-the-art architectures, it
is observed that Serf vastly outperforms ReLU (baseline) and other activation
functions including both Swish and Mish, with a markedly bigger margin on
deeper architectures. Ablation studies further demonstrate that Serf based
architectures perform better than those of Swish and Mish in varying scenarios,
validating the effectiveness and compatibility of Serf with varying depth,
complexity, optimizers, learning rates, batch sizes, initializers and dropout
rates. Finally, we investigate the mathematical relation between Swish and
Serf, thereby showing the impact of preconditioner function ingrained in the
first derivative of Serf which provides a regularization effect making
gradients smoother and optimization faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APObind: A Dataset of Ligand Unbound Protein Conformations for Machine Learning Applications in De Novo Drug Design. (arXiv:2108.09926v2 [q-bio.BM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09926">
<div class="article-summary-box-inner">
<span><p>Protein-ligand complex structures have been utilised to design benchmark
machine learning methods that perform important tasks related to drug design
such as receptor binding site detection, small molecule docking and binding
affinity prediction. However, these methods are usually trained on only ligand
bound (or holo) conformations of the protein and therefore are not guaranteed
to perform well when the protein structure is in its native unbound
conformation (or apo), which is usually the conformation available for a newly
identified receptor. A primary reason for this is that the local structure of
the binding site usually changes upon ligand binding. To facilitate solutions
for this problem, we propose a dataset called APObind that aims to provide apo
conformations of proteins present in the PDBbind dataset, a popular dataset
used in drug design. Furthermore, we explore the performance of methods
specific to three use cases on this dataset, through which, the importance of
validating them on the APObind dataset is demonstrated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoencoder-based Semantic Novelty Detection: Towards Dependable AI-based Systems. (arXiv:2108.10851v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10851">
<div class="article-summary-box-inner">
<span><p>Many autonomous systems, such as driverless taxis, perform safety critical
functions. Autonomous systems employ artificial intelligence (AI) techniques,
specifically for the environment perception. Engineers cannot completely test
or formally verify AI-based autonomous systems. The accuracy of AI-based
systems depends on the quality of training data. Thus, novelty detection -
identifying data that differ in some respect from the data used for training -
becomes a safety measure for system development and operation. In this paper,
we propose a new architecture for autoencoder-based semantic novelty detection
with two innovations: architectural guidelines for a semantic autoencoder
topology and a semantic error calculation as novelty criteria. We demonstrate
that such a semantic novelty detection outperforms autoencoder-based novelty
detection approaches known from literature by minimizing false negatives.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10904">
<div class="article-summary-box-inner">
<span><p>With recent progress in joint modeling of visual and textual representations,
Vision-Language Pretraining (VLP) has achieved impressive performance on many
multimodal downstream tasks. However, the requirement for expensive annotations
including clean image captions and regional labels limits the scalability of
existing approaches, and complicates the pretraining procedure with the
introduction of multiple dataset-specific objectives. In this work, we relax
these constraints and present a minimalist pretraining framework, named Simple
Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training
complexity by exploiting large-scale weak supervision, and is trained
end-to-end with a single prefix language modeling objective. Without utilizing
extra data or task-specific customization, the resulting model significantly
outperforms previous pretraining methods and achieves new state-of-the-art
results on a wide range of discriminative and generative vision-language
benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE
(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).
Furthermore, we demonstrate that SimVLM acquires strong generalization and
transfer ability, enabling zero-shot behavior including open-ended visual
question answering and cross-modality transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Multisource Feature Fusion for the Text Clustering. (arXiv:2108.10926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10926">
<div class="article-summary-box-inner">
<span><p>The text clustering technique is an unsupervised text mining method which are
used to partition a huge amount of text documents into groups. It has been
reported that text clustering algorithms are hard to achieve better performance
than supervised methods and their clustering performance is highly dependent on
the picked text features. Currently, there are many different types of text
feature generation algorithms, each of which extracts text features from some
specific aspects, such as VSM and distributed word embedding, thus seeking a
new way of obtaining features as complete as possible from the corpus is the
key to enhance the clustering effects. In this paper, we present a hybrid
multisource feature fusion (HMFF) framework comprising three components,
feature representation of multimodel, mutual similarity matrices and feature
fusion, in which we construct mutual similarity matrices for each feature
source and fuse discriminative features from mutual similarity matrices by
reducing dimensionality to generate HMFF features, then k-means clustering
algorithm could be configured to partition input samples into groups. The
experimental tests show our HMFF framework outperforms other recently published
algorithms on 7 of 11 public benchmark datasets and has the leading performance
on the rest 4 benchmark datasets as well. At last, we compare HMFF framework
with those competitors on a COVID-19 dataset from the wild with the unknown
cluster count, which shows the clusters generated by HMFF framework partition
those similar samples much closer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The State of SLIVAR: What's next for robots, human-robot interaction, and (spoken) dialogue systems?. (arXiv:2108.10931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10931">
<div class="article-summary-box-inner">
<span><p>We synthesize the reported results and recommendations of recent workshops
and seminars that convened to discuss open questions within the important
intersection of robotics, human-robot interaction, and spoken dialogue systems
research. The goal of this growing area of research interest is to enable
people to more effectively and naturally communicate with robots. To carry
forward opportunities networking and discussion towards concrete, potentially
fundable projects, we encourage interested parties to consider participating in
future virtual and in-person discussions and workshops.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SN Computer Science: Towards Offensive Language Identification for Tamil Code-Mixed YouTube Comments and Posts. (arXiv:2108.10939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10939">
<div class="article-summary-box-inner">
<span><p>Offensive Language detection in social media platforms has been an active
field of research over the past years. In non-native English spoken countries,
social media users mostly use a code-mixed form of text in their
posts/comments. This poses several challenges in the offensive content
identification tasks, and considering the low resources available for Tamil,
the task becomes much harder. The current study presents extensive experiments
using multiple deep learning, and transfer learning models to detect offensive
content on YouTube. We propose a novel and flexible approach of selective
translation and transliteration techniques to reap better results from
fine-tuning and ensembling multilingual transformer networks like BERT, Distil-
BERT, and XLM-RoBERTa. The experimental results showed that ULMFiT is the best
model for this task. The best performing models were ULMFiT and mBERTBiLSTM for
this Tamil code-mix dataset instead of more popular transfer learning models
such as Distil- BERT and XLM-RoBERTa and hybrid deep learning models. The
proposed model ULMFiT and mBERTBiLSTM yielded good results and are promising
for effective offensive speech identification in low-resourced languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness Evaluation of Entity Disambiguation Using Prior Probes:the Case of Entity Overshadowing. (arXiv:2108.10949v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10949">
<div class="article-summary-box-inner">
<span><p>Entity disambiguation (ED) is the last step of entity linking (EL), when
candidate entities are reranked according to the context they appear in. All
datasets for training and evaluating models for EL consist of convenience
samples, such as news articles and tweets, that propagate the prior probability
bias of the entity distribution towards more frequently occurring entities. It
was previously shown that the performance of the EL systems on such datasets is
overestimated since it is possible to obtain higher accuracy scores by merely
learning the prior. To provide a more adequate evaluation benchmark, we
introduce the ShadowLink dataset, which includes 16K short text snippets
annotated with entity mentions. We evaluate and report the performance of
popular EL systems on the ShadowLink benchmark. The results show a considerable
difference in accuracy between more and less common entities for all of the EL
systems under evaluation, demonstrating the effects of prior probability bias
and entity overshadowing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using BERT Encoding and Sentence-Level Language Model for Sentence Ordering. (arXiv:2108.10986v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10986">
<div class="article-summary-box-inner">
<span><p>Discovering the logical sequence of events is one of the cornerstones in
Natural Language Understanding. One approach to learn the sequence of events is
to study the order of sentences in a coherent text. Sentence ordering can be
applied in various tasks such as retrieval-based Question Answering, document
summarization, storytelling, text generation, and dialogue systems.
Furthermore, we can learn to model text coherence by learning how to order a
set of shuffled sentences. Previous research has relied on RNN, LSTM, and
BiLSTM architecture for learning text language models. However, these networks
have performed poorly due to the lack of attention mechanisms. We propose an
algorithm for sentence ordering in a corpus of short stories. Our proposed
method uses a language model based on Universal Transformers (UT) that captures
sentences' dependencies by employing an attention mechanism. Our method
improves the previous state-of-the-art in terms of Perfect Match Ratio (PMR)
score in the ROCStories dataset, a corpus of nearly 100K short human-made
stories. The proposed model includes three components: Sentence Encoder,
Language Model, and Sentence Arrangement with Brute Force Search. The first
component generates sentence embeddings using SBERT-WK pre-trained model
fine-tuned on the ROCStories data. Then a Universal Transformer network
generates a sentence-level language model. For decoding, the network generates
a candidate sentence as the following sentence of the current sentence. We use
cosine similarity as a scoring function to assign scores to the candidate
embedding and the embeddings of other sentences in the shuffled set. Then a
Brute Force Search is employed to maximize the sum of similarities between
pairs of consecutive sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing Accurately Categorizes Indications, Findings and Pathology Reports from Multicenter Colonoscopy. (arXiv:2108.11034v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11034">
<div class="article-summary-box-inner">
<span><p>Colonoscopy is used for colorectal cancer (CRC) screening. Extracting details
of the colonoscopy findings from free text in electronic health records (EHRs)
can be used to determine patient risk for CRC and colorectal screening
strategies. We developed and evaluated the accuracy of a deep learning model
framework to extract information for the clinical decision support system to
interpret relevant free-text reports, including indications, pathology, and
findings notes. The Bio-Bi-LSTM-CRF framework was developed using Bidirectional
Long Short-term Memory (Bi-LSTM) and Conditional Random Fields (CRF) to extract
several clinical features from these free-text reports including indications
for the colonoscopy, findings during the colonoscopy, and pathology of resected
material. We trained the Bio-Bi-LSTM-CRF and existing Bi-LSTM-CRF models on 80%
of 4,000 manually annotated notes from 3,867 patients. These clinical notes
were from a group of patients over 40 years of age enrolled in four Veterans
Affairs Medical Centers. A total of 10% of the remaining annotated notes were
used to train hyperparameter and the remaining 10% were used to evaluate the
accuracy of our model Bio-Bi-LSTM-CRF and compare to Bi-LSTM-CRF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How COVID-19 has Impacted American Attitudes Toward China: A Study on Twitter. (arXiv:2108.11040v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11040">
<div class="article-summary-box-inner">
<span><p>Past research has studied social determinants of attitudes toward foreign
countries. Confounded by potential endogeneity biases due to unobserved factors
or reverse causality, the causal impact of these factors on public opinion is
usually difficult to establish. Using social media data, we leverage the
suddenness of the COVID-19 pandemic to examine whether a major global event has
causally changed American views of another country. We collate a database of
more than 297 million posts on the social media platform Twitter about China or
COVID-19 up to June 2020, and we treat tweeting about COVID-19 as a proxy for
individual awareness of COVID-19. Using regression discontinuity and
difference-in-difference estimation, we find that awareness of COVID-19 causes
a sharp rise in anti-China attitudes. Our work has implications for
understanding how self-interest affects policy preference and how Americans
view migrant communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Viola: A Topic Agnostic Generate-and-Rank Dialogue System. (arXiv:2108.11063v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11063">
<div class="article-summary-box-inner">
<span><p>We present Viola, an open-domain dialogue system for spoken conversation that
uses a topic-agnostic dialogue manager based on a simple generate-and-rank
approach. Leveraging recent advances of generative dialogue systems powered by
large language models, Viola fetches a batch of response candidates from
various neural dialogue models trained with different datasets and
knowledge-grounding inputs. Additional responses originating from
template-based generators are also considered, depending on the user's input
and detected entities. The hand-crafted generators build on a dynamic knowledge
graph injected with rich content that is crawled from the web and automatically
processed on a daily basis. Viola's response ranker is a fine-tuned polyencoder
that chooses the best response given the dialogue history. While dedicated
annotations for the polyencoder alone can indirectly steer it away from
choosing problematic responses, we add rule-based safety nets to detect neural
degeneration and a dedicated classifier to filter out offensive content. We
analyze conversations that Viola took part in for the Alexa Prize Socialbot
Grand Challenge 4 and discuss the strengths and weaknesses of our approach.
Lastly, we suggest future work with a focus on curating conversation data
specifcially for socialbots that will contribute towards a more robust
data-driven socialbot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YANMTT: Yet Another Neural Machine Translation Toolkit. (arXiv:2108.11126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11126">
<div class="article-summary-box-inner">
<span><p>In this paper we present our open-source neural machine translation (NMT)
toolkit called "Yet Another Neural Machine Translation Toolkit" abbreviated as
YANMTT which is built on top of the Transformers library. Despite the growing
importance of sequence to sequence pre-training there surprisingly few, if not
none, well established toolkits that allow users to easily do pre-training.
Toolkits such as Fairseq which do allow pre-training, have very large codebases
and thus they are not beginner friendly. With regards to transfer learning via
fine-tuning most toolkits do not explicitly allow the user to have control over
what parts of the pre-trained models can be transferred. YANMTT aims to address
these issues via the minimum amount of code to pre-train large scale NMT
models, selectively transfer pre-trained parameters and fine-tune them, perform
translation as well as extract representations and attentions for visualization
and analyses. Apart from these core features our toolkit also provides other
advanced functionalities such as but not limited to document/multi-source NMT,
simultaneous NMT and model compression via distillation which we believe are
relevant to the purpose behind our toolkit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens. (arXiv:2108.11193v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11193">
<div class="article-summary-box-inner">
<span><p>Standard pretrained language models operate on sequences of subword tokens
without direct access to the characters that compose each token's string
representation. We probe the embedding layer of pretrained language models and
show that models learn the internal character composition of whole word and
subword tokens to a surprising extent, without ever seeing the characters
coupled with the tokens. Our results show that the embedding layer of RoBERTa
holds enough information to accurately spell up to a third of the vocabulary
and reach high average character ngram overlap on all token types. We further
test whether enriching subword models with additional character information can
improve language modeling, and observe that this method has a near-identical
learning curve as training without spelling-based enrichment. Overall, our
results suggest that language modeling objectives incentivize the model to
implicitly learn some notion of spelling, and that explicitly teaching the
model how to spell does not enhance its performance on such tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Promises of Transformer-Based LMs for the Representation of Normative Claims in the Legal Domain. (arXiv:2108.11215v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11215">
<div class="article-summary-box-inner">
<span><p>In this article, we explore the potential of transformer-based language
models (LMs) to correctly represent normative statements in the legal domain,
taking tax law as our use case. In our experiment, we use a variety of LMs as
bases for both word- and sentence-based clusterers that are then evaluated on a
small, expert-compiled test-set, consisting of real-world samples from tax law
research literature that can be clearly assigned to one of four normative
theories. The results of the experiment show that clusterers based on
sentence-BERT-embeddings deliver the most promising results. Based on this main
experiment, we make first attempts at using the best performing models in a
bootstrapping loop to build classifiers that map normative claims on one of
these four normative theories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ontology-Enhanced Slot Filling. (arXiv:2108.11275v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11275">
<div class="article-summary-box-inner">
<span><p>Slot filling is a fundamental task in dialog state tracking in task-oriented
dialog systems. In multi-domain task-oriented dialog system, user utterances
and system responses may mention multiple named entities and attributes values.
A system needs to select those that are confirmed by the user and fill them
into destined slots. One difficulty is that since a dialogue session contains
multiple system-user turns, feeding in all the tokens into a deep model such as
BERT can be challenging due to limited capacity of input word tokens and GPU
memory. In this paper, we investigate an ontology-enhanced approach by matching
the named entities occurred in all dialogue turns using ontology. The matched
entities in the previous dialogue turns will be accumulated and encoded as
additional inputs to a BERT-based dialogue state tracker. In addition, our
improvement includes ontology constraint checking and the correction of slot
name tokenization. Experimental results showed that our ontology-enhanced
dialogue state tracker improves the joint goal accuracy (slot F1) from 52.63%
(91.64%) to 53.91% (92%) on MultiWOZ 2.1 corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProoFVer: Natural Logic Theorem Proving for Fact Verification. (arXiv:2108.11357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11357">
<div class="article-summary-box-inner">
<span><p>We propose ProoFVer, a proof system for fact verification using natural
logic. The textual entailment model in ProoFVer is a seq2seq model generating
valid natural-logic based logical inferences as its proofs. The generation of
proofs makes ProoFVer an explainable system. The proof consists of iterative
lexical mutations of spans in the claim with spans in a set of retrieved
evidence sentences. Further, each such mutation is marked with an entailment
relation using natural logic operators. The veracity of a claim is determined
solely based on the sequence of natural logic relations present in the proof.
By design, this makes ProoFVer a faithful by construction system that generates
faithful explanations. ProoFVer outperforms existing fact-verification models,
with more than two percent absolute improvements in performance and robustness.
In addition to its explanations being faithful, ProoFVer also scores high on
rationale extraction, with a five point absolute improvement compared to
attention-based rationales in existing models. Finally, we find that humans
correctly simulate ProoFVer's decisions more often using the proofs, than the
decisions of an existing model that directly use the retrieved evidence for
decision making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Entity Linking: A Survey of Models Based on Deep Learning. (arXiv:2006.00575v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.00575">
<div class="article-summary-box-inner">
<span><p>In this survey, we provide a comprehensive description of recent neural
entity linking (EL) systems developed since 2015 as a result of the "deep
learning revolution" in NLP. Our goal is to systemize design features of neural
entity linking systems and compare their performance to the prominent classic
methods on common benchmarks. We distill generic architectural components of a
neural EL system, like candidate generation and entity ranking, and summarize
prominent methods for each of them. The vast variety of modifications of this
general neural entity linking architecture are grouped by several common
themes: joint entity recognition and linking, models for global linking,
domain-independent techniques including zero-shot and distant supervision
methods, and cross-lingual approaches. Since many neural models take advantage
of entity and mention/context embeddings to catch semantic meaning of them, we
provide an overview of popular embedding techniques. Finally, we briefly
discuss applications of entity linking, focusing on the recently emerged
use-case of enhancing deep pre-trained masked language models based on the
transformer architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03060">
<div class="article-summary-box-inner">
<span><p>A key challenge in training neural networks for a given medical imaging task
is often the difficulty of obtaining a sufficient number of manually labeled
examples. In contrast, textual imaging reports, which are often readily
available in medical records, contain rich but unstructured interpretations
written by experts as part of standard clinical practice. We propose using
these textual reports as a form of weak supervision to improve the image
interpretation performance of a neural network without requiring additional
manually labeled examples. We use an image-text matching task to train a
feature extractor and then fine-tune it in a transfer learning setting for a
supervised task using a small labeled dataset. The end result is a neural
network that automatically interprets imagery without requiring textual reports
during inference. This approach can be applied to any task for which text-image
pairs are readily available. We evaluate our method on three classification
tasks and find consistent performance improvements, reducing the need for
labeled data by 67%-98%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Telling the What while Pointing to the Where: Multimodal Queries for Image Retrieval. (arXiv:2102.04980v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04980">
<div class="article-summary-box-inner">
<span><p>Most existing image retrieval systems use text queries as a way for the user
to express what they are looking for. However, fine-grained image retrieval
often requires the ability to also express where in the image the content they
are looking for is. The text modality can only cumbersomely express such
localization preferences, whereas pointing is a more natural fit. In this
paper, we propose an image retrieval setup with a new form of multimodal
queries, where the user simultaneously uses both spoken natural language (the
what) and mouse traces over an empty canvas (the where) to express the
characteristics of the desired target image. We then describe simple
modifications to an existing image retrieval model, enabling it to operate in
this setup. Qualitative and quantitative experiments show that our model
effectively takes this spatial guidance into account, and provides
significantly more accurate retrieval results compared to text-only equivalent
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation. (arXiv:2104.04167v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04167">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language Navigation (VLN) requires an agent to find a path to a
remote location on the basis of natural-language instructions and a set of
photo-realistic panoramas. Most existing methods take the words in the
instructions and the discrete views of each panorama as the minimal unit of
encoding. However, this requires a model to match different nouns (e.g., TV,
table) against the same input view feature. In this work, we propose an
object-informed sequential BERT to encode visual perceptions and linguistic
instructions at the same fine-grained level, namely objects and words. Our
sequential BERT also enables the visual-textual clues to be interpreted in
light of the temporal context, which is crucial to multi-round VLN tasks.
Additionally, we enable the model to identify the relative direction (e.g.,
left/right/front/back) of each navigable location and the room type (e.g.,
bedroom, kitchen) of its current and final navigation goal, as such information
is widely mentioned in instructions implying the desired next and final
locations. We thus enable the model to know-where the objects lie in the
images, and to know-where they stand in the scene. Extensive experiments
demonstrate the effectiveness compared against several state-of-the-art methods
on three indoor VLN tasks: REVERIE, NDH, and R2R. Project repository:
https://github.com/YuankaiQi/ORIST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07650">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-tuning has achieved promising results on some few-class
classification tasks. The core idea of prompt-tuning is to insert text pieces,
i.e., template, to the input and transform a classification task into a masked
language modeling problem. However, as for relation extraction, determining the
appropriate prompt template requires domain expertise, and single label word
handcrafted or auto-searched is cumbersome and time-consuming to verify their
effectiveness in non-few-shot scenarios, which also fails to leverage the
abundant semantic knowledge in the entities and relation labels. To this end,
we focus on incorporating knowledge into prompt-tuning for relation extraction
and propose a knowledge-aware prompt-tuning with synergistic optimization
(KNIGHT) approach. Specifically, we inject entity and relation knowledge into
prompt construction with learnable virtual template words and answer words and
jointly optimize their representation with knowledge constraints. Extensive
experimental results on 5 datasets with standard and low-resource settings
demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Annotated Commodity News Corpus for Event Extraction. (arXiv:2105.08214v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08214">
<div class="article-summary-box-inner">
<span><p>Commodity News contains a wealth of information such as sum-mary of the
recent commodity price movement and notable events that led tothe movement.
Through event extraction, useful information extracted fromcommodity news is
extremely useful in mining for causal relation betweenevents and commodity
price movement, which can be used for commodity priceprediction. To facilitate
the future research, we introduce a new dataset withthe following information
identified and annotated: (i) entities (both nomi-nal and named), (ii) events
(trigger words and argument roles), (iii) eventmetadata: modality, polarity and
intensity and (iv) event-event relations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?. (arXiv:2105.09142v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09142">
<div class="article-summary-box-inner">
<span><p>The automatic detection of humor poses a grand challenge for natural language
processing. Transformer-based systems have recently achieved remarkable results
on this task, but they usually (1)~were evaluated in setups where serious vs
humorous texts came from entirely different sources, and (2)~focused on
benchmarking performance without providing insights into how the models work.
We make progress in both respects by training and analyzing transformer-based
humor recognition models on a recently introduced dataset consisting of minimal
pairs of aligned sentences, one serious, the other humorous. We find that,
although our aligned dataset is much harder than previous datasets,
transformer-based models recognize the humorous sentence in an aligned pair
with high accuracy (78%). In a careful error analysis, we characterize easy vs
hard instances. Finally, by analyzing attention weights, we obtain important
insights into the mechanisms by which transformers recognize humor. Most
remarkably, we find clear evidence that one single attention head learns to
recognize the words that make a test sentence humorous, even without access to
this information at training time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Self-supervised Method for Entity Alignment. (arXiv:2106.09395v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09395">
<div class="article-summary-box-inner">
<span><p>Entity alignment, aiming to identify equivalent entities across different
knowledge graphs (KGs), is a fundamental problem for constructing large-scale
KGs. Over the course of its development, supervision has been considered
necessary for accurate alignments. Inspired by the recent progress of
self-supervised learning, we explore the extent to which we can get rid of
supervision for entity alignment. Existing supervised methods for this task
focus on pulling each pair of positive (labeled) entities close to each other.
However, our analysis suggests that the learning of entity alignment can
actually benefit more from pushing sampled (unlabeled) negatives far away than
pulling positive aligned pairs close. We present SelfKG by leveraging this
discovery to design a contrastive learning strategy across two KGs. Extensive
experiments on benchmark datasets demonstrate that SelfKG without supervision
can match or achieve comparable results with state-of-the-art supervised
baselines. The performance of SelfKG demonstrates self-supervised learning
offers great potential for entity alignment in KGs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion. (arXiv:2108.01387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01387">
<div class="article-summary-box-inner">
<span><p>We present InferWiki, a Knowledge Graph Completion (KGC) dataset that
improves upon existing benchmarks in inferential ability, assumptions, and
patterns. First, each testing sample is predictable with supportive data in the
training set. To ensure it, we propose to utilize rule-guided train/test
generation, instead of conventional random split. Second, InferWiki initiates
the evaluation following the open-world assumption and improves the inferential
difficulty of the closed-world assumption, by providing manually annotated
negative and unknown triples. Third, we include various inference patterns
(e.g., reasoning path length and types) for comprehensive evaluation. In
experiments, we curate two settings of InferWiki varying in sizes and
structures, and apply the construction process on CoDEx as comparative
datasets. The results and empirical analyses demonstrate the necessity and
high-quality of InferWiki. Nevertheless, the performance gap among various
inferential assumptions and patterns presents the difficulty and inspires
future research direction. Our datasets can be found in
https://github.com/TaoMiner/inferwiki
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer. (arXiv:2108.09193v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09193">
<div class="article-summary-box-inner">
<span><p>Transformer has achieved great success in NLP. However, the quadratic
complexity of the self-attention mechanism in Transformer makes it inefficient
in handling long sequences. Many existing works explore to accelerate
Transformers by computing sparse self-attention instead of a dense one, which
usually attends to tokens at certain positions or randomly selected tokens.
However, manually selected or random tokens may be uninformative for context
modeling. In this paper, we propose Smart Bird, which is an efficient and
effective Transformer with learnable sparse attention. In Smart Bird, we first
compute a sketched attention matrix with a single-head low-dimensional
Transformer, which aims to find potential important interactions between
tokens. We then sample token pairs based on their probability scores derived
from the sketched attention matrix to generate different sparse attention index
matrices for different attention heads. Finally, we select token embeddings
according to the index matrices to form the input of sparse attention networks.
Extensive experiments on six benchmark datasets for different tasks validate
the efficiency and effectiveness of Smart Bird in text modeling.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10904">
<div class="article-summary-box-inner">
<span><p>With recent progress in joint modeling of visual and textual representations,
Vision-Language Pretraining (VLP) has achieved impressive performance on many
multimodal downstream tasks. However, the requirement for expensive annotations
including clean image captions and regional labels limits the scalability of
existing approaches, and complicates the pretraining procedure with the
introduction of multiple dataset-specific objectives. In this work, we relax
these constraints and present a minimalist pretraining framework, named Simple
Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training
complexity by exploiting large-scale weak supervision, and is trained
end-to-end with a single prefix language modeling objective. Without utilizing
extra data or task-specific customization, the resulting model significantly
outperforms previous pretraining methods and achieves new state-of-the-art
results on a wide range of discriminative and generative vision-language
benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE
(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).
Furthermore, we demonstrate that SimVLM acquires strong generalization and
transfer ability, enabling zero-shot behavior including open-ended visual
question answering and cross-modality transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correcting inter-scan motion artefacts in quantitative R1 mapping at 7T. (arXiv:2108.10943v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10943">
<div class="article-summary-box-inner">
<span><p>Purpose: Inter-scan motion is a substantial source of error in $R_1$
estimation, and can be expected to increase at 7T where $B_1$ fields are more
inhomogeneous. The established correction scheme does not translate to 7T since
it requires a body coil reference. Here we introduce two alternatives that
outperform the established method. Since they compute relative sensitivities
they do not require body coil images.
</p>
<p>Theory: The proposed methods use coil-combined magnitude images to obtain the
relative coil sensitivities. The first method efficiently computes the relative
sensitivities via a simple ratio; the second by fitting a more sophisticated
generative model.
</p>
<p>Methods: $R_1$ maps were computed using the variable flip angle (VFA)
approach. Multiple datasets were acquired at 3T and 7T, with and without motion
between the acquisition of the VFA volumes. $R_1$ maps were constructed without
correction, with the proposed corrections, and (at 3T) with the previously
established correction scheme.
</p>
<p>Results: At 3T, the proposed methods outperform the baseline method.
Inter-scan motion artefacts were also reduced at 7T. However, reproducibility
only converged on that of the no motion condition if position-specific transmit
field effects were also incorporated.
</p>
<p>Conclusion: The proposed methods simplify inter-scan motion correction of
$R_1$ maps and are applicable at both 3T and 7T, where a body coil is typically
not available. The open-source code for all methods is made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Field-Guide-Inspired Zero-Shot Learning. (arXiv:2108.10967v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10967">
<div class="article-summary-box-inner">
<span><p>Modern recognition systems require large amounts of supervision to achieve
accuracy. Adapting to new domains requires significant data from experts, which
is onerous and can become too expensive. Zero-shot learning requires an
annotated set of attributes for a novel category. Annotating the full set of
attributes for a novel category proves to be a tedious and expensive task in
deployment. This is especially the case when the recognition domain is an
expert domain. We introduce a new field-guide-inspired approach to zero-shot
annotation where the learner model interactively asks for the most useful
attributes that define a class. We evaluate our method on classification
benchmarks with attribute annotations like CUB, SUN, and AWA2 and show that our
model achieves the performance of a model with full annotations at the cost of
a significantly fewer number of annotations. Since the time of experts is
precious, decreasing annotation cost can be very valuable for real-world
deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Indian Sign Language (ISL) Recognition. (arXiv:2108.10970v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10970">
<div class="article-summary-box-inner">
<span><p>This paper presents a system which can recognise hand poses &amp; gestures from
the Indian Sign Language (ISL) in real-time using grid-based features. This
system attempts to bridge the communication gap between the hearing and speech
impaired and the rest of the society. The existing solutions either provide
relatively low accuracy or do not work in real-time. This system provides good
results on both the parameters. It can identify 33 hand poses and some gestures
from the ISL. Sign Language is captured from a smartphone camera and its frames
are transmitted to a remote server for processing. The use of any external
hardware (such as gloves or the Microsoft Kinect sensor) is avoided, making it
user-friendly. Techniques such as Face detection, Object stabilisation and Skin
Colour Segmentation are used for hand detection and tracking. The image is
further subjected to a Grid-based Feature Extraction technique which represents
the hand's pose in the form of a Feature Vector. Hand poses are then classified
using the k-Nearest Neighbours algorithm. On the other hand, for gesture
classification, the motion and intermediate hand poses observation sequences
are fed to Hidden Markov Model chains corresponding to the 12 pre-selected
gestures defined in ISL. Using this methodology, the system is able to achieve
an accuracy of 99.7% for static hand poses, and an accuracy of 97.23% for
gesture recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Effective Pixel-Wise Approach for Skin Colour Segmentation Using Pixel Neighbourhood Technique. (arXiv:2108.10971v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10971">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel technique for skin colour segmentation that
overcomes the limitations faced by existing techniques such as Colour Range
Thresholding. Skin colour segmentation is affected by the varied skin colours
and surrounding lighting conditions, leading to poorskin segmentation for many
techniques. We propose a new two stage Pixel Neighbourhood technique that
classifies any pixel as skin or non-skin based on its neighbourhood pixels. The
first step calculates the probability of each pixel being skin by passing HSV
values of the pixel to a Deep Neural Network model. In the next step, it
calculates the likeliness of pixel being skin using these probabilities of
neighbouring pixels. This technique performs skin colour segmentation better
than the existing techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation for Real-World Single View 3D Reconstruction. (arXiv:2108.10972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10972">
<div class="article-summary-box-inner">
<span><p>Deep learning-based object reconstruction algorithms have shown remarkable
improvements over classical methods. However, supervised learning based methods
perform poorly when the training data and the test data have different
distributions. Indeed, most current works perform satisfactorily on the
synthetic ShapeNet dataset, but dramatically fail in when presented with real
world images. To address this issue, unsupervised domain adaptation can be used
transfer knowledge from the labeled synthetic source domain and learn a
classifier for the unlabeled real target domain. To tackle this challenge of
single view 3D reconstruction in the real domain, we experiment with a variety
of domain adaptation techniques inspired by the maximum mean discrepancy (MMD)
loss, Deep CORAL, and the domain adversarial neural network (DANN). From these
findings, we additionally propose a novel architecture which takes advantage of
the fact that in this setting, target domain data is unsupervised with regards
to the 3D model but supervised for class labels. We base our framework off a
recent network called pix2vox. Results are performed with ShapeNet as the
source domain and domains within the Object Dataset Domain Suite (ODDS) dataset
as the target, which is a real world multiview, multidomain image dataset. The
domains in ODDS vary in difficulty, allowing us to assess notions of domain gap
size. Our results are the first in the multiview reconstruction literature
using this dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction. (arXiv:2108.10991v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10991">
<div class="article-summary-box-inner">
<span><p>Image reconstruction is an inverse problem that solves for a computational
image based on sampled sensor measurement. Sparsely sampled image
reconstruction poses addition challenges due to limited measurements. In this
work, we propose an implicit Neural Representation learning methodology with
Prior embedding (NeRP) to reconstruct a computational image from sparsely
sampled measurements. The method differs fundamentally from previous deep
learning-based image reconstruction approaches in that NeRP exploits the
internal information in an image prior, and the physics of the sparsely sampled
measurements to produce a representation of the unknown subject. No large-scale
data is required to train the NeRP except for a prior image and sparsely
sampled measurements. In addition, we demonstrate that NeRP is a general
methodology that generalizes to different imaging modalities such as CT and
MRI. We also show that NeRP can robustly capture the subtle yet significant
image changes required for assessing tumor progression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OOWL500: Overcoming Dataset Collection Bias in the Wild. (arXiv:2108.10992v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10992">
<div class="article-summary-box-inner">
<span><p>The hypothesis that image datasets gathered online "in the wild" can produce
biased object recognizers, e.g. preferring professional photography or certain
viewing angles, is studied. A new "in the lab" data collection infrastructure
is proposed consisting of a drone which captures images as it circles around
objects. Crucially, the control provided by this setup and the natural camera
shake inherent to flight mitigate many biases. It's inexpensive and easily
replicable nature may also potentially lead to a scalable data collection
effort by the vision community. The procedure's usefulness is demonstrated by
creating a dataset of Objects Obtained With fLight (OOWL). Denoted as OOWL500,
it contains 120,000 images of 500 objects and is the largest "in the lab" image
dataset available when both number of classes and objects per class are
considered. Furthermore, it has enabled several of new insights on object
recognition. First, a novel adversarial attack strategy is proposed, where
image perturbations are defined in terms of semantic properties such as camera
shake and pose. Indeed, experiments have shown that ImageNet has considerable
amounts of pose and professional photography bias. Second, it is used to show
that the augmentation of in the wild datasets, such as ImageNet, with in the
lab data, such as OOWL500, can significantly decrease these biases, leading to
object recognizers of improved generalization. Third, the dataset is used to
study questions on "best procedures" for dataset collection. It is revealed
that data augmentation with synthetic images does not suffice to eliminate in
the wild datasets biases, and that camera shake and pose diversity play a more
important role in object recognition robustness than previously thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wanderlust: Online Continual Object Detection in the Real World. (arXiv:2108.11005v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11005">
<div class="article-summary-box-inner">
<span><p>Online continual learning from data streams in dynamic environments is a
critical direction in the computer vision field. However, realistic benchmarks
and fundamental studies in this line are still missing. To bridge the gap, we
present a new online continual object detection benchmark with an egocentric
video dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos,
an ego-centric video stream collected over nine months by a graduate student.
OAK provides exhaustive bounding box annotations of 80 video snippets (~17.5
hours) for 105 object categories in outdoor scenes. The emergence of new object
categories in our benchmark follows a pattern similar to what a single person
might see in their day-to-day life. The dataset also captures the natural
distribution shifts as the person travels to different places. These egocentric
long-running videos provide a realistic playground for continual learning
algorithms, especially in online embodied settings. We also introduce new
evaluation metrics to evaluate the model performance and catastrophic
forgetting and provide baseline studies for online continual object detection.
We believe this benchmark will pose new exciting challenges for learning from
non-stationary data in continual learning. The OAK dataset and the associated
benchmark are released at https://oakdata.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iDARTS: Improving DARTS by Node Normalization and Decorrelation Discretization. (arXiv:2108.11014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11014">
<div class="article-summary-box-inner">
<span><p>Differentiable ARchiTecture Search (DARTS) uses a continuous relaxation of
network representation and dramatically accelerates Neural Architecture Search
(NAS) by almost thousands of times in GPU-day. However, the searching process
of DARTS is unstable, which suffers severe degradation when training epochs
become large, thus limiting its application. In this paper, we claim that this
degradation issue is caused by the imbalanced norms between different nodes and
the highly correlated outputs from various operations. We then propose an
improved version of DARTS, namely iDARTS, to deal with the two problems. In the
training phase, it introduces node normalization to maintain the norm balance.
In the discretization phase, the continuous architecture is approximated based
on the similarity between the outputs of the node and the decorrelated
operations rather than the values of the architecture parameters. Extensive
evaluation is conducted on CIFAR-10 and ImageNet, and the error rates of 2.25\%
and 24.7\% are reported within 0.2 and 1.9 GPU-day for architecture search
respectively, which shows its effectiveness. Additional analysis also reveals
that iDARTS has the advantage in robustness and generalization over other
DARTS-based counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Scaling Law for Synthetic-to-Real Transfer: A Measure of Pre-Training. (arXiv:2108.11018v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11018">
<div class="article-summary-box-inner">
<span><p>Synthetic-to-real transfer learning is a framework in which we pre-train
models with synthetically generated images and ground-truth annotations for
real tasks. Although synthetic images overcome the data scarcity issue, it
remains unclear how the fine-tuning performance scales with pre-trained models,
especially in terms of pre-training data size. In this study, we collect a
number of empirical observations and uncover the secret. Through experiments,
we observe a simple and general scaling law that consistently describes
learning curves in various tasks, models, and complexities of synthesized
pre-training data. Further, we develop a theory of transfer learning for a
simplified scenario and confirm that the derived generalization bound is
consistent with our empirical findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Layer-wise Customized Weak Segmentation Block and AIoU Loss for Accurate Object Detection. (arXiv:2108.11021v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11021">
<div class="article-summary-box-inner">
<span><p>The anchor-based detectors handle the problem of scale variation by building
the feature pyramid and directly setting different scales of anchors on each
cell in different layers. However, it is difficult for box-wise anchors to
guide the adaptive learning of scale-specific features in each layer because
there is no one-to-one correspondence between box-wise anchors and pixel-level
features. In order to alleviate the problem, in this paper, we propose a
scale-customized weak segmentation (SCWS) block at the pixel level for scale
customized object feature learning in each layer. By integrating the SCWS
blocks into the single-shot detector, a scale-aware object detector (SCOD) is
constructed to detect objects of different sizes naturally and accurately.
Furthermore, the standard location loss neglects the fact that the hard and
easy samples may be seriously imbalanced. A forthcoming problem is that it is
unable to get more accurate bounding boxes due to the imbalance. To address
this problem, an adaptive IoU (AIoU) loss via a simple yet effective squeeze
operation is specified in our SCOD. Extensive experiments on PASCAL VOC and MS
COCO demonstrate the superiority of our SCOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning. (arXiv:2108.11023v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11023">
<div class="article-summary-box-inner">
<span><p>Given a set of unlabeled images or (image, text) pairs, contrastive learning
aims to pre-train an image encoder that can be used as a feature extractor for
many downstream tasks. In this work, we propose EncoderMI, the first membership
inference method against image encoders pre-trained by contrastive learning. In
particular, given an input and a black-box access to an image encoder,
EncoderMI aims to infer whether the input is in the training dataset of the
image encoder. EncoderMI can be used 1) by a data owner to audit whether its
(public) data was used to pre-train an image encoder without its authorization
or 2) by an attacker to compromise privacy of the training data when it is
private/sensitive. Our EncoderMI exploits the overfitting of the image encoder
towards its training data. In particular, an overfitted image encoder is more
likely to output more (or less) similar feature vectors for two augmented
versions of an input in (or not in) its training dataset. We evaluate EncoderMI
on image encoders pre-trained on multiple datasets by ourselves as well as the
Contrastive Language-Image Pre-training (CLIP) image encoder, which is
pre-trained on 400 million (image, text) pairs collected from the Internet and
released by OpenAI. Our results show that EncoderMI can achieve high accuracy,
precision, and recall. We also explore a countermeasure against EncoderMI via
preventing overfitting through early stopping. Our results show that it
achieves trade-offs between accuracy of EncoderMI and utility of the image
encoder, i.e., it can reduce the accuracy of EncoderMI, but it also incurs
classification accuracy loss of the downstream classifiers built based on the
image encoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Visual Quality of Unrestricted Adversarial Examples with Wavelet-VAE. (arXiv:2108.11032v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11032">
<div class="article-summary-box-inner">
<span><p>Traditional adversarial examples are typically generated by adding
perturbation noise to the input image within a small matrix norm. In practice,
un-restricted adversarial attack has raised great concern and presented a new
threat to the AI safety. In this paper, we propose a wavelet-VAE structure to
reconstruct an input image and generate adversarial examples by modifying the
latent code. Different from perturbation-based attack, the modifications of the
proposed method are not limited but imperceptible to human eyes. Experiments
show that our method can generate high quality adversarial examples on ImageNet
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NGC: A Unified Framework for Learning with Open-World Noisy Data. (arXiv:2108.11035v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11035">
<div class="article-summary-box-inner">
<span><p>The existence of noisy data is prevalent in both the training and testing
phases of machine learning systems, which inevitably leads to the degradation
of model performance. There have been plenty of works concentrated on learning
with in-distribution (IND) noisy labels in the last decade, i.e., some training
samples are assigned incorrect labels that do not correspond to their true
classes. Nonetheless, in real application scenarios, it is necessary to
consider the influence of out-of-distribution (OOD) samples, i.e., samples that
do not belong to any known classes, which has not been sufficiently explored
yet. To remedy this, we study a new problem setup, namely Learning with
Open-world Noisy Data (LOND). The goal of LOND is to simultaneously learn a
classifier and an OOD detector from datasets with mixed IND and OOD noise. In
this paper, we propose a new graph-based framework, namely Noisy Graph Cleaning
(NGC), which collects clean samples by leveraging geometric structure of data
and model predictive confidence. Without any additional training effort, NGC
can detect and reject the OOD samples based on the learned class prototypes
directly in testing phase. We conduct experiments on multiple benchmarks with
different types of noise and the results demonstrate the superior performance
of our method against state of the arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localization Uncertainty-Based Attention for Object Detection. (arXiv:2108.11042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11042">
<div class="article-summary-box-inner">
<span><p>Object detection has been applied in a wide variety of real world scenarios,
so detection algorithms must provide confidence in the results to ensure that
appropriate decisions can be made based on their results. Accordingly, several
studies have investigated the probabilistic confidence of bounding box
regression. However, such approaches have been restricted to anchor-based
detectors, which use box confidence values as additional screening scores
during non-maximum suppression (NMS) procedures. In this paper, we propose a
more efficient uncertainty-aware dense detector (UADET) that predicts
four-directional localization uncertainties via Gaussian modeling. Furthermore,
a simple uncertainty attention module (UAM) that exploits box confidence maps
is proposed to improve performance through feature refinement. Experiments
using the MS COCO benchmark show that our UADET consistently surpasses baseline
FCOS, and that our best model, ResNext-64x4d-101-DCN, obtains a single model,
single-scale AP of 48.3% on COCO test-dev, thus achieving the state-of-the-art
among various object detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Augmented Non-Local Attention for Video Super-Resolution. (arXiv:2108.11048v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11048">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel video super-resolution method that aims at
generating high-fidelity high-resolution (HR) videos from low-resolution (LR)
ones. Previous methods predominantly leverage temporal neighbor frames to
assist the super-resolution of the current frame. Those methods achieve limited
performance as they suffer from the challenge in spatial frame alignment and
the lack of useful information from similar LR neighbor frames. In contrast, we
devise a cross-frame non-local attention mechanism that allows video
super-resolution without frame alignment, leading to be more robust to large
motions in the video. In addition, to acquire the information beyond neighbor
frames, we design a novel memory-augmented attention module to memorize general
video details during the super-resolution training. Experimental results
indicate that our method can achieve superior performance on large motion
videos comparing to the state-of-the-art methods without aligning frames. Our
source code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding of Kernels in CNN Models by Suppressing Irrelevant Visual Features in Images. (arXiv:2108.11054v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11054">
<div class="article-summary-box-inner">
<span><p>Deep learning models have shown their superior performance in various vision
tasks. However, the lack of precisely interpreting kernels in convolutional
neural networks (CNNs) is becoming one main obstacle to wide applications of
deep learning models in real scenarios. Although existing interpretation
methods may find certain visual patterns which are associated with the
activation of a specific kernel, those visual patterns may not be specific or
comprehensive enough for interpretation of a specific activation of kernel of
interest. In this paper, a simple yet effective optimization method is proposed
to interpret the activation of any kernel of interest in CNN models. The basic
idea is to simultaneously preserve the activation of the specific kernel and
suppress the activation of all other kernels at the same layer. In this way,
only visual information relevant to the activation of the specific kernel is
remained in the input. Consistent visual information from multiple modified
inputs would help users understand what kind of features are specifically
associated with specific kernel. Comprehensive evaluation shows that the
proposed method can help better interpret activation of specific kernels than
widely used methods, even when two kernels have very similar activation regions
from the same input image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Normal Learning in Videos with Attention Prototype Network. (arXiv:2108.11055v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11055">
<div class="article-summary-box-inner">
<span><p>Frame reconstruction (current or future frame) based on Auto-Encoder (AE) is
a popular method for video anomaly detection. With models trained on the normal
data, the reconstruction errors of anomalous scenes are usually much larger
than those of normal ones. Previous methods introduced the memory bank into AE,
for encoding diverse normal patterns across the training videos. However, they
are memory consuming and cannot cope with unseen new scenarios in the testing
data. In this work, we propose a self-attention prototype unit (APU) to encode
the normal latent space as prototypes in real time, free from extra memory
cost. In addition, we introduce circulative attention mechanism to our backbone
to form a novel feature extracting learner, namely Circulative Attention Unit
(CAU). It enables the fast adaption capability on new scenes by only consuming
a few iterations of update. Extensive experiments are conducted on various
benchmarks. The superior performance over the state-of-the-art demonstrates the
effectiveness of our method. Our code is available at
https://github.com/huchao-AI/APN/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Class-level Prototypes for Few-shot Learning. (arXiv:2108.11072v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11072">
<div class="article-summary-box-inner">
<span><p>Few-shot learning aims to recognize new categories using very few labeled
samples. Although few-shot learning has witnessed promising development in
recent years, most existing methods adopt an average operation to calculate
prototypes, thus limited by the outlier samples. In this work, we propose a
simple yet effective framework for few-shot classification, which can learn to
generate preferable prototypes from few support data, with the help of an
episodic prototype generator module. The generated prototype is meant to be
close to a certain \textit{\targetproto{}} and is less influenced by outlier
samples. Extensive experiments demonstrate the effectiveness of this module,
and our approach gets a significant raise over baseline models, and get a
competitive result compared to previous methods on \textit{mini}ImageNet,
\textit{tiered}ImageNet, and cross-domain (\textit{mini}ImageNet $\rightarrow$
CUB-200-2011) datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heredity-aware Child Face Image Generation with Latent Space Disentanglement. (arXiv:2108.11080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11080">
<div class="article-summary-box-inner">
<span><p>Generative adversarial networks have been widely used in image synthesis in
recent years and the quality of the generated image has been greatly improved.
However, the flexibility to control and decouple facial attributes (e.g., eyes,
nose, mouth) is still limited. In this paper, we propose a novel approach,
called ChildGAN, to generate a child's image according to the images of parents
with heredity prior. The main idea is to disentangle the latent space of a
pre-trained generation model and precisely control the face attributes of child
images with clear semantics. We use distances between face landmarks as pseudo
labels to figure out the most influential semantic vectors of the corresponding
face attributes by calculating the gradient of latent vectors to pseudo labels.
Furthermore, we disentangle the semantic vectors by weighting irrelevant
features and orthogonalizing them with Schmidt Orthogonalization. Finally, we
fuse the latent vector of the parents by leveraging the disentangled semantic
vectors under the guidance of biological genetic laws. Extensive experiments
demonstrate that our approach outperforms the existing methods with encouraging
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Face Recognition: A Survey. (arXiv:2108.11082v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11082">
<div class="article-summary-box-inner">
<span><p>Face recognition is one of the most studied research topics in the community.
In recent years, the research on face recognition has shifted to using 3D
facial surfaces, as more discriminating features can be represented by the 3D
geometric information. This survey focuses on reviewing the 3D face recognition
techniques developed in the past ten years which are generally categorized into
conventional methods and deep learning methods. The categorized techniques are
evaluated using detailed descriptions of the representative works. The
advantages and disadvantages of the techniques are summarized in terms of
accuracy, complexity and robustness to face variation (expression, pose and
occlusions, etc). The main contribution of this survey is that it
comprehensively covers both conventional methods and deep learning methods on
3D face recognition. In addition, a review of available 3D face databases is
provided, along with the discussion of future research challenges and
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Transformer for Single Image Super-Resolution. (arXiv:2108.11084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11084">
<div class="article-summary-box-inner">
<span><p>Single image super-resolution task has witnessed great strides with the
development of deep learning. However, most existing studies focus on building
a more complex neural network with a massive number of layers, bringing heavy
computational cost and memory storage. Recently, as Transformer yields
brilliant results in NLP tasks, more and more researchers start to explore the
application of Transformer in computer vision tasks. But with the heavy
computational cost and high GPU memory occupation of the vision Transformer,
the network can not be designed too deep. To address this problem, we propose a
novel Efficient Super-Resolution Transformer (ESRT) for fast and accurate image
super-resolution. ESRT is a hybrid Transformer where a CNN-based SR network is
first designed in the front to extract deep features. Specifically, there are
two backbones for formatting the ESRT: lightweight CNN backbone (LCB) and
lightweight Transformer backbone (LTB). Among them, LCB is a lightweight SR
network to extract deep SR features at a low computational cost by dynamically
adjusting the size of the feature map. LTB is made up of an efficient
Transformer (ET) with a small GPU memory occupation, which benefited from the
novel efficient multi-head attention (EMHA). In EMHA, a feature split module
(FSM) is proposed to split the long sequence into sub-segments and then these
sub-segments are applied by attention operation. This module can significantly
decrease the GPU memory occupation. Extensive experiments show that our ESRT
achieves competitive results. Compared with the original Transformer which
occupies 16057M GPU memory, the proposed ET only occupies 4191M GPU memory with
better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning From Long-Tailed Data With Noisy Labels. (arXiv:2108.11096v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11096">
<div class="article-summary-box-inner">
<span><p>Class imbalance and noisy labels are the norm rather than the exception in
many large-scale classification datasets. Nevertheless, most works in machine
learning typically assume balanced and clean data. There have been some recent
attempts to tackle, on one side, the problem of learning from noisy labels and,
on the other side, learning from long-tailed data. Each group of methods make
simplifying assumptions about the other. Due to this separation, the proposed
solutions often underperform when both assumptions are violated. In this work,
we present a simple two-stage approach based on recent advances in
self-supervised learning to treat both challenges simultaneously. It consists
of, first, task-agnostic self-supervised pre-training, followed by
task-specific fine-tuning using an appropriate loss. Most significantly, we
find that self-supervised learning approaches are effectively able to cope with
severe class imbalance. In addition, the resulting learned representations are
also remarkably robust to label noise, when fine-tuned with an imbalance- and
noise-resistant loss function. We validate our claims with experiments on
CIFAR-10 and CIFAR-100 augmented with synthetic imbalance and noise, as well as
the large-scale inherently noisy Clothing-1M dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Depth Estimation Primed by Salient Point Detection and Normalized Hessian Loss. (arXiv:2108.11098v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11098">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have recently thrived on single image depth estimation.
That being said, current developments on this topic highlight an apparent
compromise between accuracy and network size. This work proposes an accurate
and lightweight framework for monocular depth estimation based on a
self-attention mechanism stemming from salient point detection. Specifically,
we utilize a sparse set of keypoints to train a FuSaNet model that consists of
two major components: Fusion-Net and Saliency-Net. In addition, we introduce a
normalized Hessian loss term invariant to scaling and shear along the depth
direction, which is shown to substantially improve the accuracy. The proposed
method achieves state-of-the-art results on NYU-Depth-v2 and KITTI while using
3.1-38.4 times smaller model in terms of the number of parameters than baseline
approaches. Experiments on the SUN-RGBD further demonstrate the
generalizability of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Attributed and Structured Text-to-Face Synthesis. (arXiv:2108.11100v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11100">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) have revolutionized image synthesis
through many applications like face generation, photograph editing, and image
super-resolution. Image synthesis using GANs has predominantly been uni-modal,
with few approaches that can synthesize images from text or other data modes.
Text-to-image synthesis, especially text-to-face synthesis, has promising use
cases of robust face-generation from eye witness accounts and augmentation of
the reading experience with visual cues. However, only a couple of datasets
provide consolidated face data and textual descriptions for text-to-face
synthesis. Moreover, these textual annotations are less extensive and
descriptive, which reduces the diversity of faces generated from it. This paper
empirically proves that increasing the number of facial attributes in each
textual description helps GANs generate more diverse and real-looking faces. To
prove this, we propose a new methodology that focuses on using structured
textual descriptions. We also consolidate a Multi-Attributed and Structured
Text-to-face (MAST) dataset consisting of high-quality images with structured
textual annotations and make it available to researchers to experiment and
build upon. Lastly, we report benchmark Frechet's Inception Distance (FID),
Facial Semantic Similarity (FSS), and Facial Semantic Distance (FSD) scores for
the MAST dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Small Objects in Thermal Images Using Single-Shot Detector. (arXiv:2108.11101v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11101">
<div class="article-summary-box-inner">
<span><p>SSD (Single Shot Multibox Detector) is one of the most successful object
detectors for its high accuracy and fast speed. However, the features from
shallow layer (mainly Conv4_3) of SSD lack semantic information, resulting in
poor performance in small objects. In this paper, we proposed DDSSD (Dilation
and Deconvolution Single Shot Multibox Detector), an enhanced SSD with a novel
feature fusion module which can improve the performance over SSD for small
object detection. In the feature fusion module, dilation convolution module is
utilized to enlarge the receptive field of features from shallow layer and
deconvolution module is adopted to increase the size of feature maps from high
layer. Our network achieves 79.7% mAP on PASCAL VOC2007 test and 28.3% mmAP on
MS COCO test-dev at 41 FPS with only 300x300 input using a single Nvidia 1080
GPU. Especially, for small objects, DDSSD achieves 10.5% on MS COCO and 22.8%
on FLIR thermal dataset, outperforming a lot of state-of-the-art object
detection algorithms in both aspects of accuracy and speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Monocular Depth with a Novel Neural Architecture Search Method. (arXiv:2108.11105v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11105">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel neural architecture search method, called LiDNAS,
for generating lightweight monocular depth estimation models. Unlike previous
neural architecture search (NAS) approaches, where finding optimized networks
are computationally highly demanding, the introduced novel Assisted Tabu Search
leads to efficient architecture exploration. Moreover, we construct the search
space on a pre-defined backbone network to balance layer diversity and search
space size. The LiDNAS method outperforms the state-of-the-art NAS approach,
proposed for disparity and depth estimation, in terms of search efficiency and
output model performance. The LiDNAS optimized models achieve results superior
to compact depth estimation state-of-the-art on NYU-Depth-v2, KITTI, and
ScanNet, while being 7%-500% more compact in size, i.e the number of model
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransFER: Learning Relation-aware Facial Expression Representations with Transformers. (arXiv:2108.11116v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11116">
<div class="article-summary-box-inner">
<span><p>Facial expression recognition (FER) has received increasing interest in
computer vision. We propose the TransFER model which can learn rich
relation-aware local representations. It mainly consists of three components:
Multi-Attention Dropping (MAD), ViT-FER, and Multi-head Self-Attention Dropping
(MSAD). First, local patches play an important role in distinguishing various
expressions, however, few existing works can locate discriminative and diverse
local patches. This can cause serious problems when some patches are invisible
due to pose variations or viewpoint changes. To address this issue, the MAD is
proposed to randomly drop an attention map. Consequently, models are pushed to
explore diverse local patches adaptively. Second, to build rich relations
between different local patches, the Vision Transformers (ViT) are used in FER,
called ViT-FER. Since the global scope is used to reinforce each local patch, a
better representation is obtained to boost the FER performance. Thirdly, the
multi-head self-attention allows ViT to jointly attend to features from
different information subspaces at different positions. Given no explicit
guidance, however, multiple self-attentions may extract similar relations. To
address this, the MSAD is proposed to randomly drop one self-attention module.
As a result, models are forced to learn rich relations among diverse local
patches. Our proposed TransFER model outperforms the state-of-the-art methods
on several FER benchmarks, showing its effectiveness and usefulness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GlassNet: Label Decoupling-based Three-stream Neural Network for Robust Image Glass Detection. (arXiv:2108.11117v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11117">
<div class="article-summary-box-inner">
<span><p>Most of the existing object detection methods generate poor glass detection
results, due to the fact that the transparent glass shares the same appearance
with arbitrary objects behind it in an image. Different from traditional deep
learning-based wisdoms that simply use the object boundary as auxiliary
supervision, we exploit label decoupling to decompose the original labeled
ground-truth (GT) map into an interior-diffusion map and a boundary-diffusion
map. The GT map in collaboration with the two newly generated maps breaks the
imbalanced distribution of the object boundary, leading to improved glass
detection quality. We have three key contributions to solve the transparent
glass detection problem: (1) We propose a three-stream neural network (call
GlassNet for short) to fully absorb beneficial features in the three maps. (2)
We design a multi-scale interactive dilation module to explore a wider range of
contextual information. (3) We develop an attention-based boundary-aware
feature Mosaic module to integrate multi-modal information. Extensive
experiments on the benchmark dataset exhibit clear improvements of our method
over SOTAs, in terms of both the overall glass detection accuracy and boundary
clearness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Product-oriented Machine Translation with Cross-modal Cross-lingual Pre-training. (arXiv:2108.11119v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11119">
<div class="article-summary-box-inner">
<span><p>Translating e-commercial product descriptions, a.k.a product-oriented machine
translation (PMT), is essential to serve e-shoppers all over the world.
However, due to the domain specialty, the PMT task is more challenging than
traditional machine translation problems. Firstly, there are many specialized
jargons in the product description, which are ambiguous to translate without
the product image. Secondly, product descriptions are related to the image in
more complicated ways than standard image descriptions, involving various
visual aspects such as objects, shapes, colors or even subjective styles.
Moreover, existing PMT datasets are small in scale to support the research. In
this paper, we first construct a large-scale bilingual product description
dataset called Fashion-MMT, which contains over 114k noisy and 40k manually
cleaned description translations with multiple product images. To effectively
learn semantic alignments among product images and bilingual texts in
translation, we design a unified product-oriented cross-modal cross-lingual
model (\upoc~) for pre-training and fine-tuning. Experiments on the Fashion-MMT
and Multi30k datasets show that our model significantly outperforms the
state-of-the-art models even pre-trained on the same dataset. It is also shown
to benefit more from large-scale noisy data to improve the translation quality.
We will release the dataset and codes at
https://github.com/syuqings/Fashion-MMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection. (arXiv:2108.11127v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11127">
<div class="article-summary-box-inner">
<span><p>Existing deep learning-based approaches for monocular 3D object detection in
autonomous driving often model the object as a rotated 3D cuboid while the
object's geometric shape has been ignored. In this work, we propose an approach
for incorporating the shape-aware 2D/3D constraints into the 3D detection
framework. Specifically, we employ the deep neural network to learn
distinguished 2D keypoints in the 2D image domain and regress their
corresponding 3D coordinates in the local 3D object coordinate first. Then the
2D/3D geometric constraints are built by these correspondences for each object
to boost the detection performance. For generating the ground truth of 2D/3D
keypoints, an automatic model-fitting approach has been proposed by fitting the
deformed 3D object model and the object mask in the 2D image. The proposed
framework has been verified on the public KITTI dataset and the experimental
results demonstrate that by using additional geometrical constraints the
detection performance has been significantly improved as compared to the
baseline method. More importantly, the proposed framework achieves
state-of-the-art performance with real time. Data and code will be available at
https://github.com/zongdai/AutoShape
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Scene Segmentation for Robotics Applications. (arXiv:2108.11128v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11128">
<div class="article-summary-box-inner">
<span><p>Semantic scene segmentation plays a critical role in a wide range of robotics
applications, e.g., autonomous navigation. These applications are accompanied
by specific computational restrictions, e.g., operation on low-power GPUs, at
sufficient speed, and also for high-resolution input. Existing state-of-the-art
segmentation models provide evaluation results under different setups and
mainly considering high-power GPUs. In this paper, we investigate the behavior
of the most successful semantic scene segmentation models, in terms of
deployment (inference) speed, under various setups (GPUs, input sizes, etc.) in
the context of robotics applications. The target of this work is to provide a
comparative study of current state-of-the-art segmentation models so as to
select the most compliant with the robotics applications requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games. (arXiv:2108.11149v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11149">
<div class="article-summary-box-inner">
<span><p>The automatic detection of events in complex sports games like soccer and
handball using positional or video data is of large interest in research and
industry. One requirement is a fundamental understanding of underlying
concepts, i.e., events that occur on the pitch. Previous work often deals only
with so-called low-level events based on well-defined rules such as free kicks,
free throws, or goals. High-level events, such as passes, are less frequently
approached due to a lack of consistent definitions. This introduces a level of
ambiguity that necessities careful validation when regarding event annotations.
Yet, this validation step is usually neglected as the majority of studies adopt
annotations from commercial providers on private datasets of unknown quality
and focuses on soccer only. To address these issues, we present (1) a universal
taxonomy that covers a wide range of low and high-level events for invasion
games and is exemplarily refined to soccer and handball, and (2) release two
multi-modal datasets comprising video and positional data with gold-standard
annotations to foster research in fine-grained and ball-centered event
spotting. Experiments on human performance demonstrate the robustness of the
proposed taxonomy, and that disagreements and ambiguities in the annotation
increase with the complexity of the event. An I3D model for video
classification is adopted for event spotting and reveals the potential for
benchmarking. Datasets are available at: https://github.com/mm4spa/eigd
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Duo-SegNet: Adversarial Dual-Views for Semi-Supervised Medical Image Segmentation. (arXiv:2108.11154v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11154">
<div class="article-summary-box-inner">
<span><p>Segmentation of images is a long-standing challenge in medical AI. This is
mainly due to the fact that training a neural network to perform image
segmentation requires a significant number of pixel-level annotated data, which
is often unavailable. To address this issue, we propose a semi-supervised image
segmentation technique based on the concept of multi-view learning. In contrast
to the previous art, we introduce an adversarial form of dual-view training and
employ a critic to formulate the learning problem in multi-view training as a
min-max problem. Thorough quantitative and qualitative evaluations on several
datasets indicate that our proposed method outperforms state-of-the-art medical
image segmentation algorithms consistently and comfortably. The code is
publicly available at https://github.com/himashi92/Duo-SegNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarially Robust One-class Novelty Detection. (arXiv:2108.11168v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11168">
<div class="article-summary-box-inner">
<span><p>One-class novelty detectors are trained with examples of a particular class
and are tasked with identifying whether a query example belongs to the same
known class. Most recent advances adopt a deep auto-encoder style architecture
to compute novelty scores for detecting novel class data. Deep networks have
shown to be vulnerable to adversarial attacks, yet little focus is devoted to
studying the adversarial robustness of deep novelty detectors. In this paper,
we first show that existing novelty detectors are susceptible to adversarial
examples. We further demonstrate that commonly-used defense approaches for
classification tasks have limited effectiveness in one-class novelty detection.
Hence, we need a defense specifically designed for novelty detection. To this
end, we propose a defense strategy that manipulates the latent space of novelty
detectors to improve the robustness against adversarial examples. The proposed
method, referred to as Principal Latent Space (PLS), learns the
incrementally-trained cascade principal components in the latent space to
robustify novelty detectors. PLS can purify latent space against adversarial
examples and constrain latent space to exclusively model the known class
distribution. We conduct extensive experiments on multiple attacks, datasets
and novelty detectors, showing that PLS consistently enhances the adversarial
robustness of novelty detection models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Superpixel-guided Discriminative Low-rank Representation of Hyperspectral Images for Classification. (arXiv:2108.11172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11172">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel classification scheme for the remotely
sensed hyperspectral image (HSI), namely SP-DLRR, by comprehensively exploring
its unique characteristics, including the local spatial information and
low-rankness. SP-DLRR is mainly composed of two modules, i.e., the
classification-guided superpixel segmentation and the discriminative low-rank
representation, which are iteratively conducted. Specifically, by utilizing the
local spatial information and incorporating the predictions from a typical
classifier, the first module segments pixels of an input HSI (or its
restoration generated by the second module) into superpixels. According to the
resulting superpixels, the pixels of the input HSI are then grouped into
clusters and fed into our novel discriminative low-rank representation model
with an effective numerical solution. Such a model is capable of increasing the
intra-class similarity by suppressing the spectral variations locally while
promoting the inter-class discriminability globally, leading to a restored HSI
with more discriminative pixels. Experimental results on three benchmark
datasets demonstrate the significant superiority of SP-DLRR over
state-of-the-art methods, especially for the case with an extremely limited
number of training pixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recall@k Surrogate Loss with Large Batches and Similarity Mixup. (arXiv:2108.11179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11179">
<div class="article-summary-box-inner">
<span><p>Direct optimization, by gradient descent, of an evaluation metric, is not
possible when it is non-differentiable, which is the case for recall in
retrieval. In this work, a differentiable surrogate loss for the recall is
proposed. Using an implementation that sidesteps the hardware constraints of
the GPU memory, the method trains with a very large batch size, which is
essential for metrics computed on the entire retrieval database. It is assisted
by an efficient mixup approach that operates on pairwise scalar similarities
and virtually increases the batch size further. When used for deep metric
learning, the proposed method achieves state-of-the-art results in several
image retrieval benchmarks. For instance-level recognition, the method
outperforms similar approaches that train using an approximation of average
precision. The implementation will be made public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lizard: A Large-Scale Dataset for Colonic Nuclear Instance Segmentation and Classification. (arXiv:2108.11195v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11195">
<div class="article-summary-box-inner">
<span><p>The development of deep segmentation models for computational pathology
(CPath) can help foster the investigation of interpretable morphological
biomarkers. Yet, there is a major bottleneck in the success of such approaches
because supervised deep learning models require an abundance of accurately
labelled data. This issue is exacerbated in the field of CPath because the
generation of detailed annotations usually demands the input of a pathologist
to be able to distinguish between different tissue constructs and nuclei.
Manually labelling nuclei may not be a feasible approach for collecting
large-scale annotated datasets, especially when a single image region can
contain thousands of different cells. However, solely relying on automatic
generation of annotations will limit the accuracy and reliability of ground
truth. Therefore, to help overcome the above challenges, we propose a
multi-stage annotation pipeline to enable the collection of large-scale
datasets for histology image analysis, with pathologist-in-the-loop refinement
steps. Using this pipeline, we generate the largest known nuclear instance
segmentation and classification dataset, containing nearly half a million
labelled nuclei in H&amp;E stained colon tissue. We have released the dataset and
encourage the research community to utilise it to drive forward the development
of downstream cell-based models in CPath.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-domain semantic segmentation with overlapping labels. (arXiv:2108.11224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11224">
<div class="article-summary-box-inner">
<span><p>Deep supervised models have an unprecedented capacity to absorb large
quantities of training data. Hence, training on many datasets becomes a method
of choice towards graceful degradation in unusual scenes. Unfortunately,
different datasets often use incompatible labels. For instance, the Cityscapes
road class subsumes all driving surfaces, while Vistas defines separate classes
for road markings, manholes etc. We address this challenge by proposing a
principled method for seamless learning on datasets with overlapping classes
based on partial labels and probabilistic loss. Our method achieves competitive
within-dataset and cross-dataset generalization, as well as ability to learn
visual concepts which are not separately labeled in any of the training
datasets. Experiments reveal competitive or state-of-the-art performance on two
multi-domain dataset collections and on the WildDash 2 benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cell Multi-Bernoulli (Cell-MB) Sensor Control for Multi-object Search-While-Tracking (SWT). (arXiv:2108.11236v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11236">
<div class="article-summary-box-inner">
<span><p>Information driven control can be used to develop intelligent sensors that
can optimize their measurement value based on environmental feedback. In object
tracking applications, sensor actions are chosen based on the expected
reduction in uncertainty also known as information gain. Random finite set
(RFS) theory provides a formalism for quantifying and estimating information
gain in multi-object tracking problems. However, estimating information gain in
these applications remains computationally challenging. This paper presents a
new tractable approximation of the RFS expected information gain applicable to
sensor control for multi-object search and tracking. Unlike existing RFS
approaches, the approximation presented in this paper accounts for noisy
measurements, missed detections, false alarms, and object
appearance/disappearance. The effectiveness of the information driven sensor
control is demonstrated through a multi-vehicle search-while-tracking
experiment using real video data from a remote optical sensor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiscale Spatio-Temporal Graph Neural Networks for 3D Skeleton-Based Motion Prediction. (arXiv:2108.11244v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11244">
<div class="article-summary-box-inner">
<span><p>We propose a multiscale spatio-temporal graph neural network (MST-GNN) to
predict the future 3D skeleton-based human poses in an action-category-agnostic
manner. The core of MST-GNN is a multiscale spatio-temporal graph that
explicitly models the relations in motions at various spatial and temporal
scales. Different from many previous hierarchical structures, our multiscale
spatio-temporal graph is built in a data-adaptive fashion, which captures
nonphysical, yet motion-based relations. The key module of MST-GNN is a
multiscale spatio-temporal graph computational unit (MST-GCU) based on the
trainable graph structure. MST-GCU embeds underlying features at individual
scales and then fuses features across scales to obtain a comprehensive
representation. The overall architecture of MST-GNN follows an encoder-decoder
framework, where the encoder consists of a sequence of MST-GCUs to learn the
spatial and temporal features of motions, and the decoder uses a graph-based
attention gate recurrent unit (GA-GRU) to generate future poses. Extensive
experiments are conducted to show that the proposed MST-GNN outperforms
state-of-the-art methods in both short and long-term motion prediction on the
datasets of Human 3.6M, CMU Mocap and 3DPW, where MST-GNN outperforms previous
works by 5.33% and 3.67% of mean angle errors in average for short-term and
long-term prediction on Human 3.6M, and by 11.84% and 4.71% of mean angle
errors for short-term and long-term prediction on CMU Mocap, and by 1.13% of
mean angle errors on 3DPW in average, respectively. We further investigate the
learned multiscale graphs for interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation. (arXiv:2108.11249v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11249">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (DA) has gained substantial interest in
semantic segmentation. However, almost all prior arts assume concurrent access
to both labeled source and unlabeled target, making them unsuitable for
scenarios demanding source-free adaptation. In this work, we enable source-free
DA by partitioning the task into two: a) source-only domain generalization and
b) source-free target adaptation. Towards the former, we provide theoretical
insights to develop a multi-head framework trained with a virtually extended
multi-source dataset, aiming to balance generalization and specificity. Towards
the latter, we utilize the multi-head framework to extract reliable target
pseudo-labels for self-training. Additionally, we introduce a novel conditional
prior-enforcing auto-encoder that discourages spatial irregularities, thereby
enhancing the pseudo-label quality. Experiments on the standard
GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes benchmarks show our superiority
even against the non-source-free prior-arts. Further, we show our compatibility
with online adaptation enabling deployment in a sequentially changing
environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11250">
<div class="article-summary-box-inner">
<span><p>A panoptic driving perception system is an essential part of autonomous
driving. A high-precision and real-time perception system can assist the
vehicle in making the reasonable decision while driving. We present a panoptic
driving perception network (YOLOP) to perform traffic object detection,
drivable area segmentation and lane detection simultaneously. It is composed of
one encoder for feature extraction and three decoders to handle the specific
tasks. Our model performs extremely well on the challenging BDD100K dataset,
achieving state-of-the-art on all three tasks in terms of accuracy and speed.
Besides, we verify the effectiveness of our multi-task learning model for joint
training via ablative studies. To our best knowledge, this is the first work
that can process these three visual perception tasks simultaneously in
real-time on an embedded device Jetson TX2(23 FPS) and maintain excellent
accuracy. To facilitate further research, the source codes and pre-trained
models will be released at https://github.com/hustvl/YOLOP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep few-shot learning for bi-temporal building change detection. (arXiv:2108.11262v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11262">
<div class="article-summary-box-inner">
<span><p>In real-world applications (e.g., change detection), annotating images is
very expensive. To build effective deep learning models in these applications,
deep few-shot learning methods have been developed and prove to be a robust
approach in small training data. The analysis of building change detection from
high spatial resolution remote sensing observations is important research in
photogrammetry, computer vision, and remote sensing nowadays, which can be
widely used in a variety of real-world applications, such as map updating. As
manual high resolution image interpretation is expensive and time-consuming,
building change detection methods are of high interest. The interest in
developing building change detection approaches from optical remote sensing
images is rapidly increasing due to larger coverages, and lower costs of
optical images. In this study, we focus on building change detection analysis
on a small set of building change from different regions that sit in several
cities. In this paper, a new deep few-shot learning method is proposed for
building change detection using Monte Carlo dropout and remote sensing
observations. The setup is based on a small dataset, including bitemporal
optical images labeled for building change detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adversarial RetinaNet as a Reference Algorithm for the MItosis DOmain Generalization (MIDOG) Challenge. (arXiv:2108.11269v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11269">
<div class="article-summary-box-inner">
<span><p>Assessing the Mitotic Count has a known high degree of intra- and inter-rater
variability. Computer-aided systems have proven to decrease this variability
and reduce labelling time. These systems, however, are generally highly
dependent on their training domain and show poor applicability to unseen
domains. In histopathology, these domain shifts can result from various
sources, including different slide scanning systems used to digitize histologic
samples. The MItosis DOmain Generalization challenge focuses on this specific
domain shift for the task of mitotic figure detection. This work presents a
mitotic figure detection algorithm developed as a baseline for the challenge,
based on domain adversarial training. On the preliminary test set, the
algorithm scores an F$_1$ score of 0.7514.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measurement of Hybrid Rocket Solid Fuel Regression Rate for a Slab Burner using Deep Learning. (arXiv:2108.11276v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11276">
<div class="article-summary-box-inner">
<span><p>This study presents an imaging-based deep learning tool to measure the fuel
regression rate in a 2D slab burner experiment for hybrid rocket fuels. The
slab burner experiment is designed to verify mechanistic models of reacting
boundary layer combustion in hybrid rockets by the measurement of fuel
regression rates. A DSLR camera with a high intensity flash is used to capture
images throughout the burn and the images are then used to find the fuel
boundary to calculate the regression rate. A U-net convolutional neural network
architecture is explored to segment the fuel from the experimental images. A
Monte-Carlo Dropout process is used to quantify the regression rate uncertainty
produced from the network. The U-net computed regression rates are compared
with values from other techniques from literature and show error less than 10%.
An oxidizer flux dependency study is performed and shows the U-net predictions
of regression rates are accurate and independent of the oxidizer flux, when the
images in the training set are not over-saturated. Training with monochrome
images is explored and is not successful at predicting the fuel regression rate
from images with high noise. The network is superior at filtering out noise
introduced by soot, pitting, and wax deposition on the chamber glass as well as
the flame when compared to traditional image processing techniques, such as
threshold binary conversion and spatial filtering. U-net consistently provides
low error image segmentations to allow accurate computation of the regression
rate of the fuel.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Feature Highlighting in Noisy RES Data With CycleGAN. (arXiv:2108.11283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11283">
<div class="article-summary-box-inner">
<span><p>Radio echo sounding (RES) is a common technique used in subsurface glacial
imaging, which provides insight into the underlying rock and ice. However,
systematic noise is introduced into the data during collection, complicating
interpretation of the results. Researchers most often use a combination of
manual interpretation and filtering techniques to denoise data; however, these
processes are time intensive and inconsistent. Fully Convolutional Networks
have been proposed as an automated alternative to identify layer boundaries in
radargrams. However, they require high-quality manually processed training data
and struggle to interpolate data in noisy samples (Varshney et al. 2020).
</p>
<p>Herein, the authors propose a GAN based model to interpolate layer boundaries
through noise and highlight layers in two-dimensional glacial RES data. In
real-world noisy images, filtering often results in loss of data such that
interpolating layer boundaries is nearly impossible. Furthermore, traditional
machine learning approaches are not suited to this task because of the lack of
paired data, so we employ an unpaired image-to-image translation model. For
this model, we create a synthetic dataset to represent the domain of images
with clear, highlighted layers and use an existing real-world RES dataset as
our noisy domain.
</p>
<p>We implement a CycleGAN trained on these two domains to highlight layers in
noisy images that can interpolate effectively without significant loss of
structure or fidelity. Though the current implementation is not a perfect
solution, the model clearly highlights layers in noisy data and allows
researchers to determine layer size and position without mathematical
filtering, manual processing, or ground-truth images for training. This is
significant because clean images generated by our model enable subsurface
researchers to determine glacial layer thickness more efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Non-Homogeneous Atmospheric Scattering Modeling with Convolutional Neural Networks for Single Image Dehazing. (arXiv:2108.11292v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11292">
<div class="article-summary-box-inner">
<span><p>In recent years, single image dehazing models (SIDM) based on atmospheric
scattering model (ASM) have achieved remarkable results. However, it is noted
that ASM-based SIDM degrades its performance in dehazing real world hazy images
due to the limited modelling ability of ASM where the atmospheric light factor
(ALF) and the angular scattering coefficient (ASC) are assumed as constants for
one image. Obviously, the hazy images taken in real world cannot always satisfy
this assumption. Such generating modelling mismatch between the real-world
images and ASM sets up the upper bound of trained ASM-based SIDM for dehazing.
Bearing this in mind, in this study, a new fully non-homogeneous atmospheric
scattering model (FNH-ASM) is proposed for well modeling the hazy images under
complex conditions where ALF and ASC are pixel dependent. However, FNH-ASM
brings difficulty in practical application. In FNH-ASM based SIDM, the
estimation bias of parameters at different positions lead to different
distortion of dehazing result. Hence, in order to reduce the influence of
parameter estimation bias on dehazing results, two new cost sensitive loss
functions, beta-Loss and D-Loss, are innovatively developed for limiting the
parameter bias of sensitive positions that have a greater impact on the
dehazing result. In the end, based on FNH-ASM, an end-to-end CNN-based dehazing
network, FNHD-Net, is developed, which applies beta-Loss and D-Loss.
Experimental results demonstrate the effectiveness and superiority of our
proposed FNHD-Net for dehazing on both synthetic and real-world images. And the
performance improvement of our method increases more obviously in dense and
heterogeneous haze scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Attacks on Network Certification via Data Poisoning. (arXiv:2108.11299v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11299">
<div class="article-summary-box-inner">
<span><p>Certifiers for neural networks have made great progress towards provable
robustness guarantees against evasion attacks using adversarial examples.
However, introducing certifiers into deep learning systems also opens up new
attack vectors, which need to be considered before deployment. In this work, we
conduct the first systematic analysis of training time attacks against
certifiers in practical application pipelines, identifying new threat vectors
that can be exploited to degrade the overall system. Using these insights, we
design two backdoor attacks against network certifiers, which can drastically
reduce certified robustness when the backdoor is activated. For example, adding
1% poisoned data points during training is sufficient to reduce certified
robustness by up to 95 percentage points, effectively rendering the certifier
useless. We analyze how such novel attacks can compromise the overall system's
integrity or availability. Our extensive experiments across multiple datasets,
model architectures, and certifiers demonstrate the wide applicability of these
attacks. A first investigation into potential defenses shows that current
approaches only partially mitigate the issue, highlighting the need for new,
more specific solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSG-Stump: A Learning Friendly CSG-Like Representation for Interpretable Shape Parsing. (arXiv:2108.11305v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11305">
<div class="article-summary-box-inner">
<span><p>Generating an interpretable and compact representation of 3D shapes from
point clouds is an important and challenging problem. This paper presents
CSG-Stump Net, an unsupervised end-to-end network for learning shapes from
point clouds and discovering the underlying constituent modeling primitives and
operations as well. At the core is a three-level structure called {\em
CSG-Stump}, consisting of a complement layer at the bottom, an intersection
layer in the middle, and a union layer at the top. CSG-Stump is proven to be
equivalent to CSG in terms of representation, therefore inheriting the
interpretable, compact and editable nature of CSG while freeing from CSG's
complex tree structures. Particularly, the CSG-Stump has a simple and regular
structure, allowing neural networks to give outputs of a constant
dimensionality, which makes itself deep-learning friendly. Due to these
characteristics of CSG-Stump, CSG-Stump Net achieves superior results compared
to previous CSG-based methods and generates much more appealing shapes, as
confirmed by extensive experiments. Project page:
https://kimren227.github.io/projects/CSGStump/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Self-Training for Learning General Representations. (arXiv:2108.11353v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11353">
<div class="article-summary-box-inner">
<span><p>Despite the fast progress in training specialized models for various tasks,
learning a single general model that works well for many tasks is still
challenging for computer vision. Here we introduce multi-task self-training
(MuST), which harnesses the knowledge in independent specialized teacher models
(e.g., ImageNet model on classification) to train a single general student
model. Our approach has three steps. First, we train specialized teachers
independently on labeled datasets. We then use the specialized teachers to
label an unlabeled dataset to create a multi-task pseudo labeled dataset.
Finally, the dataset, which now contains pseudo labels from teacher models
trained on different datasets/tasks, is then used to train a student model with
multi-task learning. We evaluate the feature representations of the student
model on 6 vision tasks including image recognition (classification, detection,
segmentation)and 3D geometry estimation (depth and surface normal estimation).
MuST is scalable with unlabeled or partially labeled datasets and outperforms
both specialized supervised models and self-supervised models when training on
large scale datasets. Lastly, we show MuST can improve upon already strong
checkpoints trained with billions of examples. The results suggest
self-training is a promising direction to aggregate labeled and unlabeled
training data for learning general feature representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blind Image Decomposition. (arXiv:2108.11364v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11364">
<div class="article-summary-box-inner">
<span><p>We present and study a novel task named Blind Image Decomposition (BID),
which requires separating a superimposed image into constituent underlying
images in a blind setting, that is, both the source components involved in
mixing as well as the mixing mechanism are unknown. For example, rain may
consist of multiple components, such as rain streaks, raindrops, snow, and
haze. Rainy images can be treated as an arbitrary combination of these
components, some of them or all of them. How to decompose superimposed images,
like rainy images, into distinct source components is a crucial step towards
real-world vision systems. To facilitate research on this new task, we
construct three benchmark datasets, including mixed image decomposition across
multiple domains, real-scenario deraining, and joint
shadow/reflection/watermark removal. Moreover, we propose a simple yet general
Blind Image Decomposition Network (BIDeN) to serve as a strong baseline for
future work. Experimental results demonstrate the tenability of our benchmarks
and the effectiveness of BIDeN. Code and project page are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDCGen: Cross-Domain Conditional Generation via Normalizing Flows and Adversarial Training. (arXiv:2108.11368v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11368">
<div class="article-summary-box-inner">
<span><p>How to generate conditional synthetic data for a domain without utilizing
information about its labels/attributes? Our work presents a solution to the
above question. We propose a transfer learning-based framework utilizing
normalizing flows, coupled with both maximum-likelihood and adversarial
training. We model a source domain (labels available) and a target domain
(labels unavailable) with individual normalizing flows, and perform domain
alignment to a common latent space using adversarial discriminators. Due to the
invertible property of flow models, the mapping has exact cycle consistency. We
also learn the joint distribution of the data samples and attributes in the
source domain by employing an encoder to map attributes to the latent space via
adversarial training. During the synthesis phase, given any combination of
attributes, our method can generate synthetic samples conditioned on them in
the target domain. Empirical studies confirm the effectiveness of our method on
benchmarked datasets. We envision our method to be particularly useful for
synthetic data generation in label-scarce systems by generating non-trivial
augmentations via attribute transformations. These synthetic samples will
introduce more entropy into the label-scarce domain than their geometric and
photometric transformation counterparts, helpful for robust downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Semi: A Meta-learning Approach for Semi-supervised Learning. (arXiv:2007.02394v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.02394">
<div class="article-summary-box-inner">
<span><p>Deep learning based semi-supervised learning (SSL) algorithms have led to
promising results in recent years. However, they tend to introduce multiple
tunable hyper-parameters, making them less practical in real SSL scenarios
where the labeled data is scarce for extensive hyper-parameter search. In this
paper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that
requires tuning only one additional hyper-parameter, compared with a standard
supervised deep learning algorithm, to achieve competitive performance under
various conditions of SSL. We start by defining a meta optimization problem
that minimizes the loss on labeled data through dynamically reweighting the
loss on unlabeled samples, which are associated with soft pseudo labels during
training. As the meta problem is computationally intensive to solve directly,
we propose an efficient algorithm to dynamically obtain the approximate
solutions. We show theoretically that Meta-Semi converges to the stationary
point of the loss function on labeled data under mild conditions. Empirically,
Meta-Semi outperforms state-of-the-art SSL algorithms significantly on the
challenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves
competitive performance on CIFAR-10 and SVHN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03060">
<div class="article-summary-box-inner">
<span><p>A key challenge in training neural networks for a given medical imaging task
is often the difficulty of obtaining a sufficient number of manually labeled
examples. In contrast, textual imaging reports, which are often readily
available in medical records, contain rich but unstructured interpretations
written by experts as part of standard clinical practice. We propose using
these textual reports as a form of weak supervision to improve the image
interpretation performance of a neural network without requiring additional
manually labeled examples. We use an image-text matching task to train a
feature extractor and then fine-tune it in a transfer learning setting for a
supervised task using a small labeled dataset. The end result is a neural
network that automatically interprets imagery without requiring textual reports
during inference. This approach can be applied to any task for which text-image
pairs are readily available. We evaluate our method on three classification
tasks and find consistent performance improvements, reducing the need for
labeled data by 67%-98%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Feature Disentangling for Fine-Grained Few-Shot Classification. (arXiv:2010.03255v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03255">
<div class="article-summary-box-inner">
<span><p>Fine-grained few-shot recognition often suffers from the problem of training
data scarcity for novel categories.The network tends to overfit and does not
generalize well to unseen classes due to insufficient training data. Many
methods have been proposed to synthesize additional data to support the
training. In this paper, we focus one enlarging the intra-class variance of the
unseen class to improve few-shot classification performance. We assume that the
distribution of intra-class variance generalizes across the base class and the
novel class. Thus, the intra-class variance of the base set can be transferred
to the novel set for feature augmentation. Specifically, we first model the
distribution of intra-class variance on the base set via variational inference.
Then the learned distribution is transferred to the novel set to generate
additional features, which are used together with the original ones to train a
classifier. Experimental results show a significant boost over the
state-of-the-art methods on the challenging fine-grained few-shot image
classification benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Improved and Interpretable Deep Metric Learning via Attentive Grouping. (arXiv:2011.08877v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08877">
<div class="article-summary-box-inner">
<span><p>Grouping has been commonly used in deep metric learning for computing diverse
features. However, current methods are prone to overfitting and lack
interpretability. In this work, we propose an improved and interpretable
grouping method to be integrated flexibly with any metric learning framework.
Our method is based on the attention mechanism with a learnable query for each
group. The query is fully trainable and can capture group-specific information
when combined with the diversity loss. An appealing property of our method is
that it naturally lends itself interpretability. The attention scores between
the learnable query and each spatial position can be interpreted as the
importance of that position. We formally show that our proposed grouping method
is invariant to spatial permutations of features. When used as a module in
convolutional neural networks, our method leads to translational invariance. We
conduct comprehensive experiments to evaluate our method. Our quantitative
results indicate that the proposed method outperforms prior methods
consistently and significantly across different datasets, evaluation metrics,
base models, and loss functions. For the first time to the best of our
knowledge, our interpretation results clearly demonstrate that the proposed
method enables the learning of distinct and diverse features across groups. The
code is available on https://github.com/XinyiXuXD/DGML-master.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Monocular Depth Estimation with Lightweight 3D Point Fusion. (arXiv:2012.10296v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10296">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose enhancing monocular depth estimation by adding 3D
points as depth guidance. Unlike existing depth completion methods, our
approach performs well on extremely sparse and unevenly distributed point
clouds, which makes it agnostic to the source of the 3D points. We achieve this
by introducing a novel multi-scale 3D point fusion network that is both
lightweight and efficient. We demonstrate its versatility on two different
depth estimation problems where the 3D points have been acquired with
conventional structure-from-motion and LiDAR. In both cases, our network
performs on par with state-of-the-art depth completion methods and achieves
significantly higher accuracy when only a small number of points is used while
being more compact in terms of the number of parameters. We show that our
method outperforms some contemporary deep learning based multi-view stereo and
structure-from-motion methods both in accuracy and in compactness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Augmented Reinforcement Learning for Image-Goal Navigation. (arXiv:2101.05181v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05181">
<div class="article-summary-box-inner">
<span><p>In this work, we present a memory-augmented approach for image-goal
navigation. Earlier attempts, including RL-based and SLAM-based approaches have
either shown poor generalization performance, or are heavily-reliant on
pose/depth sensors. Our method uses an attention-based end-to-end model that
leverages an episodic memory to learn to navigate. First, we train a
state-embedding network in a self-supervised fashion, and then use it to embed
previously-visited states into the agent's memory. Our navigation policy takes
advantage of this information through an attention mechanism. We validate our
approach with extensive evaluations, and show that our model establishes a new
state of the art on the challenging Gibson dataset. Furthermore, we achieve
this impressive performance from RGB input alone, without access to additional
information such as position or depth, in stark contrast to related work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coupling innovation method and feasibility analysis of garbage classification. (arXiv:2102.00193v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00193">
<div class="article-summary-box-inner">
<span><p>In order to solve the recent defect in garbage classification - including low
level of intelligence, low accuracy and high cost of equipment, this paper
presents a series of methods in identification and judgment in intelligent
garbage classification, including a material identification based on thermal
principle and non-destructive laser irradiation, another material
identification based on optical diffraction and phase analysis, a profile
identification which utilizes a scenery thermal image after PCA and histogram
correction, another profile identification which utilizes computer vision with
innovated data sets and algorithms. Combining AHP and Bayesian formula, the
paper innovates a coupling algorithm which helps to make a comprehensive
judgment of the garbage sort, based on the material and profile identification.
This paper also proposes a method for real-time space measurement of garbage
cans, which based on the characteristics of air as fluid, and analyses the
functions of air cleaning and particle disposing. Instead of the single use of
garbage image recognition, this paper provides a comprehensive method to judge
the garbage sort by material and profile identifications, which greatly
enhancing the accuracy and intelligence in garbage classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance and Panoptic Segmentation Using Conditional Convolutions. (arXiv:2102.03026v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03026">
<div class="article-summary-box-inner">
<span><p>We propose a simple yet effective framework for instance and panoptic
segmentation, termed CondInst (conditional convolutions for instance and
panoptic segmentation). In the literature, top-performing instance segmentation
methods typically follow the paradigm of Mask R-CNN and rely on ROI operations
(typically ROIAlign) to attend to each instance. In contrast, we propose to
attend to the instances with dynamic conditional convolutions. Instead of using
instance-wise ROIs as inputs to the instance mask head of fixed weights, we
design dynamic instance-aware mask heads, conditioned on the instances to be
predicted. CondInst enjoys three advantages: 1.) Instance and panoptic
segmentation are unified into a fully convolutional network, eliminating the
need for ROI cropping and feature alignment. 2.) The elimination of the ROI
cropping also significantly improves the output instance mask resolution. 3.)
Due to the much improved capacity of dynamically-generated conditional
convolutions, the mask head can be very compact (e.g., 3 conv. layers, each
having only 8 channels), leading to significantly faster inference time per
instance and making the overall inference time almost constant, irrelevant to
the number of instances. We demonstrate a simpler method that can achieve
improved accuracy and inference speed on both instance and panoptic
segmentation tasks. On the COCO dataset, we outperform a few state-of-the-art
methods. We hope that CondInst can be a strong baseline for instance and
panoptic segmentation. Code is available at: https://git.io/AdelaiDet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Telling the What while Pointing to the Where: Multimodal Queries for Image Retrieval. (arXiv:2102.04980v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04980">
<div class="article-summary-box-inner">
<span><p>Most existing image retrieval systems use text queries as a way for the user
to express what they are looking for. However, fine-grained image retrieval
often requires the ability to also express where in the image the content they
are looking for is. The text modality can only cumbersomely express such
localization preferences, whereas pointing is a more natural fit. In this
paper, we propose an image retrieval setup with a new form of multimodal
queries, where the user simultaneously uses both spoken natural language (the
what) and mouse traces over an empty canvas (the where) to express the
characteristics of the desired target image. We then describe simple
modifications to an existing image retrieval model, enabling it to operate in
this setup. Qualitative and quantitative experiments show that our model
effectively takes this spatial guidance into account, and provides
significantly more accurate retrieval results compared to text-only equivalent
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigate Indistinguishable Points in Semantic Segmentation of 3D Point Cloud. (arXiv:2103.10339v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10339">
<div class="article-summary-box-inner">
<span><p>This paper investigates the indistinguishable points (difficult to predict
label) in semantic segmentation for large-scale 3D point clouds. The
indistinguishable points consist of those located in complex boundary, points
with similar local textures but different categories, and points in isolate
small hard areas, which largely harm the performance of 3D semantic
segmentation. To address this challenge, we propose a novel Indistinguishable
Area Focalization Network (IAF-Net), which selects indistinguishable points
adaptively by utilizing the hierarchical semantic features and enhances
fine-grained features for points especially those indistinguishable points. We
also introduce multi-stage loss to improve the feature representation in a
progressive way. Moreover, in order to analyze the segmentation performances of
indistinguishable areas, we propose a new evaluation metric called
Indistinguishable Points Based Metric (IPBM). Our IAF-Net achieves the
comparable results with state-of-the-art performance on several popular 3D
point cloud datasets e.g. S3DIS and ScanNet, and clearly outperforms other
methods on IPBM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation. (arXiv:2104.04167v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04167">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language Navigation (VLN) requires an agent to find a path to a
remote location on the basis of natural-language instructions and a set of
photo-realistic panoramas. Most existing methods take the words in the
instructions and the discrete views of each panorama as the minimal unit of
encoding. However, this requires a model to match different nouns (e.g., TV,
table) against the same input view feature. In this work, we propose an
object-informed sequential BERT to encode visual perceptions and linguistic
instructions at the same fine-grained level, namely objects and words. Our
sequential BERT also enables the visual-textual clues to be interpreted in
light of the temporal context, which is crucial to multi-round VLN tasks.
Additionally, we enable the model to identify the relative direction (e.g.,
left/right/front/back) of each navigable location and the room type (e.g.,
bedroom, kitchen) of its current and final navigation goal, as such information
is widely mentioned in instructions implying the desired next and final
locations. We thus enable the model to know-where the objects lie in the
images, and to know-where they stand in the scene. Extensive experiments
demonstrate the effectiveness compared against several state-of-the-art methods
on three indoor VLN tasks: REVERIE, NDH, and R2R. Project repository:
https://github.com/YuankaiQi/ORIST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Baseline for Semi-supervised Semantic Segmentation with Strong Data Augmentation. (arXiv:2104.07256v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07256">
<div class="article-summary-box-inner">
<span><p>Recently, significant progress has been made on semantic segmentation.
However, the success of supervised semantic segmentation typically relies on a
large amount of labelled data, which is time-consuming and costly to obtain.
Inspired by the success of semi-supervised learning methods in image
classification, here we propose a simple yet effective semi-supervised learning
framework for semantic segmentation. We demonstrate that the devil is in the
details: a set of simple design and training techniques can collectively
improve the performance of semi-supervised semantic segmentation significantly.
Previous works [3, 27] fail to employ strong augmentation in pseudo label
learning efficiently, as the large distribution change caused by strong
augmentation harms the batch normalisation statistics. We design a new batch
normalisation, namely distribution-specific batch normalisation (DSBN) to
address this problem and demonstrate the importance of strong augmentation for
semantic segmentation. Moreover, we design a self correction loss which is
effective in noise resistance. We conduct a series of ablation studies to show
the effectiveness of each component. Our method achieves state-of-the-art
results in the semi-supervised settings on the Cityscapes and Pascal VOC
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comprehensive Multi-Modal Interactions for Referring Image Segmentation. (arXiv:2104.10412v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10412">
<div class="article-summary-box-inner">
<span><p>We investigate Referring Image Segmentation (RIS), which outputs a
segmentation map corresponding to the given natural language description. To
solve RIS efficiently, we need to understand each word's relationship with
other words, each region in the image to other regions, and cross-modal
alignment between linguistic and visual domains. We argue that one of the
limiting factors in the recent methods is that they do not handle these
interactions simultaneously. To this end, we propose a novel architecture
called JRNet, which uses a Joint Reasoning Module(JRM) to concurrently capture
the inter-modal and intra-modal interactions. The output of JRM is passed
through a novel Cross-Modal Multi-Level Fusion (CMMLF) module which further
refines the segmentation masks by exchanging contextual information across
visual hierarchy through linguistic features acting as a bridge. We present
thorough ablation studies and validate our approach's performance on four
benchmark datasets, showing considerable performance gains over the existing
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pri3D: Can 3D Priors Help 2D Representation Learning?. (arXiv:2104.11225v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11225">
<div class="article-summary-box-inner">
<span><p>Recent advances in 3D perception have shown impressive progress in
understanding geometric structures of 3Dshapes and even scenes. Inspired by
these advances in geometric understanding, we aim to imbue image-based
perception with representations learned under geometric constraints. We
introduce an approach to learn view-invariant,geometry-aware representations
for network pre-training, based on multi-view RGB-D data, that can then be
effectively transferred to downstream 2D tasks. We propose to employ
contrastive learning under both multi-view im-age constraints and
image-geometry constraints to encode3D priors into learned 2D representations.
This results not only in improvement over 2D-only representation learning on
the image-based tasks of semantic segmentation, instance segmentation, and
object detection on real-world in-door datasets, but moreover, provides
significant improvement in the low data regime. We show a significant
improvement of 6.0% on semantic segmentation on full data as well as 11.9% on
20% data against baselines on ScanNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Explanations for Model Inversion Attacks. (arXiv:2104.12669v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12669">
<div class="article-summary-box-inner">
<span><p>The successful deployment of artificial intelligence (AI) in many domains
from healthcare to hiring requires their responsible use, particularly in model
explanations and privacy. Explainable artificial intelligence (XAI) provides
more information to help users to understand model decisions, yet this
additional knowledge exposes additional risks for privacy attacks. Hence,
providing explanation harms privacy. We study this risk for image-based model
inversion attacks and identified several attack architectures with increasing
performance to reconstruct private image data from model explanations. We have
developed several multi-modal transposed CNN architectures that achieve
significantly higher inversion performance than using the target model
prediction only. These XAI-aware inversion models were designed to exploit the
spatial knowledge in image explanations. To understand which explanations have
higher privacy risk, we analyzed how various explanation types and factors
influence inversion performance. In spite of some models not providing
explanations, we further demonstrate increased inversion performance even for
non-explainable target models by exploiting explanations of surrogate models
through attention transfer. This method first inverts an explanation from the
target prediction, then reconstructs the target image. These threats highlight
the urgent and significant privacy risks of explanations and calls attention
for new privacy preservation techniques that balance the dual-requirement for
AI explainability and privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Feature Decorrelation in Self-Supervised Learning. (arXiv:2105.00470v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00470">
<div class="article-summary-box-inner">
<span><p>In self-supervised representation learning, a common idea behind most of the
state-of-the-art approaches is to enforce the robustness of the representations
to predefined augmentations. A potential issue of this idea is the existence of
completely collapsed solutions (i.e., constant features), which are typically
avoided implicitly by carefully chosen implementation details. In this work, we
study a relatively concise framework containing the most common components from
recent approaches. We verify the existence of complete collapse and discover
another reachable collapse pattern that is usually overlooked, namely
dimensional collapse. We connect dimensional collapse with strong correlations
between axes and consider such connection as a strong motivation for feature
decorrelation (i.e., standardizing the covariance matrix). The gains from
feature decorrelation are verified empirically to highlight the importance and
the potential of this insight.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SparseConvMIL: Sparse Convolutional Context-Aware Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2105.02726v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02726">
<div class="article-summary-box-inner">
<span><p>Multiple instance learning (MIL) is the preferred approach for whole slide
image classification. However, most MIL approaches do not exploit the
interdependencies of tiles extracted from a whole slide image, which could
provide valuable cues for classification. This paper presents a novel MIL
approach that exploits the spatial relationship of tiles for classifying whole
slide images. To do so, a sparse map is built from tiles embeddings, and is
then classified by a sparse-input CNN. It obtained state-of-the-art performance
over popular MIL approaches on the classification of cancer subtype involving
10000 whole slide images. Our results suggest that the proposed approach might
(i) improve the representation learning of instances and (ii) exploit the
context of instance embeddings to enhance the classification performance. The
code of this work is open-source at {github censored for review}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Episodic Transformer for Vision-and-Language Navigation. (arXiv:2105.06453v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06453">
<div class="article-summary-box-inner">
<span><p>Interaction and navigation defined by natural language instructions in
dynamic environments pose significant challenges for neural agents. This paper
focuses on addressing two challenges: handling long sequence of subtasks, and
understanding complex human instructions. We propose Episodic Transformer
(E.T.), a multimodal transformer that encodes language inputs and the full
episode history of visual observations and actions. To improve training, we
leverage synthetic instructions as an intermediate representation that
decouples understanding the visual appearance of an environment from the
variations of natural language instructions. We demonstrate that encoding the
history with a transformer is critical to solve compositional tasks, and that
pretraining and joint training with synthetic instructions further improve the
performance. Our approach sets a new state of the art on the challenging ALFRED
benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test
splits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physically Plausible Pose Refinement using Fully Differentiable Forces. (arXiv:2105.08196v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08196">
<div class="article-summary-box-inner">
<span><p>All hand-object interaction is controlled by forces that the two bodies exert
on each other, but little work has been done in modeling these underlying
forces when doing pose and contact estimation from RGB/RGB-D data. Given the
pose of the hand and object from any pose estimation system, we propose an
end-to-end differentiable model that refines pose estimates by learning the
forces experienced by the object at each vertex in its mesh. By matching the
learned net force to an estimate of net force based on finite differences of
position, this model is able to find forces that accurately describe the
movement of the object, while resolving issues like mesh interpenetration and
lack of contact. Evaluating on the ContactPose dataset, we show this model
successfully corrects poses and finds contact maps that better match the ground
truth, despite not using any RGB or depth image data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenMatch: Open-set Consistency Regularization for Semi-supervised Learning with Outliers. (arXiv:2105.14148v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14148">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) is an effective means to leverage unlabeled
data to improve a model's performance. Typical SSL methods like FixMatch assume
that labeled and unlabeled data share the same label space. However, in
practice, unlabeled data can contain categories unseen in the labeled set,
i.e., outliers, which can significantly harm the performance of SSL algorithms.
To address this problem, we propose a novel Open-set Semi-Supervised Learning
(OSSL) approach called OpenMatch. Learning representations of inliers while
rejecting outliers is essential for the success of OSSL. To this end, OpenMatch
unifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers.
The OVA-classifier outputs the confidence score of a sample being an inlier,
providing a threshold to detect outliers. Another key contribution is an
open-set soft-consistency regularization loss, which enhances the smoothness of
the OVA-classifier with respect to input transformations and greatly improves
outlier detection. OpenMatch achieves state-of-the-art performance on three
datasets, and even outperforms a fully supervised model in detecting outliers
unseen in unlabeled data on CIFAR10.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Domain Adaptation in Ordinal Regression. (arXiv:2106.11576v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11576">
<div class="article-summary-box-inner">
<span><p>We address the problem of universal domain adaptation (UDA) in ordinal
regression (OR), which attempts to solve classification problems in which
labels are not independent, but follow a natural order. We show that the UDA
techniques developed for classification and based on the clustering assumption,
under-perform in OR settings. We propose a method that complements the OR
classifier with an auxiliary task of order learning, which plays the double
role of discriminating between common and private instances, and expanding
class labels to the private target images via ranking. Combined with
adversarial domain discrimination, our model is able to address the closed set,
partial and open set configurations. We evaluate our method on three face age
estimation datasets, and show that it outperforms the baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Object-oriented Spatio-Temporal Reasoning for Video Question Answering. (arXiv:2106.13432v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13432">
<div class="article-summary-box-inner">
<span><p>Video Question Answering (Video QA) is a powerful testbed to develop new AI
capabilities. This task necessitates learning to reason about objects,
relations, and events across visual and linguistic domains in space-time.
High-level reasoning demands lifting from associative visual pattern
recognition to symbol-like manipulation over objects, their behavior and
interactions. Toward reaching this goal we propose an object-oriented reasoning
approach in that video is abstracted as a dynamic stream of interacting
objects. At each stage of the video event flow, these objects interact with
each other, and their interactions are reasoned about with respect to the query
and under the overall context of a video. This mechanism is materialized into a
family of general-purpose neural units and their multi-level architecture
called Hierarchical Object-oriented Spatio-Temporal Reasoning (HOSTR) networks.
This neural model maintains the objects' consistent lifelines in the form of a
hierarchically nested spatio-temporal graph. Within this graph, the dynamic
interactive object-oriented representations are built up along the video
sequence, hierarchically abstracted in a bottom-up manner, and converge toward
the key information for the correct answer. The method is evaluated on multiple
major Video QA datasets and establishes new state-of-the-arts in these tasks.
Analysis into the model's behavior indicates that object-oriented reasoning is
a reliable, interpretable and efficient approach to Video QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-N-Out: Towards Good Initialization for Inpainting and Outpainting. (arXiv:2106.13953v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13953">
<div class="article-summary-box-inner">
<span><p>In computer vision, recovering spatial information by filling in masked
regions, e.g., inpainting, has been widely investigated for its usability and
wide applicability to other various applications: image inpainting, image
extrapolation, and environment map estimation. Most of them are studied
separately depending on the applications. Our focus, however, is on
accommodating the opposite task, e.g., image outpainting, which would benefit
the target applications, e.g., image inpainting. Our self-supervision method,
In-N-Out, is summarized as a training approach that leverages the knowledge of
the opposite task into the target model. We empirically show that In-N-Out --
which explores the complementary information -- effectively takes advantage
over the traditional pipelines where only task-specific learning takes place in
training. In experiments, we compare our method to the traditional procedure
and analyze the effectiveness of our method on different applications: image
inpainting, image extrapolation, and environment map estimation. For these
tasks, we demonstrate that In-N-Out consistently improves the performance of
the recent works with In-N-Out self-supervision to their training procedure.
Also, we show that our approach achieves better results than an existing
training approach for outpainting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval. (arXiv:2107.06256v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06256">
<div class="article-summary-box-inner">
<span><p>We present Retrieve in Style (RIS), an unsupervised framework for facial
feature transfer and retrieval on real images. Recent work shows capabilities
of transferring local facial features by capitalizing on the disentanglement
property of the StyleGAN latent space. RIS improves existing art on the
following: 1) Introducing more effective feature disentanglement to allow for
challenging transfers (ie, hair, pose) that were not shown possible in SoTA
methods. 2) Eliminating the need for per-image hyperparameter tuning, and for
computing a catalog over a large batch of images. 3) Enabling fine-grained face
retrieval using disentangled facial features (eg, eyes). To our best knowledge,
this is the first work to retrieve face images at this fine level. 4)
Demonstrating robust, natural editing on real images. Our qualitative and
quantitative analyses show RIS achieves both high-fidelity feature transfers
and accurate fine-grained retrievals on real images. We also discuss the
responsible applications of RIS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back Projection Augmentation. (arXiv:2107.08543v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08543">
<div class="article-summary-box-inner">
<span><p>Domain shift is one of the most salient challenges in medical computer
vision. Due to immense variability in scanners' parameters and imaging
protocols, even images obtained from the same person and the same scanner could
differ significantly. We address variability in computed tomography (CT) images
caused by different convolution kernels used in the reconstruction process, the
critical domain shift factor in CT. The choice of a convolution kernel affects
pixels' granularity, image smoothness, and noise level. We analyze a dataset of
paired CT images, where smooth and sharp images were reconstructed from the
same sinograms with different kernels, thus providing identical anatomy but
different style. Though identical predictions are desired, we show that the
consistency, measured as the average Dice between predictions on pairs, is just
0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and
surprisingly efficient approach to augment CT images in sinogram space
emulating reconstruction with different kernels. We apply the proposed method
in a zero-shot domain adaptation setup and show that the consistency boosts
from 0.54 to 0.92 outperforming other augmentation approaches. Neither specific
preparation of source domain data nor target domain data is required, so our
publicly released FBPAug can be used as a plug-and-play module for zero-shot
domain adaptation in any CT-based task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-Temporal Semantic Reasoning for the Semantic Change Detection of HR Remote Sensing Images. (arXiv:2108.06103v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06103">
<div class="article-summary-box-inner">
<span><p>Semantic change detection (SCD) extends the multi-class change detection
(MCD) task to provide not only the change locations but also the detailed
land-cover/land-use (LC/LU) categories before and after the observation
intervals. This fine-grained semantic change information is very useful in many
applications. Recent studies indicate that the SCD can be modeled through a
triple-branch Convolutional Neural Network (CNN), which contains two temporal
branches and a change branch. However, in this architecture, the communications
between the temporal branches and the change branch are in-sufficient. To
overcome the limitations in existing methods, we propose a novel CNN
architecture for the SCD, where the semantic temporal features are merged in a
deep CD unit. Furthermore, we elaborate on this architecture to reason the
bi-temporal semantic correlations. The resulting Bi-temporal Semantic Reasoning
Network (Bi-SRNet) contains two types of semantic reasoning blocks to reason
both single-temporal and cross-temporal semantic correlations, as well as a
novel loss function to improve the semantic consistency of change detection
results. Experimental results on a benchmark dataset show that the proposed
architecture obtains significant accuracy improvements over the existing
approaches, while the added designs in the Bi-SRNet further improves the
segmentation of both semantic categories and the changed areas. The codes in
this paper are accessible at: https://github.com/ggsDing/Bi-SRNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Activity Recognition for Autism Diagnosis. (arXiv:2108.07917v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07917">
<div class="article-summary-box-inner">
<span><p>A formal autism diagnosis is an inefficient and lengthy process. Families
often have to wait years before receiving a diagnosis for their child; some may
not receive one at all due to this delay. One approach to this problem is to
use digital technologies to detect the presence of behaviors related to autism,
which in aggregate may lead to remote and automated diagnostics. One of the
strongest indicators of autism is stimming, which is a set of repetitive,
self-stimulatory behaviors such as hand flapping, headbanging, and spinning.
Using computer vision to detect hand flapping is especially difficult due to
the sparsity of public training data in this space and excessive shakiness and
motion in such data. Our work demonstrates a novel method that overcomes these
issues: we use hand landmark detection over time as a feature representation
which is then fed into a Long Short-Term Memory (LSTM) model. We achieve a
validation accuracy and F1 Score of about 72% on detecting whether videos from
the Self-Stimulatory Behaviour Dataset (SSBD) contain hand flapping or not. Our
best model also predicts accurately on external videos we recorded of ourselves
outside of the dataset it was trained on. This model uses less than 26,000
parameters, providing promise for fast deployment into ubiquitous and wearable
digital settings for a remote autism diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Early-exit deep neural networks for distorted images: providing an efficient edge offloading. (arXiv:2108.09343v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09343">
<div class="article-summary-box-inner">
<span><p>Edge offloading for deep neural networks (DNNs) can be adaptive to the
input's complexity by using early-exit DNNs. These DNNs have side branches
throughout their architecture, allowing the inference to end earlier in the
edge. The branches estimate the accuracy for a given input. If this estimated
accuracy reaches a threshold, the inference ends on the edge. Otherwise, the
edge offloads the inference to the cloud to process the remaining DNN layers.
However, DNNs for image classification deals with distorted images, which
negatively impact the branches' estimated accuracy. Consequently, the edge
offloads more inferences to the cloud. This work introduces expert side
branches trained on a particular distortion type to improve robustness against
image distortion. The edge detects the distortion type and selects appropriate
expert branches to perform the inference. This approach increases the estimated
accuracy on the edge, improving the offloading decisions. We validate our
proposal in a realistic scenario, in which the edge offloads DNN inference to
Amazon EC2 instances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SERF: Towards better training of deep neural networks using log-Softplus ERror activation Function. (arXiv:2108.09598v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09598">
<div class="article-summary-box-inner">
<span><p>Activation functions play a pivotal role in determining the training dynamics
and neural network performance. The widely adopted activation function ReLU
despite being simple and effective has few disadvantages including the Dying
ReLU problem. In order to tackle such problems, we propose a novel activation
function called Serf which is self-regularized and nonmonotonic in nature. Like
Mish, Serf also belongs to the Swish family of functions. Based on several
experiments on computer vision (image classification and object detection) and
natural language processing (machine translation, sentiment classification and
multimodal entailment) tasks with different state-of-the-art architectures, it
is observed that Serf vastly outperforms ReLU (baseline) and other activation
functions including both Swish and Mish, with a markedly bigger margin on
deeper architectures. Ablation studies further demonstrate that Serf based
architectures perform better than those of Swish and Mish in varying scenarios,
validating the effectiveness and compatibility of Serf with varying depth,
complexity, optimizers, learning rates, batch sizes, initializers and dropout
rates. Finally, we investigate the mathematical relation between Swish and
Serf, thereby showing the impact of preconditioner function ingrained in the
first derivative of Serf which provides a regularization effect making
gradients smoother and optimization faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 2nd Anti-UAV Workshop & Challenge: Methods and Results. (arXiv:2108.09909v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09909">
<div class="article-summary-box-inner">
<span><p>The 2nd Anti-UAV Workshop \&amp; Challenge aims to encourage research in
developing novel and accurate methods for multi-scale object tracking. The
Anti-UAV dataset used for the Anti-UAV Challenge has been publicly released.
There are two subsets in the dataset, $i.e.$, the test-dev subset and
test-challenge subset. Both subsets consist of 140 thermal infrared video
sequences, spanning multiple occurrences of multi-scale UAVs. Around 24
participating teams from the globe competed in the 2nd Anti-UAV Challenge. In
this paper, we provide a brief summary of the 2nd Anti-UAV Workshop \&amp;
Challenge including brief introductions to the top three methods.The submission
leaderboard will be reopened for researchers that are interested in the
Anti-UAV challenge. The benchmark dataset and other information can be found
at: https://anti-uav.github.io/.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-08-26 23:02:22.390170256 UTC">2021-08-26 23:02:22 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>