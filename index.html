<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-21T01:30:00Z">04-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Named Entity Recognition for Partially Annotated Datasets. (arXiv:2204.09081v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09081">
<div class="article-summary-box-inner">
<span><p>The most common Named Entity Recognizers are usually sequence taggers trained
on fully annotated corpora, i.e. the class of all words for all entities is
known. Partially annotated corpora, i.e. some but not all entities of some
types are annotated, are too noisy for training sequence taggers since the same
entity may be annotated one time with its true type but not another time,
misleading the tagger. Therefore, we are comparing three training strategies
for partially annotated datasets and an approach to derive new datasets for new
classes of entities from Wikipedia without time-consuming manual data
annotation. In order to properly verify that our data acquisition and training
approaches are plausible, we manually annotated test datasets for two new
classes, namely food and drugs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimize_Prime@DravidianLangTech-ACL2022: Emotion Analysis in Tamil. (arXiv:2204.09087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09087">
<div class="article-summary-box-inner">
<span><p>This paper aims to perform an emotion analysis of social media comments in
Tamil. Emotion analysis is the process of identifying the emotional context of
the text. In this paper, we present the findings obtained by Team
Optimize_Prime in the ACL 2022 shared task "Emotion Analysis in Tamil." The
task aimed to classify social media comments into categories of emotion like
Joy, Anger, Trust, Disgust, etc. The task was further divided into two
subtasks, one with 11 broad categories of emotions and the other with 31
specific categories of emotion. We implemented three different approaches to
tackle this problem: transformer-based models, Recurrent Neural Networks
(RNNs), and Ensemble models. XLM-RoBERTa performed the best on the first task
with a macro-averaged f1 score of 0.27, while MuRIL provided the best results
on the second task with a macro-averaged f1 score of 0.13.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PICT@DravidianLangTech-ACL2022: Neural Machine Translation On Dravidian Languages. (arXiv:2204.09098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09098">
<div class="article-summary-box-inner">
<span><p>This paper presents a summary of the findings that we obtained based on the
shared task on machine translation of Dravidian languages. We stood first in
three of the five sub-tasks which were assigned to us for the main shared task.
We carried out neural machine translation for the following five language
pairs: Kannada to Tamil, Kannada to Telugu, Kannada to Malayalam, Kannada to
Sanskrit, and Kannada to Tulu. The datasets for each of the five language pairs
were used to train various translation models, including Seq2Seq models such as
LSTM, bidirectional LSTM, Conv2Seq, and training state-of-the-art as
transformers from scratch, and fine-tuning already pre-trained models. For some
models involving monolingual corpora, we implemented backtranslation as well.
These models' accuracy was later tested with a part of the same dataset using
BLEU score as an evaluation metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Multi-hop Question Answering and Generation. (arXiv:2204.09140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09140">
<div class="article-summary-box-inner">
<span><p>The problem of Question Answering (QA) has attracted significant research
interest for long. Its relevance to language understanding and knowledge
retrieval tasks, along with the simple setting makes the task of QA crucial for
strong AI systems. Recent success on simple QA tasks has shifted the focus to
more complex settings. Among these, Multi-Hop QA (MHQA) is one of the most
researched tasks over the recent years. The ability to answer multi-hop
questions and perform multi step reasoning can significantly improve the
utility of NLP systems. Consequently, the field has seen a sudden surge with
high quality datasets, models and evaluation strategies. The notion of
`multiple hops' is somewhat abstract which results in a large variety of tasks
that require multi-hop reasoning. This implies that different datasets and
models differ significantly which makes the field challenging to generalize and
survey. This work aims to provide a general and formal definition of MHQA task,
and organize and summarize existing MHQA frameworks. We also outline the best
methods to create MHQA datasets. The paper provides a systematic and thorough
introduction as well as the structuring of the existing attempts to this highly
interesting, yet quite challenging task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALBETO and DistilBETO: Lightweight Spanish Language Models. (arXiv:2204.09145v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09145">
<div class="article-summary-box-inner">
<span><p>In recent years there have been considerable advances in pre-trained language
models, where non-English language versions have also been made available. Due
to their increasing use, many lightweight versions of these models (with
reduced parameters) have also been released to speed up training and inference
times. However, versions of these lighter models (e.g., ALBERT, DistilBERT) for
languages other than English are still scarce. In this paper we present ALBETO
and DistilBETO, which are versions of ALBERT and DistilBERT pre-trained
exclusively on Spanish corpora. We train several versions of ALBETO ranging
from 5M to 223M parameters and one of DistilBETO with 67M parameters. We
evaluate our models in the GLUES benchmark that includes various natural
language understanding tasks in Spanish. The results show that our lightweight
models achieve competitive results to those of BETO (Spanish-BERT) despite
having fewer parameters. More specifically, our larger ALBETO model outperforms
all other models on the MLDoc, PAWS-X, XNLI, MLQA, SQAC and XQuAD datasets.
However, BETO remains unbeaten for POS and NER. As a further contribution, all
models are publicly available to the community for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment. (arXiv:2204.09148v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09148">
<div class="article-summary-box-inner">
<span><p>The instruction learning paradigm -- where a model learns to perform new
tasks from task descriptions alone -- has become popular in general-purpose
model research. The capabilities of large transformer models as instruction
learners, however, remain poorly understood. We use a controlled synthetic
environment to characterize such capabilities. Specifically, we use the task of
deciding whether a given string matches a regular expression (viewed as an
instruction) to identify properties of tasks, instructions, and instances that
make instruction learning challenging. For instance, we find that our model, a
fine-tuned T5-based text2text transformer, struggles with large regular
languages, suggesting that less precise instructions are challenging for
models. Additionally, instruction executions that require tracking longer
contexts of prior steps are also more difficult. We use our findings to
systematically construct a challenging instruction learning dataset, which we
call Hard RegSet. Fine-tuning on Hard RegSet, our large transformer learns to
correctly interpret only 65.6% of test instructions (with at least 90%
accuracy), and 11%-24% of the instructions in out-of-distribution
generalization settings. We propose Hard RegSet as a challenging instruction
learning task, and a controlled environment for studying instruction learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation. (arXiv:2204.09149v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09149">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue generation is challenging since the underlying
knowledge is often dynamic and effectively incorporating knowledge into the
learning process is hard. It is particularly challenging to generate both
human-like and informative responses in this setting. Recent research primarily
focused on various knowledge distillation methods where the underlying
relationship between the facts in a knowledge base is not effectively captured.
In this paper, we go one step further and demonstrate how the structural
information of a knowledge graph can improve the system's inference
capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue
system that effectively incorporates knowledge into a language model. Our
proposed system views relational knowledge as a knowledge graph and introduces
(1) a structure-aware knowledge embedding technique, and (2) a knowledge
graph-weighted attention masking strategy to facilitate the system selecting
relevant information during the dialogue generation. An empirical evaluation
demonstrates the effectiveness of DialoKG over state-of-the-art methods on
several standard benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Gender Representation in Multilingual Models. (arXiv:2204.09168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09168">
<div class="article-summary-box-inner">
<span><p>Multilingual language models were shown to allow for nontrivial transfer
across scripts and languages. In this work, we study the structure of the
internal representations that enable this transfer. We focus on the
representation of gender distinctions as a practical case study, and examine
the extent to which the gender concept is encoded in shared subspaces across
different languages. Our analysis shows that gender representations consist of
several prominent components that are shared across languages, alongside
language-specific components. The existence of language-independent and
language-specific components provides an explanation for an intriguing
empirical observation we make: while gender classification transfers well
across languages, interventions for gender removal, trained on a single
language, do not transfer easily to others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Representation Collapse of Sparse Mixture of Experts. (arXiv:2204.09179v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09179">
<div class="article-summary-box-inner">
<span><p>Sparse mixture of experts provides larger model capacity while requiring a
constant computational overhead. It employs the routing mechanism to distribute
input tokens to the best-matched experts according to their hidden
representations. However, learning such a routing mechanism encourages token
clustering around expert centroids, implying a trend toward representation
collapse. In this work, we propose to estimate the routing scores between
tokens and experts on a low-dimensional hypersphere. We conduct extensive
experiments on cross-lingual language model pre-training and fine-tuning on
downstream tasks. Experimental results across seven multilingual benchmarks
show that our method achieves consistent gains. We also present a comprehensive
analysis on the representation and routing behaviors of our models. Our method
alleviates the representation collapse issue and achieves more consistent
routing than the baseline mixture-of-experts methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Who Is Missing? Characterizing the Participation of Different Demographic Groups in a Korean Nationwide Daily Conversation Corpus. (arXiv:2204.09209v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09209">
<div class="article-summary-box-inner">
<span><p>A conversation corpus is essential to build interactive AI applications.
However, the demographic information of the participants in such corpora is
largely underexplored mainly due to the lack of individual data in many
corpora. In this work, we analyze a Korean nationwide daily conversation corpus
constructed by the National Institute of Korean Language (NIKL) to characterize
the participation of different demographic (age and sex) groups in the corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LingYi: Medical Conversational Question Answering System based on Multi-modal Knowledge Graphs. (arXiv:2204.09220v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09220">
<div class="article-summary-box-inner">
<span><p>The medical conversational system can relieve the burden of doctors and
improve the efficiency of healthcare, especially during the pandemic. This
paper presents a medical conversational question answering (CQA) system based
on the multi-modal knowledge graph, namely "LingYi", which is designed as a
pipeline framework to maintain high flexibility. Our system utilizes automated
medical procedures including medical triage, consultation, image-text drug
recommendation and record. To conduct knowledge-grounded dialogues with
patients, we first construct a Chinese Medical Multi-Modal Knowledge Graph
(CM3KG) and collect a large-scale Chinese Medical CQA (CMCQA) dataset. Compared
with the other existing medical question-answering systems, our system adopts
several state-of-the-art technologies including medical entity disambiguation
and medical dialogue generation, which is more friendly to provide medical
services to patients. In addition, we have open-sourced our codes which contain
back-end models and front-end web pages at https://github.com/WENGSYX/LingYi.
The datasets including CM3KG at https://github.com/WENGSYX/CM3KG and CMCQA at
https://github.com/WENGSYX/CMCQA are also released to further promote future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-LITE: Learning Transferable Visual Models with External Knowledge. (arXiv:2204.09222v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09222">
<div class="article-summary-box-inner">
<span><p>Recent state-of-the-art computer vision systems are trained from natural
language supervision, ranging from simple object category names to descriptive
captions. This free form of supervision ensures high generality and usability
of the learned visual models, based on extensive heuristics on data collection
to cover as many visual concepts as possible. Alternatively, learning with
external knowledge about images is a promising way which leverages a much more
structured source of supervision. In this paper, we propose K-LITE
(Knowledge-augmented Language-Image Training and Evaluation), a simple strategy
to leverage external knowledge to build transferable visual systems: In
training, it enriches entities in natural language with WordNet and Wiktionary
knowledge, leading to an efficient and scalable approach to learning image
representations that can understand both visual concepts and their knowledge;
In evaluation, the natural language is also augmented with external knowledge
and then used to reference learned visual concepts (or describe new ones) to
enable zero-shot and few-shot transfer of the pre-trained models. We study the
performance of K-LITE on two important computer vision problems, image
classification and object detection, benchmarking on 20 and 13 different
existing datasets, respectively. The proposed knowledge-augmented models show
significant improvement in transfer learning performance over existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-stitched Multi-modal Encoders. (arXiv:2204.09227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09227">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel architecture for multi-modal speech and
text input. We combine pretrained speech and text encoders using multi-headed
cross-modal attention and jointly fine-tune on the target problem. The
resultant architecture can be used for continuous token-level classification or
utterance-level prediction acting on simultaneous text and speech. The
resultant encoder efficiently captures both acoustic-prosodic and lexical
information. We compare the benefits of multi-headed attention-based fusion for
multi-modal utterance-level classification against a simple concatenation of
pre-pooled, modality-specific representations. Our model architecture is
compact, resource efficient, and can be trained on a single consumer GPU card.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Semantics and Inference System for Temporal Order based on Japanese CCG. (arXiv:2204.09245v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09245">
<div class="article-summary-box-inner">
<span><p>Natural Language Inference (NLI) is the task of determining whether a premise
entails a hypothesis. NLI with temporal order is a challenging task because
tense and aspect are complex linguistic phenomena involving interactions with
temporal adverbs and temporal connectives. To tackle this, temporal and
aspectual inference has been analyzed in various ways in the field of formal
semantics. However, a Japanese NLI system for temporal order based on the
analysis of formal semantics has not been sufficiently developed. We present a
logic-based NLI system that considers temporal order in Japanese based on
compositional semantics via Combinatory Categorial Grammar (CCG) syntactic
analysis. Our system performs inference involving temporal order by using
axioms for temporal relations and automated theorem provers. We evaluate our
system by experimenting with Japanese NLI datasets that involve temporal order.
We show that our system outperforms previous logic-based systems as well as
current deep learning-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Target Domain Supervision for Open Retrieval QA. (arXiv:2204.09248v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09248">
<div class="article-summary-box-inner">
<span><p>Neural passage retrieval is a new and promising approach in open retrieval
question answering. In this work, we stress-test the Dense Passage Retriever
(DPR) -- a state-of-the-art (SOTA) open domain neural retrieval model -- on
closed and specialized target domains such as COVID-19, and find that it lags
behind standard BM25 in this important real-world setting. To make DPR more
robust under domain shift, we explore its fine-tuning with synthetic training
examples, which we generate from unlabeled target domain text using a
text-to-text generator. In our experiments, this noisy but fully automated
target domain supervision gives DPR a sizable advantage over BM25 in
out-of-domain settings, making it a more viable model in practice. Finally, an
ensemble of BM25 and our improved DPR model yields the best results, further
pushing the SOTA for open retrieval QA on multiple out-of-domain test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation. (arXiv:2204.09259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09259">
<div class="article-summary-box-inner">
<span><p>Domain Adaptation (DA) of Neural Machine Translation (NMT) model often relies
on a pre-trained general NMT model which is adapted to the new domain on a
sample of in-domain parallel data. Without parallel data, there is no way to
estimate the potential benefit of DA, nor the amount of parallel samples it
would require. It is however a desirable functionality that could help MT
practitioners to make an informed decision before investing resources in
dataset creation. We propose a Domain adaptation Learning Curve prediction
(DaLC) model that predicts prospective DA performance based on in-domain
monolingual samples in the source language. Our model relies on the NMT encoder
representations combined with various instance and corpus-level features. We
demonstrate that instance-level is better able to distinguish between different
domains compared to corpus-level frameworks proposed in previous studies.
Finally, we perform in-depth analyses of the results highlighting the
limitations of our approach, and provide directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-based Cross-Modal Retrieval with Probabilistic Representations. (arXiv:2204.09268v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09268">
<div class="article-summary-box-inner">
<span><p>Probabilistic embeddings have proven useful for capturing polysemous word
meanings, as well as ambiguity in image matching. In this paper, we study the
advantages of probabilistic embeddings in a cross-modal setting (i.e., text and
images), and propose a simple approach that replaces the standard vector point
embeddings in extant image-text matching models with probabilistic
distributions that are parametrically learned. Our guiding hypothesis is that
the uncertainty encoded in the probabilistic embeddings captures the
cross-modal ambiguity in the input instances, and that it is through capturing
this uncertainty that the probabilistic models can perform better at downstream
tasks, such as image-to-text or text-to-image retrieval. Through extensive
experiments on standard and new benchmarks, we show a consistent advantage for
probabilistic representations in cross-modal retrieval, and validate the
ability of our embeddings to capture uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond. (arXiv:2204.09269v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09269">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive (NAR) generation, which is first proposed in neural
machine translation (NMT) to speed up inference, has attracted much attention
in both machine learning and natural language processing communities. While NAR
generation can significantly accelerate inference speed for machine
translation, the speedup comes at the cost of sacrificed translation accuracy
compared to its counterpart, auto-regressive (AR) generation. In recent years,
many new models and algorithms have been designed/proposed to bridge the
accuracy gap between NAR generation and AR generation. In this paper, we
conduct a systematic survey with comparisons and discussions of various
non-autoregressive translation (NAT) models from different aspects.
Specifically, we categorize the efforts of NAT into several groups, including
data manipulation, modeling methods, training criterion, decoding algorithms,
and the benefit from pre-trained models. Furthermore, we briefly review other
applications of NAR models beyond machine translation, such as dialogue
generation, text summarization, grammar error correction, semantic parsing,
speech synthesis, and automatic speech recognition. In addition, we also
discuss potential directions for future exploration, including releasing the
dependency of KD, dynamic length prediction, pre-training for NAR, and wider
applications, etc. We hope this survey can help researchers capture the latest
progress in NAR generation, inspire the design of advanced NAR models and
algorithms, and enable industry practitioners to choose appropriate solutions
for their applications. The web page of this survey is at
\url{https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Situational Perception Guided Image Matting. (arXiv:2204.09276v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09276">
<div class="article-summary-box-inner">
<span><p>Most automatic matting methods try to separate the salient foreground from
the background. However, the insufficient quantity and subjective bias of the
current existing matting datasets make it difficult to fully explore the
semantic association between object-to-object and object-to-environment in a
given image. In this paper, we propose a Situational Perception Guided Image
Matting (SPG-IM) method that mitigates subjective bias of matting annotations
and captures sufficient situational perception information for better global
saliency distilled from the visual-to-textual task. SPG-IM can better associate
inter-objects and object-to-environment saliency, and compensate the subjective
nature of image matting and its expensive annotation. We also introduce a
textual Semantic Transformation (TST) module that can effectively transform and
integrate the semantic feature stream to guide the visual representations. In
addition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed
to adaptively switch multi-scale receptive fields and focal points to enhance
both global and local details. Extensive experiments demonstrate the
effectiveness of situational perception guidance from the visual-to-textual
tasks on image matting, and our model outperforms the state-of-the-art methods.
We also analyze the significance of different components in our model. The code
will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Arabic Sentence Simplification via Classification and Generative Approaches. (arXiv:2204.09292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09292">
<div class="article-summary-box-inner">
<span><p>This paper presents an attempt to build a Modern Standard Arabic (MSA)
sentence-level simplification system. We experimented with sentence
simplification using two approaches: (i) a classification approach leading to
lexical simplification pipelines which use Arabic-BERT, a pre-trained
contextualised model, as well as a model of fastText word embeddings; and (ii)
a generative approach, a Seq2Seq technique by applying a multilingual
Text-to-Text Transfer Transformer mT5. We developed our training corpus by
aligning the original and simplified sentences from the internationally
acclaimed Arabic novel "Saaq al-Bambuu". We evaluate effectiveness of these
methods by comparing the generated simple sentences to the target simple
sentences using the BERTScore evaluation metric. The simple sentences produced
by the mT5 model achieve P 0.72, R 0.68 and F-1 0.70 via BERTScore, while,
combining Arabic-BERT and fastText achieves P 0.97, R 0.97 and F-1 0.97. In
addition, we report a manual error analysis for these experiments.
\url{https://github.com/Nouran-Khallaf/Lexical_Simplification}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Few-Shot Learning with FASL. (arXiv:2204.09347v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09347">
<div class="article-summary-box-inner">
<span><p>Recent advances in natural language processing (NLP) have led to strong text
classification models for many tasks. However, still often thousands of
examples are needed to train models with good quality. This makes it
challenging to quickly develop and deploy new models for real world problems
and business needs. Few-shot learning and active learning are two lines of
research, aimed at tackling this problem. In this work, we combine both lines
into FASL, a platform that allows training text classification models using an
iterative and fast process. We investigate which active learning methods work
best in our few-shot setup. Additionally, we develop a model to predict when to
stop annotating. This is relevant as in a few-shot setup we do not have access
to a large validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative or Contrastive? Phrase Reconstruction for Better Sentence Representation Learning. (arXiv:2204.09358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09358">
<div class="article-summary-box-inner">
<span><p>Though offering amazing contextualized token-level representations, current
pre-trained language models actually take less attention on acquiring
sentence-level representation during its self-supervised pre-training. If
self-supervised learning can be distinguished into two subcategories,
generative and contrastive, then most existing studies show that sentence
representation learning may more benefit from the contrastive methods but not
the generative methods. However, contrastive learning cannot be well compatible
with the common token-level generative self-supervised learning, and does not
guarantee good performance on downstream semantic retrieval tasks. Thus, to
alleviate such obvious inconveniences, we instead propose a novel generative
self-supervised learning objective based on phrase reconstruction. Empirical
studies show that our generative learning may yield powerful enough sentence
representation and achieve performance in Sentence Textual Similarity (STS)
tasks on par with contrastive learning. Further, in terms of unsupervised
setting, our generative method outperforms previous state-of-the-art SimCSE on
the benchmark of downstream semantic retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Intensity of Complaints on Social Media. (arXiv:2204.09366v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09366">
<div class="article-summary-box-inner">
<span><p>Complaining is a speech act that expresses a negative inconsistency between
reality and human expectations. While prior studies mostly focus on identifying
the existence or the type of complaints, in this work, we present the first
study in computational linguistics of measuring the intensity of complaints
from text. Analyzing complaints from such perspective is particularly useful,
as complaints of certain degrees may cause severe consequences for companies or
organizations. We create the first Chinese dataset containing 3,103 posts about
complaints from Weibo, a popular Chinese social media platform. These posts are
then annotated with complaints intensity scores using Best-Worst Scaling (BWS)
method. We show that complaints intensity can be accurately estimated by
computational models with the best mean square error achieving 0.11.
Furthermore, we conduct a comprehensive linguistic analysis around complaints,
including the connections between complaints and sentiment, and a cross-lingual
comparison for complaints expressions used by Chinese and English speakers. We
finally show that our complaints intensity scores can be incorporated for
better estimating the popularity of posts on social media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is BERT Robust to Label Noise? A Study on Learning with Noisy Labels in Text Classification. (arXiv:2204.09371v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09371">
<div class="article-summary-box-inner">
<span><p>Incorrect labels in training data occur when human annotators make mistakes
or when the data is generated via weak or distant supervision. It has been
shown that complex noise-handling techniques - by modeling, cleaning or
filtering the noisy instances - are required to prevent models from fitting
this label noise. However, we show in this work that, for text classification
tasks with modern NLP models like BERT, over a variety of noise types, existing
noisehandling methods do not always improve its performance, and may even
deteriorate it, suggesting the need for further investigation. We also back our
observations with a comprehensive analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploration strategies for articulatory synthesis of complex syllable onsets. (arXiv:2204.09381v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09381">
<div class="article-summary-box-inner">
<span><p>High-quality articulatory speech synthesis has many potential applications in
speech science and technology. However, developing appropriate mappings from
linguistic specification to articulatory gestures is difficult and time
consuming. In this paper we construct an optimisation-based framework as a
first step towards learning these mappings without manual intervention. We
demonstrate the production of syllables with complex onsets and discuss the
quality of the articulatory gestures with reference to coarticulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Are What You Write: Preserving Privacy in the Era of Large Language Models. (arXiv:2204.09391v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09391">
<div class="article-summary-box-inner">
<span><p>Large scale adoption of large language models has introduced a new era of
convenient knowledge transfer for a slew of natural language processing tasks.
However, these models also run the risk of undermining user trust by exposing
unwanted information about the data subjects, which may be extracted by a
malicious party, e.g. through adversarial attacks. We present an empirical
investigation into the extent of the personal information encoded into
pre-trained representations by a range of popular models, and we show a
positive correlation between the complexity of a model, the amount of data used
in pre-training, and data leakage. In this paper, we present the first wide
coverage evaluation and comparison of some of the most popular
privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment
analysis annotated with demographic information (location, age and gender). The
results show since larger and more complex models are more prone to leaking
private information, use of privacy-preserving methods is highly desirable. We
also find that highly privacy-preserving technologies like differential privacy
(DP) can have serious model utility effects, which can be ameliorated using
hybrid or metric-DP techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Corpus for Understanding and Generating Moral Stories. (arXiv:2204.09438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09438">
<div class="article-summary-box-inner">
<span><p>Teaching morals is one of the most important purposes of storytelling. An
essential ability for understanding and writing moral stories is bridging story
plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge
about abstract concepts in morals, (2) capturing inter-event discourse
relations in stories, and (3) aligning value preferences of stories and morals
concerning good or bad behavior. In this paper, we propose two understanding
tasks and two generation tasks to assess these abilities of machines. We
present STORAL, a new dataset of Chinese and English human-written moral
stories. We show the difficulty of the proposed tasks by testing various models
with automatic and manual evaluation on STORAL. Furthermore, we present a
retrieval-augmented algorithm that effectively exploits related concepts or
events in training sets as additional guidance to improve performance on these
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Transition Planning for Open-ended Text Generation. (arXiv:2204.09453v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09453">
<div class="article-summary-box-inner">
<span><p>Open-ended text generation tasks, such as dialogue generation and story
completion, require models to generate a coherent continuation given limited
preceding context. The open-ended nature of these tasks brings new challenges
to the neural auto-regressive text generators nowadays. Despite these neural
models are good at producing human-like text, it is difficult for them to
arrange causalities and relations between given facts and possible ensuing
events. To bridge this gap, we propose a novel two-stage method which
explicitly arranges the ensuing events in open-ended text generation. Our
approach can be understood as a specially-trained coarse-to-fine algorithm,
where an event transition planner provides a "coarse" plot skeleton and a text
generator in the second stage refines the skeleton. Experiments on two
open-ended text generation tasks demonstrate that our proposed method
effectively improves the quality of the generated text, especially in coherence
and diversity. The code is available at:
\url{https://github.com/qtli/EventPlanforTextGen}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Ranking and Aggregation of Label Descriptions for Zero-Shot Classifiers. (arXiv:2204.09481v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09481">
<div class="article-summary-box-inner">
<span><p>Zero-shot text classifiers based on label descriptions embed an input text
and a set of labels into the same space: measures such as cosine similarity can
then be used to select the most similar label description to the input text as
the predicted label. In a true zero-shot setup, designing good label
descriptions is challenging because no development set is available. Inspired
by the literature on Learning with Disagreements, we look at how probabilistic
models of repeated rating analysis can be used for selecting the best label
descriptions in an unsupervised fashion. We evaluate our method on a set of
diverse datasets and tasks (sentiment, topic and stance). Furthermore, we show
that multiple, noisy label descriptions can be aggregated to boost the
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing to the Future: Mitigating Entity Bias in Fake News Detection. (arXiv:2204.09484v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09484">
<div class="article-summary-box-inner">
<span><p>The wide dissemination of fake news is increasingly threatening both
individuals and society. Fake news detection aims to train a model on the past
news and detect fake news of the future. Though great efforts have been made,
existing fake news detection methods overlooked the unintended entity bias in
the real-world data, which seriously influences models' generalization ability
to future data. For example, 97\% of news pieces in 2010-2017 containing the
entity `Donald Trump' are real in our data, but the percentage falls down to
merely 33\% in 2018. This would lead the model trained on the former set to
hardly generalize to the latter, as it tends to predict news pieces about
`Donald Trump' as real for lower training loss. In this paper, we propose an
entity debiasing framework (\textbf{ENDEF}) which generalizes fake news
detection models to the future data by mitigating entity bias from a
cause-effect perspective. Based on the causal graph among entities, news
contents, and news veracity, we separately model the contribution of each cause
(entities and contents) during training. In the inference stage, we remove the
direct effect of the entities to mitigate entity bias. Extensive offline
experiments on the English and Chinese datasets demonstrate that the proposed
framework can largely improve the performance of base fake news detectors, and
online tests verify its superiority in practice. To the best of our knowledge,
this is the first work to explicitly improve the generalization ability of fake
news detection models to the future data. The code has been released at
https://github.com/ICTMCG/ENDEF-SIGIR2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Neural Abstractive Summarization Methods and Factual Consistency of Summarization. (arXiv:2204.09519v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09519">
<div class="article-summary-box-inner">
<span><p>Automatic summarization is the process of shortening a set of textual data
computationally, to create a subset (a summary) that represents the most
important pieces of information in the original text. Existing summarization
methods can be roughly divided into two types: extractive and abstractive. An
extractive summarizer explicitly selects text snippets (words, phrases,
sentences, etc.) from the source document, while an abstractive summarizer
generates novel text snippets to convey the most salient concepts prevalent in
the source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Impact Model Narratives from Social Services' Text. (arXiv:2204.09557v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09557">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) is an important task in narration extraction.
Narration, as a system of stories, provides insights into how events and
characters in the stories develop over time. This paper proposes an
architecture for NER on a corpus about social purpose organizations. This is
the first NER task specifically targeted at social service entities. We show
how this approach can be used for the sequencing of services and impacted
clients with information extracted from unstructured text. The methodology
outlines steps for extracting ontological representation of entities such as
needs and satisfiers and generating hypotheses to answer queries about impact
models defined by social purpose organizations. We evaluate the model on a
corpus of social service descriptions with empirically calculated score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-view Brain Decoding. (arXiv:2204.09564v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09564">
<div class="article-summary-box-inner">
<span><p>How the brain captures the meaning of linguistic stimuli across multiple
views is still a critical open question in neuroscience. Consider three
different views of the concept apartment: (1) picture (WP) presented with the
target word label, (2) sentence (S) using the target word, and (3) word cloud
(WC) containing the target word along with other semantically related words.
Unlike previous efforts, which focus only on single view analysis, in this
paper, we study the effectiveness of brain decoding in a zero-shot cross-view
learning setup. Further, we propose brain decoding in the novel context of
cross-view-translation tasks like image captioning (IC), image tagging (IT),
keyword extraction (KE), and sentence formation (SF). Using extensive
experiments, we demonstrate that cross-view zero-shot brain decoding is
practical leading to ~0.68 average pairwise accuracy across view pairs. Also,
the decoded representations are sufficiently detailed to enable high accuracy
for cross-view-translation tasks with following pairwise accuracy: IC (78.0),
IT (83.0), KE (83.7) and SF (74.5). Analysis of the contribution of different
brain networks reveals exciting cognitive insights: (1) A high percentage of
visual voxels are involved in image captioning and image tagging tasks, and a
high percentage of language voxels are involved in the sentence formation and
keyword extraction tasks. (2) Zero-shot accuracy of the model trained on S view
and tested on WC view is better than same-view accuracy of the model trained
and tested on WC view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Ethical Considerations of Text Simplification. (arXiv:2204.09565v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09565">
<div class="article-summary-box-inner">
<span><p>This paper outlines the ethical implications of text simplification within
the framework of assistive systems. We argue that a distinction should be made
between the technologies that perform text simplification and the realisation
of these in assistive technologies. When using the latter as a motivation for
research, it is important that the subsequent ethical implications be carefully
considered. We provide guidelines for the framing of text simplification
independently of assistive systems, as well as suggesting directions for future
research and discussion based on the concerns raised.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Descriptions of Movement Through Geovisual Analytics. (arXiv:2204.09588v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09588">
<div class="article-summary-box-inner">
<span><p>Sensemaking using automatically extracted information from text is a
challenging problem. In this paper, we address a specific type of information
extraction, namely extracting information related to descriptions of movement.
Aggregating and understanding information related to descriptions of movement
and lack of movement specified in text can lead to an improved understanding
and sensemaking of movement phenomena of various types, e.g., migration of
people and animals, impediments to travel due to COVID-19, etc. We present
GeoMovement, a system that is based on combining machine learning and
rule-based extraction of movement-related information with state-of-the-art
visualization techniques. Along with the depiction of movement, our tool can
extract and present a lack of movement. Very little prior work exists on
automatically extracting descriptions of movement, especially negation and
movement. Apart from addressing these, GeoMovement also provides a novel
integrated framework for combining these extraction modules with visualization.
We include two systematic case studies of GeoMovement that show how humans can
derive meaningful geographic movement information. GeoMovement can complement
precise movement data, e.g., obtained using sensors, or be used by itself when
precise data is unavailable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning. (arXiv:2204.09589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09589">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the named entity recognition (NER) problem under
distant supervision. Due to the incompleteness of the external dictionaries
and/or knowledge bases, such distantly annotated training data usually suffer
from a high false negative rate. To this end, we formulate the Distantly
Supervised NER (DS-NER) problem via Multi-class Positive and Unlabeled (MPU)
learning and propose a theoretically and practically novel CONFidence-based MPU
(Conf-MPU) approach. To handle the incomplete annotations, Conf-MPU consists of
two steps. First, a confidence score is estimated for each token of being an
entity token. Then, the proposed Conf-MPU risk estimation is applied to train a
multi-class classifier for the NER task. Thorough experiments on two benchmark
datasets labeled by various external knowledge demonstrate the superiority of
the proposed Conf-MPU over existing DS-NER methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Bias and Fairness in Natural Language Processing. (arXiv:2204.09591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09591">
<div class="article-summary-box-inner">
<span><p>As NLP models become more integrated with the everyday lives of people, it
becomes important to examine the social effect that the usage of these systems
has. While these models understand language and have increased accuracy on
difficult downstream tasks, there is evidence that these models amplify gender,
racial and cultural stereotypes and lead to a vicious cycle in many settings.
In this survey, we analyze the origins of biases, the definitions of fairness,
and how different subfields of NLP mitigate bias. We finally discuss how future
studies can work towards eradicating pernicious biases from NLP algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks. (arXiv:2204.09593v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09593">
<div class="article-summary-box-inner">
<span><p>Vision outlookers improve the performance of vision transformers, which
implement a self-attention mechanism by adding outlook attention, a form of
local attention.
</p>
<p>In natural language processing, as has been the case in computer vision and
other domains, transformer-based models constitute the state-of-the-art for
most processing tasks. In this domain, too, many authors have argued and
demonstrated the importance of local context.
</p>
<p>We present and evaluate an outlook attention mechanism, COOL, for natural
language processing. COOL adds, on top of the self-attention layers of a
transformer-based model, outlook attention layers that encode local syntactic
context considering word proximity and consider more pair-wise constraints than
dynamic convolution operations used by existing approaches.
</p>
<p>A comparative empirical performance evaluation of an implementation of COOL
with different transformer-based approaches confirms the opportunity of
improvement over a baseline using the neural language models alone for various
natural language processing tasks, including question answering. The proposed
approach is competitive with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Clinical Intent from Free Text Electronic Health Records. (arXiv:2204.09594v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09594">
<div class="article-summary-box-inner">
<span><p>After a patient consultation, a clinician determines the steps in the
management of the patient. A clinician may for example request to see the
patient again or refer them to a specialist. Whilst most clinicians will record
their intent as "next steps" in the patient's clinical notes, in some cases the
clinician may forget to indicate their intent as an order or request, e.g.
failure to place the follow-up order. This consequently results in patients
becoming lost-to-follow up and may in some cases lead to adverse consequences.
In this paper we train a machine learning model to detect a clinician's intent
to follow up with a patient from the patient's clinical notes. Annotators
systematically identified 22 possible types of clinical intent and annotated
3000 Bariatric clinical notes. The annotation process revealed a class
imbalance in the labeled data and we found that there was only sufficient
labeled data to train 11 out of the 22 intents. We used the data to train a
BERT based multilabel classification model and reported the following average
accuracy metrics for all intents: macro-precision: 0.91, macro-recall: 0.90,
macro-f1: 0.90.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Continuous Integrate-and-Fire for Efficient and Adaptive Simultaneous Speech Translation. (arXiv:2204.09595v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09595">
<div class="article-summary-box-inner">
<span><p>Simultaneous speech translation (SimulST) is a challenging task that aims to
directly translate streaming speech before the complete input is observed. A
SimulST system generally includes two important components: the pre-decision
that aggregates the speech information, and the policy that decides read or
write. While recent works had proposed a variety of strategies to improve the
pre-decision, they mostly adopt the fixed wait-k policy. The adaptive policies
are rarely explored. We propose to model the adaptive policy using the
Continuous Integrate-and-Fire (CIF). In our proposed model, the CIF is not only
responsible for aggregating speech information, but also deciding when to read
or write. To adapt the CIF to SimulST task, we propose two modifications: a
token-level quantity loss or an infinite lookback attention. We show that our
model can learn an adaptive policy effectively, achieving comparable or
superior performance to MMA at lower latency, while being more efficient to
train.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceiving the World: Question-guided Reinforcement Learning for Text-based Games. (arXiv:2204.09597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09597">
<div class="article-summary-box-inner">
<span><p>Text-based games provide an interactive way to study natural language
processing. While deep reinforcement learning has shown effectiveness in
developing the game playing agent, the low sample efficiency and the large
action space remain to be the two major challenges that hinder the DRL from
being applied in the real world. In this paper, we address the challenges by
introducing world-perceiving modules, which automatically decompose tasks and
prune actions by answering questions about the environment. We then propose a
two-phase training framework to decouple language learning from reinforcement
learning, which further improves the sample efficiency. The experimental
results show that the proposed method significantly improves the performance
and sample efficiency. Besides, it shows robustness against compound error and
limited pre-training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Build a Robust QA System with Transformer-based Mixture of Experts. (arXiv:2204.09598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09598">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to build a robust question answering system that can
adapt to out-of-domain datasets. A single network may overfit to the
superficial correlation in the training distribution, but with a meaningful
number of expert sub-networks, a gating network that selects a sparse
combination of experts for each input, and careful balance on the importance of
expert sub-networks, the Mixture-of-Experts (MoE) model allows us to train a
multi-task learner that can be generalized to out-of-domain datasets. We also
explore the possibility of bringing the MoE layers up to the middle of the
DistilBERT and replacing the dense feed-forward network with a
sparsely-activated switch FFN layers, similar to the Switch Transformer
architecture, which simplifies the MoE routing algorithm with reduced
communication and computational costs. In addition to model architectures, we
explore techniques of data augmentation including Easy Data Augmentation (EDA)
and back translation, to create more meaningful variance among the small
out-of-domain training data, therefore boosting the performance and robustness
of our models. In this paper, we show that our combination of best architecture
and data augmentation techniques achieves a 53.477 F1 score in the
out-of-domain evaluation, which is a 9.52% performance gain over the baseline.
On the final test set, we reported a higher 59.506 F1 and 41.651 EM. We
successfully demonstrate the effectiveness of Mixture-of-Expert architecture in
a Robust QA task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radiology Text Analysis System (RadText): Architecture and Evaluation. (arXiv:2204.09599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09599">
<div class="article-summary-box-inner">
<span><p>Analyzing radiology reports is a time-consuming and error-prone task, which
raises the need for an efficient automated radiology report analysis system to
alleviate the workloads of radiologists and encourage precise diagnosis. In
this work, we present RadText, an open-source radiology text analysis system
developed by Python. RadText offers an easy-to-use text analysis pipeline,
including de-identification, section segmentation, sentence split and word
tokenization, named entity recognition, parsing, and negation detection.
RadText features a flexible modular design, provides a hybrid text processing
schema, and supports raw text processing and local processing, which enables
better usability and improved data privacy. RadText adopts BioC as the unified
interface, and also standardizes the input / output into a structured
representation compatible with Observational Medical Outcomes Partnership
(OMOP) Common Data Model (CDM). This allows for a more systematic approach to
observational research across multiple, disparate data sources. We evaluated
RadText on the MIMIC-CXR dataset, with five new disease labels we annotated for
this work. RadText demonstrates highly accurate classification performances,
with an average precision of, a recall of 0.94, and an F-1 score of 0.92. We
have made our code, documentation, examples, and the test set available at
https://github.com/bionlplab/radtext .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical BERT for Medical Document Understanding. (arXiv:2204.09600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09600">
<div class="article-summary-box-inner">
<span><p>Medical document understanding has gained much attention recently. One
representative task is the International Classification of Disease (ICD)
diagnosis code assignment. Existing work adopts either RNN or CNN as the
backbone network because the vanilla BERT cannot handle well long documents
(&gt;2000 to kens). One issue shared across all these approaches is that they are
over specific to the ICD code assignment task, losing generality to give the
whole document-level and sentence-level embedding. As a result, it is not
straight-forward to direct them to other downstream NLU tasks. Motivated by
these observations, we propose Medical Document BERT (MDBERT) for long medical
document understanding tasks. MDBERT is not only effective in learning
representations at different levels of semantics but efficient in encoding long
documents by leveraging a bottom-up hierarchical architecture. Compared to
vanilla BERT solutions: 1, MDBERT boosts the performance up to relatively 20%
on the MIMIC-III dataset, making it comparable to current SOTA solutions; 2, it
cuts the computational complexity on self-attention modules to less than 1/100.
Other than the ICD code assignment, we conduct a variety of other NLU tasks on
a large commercial dataset named as TrialTrove, to showcase MDBERT's strength
in delivering different levels of semantics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extraction of Sleep Information from Clinical Notes of Alzheimer's Disease Patients Using Natural Language Processing. (arXiv:2204.09601v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09601">
<div class="article-summary-box-inner">
<span><p>Alzheimer's Disease (AD) is the most common form of dementia in the United
States. Sleep is one of the lifestyle-related factors that has been shown
critical for optimal cognitive function in old age. . However, there is a lack
of research studying the association between sleep and AD incidence. A major
bottleneck for conducting such research is that the traditional way to acquire
sleep information is time-consuming, inefficient, non-scalable, and limited to
patients' subjective experience. In this study, we developed a rule-based NLP
algorithm and machine learning models to automate the extraction of
sleep-related concepts, including snoring, napping, sleep problem, bad sleep
quality, daytime sleepiness, night wakings, and sleep duration, from the
clinical notes of patients diagnosed with AD. We trained and validated the
proposed models on the clinical notes retrieved from the University of
Pittsburgh of Medical Center (UPMC). The results show that the rule-based NLP
algorithm consistently achieved the best performance for all sleep concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Unintended Memorization in Language-Model-Fused ASR. (arXiv:2204.09606v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09606">
<div class="article-summary-box-inner">
<span><p>End-to-end (E2E) models are often being accompanied by language models (LMs)
via shallow fusion for boosting their overall quality as well as recognition of
rare words. At the same time, several prior works show that LMs are susceptible
to unintentionally memorizing rare or unique sequences in the training data. In
this work, we design a framework for detecting memorization of random textual
sequences (which we call canaries) in the LM training data when one has only
black-box (query) access to LM-fused speech recognizer, as opposed to direct
access to the LM. On a production-grade Conformer RNN-T E2E model fused with a
Transformer LM, we show that detecting memorization of singly-occurring
canaries from the LM training data of 300M examples is possible. Motivated to
protect privacy, we also show that such memorization gets significantly reduced
by per-example gradient-clipped LM training without compromising overall
quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The TalkMoves Dataset: K-12 Mathematics Lesson Transcripts Annotated for Teacher and Student Discursive Moves. (arXiv:2204.09652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09652">
<div class="article-summary-box-inner">
<span><p>Transcripts of teaching episodes can be effective tools to understand
discourse patterns in classroom instruction. According to most educational
experts, sustained classroom discourse is a critical component of equitable,
engaging, and rich learning environments for students. This paper describes the
TalkMoves dataset, composed of 567 human-annotated K-12 mathematics lesson
transcripts (including entire lessons or portions of lessons) derived from
video recordings. The set of transcripts primarily includes in-person lessons
with whole-class discussions and/or small group work, as well as some online
lessons. All of the transcripts are human-transcribed, segmented by the speaker
(teacher or student), and annotated at the sentence level for ten discursive
moves based on accountable talk theory. In addition, the transcripts include
utterance-level information in the form of dialogue act labels based on the
Switchboard Dialog Act Corpus. The dataset can be used by educators,
policymakers, and researchers to understand the nature of teacher and student
discourse in K-12 math classrooms. Portions of this dataset have been used to
develop the TalkMoves application, which provides teachers with automated,
immediate, and actionable feedback about their mathematics instruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages. (arXiv:2204.09653v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09653">
<div class="article-summary-box-inner">
<span><p>A recent study by Ahmed and Devanbu reported that using a corpus of code
written in multilingual datasets to fine-tune multilingual Pre-trained Language
Models (PLMs) achieves higher performance as opposed to using a corpus of code
written in just one programming language. However, no analysis was made with
respect to fine-tuning monolingual PLMs. Furthermore, some programming
languages are inherently different and code written in one language usually
cannot be interchanged with the others, i.e., Ruby and Java code possess very
different structure. To better understand how monolingual and multilingual PLMs
affect different programming languages, we investigate 1) the performance of
PLMs on Ruby for two popular Software Engineering tasks: Code Summarization and
Code Search, 2) the strategy (to select programming languages) that works well
on fine-tuning multilingual PLMs for Ruby, and 3) the performance of the
fine-tuned PLMs on Ruby given different code lengths.
</p>
<p>In this work, we analyze over a hundred of pre-trained and fine-tuned models.
Our results show that 1) multilingual PLMs have a lower Performance-to-Time
Ratio (the BLEU, METEOR, or MRR scores over the fine-tuning duration) as
compared to monolingual PLMs, 2) our proposed strategy to select target
programming languages to fine-tune multilingual PLMs is effective: it reduces
the time to fine-tune yet achieves higher performance in Code Summarization and
Code Search tasks, and 3) our proposed strategy consistently shows good
performance on different code lengths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAMNER: Code Comment Generation Using Character Language Model and Named Entity Recognition. (arXiv:2204.09654v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09654">
<div class="article-summary-box-inner">
<span><p>Code comment generation is the task of generating a high-level natural
language description for a given code method or function. Although researchers
have been studying multiple ways to generate code comments automatically,
previous work mainly considers representing a code token in its entirety
semantics form only (e.g., a language model is used to learn the semantics of a
code token), and additional code properties such as the tree structure of a
code are included as an auxiliary input to the model. There are two
limitations: 1) Learning the code token in its entirety form may not be able to
capture information succinctly in source code, and 2) The code token does not
contain additional syntactic information, inherently important in programming
languages.
</p>
<p>In this paper, we present LAnguage Model and Named Entity Recognition
(LAMNER), a code comment generator capable of encoding code constructs
effectively and capturing the structural property of a code token. A
character-level language model is used to learn the semantic representation to
encode a code token. For the structural property of a token, a Named Entity
Recognition model is trained to learn the different types of code tokens. These
representations are then fed into an encoder-decoder architecture to generate
code comments. We evaluate the generated comments from LAMNER and other
baselines on a popular Java dataset with four commonly used metrics. Our
results show that LAMNER is effective and improves over the best baseline model
in BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR, and CIDEr by 14.34%,
18.98%, 21.55%, 23.00%, 10.52%, 1.44%, and 25.86%, respectively. Additionally,
we fused LAMNER's code representation with the baseline models, and the fused
models consistently showed improvement over the non-fused models. The human
evaluation further shows that LAMNER produces high-quality code comments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntax-informed Question Answering with Heterogeneous Graph Transformer. (arXiv:2204.09655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09655">
<div class="article-summary-box-inner">
<span><p>Large neural language models are steadily contributing state-of-the-art
performance to question answering and other natural language and information
processing tasks. These models are expensive to train. We propose to evaluate
whether such pre-trained models can benefit from the addition of explicit
linguistics information without requiring retraining from scratch.
</p>
<p>We present a linguistics-informed question answering approach that extends
and fine-tunes a pre-trained transformer-based neural language model with
symbolic knowledge encoded with a heterogeneous graph transformer. We
illustrate the approach by the addition of syntactic information in the form of
dependency and constituency graphic structures connecting tokens and virtual
vertices.
</p>
<p>A comparative empirical performance evaluation with BERT as its baseline and
with Stanford Question Answering Dataset demonstrates the competitiveness of
the proposed approach. We argue, in conclusion and in the light of further
results of preliminary experiments, that the approach is extensible to further
linguistics information including semantics and pragmatics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Fast Post-Training Pruning Framework for Transformers. (arXiv:2204.09656v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09656">
<div class="article-summary-box-inner">
<span><p>Pruning is an effective way to reduce the huge inference cost of large
Transformer models. However, prior work on model pruning requires retraining
the model. This can add high cost and complexity to model deployment, making it
difficult to use in many practical situations. To address this, we propose a
fast post-training pruning framework for Transformers that does not require any
retraining. Given a resource constraint and a sample dataset, our framework
automatically prunes the Transformer model using structured sparsity methods.
To retain high accuracy without retraining, we introduce three novel
techniques: (i) a lightweight mask search algorithm that finds which heads and
filters to prune based on the Fisher information; (ii) mask rearrangement that
complements the search algorithm; and (iii) mask tuning that reconstructs the
output activations for each layer. We apply our method to BERT-BASE and
DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our
framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference
latency, while maintaining &lt; 1% loss in accuracy. Importantly, our framework
prunes Transformers in less than 3 minutes on a single GPU, which is over two
orders of magnitude faster than existing pruning approaches that retrain. Our
code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The MIT Voice Name System. (arXiv:2204.09657v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09657">
<div class="article-summary-box-inner">
<span><p>This RFC white Paper summarizes our progress on the MIT Voice Name System
(VNS) and Huey. The VNS, similar in name and function to the DNS, is a system
to reserve and use "wake words" to activate Artificial Intelligence (AI)
devices. Just like you can say "Hey Siri" to activate Apple's personal
assistant, we propose using the VNS in smart speakers and other devices to
route wake requests based on commands such as "turn off", "open grocery
shopping list" or "271, start flash card review of my computer vision class".
We also introduce Huey, an unambiguous Natural Language to interact with AI
devices. We aim to standardize voice interactions to a universal reach similar
to that of other systems such as phone numbering, with an agreed world-wide
approach to assign and use numbers, or the Internet's DNS, with a standard
naming system, that has helped flourish popular services including the
World-Wide-Web, FTP, and email. Just like these standards are "neutral", we
also aim to endow the VNS with "wake neutrality" so that each participant can
develop its own digital voice. We focus on voice as a starting point to talk to
any IoT object and explain briefly how the VNS may be expanded to other AI
technologies enabling person-to-machine conversations (really
machine-to-machine), including computer vision or neural interfaces. We also
describe briefly considerations for a broader set of standards, MIT Open AI
(MOA), including a reference architecture to serve as a starting point for the
development of a general conversational commerce infrastructure that has
standard "Wake Words", NLP commands such as "Shopping Lists" or "Flash Card
Reviews", and personalities such as Pi or 271. Privacy and security are key
elements considered because of speech-to-text errors and the amount of personal
information contained in a voice sample.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Design Ideation: A Natural Language Generation Approach. (arXiv:2204.09658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09658">
<div class="article-summary-box-inner">
<span><p>This paper aims to explore a generative approach for knowledge-based design
ideation by applying the latest pre-trained language models in artificial
intelligence (AI). Specifically, a method of fine-tuning the generative
pre-trained transformer using the USPTO patent database is proposed. The
AI-generated ideas are not only in concise and understandable language but also
able to synthesize the target design with external knowledge sources with
controllable knowledge distance. The method is tested in a case study of
rolling toy design and the results show good performance in generating ideas of
varied novelty with near-field and far-field source knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design and Development of Rule-based open-domain Question-Answering System on SQuAD v2.0 Dataset. (arXiv:2204.09659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09659">
<div class="article-summary-box-inner">
<span><p>Human mind is the palace of curious questions that seek answers.
Computational resolution of this challenge is possible through Natural Language
Processing techniques. Statistical techniques like machine learning and deep
learning require a lot of data to train and despite that they fail to tap into
the nuances of language. Such systems usually perform best on close-domain
datasets. We have proposed development of a rule-based open-domain
question-answering system which is capable of answering questions of any domain
from a corresponding context passage. We have used 1000 questions from SQuAD
2.0 dataset for testing the developed system and it gives satisfactory results.
In this paper, we have described the structure of the developed system and have
analyzed the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Dataset Classification for Kurdish Short Text over Social Media. (arXiv:2204.09660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09660">
<div class="article-summary-box-inner">
<span><p>The Facebook application is used as a resource for collecting the comments of
this dataset, The dataset consists of 6756 comments to create a Medical Kurdish
Dataset (MKD). The samples are comments of users, which are gathered from
different posts of pages (Medical, News, Economy, Education, and Sport). Six
steps as a preprocessing technique are performed on the raw dataset to clean
and remove noise in the comments by replacing characters. The comments (short
text) are labeled for positive class (medical comment) and negative class
(non-medical comment) as text classification. The percentage ratio of the
negative class is 55% while the positive class is 45%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments. (arXiv:2204.09667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09667">
<div class="article-summary-box-inner">
<span><p>Recent work in Vision-and-Language Navigation (VLN) has presented two
environmental paradigms with differing realism -- the standard VLN setting
built on topological environments where navigation is abstracted away, and the
VLN-CE setting where agents must navigate continuous 3D environments using
low-level actions. Despite sharing the high-level task and even the underlying
instruction-path data, performance on VLN-CE lags behind VLN significantly. In
this work, we explore this gap by transferring an agent from the abstract
environment of VLN to the continuous environment of VLN-CE. We find that this
sim-2-sim transfer is highly effective, improving over the prior state of the
art in VLN-CE by +12% success rate. While this demonstrates the potential for
this direction, the transfer does not fully retain the original performance of
the agent in the abstract setting. We present a sequence of experiments to
identify what differences result in performance degradation, providing clear
directions for further improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards General Purpose Vision Systems. (arXiv:2104.00743v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00743">
<div class="article-summary-box-inner">
<span><p>Computer vision systems today are primarily N-purpose systems, designed and
trained for a predefined set of tasks. Adapting such systems to new tasks is
challenging and often requires non-trivial modifications to the network
architecture (e.g. adding new output heads) or training process (e.g. adding
new losses). To reduce the time and expertise required to develop new
applications, we would like to create general purpose vision systems that can
learn and perform a range of tasks without any modification to the architecture
or learning process.
</p>
<p>In this paper, we propose GPV-1, a task-agnostic vision-language architecture
that can learn and perform tasks that involve receiving an image and producing
text and/or bounding boxes, including classification, localization, visual
question answering, captioning, and more. We also propose evaluations of
generality of architecture, skill-concept transfer, and learning efficiency
that may inform future work on general purpose vision. Our experiments indicate
GPV-1 is effective at multiple tasks, reuses some concept knowledge across
tasks, can perform the Referring Expressions task zero-shot, and further
improves upon the zero-shot performance using a few training samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach. (arXiv:2104.04886v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04886">
<div class="article-summary-box-inner">
<span><p>Adversarial regularization has been shown to improve the generalization
performance of deep learning models in various natural language processing
tasks. Existing works usually formulate the method as a zero-sum game, which is
solved by alternating gradient descent/ascent algorithms. Such a formulation
treats the adversarial and the defending players equally, which is undesirable
because only the defending player contributes to the generalization
performance. To address this issue, we propose Stackelberg Adversarial
Regularization (SALT), which formulates adversarial regularization as a
Stackelberg game. This formulation induces a competition between a leader and a
follower, where the follower generates perturbations, and the leader trains the
model subject to the perturbations. Different from conventional approaches, in
SALT, the leader is in an advantageous position. When the leader moves, it
recognizes the strategy of the follower and takes the anticipated follower's
outcomes into consideration. Such a leader's advantage enables us to improve
the model fitting to the unperturbed data. The leader's strategic information
is captured by the Stackelberg gradient, which is obtained using an unrolling
algorithm. Our experimental results on a set of machine translation and natural
language understanding tasks show that SALT outperforms existing adversarial
regularization baselines across all tasks. Our code is available at
https://github.com/SimiaoZuo/Stackelberg-Adv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trends, Limitations and Open Challenges in Automatic Readability Assessment Research. (arXiv:2105.00973v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00973">
<div class="article-summary-box-inner">
<span><p>Readability assessment is the task of evaluating the reading difficulty of a
given piece of text. Although research on computational approaches to
readability assessment is now two decades old, there is not much work on
synthesizing this research. This article is a brief survey of contemporary
research on developing computational models for readability assessment. We
identify the common approaches, discuss their shortcomings, and identify some
challenges for the future. Where possible, we also connect computational
research with insights from related work in other disciplines such as education
and psychology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization. (arXiv:2108.13684v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13684">
<div class="article-summary-box-inner">
<span><p>Despite recent progress in abstractive summarization, systems still suffer
from faithfulness errors. While prior work has proposed models that improve
faithfulness, it is unclear whether the improvement comes from an increased
level of extractiveness of the model outputs as one naive way to improve
faithfulness is to make summarization models more extractive. In this work, we
present a framework for evaluating the effective faithfulness of summarization
systems, by generating a faithfulnessabstractiveness trade-off curve that
serves as a control at different operating points on the abstractiveness
spectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as
well as a recently proposed method for improving faithfulness, are both worse
than the control at the same level of abstractiveness. Finally, we learn a
selector to identify the most faithful and abstractive summary for a given
document, and show that this system can attain higher faithfulness scores in
human evaluations while being more abstractive than the baseline system on two
datasets. Moreover, we show that our system is able to achieve a better
faithfulness-abstractiveness trade-off than the control at the same level of
abstractiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARCH: Efficient Adversarial Regularized Training with Caching. (arXiv:2109.07048v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07048">
<div class="article-summary-box-inner">
<span><p>Adversarial regularization can improve model generalization in many natural
language processing tasks. However, conventional approaches are computationally
expensive since they need to generate a perturbation for each sample in each
epoch. We propose a new adversarial regularization method ARCH (adversarial
regularization with caching), where perturbations are generated and cached once
every several epochs. As caching all the perturbations imposes memory usage
concerns, we adopt a K-nearest neighbors-based strategy to tackle this issue.
The strategy only requires caching a small amount of perturbations, without
introducing additional training time. We evaluate our proposed method on a set
of neural machine translation and natural language understanding tasks. We
observe that ARCH significantly eases the computational burden (saves up to 70%
of computational time in comparison with conventional approaches). More
surprisingly, by reducing the variance of stochastic gradients, ARCH produces a
notably better (in most of the tasks) or comparable model generalization. Our
code is available at https://github.com/SimiaoZuo/Caching-Adv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mention Memory: incorporating textual knowledge into Transformers through entity mention attention. (arXiv:2110.06176v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06176">
<div class="article-summary-box-inner">
<span><p>Natural language understanding tasks such as open-domain question answering
often require retrieving and assimilating factual information from multiple
sources. We propose to address this problem by integrating a semi-parametric
representation of a large text corpus into a Transformer model as a source of
factual knowledge. Specifically, our method represents knowledge with `mention
memory', a table of dense vector representations of every entity mention in a
corpus. The proposed model - TOME - is a Transformer that accesses the
information through internal memory layers in which each entity mention in the
input passage attends to the mention memory. This approach enables synthesis of
and reasoning over many disparate sources of information within a single
Transformer model. In experiments using a memory of 150 million Wikipedia
mentions, TOME achieves strong performance on several open-domain
knowledge-intensive tasks, including the claim verification benchmarks HoVer
and FEVER and several entity-based QA benchmarks. We also show that the model
learns to attend to informative mentions without any direct supervision.
Finally we demonstrate that the model can generalize to new unseen entities by
updating the memory without retraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Dense Retrieval for Dialogue Response Selection. (arXiv:2110.06612v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06612">
<div class="article-summary-box-inner">
<span><p>Recent progress in deep learning has continuously improved the accuracy of
dialogue response selection. In particular, sophisticated neural network
architectures are leveraged to capture the rich interactions between dialogue
context and response candidates. While remarkably effective, these models also
bring in a steep increase in computational cost. Consequently, such models can
only be used as a re-rank module in practice. In this study, we present a
solution to directly select proper responses from a large corpus or even a
nonparallel corpus that only consists of unpaired sentences, using a dense
retrieval model. To push the limits of dense retrieval, we design an
interaction layer upon the dense retrieval models and apply a set of
tailor-designed learning strategies. Our model shows superiority over strong
baselines on the conventional re-rank evaluation setting, which is remarkable
given its efficiency. To verify the effectiveness of our approach in realistic
scenarios, we also conduct full-rank evaluation, where the target is to select
proper responses from a full candidate pool that may contain millions of
candidates and evaluate them fairly through human annotations. Our proposed
model notably outperforms pipeline baselines that integrate fast recall and
expressive re-rank modules. Human evaluation results show that enlarging the
candidate pool with nonparallel corpora improves response quality further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Curriculum Learning for AMR Parsing. (arXiv:2110.07855v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07855">
<div class="article-summary-box-inner">
<span><p>Abstract Meaning Representation (AMR) parsing aims to translate sentences to
semantic representation with a hierarchical structure, and is recently
empowered by pretrained sequence-to-sequence models. However, there exists a
gap between their flat training objective (i.e., equally treats all output
tokens) and the hierarchical AMR structure, which limits the model
generalization. To bridge this gap, we propose a Hierarchical Curriculum
Learning (HCL) framework with Structure-level (SC) and Instance-level Curricula
(IC). SC switches progressively from core to detail AMR semantic elements while
IC transits from structure-simple to -complex AMR instances during training.
Through these two warming-up processes, HCL reduces the difficulty of learning
complex structures, thus the flat model can better adapt to the AMR hierarchy.
Extensive experiments on AMR2.0, AMR3.0, structure-complex and
out-of-distribution situations verify the effectiveness of HCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KazakhTTS2: Extending the Open-Source Kazakh TTS Corpus With More Data, Speakers, and Topics. (arXiv:2201.05771v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05771">
<div class="article-summary-box-inner">
<span><p>We present an expanded version of our previously released Kazakh
text-to-speech (KazakhTTS) synthesis corpus. In the new KazakhTTS2 corpus, the
overall size has increased from 93 hours to 271 hours, the number of speakers
has risen from two to five (three females and two males), and the topic
coverage has been diversified with the help of new sources, including a book
and Wikipedia articles. This corpus is necessary for building high-quality TTS
systems for Kazakh, a Central Asian agglutinative language from the Turkic
family, which presents several linguistic challenges. We describe the corpus
construction process and provide the details of the training and evaluation
procedures for the TTS system. Our experimental results indicate that the
constructed corpus is sufficient to build robust TTS models for real-world
applications, with a subjective mean opinion score ranging from 3.6 to 4.2 for
all the five speakers. We believe that our corpus will facilitate speech and
language research for Kazakh and other Turkic languages, which are widely
considered to be low-resource due to the limited availability of free
linguistic data. The constructed corpus, code, and pretrained models are
publicly available in our GitHub repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMS_ADRN at SemEval-2022 Task 5: A Suitable Image-text Multimodal Joint Modeling Method for Multi-task Misogyny Identification. (arXiv:2202.09099v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09099">
<div class="article-summary-box-inner">
<span><p>Women are influential online, especially in image-based social media such as
Twitter and Instagram. However, many in the network environment contain gender
discrimination and aggressive information, which magnify gender stereotypes and
gender inequality. Therefore, the filtering of illegal content such as gender
discrimination is essential to maintain a healthy social network environment.
In this paper, we describe the system developed by our team for SemEval-2022
Task 5: Multimedia Automatic Misogyny Identification. More specifically, we
introduce two novel system to analyze these posts: a multimodal multi-task
learning architecture that combines Bertweet for text encoding with ResNet-18
for image representation, and a single-flow transformer structure which
combines text embeddings from BERT-Embeddings and image embeddings from several
different modules such as EfficientNet and ResNet. In this manner, we show that
the information behind them can be properly revealed. Our approach achieves
good performance on each of the two subtasks of the current competition,
ranking 15th for Subtask A (0.746 macro F1-score), 11th for Subtask B (0.706
macro F1-score) while exceeding the official baseline results by high margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"splink" is happy and "phrouth" is scary: Emotion Intensity Analysis for Nonsense Words. (arXiv:2202.12132v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12132">
<div class="article-summary-box-inner">
<span><p>People associate affective meanings to words - "death" is scary and sad while
"party" is connotated with surprise and joy. This raises the question if the
association is purely a product of the learned affective imports inherent to
semantic meanings, or is also an effect of other features of words, e.g.,
morphological and phonological patterns. We approach this question with an
annotation-based analysis leveraging nonsense words. Specifically, we conduct a
best-worst scaling crowdsourcing study in which participants assign intensity
scores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense
words and, for comparison of the results to previous work, to 68 real words.
Based on this resource, we develop character-level and phonology-based
intensity regressors. We evaluate them on both nonsense words and real words
(making use of the NRC emotion intensity lexicon of 7493 words), across six
emotion categories. The analysis of our data reveals that some phonetic
patterns show clear differences between emotion intensities. For instance, s as
a first phoneme contributes to joy, sh to surprise, p as last phoneme more to
disgust than to anger and fear. In the modelling experiments, a regressor
trained on real words from the NRC emotion intensity lexicon shows a higher
performance (r = 0.17) than regressors that aim at learning the emotion
connotation purely from nonsense words. We conclude that humans do associate
affective meaning to words based on surface patterns, but also based on
similarities to existing words ("juy" to "joy", or "flike" to "like").
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FiNER: Financial Numeric Entity Recognition for XBRL Tagging. (arXiv:2203.06482v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06482">
<div class="article-summary-box-inner">
<span><p>Publicly traded companies are required to submit periodic reports with
eXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging
the reports is tedious and costly. We, therefore, introduce XBRL tagging as a
new entity extraction task for the financial domain and release FiNER-139, a
dataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction
datasets, FiNER-139 uses a much larger label set of 139 entity types. Most
annotated tokens are numeric, with the correct tag per token depending mostly
on context, rather than the token itself. We show that subword fragmentation of
numeric expressions harms BERT's performance, allowing word-level BILSTMs to
perform better. To improve BERT's performance, we propose two simple and
effective solutions that replace numeric expressions with pseudo-tokens
reflecting original token shapes and numeric magnitudes. We also experiment
with FIN-BERT, an existing BERT model for the financial domain, and release our
own BERT (SEC-BERT), pre-trained on financial filings, which performs best.
Through data and error analysis, we finally identify possible limitations to
inspire future work on XBRL tagging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models. (arXiv:2203.07911v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07911">
<div class="article-summary-box-inner">
<span><p>Natural language processing models learn word representations based on the
distributional hypothesis, which asserts that word context (e.g.,
co-occurrence) correlates with meaning. We propose that $n$-grams composed of
random character sequences, or $garble$, provide a novel context for studying
word meaning both within and beyond extant language. In particular, randomly
generated character $n$-grams lack meaning but contain primitive information
based on the distribution of characters they contain. By studying the
embeddings of a large corpus of garble, extant language, and pseudowords using
CharacterBERT, we identify an axis in the model's high-dimensional embedding
space that separates these classes of $n$-grams. Furthermore, we show that this
axis relates to structure within extant language, including word
part-of-speech, morphology, and concept concreteness. Thus, in contrast to
studies that are mainly limited to extant language, our work reveals that
meaning and primitive information are intrinsically linked.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Roadmap for Big Model. (arXiv:2203.14101v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14101">
<div class="article-summary-box-inner">
<span><p>With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&amp;Interpretability, Commonsense Reasoning, Reliability&amp;Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Learning with Siamese Networks and Label Tuning. (arXiv:2203.14655v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14655">
<div class="article-summary-box-inner">
<span><p>We study the problem of building text classifiers with little or no training
data, commonly known as zero and few-shot text classification. In recent years,
an approach based on neural textual entailment models has been found to give
strong results on a diverse range of tasks. In this work, we show that with
proper pre-training, Siamese Networks that embed texts and labels offer a
competitive alternative. These models allow for a large reduction in inference
cost: constant in the number of labels rather than linear. Furthermore, we
introduce label tuning, a simple and computationally efficient approach that
allows to adapt the models in a few-shot setup by only changing the label
embeddings. While giving lower performance than model fine-tuning, this
approach has the architectural advantage that a single encoder can be shared by
many different tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space. (arXiv:2203.14680v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14680">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models (LMs) are at the core of modern NLP, but
their internal prediction construction process is opaque and largely not
understood. In this work, we make a substantial step towards unveiling this
underlying prediction process, by reverse-engineering the operation of the
feed-forward network (FFN) layers, one of the building blocks of transformer
models. We view the token representation as a changing distribution over the
vocabulary, and the output from each FFN layer as an additive update to that
distribution. Then, we analyze the FFN updates in the vocabulary space, showing
that each update can be decomposed to sub-updates corresponding to single FFN
parameter vectors, each promoting concepts that are often human-interpretable.
We then leverage these findings for controlling LM predictions, where we reduce
the toxicity of GPT2 by almost 50%, and for improving computation efficiency
with a simple early exit rule, saving 20% of computation on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics. (arXiv:2203.15858v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15858">
<div class="article-summary-box-inner">
<span><p>Current practices in metric evaluation focus on one single dataset, e.g.,
Newstest dataset in each year's WMT Metrics Shared Task. However, in this
paper, we qualitatively and quantitatively show that the performances of
metrics are sensitive to data. The ranking of metrics varies when the
evaluation is conducted on different datasets. Then this paper further
investigates two potential hypotheses, i.e., insignificant data points and the
deviation of Independent and Identically Distributed (i.i.d) assumption, which
may take responsibility for the issue of data variance. In conclusion, our
findings suggest that when evaluating automatic translation metrics,
researchers should take data variance into account and be cautious to claim the
result on a single dataset, because it may leads to inconsistent results with
most of other datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Effective Unsupervised Speech Synthesis. (arXiv:2204.02524v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02524">
<div class="article-summary-box-inner">
<span><p>We introduce the first unsupervised speech synthesis system based on a
simple, yet effective recipe. The framework leverages recent work in
unsupervised speech recognition as well as existing neural-based speech
synthesis. Using only unlabeled speech audio and unlabeled text as well as a
lexicon, our method enables speech synthesis without the need for a
human-labeled corpus. Experiments demonstrate the unsupervised system can
synthesize speech similar to a supervised counterpart in terms of naturalness
and intelligibility measured by human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Simultaneous Speech Translation need Simultaneous Models?. (arXiv:2204.03783v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03783">
<div class="article-summary-box-inner">
<span><p>In simultaneous speech translation (SimulST), finding the best trade-off
between high translation quality and low latency is a challenging task. To meet
the latency constraints posed by the different application scenarios, multiple
dedicated SimulST models are usually trained and maintained, generating high
computational costs. In this paper, motivated by the increased social and
environmental impact caused by these costs, we investigate whether a single
model trained offline can serve not only the offline but also the simultaneous
task without the need for any additional training or adaptation. Experiments on
en-&gt;{de, es} indicate that, aside from facilitating the adoption of
well-established offline techniques and architectures without affecting
latency, the offline solution achieves similar or better translation quality
compared to the same model trained in simultaneous settings, as well as being
competitive with the SimulST state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Biomedical Entity Linking via Knowledge Base-Guided Pre-training and Synonyms-Aware Fine-tuning. (arXiv:2204.05164v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05164">
<div class="article-summary-box-inner">
<span><p>Entities lie in the heart of biomedical natural language understanding, and
the biomedical entity linking (EL) task remains challenging due to the
fine-grained and diversiform concept names. Generative methods achieve
remarkable performances in general domain EL with less memory usage while
requiring expensive pre-training. Previous biomedical EL methods leverage
synonyms from knowledge bases (KB) which is not trivial to inject into a
generative method. In this work, we use a generative approach to model
biomedical EL and propose to inject synonyms knowledge in it. We propose
KB-guided pre-training by constructing synthetic samples with synonyms and
definitions from KB and require the model to recover concept names. We also
propose synonyms-aware fine-tuning to select concept names for training, and
propose decoder prompt and multi-synonyms constrained prefix tree for
inference. Our method achieves state-of-the-art results on several biomedical
EL tasks without candidate selection which displays the effectiveness of
proposed pre-training and fine-tuning strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impossible Triangle: What's Next for Pre-trained Language Models?. (arXiv:2204.06130v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06130">
<div class="article-summary-box-inner">
<span><p>Recent development of large-scale pre-trained language models (PLM) have
significantly improved the capability of models in various NLP tasks, in terms
of performance after task-specific fine-tuning and zero-shot / few-shot
learning. However, many of such models come with a dauntingly huge size that
few institutions can afford to pre-train, fine-tune or even deploy, while
moderate-sized models usually lack strong generalized few-shot learning
capabilities. In this paper, we first elaborate the current obstacles of using
PLM models in terms of the Impossible Triangle: 1) moderate model size, 2)
state-of-the-art few-shot learning capability, and 3) state-of-the-art
fine-tuning capability. We argue that all existing PLM models lack one or more
properties from the Impossible Triangle. To remedy these missing properties of
PLMs, various techniques have been proposed, such as knowledge distillation,
data augmentation and prompt learning, which inevitably brings additional work
to the application of PLMs in real scenarios. We then offer insights into
future research directions of PLMs to achieve the Impossible Triangle, and
break down the task into several key phases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polling Latent Opinions: A Method for Computational Sociolinguistics Using Transformer Language Models. (arXiv:2204.07483v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07483">
<div class="article-summary-box-inner">
<span><p>Text analysis of social media for sentiment, topic analysis, and other
analysis depends initially on the selection of keywords and phrases that will
be used to create the research corpora. However, keywords that researchers
choose may occur infrequently, leading to errors that arise from using small
samples. In this paper, we use the capacity for memorization, interpolation,
and extrapolation of Transformer Language Models such as the GPT series to
learn the linguistic behaviors of a subgroup within larger corpora of Yelp
reviews. We then use prompt-based queries to generate synthetic text that can
be analyzed to produce insights into specific opinions held by the populations
that the models were trained on. Once learned, more specific sentiment queries
can be made of the model with high levels of accuracy when compared to
traditional keyword searches. We show that even in cases where a specific
keyphrase is limited or not present at all in the training corpora, the GPT is
able to accurately generate large volumes of text that have the correct
sentiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Idiom Paraphrasing. (arXiv:2204.07555v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07555">
<div class="article-summary-box-inner">
<span><p>Idioms, are a kind of idiomatic expression in Chinese, most of which consist
of four Chinese characters. Due to the properties of non-compositionality and
metaphorical meaning, Chinese Idioms are hard to be understood by children and
non-native speakers. This study proposes a novel task, denoted as Chinese Idiom
Paraphrasing (CIP). CIP aims to rephrase idioms-included sentences to
non-idiomatic ones under the premise of preserving the original sentence's
meaning. Since the sentences without idioms are easier handled by Chinese NLP
systems, CIP can be used to pre-process Chinese datasets, thereby facilitating
and improving the performance of Chinese NLP tasks, e.g., machine translation
system, Chinese idiom cloze, and Chinese idiom embeddings. In this study, CIP
task is treated as a special paraphrase generation task. To circumvent
difficulties in acquiring annotations, we first establish a large-scale CIP
dataset based on human and machine collaboration, which consists of 115,530
sentence pairs. We further deploy three baselines and two novel CIP approaches
to deal with CIP problems. The results show that the proposed methods have
better performances than the baselines based on the established CIP dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Mixed-initiative Conversational Search Systems via User Simulation. (arXiv:2204.08046v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08046">
<div class="article-summary-box-inner">
<span><p>Clarifying the underlying user information need by asking clarifying
questions is an important feature of modern conversational search system.
However, evaluation of such systems through answering prompted clarifying
questions requires significant human effort, which can be time-consuming and
expensive. In this paper, we propose a conversational User Simulator, called
USi, for automatic evaluation of such conversational search systems. Given a
description of an information need, USi is capable of automatically answering
clarifying questions about the topic throughout the search session. Through a
set of experiments, including automated natural language generation metrics and
crowdsourcing studies, we show that responses generated by USi are both inline
with the underlying information need and comparable to human-generated answers.
Moreover, we make the first steps towards multi-turn interactions, where
conversational search systems asks multiple questions to the (simulated) user
with a goal of clarifying the user need. To this end, we expand on currently
available datasets for studying clarifying questions, i.e., Qulac and ClariQ,
by performing a crowdsourcing-based multi-turn data acquisition. We show that
our generative, GPT2-based model, is capable of providing accurate and natural
answers to unseen clarifying questions in the single-turn setting and discuss
capabilities of our model in the multi-turn setting. We provide the code, data,
and the pre-trained model to be used for further research on the topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08790">
<div class="article-summary-box-inner">
<span><p>Learning visual representations from natural language supervision has
recently shown great promise in a number of pioneering works. In general, these
language-augmented visual models demonstrate strong transferability to a
variety of datasets/tasks. However, it remains a challenge to evaluate the
transferablity of these foundation models due to the lack of easy-to-use
toolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation
of Language-augmented Visual Task-level Transfer), the first benchmark to
compare and evaluate pre-trained language-augmented visual models. Several
highlights include: (i) Datasets. As downstream evaluation suites, it consists
of 20 image classification datasets and 35 object detection datasets, each of
which is augmented with external knowledge. (ii) Toolkit. An automatic
hyper-parameter tuning toolkit is developed to ensure the fairness in model
adaption. To leverage the full power of language-augmented visual models, novel
language-aware initialization methods are proposed to significantly improve the
adaption performance. (iii) Metrics. A variety of evaluation metrics are used,
including sample-efficiency (zero-shot and few-shot) and parameter-efficiency
(linear probing and full model fine-tuning). We will release our toolkit and
evaluation platforms for the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ATP: AMRize Then Parse! Enhancing AMR Parsing with PseudoAMRs. (arXiv:2204.08875v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08875">
<div class="article-summary-box-inner">
<span><p>As Abstract Meaning Representation (AMR) implicitly involves compound
semantic annotations, we hypothesize auxiliary tasks which are semantically or
formally related can better enhance AMR parsing. We find that 1) Semantic role
labeling (SRL) and dependency parsing (DP), would bring more performance gain
than other tasks e.g. MT and summarization in the text-to-AMR transition even
with much less data. 2) To make a better fit for AMR, data from auxiliary tasks
should be properly "AMRized" to PseudoAMR before training. Knowledge from
shallow level parsing tasks can be better transferred to AMR Parsing with
structure transform. 3) Intermediate-task learning is a better paradigm to
introduce auxiliary tasks to AMR parsing, compared to multitask learning. From
an empirical perspective, we propose a principled method to involve auxiliary
tasks to boost AMR parsing. Extensive experiments show that our method achieves
new state-of-the-art performance on different benchmarks especially in
topology-related scores.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">PR-DAD: Phase Retrieval Using Deep Auto-Decoders. (arXiv:2204.09051v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09051">
<div class="article-summary-box-inner">
<span><p>Phase retrieval is a well known ill-posed inverse problem where one tries to
recover images given only the magnitude values of their Fourier transform as
input. In recent years, new algorithms based on deep learning have been
proposed, providing breakthrough results that surpass the results of the
classical methods. In this work we provide a novel deep learning architecture
PR-DAD (Phase Retrieval Using Deep Auto- Decoders), whose components are
carefully designed based on mathematical modeling of the phase retrieval
problem. The architecture provides experimental results that surpass all
current results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embodied Navigation at the Art Gallery. (arXiv:2204.09069v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09069">
<div class="article-summary-box-inner">
<span><p>Embodied agents, trained to explore and navigate indoor photorealistic
environments, have achieved impressive results on standard datasets and
benchmarks. So far, experiments and evaluations have involved domestic and
working scenes like offices, flats, and houses. In this paper, we build and
release a new 3D space with unique characteristics: the one of a complete art
museum. We name this environment ArtGallery3D (AG3D). Compared with existing 3D
scenes, the collected space is ampler, richer in visual features, and provides
very sparse occupancy information. This feature is challenging for
occupancy-based agents which are usually trained in crowded domestic
environments with plenty of occupancy information. Additionally, we annotate
the coordinates of the main points of interest inside the museum, such as
paintings, statues, and other items. Thanks to this manual process, we deliver
a new benchmark for PointGoal navigation inside this new space. Trajectories in
this dataset are far more complex and lengthy than existing ground-truth paths
for navigation in Gibson and Matterport3D. We carry on extensive experimental
evaluation using our new space for evaluation and prove that existing methods
hardly adapt to this scenario. As such, we believe that the availability of
this 3D model will foster future research and help improve existing solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Tool based Edited Images from Error Level Analysis and Convolutional Neural Network. (arXiv:2204.09075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09075">
<div class="article-summary-box-inner">
<span><p>Image Forgery is a problem of image forensics and its detection can be
leveraged using Deep Learning. In this paper we present an approach for
identification of authentic and tampered images done using image editing tools
with Error Level Analysis and Convolutional Neural Network. The process is
performed on CASIA ITDE v2 dataset and trained for 50 and 100 epochs
respectively. The respective accuracies of the training and validation sets are
represented using graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Photometric single-view dense 3D reconstruction in endoscopy. (arXiv:2204.09083v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09083">
<div class="article-summary-box-inner">
<span><p>Visual SLAM inside the human body will open the way to computer-assisted
navigation in endoscopy. However, due to space limitations, medical endoscopes
only provide monocular images, leading to systems lacking true scale. In this
paper, we exploit the controlled lighting in colonoscopy to achieve the first
in-vivo 3D reconstruction of the human colon using photometric stereo on a
calibrated monocular endoscope. Our method works in a real medical environment,
providing both a suitable in-place calibration procedure and a depth estimation
technique adapted to the colon's tubular geometry. We validate our method on
simulated colonoscopies, obtaining a mean error of 7% on depth estimation,
which is below 3 mm on average. Our qualitative results on the EndoMapper
dataset show that the method is able to correctly estimate the colon shape in
real human colonoscopies, paving the ground for true-scale monocular SLAM in
endoscopy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">4D-MultispectralNet: Multispectral Stereoscopic Disparity Estimation using Human Masks. (arXiv:2204.09089v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09089">
<div class="article-summary-box-inner">
<span><p>Multispectral stereoscopy is an emerging field. A lot of work has been done
in classical stereoscopy, but multispectral stereoscopy is not studied as
frequently. This type of stereoscopy can be used in autonomous vehicles to
complete the information given by RGB cameras. It helps to identify objects in
the surroundings when the conditions are more difficult, such as in night
scenes. This paper focuses on the RGB-LWIR spectrum. RGB-LWIR stereoscopy has
the same challenges as classical stereoscopy, that is occlusions, textureless
surfaces and repetitive patterns, plus specific ones related to the different
modalities. Finding matches between two spectrums adds another layer of
complexity. Color, texture and shapes are more likely to vary from a spectrum
to another. To address this additional challenge, this paper focuses on
estimating the disparity of people present in a scene. Given the fact that
people's shape is captured in both RGB and LWIR, we propose a novel method that
uses segmentation masks of the human in both spectrum and than concatenate them
to the original images before the first layer of a Siamese Network. This method
helps to improve the accuracy, particularly within the one pixel error range.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Behind the Machine's Gaze: Biologically Constrained Neural Networks Exhibit Human-like Visual Attention. (arXiv:2204.09093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09093">
<div class="article-summary-box-inner">
<span><p>By and large, existing computational models of visual attention tacitly
assume perfect vision and full access to the stimulus and thereby deviate from
foveated biological vision. Moreover, modelling top-down attention is generally
reduced to the integration of semantic features without incorporating the
signal of a high-level visual tasks that have shown to partially guide human
attention. We propose the Neural Visual Attention (NeVA) algorithm to generate
visual scanpaths in a top-down manner. With our method, we explore the ability
of neural networks on which we impose the biological constraints of foveated
vision to generate human-like scanpaths. Thereby, the scanpaths are generated
to maximize the performance with respect to the underlying visual task (i.e.,
classification or reconstruction). Extensive experiments show that the proposed
method outperforms state-of-the-art unsupervised human attention models in
terms of similarity to human scanpaths. Additionally, the flexibility of the
framework allows to quantitatively investigate the role of different tasks in
the generated visual behaviours. Finally, we demonstrate the superiority of the
approach in a novel experiment that investigates the utility of scanpaths in
real-world applications, where imperfect viewing conditions are given.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical Remote Sensing Image Understanding with Weak Supervision: Concepts, Methods, and Perspectives. (arXiv:2204.09120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09120">
<div class="article-summary-box-inner">
<span><p>In recent years, supervised learning has been widely used in various tasks of
optical remote sensing image understanding, including remote sensing image
classification, pixel-wise segmentation, change detection, and object
detection. The methods based on supervised learning need a large amount of
high-quality training data and their performance highly depends on the quality
of the labels. However, in practical remote sensing applications, it is often
expensive and time-consuming to obtain large-scale data sets with high-quality
labels, which leads to a lack of sufficient supervised information. In some
cases, only coarse-grained labels can be obtained, resulting in the lack of
exact supervision. In addition, the supervised information obtained manually
may be wrong, resulting in a lack of accurate supervision. Therefore, remote
sensing image understanding often faces the problems of incomplete, inexact,
and inaccurate supervised information, which will affect the breadth and depth
of remote sensing applications. In order to solve the above-mentioned problems,
researchers have explored various tasks in remote sensing image understanding
under weak supervision. This paper summarizes the research progress of weakly
supervised learning in the field of remote sensing, including three typical
weakly supervised paradigms: 1) Incomplete supervision, where only a subset of
training data is labeled; 2) Inexact supervision, where only coarse-grained
labels of training data are given; 3) Inaccurate supervision, where the labels
given are not always true on the ground.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Importance is in your attention: agent importance prediction for autonomous driving. (arXiv:2204.09121v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09121">
<div class="article-summary-box-inner">
<span><p>Trajectory prediction is an important task in autonomous driving.
State-of-the-art trajectory prediction models often use attention mechanisms to
model the interaction between agents. In this paper, we show that the attention
information from such models can also be used to measure the importance of each
agent with respect to the ego vehicle's future planned trajectory. Our
experiment results on the nuPlans dataset show that our method can effectively
find and rank surrounding agents by their impact on the ego's plan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Imagenet Models Transfer Better. (arXiv:2204.09134v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09134">
<div class="article-summary-box-inner">
<span><p>A commonly accepted hypothesis is that models with higher accuracy on
Imagenet perform better on other downstream tasks, leading to much research
dedicated to optimizing Imagenet accuracy. Recently this hypothesis has been
challenged by evidence showing that self-supervised models transfer better than
their supervised counterparts, despite their inferior Imagenet accuracy. This
calls for identifying the additional factors, on top of Imagenet accuracy, that
make models transferable. In this work we show that high diversity of the
features learnt by the model promotes transferability jointly with Imagenet
accuracy. Encouraged by the recent transferability results of self-supervised
models, we propose a method that combines self-supervised and supervised
pretraining to generate models with both high diversity and high accuracy, and
as a result high transferability. We demonstrate our results on several
architectures and multiple downstream tasks, including both single-label and
multi-label classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RangeUDF: Semantic Surface Reconstruction from 3D Point Clouds. (arXiv:2204.09138v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09138">
<div class="article-summary-box-inner">
<span><p>We present RangeUDF, a new implicit representation based framework to recover
the geometry and semantics of continuous 3D scene surfaces from point clouds.
Unlike occupancy fields or signed distance fields which can only model closed
3D surfaces, our approach is not restricted to any type of topology. Being
different from the existing unsigned distance fields, our framework does not
suffer from any surface ambiguity. In addition, our RangeUDF can jointly
estimate precise semantics for continuous surfaces. The key to our approach is
a range-aware unsigned distance function together with a surface-oriented
semantic segmentation module. Extensive experiments show that RangeUDF clearly
surpasses state-of-the-art approaches for surface reconstruction on four point
cloud datasets. Moreover, RangeUDF demonstrates superior generalization
capability across multiple unseen datasets, which is nearly impossible for all
existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Camera Multiple 3D Object Tracking on the Move for Autonomous Vehicles. (arXiv:2204.09151v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09151">
<div class="article-summary-box-inner">
<span><p>The development of autonomous vehicles provides an opportunity to have a
complete set of camera sensors capturing the environment around the car. Thus,
it is important for object detection and tracking to address new challenges,
such as achieving consistent results across views of cameras. To address these
challenges, this work presents a new Global Association Graph Model with Link
Prediction approach to predict existing tracklets location and link detections
with tracklets via cross-attention motion modeling and appearance
re-identification. This approach aims at solving issues caused by inconsistent
3D object detection. Moreover, our model exploits to improve the detection
accuracy of a standard 3D object detector in the nuScenes detection challenge.
The experimental results on the nuScenes dataset demonstrate the benefits of
the proposed method to produce SOTA performance on the existing vision-based
tracking dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Performance Evaluation of Action Recognition Models on Transcoded Low Quality Videos. (arXiv:2204.09166v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09166">
<div class="article-summary-box-inner">
<span><p>In the design of action recognition models, the quality of videos in the
dataset is an important issue, however the trade-off between the quality and
performance is often ignored. In general, action recognition models are trained
and tested on high-quality videos, but in actual situations where action
recognition models are deployed, sometimes it might not be assumed that the
input videos are of high quality. In this study, we report qualitative
evaluations of action recognition models for the quality degradation associated
with transcoding by JPEG and H.264/AVC. Experimental results are shown for
evaluating the performance of pre-trained models on the transcoded validation
videos of Kinetics400. The models are also trained on the transcoded training
videos. From these results, we quantitatively show the degree of degradation of
the model performance with respect to the degradation of the video quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Monocular Depth Priors in Visual-Inertial Initialization. (arXiv:2204.09171v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09171">
<div class="article-summary-box-inner">
<span><p>Visual-inertial odometry (VIO) is the pose estimation backbone for most AR/VR
and autonomous robotic systems today, in both academia and industry. However,
these systems are highly sensitive to the initialization of key parameters such
as sensor biases, gravity direction, and metric scale. In practical scenarios
where high-parallax or variable acceleration assumptions are rarely met (e.g.
hovering aerial robot, smartphone AR user not gesticulating with phone),
classical visual-inertial initialization formulations often become
ill-conditioned and/or fail to meaningfully converge. In this paper we target
visual-inertial initialization specifically for these low-excitation scenarios
critical to in-the-wild usage. We propose to circumvent the limitations of
classical visual-inertial structure-from-motion (SfM) initialization by
incorporating a new learning-based measurement as a higher-level input. We
leverage learned monocular depth images (mono-depth) to constrain the relative
depth of features, and upgrade the mono-depth to metric scale by jointly
optimizing for its scale and shift. Our experiments show a significant
improvement in problem conditioning compared to a classical formulation for
visual-inertial initialization, and demonstrate significant accuracy and
robustness improvements relative to the state-of-the-art on public benchmarks,
particularly under motion-restricted scenarios. We further extend this
improvement to implementation within an existing odometry system to illustrate
the impact of our improved initialization method on resulting tracking
trajectories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstruction-Aware Prior Distillation for Semi-supervised Point Cloud Completion. (arXiv:2204.09186v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09186">
<div class="article-summary-box-inner">
<span><p>Point clouds scanned by real-world sensors are always incomplete, irregular,
and noisy, making the point cloud completion task become increasingly more
important. Though many point cloud completion methods have been proposed, most
of them require a large number of paired complete-incomplete point clouds for
training, which is labor exhausted. In contrast, this paper proposes a novel
Reconstruction-Aware Prior Distillation semi-supervised point cloud completion
method named RaPD, which takes advantage of a two-stage training scheme to
reduce the dependence on a large-scale paired dataset. In training stage 1, the
so-called deep semantic prior is learned from both unpaired complete and
unpaired incomplete point clouds using a reconstruction-aware pretraining
process. While in training stage 2, we introduce a semi-supervised prior
distillation process, where an encoder-decoder-based completion network is
trained by distilling the prior into the network utilizing only a small number
of paired training samples. A self-supervised completion module is further
introduced, excavating the value of a large number of unpaired incomplete point
clouds, leading to an increase in the network's performance. Extensive
experiments on several widely used datasets demonstrate that RaPD, the first
semi-supervised point cloud completion method, achieves superior performance to
previous methods on both homologous and heterologous scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NTIRE 2022 Challenge on Stereo Image Super-Resolution: Methods and Results. (arXiv:2204.09197v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09197">
<div class="article-summary-box-inner">
<span><p>In this paper, we summarize the 1st NTIRE challenge on stereo image
super-resolution (restoration of rich details in a pair of low-resolution
stereo images) with a focus on new solutions and results. This challenge has 1
track aiming at the stereo image super-resolution problem under a standard
bicubic degradation. In total, 238 participants were successfully registered,
and 21 teams competed in the final testing phase. Among those participants, 20
teams successfully submitted results with PSNR (RGB) scores better than the
baseline. This challenge establishes a new benchmark for stereo image SR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction. (arXiv:2204.09204v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09204">
<div class="article-summary-box-inner">
<span><p>When applying multi-instance learning (MIL) to make predictions for bags of
instances, the prediction accuracy of an instance often depends on not only the
instance itself but also its context in the corresponding bag. From the
viewpoint of causal inference, such bag contextual prior works as a confounder
and may result in model robustness and interpretability issues. Focusing on
this problem, we propose a novel interventional multi-instance learning (IMIL)
framework to achieve deconfounded instance-level prediction. Unlike traditional
likelihood-based strategies, we design an Expectation-Maximization (EM)
algorithm based on causal intervention, providing a robust instance selection
in the training phase and suppressing the bias caused by the bag contextual
prior. Experiments on pathological image analysis demonstrate that our IMIL
method substantially reduces false positives and outperforms state-of-the-art
MIL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Interference Exist When Training a Once-For-All Network?. (arXiv:2204.09210v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09210">
<div class="article-summary-box-inner">
<span><p>The Once-For-All (OFA) method offers an excellent pathway to deploy a trained
neural network model into multiple target platforms by utilising the
supernet-subnet architecture. Once trained, a subnet can be derived from the
supernet (both architecture and trained weights) and deployed directly to the
target platform with little to no retraining or fine-tuning. To train the
subnet population, OFA uses a novel training method called Progressive
Shrinking (PS) which is designed to limit the negative impact of interference
during training. It is believed that higher interference during training
results in lower subnet population accuracies. In this work we take a second
look at this interference effect. Surprisingly, we find that interference
mitigation strategies do not have a large impact on the overall subnet
population performance. Instead, we find the subnet architecture selection bias
during training to be a more important aspect. To show this, we propose a
simple-yet-effective method called Random Subnet Sampling (RSS), which does not
have mitigation on the interference effect. Despite no mitigation, RSS is able
to produce a better performing subnet population than PS in four
small-to-medium-sized datasets; suggesting that the interference effect does
not play a pivotal role in these datasets. Due to its simplicity, RSS provides
a $1.9\times$ reduction in training times compared to PS. A $6.1\times$
reduction can also be achieved with a reasonable drop in performance when the
number of RSS training epochs are reduced. Code available at
https://github.com/Jordan-HS/RSS-Interference-CVPRW2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Progressive High Dynamic Range Image Restoration via Attention and Alignment Network. (arXiv:2204.09213v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09213">
<div class="article-summary-box-inner">
<span><p>HDR is an important part of computational photography technology. In this
paper, we propose a lightweight neural network called Efficient
Attention-and-alignment-guided Progressive Network (EAPNet) for the challenge
NTIRE 2022 HDR Track 1 and Track 2. We introduce a multi-dimensional
lightweight encoding module to extract features. Besides, we propose
Progressive Dilated U-shape Block (PDUB) that can be a progressive
plug-and-play module for dynamically tuning MAccs and PSNR. Finally, we use
fast and low-power feature-align module to deal with misalignment problem in
place of the time-consuming Deformable Convolutional Network (DCN). The
experiments show that our method achieves about 20 times compression on MAccs
with better mu-PSNR and PSNR compared to the state-of-the-art method. We got
the second place of both two tracks during the testing phase. Figure1. shows
the visualized result of NTIRE 2022 HDR challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision System of Curling Robots: Thrower and Skip. (arXiv:2204.09221v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09221">
<div class="article-summary-box-inner">
<span><p>We built a vision system of curling robot which can be expected to play with
human curling player. Basically, we built two types of vision systems for
thrower and skip robots, respectively. First, the thrower robot drives towards
a given point of curling sheet to release a stone. Our vision system in the
thrower robot initialize 3DoF pose on two dimensional curling sheet and updates
the pose to decide for the decision of stone release. Second, the skip robot
stands at the opposite side of the thrower robot and monitors the state of the
game to make a strategic decision. Our vision system in the skip robot
recognize every stones on the curling sheet precisely. Since the viewpoint is
quite perspective, many stones are occluded by each others so it is challenging
to estimate the accurate position of stone. Thus, we recognize the ellipses of
stone handles outline to find the exact midpoint of the stones using
perspective Hough transform. Furthermore, we perform tracking of a thrown stone
to produce a trajectory for ice condition analysis. Finally, we implemented our
vision systems on two mobile robots and successfully perform a single turn and
even careful gameplay. Specifically, our vision system includes three cameras
with different viewpoint for their respective purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-LITE: Learning Transferable Visual Models with External Knowledge. (arXiv:2204.09222v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09222">
<div class="article-summary-box-inner">
<span><p>Recent state-of-the-art computer vision systems are trained from natural
language supervision, ranging from simple object category names to descriptive
captions. This free form of supervision ensures high generality and usability
of the learned visual models, based on extensive heuristics on data collection
to cover as many visual concepts as possible. Alternatively, learning with
external knowledge about images is a promising way which leverages a much more
structured source of supervision. In this paper, we propose K-LITE
(Knowledge-augmented Language-Image Training and Evaluation), a simple strategy
to leverage external knowledge to build transferable visual systems: In
training, it enriches entities in natural language with WordNet and Wiktionary
knowledge, leading to an efficient and scalable approach to learning image
representations that can understand both visual concepts and their knowledge;
In evaluation, the natural language is also augmented with external knowledge
and then used to reference learned visual concepts (or describe new ones) to
enable zero-shot and few-shot transfer of the pre-trained models. We study the
performance of K-LITE on two important computer vision problems, image
classification and object detection, benchmarking on 20 and 13 different
existing datasets, respectively. The proposed knowledge-augmented models show
significant improvement in transfer learning performance over existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dark Spot Detection from SAR Images Based on Superpixel Deeper Graph Convolutional Network. (arXiv:2204.09230v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09230">
<div class="article-summary-box-inner">
<span><p>Synthetic Aperture Radar (SAR) is the main instrument utilized for the
detection of oil slicks on the ocean surface. In SAR images, some areas
affected by ocean phenomena, such as rain cells, upwellings, and internal
waves, or discharge from oil spills appear as dark spots on images. Dark spot
detection is the first step in the detection of oil spills, which then become
oil slick candidates. The accuracy of dark spot segmentation ultimately affects
the accuracy of oil slick identification. Although some advanced deep learning
methods that use pixels as processing units perform well in remote sensing
image semantic segmentation, detecting some dark spots with weak boundaries
from noisy SAR images remains a huge challenge. We propose a dark spot
detection method based on superpixels deeper graph convolutional networks
(SGDCN) in this paper, which takes the superpixels as the processing units and
extracts features for each superpixel. The features calculated from superpixel
regions are more robust than those from fixed pixel neighborhoods. To reduce
the difficulty of learning tasks, we discard irrelevant features and obtain an
optimal subset of features. After superpixel segmentation, the images are
transformed into graphs with superpixels as nodes, which are fed into the
deeper graph convolutional neural network for node classification. This graph
neural network uses a differentiable aggregation function to aggregate the
features of nodes and neighbors to form more advanced features. It is the first
time using it for dark spot detection. To validate our method, we mark all dark
spots on six SAR images covering the Baltic Sea and construct a dark spots
detection dataset, which has been made publicly available
(https://drive.google.com/drive/folders/12UavrntkDSPrItISQ8iGefXn2gIZHxJ6?usp=sharing).
The experimental results demonstrate that our proposed SGDCN is robust and
effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual-based Positioning and Pose Estimation. (arXiv:2204.09232v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09232">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning and computer vision offer an excellent
opportunity to investigate high-level visual analysis tasks such as human
localization and human pose estimation. Although the performance of human
localization and human pose estimation has significantly improved in recent
reports, they are not perfect and erroneous localization and pose estimation
can be expected among video frames. Studies on the integration of these
techniques into a generic pipeline that is robust to noise introduced from
those errors are still lacking. This paper fills the missing study. We explored
and developed two working pipelines that suited the visual-based positioning
and pose estimation tasks. Analyses of the proposed pipelines were conducted on
a badminton game. We showed that the concept of tracking by detection could
work well, and errors in position and pose could be effectively handled by a
linear interpolation technique using information from nearby frames. The
results showed that the Visual-based Positioning and Pose Estimation could
deliver position and pose estimations with good spatial and temporal
resolutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving The Long-Tailed Problem via Intra- and Inter-Category Balance. (arXiv:2204.09234v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09234">
<div class="article-summary-box-inner">
<span><p>Benchmark datasets for visual recognition assume that data is uniformly
distributed, while real-world datasets obey long-tailed distribution. Current
approaches handle the long-tailed problem to transform the long-tailed dataset
to uniform distribution by re-sampling or re-weighting strategies. These
approaches emphasize the tail classes but ignore the hard examples in head
classes, which result in performance degradation. In this paper, we propose a
novel gradient harmonized mechanism with category-wise adaptive precision to
decouple the difficulty and sample size imbalance in the long-tailed problem,
which are correspondingly solved via intra- and inter-category balance
strategies. Specifically, intra-category balance focuses on the hard examples
in each category to optimize the decision boundary, while inter-category
balance aims to correct the shift of decision boundary by taking each category
as a unit. Extensive experiments demonstrate that the proposed method
consistently outperforms other approaches on all the datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-based Cross-Modal Retrieval with Probabilistic Representations. (arXiv:2204.09268v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09268">
<div class="article-summary-box-inner">
<span><p>Probabilistic embeddings have proven useful for capturing polysemous word
meanings, as well as ambiguity in image matching. In this paper, we study the
advantages of probabilistic embeddings in a cross-modal setting (i.e., text and
images), and propose a simple approach that replaces the standard vector point
embeddings in extant image-text matching models with probabilistic
distributions that are parametrically learned. Our guiding hypothesis is that
the uncertainty encoded in the probabilistic embeddings captures the
cross-modal ambiguity in the input instances, and that it is through capturing
this uncertainty that the probabilistic models can perform better at downstream
tasks, such as image-to-text or text-to-image retrieval. Through extensive
experiments on standard and new benchmarks, we show a consistent advantage for
probabilistic representations in cross-modal retrieval, and validate the
ability of our embeddings to capture uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Video-based Action Quality Assessment. (arXiv:2204.09271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09271">
<div class="article-summary-box-inner">
<span><p>Human action recognition and analysis have great demand and important
application significance in video surveillance, video retrieval, and
human-computer interaction. The task of human action quality evaluation
requires the intelligent system to automatically and objectively evaluate the
action completed by the human. The action quality assessment model can reduce
the human and material resources spent in action evaluation and reduce
subjectivity. In this paper, we provide a comprehensive survey of existing
papers on video-based action quality assessment. Different from human action
recognition, the application scenario of action quality assessment is
relatively narrow. Most of the existing work focuses on sports and medical
care. We first introduce the definition and challenges of human action quality
assessment. Then we present the existing datasets and evaluation metrics. In
addition, we summarized the methods of sports and medical care according to the
model categories and publishing institutions according to the characteristics
of the two fields. At the end, combined with recent work, the promising
development direction in action quality assessment is discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sound-Guided Semantic Video Generation. (arXiv:2204.09273v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09273">
<div class="article-summary-box-inner">
<span><p>The recent success in StyleGAN demonstrates that pre-trained StyleGAN latent
space is useful for realistic video generation. However, the generated motion
in the video is usually not semantically meaningful due to the difficulty of
determining the direction and magnitude in the StyleGAN latent space. In this
paper, we propose a framework to generate realistic videos by leveraging
multimodal (sound-image-text) embedding space. As sound provides the temporal
contexts of the scene, our framework learns to generate a video that is
semantically consistent with sound. First, our sound inversion module maps the
audio directly into the StyleGAN latent space. We then incorporate the
CLIP-based multimodal embedding space to further provide the audio-visual
relationships. Finally, the proposed frame generator learns to find the
trajectory in the latent space which is coherent with the corresponding sound
and generates a video in a hierarchical manner. We provide the new
high-resolution landscape video dataset (audio-visual pair) for the
sound-guided video generation task. The experiments show that our model
outperforms the state-of-the-art methods in terms of video quality. We further
show several applications including image and video editing to verify the
effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Situational Perception Guided Image Matting. (arXiv:2204.09276v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09276">
<div class="article-summary-box-inner">
<span><p>Most automatic matting methods try to separate the salient foreground from
the background. However, the insufficient quantity and subjective bias of the
current existing matting datasets make it difficult to fully explore the
semantic association between object-to-object and object-to-environment in a
given image. In this paper, we propose a Situational Perception Guided Image
Matting (SPG-IM) method that mitigates subjective bias of matting annotations
and captures sufficient situational perception information for better global
saliency distilled from the visual-to-textual task. SPG-IM can better associate
inter-objects and object-to-environment saliency, and compensate the subjective
nature of image matting and its expensive annotation. We also introduce a
textual Semantic Transformation (TST) module that can effectively transform and
integrate the semantic feature stream to guide the visual representations. In
addition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed
to adaptively switch multi-scale receptive fields and focal points to enhance
both global and local details. Extensive experiments demonstrate the
effectiveness of situational perception guidance from the visual-to-textual
tasks on image matting, and our model outperforms the state-of-the-art methods.
We also analyze the significance of different components in our model. The code
will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforced Structured State-Evolution for Vision-Language Navigation. (arXiv:2204.09280v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09280">
<div class="article-summary-box-inner">
<span><p>Vision-and-language Navigation (VLN) task requires an embodied agent to
navigate to a remote location following a natural language instruction.
Previous methods usually adopt a sequence model (e.g., Transformer and LSTM) as
the navigator. In such a paradigm, the sequence model predicts action at each
step through a maintained navigation state, which is generally represented as a
one-dimensional vector. However, the crucial navigation clues (i.e.,
object-level environment layout) for embodied navigation task is discarded
since the maintained vector is essentially unstructured. In this paper, we
propose a novel Structured state-Evolution (SEvol) model to effectively
maintain the environment layout clues for VLN. Specifically, we utilise the
graph-based feature to represent the navigation state instead of the
vector-based state. Accordingly, we devise a Reinforced Layout clues Miner
(RLM) to mine and detect the most crucial layout graph for long-term navigation
via a customised reinforcement learning strategy. Moreover, the Structured
Evolving Module (SEM) is proposed to maintain the structured graph-based state
during navigation, where the state is gradually evolved to learn the
object-level spatial-temporal relationship. The experiments on the R2R and R4R
datasets show that the proposed SEvol model improves VLN models' performance by
large margins, e.g., +3% absolute SPL accuracy for NvEM and +8% for EnvDrop on
the R2R test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-Object Interaction Detection via Disentangled Transformer. (arXiv:2204.09290v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09290">
<div class="article-summary-box-inner">
<span><p>Human-Object Interaction Detection tackles the problem of joint localization
and classification of human object interactions. Existing HOI transformers
either adopt a single decoder for triplet prediction, or utilize two parallel
decoders to detect individual objects and interactions separately, and compose
triplets by a matching process. In contrast, we decouple the triplet prediction
into human-object pair detection and interaction classification. Our main
motivation is that detecting the human-object instances and classifying
interactions accurately needs to learn representations that focus on different
regions. To this end, we present Disentangled Transformer, where both encoder
and decoder are disentangled to facilitate learning of two sub-tasks. To
associate the predictions of disentangled decoders, we first generate a unified
representation for HOI triplets with a base decoder, and then utilize it as
input feature of each disentangled decoder. Extensive experiments show that our
method outperforms prior work on two public HOI benchmarks by a sizeable
margin. Code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A 3-stage Spectral-spatial Method for Hyperspectral Image Classification. (arXiv:2204.09294v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09294">
<div class="article-summary-box-inner">
<span><p>Hyperspectral images often have hundreds of spectral bands of different
wavelengths captured by aircraft or satellites that record land coverage.
Identifying detailed classes of pixels becomes feasible due to the enhancement
in spectral and spatial resolution of hyperspectral images. In this work, we
propose a novel framework that utilizes both spatial and spectral information
for classifying pixels in hyperspectral images. The method consists of three
stages. In the first stage, the pre-processing stage, Nested Sliding Window
algorithm is used to reconstruct the original data by {enhancing the
consistency of neighboring pixels} and then Principal Component Analysis is
used to reduce the dimension of data. In the second stage, Support Vector
Machines are trained to estimate the pixel-wise probability map of each class
using the spectral information from the images. Finally, a smoothed total
variation model is applied to smooth the class probability vectors by {ensuring
spatial connectivity} in the images. We demonstrate the superiority of our
method against three state-of-the-art algorithms on six benchmark hyperspectral
data sets with 10 to 50 training labels for each class. The results show that
our method gives the overall best performance in accuracy. Especially, our gain
in accuracy increases when the number of labeled pixels decreases and therefore
our method is more advantageous to be applied to problems with small training
set. Hence it is of great practical significance since expert annotations are
often expensive and difficult to collect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Restoration in Non-Linear Filtering Domain using MDB approach. (arXiv:2204.09296v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09296">
<div class="article-summary-box-inner">
<span><p>This paper proposes a new technique based on a non-linear Minmax Detector
Based (MDB) filter for image restoration. The aim of image enhancement is to
reconstruct the true image from the corrupted image. The process of image
acquisition frequently leads to degradation and the quality of the digitized
image becomes inferior to the original image. Image degradation can be due to
the addition of different types of noise in the original image. Image noise can
be modelled of many types and impulse noise is one of them. Impulse noise
generates pixels with gray value not consistent with their local neighbourhood.
It appears as a sprinkle of both light and dark or only light spots in the
image. Filtering is a technique for enhancing the image. Linear filter is the
filtering in which the value of an output pixel is a linear combination of
neighborhood values, which can produce blur in the image. Thus a variety of
smoothing techniques have been developed that are non linear. Median filter is
the one of the most popular non-linear filter. When considering a small
neighborhood it is highly efficient but for large window and in case of high
noise it gives rise to more blurring to image. The Centre Weighted Mean (CWM)
filter has got a better average performance over the median filter. However the
original pixel corrupted and noise reduction is substantial under high noise
condition. Hence this technique has also blurring affect on the image. To
illustrate the superiority of the proposed approach, the proposed new scheme
has been simulated along with the standard ones and various restored
performance measures have been compared.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Non-linear Filtering Technique for Image Restoration. (arXiv:2204.09302v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09302">
<div class="article-summary-box-inner">
<span><p>Removing noise from the any processed images is very important. Noise should
be removed in such a way that important information of image should be
preserved. A decisionbased nonlinear algorithm for elimination of band lines,
drop lines, mark, band lost and impulses in images is presented in this paper.
The algorithm performs two simultaneous operations, namely, detection of
corrupted pixels and evaluation of new pixels for replacing the corrupted
pixels. Removal of these artifacts is achieved without damaging edges and
details. However, the restricted window size renders median operation less
effective whenever noise is excessive in that case the proposed algorithm
automatically switches to mean filtering. The performance of the algorithm is
analyzed in terms of Mean Square Error [MSE], Peak-Signal-to-Noise Ratio
[PSNR], Signal-to-Noise Ratio Improved [SNRI], Percentage Of Noise Attenuated
[PONA], and Percentage Of Spoiled Pixels [POSP]. This is compared with standard
algorithms already in use and improved performance of the proposed algorithm is
presented. The advantage of the proposed algorithm is that a single algorithm
can replace several independent algorithms which are required for removal of
different artifacts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention in Attention: Modeling Context Correlation for Efficient Video Classification. (arXiv:2204.09303v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09303">
<div class="article-summary-box-inner">
<span><p>Attention mechanisms have significantly boosted the performance of video
classification neural networks thanks to the utilization of perspective
contexts. However, the current research on video attention generally focuses on
adopting a specific aspect of contexts (e.g., channel, spatial/temporal, or
global context) to refine the features and neglects their underlying
correlation when computing attentions. This leads to incomplete context
utilization and hence bears the weakness of limited performance improvement. To
tackle the problem, this paper proposes an efficient attention-in-attention
(AIA) method for element-wise feature refinement, which investigates the
feasibility of inserting the channel context into the spatio-temporal attention
learning module, referred to as CinST, and also its reverse variant, referred
to as STinC. Specifically, we instantiate the video feature contexts as
dynamics aggregated along a specific axis with global average and max pooling
operations. The workflow of an AIA module is that the first attention block
uses one kind of context information to guide the gating weights calculation of
the second attention that targets at the other context. Moreover, all the
computational operations in attention units act on the pooled dimension, which
results in quite few computational cost increase ($&lt;$0.02\%). To verify our
method, we densely integrate it into two classical video network backbones and
conduct extensive experiments on several standard video classification
benchmarks. The source code of our AIA is available at
\url{https://github.com/haoyanbin918/Attention-in-Attention}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deeper Look into Aleatoric and Epistemic Uncertainty Disentanglement. (arXiv:2204.09308v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09308">
<div class="article-summary-box-inner">
<span><p>Neural networks are ubiquitous in many tasks, but trusting their predictions
is an open issue. Uncertainty quantification is required for many applications,
and disentangled aleatoric and epistemic uncertainties are best. In this paper,
we generalize methods to produce disentangled uncertainties to work with
different uncertainty quantification methods, and evaluate their capability to
produce disentangled uncertainties. Our results show that: there is an
interaction between learning aleatoric and epistemic uncertainty, which is
unexpected and violates assumptions on aleatoric uncertainty, some methods like
Flipout produce zero epistemic uncertainty, aleatoric uncertainty is unreliable
in the out-of-distribution setting, and Ensembles provide overall the best
disentangling quality. We also explore the error produced by the number of
samples hyper-parameter in the sampling softmax function, recommending N &gt; 100
samples. We expect that our formulation and results help practitioners and
researchers choose uncertainty methods and expand the use of disentangled
uncertainties, as well as motivate additional research into this topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video: Dataset, Methods and Results. (arXiv:2204.09314v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09314">
<div class="article-summary-box-inner">
<span><p>This paper reviews the NTIRE 2022 Challenge on Super-Resolution and Quality
Enhancement of Compressed Video. In this challenge, we proposed the LDV 2.0
dataset, which includes the LDV dataset (240 videos) and 95 additional videos.
This challenge includes three tracks. Track 1 aims at enhancing the videos
compressed by HEVC at a fixed QP. Track 2 and Track 3 target both the
super-resolution and quality enhancement of HEVC compressed video. They require
x2 and x4 super-resolution, respectively. The three tracks totally attract more
than 600 registrations. In the test phase, 8 teams, 8 teams and 12 teams
submitted the final results to Tracks 1, 2 and 3, respectively. The proposed
methods and solutions gauge the state-of-the-art of super-resolution and
quality enhancement of compressed video. The proposed LDV 2.0 dataset is
available at https://github.com/RenYang-home/LDV_dataset. The homepage of this
challenge (including open-sourced codes) is at
https://github.com/RenYang-home/NTIRE22_VEnh_SR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logarithmic Morphological Neural Nets robust to lighting variations. (arXiv:2204.09319v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09319">
<div class="article-summary-box-inner">
<span><p>Morphological neural networks allow to learn the weights of a structuring
function knowing the desired output image. However, those networks are not
intrinsically robust to lighting variations in images with an optical cause,
such as a change of light intensity. In this paper, we introduce a
morphological neural network which possesses such a robustness to lighting
variations. It is based on the recent framework of Logarithmic Mathematical
Morphology (LMM), i.e. Mathematical Morphology defined with the Logarithmic
Image Processing (LIP) model. This model has a LIP additive law which simulates
in images a variation of the light intensity. We especially learn the
structuring function of a LMM operator robust to those variations, namely : the
map of LIP-additive Asplund distances. Results in images show that our neural
network verifies the required property.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpiderNet: Hybrid Differentiable-Evolutionary Architecture Search via Train-Free Metrics. (arXiv:2204.09320v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09320">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) algorithms are intended to remove the burden
of manual neural network design, and have shown to be capable of designing
excellent models for a variety of well-known problems. However, these
algorithms require a variety of design parameters in the form of user
configuration or hard-coded decisions which limit the variety of networks that
can be discovered. This means that NAS algorithms do not eliminate model design
tuning, they instead merely shift the burden of where that tuning needs to be
applied. In this paper, we present SpiderNet, a hybrid
differentiable-evolutionary and hardware-aware algorithm that rapidly and
efficiently produces state-of-the-art networks. More importantly, SpiderNet is
a proof-of-concept of a minimally-configured NAS algorithm; the majority of
design choices seen in other algorithms are incorporated into SpiderNet's
dynamically-evolving search space, minimizing the number of user choices to
just two: reduction cell count and initial channel count. SpiderNet produces
models highly-competitive with the state-of-the-art, and outperforms random
search in accuracy, runtime, memory size, and parameter count.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Learning for Sonar Image Classification. (arXiv:2204.09323v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09323">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has proved to be a powerful approach to learn image
representations without the need of large labeled datasets. For underwater
robotics, it is of great interest to design computer vision algorithms to
improve perception capabilities such as sonar image classification. Due to the
confidential nature of sonar imaging and the difficulty to interpret sonar
images, it is challenging to create public large labeled sonar datasets to
train supervised learning algorithms. In this work, we investigate the
potential of three self-supervised learning methods (RotNet, Denoising
Autoencoders, and Jigsaw) to learn high-quality sonar image representation
without the need of human labels. We present pre-training and transfer learning
results on real-life sonar image datasets. Our results indicate that
self-supervised pre-training yields classification performance comparable to
supervised pre-training in a few-shot transfer learning setup across all three
methods. Code and self-supervised pre-trained models are be available at
https://github.com/agrija9/ssl-sonar-images
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NFormer: Robust Person Re-identification with Neighbor Transformer. (arXiv:2204.09331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09331">
<div class="article-summary-box-inner">
<span><p>Person re-identification aims to retrieve persons in highly varying settings
across different cameras and scenarios, in which robust and discriminative
representation learning is crucial. Most research considers learning
representations from single images, ignoring any potential interactions between
them. However, due to the high intra-identity variations, ignoring such
interactions typically leads to outlier features. To tackle this issue, we
propose a Neighbor Transformer Network, or NFormer, which explicitly models
interactions across all input images, thus suppressing outlier features and
leading to more robust representations overall. As modelling interactions
between enormous amount of images is a massive task with lots of distractors,
NFormer introduces two novel modules, the Landmark Agent Attention, and the
Reciprocal Neighbor Softmax. Specifically, the Landmark Agent Attention
efficiently models the relation map between images by a low-rank factorization
with a few landmarks in feature space. Moreover, the Reciprocal Neighbor
Softmax achieves sparse attention to relevant -- rather than all -- neighbors
only, which alleviates interference of irrelevant representations and further
relieves the computational burden. In experiments on four large-scale datasets,
NFormer achieves a new state-of-the-art. The code is released at
\url{https://github.com/haochenheheda/NFormer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Cardiac Segmentation: Towards Structure Mutual Information Maximization. (arXiv:2204.09334v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09334">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation approaches have recently succeeded in various
medical image segmentation tasks. The reported works often tackle the domain
shift problem by aligning the domain-invariant features and minimizing the
domain-specific discrepancies. That strategy works well when the difference
between a specific domain and between different domains is slight. However, the
generalization ability of these models on diverse imaging modalities remains a
significant challenge. This paper introduces UDA-VAE++, an unsupervised domain
adaptation framework for cardiac segmentation with a compact loss function
lower bound. To estimate this new lower bound, we develop a novel Structure
Mutual Information Estimation (SMIE) block with a global estimator, a local
estimator, and a prior information matching estimator to maximize the mutual
information between the reconstruction and segmentation tasks. Specifically, we
design a novel sequential reparameterization scheme that enables information
flow and variance correction from the low-resolution latent space to the
high-resolution latent space. Comprehensive experiments on benchmark cardiac
segmentation datasets demonstrate that our model outperforms previous
state-of-the-art qualitatively and quantitatively. The code is available at
https://github.com/LOUEY233/Toward-Mutual-Information}{https://github.com/LOUEY233/Toward-Mutual-Information
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Point Clouds: A Survey. (arXiv:2204.09337v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09337">
<div class="article-summary-box-inner">
<span><p>Point cloud has drawn more and more research attention as well as real-world
applications. However, many of these applications (e.g. autonomous driving and
robotic manipulation) are actually based on sequential point clouds (i.e. four
dimensions) because the information of the static point cloud data could
provide is still limited. Recently, researchers put more and more effort into
sequential point clouds. This paper presents an extensive review of the deep
learning-based methods for sequential point cloud research including dynamic
flow estimation, object detection \&amp; tracking, point cloud segmentation, and
point cloud forecasting. This paper further summarizes and compares the
quantitative results of the reviewed methods over the public benchmark
datasets. Finally, this paper is concluded by discussing the challenges in the
current sequential point cloud research and pointing out insightful potential
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OutCast: Outdoor Single-image Relighting with Cast Shadows. (arXiv:2204.09341v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09341">
<div class="article-summary-box-inner">
<span><p>We propose a relighting method for outdoor images. Our method mainly focuses
on predicting cast shadows in arbitrary novel lighting directions from a single
image while also accounting for shading and global effects such the sun light
color and clouds. Previous solutions for this problem rely on reconstructing
occluder geometry, e.g. using multi-view stereo, which requires many images of
the scene. Instead, in this work we make use of a noisy off-the-shelf
single-image depth map estimation as a source of geometry. Whilst this can be a
good guide for some lighting effects, the resulting depth map quality is
insufficient for directly ray-tracing the shadows. Addressing this, we propose
a learned image space ray-marching layer that converts the approximate depth
map into a deep 3D representation that is fused into occlusion queries using a
learned traversal. Our proposed method achieves, for the first time,
state-of-the-art relighting results, with only a single image as input. For
supplementary material visit our project page at:
https://dgriffiths.uk/outcast.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing unsupervised learning to improve sward content prediction and herbage mass estimation. (arXiv:2204.09343v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09343">
<div class="article-summary-box-inner">
<span><p>Sward species composition estimation is a tedious one. Herbage must be
collected in the field, manually separated into components, dried and weighed
to estimate species composition. Deep learning approaches using neural networks
have been used in previous work to propose faster and more cost efficient
alternatives to this process by estimating the biomass information from a
picture of an area of pasture alone. Deep learning approaches have, however,
struggled to generalize to distant geographical locations and necessitated
further data collection to retrain and perform optimally in different climates.
In this work, we enhance the deep learning solution by reducing the need for
ground-truthed (GT) images when training the neural network. We demonstrate how
unsupervised contrastive learning can be used in the sward composition
prediction problem and compare with the state-of-the-art on the publicly
available GrassClover dataset collected in Denmark as well as a more recent
dataset from Ireland where we tackle herbage mass and height estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cyber-Forensic Review of Human Footprint and Gait for Personal Identification. (arXiv:2204.09344v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09344">
<div class="article-summary-box-inner">
<span><p>The human footprint is having a unique set of ridges unmatched by any other
human being, and therefore it can be used in different identity documents for
example birth certificate, Indian biometric identification system AADHAR card,
driving license, PAN card, and passport. There are many instances of the crime
scene where an accused must walk around and left the footwear impressions as
well as barefoot prints and therefore, it is very crucial to recovering the
footprints from identifying the criminals. Footprint-based biometric is a
considerably newer technique for personal identification. Fingerprints, retina,
iris and face recognition are the methods most useful for attendance record of
the person. This time the world is facing the problem of global terrorism. It
is challenging to identify the terrorist because they are living as regular as
the citizens do. Their soft target includes the industries of special interests
such as defence, silicon and nanotechnology chip manufacturing units, pharmacy
sectors. They pretend themselves as religious persons, so temples and other
holy places, even in markets is in their targets. These are the places where
one can obtain their footprints quickly. The gait itself is sufficient to
predict the behaviour of the suspects. The present research is driven to
identify the usefulness of footprint and gait as an alternative to personal
identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive Dual Stream Siamese U-net for Flood Detection on Multi-temporal Sentinel-1 Data. (arXiv:2204.09387v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09387">
<div class="article-summary-box-inner">
<span><p>Due to climate and land-use change, natural disasters such as flooding have
been increasing in recent years. Timely and reliable flood detection and
mapping can help emergency response and disaster management. In this work, we
propose a flood detection network using bi-temporal SAR acquisitions. The
proposed segmentation network has an encoder-decoder architecture with two
Siamese encoders for pre and post-flood images. The network's feature maps are
fused and enhanced using attention blocks to achieve more accurate detection of
the flooded areas. Our proposed network is evaluated on publicly available
Sen1Flood11 benchmark dataset. The network outperformed the existing
state-of-the-art (uni-temporal) flood detection method by 6\% IOU. The
experiments highlight that the combination of bi-temporal SAR data with an
effective network architecture achieves more accurate flood detection than
uni-temporal methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Epistemic Uncertainty-Weighted Loss for Visual Bias Mitigation. (arXiv:2204.09389v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09389">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are highly susceptible to learning biases in visual
data. While various methods have been proposed to mitigate such bias, the
majority require explicit knowledge of the biases present in the training data
in order to mitigate. We argue the relevance of exploring methods which are
completely ignorant of the presence of any bias, but are capable of identifying
and mitigating them. Furthermore, we propose using Bayesian neural networks
with an epistemic uncertainty-weighted loss function to dynamically identify
potential bias in individual training samples and to weight them during
training. We find a positive correlation between samples subject to bias and
higher epistemic uncertainties. Finally, we show the method has potential to
mitigate visual bias on a bias benchmark dataset and on a real-world face
detection problem, and we consider the merits and weaknesses of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Scratches: Deployable Attacks to CNN Classifiers. (arXiv:2204.09397v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09397">
<div class="article-summary-box-inner">
<span><p>A growing body of work has shown that deep neural networks are susceptible to
adversarial examples. These take the form of small perturbations applied to the
model's input which lead to incorrect predictions. Unfortunately, most
literature focuses on visually imperceivable perturbations to be applied to
digital images that often are, by design, impossible to be deployed to physical
targets. We present Adversarial Scratches: a novel L0 black-box attack, which
takes the form of scratches in images, and which possesses much greater
deployability than other state-of-the-art attacks. Adversarial Scratches
leverage B\'ezier Curves to reduce the dimension of the search space and
possibly constrain the attack to a specific location. We test Adversarial
Scratches in several scenarios, including a publicly available API and images
of traffic signs. Results show that, often, our attack achieves higher fooling
rate than other deployable state-of-the-art methods, while requiring
significantly fewer queries and modifying very few pixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Case-Aware Adversarial Training. (arXiv:2204.09398v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09398">
<div class="article-summary-box-inner">
<span><p>The neural network (NN) becomes one of the most heated type of models in
various signal processing applications. However, NNs are extremely vulnerable
to adversarial examples (AEs). To defend AEs, adversarial training (AT) is
believed to be the most effective method while due to the intensive
computation, AT is limited to be applied in most applications. In this paper,
to resolve the problem, we design a generic and efficient AT improvement
scheme, namely case-aware adversarial training (CAT). Specifically, the
intuition stems from the fact that a very limited part of informative samples
can contribute to most of model performance. Alternatively, if only the most
informative AEs are used in AT, we can lower the computation complexity of AT
significantly as maintaining the defense effect. To achieve this, CAT achieves
two breakthroughs. First, a method to estimate the information degree of
adversarial examples is proposed for AE filtering. Second, to further enrich
the information that the NN can obtain from AEs, CAT involves a weight
estimation and class-level balancing based sampling strategy to increase the
diversity of AT at each iteration. Extensive experiments show that CAT is
faster than vanilla AT by up to 3x while achieving competitive defense effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Probabilistic Time-Evolving Approach to Scanpath Prediction. (arXiv:2204.09404v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09404">
<div class="article-summary-box-inner">
<span><p>Human visual attention is a complex phenomenon that has been studied for
decades. Within it, the particular problem of scanpath prediction poses a
challenge, particularly due to the inter- and intra-observer variability, among
other reasons. Besides, most existing approaches to scanpath prediction have
focused on optimizing the prediction of a gaze point given the previous ones.
In this work, we present a probabilistic time-evolving approach to scanpath
prediction, based on Bayesian deep learning. We optimize our model using a
novel spatio-temporal loss function based on a combination of Kullback-Leibler
divergence and dynamic time warping, jointly considering the spatial and
temporal dimensions of scanpaths. Our scanpath prediction framework yields
results that outperform those of current state-of-the-art approaches, and are
almost on par with the human baseline, suggesting that our model is able to
generate scanpaths whose behavior closely resembles those of the real ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Moment Retrieval from Text Queries via Single Frame Annotation. (arXiv:2204.09409v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09409">
<div class="article-summary-box-inner">
<span><p>Video moment retrieval aims at finding the start and end timestamps of a
moment (part of a video) described by a given natural language query. Fully
supervised methods need complete temporal boundary annotations to achieve
promising results, which is costly since the annotator needs to watch the whole
moment. Weakly supervised methods only rely on the paired video and query, but
the performance is relatively poor. In this paper, we look closer into the
annotation process and propose a new paradigm called "glance annotation". This
paradigm requires the timestamp of only one single random frame, which we refer
to as a "glance", within the temporal boundary of the fully supervised
counterpart. We argue this is beneficial because comparing to weak supervision,
trivial cost is added yet more potential in performance is provided. Under the
glance annotation setting, we propose a method named as Video moment retrieval
via Glance Annotation (ViGA) based on contrastive learning. ViGA cuts the input
video into clips and contrasts between clips and queries, in which glance
guided Gaussian distributed weights are assigned to all clips. Our extensive
experiments indicate that ViGA achieves better results than the
state-of-the-art weakly supervised methods by a large margin, even comparable
to fully supervised methods in some cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HRPose: Real-Time High-Resolution 6D Pose Estimation Network Using Knowledge Distillation. (arXiv:2204.09429v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09429">
<div class="article-summary-box-inner">
<span><p>Real-time 6D object pose estimation is essential for many real-world
applications, such as robotic grasping and augmented reality. To achieve an
accurate object pose estimation from RGB images in real-time, we propose an
effective and lightweight model, namely High-Resolution 6D Pose Estimation
Network (HRPose). We adopt the efficient and small HRNetV2-W18 as a feature
extractor to reduce computational burdens while generating accurate 6D poses.
With only 33\% of the model size and lower computational costs, our HRPose
achieves comparable performance compared with state-of-the-art models.
Moreover, by transferring knowledge from a large model to our proposed HRPose
through output and feature-similarity distillations, the performance of our
HRPose is improved in effectiveness and efficiency. Numerical experiments on
the widely-used benchmark LINEMOD demonstrate the superiority of our proposed
HRPose against state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Mobile Food Recognition System for Dietary Assessment. (arXiv:2204.09432v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09432">
<div class="article-summary-box-inner">
<span><p>Food recognition is an important task for a variety of applications,
including managing health conditions and assisting visually impaired people.
Several food recognition studies have focused on generic types of food or
specific cuisines, however, food recognition with respect to Middle Eastern
cuisines has remained unexplored. Therefore, in this paper we focus on
developing a mobile friendly, Middle Eastern cuisine focused food recognition
application for assisted living purposes. In order to enable a low-latency,
high-accuracy food classification system, we opted to utilize the Mobilenet-v2
deep learning model. As some of the foods are more popular than the others, the
number of samples per class in the used Middle Eastern food dataset is
relatively imbalanced. To compensate for this problem, data augmentation
methods are applied on the underrepresented classes. Experimental results show
that using Mobilenet-v2 architecture for this task is beneficial in terms of
both accuracy and the memory usage. With the model achieving 94% accuracy on 23
food classes, the developed mobile application has potential to serve the
visually impaired in automatic food recognition via images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PP-Matting: High-Accuracy Natural Image Matting. (arXiv:2204.09433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09433">
<div class="article-summary-box-inner">
<span><p>Natural image matting is a fundamental and challenging computer vision task.
It has many applications in image editing and composition. Recently, deep
learning-based approaches have achieved great improvements in image matting.
However, most of them require a user-supplied trimap as an auxiliary input,
which limits the matting applications in the real world. Although some
trimap-free approaches have been proposed, the matting quality is still
unsatisfactory compared to trimap-based ones. Without the trimap guidance, the
matting models suffer from foreground-background ambiguity easily, and also
generate blurry details in the transition area. In this work, we propose
PP-Matting, a trimap-free architecture that can achieve high-accuracy natural
image matting. Our method applies a high-resolution detail branch (HRDB) that
extracts fine-grained details of the foreground with keeping feature resolution
unchanged. Also, we propose a semantic context branch (SCB) that adopts a
semantic segmentation subtask. It prevents the detail prediction from local
ambiguity caused by semantic context missing. In addition, we conduct extensive
experiments on two well-known benchmarks: Composition-1k and Distinctions-646.
The results demonstrate the superiority of PP-Matting over previous methods.
Furthermore, we provide a qualitative evaluation of our method on human matting
which shows its outstanding performance in the practical application. The code
and pre-trained models will be available at PaddleSeg:
https://github.com/PaddlePaddle/PaddleSeg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FenceNet: Fine-grained Footwork Recognition in Fencing. (arXiv:2204.09434v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09434">
<div class="article-summary-box-inner">
<span><p>Current data analysis for the Canadian Olympic fencing team is primarily done
manually by coaches and analysts. Due to the highly repetitive, yet dynamic and
subtle movements in fencing, manual data analysis can be inefficient and
inaccurate. We propose FenceNet as a novel architecture to automate the
classification of fine-grained footwork techniques in fencing. FenceNet takes
2D pose data as input and classifies actions using a skeleton-based action
recognition approach that incorporates temporal convolutional networks to
capture temporal information. We train and evaluate FenceNet on the Fencing
Footwork Dataset (FFD), which contains 10 fencers performing 6 different
footwork actions for 10-11 repetitions each (652 total videos). FenceNet
achieves 85.4% accuracy under 10-fold cross-validation, where each fencer is
left out as the test set. This accuracy is within 1% of the current
state-of-the-art method, JLJA (86.3%), which selects and fuses features
engineered from skeleton data, depth videos, and inertial measurement units.
BiFenceNet, a variant of FenceNet that captures the "bidirectionality" of human
movement through two separate networks, achieves 87.6% accuracy, outperforming
JLJA. Since neither FenceNet nor BiFenceNet requires data from wearable
sensors, unlike JLJA, they could be directly applied to most fencing videos,
using 2D pose data as input extracted from off-the-shelf 2D human pose
estimators. In comparison to JLJA, our methods are also simpler as they do not
require manual feature engineering, selection, or fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hephaestus: A large scale multitask dataset towards InSAR understanding. (arXiv:2204.09435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09435">
<div class="article-summary-box-inner">
<span><p>Synthetic Aperture Radar (SAR) data and Interferometric SAR (InSAR) products
in particular, are one of the largest sources of Earth Observation data. InSAR
provides unique information on diverse geophysical processes and geology, and
on the geotechnical properties of man-made structures. However, there are only
a limited number of applications that exploit the abundance of InSAR data and
deep learning methods to extract such knowledge. The main barrier has been the
lack of a large curated and annotated InSAR dataset, which would be costly to
create and would require an interdisciplinary team of experts experienced on
InSAR data interpretation. In this work, we put the effort to create and make
available the first of its kind, manually annotated dataset that consists of
19,919 individual Sentinel-1 interferograms acquired over 44 different
volcanoes globally, which are split into 216,106 InSAR patches. The annotated
dataset is designed to address different computer vision problems, including
volcano state classification, semantic segmentation of ground deformation,
detection and classification of atmospheric signals in InSAR imagery,
interferogram captioning, text to InSAR generation, and InSAR image quality
assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAM-GAN : Image Inpainting using Dynamic Attention Map based on Fake Texture Detection. (arXiv:2204.09442v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09442">
<div class="article-summary-box-inner">
<span><p>Deep neural advancements have recently brought remarkable image synthesis
performance to the field of image inpainting. The adaptation of generative
adversarial networks (GAN) in particular has accelerated significant progress
in high-quality image reconstruction. However, although many notable GAN-based
networks have been proposed for image inpainting, still pixel artifacts or
color inconsistency occur in synthesized images during the generation process,
which are usually called fake textures. To reduce pixel inconsistency disorder
resulted from fake textures, we introduce a GAN-based model using dynamic
attention map (DAM-GAN). Our proposed DAM-GAN concentrates on detecting fake
texture and products dynamic attention maps to diminish pixel inconsistency
from the feature maps in the generator. Evaluation results on CelebA-HQ and
Places2 datasets with other image inpainting approaches show the superiority of
our network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIMO: Gaze-Informed Human Motion Prediction in Context. (arXiv:2204.09443v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09443">
<div class="article-summary-box-inner">
<span><p>Predicting human motion is critical for assistive robots and AR/VR
applications, where the interaction with humans needs to be safe and
comfortable. Meanwhile, an accurate prediction depends on understanding both
the scene context and human intentions. Even though many works study
scene-aware human motion prediction, the latter is largely underexplored due to
the lack of ego-centric views that disclose human intent and the limited
diversity in motion and scenes. To reduce the gap, we propose a large-scale
human motion dataset that delivers high-quality body pose sequences, scene
scans, as well as ego-centric views with eye gaze that serves as a surrogate
for inferring human intent. By employing inertial sensors for motion capture,
our data collection is not tied to specific scenes, which further boosts the
motion dynamics observed from our subjects. We perform an extensive study of
the benefits of leveraging eye gaze for ego-centric human motion prediction
with various state-of-the-art architectures. Moreover, to realize the full
potential of gaze, we propose a novel network architecture that enables
bidirectional communication between the gaze and motion branches. Our network
achieves the top performance in human motion prediction on the proposed
dataset, thanks to the intent information from the gaze and the denoised gaze
feature modulated by the motion. The proposed dataset and our network
implementation will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STAU: A SpatioTemporal-Aware Unit for Video Prediction and Beyond. (arXiv:2204.09456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09456">
<div class="article-summary-box-inner">
<span><p>Video prediction aims to predict future frames by modeling the complex
spatiotemporal dynamics in videos. However, most of the existing methods only
model the temporal information and the spatial information for videos in an
independent manner but haven't fully explored the correlations between both
terms. In this paper, we propose a SpatioTemporal-Aware Unit (STAU) for video
prediction and beyond by exploring the significant spatiotemporal correlations
in videos. On the one hand, the motion-aware attention weights are learned from
the spatial states to help aggregate the temporal states in the temporal
domain. On the other hand, the appearance-aware attention weights are learned
from the temporal states to help aggregate the spatial states in the spatial
domain. In this way, the temporal information and the spatial information can
be greatly aware of each other in both domains, during which, the
spatiotemporal receptive field can also be greatly broadened for more reliable
spatiotemporal modeling. Experiments are not only conducted on traditional
video prediction tasks but also other tasks beyond video prediction, including
the early action recognition and object detection tasks. Experimental results
show that our STAU can outperform other methods on all tasks in terms of
performance and computation efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">THORN: Temporal Human-Object Relation Network for Action Recognition. (arXiv:2204.09468v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09468">
<div class="article-summary-box-inner">
<span><p>Most action recognition models treat human activities as unitary events.
However, human activities often follow a certain hierarchy. In fact, many human
activities are compositional. Also, these actions are mostly human-object
interactions. In this paper we propose to recognize human action by leveraging
the set of interactions that define an action. In this work, we present an
end-to-end network: THORN, that can leverage important human-object and
object-object interactions to predict actions. This model is built on top of a
3D backbone network. The key components of our model are: 1) An object
representation filter for modeling object. 2) An object relation reasoning
module to capture object relations. 3) A classification layer to predict the
action labels. To show the robustness of THORN, we evaluate it on
EPIC-Kitchen55 and EGTEA Gaze+, two of the largest and most challenging
first-person and human-object interaction datasets. THORN achieves
state-of-the-art performance on both datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GazeOnce: Real-Time Multi-Person Gaze Estimation. (arXiv:2204.09480v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09480">
<div class="article-summary-box-inner">
<span><p>Appearance-based gaze estimation aims to predict the 3D eye gaze direction
from a single image. While recent deep learning-based approaches have
demonstrated excellent performance, they usually assume one calibrated face in
each input image and cannot output multi-person gaze in real time. However,
simultaneous gaze estimation for multiple people in the wild is necessary for
real-world applications. In this paper, we propose the first one-stage
end-to-end gaze estimation method, GazeOnce, which is capable of simultaneously
predicting gaze directions for multiple faces (&gt;10) in an image. In addition,
we design a sophisticated data generation pipeline and propose a new dataset,
MPSGaze, which contains full images of multiple people with 3D gaze ground
truth. Experimental results demonstrate that our unified framework not only
offers a faster speed, but also provides a lower gaze estimation error compared
with state-of-the-art methods. This technique can be useful in real-time
applications with multiple users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Special Session: Towards an Agile Design Methodology for Efficient, Reliable, and Secure ML Systems. (arXiv:2204.09514v1 [cs.AR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09514">
<div class="article-summary-box-inner">
<span><p>The real-world use cases of Machine Learning (ML) have exploded over the past
few years. However, the current computing infrastructure is insufficient to
support all real-world applications and scenarios. Apart from high efficiency
requirements, modern ML systems are expected to be highly reliable against
hardware failures as well as secure against adversarial and IP stealing
attacks. Privacy concerns are also becoming a first-order issue. This article
summarizes the main challenges in agile development of efficient, reliable and
secure ML systems, and then presents an outline of an agile design methodology
to generate efficient, reliable and secure ML systems based on user-defined
constraints and objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">De-biasing facial detection system using VAE. (arXiv:2204.09556v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09556">
<div class="article-summary-box-inner">
<span><p>Bias in AI/ML-based systems is a ubiquitous problem and bias in AI/ML systems
may negatively impact society. There are many reasons behind a system being
biased. The bias can be due to the algorithm we are using for our problem or
may be due to the dataset we are using, having some features over-represented
in it. In the face detection system bias due to the dataset is majorly seen.
Sometimes models learn only features that are over-represented in data and
ignore rare features from data which results in being biased toward those
over-represented features. In real life, these biased systems are dangerous to
society. The proposed approach uses generative models which are best suited for
learning underlying features(latent variables) from the dataset and by using
these learned features models try to reduce the threats which are there due to
bias in the system. With the help of an algorithm, the bias present in the
dataset can be removed. And then we train models on two datasets and compare
the results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-view Brain Decoding. (arXiv:2204.09564v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09564">
<div class="article-summary-box-inner">
<span><p>How the brain captures the meaning of linguistic stimuli across multiple
views is still a critical open question in neuroscience. Consider three
different views of the concept apartment: (1) picture (WP) presented with the
target word label, (2) sentence (S) using the target word, and (3) word cloud
(WC) containing the target word along with other semantically related words.
Unlike previous efforts, which focus only on single view analysis, in this
paper, we study the effectiveness of brain decoding in a zero-shot cross-view
learning setup. Further, we propose brain decoding in the novel context of
cross-view-translation tasks like image captioning (IC), image tagging (IT),
keyword extraction (KE), and sentence formation (SF). Using extensive
experiments, we demonstrate that cross-view zero-shot brain decoding is
practical leading to ~0.68 average pairwise accuracy across view pairs. Also,
the decoded representations are sufficiently detailed to enable high accuracy
for cross-view-translation tasks with following pairwise accuracy: IC (78.0),
IT (83.0), KE (83.7) and SF (74.5). Analysis of the contribution of different
brain networks reveals exciting cognitive insights: (1) A high percentage of
visual voxels are involved in image captioning and image tagging tasks, and a
high percentage of language voxels are involved in the sentence formation and
keyword extraction tasks. (2) Zero-shot accuracy of the model trained on S view
and tested on WC view is better than same-view accuracy of the model trained
and tested on WC view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fetal Brain Tissue Annotation and Segmentation Challenge Results. (arXiv:2204.09573v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09573">
<div class="article-summary-box-inner">
<span><p>In-utero fetal MRI is emerging as an important tool in the diagnosis and
analysis of the developing human brain. Automatic segmentation of the
developing fetal brain is a vital step in the quantitative analysis of prenatal
neurodevelopment both in the research and clinical context. However, manual
segmentation of cerebral structures is time-consuming and prone to error and
inter-observer variability. Therefore, we organized the Fetal Tissue Annotation
(FeTA) Challenge in 2021 in order to encourage the development of automatic
segmentation algorithms on an international level. The challenge utilized FeTA
Dataset, an open dataset of fetal brain MRI reconstructions segmented into
seven different tissues (external cerebrospinal fluid, grey matter, white
matter, ventricles, cerebellum, brainstem, deep grey matter). 20 international
teams participated in this challenge, submitting a total of 21 algorithms for
evaluation. In this paper, we provide a detailed analysis of the results from
both a technical and clinical perspective. All participants relied on deep
learning methods, mainly U-Nets, with some variability present in the network
architecture, optimization, and image pre- and post-processing. The majority of
teams used existing medical imaging deep learning frameworks. The main
differences between the submissions were the fine tuning done during training,
and the specific pre- and post-processing steps performed. The challenge
results showed that almost all submissions performed similarly. Four of the top
five teams used ensemble learning methods. However, one team's algorithm
performed significantly superior to the other submissions, and consisted of an
asymmetrical U-Net network architecture. This paper provides a first of its
kind benchmark for future automatic multi-tissue segmentation algorithms for
the developing human brain in utero.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Robust Femur Segmentation from Computed Tomography Images for Patient-Specific Hip Fracture Risk Screening. (arXiv:2204.09575v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09575">
<div class="article-summary-box-inner">
<span><p>Osteoporosis is a common bone disease that increases the risk of bone
fracture. Hip-fracture risk screening methods based on finite element analysis
depend on segmented computed tomography (CT) images; however, current femur
segmentation methods require manual delineations of large data sets. Here we
propose a deep neural network for fully automated, accurate, and fast
segmentation of the proximal femur from CT. Evaluation on a set of 1147
proximal femurs with ground truth segmentations demonstrates that our method is
apt for hip-fracture risk screening, bringing us one step closer to a
clinically viable option for screening at-risk patients for hip-fracture
susceptibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical BERT for Medical Document Understanding. (arXiv:2204.09600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09600">
<div class="article-summary-box-inner">
<span><p>Medical document understanding has gained much attention recently. One
representative task is the International Classification of Disease (ICD)
diagnosis code assignment. Existing work adopts either RNN or CNN as the
backbone network because the vanilla BERT cannot handle well long documents
(&gt;2000 to kens). One issue shared across all these approaches is that they are
over specific to the ICD code assignment task, losing generality to give the
whole document-level and sentence-level embedding. As a result, it is not
straight-forward to direct them to other downstream NLU tasks. Motivated by
these observations, we propose Medical Document BERT (MDBERT) for long medical
document understanding tasks. MDBERT is not only effective in learning
representations at different levels of semantics but efficient in encoding long
documents by leveraging a bottom-up hierarchical architecture. Compared to
vanilla BERT solutions: 1, MDBERT boosts the performance up to relatively 20%
on the MIMIC-III dataset, making it comparable to current SOTA solutions; 2, it
cuts the computational complexity on self-attention modules to less than 1/100.
Other than the ICD code assignment, we conduct a variety of other NLU tasks on
a large commercial dataset named as TrialTrove, to showcase MDBERT's strength
in delivering different levels of semantics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assembly Planning from Observations under Physical Constraints. (arXiv:2204.09616v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09616">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of copying an unknown assembly of primitives
with known shape and appearance using information extracted from a single
photograph by an off-the-shelf procedure for object detection and pose
estimation. The proposed algorithm uses a simple combination of physical
stability constraints, convex optimization and Monte Carlo tree search to plan
assemblies as sequences of pick-and-place operations represented by STRIPS
operators. It is efficient and, most importantly, robust to the errors in
object detection and pose estimation unavoidable in any real robotic system.
The proposed approach is demonstrated with thorough experiments on a UR5
manipulator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Mixture of Experts. (arXiv:2204.09636v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09636">
<div class="article-summary-box-inner">
<span><p>Mixture of Experts (MoE) is able to scale up vision transformers effectively.
However, it requires prohibiting computation resources to train a large MoE
transformer. In this paper, we propose Residual Mixture of Experts (RMoE), an
efficient training pipeline for MoE vision transformers on downstream tasks,
such as segmentation and detection. RMoE achieves comparable results with the
upper-bound MoE training, while only introducing minor additional training cost
than the lower-bound non-MoE training pipelines. The efficiency is supported by
our key observation: the weights of an MoE transformer can be factored into an
input-independent core and an input-dependent residual. Compared with the
weight core, the weight residual can be efficiently trained with much less
computation resource, e.g., finetuning on the downstream data. We show that,
compared with the current MoE training pipeline, we get comparable results
while saving over 30% training cost. When compared with state-of-the-art non-
MoE transformers, such as Swin-T / CvT-13 / Swin-L, we get +1.1 / 0.9 / 1.0
mIoU gain on ADE20K segmentation and +1.4 / 1.6 / 0.6 AP gain on MS-COCO object
detection task with less than 3% additional training cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-Class Model for Fabric Defect Detection. (arXiv:2204.09648v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09648">
<div class="article-summary-box-inner">
<span><p>An automated and accurate fabric defect inspection system is in high demand
as a replacement for slow, inconsistent, error-prone, and expensive human
operators in the textile industry. Previous efforts focused on certain types of
fabrics or defects, which is not an ideal solution. In this paper, we propose a
novel one-class model that is capable of detecting various defects on different
fabric types. Our model takes advantage of a well-designed Gabor filter bank to
analyze fabric texture. We then leverage an advanced deep learning algorithm,
autoencoder, to learn general feature representations from the outputs of the
Gabor filter bank. Lastly, we develop a nearest neighbor density estimator to
locate potential defects and draw them on the fabric images. We demonstrate the
effectiveness and robustness of the proposed model by testing it on various
types of fabrics such as plain, patterned, and rotated fabrics. Our model also
achieves a true positive rate (a.k.a recall) value of 0.895 with no false
alarms on our dataset based upon the Standard Fabric Defect Glossary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments. (arXiv:2204.09667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09667">
<div class="article-summary-box-inner">
<span><p>Recent work in Vision-and-Language Navigation (VLN) has presented two
environmental paradigms with differing realism -- the standard VLN setting
built on topological environments where navigation is abstracted away, and the
VLN-CE setting where agents must navigate continuous 3D environments using
low-level actions. Despite sharing the high-level task and even the underlying
instruction-path data, performance on VLN-CE lags behind VLN significantly. In
this work, we explore this gap by transferring an agent from the abstract
environment of VLN to the continuous environment of VLN-CE. We find that this
sim-2-sim transfer is highly effective, improving over the prior state of the
art in VLN-CE by +12% success rate. While this demonstrates the potential for
this direction, the transfer does not fully retain the original performance of
the agent in the abstract setting. We present a sequence of experiments to
identify what differences result in performance degradation, providing clear
directions for further improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Segmentation Networks should be Latency Aware. (arXiv:2004.02574v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.02574">
<div class="article-summary-box-inner">
<span><p>As scene segmentation systems reach visually accurate results, many recent
papers focus on making these network architectures faster, smaller and more
efficient. In particular, studies often aim at designingreal-time'systems.
Achieving this goal is particularly relevant in the context of real-time video
understanding for autonomous vehicles, and robots. In this paper, we argue that
the commonly used performance metric of mean Intersection over Union (mIoU)
does not fully capture the information required to estimate the true
performance of these networks when they operate inreal-time'. We propose a
change of objective in the segmentation task, and its associated metric that
encapsulates this missing information in the following way: We propose to
predict the future output segmentation map that will match the future input
frame at the time when the network finishes the processing. We introduce the
associated latency-aware metric, from which we can determine a ranking. We
perform latency timing experiments of some recent networks on different
hardware and assess the performances of these networks on our proposed task. We
propose improvements to scene segmentation networks to better perform on our
task by using multi-frames input and increasing capacity in the initial
convolutional layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Human Sex Be Learned Using Only 2D Body Keypoint Estimations?. (arXiv:2011.03104v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.03104">
<div class="article-summary-box-inner">
<span><p>In this paper, we analyze human male and female sex recognition problem and
present a fully automated classification system using only 2D keypoints. The
keypoints represent human joints. A keypoint set consists of 15 joints and the
keypoint estimations are obtained using an OpenPose 2D keypoint detector. We
learn a deep learning model to distinguish males and females using the
keypoints as input and binary labels as output. We use two public datasets in
the experimental section - 3DPeople and PETA. On PETA dataset, we report a 77%
accuracy. We provide model performance details on both PETA and 3DPeople. To
measure the effect of noisy 2D keypoint detections on the performance, we run
separate experiments on 3DPeople ground truth and noisy keypoint data. Finally,
we extract a set of factors that affect the classification accuracy and propose
future work. The advantage of the approach is that the input is small and the
architecture is simple, which enables us to run many experiments and keep the
real-time performance in inference. The source code, with the experiments and
data preparation scripts, are available on GitHub
(https://github.com/kristijanbartol/human-sex-classifier).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Subsampling of Realistic Images From GANs Conditional on a Class or a Continuous Variable. (arXiv:2103.11166v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11166">
<div class="article-summary-box-inner">
<span><p>Recently, subsampling or refining images generated from unconditional GANs
has been actively studied to improve the overall image quality. Unfortunately,
these methods are often observed less effective or inefficient in handling
conditional GANs (cGANs) -- conditioning on a class (aka class-conditional
GANs) or a continuous variable (aka continuous cGANs or CcGANs). In this work,
we introduce an effective and efficient subsampling scheme, named conditional
density ratio-guided rejection sampling (cDR-RS), to sample high-quality images
from cGANs. Specifically, we first develop a novel conditional density ratio
estimation method, termed cDRE-F-cSP, by proposing the conditional Softplus
(cSP) loss and an improved feature extraction mechanism. We then derive the
error bound of a density ratio model trained with the cSP loss. Finally, we
accept or reject a fake image in terms of its estimated conditional density
ratio. A filtering scheme is also developed to increase fake images' label
consistency without losing diversity when sampling from CcGANs. We extensively
test the effectiveness and efficiency of cDR-RS in sampling from both
class-conditional GANs and CcGANs on five benchmark datasets. When sampling
from class-conditional GANs, cDR-RS outperforms modern state-of-the-art methods
by a large margin (except DRE-F-SP+RS) in terms of effectiveness. Although the
effectiveness of cDR-RS is often comparable to that of DRE-F-SP+RS, cDR-RS is
substantially more efficient. When sampling from CcGANs, the superiority of
cDR-RS is even more noticeable in terms of both effectiveness and efficiency.
Notably, with the consumption of reasonable computational resources, cDR-RS can
substantially reduce Label Score without decreasing the diversity of
CcGAN-generated images, while other methods often need to trade much diversity
for slightly improved Label Score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards General Purpose Vision Systems. (arXiv:2104.00743v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00743">
<div class="article-summary-box-inner">
<span><p>Computer vision systems today are primarily N-purpose systems, designed and
trained for a predefined set of tasks. Adapting such systems to new tasks is
challenging and often requires non-trivial modifications to the network
architecture (e.g. adding new output heads) or training process (e.g. adding
new losses). To reduce the time and expertise required to develop new
applications, we would like to create general purpose vision systems that can
learn and perform a range of tasks without any modification to the architecture
or learning process.
</p>
<p>In this paper, we propose GPV-1, a task-agnostic vision-language architecture
that can learn and perform tasks that involve receiving an image and producing
text and/or bounding boxes, including classification, localization, visual
question answering, captioning, and more. We also propose evaluations of
generality of architecture, skill-concept transfer, and learning efficiency
that may inform future work on general purpose vision. Our experiments indicate
GPV-1 is effective at multiple tasks, reuses some concept knowledge across
tasks, can perform the Referring Expressions task zero-shot, and further
improves upon the zero-shot performance using a few training samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel three-stage training strategy for long-tailed classification. (arXiv:2104.09830v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09830">
<div class="article-summary-box-inner">
<span><p>The long-tailed distribution datasets poses great challenges for deep
learning based classification models on how to handle the class imbalance
problem. Existing solutions usually involve class-balacing strategies or
transfer learing from head- to tail-classes or use two-stages learning strategy
to re-train the classifier. However, the existing methods are difficult to
solve the low quality problem when images are obtained by SAR. To address this
problem, we establish a novel three-stages training strategy, which has
excellent results for processing SAR image datasets with long-tailed
distribution. Specifically, we divide training procedure into three stages. The
first stage is to use all kinds of images for rough-training, so as to get the
rough-training model with rich content. The second stage is to make the rough
model learn the feature expression by using the residual dataset with the class
0 removed. The third stage is to fine tune the model using class-balanced
datasets with all 10 classes (including the overall model fine tuning and
classifier re-optimization). Through this new training strategy, we only use
the information of SAR image dataset and the network model with very small
parameters to achieve the top 1 accuracy of 22.34 in development phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment. (arXiv:2105.06224v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06224">
<div class="article-summary-box-inner">
<span><p>Table structure recognition is a challenging task due to the various
structures and complicated cell spanning relations. Previous methods handled
the problem starting from elements in different granularities (rows/columns,
text regions), which somehow fell into the issues like lossy heuristic rules or
neglect of empty cell division. Based on table structure characteristics, we
find that obtaining the aligned bounding boxes of text region can effectively
maintain the entire relevant range of different cells. However, the aligned
bounding boxes are hard to be accurately predicted due to the visual
ambiguities. In this paper, we aim to obtain more reliable aligned bounding
boxes by fully utilizing the visual information from both text regions in
proposed local features and cell relations in global features. Specifically, we
propose the framework of Local and Global Pyramid Mask Alignment, which adopts
the soft pyramid mask learning mechanism in both the local and global feature
maps. It allows the predicted boundaries of bounding boxes to break through the
limitation of original proposals. A pyramid mask re-scoring module is then
integrated to compromise the local and global information and refine the
predicted boundaries. Finally, we propose a robust table structure recovery
pipeline to obtain the final structure, in which we also effectively solve the
problems of empty cells locating and division. Experimental results show that
the proposed method achieves competitive and even new state-of-the-art
performance on several public benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robotic Inspection of Underground Utilities for Construction Survey Using a Ground Penetrating Radar. (arXiv:2106.01907v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01907">
<div class="article-summary-box-inner">
<span><p>Ground Penetrating Radar (GPR) is a very useful non-destructive evaluation
(NDE) device for locating and mapping underground assets prior to digging and
trenching efforts in construction. This paper presents a novel robotic system
to automate the GPR data collection process, localize the underground
utilities, interpret and reconstruct the underground objects for better
visualization allowing regular non-professional users to understand the survey
results. This system is composed of three modules: 1) an Omni-directional
robotic data collection platform, that carries an RGB-D camera with an Inertial
Measurement Unit (IMU) and a GPR antenna to perform automatic GPR data
collection, and tag each GPR measurement with visual positioning information at
every sampling step; 2) a learning-based migration module to interpret the raw
GPR B-scan image into a 2D cross-section model of objects; 3) a 3D
reconstruction module, i.e., GPRNet, to generate underground utility model
represented as fine 3D point cloud. Comparative studies are performed on
synthetic data and field GPR raw data with various incompleteness and noise.
Experimental results demonstrate that our proposed method achieves a $30.0\%$
higher GPR imaging accuracy in mean Intersection Over Union (IoU) than the
conventional back projection (BP) migration approach and $6.9\%$-$7.2\%$ less
loss in Chamfer Distance (CD) than baseline methods regarding point cloud model
reconstruction. The GPR-based robotic inspection provides an effective tool for
civil engineers to detect and survey underground utilities before construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15754">
<div class="article-summary-box-inner">
<span><p>Long-range contextual information is crucial for the semantic segmentation of
High-Resolution (HR) Remote Sensing Images (RSIs). However, image cropping
operations, commonly used for training neural networks, limit the perception of
long-range contexts in large RSIs. To overcome this limitation, we propose a
Wide-Context Network (WiCoNet) for the semantic segmentation of HR RSIs. Apart
from extracting local features with a conventional CNN, the WiCoNet has an
extra context branch to aggregate information from a larger image area.
Moreover, we introduce a Context Transformer to embed contextual information
from the context branch and selectively project it onto the local features. The
Context Transformer extends the Vision Transformer, an emerging kind of neural
network, to model the dual-branch semantic correlations. It overcomes the
locality limitation of CNNs and enables the WiCoNet to see the bigger picture
before segmenting the land-cover/land-use (LCLU) classes. Ablation studies and
comparative experiments conducted on several benchmark datasets demonstrate the
effectiveness of the proposed method. In addition, we present a new Beijing
Land-Use (BLU) dataset. This is a large-scale HR satellite dataset with
high-quality and fine-grained reference labels, which can facilitate future
studies in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Parametric Wireframe Extraction Based on Distance Fields. (arXiv:2107.06165v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06165">
<div class="article-summary-box-inner">
<span><p>We present a pipeline for parametric wireframe extraction from densely
sampled point clouds. Our approach processes a scalar distance field that
represents proximity to the nearest sharp feature curve. In intermediate
stages, it detects corners, constructs curve segmentation, and builds a
topological graph fitted to the wireframe. As an output, we produce parametric
spline curves that can be edited and sampled arbitrarily. We evaluate our
method on 50 complex 3D shapes and compare it to the novel deep learning-based
technique, demonstrating superior quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dynamic 3D Spontaneous Micro-expression Database: Establishment and Evaluation. (arXiv:2108.00166v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00166">
<div class="article-summary-box-inner">
<span><p>Micro-expressions are spontaneous, unconscious facial movements that show
people's true inner emotions and have great potential in related fields of
psychological testing. Since the face is a 3D deformation object, the
occurrence of an expression can arouse spatial deformation of the face, but
limited by the available databases are 2D videos, lacking the description of 3D
spatial information of micro-expressions. Therefore, we proposed a new
micro-expression database containing 2D video sequences and 3D point clouds
sequences. The database includes 373 micro-expressions sequences, and these
samples were classified using the objective method based on facial action
coding system, as well as the non-objective method that combines video contents
and participants' self-reports. We extracted 2D and 3D features using the local
binary patterns on three orthogonal planes (LBP-TOP) and curvature algorithms,
respectively, and evaluated the classification accuracies of these two features
and their fusion results with leave-one-subject-out (LOSO) and 10-fold
cross-validation. Further, we performed various neural network algorithms for
database classification, the results show that classification accuracies are
improved by fusing 3D features than using only 2D features. The database offers
original and cropped micro-expression samples, which will facilitate the
exploration and research on 3D Spatio-temporal features of micro-expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I3CL:Intra- and Inter-Instance Collaborative Learning for Arbitrary-shaped Scene Text Detection. (arXiv:2108.01343v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01343">
<div class="article-summary-box-inner">
<span><p>Existing methods for arbitrary-shaped text detection in natural scenes face
two critical issues, i.e., 1) fracture detections at the gaps in a text
instance; and 2) inaccurate detections of arbitrary-shaped text instances with
diverse background context. To address these issues, we propose a novel method
named Intra- and Inter-Instance Collaborative Learning (I3CL). Specifically, to
address the first issue, we design an effective convolutional module with
multiple receptive fields, which is able to collaboratively learn better
character and gap feature representations at local and long ranges inside a
text instance. To address the second issue, we devise an instance-based
transformer module to exploit the dependencies between different text instances
and a global context module to exploit the semantic context from the shared
background, which are able to collaboratively learn more discriminative text
feature representation. In this way, I3CL can effectively exploit the intra-
and inter-instance dependencies together in a unified end-to-end trainable
framework. Besides, to make full use of the unlabeled data, we design an
effective semi-supervised learning method to leverage the pseudo labels via an
ensemble strategy. Without bells and whistles, experimental results show that
the proposed I3CL sets new state-of-the-art results on three challenging public
benchmarks, i.e., an F-measure of 77.5% on ICDAR2019-ArT, 86.9% on Total-Text,
and 86.4% on CTW-1500. Notably, our I3CL with the ResNeSt-101 backbone ranked
1st place on the ICDAR2019-ArT leaderboard. The source code will be available
at https://github.com/ViTAE-Transformer/ViTAE-Transformer-Scene-Text-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localized Shape Modelling with Global Coherence: An Inverse Spectral Approach. (arXiv:2108.02161v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02161">
<div class="article-summary-box-inner">
<span><p>Many natural shapes have most of their characterizing features concentrated
over a few regions in space. For example, humans and animals have distinctive
head shapes, while inorganic objects like chairs and airplanes are made of
well-localized functional parts with specific geometric features. Often, these
features are strongly correlated -- a modification of facial traits in a
quadruped should induce changes to the body structure. However, in shape
modelling applications, these types of edits are among the hardest ones; they
require high precision, but also a global awareness of the entire shape. Even
in the deep learning era, obtaining manipulable representations that satisfy
such requirements is an open problem posing significant constraints. In this
work, we address this problem by defining a data-driven model upon a family of
linear operators (variants of the mesh Laplacian), whose spectra capture global
and local geometric properties of the shape at hand. Modifications to these
spectra are translated to semantically valid deformations of the corresponding
surface. By explicitly decoupling the global from the local surface features,
our pipeline allows to perform local edits while simultaneously maintaining a
global stylistic coherence. We empirically demonstrate how our learning-based
model generalizes to shape representations not seen at training time, and we
systematically analyze different choices of local operators over diverse shape
categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tikhonov Regularization of Circle-Valued Signals. (arXiv:2108.02602v2 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02602">
<div class="article-summary-box-inner">
<span><p>It is common to have to process signals or images whose values are cyclic and
can be represented as points on the complex circle, like wrapped phases,
angles, orientations, or color hues. We consider a Tikhonov-type regularization
model to smoothen or interpolate circle-valued signals defined on arbitrary
graphs. We propose a convex relaxation of this nonconvex problem as a
semidefinite program, and an efficient algorithm to solve it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DARTS for Inverse Problems: a Study on Hyperparameter Sensitivity. (arXiv:2108.05647v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05647">
<div class="article-summary-box-inner">
<span><p>Differentiable architecture search (DARTS) is a widely researched tool for
the discovery of novel architectures, due to its promising results for image
classification. The main benefit of DARTS is the effectiveness achieved through
the weight-sharing one-shot paradigm, which allows efficient architecture
search. In this work, we investigate DARTS in a systematic case study of
inverse problems, which allows us to analyze these potential benefits in a
controlled manner. We demonstrate that the success of DARTS can be extended
from image classification to signal reconstruction, in principle. However, our
experiments also expose three fundamental difficulties in the evaluation of
DARTS-based methods in inverse problems: First, the results show a large
variance in all test cases. Second, the final performance is highly dependent
on the hyperparameters of the optimizer. And third, the performance of the
weight-sharing architecture used during training does not reflect the final
performance of the found architecture well. Thus, we conclude the necessity to
1) report the results of any DARTS-based methods from several runs along with
its underlying performance statistics, 2) show the correlation of the training
and final architecture performance, and 3) carefully consider if the
computational efficiency of DARTS outweighs the costs of hyperparameter
optimization and multiple runs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining chest X-rays and electronic health record (EHR) data using machine learning to diagnose acute respiratory failure. (arXiv:2108.12530v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12530">
<div class="article-summary-box-inner">
<span><p>Objective: When patients develop acute respiratory failure, accurately
identifying the underlying etiology is essential for determining the best
treatment. However, differentiating between common medical diagnoses can be
challenging in clinical practice. Machine learning models could improve medical
diagnosis by aiding in the diagnostic evaluation of these patients. Materials
and Methods: Machine learning models were trained to predict the common causes
of acute respiratory failure (pneumonia, heart failure, and/or COPD). Models
were trained using chest radiographs and clinical data from the electronic
health record (EHR) and applied to an internal and external cohort. Results:
The internal cohort of 1,618 patients included 508 (31%) with pneumonia, 363
(22%) with heart failure, and 137 (8%) with COPD based on physician chart
review. A model combining chest radiographs and EHR data outperformed models
based on each modality alone. Models had similar or better performance compared
to a randomly selected physician reviewer. For pneumonia, the combined model
area under the receiver operating characteristic curve (AUROC) was 0.79
(0.77-0.79), image model AUROC was 0.74 (0.72-0.75), and EHR model AUROC was
0.74 (0.70-0.76). For heart failure, combined: 0.83 (0.77-0.84), image: 0.80
(0.71-0.81), and EHR: 0.79 (0.75-0.82). For COPD, combined: AUROC = 0.88
(0.83-0.91), image: 0.83 (0.77-0.89), and EHR: 0.80 (0.76-0.84). In the
external cohort, performance was consistent for heart failure and increased for
COPD, but declined slightly for pneumonia. Conclusions: Machine learning models
combining chest radiographs and EHR data can accurately differentiate between
common causes of acute respiratory failure. Further work is needed to determine
how these models could act as a diagnostic aid to clinicians in clinical
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2109.09960v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09960">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel mutual consistency network (MC-Net+) to
effectively exploit the unlabeled data for semi-supervised medical image
segmentation. The MC-Net+ model is motivated by the observation that deep
models trained with limited annotations are prone to output highly uncertain
and easily mis-classified predictions in ambiguous regions (e.g., adhesive
edges or thin branches) for medical image segmentation. Leveraging these
region-level challenging samples can make the semi-supervised segmentation
model training more effective. Therefore, our proposed MC-Net+ model consists
of two new designs. First, the model contains one shared encoder and multiple
slightly different decoders (i.e., using different up-sampling strategies). The
statistical discrepancy of multiple decoders' outputs is computed to denote the
model's uncertainty, which indicates the unlabeled hard regions. Second, we
apply a novel mutual consistency constraint between one decoder's probability
output and other decoders' soft pseudo labels. In this way, we minimize the
discrepancy of multiple outputs (i.e., the model uncertainty) during training
and force the model to generate invariant results in such challenging regions,
aiming at capturing more useful features. We compared the segmentation results
of our MC-Net+ with five state-of-the-art semi-supervised approaches on three
public medical datasets. Extension experiments with two common semi-supervised
settings demonstrate the superior performance of our model over other existing
methods, which sets a new state of the art for semi-supervised medical image
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Human Pose Triangulation. (arXiv:2110.00280v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00280">
<div class="article-summary-box-inner">
<span><p>We address the problem of generalizability for multi-view 3D human pose
estimation. The standard approach is to first detect 2D keypoints in images and
then apply triangulation from multiple views. Even though the existing methods
achieve remarkably accurate 3D pose estimation on public benchmarks, most of
them are limited to a single spatial camera arrangement and their number.
Several methods address this limitation but demonstrate significantly degraded
performance on novel views. We propose a stochastic framework for human pose
triangulation and demonstrate a superior generalization across different camera
arrangements on two public datasets. In addition, we apply the same approach to
the fundamental matrix estimation problem, showing that the proposed method can
successfully apply to other computer vision problems. The stochastic framework
achieves more than 8.8% improvement on the 3D pose estimation task, compared to
the state-of-the-art, and more than 30% improvement for fundamental matrix
estimation, compared to a standard algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus on the Common Good: Group Distributional Robustness Follows. (arXiv:2110.02619v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02619">
<div class="article-summary-box-inner">
<span><p>We consider the problem of training a classification model with group
annotated training data. Recent work has established that, if there is
distribution shift across different groups, models trained using the standard
empirical risk minimization (ERM) objective suffer from poor performance on
minority groups and that group distributionally robust optimization (Group-DRO)
objective is a better alternative. The starting point of this paper is the
observation that though Group-DRO performs better than ERM on minority groups
for some benchmark datasets, there are several other datasets where it performs
much worse than ERM. Inspired by ideas from the closely related problem of
domain generalization, this paper proposes a new and simple algorithm that
explicitly encourages learning of features that are shared across various
groups. The key insight behind our proposed algorithm is that while Group-DRO
focuses on groups with worst regularized loss, focusing instead, on groups that
enable better performance even on other groups, could lead to learning of
shared/common features, thereby enhancing minority performance beyond what is
achieved by Group-DRO. Empirically, we show that our proposed algorithm matches
or achieves better performance compared to strong contemporary baselines
including ERM and Group-DRO on standard benchmarks on both minority groups and
across all groups. Theoretically, we show that the proposed algorithm is a
descent method and finds first order stationary points of smooth nonconvex
functions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FocusNet: Classifying Better by Focusing on Confusing Classes. (arXiv:2110.07307v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07307">
<div class="article-summary-box-inner">
<span><p>Nowadays, most classification networks use one-hot encoding to represent
categorical data because of its simplicity. However, one-hot encoding may
affect the generalization ability as it neglects inter-class correlations. We
observe that, even when a neural network trained with one-hot labels produces
incorrect predictions, it still pays attention to the target image region and
reveals which classes confuse the network. Inspired by this observation, we
propose a confusion-focusing mechanism to address the class-confusion issue.
Our confusion-focusing mechanism is implemented by a two-branch network
architecture. Its baseline branch generates confusing classes, and its FocusNet
branch, whose architecture is flexible, discriminates correct labels from these
confusing classes. We also introduce a novel focus-picking loss function to
improve classification accuracy by encouraging FocusNet to focus on the most
confusing classes. The experimental results validate that our FocusNet is
effective for image classification on common datasets, and that our
focus-picking loss function can also benefit the current neural networks in
improving their classification accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-OOCS: Learning Prostate Segmentation with Inductive Bias. (arXiv:2110.15664v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15664">
<div class="article-summary-box-inner">
<span><p>Despite the great success of convolutional neural networks (CNN) in 3D
medical image segmentation tasks, the methods currently in use are still not
robust enough to the different protocols utilized by different scanners, and to
the variety of image properties or artefacts they produce. To this end, we
introduce OOCS-enhanced networks, a novel architecture inspired by the innate
nature of visual processing in the vertebrates. With different 3D U-Net
variants as the base, we add two 3D residual components to the second encoder
blocks: on and off center-surround (OOCS). They generalise the ganglion
pathways in the retina to a 3D setting. The use of 2D-OOCS in any standard CNN
network complements the feedforward framework with sharp edge-detection
inductive biases. The use of 3D-OOCS also helps 3D U-Nets to scrutinise and
delineate anatomical structures present in 3D images with increased accuracy.We
compared the state-of-the-art 3D U-Nets with their 3D-OOCS extensions and
showed the superior accuracy and robustness of the latter in automatic prostate
segmentation from 3D Magnetic Resonance Images (MRIs). For a fair comparison,
we trained and tested all the investigated 3D U-Nets with the same pipeline,
including automatic hyperparameter optimisation and data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auxiliary Loss Reweighting for Image Inpainting. (arXiv:2111.07279v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07279">
<div class="article-summary-box-inner">
<span><p>Image Inpainting is a task that aims to fill in missing regions of corrupted
images with plausible contents. Recent inpainting methods have introduced
perceptual and style losses as auxiliary losses to guide the learning of
inpainting generators. Perceptual and style losses help improve the perceptual
quality of inpainted results by supervising deep features of generated regions.
However, two challenges have emerged with the usage of perceptual and style
losses: (i) the time-consuming grid search is required to decide weights for
perceptual and style losses to properly perform, and (ii) loss terms with
different auxiliary abilities are equally weighted by perceptual and style
losses. To meet these two challenges, we propose a novel framework that
independently weights auxiliary loss terms and adaptively adjusts their weights
within a single training process, without a time-consuming grid search.
Specifically, to release the auxiliary potential of perceptual and style
losses, we propose two auxiliary losses, Tunable Perceptual Loss (TPL) and
Tunable Style Loss (TSL) by using different tunable weights to consider the
contributions of different loss terms. TPL and TSL are supersets of perceptual
and style losses and release the auxiliary potential of standard perceptual and
style losses. We further propose the Adaptive Auxiliary Loss (AAL) algorithm,
which efficiently reweights TPL and TSL in a single training process. AAL is
based on the principle that the best auxiliary weights would lead to the most
improvement in inpainting performance. We conduct experiments on publically
available datasets and find that our framework helps current SOTA methods
achieve better results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Network Kalman filtering for 3D object tracking from linear array ultrasound data. (arXiv:2111.09631v2 [stat.AP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09631">
<div class="article-summary-box-inner">
<span><p>Many interventional surgical procedures rely on medical imaging to visualise
and track instruments. Such imaging methods not only need to be real-time
capable, but also provide accurate and robust positional information. In
ultrasound applications, typically only two-dimensional data from a linear
array are available, and as such obtaining accurate positional estimation in
three dimensions is non-trivial. In this work, we first train a neural network,
using realistic synthetic training data, to estimate the out-of-plane offset of
an object with the associated axial aberration in the reconstructed ultrasound
image. The obtained estimate is then combined with a Kalman filtering approach
that utilises positioning estimates obtained in previous time-frames to improve
localisation robustness and reduce the impact of measurement noise. The
accuracy of the proposed method is evaluated using simulations, and its
practical applicability is demonstrated on experimental data obtained using a
novel optical ultrasound imaging setup. Accurate and robust positional
information is provided in real-time. Axial and lateral coordinates for
out-of-plane objects are estimated with a mean error of 0.1mm for simulated
data and a mean error of 0.2mm for experimental data. Three-dimensional
localisation is most accurate for elevational distances larger than 1mm, with a
maximum distance of 6mm considered for a 25mm aperture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RADU: Ray-Aligned Depth Update Convolutions for ToF Data Denoising. (arXiv:2111.15513v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15513">
<div class="article-summary-box-inner">
<span><p>Time-of-Flight (ToF) cameras are subject to high levels of noise and
distortions due to Multi-Path-Interference (MPI). While recent research showed
that 2D neural networks are able to outperform previous traditional
State-of-the-Art (SOTA) methods on denoising ToF-Data, little research on
learning-based approaches has been done to make direct use of the 3D
information present in depth images. In this paper, we propose an iterative
denoising approach operating in 3D space, that is designed to learn on 2.5D
data by enabling 3D point convolutions to correct the points' positions along
the view direction. As labeled real world data is scarce for this task, we
further train our network with a self-training approach on unlabeled real world
data to account for real world statistics. We demonstrate that our method is
able to outperform SOTA methods on several datasets, including two real world
datasets and a new large-scale synthetic data set introduced in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separated Contrastive Learning for Organ-at-Risk and Gross-Tumor-Volume Segmentation with Limited Annotation. (arXiv:2112.02743v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02743">
<div class="article-summary-box-inner">
<span><p>Automatic delineation of organ-at-risk (OAR) and gross-tumor-volume (GTV) is
of great significance for radiotherapy planning. However, it is a challenging
task to learn powerful representations for accurate delineation under limited
pixel (voxel)-wise annotations. Contrastive learning at pixel-level can
alleviate the dependency on annotations by learning dense representations from
unlabeled data. Recent studies in this direction design various contrastive
losses on the feature maps, to yield discriminative features for each pixel in
the map. However, pixels in the same map inevitably share semantics to be
closer than they actually are, which may affect the discrimination of pixels in
the same map and lead to the unfair comparison to pixels in other maps. To
address these issues, we propose a separated region-level contrastive learning
scheme, namely SepaReg, the core of which is to separate each image into
regions and encode each region separately. Specifically, SepaReg comprises two
components: a structure-aware image separation (SIS) module and an intra- and
inter-organ distillation (IID) module. The SIS is proposed to operate on the
image set to rebuild a region set under the guidance of structural information.
The inter-organ representation will be learned from this set via typical
contrastive losses cross regions. On the other hand, the IID is proposed to
tackle the quantity imbalance in the region set as tiny organs may produce
fewer regions, by exploiting intra-organ representations. We conducted
extensive experiments to evaluate the proposed model on a public dataset and
two private datasets. The experimental results demonstrate the effectiveness of
the proposed model, consistently achieving better performance than
state-of-the-art approaches. Code is available at
https://github.com/jcwang123/Separate_CL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning -- Demonstrated for Offshore Wind Farm Detection. (arXiv:2112.02829v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02829">
<div class="article-summary-box-inner">
<span><p>With the emergence of deep learning in the last years, new opportunities
arose in Earth observation research. Nevertheless, they also brought with them
new challenges. The data-hungry training processes of deep learning models
demand large, resource expensive, annotated data sets and partly replaced
knowledge-driven approaches so that model behaviour and the final prediction
process became a black box. The proposed SyntEO approach enables Earth
observation researchers to automatically generate large deep learning ready
data sets by merging existing and procedural data. SyntEO does this by
including expert knowledge in the data generation process in a highly
structured manner to control the automatic image and label generation by
employing an ontology. In this way, fully controllable experiment environments
are set up, which support insights in the model training on the synthetic data
sets. Thus, SyntEO makes the learning process approachable, which is an
important cornerstone for explainable machine learning. We demonstrate the
SyntEO approach by predicting offshore wind farms in Sentinel-1 images on two
of the worlds largest offshore wind energy production sites. The largest
generated data set has 90,000 training examples. A basic convolutional neural
network for object detection, that is only trained on this synthetic data,
confidently detects offshore wind farms by minimising false detections in
challenging environments. In addition, four sequential data sets are generated,
demonstrating how the SyntEO approach can precisely define the data set
structure and influence the training process. SyntEO is thus a hybrid approach
that creates an interface between expert knowledge and data-driven image
analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPTS: Single-Point Text Spotting. (arXiv:2112.07917v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07917">
<div class="article-summary-box-inner">
<span><p>Existing scene text spotting (i.e., end-to-end text detection and
recognition) methods rely on costly bounding box annotations (e.g., text-line,
word-level, or character-level bounding boxes). For the first time, we
demonstrate that training scene text spotting models can be achieved with an
extremely low-cost annotation of a single-point for each instance. We propose
an end-to-end scene text spotting method that tackles scene text spotting as a
sequence prediction task. Given an image as input, we formulate the desired
detection and recognition results as a sequence of discrete tokens and use an
auto-regressive Transformer to predict the sequence. The proposed method is
simple yet effective, which can achieve state-of-the-art results on widely used
benchmarks. Most significantly, we show that the performance is not very
sensitive to the positions of the point annotation, meaning that it can be much
easier to be annotated or even be automatically generated than the bounding box
that requires precise positions. We believe that such a pioneer attempt
indicates a significant opportunity for scene text spotting applications of a
much larger scale than previously possible. The code will be publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Putting People in their Place: Monocular Regression of 3D People in Depth. (arXiv:2112.08274v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08274">
<div class="article-summary-box-inner">
<span><p>Given an image with multiple people, our goal is to directly regress the pose
and shape of all the people as well as their relative depth. Inferring the
depth of a person in an image, however, is fundamentally ambiguous without
knowing their height. This is particularly problematic when the scene contains
people of very different sizes, e.g. from infants to adults. To solve this, we
need several things. First, we develop a novel method to infer the poses and
depth of multiple people in a single image. While previous work that estimates
multiple people does so by reasoning in the image plane, our method, called
BEV, adds an additional imaginary Bird's-Eye-View representation to explicitly
reason about depth. BEV reasons simultaneously about body centers in the image
and in depth and, by combing these, estimates 3D body position. Unlike prior
work, BEV is a single-shot method that is end-to-end differentiable. Second,
height varies with age, making it impossible to resolve depth without also
estimating the age of people in the image. To do so, we exploit a 3D body model
space that lets BEV infer shapes from infants to adults. Third, to train BEV,
we need a new dataset. Specifically, we create a "Relative Human" (RH) dataset
that includes age labels and relative depth relationships between the people in
the images. Extensive experiments on RH and AGORA demonstrate the effectiveness
of the model and training scheme. BEV outperforms existing methods on depth
reasoning, child shape estimation, and robustness to occlusion. The code and
dataset are released for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape from Polarization for Complex Scenes in the Wild. (arXiv:2112.11377v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11377">
<div class="article-summary-box-inner">
<span><p>We present a new data-driven approach with physics-based priors to
scene-level normal estimation from a single polarization image. Existing shape
from polarization (SfP) works mainly focus on estimating the normal of a single
object rather than complex scenes in the wild. A key barrier to high-quality
scene-level SfP is the lack of real-world SfP data in complex scenes. Hence, we
contribute the first real-world scene-level SfP dataset with paired input
polarization images and ground-truth normal maps. Then we propose a
learning-based framework with a multi-head self-attention module and viewing
encoding, which is designed to handle increasing polarization ambiguities
caused by complex materials and non-orthographic projection in scene-level SfP.
Our trained model can be generalized to far-field outdoor scenes as the
relationship between polarized light and surface normals is not affected by
distance. Experimental results demonstrate that our approach significantly
outperforms existing SfP models on two datasets. Our dataset and source code
will be publicly available at https://github.com/ChenyangLEI/sfp-wild
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reflash Dropout in Image Super-Resolution. (arXiv:2112.12089v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12089">
<div class="article-summary-box-inner">
<span><p>Dropout is designed to relieve the overfitting problem in high-level vision
tasks but is rarely applied in low-level vision tasks, like image
super-resolution (SR). As a classic regression problem, SR exhibits a different
behaviour as high-level tasks and is sensitive to the dropout operation.
However, in this paper, we show that appropriate usage of dropout benefits SR
networks and improves the generalization ability. Specifically, dropout is
better embedded at the end of the network and is significantly helpful for the
multi-degradation settings. This discovery breaks our common sense and inspires
us to explore its working mechanism. We further use two analysis tools -- one
is from recent network interpretation works, and the other is specially
designed for this task. The analysis results provide side proofs to our
experimental findings and show us a new perspective to understand SR networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Video Representation Learning with Cascade Positive Retrieval. (arXiv:2201.07989v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07989">
<div class="article-summary-box-inner">
<span><p>Self-supervised video representation learning has been shown to effectively
improve downstream tasks such as video retrieval and action recognition. In
this paper, we present the Cascade Positive Retrieval (CPR) that successively
mines positive examples w.r.t. the query for contrastive learning in a cascade
of stages. Specifically, CPR exploits multiple views of a query example in
different modalities, where an alternative view may help find another positive
example dissimilar in the query view. We explore the effects of possible CPR
configurations in ablations including the number of mining stages, the top
similar example selection ratio in each stage, and progressive training with an
incremental number of the final Top-k selection. The overall mining quality is
measured to reflect the recall across training set classes. CPR reaches a
median class mining recall of 83.3%, outperforming previous work by 5.5%.
Implementation-wise, CPR is complementary to pretext tasks and can be easily
applied to previous work. In the evaluation of pretraining on UCF101, CPR
consistently improves existing work and even achieves state-of-the-art R@1 of
56.7% and 24.4% in video retrieval as well as 83.8% and 54.8% in action
recognition on UCF101 and HMDB51. The code is available at https://github.
com/necla-ml/CPR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices. (arXiv:2201.12382v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12382">
<div class="article-summary-box-inner">
<span><p>Abstract visual reasoning (AVR) domain encompasses problems solving which
requires the ability to reason about relations among entities present in a
given scene. While humans, generally, solve AVR tasks in a "natural" way, even
without prior experience, this type of problems has proven difficult for
current machine learning systems. The paper summarises recent progress in
applying deep learning methods to solving AVR problems, as a proxy for studying
machine intelligence. We focus on the most common type of AVR tasks -- the
Raven's Progressive Matrices (RPMs) -- and provide a comprehensive review of
the learning methods and deep neural models applied to solve RPMs, as well as,
the RPM benchmark sets. Performance analysis of the state-of-the-art approaches
to solving RPMs leads to formulation of certain insights and remarks on the
current and future trends in this area. We conclude the paper by demonstrating
how real-world problems can benefit from the discoveries of RPM studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Review of Serial and Parallel Min-Cut/Max-Flow Algorithms for Computer Vision. (arXiv:2202.00418v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00418">
<div class="article-summary-box-inner">
<span><p>Minimum cut/maximum flow (min-cut/max-flow) algorithms solve a variety of
problems in computer vision and thus significant effort has been put into
developing fast min-cut/max-flow algorithms. As a result, it is difficult to
choose an ideal algorithm for a given problem. Furthermore, parallel algorithms
have not been thoroughly compared. In this paper, we evaluate the
state-of-the-art serial and parallel min-cut/max-flow algorithms on the largest
set of computer vision problems yet. We focus on generic algorithms, i.e., for
unstructured graphs, but also compare with the specialized GridCut
implementation. When applicable, GridCut performs best. Otherwise, the two
pseudoflow algorithms, Hochbaum pseudoflow and excesses incremental breadth
first search, achieves the overall best performance. The most memory efficient
implementation tested is the Boykov-Kolmogorov algorithm. Amongst generic
parallel algorithms, we find the bottom-up merging approach by Liu and Sun to
be best, but no method is dominant. Of the generic parallel methods, only the
parallel preflow push-relabel algorithm is able to efficiently scale with many
processors across problem sizes, and no generic parallel method consistently
outperforms serial algorithms. Finally, we provide and evaluate strategies for
algorithm selection to obtain good expected performance. We make our dataset
and implementations publicly available for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors. (arXiv:2203.06691v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06691">
<div class="article-summary-box-inner">
<span><p>The main question this work aims at answering is: "can morphing attack
detection (MAD) solutions be successfully developed based on synthetic data?".
Towards that, this work introduces the first synthetic-based MAD development
dataset, namely the Synthetic Morphing Attack Detection Development dataset
(SMDD). This dataset is utilized successfully to train three MAD backbones
where it proved to lead to high MAD performance, even on completely unknown
attack types. Additionally, an essential aspect of this work is the detailed
legal analyses of the challenges of using and sharing real biometric data,
rendering our proposed SMDD dataset extremely essential. The SMDD dataset,
consisting of 30,000 attack and 50,000 bona fide samples, is publicly available
for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial Expression Recognition. (arXiv:2203.13052v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13052">
<div class="article-summary-box-inner">
<span><p>Facial expression recognition plays an important role in human-computer
interaction. In this paper, we propose the Coarse-to-Fine Cascaded network with
Smooth Predicting (CFC-SP) to improve the performance of facial expression
recognition. CFC-SP contains two core components, namely Coarse-to-Fine
Cascaded networks (CFC) and Smooth Predicting (SP). For CFC, it first groups
several similar emotions to form a rough category, and then employs a network
to conduct a coarse but accurate classification. Later, an additional network
for these grouped emotions is further used to obtain fine-grained predictions.
For SP, it improves the recognition capability of the model by capturing both
universal and unique expression features. To be specific, the universal
features denote the general characteristic of facial emotions within a period
and the unique features denote the specific characteristic at this moment.
Experiments on Aff-Wild2 show the effectiveness of the proposed CFSP. We
achieved 3rd place in the Expression Classification Challenge of the 3rd
Competition on Affective Behavior Analysis in-the-wild. The code will be
released at https://github.com/BR-IDL/PaddleViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition. (arXiv:2203.14779v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14779">
<div class="article-summary-box-inner">
<span><p>Multimodal emotion recognition has recently gained much attention since it
can leverage diverse and complementary relationships over multiple modalities
(e.g., audio, visual, biosignals, etc.), and can provide some robustness to
noisy modalities. Most state-of-the-art methods for audio-visual (A-V) fusion
rely on recurrent networks or conventional attention mechanisms that do not
effectively leverage the complementary nature of A-V modalities. In this paper,
we focus on dimensional emotion recognition based on the fusion of facial and
vocal modalities extracted from videos. Specifically, we propose a joint
cross-attention model that relies on the complementary relationships to extract
the salient features across A-V modalities, allowing for accurate prediction of
continuous values of valence and arousal. The proposed fusion model efficiently
leverages the inter-modal relationships, while reducing the heterogeneity
between the features. In particular, it computes the cross-attention weights
based on correlation between the combined feature representation and individual
modalities. By deploying the combined A-V feature representation into the
cross-attention module, the performance of our fusion module improves
significantly over the vanilla cross-attention module. Experimental results on
validation-set videos from the AffWild2 dataset indicate that our proposed A-V
fusion model provides a cost-effective solution that can outperform
state-of-the-art approaches. The code is available on GitHub:
https://github.com/praveena2j/JointCrossAttentional-AV-Fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Agnostic Prior for Transfer Semantic Segmentation. (arXiv:2204.02684v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02684">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) is an important topic in the computer
vision community. The key difficulty lies in defining a common property between
the source and target domains so that the source-domain features can align with
the target-domain semantics. In this paper, we present a simple and effective
mechanism that regularizes cross-domain representation learning with a
domain-agnostic prior (DAP) that constrains the features extracted from source
and target domains to align with a domain-agnostic space. In practice, this is
easily implemented as an extra loss term that requires a little extra costs. In
the standard evaluation protocol of transferring synthesized data to real data,
we validate the effectiveness of different types of DAP, especially that
borrowed from a text embedding model that shows favorable performance beyond
the state-of-the-art UDA approaches in terms of segmentation accuracy. Our
research reveals that UDA benefits much from better proxies, possibly from
other data modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Trajectory-Aware Transformer for Video Super-Resolution. (arXiv:2204.04216v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04216">
<div class="article-summary-box-inner">
<span><p>Video super-resolution (VSR) aims to restore a sequence of high-resolution
(HR) frames from their low-resolution (LR) counterparts. Although some progress
has been made, there are grand challenges to effectively utilize temporal
dependency in entire video sequences. Existing approaches usually align and
aggregate video frames from limited adjacent frames (e.g., 5 or 7 frames),
which prevents these approaches from satisfactory results. In this paper, we
take one step further to enable effective spatio-temporal learning in videos.
We propose a novel Trajectory-aware Transformer for Video Super-Resolution
(TTVSR). In particular, we formulate video frames into several pre-aligned
trajectories which consist of continuous visual tokens. For a query token,
self-attention is only learned on relevant visual tokens along spatio-temporal
trajectories. Compared with vanilla vision Transformers, such a design
significantly reduces the computational cost and enables Transformers to model
long-range features. We further propose a cross-scale feature tokenization
module to overcome scale-changing problems that often occur in long-range
videos. Experimental results demonstrate the superiority of the proposed TTVSR
over state-of-the-art models, by extensive quantitative and qualitative
evaluations in four widely-used video super-resolution benchmarks. Both code
and pre-trained models can be downloaded at
https://github.com/researchmm/TTVSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transparent Shape from Single Polarization Images. (arXiv:2204.06331v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06331">
<div class="article-summary-box-inner">
<span><p>This paper presents a data-driven approach for transparent shape from
polarization. Due to the inherent high transmittance, the previous shape from
polarization(SfP) methods based on specular reflection model have difficulty in
estimating transparent shape, and the lack of datasets for transparent SfP also
limits the application of the data-driven approach. Hence, we construct the
transparent SfP dataset which consists of both synthetic and real-world
datasets. To determine the reliability of the physics-based reflection model,
we define the physics-based prior confidence by exploiting the inherent fault
of polarization information, then we propose a multi-branch fusion network to
embed the confidence. Experimental results show that our approach outperforms
other SfP methods. Compared with the previous method, the mean and median
angular error of our approach are reduced from $19.00^\circ$ and $14.91^\circ$
to $16.72^\circ$ and $13.36^\circ$, and the accuracy $11.25^\circ, 22.5^\circ,
30^\circ$ are improved from $38.36\%, 77.36\%, 87.48\%$ to $45.51\%, 78.86\%,
89.98\%$, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HASA: Hybrid Architecture Search with Aggregation Strategy for Echinococcosis Classification and Ovary Segmentation in Ultrasound Images. (arXiv:2204.06697v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06697">
<div class="article-summary-box-inner">
<span><p>Different from handcrafted features, deep neural networks can automatically
learn task-specific features from data. Due to this data-driven nature, they
have achieved remarkable success in various areas. However, manual design and
selection of suitable network architectures are time-consuming and require
substantial effort of human experts. To address this problem, researchers have
proposed neural architecture search (NAS) algorithms which can automatically
generate network architectures but suffer from heavy computational cost and
instability if searching from scratch. In this paper, we propose a hybrid NAS
framework for ultrasound (US) image classification and segmentation. The hybrid
framework consists of a pre-trained backbone and several searched cells (i.e.,
network building blocks), which takes advantage of the strengths of both NAS
and the expert knowledge from existing convolutional neural networks.
Specifically, two effective and lightweight operations, a mixed depth-wise
convolution operator and a squeeze-and-excitation block, are introduced into
the candidate operations to enhance the variety and capacity of the searched
cells. These two operations not only decrease model parameters but also boost
network performance. Moreover, we propose a re-aggregation strategy for the
searched cells, aiming to further improve the performance for different vision
tasks. We tested our method on two large US image datasets, including a 9-class
echinococcosis dataset containing 9566 images for classification and an ovary
dataset containing 3204 images for segmentation. Ablation experiments and
comparison with other handcrafted or automatically searched architectures
demonstrate that our method can generate more powerful and lightweight models
for the above US image classification and segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SoccerNet-Tracking: Multiple Object Tracking Dataset and Benchmark in Soccer Videos. (arXiv:2204.06918v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06918">
<div class="article-summary-box-inner">
<span><p>Tracking objects in soccer videos is extremely important to gather both
player and team statistics, whether it is to estimate the total distance run,
the ball possession or the team formation. Video processing can help automating
the extraction of those information, without the need of any invasive sensor,
hence applicable to any team on any stadium. Yet, the availability of datasets
to train learnable models and benchmarks to evaluate methods on a common
testbed is very limited. In this work, we propose a novel dataset for multiple
object tracking composed of 200 sequences of 30s each, representative of
challenging soccer scenarios, and a complete 45-minutes half-time for long-term
tracking. The dataset is fully annotated with bounding boxes and tracklet IDs,
enabling the training of MOT baselines in the soccer domain and a full
benchmarking of those methods on our segregated challenge sets. Our analysis
shows that multiple player, referee and ball tracking in soccer videos is far
from being solved, with several improvement required in case of fast motion or
in scenarios of severe occlusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Deep Learning to Identify the Critical 3D Structural Features of the Optic Nerve Head for Glaucoma Diagnosis. (arXiv:2204.06931v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06931">
<div class="article-summary-box-inner">
<span><p>Purpose: The optic nerve head (ONH) undergoes complex and deep 3D
morphological changes during the development and progression of glaucoma.
Optical coherence tomography (OCT) is the current gold standard to visualize
and quantify these changes, however the resulting 3D deep-tissue information
has not yet been fully exploited for the diagnosis and prognosis of glaucoma.
To this end, we aimed: (1) To compare the performance of two relatively recent
geometric deep learning techniques in diagnosing glaucoma from a single OCT
scan of the ONH; and (2) To identify the 3D structural features of the ONH that
are critical for the diagnosis of glaucoma.
</p>
<p>Methods: In this study, we included a total of 2,247 non-glaucoma and 2,259
glaucoma scans from 1,725 subjects. All subjects had their ONHs imaged in 3D
with Spectralis OCT. All OCT scans were automatically segmented using deep
learning to identify major neural and connective tissues. Each ONH was then
represented as a 3D point cloud. We used PointNet and dynamic graph
convolutional neural network (DGCNN) to diagnose glaucoma from such 3D ONH
point clouds and to identify the critical 3D structural features of the ONH for
glaucoma diagnosis.
</p>
<p>Results: Both the DGCNN (AUC: 0.97$\pm$0.01) and PointNet (AUC:
0.95$\pm$0.02) were able to accurately detect glaucoma from 3D ONH point
clouds. The critical points formed an hourglass pattern with most of them
located in the inferior and superior quadrant of the ONH.
</p>
<p>Discussion: The diagnostic accuracy of both geometric deep learning
approaches was excellent. Moreover, we were able to identify the critical 3D
structural features of the ONH for glaucoma diagnosis that tremendously
improved the transparency and interpretability of our method. Consequently, our
approach may have strong potential to be used in clinical applications for the
diagnosis and prognosis of a wide range of ophthalmic disorders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic spinal curvature measurement on ultrasound spine images using Faster R-CNN. (arXiv:2204.07988v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07988">
<div class="article-summary-box-inner">
<span><p>Ultrasound spine imaging technique has been applied to the assessment of
spine deformity. However, manual measurements of scoliotic angles on ultrasound
images are time-consuming and heavily rely on raters experience. The objectives
of this study are to construct a fully automatic framework based on Faster
R-CNN for detecting vertebral lamina and to measure the fitting spinal curves
from the detected lamina pairs. The framework consisted of two closely linked
modules: 1) the lamina detector for identifying and locating each lamina pairs
on ultrasound coronal images, and 2) the spinal curvature estimator for
calculating the scoliotic angles based on the chain of detected lamina. Two
hundred ultrasound images obtained from AIS patients were identified and used
for the training and evaluation of the proposed method. The experimental
results showed the 0.76 AP on the test set, and the Mean Absolute Difference
(MAD) between automatic and manual measurement which was within the clinical
acceptance error. Meanwhile the correlation between automatic measurement and
Cobb angle from radiographs was 0.79. The results revealed that our proposed
technique could provide accurate and reliable automatic curvature measurements
on ultrasound spine images for spine deformities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning 3D Semantics from Pose-Noisy 2D Images with Hierarchical Full Attention Network. (arXiv:2204.08084v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08084">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework to learn 3D point cloud semantics from 2D
multi-view image observations containing pose error. On the one hand, directly
learning from the massive, unstructured and unordered 3D point cloud is
computationally and algorithmically more difficult than learning from
compactly-organized and context-rich 2D RGB images. On the other hand, both
LiDAR point cloud and RGB images are captured in standard automated-driving
datasets. This motivates us to conduct a "task transfer" paradigm so that 3D
semantic segmentation benefits from aggregating 2D semantic cues, albeit pose
noises are contained in 2D image observations. Among all difficulties, pose
noise and erroneous prediction from 2D semantic segmentation approaches are the
main challenges for the task transfer. To alleviate the influence of those
factor, we perceive each 3D point using multi-view images and for each single
image a patch observation is associated. Moreover, the semantic labels of a
block of neighboring 3D points are predicted simultaneously, enabling us to
exploit the point structure prior to further improve the performance. A
hierarchical full attention network~(HiFANet) is designed to sequentially
aggregates patch, bag-of-frames and inter-point semantic cues, with
hierarchical attention mechanism tailored for different level of semantic cues.
Also, each preceding attention block largely reduces the feature size before
feeding to the next attention block, making our framework slim. Experiment
results on Semantic-KITTI show that the proposed framework outperforms existing
3D point cloud based methods significantly, it requires much less training data
and exhibits tolerance to pose noise. The code is available at
https://github.com/yuhanghe01/HiFANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Distracted Driving (SynDD1) dataset for analyzing distracted behaviors and various gaze zones of a driver. (arXiv:2204.08096v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08096">
<div class="article-summary-box-inner">
<span><p>This article presents a synthetic distracted driving (SynDD1) dataset for
machine learning models to detect and analyze drivers' various distracted
behavior and different gaze zones. We collected the data in a stationary
vehicle using three in-vehicle cameras positioned at locations: on the
dashboard, near the rearview mirror, and on the top right-side window corner.
The dataset contains two activity types: distracted activities, and gaze zones
for each participant and each activity type has two sets: without appearance
blocks and with appearance blocks such as wearing a hat or sunglasses. The
order and duration of each activity for each participant are random. In
addition, the dataset contains manual annotations for each activity, having its
start and end time annotated. Researchers could use this dataset to evaluate
the performance of machine learning algorithms for the classification of
various distracting activities and gaze zones of drivers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Optimal Transport for Comparing Histopathology Datasets. (arXiv:2204.08324v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08324">
<div class="article-summary-box-inner">
<span><p>Scarcity of labeled histopathology data limits the applicability of deep
learning methods to under-profiled cancer types and labels. Transfer learning
allows researchers to overcome the limitations of small datasets by
pre-training machine learning models on larger datasets similar to the small
target dataset. However, similarity between datasets is often determined
heuristically. In this paper, we propose a principled notion of distance
between histopathology datasets based on a hierarchical generalization of
optimal transport distances. Our method does not require any training, is
agnostic to model type, and preserves much of the hierarchical structure in
histopathology datasets imposed by tiling. We apply our method to H&amp;E stained
slides from The Cancer Genome Atlas from six different cancer types. We show
that our method outperforms a baseline distance in a cancer-type prediction
task. Our results also show that our optimal transport distance predicts
difficulty of transferability in a tumor vs.normal prediction setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer. (arXiv:2204.08680v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08680">
<div class="article-summary-box-inner">
<span><p>Vision transformers have achieved great successes in many computer vision
tasks. Most methods generate vision tokens by splitting an image into a regular
and fixed grid and treating each cell as a token. However, not all regions are
equally important in human-centric vision tasks, e.g., the human body needs a
fine representation with many tokens, while the image background can be modeled
by a few tokens. To address this problem, we propose a novel Vision
Transformer, called Token Clustering Transformer (TCFormer), which merges
tokens by progressive clustering, where the tokens can be merged from different
locations with flexible shapes and sizes. The tokens in TCFormer can not only
focus on important areas but also adjust the token shapes to fit the semantic
concept and adopt a fine resolution for regions containing critical details,
which is beneficial to capturing detailed information. Extensive experiments
show that TCFormer consistently outperforms its counterparts on different
challenging human-centric tasks and datasets, including whole-body pose
estimation on COCO-WholeBody and 3D human mesh reconstruction on 3DPW. Code is
available at https://github.com/ zengwang430521/TCFormer.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning. (arXiv:2204.08770v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08770">
<div class="article-summary-box-inner">
<span><p>Demystifying the interactions among multiple agents from their past
trajectories is fundamental to precise and interpretable trajectory prediction.
However, previous works only consider pair-wise interactions with limited
relational reasoning. To promote more comprehensive interaction modeling for
relational reasoning, we propose GroupNet, a multiscale hypergraph neural
network, which is novel in terms of both interaction capturing and
representation learning. From the aspect of interaction capturing, we propose a
trainable multiscale hypergraph to capture both pair-wise and group-wise
interactions at multiple group sizes. From the aspect of interaction
representation learning, we propose a three-element format that can be learnt
end-to-end and explicitly reason some relational factors including the
interaction strength and category. We apply GroupNet into both CVAE-based
prediction system and previous state-of-the-art prediction systems for
predicting socially plausible trajectories with relational reasoning. To
validate the ability of relational reasoning, we experiment with synthetic
physics simulations to reflect the ability to capture group behaviors, reason
interaction strength and interaction category. To validate the effectiveness of
prediction, we conduct extensive experiments on three real-world trajectory
prediction datasets, including NBA, SDD and ETH-UCY; and we show that with
GroupNet, the CVAE-based prediction system outperforms state-of-the-art
methods. We also show that adding GroupNet will further improve the performance
of previous state-of-the-art prediction systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08790">
<div class="article-summary-box-inner">
<span><p>Learning visual representations from natural language supervision has
recently shown great promise in a number of pioneering works. In general, these
language-augmented visual models demonstrate strong transferability to a
variety of datasets/tasks. However, it remains a challenge to evaluate the
transferablity of these foundation models due to the lack of easy-to-use
toolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation
of Language-augmented Visual Task-level Transfer), the first benchmark to
compare and evaluate pre-trained language-augmented visual models. Several
highlights include: (i) Datasets. As downstream evaluation suites, it consists
of 20 image classification datasets and 35 object detection datasets, each of
which is augmented with external knowledge. (ii) Toolkit. An automatic
hyper-parameter tuning toolkit is developed to ensure the fairness in model
adaption. To leverage the full power of language-augmented visual models, novel
language-aware initialization methods are proposed to significantly improve the
adaption performance. (iii) Metrics. A variety of evaluation metrics are used,
including sample-efficiency (zero-shot and few-shot) and parameter-efficiency
(linear probing and full model fine-tuning). We will release our toolkit and
evaluation platforms for the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised 3D shape segmentation with multilevel consistency and part substitution. (arXiv:2204.08824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08824">
<div class="article-summary-box-inner">
<span><p>The lack of fine-grained 3D shape segmentation data is the main obstacle to
developing learning-based 3D segmentation techniques. We propose an effective
semi-supervised method for learning 3D segmentations from a few labeled 3D
shapes and a large amount of unlabeled 3D data. For the unlabeled data, we
present a novel multilevel consistency loss to enforce consistency of network
predictions between perturbed copies of a 3D shape at multiple levels:
point-level, part-level, and hierarchical level. For the labeled data, we
develop a simple yet effective part substitution scheme to augment the labeled
3D shapes with more structural variations to enhance training. Our method has
been extensively validated on the task of 3D object semantic segmentation on
PartNet and ShapeNetPart, and indoor scene semantic segmentation on ScanNet. It
exhibits superior performance to existing semi-supervised and unsupervised
pre-training 3D approaches. Our code and trained models are publicly available
at https://github.com/isunchy/semi_supervised_3d_segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Core Box Image Recognition and its Improvement with a New Augmentation Technique. (arXiv:2204.08853v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08853">
<div class="article-summary-box-inner">
<span><p>Most methods for automated full-bore rock core image analysis (description,
colour, properties distribution, etc.) are based on separate core column
analyses. The core is usually imaged in a box because of the significant amount
of time taken to get an image for each core column. The work presents an
innovative method and algorithm for core columns extraction from core boxes.
The conditions for core boxes imaging may differ tremendously. Such differences
are disastrous for machine learning algorithms which need a large dataset
describing all possible data variations. Still, such images have some standard
features - a box and core. Thus, we can emulate different environments with a
unique augmentation described in this work. It is called template-like
augmentation (TLA). The method is described and tested on various environments,
and results are compared on an algorithm trained on both 'traditional' data and
a mix of traditional and TLA data. The algorithm trained with TLA data provides
better metrics and can detect core on most new images, unlike the algorithm
trained on data without TLA. The algorithm for core column extraction
implemented in an automated core description system speeds up the core box
processing by a factor of 20.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Invariant Representation Learning from EEG with Private Encoders. (arXiv:2201.11613v1 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11613">
<div class="article-summary-box-inner">
<span><p>Deep learning based electroencephalography (EEG) signal processing methods
are known to suffer from poor test-time generalization due to the changes in
data distribution. This becomes a more challenging problem when
privacy-preserving representation learning is of interest such as in clinical
settings. To that end, we propose a multi-source learning architecture where we
extract domain-invariant representations from dataset-specific private
encoders. Our model utilizes a maximum-mean-discrepancy (MMD) based domain
alignment approach to impose domain-invariance for encoded representations,
which outperforms state-of-the-art approaches in EEG-based emotion
classification. Furthermore, representations learned in our pipeline preserve
domain privacy as dataset-specific private encoding alleviates the need for
conventional, centralized EEG-based deep neural network training approaches
with shared parameters.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-21 23:08:15.872214215 UTC">2022-04-21 23:08:15 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>