<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-20T01:30:00Z">04-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning Helps Pretrained Models Learn the Intended Task. (arXiv:2204.08491v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08491">
<div class="article-summary-box-inner">
<span><p>Models can fail in unpredictable ways during deployment due to task
ambiguity, when multiple behaviors are consistent with the provided training
data. An example is an object classifier trained on red squares and blue
circles: when encountering blue squares, the intended behavior is undefined. We
investigate whether pretrained models are better active learners, capable of
disambiguating between the possible tasks a user may be trying to specify.
Intriguingly, we find that better active learning is an emergent property of
the pretraining process: pretrained models require up to 5 times fewer labels
when using uncertainty-based active learning, while non-pretrained models see
no or even negative benefit. We find these gains come from an ability to select
examples with attributes that disambiguate the intended behavior, such as rare
product categories or atypical backgrounds. These attributes are far more
linearly separable in pretrained model's representation spaces vs
non-pretrained models, suggesting a possible mechanism for this behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imagination-Augmented Natural Language Understanding. (arXiv:2204.08535v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08535">
<div class="article-summary-box-inner">
<span><p>Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CBR-iKB: A Case-Based Reasoning Approach for Question Answering over Incomplete Knowledge Bases. (arXiv:2204.08554v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08554">
<div class="article-summary-box-inner">
<span><p>Knowledge bases (KBs) are often incomplete and constantly changing in
practice. Yet, in many question answering applications coupled with knowledge
bases, the sparse nature of KBs is often overlooked. To this end, we propose a
case-based reasoning approach, CBR-iKB, for knowledge base question answering
(KBQA) with incomplete-KB as our main focus. Our method ensembles decisions
from multiple reasoning chains with a novel nonparametric reasoning algorithm.
By design, CBR-iKB can seamlessly adapt to changes in KBs without any
task-specific training or fine-tuning. Our method achieves 100% accuracy on
MetaQA and establishes new state-of-the-art on multiple benchmarks. For
instance, CBR-iKB achieves an accuracy of 70% on WebQSP under the incomplete-KB
setting, outperforming the existing state-of-the-art method by 22.3%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages. (arXiv:2204.08582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08582">
<div class="article-summary-box-inner">
<span><p>We present the MASSIVE dataset--Multilingual Amazon Slu resource package
(SLURP) for Slot-filling, Intent classification, and Virtual assistant
Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant
utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE
was created by tasking professional translators to localize the English-only
SLURP dataset into 50 typologically diverse languages from 29 genera. We also
present modeling results on XLM-R and mT5, including exact match accuracy,
intent classification accuracy, and slot-filling F1 score. We have released our
dataset, modeling code, and models publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Syntax-aware Language Modeling through Dependency Tree Conversion. (arXiv:2204.08644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08644">
<div class="article-summary-box-inner">
<span><p>Incorporating stronger syntactic biases into neural language models (LMs) is
a long-standing goal, but research in this area often focuses on modeling
English text, where constituent treebanks are readily available. Extending
constituent tree-based LMs to the multilingual setting, where dependency
treebanks are more common, is possible via dependency-to-constituency
conversion methods. However, this raises the question of which tree formats are
best for learning the model, and for which languages. We investigate this
question by training recurrent neural network grammars (RNNGs) using various
conversion methods, and evaluating them empirically in a multilingual setting.
We examine the effect on LM performance across nine conversion methods and five
languages through seven types of syntactic tests. On average, the performance
of our best model represents a 19 \% increase in accuracy over the worst choice
across all languages. Our best model shows the advantage over
sequential/overparameterized LMs, suggesting the positive effect of syntax
injection in a multilingual setting. Our experiments highlight the importance
of choosing the right tree formalism, and provide insights into making an
informed decision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LitMC-BERT: transformer-based multi-label classification of biomedical literature with an application on COVID-19 literature curation. (arXiv:2204.08649v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08649">
<div class="article-summary-box-inner">
<span><p>The rapid growth of biomedical literature poses a significant challenge for
curation and interpretation. This has become more evident during the COVID-19
pandemic. LitCovid, a literature database of COVID-19 related papers in PubMed,
has accumulated over 180,000 articles with millions of accesses. Approximately
10,000 new articles are added to LitCovid every month. A main curation task in
LitCovid is topic annotation where an article is assigned with up to eight
topics, e.g., Treatment and Diagnosis. The annotated topics have been widely
used both in LitCovid (e.g., accounting for ~18% of total uses) and downstream
studies such as network generation. However, it has been a primary curation
bottleneck due to the nature of the task and the rapid literature growth. This
study proposes LITMC-BERT, a transformer-based multi-label classification
method in biomedical literature. It uses a shared transformer backbone for all
the labels while also captures label-specific features and the correlations
between label pairs. We compare LITMC-BERT with three baseline models on two
datasets. Its micro-F1 and instance-based F1 are 5% and 4% higher than the
current best results, respectively, and only requires ~18% of the inference
time than the Binary BERT baseline. The related datasets and models are
available via https://github.com/ncbi/ml-transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On The Cross-Modal Transfer from Natural Language to Code through Adapter Modules. (arXiv:2204.08653v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08653">
<div class="article-summary-box-inner">
<span><p>Pre-trained neural Language Models (PTLM), such as CodeBERT, are recently
used in software engineering as models pre-trained on large source code
corpora. Their knowledge is transferred to downstream tasks (e.g. code clone
detection) via fine-tuning. In natural language processing (NLP), other
alternatives for transferring the knowledge of PTLMs are explored through using
adapters, compact, parameter efficient modules inserted in the layers of the
PTLM. Although adapters are known to facilitate adapting to many downstream
tasks compared to fine-tuning the model that require retraining all of the
models' parameters -- which owes to the adapters' plug and play nature and
being parameter efficient -- their usage in software engineering is not
explored.
</p>
<p>Here, we explore the knowledge transfer using adapters and based on the
Naturalness Hypothesis proposed by Hindle et. al \cite{hindle2016naturalness}.
Thus, studying the bimodality of adapters for two tasks of cloze test and code
clone detection, compared to their benchmarks from the CodeXGLUE platform.
These adapters are trained using programming languages and are inserted in a
PTLM that is pre-trained on English corpora (N-PTLM). Three programming
languages, C/C++, Python, and Java, are studied along with extensive
experiments on the best setup used for adapters. Improving the results of the
N-PTLM confirms the success of the adapters in knowledge transfer to software
engineering, which sometimes are in par with or exceed the results of a PTLM
trained on source code; while being more efficient in terms of the number of
parameters, memory usage, and inference time. Our results can open new
directions to build smaller models for more software engineering tasks. We open
source all the scripts and the trained adapters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mono vs Multilingual BERT for Hate Speech Detection and Text Classification: A Case Study in Marathi. (arXiv:2204.08669v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08669">
<div class="article-summary-box-inner">
<span><p>Transformers are the most eminent architectures used for a vast range of
Natural Language Processing tasks. These models are pre-trained over a large
text corpus and are meant to serve state-of-the-art results over tasks like
text classification. In this work, we conduct a comparative study between
monolingual and multilingual BERT models. We focus on the Marathi language and
evaluate the models on the datasets for hate speech detection, sentiment
analysis and simple text classification in Marathi. We use standard
multilingual models such as mBERT, indicBERT and xlm-RoBERTa and compare with
MahaBERT, MahaALBERT and MahaRoBERTa, the monolingual models for Marathi. We
further show that Marathi monolingual models outperform the multilingual BERT
variants on five different downstream fine-tuning experiments. We also evaluate
sentence embeddings from these models by freezing the BERT encoder layers. We
show that monolingual MahaBERT based models provide rich representations as
compared to sentence embeddings from multi-lingual counterparts. However, we
observe that these embeddings are not generic enough and do not work well on
out of domain social media datasets. We consider two Marathi hate speech
datasets L3Cube-MahaHate, HASOC-2021, a Marathi sentiment classification
dataset L3Cube-MahaSent, and Marathi Headline, Articles classification
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks. (arXiv:2204.08688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08688">
<div class="article-summary-box-inner">
<span><p>Since 2017, the Transformer-based models play critical roles in various
downstream Natural Language Processing tasks. However, a common limitation of
the attention mechanism utilized in Transformer Encoder is that it cannot
automatically capture the information of word order, so explicit position
embeddings are generally required to be fed into the target model. In contrast,
Transformer Decoder with the causal attention masks is naturally sensitive to
the word order. In this work, we focus on improving the position encoding
ability of BERT with the causal attention masks. Furthermore, we propose a new
pre-trained language model DecBERT and evaluate it on the GLUE benchmark.
Experimental results show that (1) the causal attention mask is effective for
BERT on the language understanding tasks; (2) our DecBERT model without
position embeddings achieve comparable performance on the GLUE benchmark; and
(3) our modification accelerates the pre-training process and DecBERT w/ PE
achieves better overall performance than the baseline systems when pre-training
with the same amount of computational resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Authentic Adversarial Examples beyond Meaning-preserving with Doubly Round-trip Translation. (arXiv:2204.08689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08689">
<div class="article-summary-box-inner">
<span><p>Generating adversarial examples for Neural Machine Translation (NMT) with
single Round-Trip Translation (RTT) has achieved promising results by releasing
the meaning-preserving restriction. However, a potential pitfall for this
approach is that we cannot decide whether the generated examples are
adversarial to the target NMT model or the auxiliary backward one, as the
reconstruction error through the RTT can be related to either. To remedy this
problem, we propose a new criterion for NMT adversarial examples based on the
Doubly Round-Trip Translation (DRTT). Specifically, apart from the
source-target-source RTT, we also consider the target-source-target one, which
is utilized to pick out the authentic adversarial examples for the target NMT
model. Additionally, to enhance the robustness of the NMT model, we introduce
the masked language models to construct bilingual adversarial pairs based on
DRTT, which are used to train the NMT model directly. Extensive experiments on
both the clean and noisy test sets (including the artificial and natural noise)
show that our approach substantially improves the robustness of NMT models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Table-based Fact Verification with Self-adaptive Mixture of Experts. (arXiv:2204.08753v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08753">
<div class="article-summary-box-inner">
<span><p>The table-based fact verification task has recently gained widespread
attention and yet remains to be a very challenging problem. It inherently
requires informative reasoning over natural language together with different
numerical and logical reasoning on tables (e.g., count, superlative,
comparative). Considering that, we exploit mixture-of-experts and present in
this paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE).
Specifically, we have developed a mixture-of-experts neural network to
recognize and execute different types of reasoning -- the network is composed
of multiple experts, each handling a specific part of the semantics for
reasoning, whereas a management module is applied to decide the contribution of
each expert network to the verification result. A self-adaptive method is
developed to teach the management module combining results of different experts
more efficiently without external knowledge. The experimental results
illustrate that our framework achieves 85.1% accuracy on the benchmark dataset
TabFact, comparable with the previous state-of-the-art models. We hope our
framework can serve as a new baseline for table-based verification. Our code is
available at https://github.com/THUMLP/SaMoE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndicXNLI: Evaluating Multilingual Inference for Indian Languages. (arXiv:2204.08776v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08776">
<div class="article-summary-box-inner">
<span><p>While Indic NLP has made rapid advances recently in terms of the availability
of corpora and pre-trained models, benchmark datasets on standard NLU tasks are
limited. To this end, we introduce IndicXNLI, an NLI dataset for 11 Indic
languages. It has been created by high-quality machine translation of the
original English XNLI dataset and our analysis attests to the quality of
IndicXNLI. By finetuning different pre-trained LMs on this IndicXNLI, we
analyze various cross-lingual transfer techniques with respect to the impact of
the choice of language models, languages, multi-linguality, mix-language input,
etc. These experiments provide us with useful insights into the behaviour of
pre-trained models for a diverse set of languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where Was COVID-19 First Discovered? Designing a Question-Answering System for Pandemic Situations. (arXiv:2204.08787v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08787">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic is accompanied by a massive "infodemic" that makes it
hard to identify concise and credible information for COVID-19-related
questions, like incubation time, infection rates, or the effectiveness of
vaccines. As a novel solution, our paper is concerned with designing a
question-answering system based on modern technologies from natural language
processing to overcome information overload and misinformation in pandemic
situations. To carry out our research, we followed a design science research
approach and applied Ingwersen's cognitive model of information retrieval
interaction to inform our design process from a socio-technical lens. On this
basis, we derived prescriptive design knowledge in terms of design requirements
and design principles, which we translated into the construction of a
prototypical instantiation. Our implementation is based on the comprehensive
CORD-19 dataset, and we demonstrate our artifact's usefulness by evaluating its
answer quality based on a sample of COVID-19 questions labeled by biomedical
experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08790">
<div class="article-summary-box-inner">
<span><p>Learning visual representations from natural language supervision has
recently shown great promise in a number of pioneering works. In general, these
language-augmented visual models demonstrate strong transferability to a
variety of datasets/tasks. However, it remains a challenge to evaluate the
transferablity of these foundation models due to the lack of easy-to-use
toolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation
of Language-augmented Visual Task-level Transfer), the first benchmark to
compare and evaluate pre-trained language-augmented visual models. Several
highlights include: (i) Datasets. As downstream evaluation suites, it consists
of 20 image classification datasets and 35 object detection datasets, each of
which is augmented with external knowledge. (ii) Toolkit. An automatic
hyper-parameter tuning toolkit is developed to ensure the fairness in model
adaption. To leverage the full power of language-augmented visual models, novel
language-aware initialization methods are proposed to significantly improve the
adaption performance. (iii) Metrics. A variety of evaluation metrics are used,
including sample-efficiency (zero-shot and few-shot) and parameter-efficiency
(linear probing and full model fine-tuning). We will release our toolkit and
evaluation platforms for the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Toxicity Triggers on Reddit in the Context of Singapore. (arXiv:2204.08806v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08806">
<div class="article-summary-box-inner">
<span><p>While the contagious nature of online toxicity sparked increasing interest in
its early detection and prevention, most of the literature focuses on the
Western world. In this work, we demonstrate that 1) it is possible to detect
toxicity triggers in an Asian online community, and 2) toxicity triggers can be
strikingly different between Western and Eastern contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SmartSales: Sales Script Extraction and Analysis from Sales Chatlog. (arXiv:2204.08811v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08811">
<div class="article-summary-box-inner">
<span><p>In modern sales applications, automatic script extraction and management
greatly decrease the need for human labor to collect the winning sales scripts,
which largely boost the success rate for sales and can be shared across the
sales teams. In this work, we present the SmartSales system to serve both the
sales representatives and managers to attain the sales insights from the
large-scale sales chatlog. SmartSales consists of three modules: 1) Customer
frequently asked questions (FAQ) extraction aims to enrich the FAQ knowledge
base by harvesting high quality customer question-answer pairs from the
chatlog. 2) Customer objection response assists the salespeople to figure out
the typical customer objections and corresponding winning sales scripts, as
well as search for proper sales responses for a certain customer objection. 3)
Sales manager dashboard helps sales managers to monitor whether a specific
sales representative or team follows the sales standard operating procedures
(SOP). The proposed prototype system is empowered by the state-of-the-art
conversational intelligence techniques and has been running on the Tencent
Cloud to serve the sales teams from several different areas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing for the Usage of Grammatical Number. (arXiv:2204.08831v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08831">
<div class="article-summary-box-inner">
<span><p>A central quest of probing is to uncover how pre-trained models encode a
linguistic property within their representations. An encoding, however, might
be spurious-i.e., the model might not rely on it when making predictions. In
this paper, we try to find encodings that the model actually uses, introducing
a usage-based probing setup. We first choose a behavioral task which cannot be
solved without using the linguistic property. Then, we attempt to remove the
property by intervening on the model's representations. We contend that, if an
encoding is used by the model, its removal should harm the performance on the
chosen behavioral task. As a case study, we focus on how BERT encodes
grammatical number, and on how it uses this encoding to solve the number
agreement task. Experimentally, we find that BERT relies on a linear encoding
of grammatical number to produce the correct behavioral output. We also find
that BERT uses a separate encoding of grammatical number for nouns and verbs.
Finally, we identify in which layers information about grammatical number is
transferred from a noun to its head verb.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Tokenization on Language Models: An Analysis for Turkish. (arXiv:2204.08832v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08832">
<div class="article-summary-box-inner">
<span><p>Tokenization is an important text preprocessing step to prepare input tokens
for deep language models. WordPiece and BPE are de facto methods employed by
important models, such as BERT and GPT. However, the impact of tokenization can
be different for morphologically rich languages, such as Turkic languages,
where many words can be generated by adding prefixes and suffixes. We compare
five tokenizers at different granularity levels, i.e. their outputs vary from
smallest pieces of characters to the surface form of words, including a
Morphological-level tokenizer. We train these tokenizers and pretrain
medium-sized language models using RoBERTa pretraining procedure on the Turkish
split of the OSCAR corpus. We then fine-tune our models on six downstream
tasks. Our experiments, supported by statistical tests, reveal that
Morphological-level tokenizer has challenging performance with de facto
tokenizers. Furthermore, we find that increasing the vocabulary size improves
the performance of Morphological and Word-level tokenizers more than that of de
facto tokenizers. The ratio of the number of vocabulary parameters to the total
number of model parameters can be empirically chosen as 20% for de facto
tokenizers and 40% for other tokenizers to obtain a reasonable trade-off
between model size and performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I still have Time(s): Extending HeidelTime for German Texts. (arXiv:2204.08848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08848">
<div class="article-summary-box-inner">
<span><p>HeidelTime is one of the most widespread and successful tools for detecting
temporal expressions in texts. Since HeidelTime's pattern matching system is
based on regular expression, it can be extended in a convenient way. We present
such an extension for the German resources of HeidelTime: HeidelTime-EXT . The
extension has been brought about by means of observing false negatives within
real world texts and various time banks. The gain in coverage is 2.7% or 8.5%,
depending on the admitted degree of potential overgeneralization. We describe
the development of HeidelTime-EXT, its evaluation on text samples from various
genres, and share some linguistic observations. HeidelTime ext can be obtained
from https://github.com/texttechnologylab/heideltime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ATP: AMRize Then Parse! Enhancing AMR Parsing with PseudoAMRs. (arXiv:2204.08875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08875">
<div class="article-summary-box-inner">
<span><p>As Abstract Meaning Representation (AMR) implicitly involves compound
semantic annotations, we hypothesize auxiliary tasks which are semantically or
formally related can better enhance AMR parsing. We find that 1) Semantic role
labeling (SRL) and dependency parsing (DP), would bring more performance gain
than other tasks e.g. MT and summarization in the text-to-AMR transition even
with much less data. 2) To make a better fit for AMR, data from auxiliary tasks
should be properly "AMRized" to PseudoAMR before training. Knowledge from
shallow level parsing tasks can be better transferred to AMR Parsing with
structure transform. 3) Intermediate-task learning is a better paradigm to
introduce auxiliary tasks to AMR parsing, compared to multitask learning. From
an empirical perspective, we propose a principled method to involve auxiliary
tasks to boost AMR parsing. Extensive experiments show that our method achieves
new state-of-the-art performance on different benchmarks especially in
topology-related scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Phrase Retrieval. (arXiv:2204.08887v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08887">
<div class="article-summary-box-inner">
<span><p>Cross-lingual retrieval aims to retrieve relevant text across languages.
Current methods typically achieve cross-lingual retrieval by learning
language-agnostic text representations in word or sentence level. However, how
to learn phrase representations for cross-lingual phrase retrieval is still an
open problem. In this paper, we propose XPR, a cross-lingual phrase retriever
that extracts phrase representations from unlabeled example sentences.
Moreover, we create a large-scale cross-lingual phrase retrieval dataset, which
contains 65K bilingual phrase pairs and 4.2M example sentences in 8
English-centric language pairs. Experimental results show that XPR outperforms
state-of-the-art baselines which utilize word-level or sentence-level
representations. XPR also shows impressive zero-shot transferability that
enables the model to perform retrieval in an unseen language pair during
training. Our dataset, code, and trained models are publicly available at
www.github.com/cwszz/XPR/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A survey on improving NLP models with human explanations. (arXiv:2204.08892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08892">
<div class="article-summary-box-inner">
<span><p>Training a model with access to human explanations can improve data
efficiency and model performance on in- and out-of-domain data. Adding to these
empirical findings, similarity with the process of human learning makes
learning from explanations a promising way to establish a fruitful
human-machine interaction. Several methods have been proposed for improving
natural language processing (NLP) models with human explanations, that rely on
different explanation types and mechanism for integrating these explanations
into the learning process. These methods are rarely compared with each other,
making it hard for practitioners to choose the best combination of explanation
type and integration mechanism for a specific use-case. In this paper, we give
an overview of different methods for learning from human explanations, and
discuss different factors that can inform the decision of which method to
choose for a specific use-case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blockwise Streaming Transformer for Spoken Language Understanding and Simultaneous Speech Translation. (arXiv:2204.08920v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08920">
<div class="article-summary-box-inner">
<span><p>Although Transformers have gained success in several speech processing tasks
like spoken language understanding (SLU) and speech translation (ST), achieving
online processing while keeping competitive performance is still essential for
real-world interaction. In this paper, we take the first step on streaming SLU
and simultaneous ST using a blockwise streaming Transformer, which is based on
contextual block processing and blockwise synchronous beam search. Furthermore,
we design an automatic speech recognition (ASR)-based intermediate loss
regularization for the streaming SLU task to improve the classification
performance further. As for the simultaneous ST task, we propose a
cross-lingual encoding method, which employs a CTC branch optimized with target
language translations. In addition, the CTC translation output is also used to
refine the search space with CTC prefix score, achieving joint CTC/attention
simultaneous translation for the first time. Experiments for SLU are conducted
on FSC and SLURP corpora, while the ST task is evaluated on Fisher-CallHome
Spanish and MuST-C En-De corpora. Experimental results show that the blockwise
streaming Transformer achieves competitive results compared to offline models,
especially with our proposed methods that further yield a 2.4% accuracy gain on
the SLU task and a 4.3 BLEU gain on the ST task over streaming baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Structure Distillation for BERT Transferring. (arXiv:2204.08922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08922">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation is an approach to transfer information on
representations from a teacher to a student by reducing their difference. A
challenge of this approach is to reduce the flexibility of the student's
representations inducing inaccurate learning of the teacher's knowledge. To
resolve it in BERT transferring, we investigate distillation of structures of
representations specified to three types: intra-feature, local inter-feature,
global inter-feature structures. To transfer them, we introduce \textit{feature
structure distillation} methods based on the Centered Kernel Alignment, which
assigns a consistent value to similar features structures and reveals more
informative relations. In particular, a memory-augmented transfer method with
clustering is implemented for the global structures. In the experiments on the
nine tasks for language understanding of the GLUE dataset, the proposed methods
effectively transfer the three types of structures and improve performance
compared to state-of-the-art distillation methods. Indeed, the code for the
methods is available in https://github.com/maroo-sky/FSD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex. (arXiv:2204.08941v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08941">
<div class="article-summary-box-inner">
<span><p>CodexDB is an SQL processing engine whose internals can be customized via
natural language instructions. CodexDB is based on OpenAI's GPT-3 Codex model
which translates text into code. It is a framework on top of GPT-3 Codex that
decomposes complex SQL queries into a series of simple processing steps,
described in natural language. Processing steps are enriched with user-provided
instructions and descriptions of database properties. Codex translates the
resulting text into query processing code. An early prototype of CodexDB is
able to generate correct code for a majority of queries of the WikiSQL
benchmark and can be customized in various ways.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies. (arXiv:2204.08952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08952">
<div class="article-summary-box-inner">
<span><p>Prior studies in privacy policies frame the question answering (QA) tasks as
identifying the most relevant text segment or a list of sentences from the
policy document for a user query. However, annotating such a dataset is
challenging as it requires specific domain expertise (e.g., law academics).
Even if we manage a small-scale one, a bottleneck that remains is that the
labeled data are heavily imbalanced (only a few segments are relevant)
--limiting the gain in this domain. Therefore, in this paper, we develop a
novel data augmentation framework based on ensembling retriever models that
captures the relevant text segments from unlabeled policy documents and expand
the positive examples in the training set. In addition, to improve the
diversity and quality of the augmented data, we leverage multiple pre-trained
language models (LMs) and cascaded them with noise reduction oracles. Using our
augmented data on the PrivacyQA benchmark, we elevate the existing baseline by
a large margin (10\% F1) and achieve a new state-of-the-art F1 score of 50\%.
Our ablation studies provide further insights into the effectiveness of our
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Odia Shallow Parser. (arXiv:2204.08960v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08960">
<div class="article-summary-box-inner">
<span><p>Shallow parsing is an essential task for many NLP applications like machine
translation, summarization, sentiment analysis, aspect identification and many
more. Quality annotated corpora is critical for building accurate shallow
parsers. Many Indian languages are resource poor with respect to the
availability of corpora in general. So, this paper is an attempt towards
creating quality corpora for shallow parsers. The contribution of this paper is
two folds: creation pos and chunk annotated corpora for Odia and development of
baseline systems for pos tagging and chunking in Odia.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Text Formality: A Study of Text Classification Approaches. (arXiv:2204.08975v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08975">
<div class="article-summary-box-inner">
<span><p>Formality is an important characteristic of text documents. The automatic
detection of the formality level of a text is potentially beneficial for
various natural language processing tasks, such as retrieval of texts with a
desired formality level, integration in language learning and document editing
platforms, or evaluating the desired conversation tone by chatbots. Recently
two large-scale datasets were introduced for multiple languages featuring
formality annotation. However, they were primarily used for the training of
style transfer models. However, detection text formality on its own may also be
a useful application. This work proposes the first systematic study of
formality detection methods based on current (and more classic) machine
learning methods and delivers the best-performing models for public usage. We
conducted three types of experiments -- monolingual, multilingual, and
cross-lingual. The study shows the overcome of BiLSTM-based models over
transformer-based ones for the formality classification task. We release
formality detection models for several languages yielding state of the art
results and possessing tested cross-lingual capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets. (arXiv:2204.08997v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08997">
<div class="article-summary-box-inner">
<span><p>In recent years, interest has arisen in using machine learning to improve the
efficiency of automatic medical consultation and enhance patient experience. In
this paper, we propose two frameworks to support automatic medical
consultation, namely doctor-patient dialogue understanding and task-oriented
interaction. A new large medical dialogue dataset with multi-level fine-grained
annotations is introduced and five independent tasks are established, including
named entity recognition, dialogue act classification, symptom label inference,
medical report generation and diagnosis-oriented dialogue policy. We report a
set of benchmark results for each task, which shows the usability of the
dataset and sets a baseline for future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Locality of Attention in Direct Speech Translation. (arXiv:2204.09028v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09028">
<div class="article-summary-box-inner">
<span><p>Transformers have achieved state-of-the-art results across multiple NLP
tasks. However, the self-attention mechanism complexity scales quadratically
with the sequence length, creating an obstacle for tasks involving long
sequences, like in the speech domain. In this paper, we discuss the usefulness
of self-attention for Direct Speech Translation. First, we analyze the
layer-wise token contributions in the self-attention of the encoder, unveiling
local diagonal patterns. To prove that some attention weights are avoidable, we
propose to substitute the standard self-attention with a local efficient one,
setting the amount of context used based on the results of the analysis. With
this approach, our model matches the baseline performance, and improves the
efficiency by skipping the computation of those weights that standard attention
discards.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good, Better, Best: Textual Distractors Generation for Multiple-Choice Visual Question Answering via Reinforcement Learning. (arXiv:1910.09134v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.09134">
<div class="article-summary-box-inner">
<span><p>Multiple-choice VQA has drawn increasing attention from researchers and
end-users recently. As the demand for automatically constructing large-scale
multiple-choice VQA data grows, we introduce a novel task called textual
Distractors Generation for VQA (DG-VQA) focusing on generating challenging yet
meaningful distractors given the context image, question, and correct answer.
The DG-VQA task aims at generating distractors without ground-truth training
samples since such resources are rarely available. To tackle the DG-VQA
unsupervisedly, we propose Gobbet, a reinforcement learning(RL) based framework
that utilizes pre-trained VQA models as an alternative knowledge base to guide
the distractor generation process. In Gobbet, a pre-trained VQA model serves as
the environment in RL setting to provide feedback for the input multi-modal
query, while a neural distractor generator serves as the agent to take actions
accordingly. We propose to use existing VQA models' performance degradation as
indicators of the quality of generated distractors. On the other hand, we show
the utility of generated distractors through data augmentation experiments,
since robustness is more and more important when AI models apply to
unpredictable open-domain scenarios or security-sensitive applications. We
further conduct a manual case study on the factors why distractors generated by
Gobbet can fool existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models' Transferability. (arXiv:2103.07162v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07162">
<div class="article-summary-box-inner">
<span><p>This paper investigates whether the power of the models pre-trained on text
data, such as BERT, can be transferred to general token sequence classification
applications. To verify pre-trained models' transferability, we test the
pre-trained models on text classification tasks with meanings of tokens
mismatches, and real-world non-text token sequence classification data,
including amino acid, DNA, and music. We find that even on non-text data, the
models pre-trained on text converge faster, perform better than the randomly
initialized models, and only slightly worse than the models using task-specific
knowledge. We also find that the representations of the text and non-text
pre-trained models share non-trivial similarities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XLM-E: Cross-lingual Language Model Pre-training via ELECTRA. (arXiv:2106.16138v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.16138">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce ELECTRA-style tasks to cross-lingual language
model pre-training. Specifically, we present two pre-training tasks, namely
multilingual replaced token detection, and translation replaced token
detection. Besides, we pretrain the model, named as XLM-E, on both multilingual
and parallel corpora. Our model outperforms the baseline models on various
cross-lingual understanding tasks with much less computation cost. Moreover,
analysis shows that XLM-E tends to obtain better cross-lingual transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters. (arXiv:2108.08103v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08103">
<div class="article-summary-box-inner">
<span><p>The open-access dissemination of pretrained language models through online
repositories has led to a democratization of state-of-the-art natural language
processing (NLP) research. This also allows people outside of NLP to use such
models and adapt them to specific use-cases. However, a certain amount of
technical proficiency is still required which is an entry barrier for users who
want to apply these models to a certain task but lack the necessary knowledge
or resources. In this work, we aim to overcome this gap by providing a tool
which allows researchers to leverage pretrained models without writing a single
line of code. Built upon the parameter-efficient adapter modules for transfer
learning, our AdapterHub Playground provides an intuitive interface, allowing
the usage of adapters for prediction, training and analysis of textual data for
a variety of NLP tasks. We present the tool's architecture and demonstrate its
advantages with prototypical use-cases, where we show that predictive
performance can easily be increased in a few-shot learning scenario. Finally,
we evaluate its usability in a user study. We provide the code and a live
interface at https://adapter-hub.github.io/playground.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting and Inferring Personal Attributes from Dialogue. (arXiv:2109.12702v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12702">
<div class="article-summary-box-inner">
<span><p>Personal attributes represent structured information about a person, such as
their hobbies, pets, family, likes and dislikes. We introduce the tasks of
extracting and inferring personal attributes from human-human dialogue, and
analyze the linguistic demands of these tasks. To meet these challenges, we
introduce a simple and extensible model that combines an autoregressive
language model utilizing constrained attribute generation with a discriminative
reranker. Our model outperforms strong baselines on extracting personal
attributes as well as inferring personal attributes that are not contained
verbatim in utterances and instead requires commonsense reasoning and lexical
inferences, which occur frequently in everyday conversation. Finally, we
demonstrate the benefit of incorporating personal attributes in social
chit-chat and task-oriented dialogue settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts. (arXiv:2110.07280v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07280">
<div class="article-summary-box-inner">
<span><p>Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of
the factual information extracted from Large Language Models (LLMs) depends on
the prompts used to query them. This inconsistency is problematic because
different users will query LLMs for the same information using different
wording, but should receive the same, accurate responses regardless. In this
work we aim to address this shortcoming by introducing P-Adapters: lightweight
models that sit between the embedding layer and first attention layer of LLMs.
They take LLM embeddings as input and output continuous prompts that are used
to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models
that learn a set of continuous prompts ("experts") and select one to query the
LLM. They require a separate classifier trained on human-annotated data to map
natural language prompts to the continuous ones. P-Adapters perform comparably
to the more complex MoE models in extracting factual information from BERT and
RoBERTa while eliminating the need for additional annotations. P-Adapters show
between 12-26% absolute improvement in precision and 36-50% absolute
improvement in consistency over a baseline of only using natural language
queries. Finally, we investigate what makes P-Adapters successful and conclude
that a significant factor is access to the LLM's embeddings of the original
natural language prompt, particularly the subject of the entity pair being
queried.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. (arXiv:2111.02735v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02735">
<div class="article-summary-box-inner">
<span><p>Speech self-supervised models such as wav2vec 2.0 and HuBERT are making
revolutionary progress in Automatic Speech Recognition (ASR). However, they
have not been totally proved to produce better performance on tasks other than
ASR. In this work, we explore partial fine-tuning and entire fine-tuning on
wav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks :
Speech Emotion Recognition, Speaker Verification and Spoken Language
Understanding. With simple proposed down-stream frameworks, the best scores
reach 79.58% weighted accuracy for Speech Emotion Recognition on IEMOCAP, 2.36%
equal error rate for Speaker Verification on VoxCeleb1, 89.38% accuracy for
Intent Classification and 78.92% F1 for Slot Filling on SLURP, thus setting new
state-of-the-art on the three benchmarks, showing the strong power of
fine-tuned wav2vec 2.0 and HuBERT models on learning prosodic, voice-print and
semantic representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Step-unrolled Denoising Autoencoders for Text Generation. (arXiv:2112.06749v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06749">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a new generative model of text, Step-unrolled
Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models.
Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a
sequence of tokens, starting from random inputs and improving them each time
until convergence. We present a simple new improvement operator that converges
in fewer iterations than diffusion methods, while qualitatively producing
better samples on natural language datasets. SUNDAE achieves state-of-the-art
results (among non-autoregressive methods) on the WMT'14 English-to-German
translation task and good qualitative results on unconditional language
modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python
code from GitHub. The non-autoregressive nature of SUNDAE opens up
possibilities beyond left-to-right prompted generation, by filling in arbitrary
blank patterns in a template.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Key Multimodal Backdoors for Visual Question Answering. (arXiv:2112.07668v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07668">
<div class="article-summary-box-inner">
<span><p>The success of deep learning has enabled advances in multimodal tasks that
require non-trivial fusion of multiple input domains. Although multimodal
models have shown potential in many problems, their increased complexity makes
them more vulnerable to attacks. A Backdoor (or Trojan) attack is a class of
security vulnerability wherein an attacker embeds a malicious secret behavior
into a network (e.g. targeted misclassification) that is activated when an
attacker-specified trigger is added to an input. In this work, we show that
multimodal networks are vulnerable to a novel type of attack that we refer to
as Dual-Key Multimodal Backdoors. This attack exploits the complex fusion
mechanisms used by state-of-the-art networks to embed backdoors that are both
effective and stealthy. Instead of using a single trigger, the proposed attack
embeds a trigger in each of the input modalities and activates the malicious
behavior only when both the triggers are present. We present an extensive study
of multimodal backdoors on the Visual Question Answering (VQA) task with
multiple architectures and visual feature backbones. A major challenge in
embedding backdoors in VQA models is that most models use visual features
extracted from a fixed pretrained object detector. This is challenging for the
attacker as the detector can distort or ignore the visual trigger entirely,
which leads to models where backdoors are over-reliant on the language trigger.
We tackle this problem by proposing a visual trigger optimization strategy
designed for pretrained object detectors. Through this method, we create
Dual-Key Backdoors with over a 98% attack success rate while only poisoning 1%
of the training data. Finally, we release TrojVQA, a large collection of clean
and trojan VQA models to enable research in defending against multimodal
backdoors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks. (arXiv:2201.09745v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09745">
<div class="article-summary-box-inner">
<span><p>Since a vast number of tables can be easily collected from web pages,
spreadsheets, PDFs, and various other document types, a flurry of table
pre-training frameworks have been proposed following the success of text and
images, and they have achieved new state-of-the-arts on various tasks such as
table question answering, table type recognition, column relation
classification, table search, formula prediction, etc. To fully use the
supervision signals in unlabeled tables, a variety of pre-training objectives
have been designed and evaluated, for example, denoising cell values,
predicting numerical relationships, and implicitly executing SQLs. And to best
leverage the characteristics of (semi-)structured tables, various tabular
language models, particularly with specially-designed attention mechanisms,
have been explored. Since tables usually appear and interact with free-form
text, table pre-training usually takes the form of table-text joint
pre-training, which attracts significant research interests from multiple
domains. This survey aims to provide a comprehensive review of different model
designs, pre-training objectives, and downstream tasks for table pre-training,
and we further share our thoughts and vision on existing challenges and future
opportunities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Semi-supervised Learning Approach with Two Teachers to Improve Breakdown Identification in Dialogues. (arXiv:2202.10948v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10948">
<div class="article-summary-box-inner">
<span><p>Identifying breakdowns in ongoing dialogues helps to improve communication
effectiveness. Most prior work on this topic relies on human annotated data and
data augmentation to learn a classification model. While quality labeled
dialogue data requires human annotation and is usually expensive to obtain,
unlabeled data is easier to collect from various sources. In this paper, we
propose a novel semi-supervised teacher-student learning framework to tackle
this task. We introduce two teachers which are trained on labeled data and
perturbed labeled data respectively. We leverage unlabeled data to improve
classification in student training where we employ two teachers to refine the
labeling of unlabeled data through teacher-student learning in a bootstrapping
manner. Through our proposed training approach, the student can achieve
improvements over single-teacher performance. Experimental results on the
Dialogue Breakdown Detection Challenge dataset DBDC5 and Learning to Identify
Follow-Up Questions dataset LIF show that our approach outperforms all previous
published approaches as well as other supervised and semi-supervised baseline
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05297">
<div class="article-summary-box-inner">
<span><p>Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11480">
<div class="article-summary-box-inner">
<span><p>Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships. (arXiv:2203.14260v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14260">
<div class="article-summary-box-inner">
<span><p>Understanding realistic visual scene images together with language
descriptions is a fundamental task towards generic visual understanding.
Previous works have shown compelling comprehensive results by building
hierarchical structures for visual scenes (e.g., scene graphs) and natural
languages (e.g., dependency trees), individually. However, how to construct a
joint vision-language (VL) structure has barely been investigated. More
challenging but worthwhile, we introduce a new task that targets on inducing
such a joint VL structure in an unsupervised manner. Our goal is to bridge the
visual scene graphs and linguistic dependency trees seamlessly. Due to the lack
of VL structural data, we start by building a new dataset VLParse. Rather than
using labor-intensive labeling from scratch, we propose an automatic alignment
procedure to produce coarse structures followed by human refinement to produce
high-quality ones. Moreover, we benchmark our dataset by proposing a
contrastive learning (CL)-based framework VLGAE, short for Vision-Language
Graph Autoencoder. Our model obtains superior performance on two derived tasks,
i.e., language grammar induction and VL phrase grounding. Ablations show the
effectiveness of both visual cues and dependency relationships on fine-grained
VL structure construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained Sequence-to-Tree Generation for Hierarchical Text Classification. (arXiv:2204.00811v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00811">
<div class="article-summary-box-inner">
<span><p>Hierarchical Text Classification (HTC) is a challenging task where a document
can be assigned to multiple hierarchically structured categories within a
taxonomy. The majority of prior studies consider HTC as a flat multi-label
classification problem, which inevitably leads to "label inconsistency"
problem. In this paper, we formulate HTC as a sequence generation task and
introduce a sequence-to-tree framework (Seq2Tree) for modeling the hierarchical
label structure. Moreover, we design a constrained decoding strategy with
dynamic vocabulary to secure the label consistency of the results. Compared
with previous works, the proposed approach achieves significant and consistent
improvements on three benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient comparison of sentence embeddings. (arXiv:2204.00820v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00820">
<div class="article-summary-box-inner">
<span><p>The domain of natural language processing (NLP), which has greatly evolved
over the last years, has highly benefited from the recent developments in word
and sentence embeddings. Such embeddings enable the transformation of complex
NLP tasks, like semantic similarity or Question and Answering (Q&amp;A), into much
simpler to perform vector comparisons. However, such a problem transformation
raises new challenges like the efficient comparison of embeddings and their
manipulation. In this work, we will discuss about various word and sentence
embeddings algorithms, we will select a sentence embedding algorithm, BERT, as
our algorithm of choice and we will evaluate the performance of two vector
comparison approaches, FAISS and Elasticsearch, in the specific problem of
sentence embeddings. According to the results, FAISS outperforms Elasticsearch
when used in a centralized environment with only one node, especially when big
datasets are included.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaLM: Scaling Language Modeling with Pathways. (arXiv:2204.02311v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02311">
<div class="article-summary-box-inner">
<span><p>Large language models have been shown to achieve remarkable performance
across a variety of natural language tasks using few-shot learning, which
drastically reduces the number of task-specific training examples needed to
adapt the model to a particular application. To further our understanding of
the impact of scale on few-shot learning, we trained a 540-billion parameter,
densely activated, Transformer language model, which we call Pathways Language
Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML
system which enables highly efficient training across multiple TPU Pods. We
demonstrate continued benefits of scaling by achieving state-of-the-art
few-shot learning results on hundreds of language understanding and generation
benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough
performance, outperforming the finetuned state-of-the-art on a suite of
multi-step reasoning tasks, and outperforming average human performance on the
recently released BIG-bench benchmark. A significant number of BIG-bench tasks
showed discontinuous improvements from model scale, meaning that performance
steeply increased as we scaled to our largest model. PaLM also has strong
capabilities in multilingual tasks and source code generation, which we
demonstrate on a wide array of benchmarks. We additionally provide a
comprehensive analysis on bias and toxicity, and study the extent of training
data memorization with respect to model scale. Finally, we discuss the ethical
considerations related to large language models and discuss potential
mitigation strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation. (arXiv:2204.02470v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02470">
<div class="article-summary-box-inner">
<span><p>Self-Supervised Learning (SSL) models have been successfully applied in
various deep learning-based speech tasks, particularly those with a limited
amount of data. However, the quality of SSL representations depends highly on
the relatedness between the SSL training domain(s) and the target data domain.
On the contrary, spectral feature (SF) extractors such as log Mel-filterbanks
are hand-crafted non-learnable components, and could be more robust to domain
shifts. The present work examines the assumption that combining non-learnable
SF extractors to SSL models is an effective approach to low resource speech
tasks. We propose a learnable and interpretable framework to combine SF and SSL
representations. The proposed framework outperforms significantly both baseline
and SSL models on Automatic Speech Recognition (ASR) and Speech Translation
(ST) tasks on three low resource datasets. We additionally design a mixture of
experts based combination model. This last model reveals that the relative
contribution of SSL models over conventional SF extractors is very small in
case of domain mismatch between SSL training set and the target language data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Base Index Compression via Dimensionality and Precision Reduction. (arXiv:2204.02906v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02906">
<div class="article-summary-box-inner">
<span><p>Recently neural network based approaches to knowledge-intensive NLP tasks,
such as question answering, started to rely heavily on the combination of
neural retrievers and readers. Retrieval is typically performed over a large
textual knowledge base (KB) which requires significant memory and compute
resources, especially when scaled up. On HotpotQA we systematically investigate
reducing the size of the KB index by means of dimensionality (sparse random
projections, PCA, autoencoders) and numerical precision reduction.
</p>
<p>Our results show that PCA is an easy solution that requires very little data
and is only slightly worse than autoencoders, which are less stable. All
methods are sensitive to pre- and post-processing and data should always be
centered and normalized both before and after dimension reduction. Finally, we
show that it is possible to combine PCA with using 1bit per dimension. Overall
we achieve (1) 100$\times$ compression with 75%, and (2) 24$\times$ compression
with 92% original retrieval performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detect Rumors in Microblog Posts for Low-Resource Domains via Adversarial Contrastive Learning. (arXiv:2204.08143v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08143">
<div class="article-summary-box-inner">
<span><p>Massive false rumors emerging along with breaking news or trending topics
severely hinder the truth. Existing rumor detection approaches achieve
promising performance on the yesterday's news, since there is enough corpus
collected from the same domain for model training. However, they are poor at
detecting rumors about unforeseen events especially those propagated in
different languages due to the lack of training data and prior knowledge (i.e.,
low-resource regimes). In this paper, we propose an adversarial contrastive
learning framework to detect rumors by adapting the features learned from
well-resourced rumor data to that of the low-resourced. Our model explicitly
overcomes the restriction of domain and/or language usage via language
alignment and a novel supervised contrastive training paradigm. Moreover, we
develop an adversarial augmentation mechanism to further enhance the robustness
of low-resource rumor representation. Extensive experiments conducted on two
low-resource datasets collected from real-world microblog platforms demonstrate
that our framework achieves much better performance than state-of-the-art
methods and exhibits a superior capacity for detecting rumors at early stages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. (arXiv:2204.08387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08387">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-training techniques have achieved remarkable progress in
Document AI. Most multimodal pre-trained models use a masked language modeling
objective to learn bidirectional representations on the text modality, but they
differ in pre-training objectives for the image modality. This discrepancy adds
difficulty to multimodal representation learning. In this paper, we propose
LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified
text and image masking. Additionally, LayoutLMv3 is pre-trained with a
word-patch alignment objective to learn cross-modal alignment by predicting
whether the corresponding image patch of a text word is masked. The simple
unified architecture and training objectives make LayoutLMv3 a general-purpose
pre-trained model for both text-centric and image-centric Document AI tasks.
Experimental results show that LayoutLMv3 achieves state-of-the-art performance
not only in text-centric tasks, including form understanding, receipt
understanding, and document visual question answering, but also in
image-centric tasks such as document image classification and document layout
analysis. The code and models are publicly available at
https://aka.ms/layoutlmv3.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">SuperpixelGridCut, SuperpixelGridMean and SuperpixelGridMix Data Augmentation. (arXiv:2204.08458v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08458">
<div class="article-summary-box-inner">
<span><p>A novel approach of data augmentation based on irregular superpixel
decomposition is proposed. This approach called SuperpixelGridMasks permits to
extend original image datasets that are required by training stages of machine
learning-related analysis architectures towards increasing their performances.
Three variants named SuperpixelGridCut, SuperpixelGridMean and
SuperpixelGridMix are presented. These grid-based methods produce a new style
of image transformations using the dropping and fusing of information.
Extensive experiments using various image classification models and datasets
show that baseline performances can be significantly outperformed using our
methods. The comparative study also shows that our methods can overpass the
performances of other data augmentations. Experimental results obtained over
image recognition datasets of varied natures show the efficiency of these new
methods. SuperpixelGridCut, SuperpixelGridMean and SuperpixelGridMix codes are
publicly available at https://github.com/hammoudiproject/SuperpixelGridMasks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Convolutional Networks for Action Recognition: Application to Sport Gesture Recognition. (arXiv:2204.08460v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08460">
<div class="article-summary-box-inner">
<span><p>3D convolutional networks is a good means to perform tasks such as video
segmentation into coherent spatio-temporal chunks and classification of them
with regard to a target taxonomy. In the chapter we are interested in the
classification of continuous video takes with repeatable actions, such as
strokes of table tennis. Filmed in a free marker less ecological environment,
these videos represent a challenge from both segmentation and classification
point of view. The 3D convnets are an efficient tool for solving these problems
with window-based approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Temporal Convolutional Neural Networks for Satellite Image Time Series Classification. (arXiv:2204.08461v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08461">
<div class="article-summary-box-inner">
<span><p>Satellite Image Time Series (SITS) of the Earth's surface provide detailed
land cover maps, with their quality in the spatial and temporal dimensions
consistently improving. These image time series are integral for developing
systems that aim to produce accurate, up-to-date land cover maps of the Earth's
surface. Applications are wide-ranging, with notable examples including
ecosystem mapping, vegetation process monitoring and anthropogenic land-use
change tracking. Recently proposed methods for SITS classification have
demonstrated respectable merit, but these methods tend to lack native
mechanisms that exploit the temporal dimension of the data; commonly resulting
in extensive data pre-processing prohibitively long training times. To overcome
these shortcomings, this paper seeks to study and enhance the newly proposed
method for SITS classification from literature; namely Temporal CNNs.
Comprehensive experiments are carried out on two benchmark SITS datasets with
the results demonstrating that Temporal CNNs display a superior or competitive
performance to the benchmark algorithms for both datasets. Investigations into
the Temporal CNNs architecture also highlighted the non-trivial task of
optimising the model for a new dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CapillaryX: A Software Design Pattern for Analyzing Medical Images in Real-time using Deep Learning. (arXiv:2204.08462v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08462">
<div class="article-summary-box-inner">
<span><p>Recent advances in digital imaging, e.g., increased number of pixels
captured, have meant that the volume of data to be processed and analyzed from
these images has also increased. Deep learning algorithms are state-of-the-art
for analyzing such images, given their high accuracy when trained with a large
data volume of data. Nevertheless, such analysis requires considerable
computational power, making such algorithms time- and resource-demanding. Such
high demands can be met by using third-party cloud service providers. However,
analyzing medical images using such services raises several legal and privacy
challenges and does not necessarily provide real-time results. This paper
provides a computing architecture that locally and in parallel can analyze
medical images in real-time using deep learning thus avoiding the legal and
privacy challenges stemming from uploading data to a third-party cloud
provider. To make local image processing efficient on modern multi-core
processors, we utilize parallel execution to offset the resource-intensive
demands of deep neural networks. We focus on a specific medical-industrial case
study, namely the quantifying of blood vessels in microcirculation images for
which we have developed a working system. It is currently used in an
industrial, clinical research setting as part of an e-health application. Our
results show that our system is approximately 78% faster than its serial system
counterpart and 12% faster than a master-slave parallel system architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning-Based Automated Thermal Comfort Prediction: Integration of Low-Cost Thermal and Visual Cameras for Higher Accuracy. (arXiv:2204.08463v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08463">
<div class="article-summary-box-inner">
<span><p>Recent research is trying to leverage occupants' demand in the building's
control loop to consider individuals' well-being and the buildings' energy
savings. To that end, a real-time feedback system is needed to provide data
about occupants' comfort conditions that can be used to control the building's
heating, cooling, and air conditioning (HVAC) system. The emergence of thermal
imaging techniques provides an excellent opportunity for contactless data
gathering with no interruption in occupant conditions and activities. There is
increasing attention to infrared thermal camera usage in public buildings
because of their non-invasive quality in reading the human skin temperature.
However, the state-of-the-art methods need additional modifications to become
more reliable. To capitalize potentials and address some existing limitations,
new solutions are required to bring a more holistic view toward non-intrusive
thermal scanning by leveraging the benefit of machine learning and image
processing. This research implements an automated approach to collect and
register simultaneous thermal and visual images and read the facial temperature
in different regions. This paper also presents two additional investigations.
First, through utilizing IButton wearable thermal sensors on the forehead area,
we investigate the reliability of an in-expensive thermal camera (FLIR Lepton)
in reading the skin temperature. Second, by studying the false-color version of
thermal images, we look into the possibility of non-radiometric thermal images
for predicting personalized thermal comfort. The results shows the strong
performance of Random Forest and K-Nearest Neighbor prediction algorithms in
predicting personalized thermal comfort. In addition, we have found that
non-radiometric images can also indicate thermal comfort when the algorithm is
trained with larger amounts of data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography. (arXiv:2204.08466v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08466">
<div class="article-summary-box-inner">
<span><p>Although robust PCA has been increasingly adopted to extract vessels from
X-ray coronary angiography (XCA) images, challenging problems such as
inefficient vessel-sparsity modelling, noisy and dynamic background artefacts,
and high computational cost still remain unsolved. Therefore, we propose a
novel robust PCA unrolling network with sparse feature selection for
super-resolution XCA vessel imaging. Being embedded within a patch-wise
spatiotemporal super-resolution framework that is built upon a pooling layer
and a convolutional long short-term memory network, the proposed network can
not only gradually prune complex vessel-like artefacts and noisy backgrounds in
XCA during network training but also iteratively learn and select the
high-level spatiotemporal semantic information of moving contrast agents
flowing in the XCA-imaged vessels. The experimental results show that the
proposed method significantly outperforms state-of-the-art methods, especially
in the imaging of the vessel network and its distal vessels, by restoring the
intensity and geometry profiles of heterogeneous vessels against complex and
dynamic backgrounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation. (arXiv:2204.08467v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08467">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) allows multiple medical institutions to
collaboratively learn a global model without centralizing all clients data. It
is difficult, if possible at all, for such a global model to commonly achieve
optimal performance for each individual client, due to the heterogeneity of
medical data from various scanners and patient demographics. This problem
becomes even more significant when deploying the global model to unseen clients
outside the FL with new distributions not presented during federated training.
To optimize the prediction accuracy of each individual client for critical
medical tasks, we propose a novel unified framework for both Inside and Outside
model Personalization in FL (IOP-FL). Our inside personalization is achieved by
a lightweight gradient-based approach that exploits the local adapted model for
each client, by accumulating both the global gradients for common knowledge and
local gradients for client-specific optimization. Moreover, and importantly,
the obtained local personalized models and the global model can form a diverse
and informative routing space to personalize a new model for outside FL
clients. Hence, we design a new test-time routing scheme inspired by the
consistency loss with a shape constraint to dynamically incorporate the models,
given the distribution information conveyed by the test data. Our extensive
experimental results on two medical image segmentation tasks present
significant improvements over SOTA methods on both inside and outside
personalization, demonstrating the great potential of our IOP-FL scheme for
clinical practice. Code will be released at https://github.com/med-air/IOP-FL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face recognition with small and large size databases. (arXiv:2204.08468v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08468">
<div class="article-summary-box-inner">
<span><p>This paper presents experimental results using the ORL (40 people) and FERET
(994 people) databases. The ORL database can be useful for securing
applications where few users attempting to access are expected. This is the
case, for instance, of a PDA or PC where the password is the face of the user.
On the other hand, the FERET database is useful for studying those situations
where the number of authorized users is around a thousand people.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hand Geometry Based Recognition with a MLP Classifier. (arXiv:2204.08469v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08469">
<div class="article-summary-box-inner">
<span><p>This paper presents a biometric recognition system based on hand geometry. We
describe a database specially collected for research purposes, which consists
of 50 people and 10 different acquisitions of the right hand. This database can
be freely downloaded. In addition, we describe a feature extraction procedure
and we obtain experimental results using different classification strategies
based on Multi Layer Perceptrons (MLP). We have evaluated identification rates
and Detection Cost Function (DCF) values for verification applications.
Experimental results reveal up to 100% identification and 0% DCF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">U-Net and its variants for Medical Image Segmentation : A short review. (arXiv:2204.08470v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08470">
<div class="article-summary-box-inner">
<span><p>The paper is a short review of medical image segmentation using U-Net and its
variants. As we understand going through a medical images is not an easy job
for any clinician either radiologist or pathologist. Analysing medical images
is the only way to perform non-invasive diagnosis. Segmenting out the regions
of interest has significant importance in medical images and is key for
diagnosis. This paper also gives a bird eye view of how medical image
segmentation has evolved. Also discusses challenge's and success of the deep
neural architectures. Following how different hybrid architectures have built
upon strong techniques from visual recognition tasks. In the end we will see
current challenges and future directions for medical image segmentation(MIS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simultaneous Multiple-Prompt Guided Generation Using Differentiable Optimal Transport. (arXiv:2204.08472v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08472">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning, such as powerful generative models and
joint text-image embeddings, have provided the computational creativity
community with new tools, opening new perspectives for artistic pursuits.
Text-to-image synthesis approaches that operate by generating images from text
cues provide a case in point. These images are generated with a latent vector
that is progressively refined to agree with text cues. To do so, patches are
sampled within the generated image, and compared with the text prompts in the
common text-image embedding space; The latent vector is then updated, using
gradient descent, to reduce the mean (average) distance between these patches
and text cues. While this approach provides artists with ample freedom to
customize the overall appearance of images, through their choice in generative
models, the reliance on a simple criterion (mean of distances) often causes
mode collapse: The entire image is drawn to the average of all text cues,
thereby losing their diversity. To address this issue, we propose using
matching techniques found in the optimal transport (OT) literature, resulting
in images that are able to reflect faithfully a wide diversity of prompts. We
provide numerous illustrations showing that OT avoids some of the pitfalls
arising from estimating vectors with mean distances, and demonstrate the
capacity of our proposed method to perform better in experiments, qualitatively
and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self Supervised Lesion Recognition For Breast Ultrasound Diagnosis. (arXiv:2204.08477v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08477">
<div class="article-summary-box-inner">
<span><p>Previous deep learning based Computer Aided Diagnosis (CAD) system treats
multiple views of the same lesion as independent images. Since an ultrasound
image only describes a partial 2D projection of a 3D lesion, such paradigm
ignores the semantic relationship between different views of a lesion, which is
inconsistent with the traditional diagnosis where sonographers analyze a lesion
from at least two views. In this paper, we propose a multi-task framework that
complements Benign/Malignant classification task with lesion recognition (LR)
which helps leveraging relationship among multiple views of a single lesion to
learn a complete representation of the lesion. To be specific, LR task employs
contrastive learning to encourage representation that pulls multiple views of
the same lesion and repels those of different lesions. The task therefore
facilitates a representation that is not only invariant to the view change of
the lesion, but also capturing fine-grained features to distinguish between
different lesions. Experiments show that the proposed multi-task framework
boosts the performance of Benign/Malignant classification as two sub-tasks
complement each other and enhance the learned representation of ultrasound
images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Non-mass Breast Ultrasound Cancer Classification With Knowledge Transfer. (arXiv:2204.08478v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08478">
<div class="article-summary-box-inner">
<span><p>Much progress has been made in the deep neural network (DNN) based diagnosis
of mass lesions breast ultrasound (BUS) images. However, the non-mass lesion is
less investigated because of the limited data. Based on the insight that mass
data is sufficient and shares the same knowledge structure with non-mass data
of identifying the malignancy of a lesion based on the ultrasound image, we
propose a novel transfer learning framework to enhance the generalizability of
the DNN model for non-mass BUS with the help of mass BUS. Specifically, we
train a shared DNN with combined non-mass and mass data. With the prior of
different marginal distributions in input and output space, we employ two
domain alignment strategies in the proposed transfer learning framework with
the insight of capturing domain-specific distribution to address the issue of
domain shift. Moreover, we propose a cross-domain semantic-preserve data
generation module called CrossMix to recover the missing distribution between
non-mass and mass data that is not presented in training data. Experimental
results on an in-house dataset demonstrate that the DNN model trained with
combined data by our framework achieves a 10% improvement in AUC on the
malignancy prediction task of non-mass BUS compared to training directly on
non-mass data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inductive Biases for Object-Centric Representations of Complex Textures. (arXiv:2204.08479v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08479">
<div class="article-summary-box-inner">
<span><p>Understanding which inductive biases could be useful for the unsupervised
learning of object-centric representations of natural scenes is challenging.
Here, we use neural style transfer to generate datasets where objects have
complex textures while still retaining ground-truth annotations. We find that,
when a model effectively balances the importance of shape and appearance in the
training objective, it can achieve better separation of the objects and learn
more useful object representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning Helps Pretrained Models Learn the Intended Task. (arXiv:2204.08491v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08491">
<div class="article-summary-box-inner">
<span><p>Models can fail in unpredictable ways during deployment due to task
ambiguity, when multiple behaviors are consistent with the provided training
data. An example is an object classifier trained on red squares and blue
circles: when encountering blue squares, the intended behavior is undefined. We
investigate whether pretrained models are better active learners, capable of
disambiguating between the possible tasks a user may be trying to specify.
Intriguingly, we find that better active learning is an emergent property of
the pretraining process: pretrained models require up to 5 times fewer labels
when using uncertainty-based active learning, while non-pretrained models see
no or even negative benefit. We find these gains come from an ability to select
examples with attributes that disambiguate the intended behavior, such as rare
product categories or atypical backgrounds. These attributes are far more
linearly separable in pretrained model's representation spaces vs
non-pretrained models, suggesting a possible mechanism for this behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning. (arXiv:2204.08499v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08499">
<div class="article-summary-box-inner">
<span><p>Coreset selection, which aims to select a subset of the most informative
training samples, is a long-standing learning problem that can benefit many
downstream tasks such as data-efficient learning, continual learning, neural
architecture search, active learning, etc. However, many existing coreset
selection methods are not designed for deep learning, which may have high
complexity and poor generalization ability to unseen representations. In
addition, the recently proposed methods are evaluated on models, datasets, and
settings of different complexities. To advance the research of coreset
selection in deep learning, we contribute a comprehensive code library, namely
DeepCore, and provide an empirical study on popular coreset selection methods
on CIFAR10 and ImageNet datasets. Extensive experiment results show that,
although some methods perform better in certain experiment settings, random
selection is still a strong baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spot the Difference: A Novel Task for Embodied Agents in Changing Environments. (arXiv:2204.08502v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08502">
<div class="article-summary-box-inner">
<span><p>Embodied AI is a recent research area that aims at creating intelligent
agents that can move and operate inside an environment. Existing approaches in
this field demand the agents to act in completely new and unexplored scenes.
However, this setting is far from realistic use cases that instead require
executing multiple tasks in the same environment. Even if the environment
changes over time, the agent could still count on its global knowledge about
the scene while trying to adapt its internal representation to the current
state of the environment. To make a step towards this setting, we propose Spot
the Difference: a novel task for Embodied AI where the agent has access to an
outdated map of the environment and needs to recover the correct layout in a
fixed time budget. To this end, we collect a new dataset of occupancy maps
starting from existing datasets of 3D spaces and generating a number of
possible layouts for a single environment. This dataset can be employed in the
popular Habitat simulator and is fully compliant with existing methods that
employ reconstructed occupancy maps during navigation. Furthermore, we propose
an exploration policy that can take advantage of previous knowledge of the
environment and identify changes in the scene faster and more effectively than
existing agents. Experimental results show that the proposed architecture
outperforms existing state-of-the-art models for exploration on this new
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dress Code: High-Resolution Multi-Category Virtual Try-On. (arXiv:2204.08532v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08532">
<div class="article-summary-box-inner">
<span><p>Image-based virtual try-on strives to transfer the appearance of a clothing
item onto the image of a target person. Prior work focuses mainly on upper-body
clothes (e.g. t-shirts, shirts, and tops) and neglects full-body or lower-body
items. This shortcoming arises from a main factor: current publicly available
datasets for image-based virtual try-on do not account for this variety, thus
limiting progress in the field. To address this deficiency, we introduce Dress
Code, which contains images of multi-category clothes. Dress Code is more than
3x larger than publicly available datasets for image-based virtual try-on and
features high-resolution paired images (1024 x 768) with front-view, full-body
reference models. To generate HD try-on images with high visual quality and
rich in details, we propose to learn fine-grained discriminating features.
Specifically, we leverage a semantic-aware discriminator that makes predictions
at pixel-level instead of image- or patch-level. Extensive experimental
evaluation demonstrates that the proposed approach surpasses the baselines and
state-of-the-art competitors in terms of visual quality and quantitative
results. The Dress Code dataset is publicly available at
https://github.com/aimagelab/dress-code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Region Duplication Detection Algorithm Based on Hybrid Approach. (arXiv:2204.08545v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08545">
<div class="article-summary-box-inner">
<span><p>The digital images from various sources are ubiquitous due to easy
availability of high bandwidth Internet. Digital images are easy to tamper with
good or bad intentions. Non-availability of pre-embedded information in digital
images makes the tampering detection process more difficult in case of digital
forensics. Thus, passive image tampering is difficult to detect. There are
various algorithms available for detecting image tampering. However, these
algorithms have some drawbacks, due to which all types of tampering cannot be
detected. In this paper researchers intend to present the types of image
tampering and its detection techniques with example based approach. This paper
also illustrates insights into the various existing algorithms and tries to
find out efficient algorithm out of them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cylin-Painting: Seamless 360{\deg} Panoramic Image Outpainting and Beyond with Cylinder-Style Convolutions. (arXiv:2204.08563v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08563">
<div class="article-summary-box-inner">
<span><p>Image outpainting gains increasing attention since it can generate the
complete scene from a partial view, providing a valuable solution to construct
360{\deg} panoramic images. As image outpainting suffers from the intrinsic
issue of unidirectional completion flow, previous methods convert the original
problem into inpainting, which allows a bidirectional flow. However, we find
that inpainting has its own limitations and is inferior to outpainting in
certain situations. The question of how they may be combined for the best of
both has as yet remained under-explored. In this paper, we provide a deep
analysis of the differences between inpainting and outpainting, which
essentially depends on how the source pixels contribute to the unknown regions
under different spatial arrangements. Motivated by this analysis, we present a
Cylin-Painting framework that involves meaningful collaborations between
inpainting and outpainting and efficiently fuses the different arrangements,
with a view to leveraging their complementary benefits on a consistent and
seamless cylinder. Nevertheless, directly applying the cylinder-style
convolution often generates visually unpleasing results as it could discard
important positional information. To address this issue, we further present a
learnable positional embedding strategy and incorporate the missing component
of positional encoding into the cylinder convolution, which significantly
improves the panoramic results. Note that while developed for image
outpainting, the proposed solution can be effectively extended to other
panoramic vision tasks, such as object detection, depth estimation, and image
super resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance. (arXiv:2204.08583v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08583">
<div class="article-summary-box-inner">
<span><p>Generating and editing images from open domain text prompts is a challenging
task that heretofore has required expensive and specially trained models. We
demonstrate a novel methodology for both tasks which is capable of producing
images of high visual quality from text prompts of significant semantic
complexity without any training by using a multimodal encoder to guide image
generations. We demonstrate on a variety of tasks how using CLIP [37] to guide
VQGAN [11] produces higher visual quality outputs than prior, less flexible
approaches like DALL-E [38], GLIDE [33] and Open-Edit [24], despite not being
trained for the tasks presented. Our code is available in a public repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Region-Based Deep Learning Approach to Automated Retail Checkout. (arXiv:2204.08584v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08584">
<div class="article-summary-box-inner">
<span><p>Automating the product checkout process at conventional retail stores is a
task poised to have large impacts on society generally speaking. Towards this
end, reliable deep learning models that enable automated product counting for
fast customer checkout can make this goal a reality. In this work, we propose a
novel, region-based deep learning approach to automate product counting using a
customized YOLOv5 object detection pipeline and the DeepSORT algorithm. Our
results on challenging, real-world test videos demonstrate that our method can
generalize its predictions to a sufficient level of accuracy and with a fast
enough runtime to warrant deployment to real-world commercial settings. Our
proposed method won 4th place in the 2022 AI City Challenge, Track 4, with an
F1 score of 0.4400 on experimental validation data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Tour of Visualization Techniques for Computer Vision Datasets. (arXiv:2204.08601v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08601">
<div class="article-summary-box-inner">
<span><p>We survey a number of data visualization techniques for analyzing Computer
Vision (CV) datasets. These techniques help us understand properties and latent
patterns in such data, by applying dataset-level analysis. We present various
examples of how such analysis helps predict the potential impact of the dataset
properties on CV models and informs appropriate mitigation of their
shortcomings. Finally, we explore avenues for further visualization techniques
of different modalities of CV datasets as well as ones that are tailored to
support specific CV tasks and analysis needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Data Augmentation for Deep Learning: A Survey. (arXiv:2204.08610v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08610">
<div class="article-summary-box-inner">
<span><p>Deep learning has achieved remarkable results in many computer vision tasks.
Deep neural networks typically rely on large amounts of training data to avoid
overfitting. However, labeled data for real-world applications may be limited.
By improving the quantity and diversity of training data, data augmentation has
become an inevitable part of deep learning model training with image data.
</p>
<p>As an effective way to improve the sufficiency and diversity of training
data, data augmentation has become a necessary part of successful application
of deep learning models on image data. In this paper, we systematically review
different image data augmentation methods. We propose a taxonomy of reviewed
methods and present the strengths and limitations of these methods. We also
conduct extensive experiments with various data augmentation methods on three
typical computer vision tasks, including semantic segmentation, image
classification and object detection. Finally, we discuss current challenges
faced by data augmentation and future research directions to put forward some
useful research guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors. (arXiv:2204.08612v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08612">
<div class="article-summary-box-inner">
<span><p>Deepfakes utilise Artificial Intelligence (AI) techniques to create synthetic
media where the likeness of one person is replaced with another. There are
growing concerns that deepfakes can be maliciously used to create misleading
and harmful digital contents. As deepfakes become more common, there is a dire
need for deepfake detection technology to help spot deepfake media. Present
deepfake detection models are able to achieve outstanding accuracy (&gt;90%).
However, most of them are limited to within-dataset scenario, where the same
dataset is used for training and testing. Most models do not generalise well
enough in cross-dataset scenario, where models are tested on unseen datasets
from another source. Furthermore, state-of-the-art deepfake detection models
rely on neural network-based classification models that are known to be
vulnerable to adversarial attacks. Motivated by the need for a robust deepfake
detection model, this study adapts metamorphic testing (MT) principles to help
identify potential factors that could influence the robustness of the examined
model, while overcoming the test oracle problem in this domain. Metamorphic
testing is specifically chosen as the testing technique as it fits our demand
to address learning-based system testing with probabilistic outcomes from
largely black-box components, based on potentially large input domains. We
performed our evaluations on MesoInception-4 and TwoStreamNet models, which are
the state-of-the-art deepfake detection models. This study identified makeup
application as an adversarial attack that could fool deepfake detectors. Our
experimental results demonstrate that both the MesoInception-4 and TwoStreamNet
models degrade in their performance by up to 30\% when the input data is
perturbed with makeup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Equivariant Learning for Oriented Keypoint Detection. (arXiv:2204.08613v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08613">
<div class="article-summary-box-inner">
<span><p>Detecting robust keypoints from an image is an integral part of many computer
vision problems, and the characteristic orientation and scale of keypoints play
an important role for keypoint description and matching. Existing
learning-based methods for keypoint detection rely on standard
translation-equivariant CNNs but often fail to detect reliable keypoints
against geometric variations. To learn to detect robust oriented keypoints, we
introduce a self-supervised learning framework using rotation-equivariant CNNs.
We propose a dense orientation alignment loss by an image pair generated by
synthetic transformations for training a histogram-based orientation map. Our
method outperforms the previous methods on an image matching benchmark and a
camera pose estimation benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CorrGAN: Input Transformation Technique Against Natural Corruptions. (arXiv:2204.08623v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08623">
<div class="article-summary-box-inner">
<span><p>Because of the increasing accuracy of Deep Neural Networks (DNNs) on
different tasks, a lot of real times systems are utilizing DNNs. These DNNs are
vulnerable to adversarial perturbations and corruptions. Specifically, natural
corruptions like fog, blur, contrast etc can affect the prediction of DNN in an
autonomous vehicle. In real time, these corruptions are needed to be detected
and also the corrupted inputs are needed to be de-noised to be predicted
correctly. In this work, we propose CorrGAN approach, which can generate benign
input when a corrupted input is provided. In this framework, we train
Generative Adversarial Network (GAN) with novel intermediate output-based loss
function. The GAN can denoise the corrupted input and generate benign input.
Through experimentation, we show that up to 75.2% of the corrupted
misclassified inputs can be classified correctly by DNN using CorrGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topology and geometry of data manifold in deep learning. (arXiv:2204.08624v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08624">
<div class="article-summary-box-inner">
<span><p>Despite significant advances in the field of deep learning in applications to
various fields, explaining the inner processes of deep learning models remains
an important and open question. The purpose of this article is to describe and
substantiate the geometric and topological view of the learning process of
neural networks. Our attention is focused on the internal representation of
neural networks and on the dynamics of changes in the topology and geometry of
the data manifold on different layers. We also propose a method for assessing
the generalizing ability of neural networks based on topological descriptors.
In this paper, we use the concepts of topological data analysis and intrinsic
dimension, and we present a wide range of experiments on different datasets and
different configurations of convolutional neural network architectures. In
addition, we consider the issue of the geometry of adversarial attacks in the
classification task and spoofing attacks on face recognition systems. Our work
is a contribution to the development of an important area of explainable and
interpretable AI through the example of computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quaternion Optimized Model with Sparse Regularization for Color Image Recovery. (arXiv:2204.08629v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08629">
<div class="article-summary-box-inner">
<span><p>This paper addresses the color image completion problem in accordance with
low-rank quatenrion matrix optimization that is characterized by sparse
regularization in a transformed domain. This research was inspired by an
appreciation of the fact that different signal types, including audio formats
and images, possess structures that are inherently sparse in respect of their
respective bases. Since color images can be processed as a whole in the
quaternion domain, we depicted the sparsity of the color image in the
quaternion discrete cosine transform (QDCT) domain. In addition, the
representation of a low-rank structure that is intrinsic to the color image is
a vital issue in the quaternion matrix completion problem. To achieve a more
superior low-rank approximation, the quatenrion-based truncated nuclear norm
(QTNN) is employed in the proposed model. Moreover, this model is facilitated
by a competent alternating direction method of multipliers (ADMM) based on the
algorithm. Extensive experimental results demonstrate that the proposed method
can yield vastly superior completion performance in comparison with the
state-of-the-art low-rank matrix/quaternion matrix approximation methods tested
on color image recovery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interaction-Aware Labeled Multi-Bernoulli Filter. (arXiv:2204.08655v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08655">
<div class="article-summary-box-inner">
<span><p>Tracking multiple objects through time is an important part of an intelligent
transportation system. Random finite set (RFS)-based filters are one of the
emerging techniques for tracking multiple objects. In multi-object tracking
(MOT), a common assumption is that each object is moving independent of its
surroundings. But in many real-world applications, target objects interact with
one another and the environment. Such interactions, when considered for
tracking, are usually modeled by an interactive motion model which is
application specific. In this paper, we present a novel approach to incorporate
target interactions within the prediction step of an RFS-based multi-target
filter, i.e. labeled multi-Bernoulli (LMB) filter. The method has been
developed for two practical applications of tracking a coordinated swarm and
vehicles. The method has been tested for a complex vehicle tracking dataset and
compared with the LMB filter through the OSPA and OSPA$^{(2)}$ metrics. The
results demonstrate that the proposed interaction-aware method depicts
considerable performance enhancement over the LMB filter in terms of the
selected metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ActAR: Actor-Driven Pose Embeddings for Video Action Recognition. (arXiv:2204.08671v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08671">
<div class="article-summary-box-inner">
<span><p>Human action recognition (HAR) in videos is one of the core tasks of video
understanding. Based on video sequences, the goal is to recognize actions
performed by humans. While HAR has received much attention in the visible
spectrum, action recognition in infrared videos is little studied. Accurate
recognition of human actions in the infrared domain is a highly challenging
task because of the redundant and indistinguishable texture features present in
the sequence. Furthermore, in some cases, challenges arise from the irrelevant
information induced by the presence of multiple active persons not contributing
to the actual action of interest. Therefore, most existing methods consider a
standard paradigm that does not take into account these challenges, which is in
some part due to the ambiguous definition of the recognition task in some
cases. In this paper, we propose a new method that simultaneously learns to
recognize efficiently human actions in the infrared spectrum, while
automatically identifying the key-actors performing the action without using
any prior knowledge or explicit annotations. Our method is composed of three
stages. In the first stage, optical flow-based key-actor identification is
performed. Then for each key-actor, we estimate key-poses that will guide the
frame selection process. A scale-invariant encoding process along with embedded
pose filtering are performed in order to enhance the quality of action
representations. Experimental results on InfAR dataset show that our proposed
model achieves promising recognition performance and learns useful action
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer. (arXiv:2204.08680v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08680">
<div class="article-summary-box-inner">
<span><p>Vision transformers have achieved great successes in many computer vision
tasks. Most methods generate vision tokens by splitting an image into a regular
and fixed grid and treating each cell as a token. However, not all regions are
equally important in human-centric vision tasks, e.g., the human body needs a
fine representation with many tokens, while the image background can be modeled
by a few tokens. To address this problem, we propose a novel Vision
Transformer, called Token Clustering Transformer (TCFormer), which merges
tokens by progressive clustering, where the tokens can be merged from different
locations with flexible shapes and sizes. The tokens in TCFormer can not only
focus on important areas but also adjust the token shapes to fit the semantic
concept and adopt a fine resolution for regions containing critical details,
which is beneficial to capturing detailed information. Extensive experiments
show that TCFormer consistently outperforms its counterparts on different
challenging human-centric tasks and datasets, including whole-body pose
estimation on COCO-WholeBody and 3D human mesh reconstruction on 3DPW. Code is
available at https://github.com/ zengwang430521/TCFormer.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Thin Format Vision-Based Tactile Sensor with A Micro Lens Array (MLA). (arXiv:2204.08691v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08691">
<div class="article-summary-box-inner">
<span><p>Vision-based tactile sensors have been widely studied in the robotics field
for high spatial resolution and compatibility with machine learning algorithms.
However, the currently employed sensor's imaging system is bulky limiting its
further application. Here we present a micro lens array (MLA) based vison
system to achieve a low thickness format of the sensor package with high
tactile sensing performance. Multiple micromachined micro lens units cover the
whole elastic touching layer and provide a stitched clear tactile image,
enabling high spatial resolution with a thin thickness of 5 mm. The thermal
reflow and soft lithography method ensure the uniform spherical profile and
smooth surface of micro lens. Both optical and mechanical characterization
demonstrated the sensor's stable imaging and excellent tactile sensing,
enabling precise 3D tactile information, such as displacement mapping and force
distribution with an ultra compact-thin structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTCNet: A CNN-Transformer Cooperation Network for Face Image Super-Resolution. (arXiv:2204.08696v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08696">
<div class="article-summary-box-inner">
<span><p>Recently, deep convolution neural networks (CNNs) steered face
super-resolution methods have achieved great progress in restoring degraded
facial details by jointly training with facial priors. However, these methods
have some obvious limitations. On the one hand, multi-task joint learning
requires additional marking on the dataset, and the introduced prior network
will significantly increase the computational cost of the model. On the other
hand, the limited receptive field of CNN will reduce the fidelity and
naturalness of the reconstructed facial images, resulting in suboptimal
reconstructed images. In this work, we propose an efficient CNN-Transformer
Cooperation Network (CTCNet) for face super-resolution tasks, which uses the
multi-scale connected encoder-decoder architecture as the backbone.
Specifically, we first devise a novel Local-Global Feature Cooperation Module
(LGCM), which is composed of a Facial Structure Attention Unit (FSAU) and a
Transformer block, to promote the consistency of local facial detail and global
facial structure restoration simultaneously. Then, we design an efficient Local
Feature Refinement Module (LFRM) to enhance the local facial structure
information. Finally, to further improve the restoration of fine facial
details, we present a Multi-scale Feature Fusion Unit (MFFU) to adaptively fuse
the features from different stages in the encoder procedure. Comprehensive
evaluations on various datasets have assessed that the proposed CTCNet can
outperform other state-of-the-art methods significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Software Engineering Approaches for TinyML based IoT Embedded Vision: A Systematic Literature Review. (arXiv:2204.08702v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08702">
<div class="article-summary-box-inner">
<span><p>Internet of Things (IoT) has catapulted human ability to control our
environments through ubiquitous sensing, communication, computation, and
actuation. Over the past few years, IoT has joined forces with Machine Learning
(ML) to embed deep intelligence at the far edge. TinyML (Tiny Machine Learning)
has enabled the deployment of ML models for embedded vision on extremely lean
edge hardware, bringing the power of IoT and ML together. However, TinyML
powered embedded vision applications are still in a nascent stage, and they are
just starting to scale to widespread real-world IoT deployment. To harness the
true potential of IoT and ML, it is necessary to provide product developers
with robust, easy-to-use software engineering (SE) frameworks and best
practices that are customized for the unique challenges faced in TinyML
engineering. Through this systematic literature review, we aggregated the key
challenges reported by TinyML developers and identified state-of-art SE
approaches in large-scale Computer Vision, Machine Learning, and Embedded
Systems that can help address key challenges in TinyML based IoT embedded
vision. In summary, our study draws synergies between SE expertise that
embedded systems developers and ML developers have independently developed to
help address the unique challenges in the engineering of TinyML based IoT
embedded vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Contrastive Hashing for Cross-Modal Retrieval in Remote Sensing. (arXiv:2204.08707v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08707">
<div class="article-summary-box-inner">
<span><p>The development of cross-modal retrieval systems that can search and retrieve
semantically relevant data across different modalities based on a query in any
modality has attracted great attention in remote sensing (RS). In this paper,
we focus our attention on cross-modal text-image retrieval, where queries from
one modality (e.g., text) can be matched to archive entries from another (e.g.,
image). Most of the existing cross-modal text-image retrieval systems in RS
require a high number of labeled training samples and also do not allow fast
and memory-efficient retrieval. These issues limit the applicability of the
existing cross-modal retrieval systems for large-scale applications in RS. To
address this problem, in this paper we introduce a novel unsupervised
cross-modal contrastive hashing (DUCH) method for text-image retrieval in RS.
To this end, the proposed DUCH is made up of two main modules: 1) feature
extraction module, which extracts deep representations of two modalities; 2)
hashing module that learns to generate cross-modal binary hash codes from the
extracted representations. We introduce a novel multi-objective loss function
including: i) contrastive objectives that enable similarity preservation in
intra- and inter-modal similarities; ii) an adversarial objective that is
enforced across two modalities for cross-modal representation consistency; and
iii) binarization objectives for generating hash codes. Experimental results
show that the proposed DUCH outperforms state-of-the-art methods. Our code is
publicly available at https://git.tu-berlin.de/rsim/duch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAFSSR: Stereo Image Super-Resolution Using NAFNet. (arXiv:2204.08714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08714">
<div class="article-summary-box-inner">
<span><p>Stereo image super-resolution aims at enhancing the quality of
super-resolution results by utilizing the complementary information provided by
binocular systems. To obtain reasonable performance, most methods focus on
finely designing modules, loss functions, and etc. to exploit information from
another viewpoint. This has the side effect of increasing system complexity,
making it difficult for researchers to evaluate new ideas and compare methods.
This paper inherits a strong and simple image restoration model, NAFNet, for
single-view feature extraction and extends it by adding cross attention modules
to fuse features between views to adapt to binocular scenarios. The proposed
baseline for stereo image super-resolution is noted as NAFSSR. Furthermore,
training/testing strategies are proposed to fully exploit the performance of
NAFSSR. Extensive experiments demonstrate the effectiveness of our method. In
particular, NAFSSR outperforms the state-of-the-art methods on the KITTI 2012,
KITTI 2015, Middlebury, and Flickr1024 datasets. With NAFSSR, we won 1st place
in the NTIRE 2022 Stereo Image Super-resolution Challenge. Codes and models
will be released at https://github.com/megvii-research/NAFNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape-Aware Monocular 3D Object Detection. (arXiv:2204.08717v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08717">
<div class="article-summary-box-inner">
<span><p>The detection of 3D objects through a single perspective camera is a
challenging issue. The anchor-free and keypoint-based models receive increasing
attention recently due to their effectiveness and simplicity. However, most of
these methods are vulnerable to occluded and truncated objects. In this paper,
a single-stage monocular 3D object detection model is proposed. An
instance-segmentation head is integrated into the model training, which allows
the model to be aware of the visible shape of a target object. The detection
largely avoids interference from irrelevant regions surrounding the target
objects. In addition, we also reveal that the popular IoU-based evaluation
metrics, which were originally designed for evaluating stereo or LiDAR-based
detection methods, are insensitive to the improvement of monocular 3D object
detection algorithms. A novel evaluation metric, namely average depth
similarity (ADS) is proposed for the monocular 3D object detection models. Our
method outperforms the baseline on both the popular and the proposed evaluation
metrics while maintaining real-time efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Token Fusion for Vision Transformers. (arXiv:2204.08721v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08721">
<div class="article-summary-box-inner">
<span><p>Many adaptations of transformers have emerged to address the single-modal
vision tasks, where self-attention modules are stacked to handle input sources
like images. Intuitively, feeding multiple modalities of data to vision
transformers could improve the performance, yet the inner-modal attentive
weights may also be diluted, which could thus undermine the final performance.
In this paper, we propose a multimodal token fusion method (TokenFusion),
tailored for transformer-based vision tasks. To effectively fuse multiple
modalities, TokenFusion dynamically detects uninformative tokens and
substitutes these tokens with projected and aggregated inter-modal features.
Residual positional alignment is also adopted to enable explicit utilization of
the inter-modal alignments after fusion. The design of TokenFusion allows the
transformer to learn correlations among multimodal features, while the
single-modal transformer architecture remains largely intact. Extensive
experiments are conducted on a variety of homogeneous and heterogeneous
modalities and demonstrate that TokenFusion surpasses state-of-the-art methods
in three typical vision tasks: multimodal image-to-image translation, RGB-depth
semantic segmentation, and 3D object detection with point cloud and images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jacobian Ensembles Improve Robustness Trade-offs to Adversarial Attacks. (arXiv:2204.08726v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08726">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have become an integral part of our software
infrastructure and are being deployed in many widely-used and safety-critical
applications. However, their integration into many systems also brings with it
the vulnerability to test time attacks in the form of Universal Adversarial
Perturbations (UAPs). UAPs are a class of perturbations that when applied to
any input causes model misclassification. Although there is an ongoing effort
to defend models against these adversarial attacks, it is often difficult to
reconcile the trade-offs in model accuracy and robustness to adversarial
attacks. Jacobian regularization has been shown to improve the robustness of
models against UAPs, whilst model ensembles have been widely adopted to improve
both predictive performance and model robustness. In this work, we propose a
novel approach, Jacobian Ensembles-a combination of Jacobian regularization and
model ensembles to significantly increase the robustness against UAPs whilst
maintaining or improving model accuracy. Our results show that Jacobian
Ensembles achieves previously unseen levels of accuracy and robustness, greatly
improving over previous methods that tend to skew towards only either accuracy
or robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proposal-free Lidar Panoptic Segmentation with Pillar-level Affinity. (arXiv:2204.08744v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08744">
<div class="article-summary-box-inner">
<span><p>We propose a simple yet effective proposal-free architecture for lidar
panoptic segmentation. We jointly optimize both semantic segmentation and
class-agnostic instance classification in a single network using a pillar-based
bird's-eye view representation. The instance classification head learns
pairwise affinity between pillars to determine whether the pillars belong to
the same instance or not. We further propose a local clustering algorithm to
propagate instance ids by merging semantic segmentation and affinity
predictions. Our experiments on nuScenes dataset show that our approach
outperforms previous proposal-free methods and is comparable to proposal-based
methods which requires extra annotation from object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmentation of Atmospheric Turbulence Effects on Thermal Adapted Object Detection Models. (arXiv:2204.08745v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08745">
<div class="article-summary-box-inner">
<span><p>Atmospheric turbulence has a degrading effect on the image quality of
long-range observation systems. As a result of various elements such as
temperature, wind velocity, humidity, etc., turbulence is characterized by
random fluctuations in the refractive index of the atmosphere. It is a
phenomenon that may occur in various imaging spectra such as the visible or the
infrared bands. In this paper, we analyze the effects of atmospheric turbulence
on object detection performance in thermal imagery. We use a geometric
turbulence model to simulate turbulence effects on a medium-scale thermal image
set, namely "FLIR ADAS v2". We apply thermal domain adaptation to
state-of-the-art object detectors and propose a data augmentation strategy to
increase the performance of object detectors which utilizes turbulent images in
different severity levels as training data. Our results show that the proposed
data augmentation strategy yields an increase in performance for both turbulent
and non-turbulent thermal test images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Spatial-Temporal Network for Continuous Sign Language Recognition. (arXiv:2204.08747v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08747">
<div class="article-summary-box-inner">
<span><p>Sign language is a beautiful visual language and is also the primary language
used by speaking and hearing-impaired people. However, sign language has many
complex expressions, which are difficult for the public to understand and
master. Sign language recognition algorithms will significantly facilitate
communication between hearing-impaired people and normal people. Traditional
continuous sign language recognition often uses a sequence learning method
based on Convolutional Neural Network (CNN) and Long Short-Term Memory Network
(LSTM). These methods can only learn spatial and temporal features separately,
which cannot learn the complex spatial-temporal features of sign language. LSTM
is also difficult to learn long-term dependencies. To alleviate these problems,
this paper proposes a multi-view spatial-temporal continuous sign language
recognition network. The network consists of three parts. The first part is a
Multi-View Spatial-Temporal Feature Extractor Network (MSTN), which can
directly extract the spatial-temporal features of RGB and skeleton data; the
second is a sign language encoder network based on Transformer, which can learn
long-term dependencies; the third is a Connectionist Temporal Classification
(CTC) decoder network, which is used to predict the whole meaning of the
continuous sign language. Our algorithm is tested on two public sign language
datasets SLR-100 and PHOENIX-Weather 2014T (RWTH). As a result, our method
achieves excellent performance on both datasets. The word error rate on the
SLR-100 dataset is 1.9%, and the word error rate on the RWTHPHOENIX-Weather
dataset is 22.8%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Point Cloud Denoising via Gradient Fields. (arXiv:2204.08755v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08755">
<div class="article-summary-box-inner">
<span><p>3D dynamic point clouds provide a discrete representation of real-world
objects or scenes in motion, which have been widely applied in immersive
telepresence, autonomous driving, surveillance, etc. However, point clouds
acquired from sensors are usually perturbed by noise, which affects downstream
tasks such as surface reconstruction and analysis. Although many efforts have
been made for static point cloud denoising, dynamic point cloud denoising
remains under-explored. In this paper, we propose a novel gradient-field-based
dynamic point cloud denoising method, exploiting the temporal correspondence
via the estimation of gradient fields -- a fundamental problem in dynamic point
cloud processing and analysis. The gradient field is the gradient of the
log-probability function of the noisy point cloud, based on which we perform
gradient ascent so as to converge each point to the underlying clean surface.
We estimate the gradient of each surface patch and exploit the temporal
correspondence, where the temporally corresponding patches are searched
leveraging on rigid motion in classical mechanics. In particular, we treat each
patch as a rigid object, which moves in the gradient field of an adjacent frame
via force until reaching a balanced state, i.e., when the sum of gradients over
the patch reaches 0. Since the gradient would be smaller when the point is
closer to the underlying surface, the balanced patch would fit the underlying
surface well, thus leading to the temporal correspondence. Finally, the
position of each point in the patch is updated along the direction of the
gradient averaged from corresponding patches in adjacent frames. Experimental
results demonstrate that the proposed model outperforms state-of-the-art
methods under both synthetic noise and simulated real-world noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edge-enhanced Feature Distillation Network for Efficient Super-Resolution. (arXiv:2204.08759v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08759">
<div class="article-summary-box-inner">
<span><p>With the recently massive development in convolution neural networks,
numerous lightweight CNN-based image super-resolution methods have been
proposed for practical deployments on edge devices. However, most existing
methods focus on one specific aspect: network or loss design, which leads to
the difficulty of minimizing the model size. To address the issue, we conclude
block devising, architecture searching, and loss design to obtain a more
efficient SR structure. In this paper, we proposed an edge-enhanced feature
distillation network, named EFDN, to preserve the high-frequency information
under constrained resources. In detail, we build an edge-enhanced convolution
block based on the existing reparameterization methods. Meanwhile, we propose
edge-enhanced gradient loss to calibrate the reparameterized path training.
Experimental results show that our edge-enhanced strategies preserve the edge
and significantly improve the final restoration quality. Code is available at
https://github.com/icandle/EFDN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Semi-Supervised and Positive-Unlabeled Learning for Boosting Full Reference Image Quality Assessment. (arXiv:2204.08763v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08763">
<div class="article-summary-box-inner">
<span><p>Full-reference (FR) image quality assessment (IQA) evaluates the visual
quality of a distorted image by measuring its perceptual difference with
pristine-quality reference, and has been widely used in low-level vision tasks.
Pairwise labeled data with mean opinion score (MOS) are required in training
FR-IQA model, but is time-consuming and cumbersome to collect. In contrast,
unlabeled data can be easily collected from an image degradation or restoration
process, making it encouraging to exploit unlabeled training data to boost
FR-IQA performance. Moreover, due to the distribution inconsistency between
labeled and unlabeled data, outliers may occur in unlabeled data, further
increasing the training difficulty. In this paper, we suggest to incorporate
semi-supervised and positive-unlabeled (PU) learning for exploiting unlabeled
data while mitigating the adverse effect of outliers. Particularly, by treating
all labeled data as positive samples, PU learning is leveraged to identify
negative samples (i.e., outliers) from unlabeled data. Semi-supervised learning
(SSL) is further deployed to exploit positive unlabeled data by dynamically
generating pseudo-MOS. We adopt a dual-branch network including reference and
distortion branches. Furthermore, spatial attention is introduced in the
reference branch to concentrate more on the informative regions, and sliced
Wasserstein distance is used for robust difference map computation to address
the misalignment issues caused by images recovered by GAN models. Extensive
experiments show that our method performs favorably against state-of-the-arts
on the benchmark datasets PIPAL, KADID-10k, TID2013, LIVE and CSIQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Missing Annotations for Incremental Learning in Object Detection. (arXiv:2204.08766v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08766">
<div class="article-summary-box-inner">
<span><p>Despite the recent advances in the field of object detection, common
architectures are still ill-suited to incrementally detect new categories over
time. They are vulnerable to catastrophic forgetting: they forget what has been
already learned while updating their parameters in absence of the original
training data. Previous works extended standard classification methods in the
object detection task, mainly adopting the knowledge distillation framework.
However, we argue that object detection introduces an additional problem, which
has been overlooked. While objects belonging to new classes are learned thanks
to their annotations, if no supervision is provided for other objects that may
still be present in the input, the model learns to associate them to background
regions. We propose to handle these missing annotations by revisiting the
standard knowledge distillation framework. Our approach outperforms current
state-of-the-art methods in every setting of the Pascal-VOC dataset. We further
propose an extension to instance segmentation, outperforming the other
baselines. In this work, we propose to handle the missing annotations by
revisiting the standard knowledge distillation framework. We show that our
approach outperforms current state-of-the-art methods in every setting of the
Pascal-VOC 2007 dataset. Moreover, we propose a simple extension to instance
segmentation, showing that it outperforms the other baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Binary Multi Channel Morphological Neural Network. (arXiv:2204.08768v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08768">
<div class="article-summary-box-inner">
<span><p>Neural networks and particularly Deep learning have been comparatively little
studied from the theoretical point of view. Conversely, Mathematical Morphology
is a discipline with solid theoretical foundations. We combine these domains to
propose a new type of neural architecture that is theoretically more
explainable. We introduce a Binary Morphological Neural Network (BiMoNN) built
upon the convolutional neural network. We design it for learning morphological
networks with binary inputs and outputs. We demonstrate an equivalence between
BiMoNNs and morphological operators that we can use to binarize entire
networks. These can learn classical morphological operators and show promising
results on a medical imaging application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning. (arXiv:2204.08770v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08770">
<div class="article-summary-box-inner">
<span><p>Demystifying the interactions among multiple agents from their past
trajectories is fundamental to precise and interpretable trajectory prediction.
However, previous works only consider pair-wise interactions with limited
relational reasoning. To promote more comprehensive interaction modeling for
relational reasoning, we propose GroupNet, a multiscale hypergraph neural
network, which is novel in terms of both interaction capturing and
representation learning. From the aspect of interaction capturing, we propose a
trainable multiscale hypergraph to capture both pair-wise and group-wise
interactions at multiple group sizes. From the aspect of interaction
representation learning, we propose a three-element format that can be learnt
end-to-end and explicitly reason some relational factors including the
interaction strength and category. We apply GroupNet into both CVAE-based
prediction system and previous state-of-the-art prediction systems for
predicting socially plausible trajectories with relational reasoning. To
validate the ability of relational reasoning, we experiment with synthetic
physics simulations to reflect the ability to capture group behaviors, reason
interaction strength and interaction category. To validate the effectiveness of
prediction, we conduct extensive experiments on three real-world trajectory
prediction datasets, including NBA, SDD and ETH-UCY; and we show that with
GroupNet, the CVAE-based prediction system outperforms state-of-the-art
methods. We also show that adding GroupNet will further improve the performance
of previous state-of-the-art prediction systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensor Data Fusion in Top-View Grid Maps using Evidential Reasoning with Advanced Conflict Resolution. (arXiv:2204.08780v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08780">
<div class="article-summary-box-inner">
<span><p>We present a new method to combine evidential top-view grid maps estimated
based on heterogeneous sensor sources. Dempster's combination rule that is
usually applied in this context provides undesired results with highly
conflicting inputs. Therefore, we use more advanced evidential reasoning
techniques and improve the conflict resolution by modeling the reliability of
the evidence sources. We propose a data-driven reliability estimation to
optimize the fusion quality using the Kitti-360 dataset. We apply the proposed
method to the fusion of LiDAR and stereo camera data and evaluate the results
qualitatively and quantitatively. The results demonstrate that our proposed
method robustly combines measurements from heterogeneous sensors and
successfully resolves sensor conflicts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08790">
<div class="article-summary-box-inner">
<span><p>Learning visual representations from natural language supervision has
recently shown great promise in a number of pioneering works. In general, these
language-augmented visual models demonstrate strong transferability to a
variety of datasets/tasks. However, it remains a challenge to evaluate the
transferablity of these foundation models due to the lack of easy-to-use
toolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation
of Language-augmented Visual Task-level Transfer), the first benchmark to
compare and evaluate pre-trained language-augmented visual models. Several
highlights include: (i) Datasets. As downstream evaluation suites, it consists
of 20 image classification datasets and 35 object detection datasets, each of
which is augmented with external knowledge. (ii) Toolkit. An automatic
hyper-parameter tuning toolkit is developed to ensure the fairness in model
adaption. To leverage the full power of language-augmented visual models, novel
language-aware initialization methods are proposed to significantly improve the
adaption performance. (iii) Metrics. A variety of evaluation metrics are used,
including sample-efficiency (zero-shot and few-shot) and parameter-efficiency
(linear probing and full model fine-tuning). We will release our toolkit and
evaluation platforms for the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A qualitative investigation of optical flow algorithms for video denoising. (arXiv:2204.08791v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08791">
<div class="article-summary-box-inner">
<span><p>A good optical flow estimation is crucial in many video analysis and
restoration algorithms employed in application fields like media industry,
industrial inspection and automotive. In this work, we investigate how well
optical flow algorithms perform qualitatively when integrated into a state of
the art video denoising algorithm. Both classic optical flow algorithms (e.g.
TV-L1) as well as recent deep learning based algorithm (like RAFT or BMBC) will
be taken into account. For the qualitative investigation, we will employ
realistic content with challenging characteristic (noisy content, large motion
etc.) instead of the standard images used in most publications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Stream Graph Convolutional Network for Intra-oral Scanner Image Segmentation. (arXiv:2204.08797v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08797">
<div class="article-summary-box-inner">
<span><p>Precise segmentation of teeth from intra-oral scanner images is an essential
task in computer-aided orthodontic surgical planning. The state-of-the-art deep
learning-based methods often simply concatenate the raw geometric attributes
(i.e., coordinates and normal vectors) of mesh cells to train a single-stream
network for automatic intra-oral scanner image segmentation. However, since
different raw attributes reveal completely different geometric information, the
naive concatenation of different raw attributes at the (low-level) input stage
may bring unnecessary confusion in describing and differentiating between mesh
cells, thus hampering the learning of high-level geometric representations for
the segmentation task. To address this issue, we design a two-stream graph
convolutional network (i.e., TSGCN), which can effectively handle inter-view
confusion between different raw attributes to more effectively fuse their
complementary information and learn discriminative multi-view geometric
representations. Specifically, our TSGCN adopts two input-specific
graph-learning streams to extract complementary high-level geometric
representations from coordinates and normal vectors, respectively. Then, these
single-view representations are further fused by a self-attention module to
adaptively balance the contributions of different views in learning more
discriminative multi-view representations for accurate and fully automatic
tooth segmentation. We have evaluated our TSGCN on a real-patient dataset of
dental (mesh) models acquired by 3D intraoral scanners. Experimental results
show that our TSGCN significantly outperforms state-of-the-art methods in 3D
tooth (surface) segmentation. Github:
https://github.com/ZhangLingMing1/TSGCNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Energy-Based Prior for Generative Saliency. (arXiv:2204.08803v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08803">
<div class="article-summary-box-inner">
<span><p>We propose a novel energy-based prior for generative saliency prediction,
where the latent variables follow an informative energy-based prior. Both the
saliency generator and the energy-based prior are jointly trained via Markov
chain Monte Carlo-based maximum likelihood estimation, in which the sampling
from the intractable posterior and prior distributions of the latent variables
are performed by Langevin dynamics. With the generative saliency model, we can
obtain a pixel-wise uncertainty map from an image, indicating model confidence
in the saliency prediction. Different from existing generative models, which
define the prior distribution of the latent variable as a simple isotropic
Gaussian distribution, our model uses an energy-based informative prior which
can be more expressive in capturing the latent space of the data. With the
informative energy-based prior, we extend the Gaussian distribution assumption
of generative models to achieve a more representative distribution of the
latent space, leading to more reliable uncertainty estimation. We apply the
proposed frameworks to both RGB and RGB-D salient object detection tasks with
both transformer and convolutional neural network backbones. Experimental
results show that our generative saliency model with an energy-based prior can
achieve not only accurate saliency predictions but also reliable uncertainty
maps that are consistent with human perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic Segmentation. (arXiv:2204.08808v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08808">
<div class="article-summary-box-inner">
<span><p>Domain adaptive semantic segmentation attempts to make satisfactory dense
predictions on an unlabeled target domain by utilizing the model trained on a
labeled source domain. One solution is self-training, which retrains models
with target pseudo labels. Many methods tend to alleviate noisy pseudo labels,
however, they ignore intrinsic connections among cross-domain pixels with
similar semantic concepts. Thus, they would struggle to deal with the semantic
variations across domains, leading to less discrimination and poor
generalization. In this work, we propose Semantic-Guided Pixel Contrast
(SePiCo), a novel one-stage adaptation framework that highlights the semantic
concepts of individual pixel to promote learning of class-discriminative and
class-balanced pixel embedding space across domains. Specifically, to explore
proper semantic concepts, we first investigate a centroid-aware pixel contrast
that employs the category centroids of the entire source domain or a single
source image to guide the learning of discriminative features. Considering the
possible lack of category diversity in semantic concepts, we then blaze a trail
of distributional perspective to involve a sufficient quantity of instances,
namely distribution-aware pixel contrast, in which we approximate the true
distribution of each semantic category from the statistics of labeled source
data. Moreover, such an optimization objective can derive a closed-form upper
bound by implicitly involving an infinite number of (dis)similar pairs.
Extensive experiments show that SePiCo not only helps stabilize training but
also yields discriminative features, making significant progress in both
daytime and nighttime scenarios. Most notably, SePiCo establishes excellent
results on tasks of GTAV/SYNTHIA-to-Cityscapes and Cityscapes-to-Dark Zurich,
improving by 12.8, 8.8, and 9.2 mIoUs compared to the previous best method,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UID2021: An Underwater Image Dataset for Evaluation of No-reference Quality Assessment Metrics. (arXiv:2204.08813v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08813">
<div class="article-summary-box-inner">
<span><p>Achieving subjective and objective quality assessment of underwater images is
of high significance in underwater visual perception and image/video
processing. However, the development of underwater image quality assessment
(UIQA) is limited for the lack of comprehensive human subjective user study
with publicly available dataset and reliable objective UIQA metric. To address
this issue, we establish a large-scale underwater image dataset, dubbed
UID2021, for evaluating no-reference UIQA metrics. The constructed dataset
contains 60 multiply degraded underwater images collected from various sources,
covering six common underwater scenes (i.e. bluish scene, bluish-green scene,
greenish scene, hazy scene, low-light scene, and turbid scene), and their
corresponding 900 quality improved versions generated by employing fifteen
state-of-the-art underwater image enhancement and restoration algorithms. Mean
opinion scores (MOS) for UID2021 are also obtained by using the pair comparison
sorting method with 52 observers. Both in-air NR-IQA and underwater-specific
algorithms are tested on our constructed dataset to fairly compare the
performance and analyze their strengths and weaknesses. Our proposed UID2021
dataset enables ones to evaluate NR UIQA algorithms comprehensively and paves
the way for further research on UIQA. Our UID2021 will be a free download and
utilized for research purposes at: https://github.com/Hou-Guojia/UID2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Domain-Incremental Learning Approach to Drive in All Weather Conditions. (arXiv:2204.08817v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08817">
<div class="article-summary-box-inner">
<span><p>Although deep neural networks enable impressive visual perception performance
for autonomous driving, their robustness to varying weather conditions still
requires attention. When adapting these models for changed environments, such
as different weather conditions, they are prone to forgetting previously
learned information. This catastrophic forgetting is typically addressed via
incremental learning approaches which usually re-train the model by either
keeping a memory bank of training samples or keeping a copy of the entire model
or model parameters for each scenario. While these approaches show impressive
results, they can be prone to scalability issues and their applicability for
autonomous driving in all weather conditions has not been shown. In this paper
we propose DISC -- Domain Incremental through Statistical Correction -- a
simple online zero-forgetting approach which can incrementally learn new tasks
(i.e weather conditions) without requiring re-training or expensive memory
banks. The only information we store for each task are the statistical
parameters as we categorize each domain by the change in first and second order
statistics. Thus, as each task arrives, we simply 'plug and play' the
statistical vectors for the corresponding task into the model and it
immediately starts to perform well on that task. We show the efficacy of our
approach by testing it for object detection in a challenging domain-incremental
autonomous driving scenario where we encounter different adverse weather
conditions, such as heavy rain, fog, and snow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised 3D shape segmentation with multilevel consistency and part substitution. (arXiv:2204.08824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08824">
<div class="article-summary-box-inner">
<span><p>The lack of fine-grained 3D shape segmentation data is the main obstacle to
developing learning-based 3D segmentation techniques. We propose an effective
semi-supervised method for learning 3D segmentations from a few labeled 3D
shapes and a large amount of unlabeled 3D data. For the unlabeled data, we
present a novel \emph{multilevel consistency} loss to enforce consistency of
network predictions between perturbed copies of a 3D shape at multiple levels:
point-level, part-level, and hierarchical level. For the labeled data, we
develop a simple yet effective part substitution scheme to augment the labeled
3D shapes with more structural variations to enhance training. Our method has
been extensively validated on the task of 3D object semantic segmentation on
PartNet and ShapeNetPart, and indoor scene semantic segmentation on ScanNet. It
exhibits superior performance to existing semi-supervised and unsupervised
pre-training 3D approaches. Our code and trained models are publicly available
at \url{https://github.com/isunchy/semi_supervised_3d_segmentation}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detect-and-describe: Joint learning framework for detection and description of objects. (arXiv:2204.08828v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08828">
<div class="article-summary-box-inner">
<span><p>Traditional object detection answers two questions; "what" (what the object
is?) and "where" (where the object is?). "what" part of the object detection
can be fine-grained further i.e. "what type", "what shape" and "what material"
etc. This results in the shifting of the object detection tasks to the object
description paradigm. Describing an object provides additional detail that
enables us to understand the characteristics and attributes of the object
("plastic boat" not just boat, "glass bottle" not just bottle). This additional
information can implicitly be used to gain insight into unseen objects (e.g.
unknown object is "metallic", "has wheels"), which is not possible in
traditional object detection. In this paper, we present a new approach to
simultaneously detect objects and infer their attributes, we call it Detect and
Describe (DaD) framework. DaD is a deep learning-based approach that extends
object detection to object attribute prediction as well. We train our model on
aPascal train set and evaluate our approach on aPascal test set. We achieve
97.0% in Area Under the Receiver Operating Characteristic Curve (AUC) for
object attributes prediction on aPascal test set. We also show qualitative
results for object attribute prediction on unseen objects, which demonstrate
the effectiveness of our approach for describing unknown objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations. (arXiv:2204.08839v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08839">
<div class="article-summary-box-inner">
<span><p>We propose an unsupervised method for 3D geometry-aware representation
learning of articulated objects. Though photorealistic images of articulated
objects can be rendered with explicit pose control through existing 3D neural
representations, these methods require ground truth 3D pose and foreground
masks for training, which are expensive to obtain. We obviate this need by
learning the representations with GAN training. From random poses and latent
vectors, the generator is trained to produce realistic images of articulated
objects by adversarial training. To avoid a large computational cost for GAN
training, we propose an efficient neural representation for articulated objects
based on tri-planes and then present a GAN-based framework for its unsupervised
training. Experiments demonstrate the efficiency of our method and show that
GAN-based training enables learning of controllable 3D representations without
supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Core Box Image Recognition and its Improvement with a New Augmentation Technique. (arXiv:2204.08853v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08853">
<div class="article-summary-box-inner">
<span><p>Most methods for automated full-bore rock core image analysis (description,
colour, properties distribution, etc.) are based on separate core column
analyses. The core is usually imaged in a box because of the significant amount
of time taken to get an image for each core column. The work presents an
innovative method and algorithm for core columns extraction from core boxes.
The conditions for core boxes imaging may differ tremendously. Such differences
are disastrous for machine learning algorithms which need a large dataset
describing all possible data variations. Still, such images have some standard
features - a box and core. Thus, we can emulate different environments with a
unique augmentation described in this work. It is called template-like
augmentation (TLA). The method is described and tested on various environments,
and results are compared on an algorithm trained on both 'traditional' data and
a mix of traditional and TLA data. The algorithm trained with TLA data provides
better metrics and can detect core on most new images, unlike the algorithm
trained on data without TLA. The algorithm for core column extraction
implemented in an automated core description system speeds up the core box
processing by a factor of 20.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenGlue: Open Source Graph Neural Net Based Pipeline for Image Matching. (arXiv:2204.08870v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08870">
<div class="article-summary-box-inner">
<span><p>We present OpenGlue: a free open-source framework for image matching, that
uses a Graph Neural Network-based matcher inspired by SuperGlue
\cite{sarlin20superglue}. We show that including additional geometrical
information, such as local feature scale, orientation, and affine geometry,
when available (e.g. for SIFT features), significantly improves the performance
of the OpenGlue matcher. We study the influence of the various attention
mechanisms on accuracy and speed. We also present a simple architectural
improvement by combining local descriptors with context-aware descriptors. The
code and pretrained OpenGlue models for the different local features are
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less than Few: Self-Shot Video Instance Segmentation. (arXiv:2204.08874v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08874">
<div class="article-summary-box-inner">
<span><p>The goal of this paper is to bypass the need for labelled examples in
few-shot video understanding at run time. While proven effective, in many
practical video settings even labelling a few examples appears unrealistic.
This is especially true as the level of details in spatio-temporal video
understanding and with it, the complexity of annotations continues to increase.
Rather than performing few-shot learning with a human oracle to provide a few
densely labelled support videos, we propose to automatically learn to find
appropriate support videos given a query. We call this self-shot learning and
we outline a simple self-supervised learning method to generate an embedding
space well-suited for unsupervised retrieval of relevant samples. To showcase
this novel setting, we tackle, for the first time, video instance segmentation
in a self-shot (and few-shot) setting, where the goal is to segment instances
at the pixel-level across the spatial and temporal domains. We provide strong
baseline performances that utilize a novel transformer-based model and show
that self-shot learning can even surpass few-shot and can be positively
combined for further performance gains. Experiments on new benchmarks show that
our approach achieves strong performance, is competitive to oracle support in
some settings, scales to large unlabelled video collections, and can be
combined in a semi-supervised setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invertible Mask Network for Face Privacy-Preserving. (arXiv:2204.08895v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08895">
<div class="article-summary-box-inner">
<span><p>Face privacy-preserving is one of the hotspots that arises dramatic interests
of research. However, the existing face privacy-preserving methods aim at
causing the missing of semantic information of face and cannot preserve the
reusability of original facial information. To achieve the naturalness of the
processed face and the recoverability of the original protected face, this
paper proposes face privacy-preserving method based on Invertible "Mask"
Network (IMN). In IMN, we introduce a Mask-net to generate "Mask" face firstly.
Then, put the "Mask" face onto the protected face and generate the masked face,
in which the masked face is indistinguishable from "Mask" face. Finally, "Mask"
face can be put off from the masked face and obtain the recovered face to the
authorized users, in which the recovered face is visually indistinguishable
from the protected face. The experimental results show that the proposed method
can not only effectively protect the privacy of the protected face, but also
almost perfectly recover the protected face from the masked face.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Efficient Single Image Dehazing and Desnowing. (arXiv:2204.08899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08899">
<div class="article-summary-box-inner">
<span><p>Removing adverse weather conditions like rain, fog, and snow from images is a
challenging problem. Although the current recovery algorithms targeting a
specific condition have made impressive progress, it is not flexible enough to
deal with various degradation types. We propose an efficient and compact image
restoration network named DAN-Net (Degradation-Adaptive Neural Network) to
address this problem, which consists of multiple compact expert networks with
one adaptive gated neural. A single expert network efficiently addresses
specific degradation in nasty winter scenes relying on the compact architecture
and three novel components. Based on the Mixture of Experts strategy, DAN-Net
captures degradation information from each input image to adaptively modulate
the outputs of task-specific expert networks to remove various adverse winter
weather conditions. Specifically, it adopts a lightweight Adaptive Gated Neural
Network to estimate gated attention maps of the input image, while different
task-specific experts with the same topology are jointly dispatched to process
the degraded image. Such novel image restoration pipeline handles different
types of severe weather scenes effectively and efficiently. It also enjoys the
benefit of coordinate boosting in which the whole network outperforms each
expert trained without coordination.
</p>
<p>Extensive experiments demonstrate that the presented manner outperforms the
state-of-the-art single-task methods on image quality and has better inference
efficiency. Furthermore, we have collected the first real-world winter scenes
dataset to evaluate winter image restoration methods, which contains various
hazy and snowy images snapped in winter. Both the dataset and source code will
be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing. (arXiv:2204.08906v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08906">
<div class="article-summary-box-inner">
<span><p>We present PHORHUM, a novel, end-to-end trainable, deep neural network
methodology for photorealistic 3D human reconstruction given just a monocular
RGB image. Our pixel-aligned method estimates detailed 3D geometry and, for the
first time, the unshaded surface color together with the scene illumination.
Observing that 3D supervision alone is not sufficient for high fidelity color
reconstruction, we introduce patch-based rendering losses that enable reliable
color reconstruction on visible parts of the human, and detailed and plausible
color estimation for the non-visible parts. Moreover, our method specifically
addresses methodological and practical limitations of prior work in terms of
representing geometry, albedo, and illumination effects, in an end-to-end model
where factors can be effectively disentangled. In extensive experiments, we
demonstrate the versatility and robustness of our approach. Our
state-of-the-art results validate the method qualitatively and for different
metrics, for both geometric and color reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Calibrated Efficient Transformer for Lightweight Super-Resolution. (arXiv:2204.08913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08913">
<div class="article-summary-box-inner">
<span><p>Recently, deep learning has been successfully applied to the single-image
super-resolution (SISR) with remarkable performance. However, most existing
methods focus on building a more complex network with a large number of layers,
which can entail heavy computational costs and memory storage. To address this
problem, we present a lightweight Self-Calibrated Efficient Transformer (SCET)
network to solve this problem. The architecture of SCET mainly consists of the
self-calibrated module and efficient transformer block, where the
self-calibrated module adopts the pixel attention mechanism to extract image
features effectively. To further exploit the contextual information from
features, we employ an efficient transformer to help the network obtain similar
features over long distances and thus recover sufficient texture details. We
provide comprehensive results on different settings of the overall network. Our
proposed method achieves more remarkable performance than baseline methods. The
source code and pre-trained models are available at
https://github.com/AlexZou14/SCET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global-and-Local Collaborative Learning for Co-Salient Object Detection. (arXiv:2204.08917v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08917">
<div class="article-summary-box-inner">
<span><p>The goal of co-salient object detection (CoSOD) is to discover salient
objects that commonly appear in a query group containing two or more relevant
images. Therefore, how to effectively extract inter-image correspondence is
crucial for the CoSOD task. In this paper, we propose a global-and-local
collaborative learning architecture, which includes a global correspondence
modeling (GCM) and a local correspondence modeling (LCM) to capture
comprehensive inter-image corresponding relationship among different images
from the global and local perspectives. Firstly, we treat different images as
different time slices and use 3D convolution to integrate all intra features
intuitively, which can more fully extract the global group semantics. Secondly,
we design a pairwise correlation transformation (PCT) to explore similarity
correspondence between pairwise images and combine the multiple local pairwise
correspondences to generate the local inter-image relationship. Thirdly, the
inter-image relationships of the GCM and LCM are integrated through a
global-and-local correspondence aggregation (GLA) module to explore more
comprehensive inter-image collaboration cues. Finally, the intra- and
inter-features are adaptively integrated by an intra-and-inter weighting fusion
(AEWF) module to learn co-saliency features and predict the co-saliency map.
The proposed GLNet is evaluated on three prevailing CoSOD benchmark datasets,
demonstrating that our model trained on a small dataset (about 3k images) still
outperforms eleven state-of-the-art competitors trained on some large datasets
(about 8k-200k images).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Imagine: Diversify Memory for Incremental Learning using Unlabeled Data. (arXiv:2204.08932v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08932">
<div class="article-summary-box-inner">
<span><p>Deep neural network (DNN) suffers from catastrophic forgetting when learning
incrementally, which greatly limits its applications. Although maintaining a
handful of samples (called `exemplars`) of each task could alleviate forgetting
to some extent, existing methods are still limited by the small number of
exemplars since these exemplars are too few to carry enough task-specific
knowledge, and therefore the forgetting remains. To overcome this problem, we
propose to `imagine` diverse counterparts of given exemplars referring to the
abundant semantic-irrelevant information from unlabeled data. Specifically, we
develop a learnable feature generator to diversify exemplars by adaptively
generating diverse counterparts of exemplars based on semantic information from
exemplars and semantically-irrelevant information from unlabeled data. We
introduce semantic contrastive learning to enforce the generated samples to be
semantic consistent with exemplars and perform semanticdecoupling contrastive
learning to encourage diversity of generated samples. The diverse generated
samples could effectively prevent DNN from forgetting when learning new tasks.
Our method does not bring any extra inference cost and outperforms
state-of-the-art methods on two benchmarks CIFAR-100 and ImageNet-Subset by a
clear margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning-based surrogate model for 3-D patient-specific computational fluid dynamics. (arXiv:2204.08939v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08939">
<div class="article-summary-box-inner">
<span><p>Optimization and uncertainty quantification have been playing an increasingly
important role in computational hemodynamics. However, existing methods based
on principled modeling and classic numerical techniques have faced significant
challenges, particularly when it comes to complex 3D patient-specific shapes in
the real world. First, it is notoriously challenging to parameterize the input
space of arbitrarily complex 3-D geometries. Second, the process often involves
massive forward simulations, which are extremely computationally demanding or
even infeasible. We propose a novel deep learning surrogate modeling solution
to address these challenges and enable rapid hemodynamic predictions.
Specifically, a statistical generative model for 3-D patient-specific shapes is
developed based on a small set of baseline patient-specific geometries. An
unsupervised shape correspondence solution is used to enable geometric morphing
and scalable shape synthesis statistically. Moreover, a simulation routine is
developed for automatic data generation by automatic meshing, boundary setting,
simulation, and post-processing. An efficient supervised learning solution is
proposed to map the geometric inputs to the hemodynamics predictions in latent
spaces. Numerical studies on aortic flows are conducted to demonstrate the
effectiveness and merit of the proposed techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Missingness Bias in Model Debugging. (arXiv:2204.08945v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08945">
<div class="article-summary-box-inner">
<span><p>Missingness, or the absence of features from an input, is a concept
fundamental to many model debugging tools. However, in computer vision, pixels
cannot simply be removed from an image. One thus tends to resort to heuristics
such as blacking out pixels, which may in turn introduce bias into the
debugging process. We study such biases and, in particular, show how
transformer-based architectures can enable a more natural implementation of
missingness, which side-steps these issues and improves the reliability of
model debugging in practice. Our code is available at
https://github.com/madrylab/missingness
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Vicinal Risk Minimization for Partially Supervised Multi-Label Classification Under Data Scarcity. (arXiv:2204.08954v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08954">
<div class="article-summary-box-inner">
<span><p>Due to the high human cost of annotation, it is non-trivial to curate a
large-scale medical dataset that is fully labeled for all classes of interest.
Instead, it would be convenient to collect multiple small partially labeled
datasets from different matching sources, where the medical images may have
only been annotated for a subset of classes of interest. This paper offers an
empirical understanding of an under-explored problem, namely partially
supervised multi-label classification (PSMLC), where a multi-label classifier
is trained with only partially labeled medical images. In contrast to the fully
supervised counterpart, the partial supervision caused by medical data scarcity
has non-trivial negative impacts on the model performance. A potential remedy
could be augmenting the partial labels. Though vicinal risk minimization (VRM)
has been a promising solution to improve the generalization ability of the
model, its application to PSMLC remains an open question. To bridge the
methodological gap, we provide the first VRM-based solution to PSMLC. The
empirical results also provide insights into future research directions on
partially supervised learning under data scarcity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment. (arXiv:2204.08958v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08958">
<div class="article-summary-box-inner">
<span><p>No-Reference Image Quality Assessment (NR-IQA) aims to assess the perceptual
quality of images in accordance with human subjective perception.
Unfortunately, existing NR-IQA methods are far from meeting the needs of
predicting accurate quality scores on GAN-based distortion images. To this end,
we propose Multi-dimension Attention Network for no-reference Image Quality
Assessment (MANIQA) to improve the performance on GAN-based distortion. We
firstly extract features via ViT, then to strengthen global and local
interactions, we propose the Transposed Attention Block (TAB) and the Scale
Swin Transformer Block (SSTB). These two modules apply attention mechanisms
across the channel and spatial dimension, respectively. In this
multi-dimensional manner, the modules cooperatively increase the interaction
among different regions of images globally and locally. Finally, a dual branch
structure for patch-weighted quality prediction is applied to predict the final
score depending on the weight of each patch's score. Experimental results
demonstrate that MANIQA outperforms state-of-the-art methods on four standard
datasets (LIVE, TID2013, CSIQ, and KADID-10K) by a large margin. Besides, our
method ranked first place in the final testing phase of the NTIRE 2022
Perceptual Image Quality Assessment Challenge Track 2: No-Reference. Codes and
models are available at https://github.com/IIGROUP/MANIQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rendering Nighttime Image Via Cascaded Color and Brightness Compensation. (arXiv:2204.08970v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08970">
<div class="article-summary-box-inner">
<span><p>Image signal processing (ISP) is crucial for camera imaging, and neural
networks (NN) solutions are extensively deployed for daytime scenes. The lack
of sufficient nighttime image dataset and insights on nighttime illumination
characteristics poses a great challenge for high-quality rendering using
existing NN ISPs. To tackle it, we first built a high-resolution nighttime
RAW-RGB (NR2R) dataset with white balance and tone mapping annotated by expert
professionals. Meanwhile, to best capture the characteristics of nighttime
illumination light sources, we develop the CBUnet, a two-stage NN ISP to
cascade the compensation of color and brightness attributes. Experiments show
that our method has better visual quality compared to traditional ISP pipeline,
and is ranked at the second place in the NTIRE 2022 Night Photography Rendering
Challenge for two tracks by respective People's and Professional Photographer's
choices. The code and relevant materials are avaiable on our website:
https://njuvision.github.io/CBUnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shallow camera pipeline for night photography rendering. (arXiv:2204.08972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08972">
<div class="article-summary-box-inner">
<span><p>We introduce a camera pipeline for rendering visually pleasing photographs in
low light conditions, as part of the NTIRE2022 Night Photography Rendering
challenge. Given the nature of the task, where the objective is verbally
defined by an expert photographer instead of relying on explicit ground truth
images, we design an handcrafted solution, characterized by a shallow structure
and by a low parameter count. Our pipeline exploits a local light enhancer as a
form of high dynamic range correction, followed by a global adjustment of the
image histogram to prevent washed-out results. We proportionally apply image
denoising to darker regions, where it is more easily perceived, without losing
details on brighter regions. The solution reached the fifth place in the
competition, with a preference vote count comparable to those of other entries,
based on deep convolutional neural networks. Code is available at
www.github.com/AvailableAfterAcceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comparison of different atmospheric turbulence simulation methods for image restoration. (arXiv:2204.08974v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08974">
<div class="article-summary-box-inner">
<span><p>Atmospheric turbulence deteriorates the quality of images captured by
long-range imaging systems by introducing blur and geometric distortions to the
captured scene. This leads to a drastic drop in performance when computer
vision algorithms like object/face recognition and detection are performed on
these images. In recent years, various deep learning-based atmospheric
turbulence mitigation methods have been proposed in the literature. These
methods are often trained using synthetically generated images and tested on
real-world images. Hence, the performance of these restoration methods depends
on the type of simulation used for training the network. In this paper, we
systematically evaluate the effectiveness of various turbulence simulation
methods on image restoration. In particular, we evaluate the performance of two
state-or-the-art restoration networks using six simulations method on a
real-world LRFID dataset consisting of face images degraded by turbulence. This
paper will provide guidance to the researchers and practitioners working in
this field to choose the suitable data generation models for training deep
models for turbulence mitigation. The implementation codes for the simulation
methods, source codes for the networks, and the pre-trained models will be
publicly made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Face Recognition System. (arXiv:2204.08978v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08978">
<div class="article-summary-box-inner">
<span><p>Over the past few decades, interest in algorithms for face recognition has
been growing rapidly and has even surpassed human-level performance. Despite
their accomplishments, their practical integration with a real-time
performance-hungry system is not feasible due to high computational costs. So
in this paper, we explore the recent, fast, and accurate face recognition
system that can be easily integrated with real-time devices, and tested the
algorithms on robot hardware platforms to confirm their robustness and speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones. (arXiv:2204.08989v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08989">
<div class="article-summary-box-inner">
<span><p>Nowadays, due to the widespread use of smartphones in everyday life and the
improvement of computational capabilities of these devices, many complex tasks
can now be deployed on them. Concerning the need for continuous monitoring of
vital signs, especially for the elderly or those with certain types of
diseases, the development of algorithms that can estimate vital signs using
smartphones has attracted researchers worldwide. Such algorithms estimate vital
signs (heart rate and oxygen saturation level) by processing an input PPG
signal. These methods often apply multiple pre-processing steps to the input
signal before the prediction step. This can increase the computational
complexity of these methods, meaning only a limited number of mobile devices
can run them. Furthermore, multiple pre-processing steps also require the
design of a couple of hand-crafted stages to obtain an optimal result. This
research proposes a novel end-to-end solution to mobile-based vital sign
estimation by deep learning. The proposed method does not require any
pre-processing. Due to the use of fully convolutional architecture, the
parameter count of our proposed model is, on average, a quarter of the ordinary
architectures that use fully-connected layers as the prediction heads. As a
result, the proposed model has less over-fitting chance and computational
complexity. A public dataset for vital sign estimation, including 62 videos
collected from 35 men and 27 women, is also provided. The experimental results
demonstrate state-of-the-art estimation accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Domain Image Synthesis using Segmentation-Guided GAN. (arXiv:2204.09015v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09015">
<div class="article-summary-box-inner">
<span><p>We introduce a segmentation-guided approach to synthesise images that
integrate features from two distinct domains. Images synthesised by our
dual-domain model belong to one domain within the semantic mask, and to another
in the rest of the image - smoothly integrated. We build on the successes of
few-shot StyleGAN and single-shot semantic segmentation to minimise the amount
of training required in utilising two domains. The method combines a few-shot
cross-domain StyleGAN with a latent optimiser to achieve images containing
features of two distinct domains. We use a segmentation-guided perceptual loss,
which compares both pixel-level and activations between domain-specific and
dual-domain synthetic images. Results demonstrate qualitatively and
quantitatively that our model is capable of synthesising dual-domain images on
a variety of objects (faces, horses, cats, cars), domains (natural, caricature,
sketches) and part-based masks (eyes, nose, mouth, hair, car bonnet). The code
is publicly available at:
https://github.com/denabazazian/Dual-Domain-Synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised detection of ash dieback disease (Hymenoscyphus fraxineus) using diffusion-based hyperspectral image clustering. (arXiv:2204.09041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09041">
<div class="article-summary-box-inner">
<span><p>Ash dieback (Hymenoscyphus fraxineus) is an introduced fungal disease that is
causing the widespread death of ash trees across Europe. Remote sensing
hyperspectral images encode rich structure that has been exploited for the
detection of dieback disease in ash trees using supervised machine learning
techniques. However, to understand the state of forest health at
landscape-scale, accurate unsupervised approaches are needed. This article
investigates the use of the unsupervised Diffusion and VCA-Assisted Image
Segmentation (D-VIS) clustering algorithm for the detection of ash dieback
disease in a forest site near Cambridge, United Kingdom. The unsupervised
clustering presented in this work has high overlap with the supervised
classification of previous work on this scene (overall accuracy = 71%). Thus,
unsupervised learning may be used for the remote detection of ash dieback
disease without the need for expert labeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good, Better, Best: Textual Distractors Generation for Multiple-Choice Visual Question Answering via Reinforcement Learning. (arXiv:1910.09134v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.09134">
<div class="article-summary-box-inner">
<span><p>Multiple-choice VQA has drawn increasing attention from researchers and
end-users recently. As the demand for automatically constructing large-scale
multiple-choice VQA data grows, we introduce a novel task called textual
Distractors Generation for VQA (DG-VQA) focusing on generating challenging yet
meaningful distractors given the context image, question, and correct answer.
The DG-VQA task aims at generating distractors without ground-truth training
samples since such resources are rarely available. To tackle the DG-VQA
unsupervisedly, we propose Gobbet, a reinforcement learning(RL) based framework
that utilizes pre-trained VQA models as an alternative knowledge base to guide
the distractor generation process. In Gobbet, a pre-trained VQA model serves as
the environment in RL setting to provide feedback for the input multi-modal
query, while a neural distractor generator serves as the agent to take actions
accordingly. We propose to use existing VQA models' performance degradation as
indicators of the quality of generated distractors. On the other hand, we show
the utility of generated distractors through data augmentation experiments,
since robustness is more and more important when AI models apply to
unpredictable open-domain scenarios or security-sensitive applications. We
further conduct a manual case study on the factors why distractors generated by
Gobbet can fool existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Deep Hashing Methods. (arXiv:2003.03369v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.03369">
<div class="article-summary-box-inner">
<span><p>Nearest neighbor search is to find the data points in the database such that
the distances from them to the query are the smallest, which is a fundamental
problem in various domains, such as computer vision, recommendation systems and
machine learning. Hashing is one of the most widely used methods for its
computational and storage efficiency. With the development of deep learning,
deep hashing methods show more advantages than traditional methods. In this
paper, we present a comprehensive survey of the deep hashing algorithms
including deep supervised hashing and deep unsupervised hashing. Specifically,
we categorize deep supervised hashing methods into pairwise methods,
ranking-based methods, pointwise methods as well as quantization according to
how measuring the similarities of the learned hash codes. Moreover, deep
unsupervised hashing is categorized into similarity reconstruction-based
methods, pseudo-label-based methods and prediction-free self-supervised
learning-based methods based on their semantic learning manners. We also
introduce three related important topics including semi-supervised deep
hashing, domain adaption deep hashing and multi-modal deep hashing. Meanwhile,
we present some commonly used public datasets and the scheme to measure the
performance of deep hashing algorithms. Finally, we discuss some potential
research directions in conclusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptron Synthesis Network: Rethinking the Action Scale Variances in Videos. (arXiv:2007.11460v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.11460">
<div class="article-summary-box-inner">
<span><p>Video action recognition has been partially addressed by the CNNs stacking of
fixed-size 3D kernels. However, these methods may under-perform for only
capturing rigid spatial-temporal patterns in single-scale spaces, while
neglecting the scale variances across different action primitives. To overcome
this limitation, we propose to learn the optimal-scale kernels from the data.
More specifically, an \textit{action perceptron synthesizer} is proposed to
generate the kernels from a bag of fixed-size kernels that are interacted by
dense routing paths. To guarantee the interaction richness and the information
capacity of the paths, we design the novel \textit{optimized feature fusion
layer}. This layer establishes a principled universal paradigm that suffices to
cover most of the current feature fusion techniques (e.g., channel shuffling,
and channel dropout) for the first time. By inserting the \textit{synthesizer},
our method can easily adapt the traditional 2D CNNs to the video understanding
tasks such as action recognition with marginal additional computation cost. The
proposed method is thoroughly evaluated over several challenging datasets
(i.e., Somehting-to-Somthing, Kinetics and Diving48) that highly require
temporal reasoning or appearance discriminating, achieving new state-of-the-art
results. Particularly, our low-resolution model outperforms the recent strong
baseline methods, i.e., TSM and GST, with less than 30\% of their computation
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralAnnot: Neural Annotator for 3D Human Mesh Training Sets. (arXiv:2011.11232v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11232">
<div class="article-summary-box-inner">
<span><p>Most 3D human mesh regressors are fully supervised with 3D pseudo-GT human
model parameters and weakly supervised with GT 2D/3D joint coordinates as the
3D pseudo-GTs bring great performance gain. The 3D pseudo-GTs are obtained by
annotators, systems that iteratively fit 3D human model parameters to GT 2D/3D
joint coordinates of training sets in the pre-processing stage of the
regressors. The fitted 3D parameters at the last fitting iteration become the
3D pseudo-GTs, used to fully supervise the regressors. Optimization-based
annotators, such as SMPLify-X, have been widely used to obtain the 3D
pseudo-GTs. However, they often produce wrong 3D pseudo-GTs as they fit the 3D
parameters to GT of each sample independently. To overcome the limitation, we
present NeuralAnnot, a neural network-based annotator. The main idea of
NeuralAnnot is to employ a neural network-based regressor and dedicate it for
the annotation. Assuming no 3D pseudo-GTs are available, NeuralAnnot is weakly
supervised with GT 2D/3D joint coordinates of training sets. The testing
results on the same training sets become 3D pseudo-GTs, used to fully supervise
the regressors. We show that 3D pseudo-GTs of NeuralAnnot are highly beneficial
to train the regressors. We made our 3D pseudo-GTs publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation. (arXiv:2011.11534v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11534">
<div class="article-summary-box-inner">
<span><p>Whole-body 3D human mesh estimation aims to reconstruct the 3D human body,
hands, and face simultaneously. Although several methods have been proposed,
accurate prediction of 3D hands, which consist of 3D wrist and fingers, still
remains challenging due to two reasons. First, the human kinematic chain has
not been carefully considered when predicting the 3D wrists. Second, previous
works utilize body features for the 3D fingers, where the body feature barely
contains finger information. To resolve the limitations, we present Hand4Whole,
which has two strong points over previous works. First, we design Pose2Pose, a
module that utilizes joint features for 3D joint rotations. Using Pose2Pose,
Hand4Whole utilizes hand MCP joint features to predict 3D wrists as MCP joints
largely contribute to 3D wrist rotations in the human kinematic chain. Second,
Hand4Whole discards the body feature when predicting 3D finger rotations. Our
Hand4Whole is trained in an end-to-end manner and produces much better 3D hand
results than previous whole-body 3D human mesh estimation methods. The codes
are available here at https://github.com/mks0601/Hand4Whole_RELEASE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Density Ratio-Guided Subsampling of Conditional GANs, With Conditioning on a Class or a Continuous Variable. (arXiv:2103.11166v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11166">
<div class="article-summary-box-inner">
<span><p>Recently, subsampling or refining images generated from unconditional GANs
has been actively studied to improve the overall image quality. Unfortunately,
these methods are often observed less effective or inefficient in handling
conditional GANs (cGANs) -- conditioning on a class (aka class-conditional
GANs) or a continuous variable (aka continuous cGANs or CcGANs). In this work,
we introduce an effective and efficient subsampling scheme, named conditional
density ratio-guided rejection sampling (cDR-RS), to sample high-quality images
from cGANs. Specifically, we first develop a novel conditional density ratio
estimation method, termed cDRE-F-cSP, by proposing the conditional Softplus
(cSP) loss and an improved feature extraction mechanism. We then derive the
error bound of a density ratio model trained with the cSP loss. Finally, we
accept or reject a fake image in terms of its estimated conditional density
ratio. A filtering scheme is also developed to increase fake images' label
consistency without losing diversity when sampling from CcGANs. We extensively
test the effectiveness and efficiency of cDR-RS in sampling from both
class-conditional GANs and CcGANs on five benchmark datasets. When sampling
from class-conditional GANs, cDR-RS outperforms modern state-of-the-art methods
by a large margin (except DRE-F-SP+RS) in terms of effectiveness. Although the
effectiveness of cDR-RS is often comparable to that of DRE-F-SP+RS, cDR-RS is
substantially more efficient. When sampling from CcGANs, the superiority of
cDR-RS is even more noticeable in terms of both effectiveness and efficiency.
Notably, with the consumption of reasonable computational resources, cDR-RS can
substantially reduce Label Score without decreasing the diversity of
CcGAN-generated images, while other methods often need to trade much diversity
for slightly improved Label Score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine learning method for light field refocusing. (arXiv:2103.16020v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16020">
<div class="article-summary-box-inner">
<span><p>Light field imaging introduced the capability to refocus an image after
capturing. Currently there are two popular methods for refocusing,
shift-and-sum and Fourier slice methods. Neither of these two methods can
refocus the light field in real-time without any pre-processing. In this paper
we introduce a machine learning based refocusing technique that is capable of
extracting 16 refocused images with refocusing parameters of
\alpha=0.125,0.250,0.375,...,2.0 in real-time. We have trained our network,
which is called RefNet, in two experiments. Once using the Fourier slice method
as the training -- i.e., "ground truth" -- data and another using the
shift-and-sum method as the training data. We showed that in both cases, not
only is the RefNet method at least 134x faster than previous approaches, but
also the color prediction of RefNet is superior to both Fourier slice and
shift-and-sum methods while having similar depth of field and focus distance
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fourier Image Transformer. (arXiv:2104.02555v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02555">
<div class="article-summary-box-inner">
<span><p>Transformer architectures show spectacular performance on NLP tasks and have
recently also been used for tasks such as image completion or image
classification. Here we propose to use a sequential image representation, where
each prefix of the complete sequence describes the whole image at reduced
resolution. Using such Fourier Domain Encodings (FDEs), an auto-regressive
image completion task is equivalent to predicting a higher resolution output
given a low-resolution input. Additionally, we show that an encoder-decoder
setup can be used to query arbitrary Fourier coefficients given a set of
Fourier domain observations. We demonstrate the practicality of this approach
in the context of computed tomography (CT) image reconstruction. In summary, we
show that Fourier Image Transformer (FIT) can be used to solve relevant image
analysis tasks in Fourier space, a domain inherently inaccessible to
convolutional architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling and Transferring Knowledge via cGAN-generated Samples for Image Classification and Regression. (arXiv:2104.03164v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03164">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) has been actively studied for image
classification tasks in deep learning, aiming to improve the performance of a
student based on the knowledge from a teacher. However, applying KD in image
regression with a scalar response variable has been rarely studied, and there
exists no KD method applicable to both classification and regression tasks yet.
Moreover, existing KD methods often require a practitioner to carefully select
or adjust the teacher and student architectures, making these methods less
flexible in practice. To address the above problems in a unified way, we
propose a comprehensive KD framework based on cGANs, termed cGAN-KD.
Fundamentally different from existing KD methods, cGAN-KD distills and
transfers knowledge from a teacher model to a student model via cGAN-generated
samples. This novel mechanism makes cGAN-KD suitable for both classification
and regression tasks, compatible with other KD methods, and insensitive to the
teacher and student architectures. An error bound for a student model trained
in the cGAN-KD framework is derived in this work, providing a theory for why
cGAN-KD is effective as well as guiding the practical implementation of
cGAN-KD. Extensive experiments on CIFAR-100 and ImageNet-100 show that we can
combine state of the art KD methods with the cGAN-KD framework to yield a new
state of the art. Moreover, experiments on Steering Angle and UTKFace
demonstrate the effectiveness of cGAN-KD in image regression tasks, where
existing KD methods are inapplicable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection. (arXiv:2104.09770v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09770">
<div class="article-summary-box-inner">
<span><p>The widespread dissemination of Deepfakes demands effective approaches that
can detect perceptually convincing forged images. In this paper, we aim to
capture the subtle manipulation artifacts at different scales using transformer
models. In particular, we introduce a Multi-modal Multi-scale TRansformer
(M2TR), which operates on patches of different sizes to detect local
inconsistencies in images at different spatial levels. M2TR further learns to
detect forgery artifacts in the frequency domain to complement RGB information
through a carefully designed cross modality fusion block. In addition, to
stimulate Deepfake detection research, we introduce a high-quality Deepfake
dataset, SR-DF, which consists of 4,000 DeepFake videos generated by
state-of-the-art face swapping and facial reenactment methods. We conduct
extensive experiments to verify the effectiveness of the proposed method, which
outperforms state-of-the-art Deepfake detection methods by clear margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation. (arXiv:2104.14639v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14639">
<div class="article-summary-box-inner">
<span><p>We propose a robust and accurate method for estimating the 3D poses of two
hands in close interaction from a single color image. This is a very
challenging problem, as large occlusions and many confusions between the joints
may happen. State-of-the-art methods solve this problem by regressing a heatmap
for each joint, which requires solving two problems simultaneously: localizing
the joints and recognizing them. In this work, we propose to separate these
tasks by relying on a CNN to first localize joints as 2D keypoints, and on
self-attention between the CNN features at these keypoints to associate them
with the corresponding hand joint. The resulting architecture, which we call
"Keypoint Transformer", is highly efficient as it achieves state-of-the-art
performance with roughly half the number of model parameters on the
InterHand2.6M dataset. We also show it can be easily extended to estimate the
3D pose of an object manipulated by one or two hands with high performance.
Moreover, we created a new dataset of more than 75,000 images of two hands
manipulating an object fully annotated in 3D and will make it publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSRNet: Cascaded Selective Resolution Network for Real-time Semantic Segmentation. (arXiv:2106.04400v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04400">
<div class="article-summary-box-inner">
<span><p>Real-time semantic segmentation has received considerable attention due to
growing demands in many practical applications, such as autonomous vehicles,
robotics, etc. Existing real-time segmentation approaches often utilize feature
fusion to improve segmentation accuracy. However, they fail to fully consider
the feature information at different resolutions and the receptive fields of
the networks are relatively limited, thereby compromising the performance. To
tackle this problem, we propose a light Cascaded Selective Resolution Network
(CSRNet) to improve the performance of real-time segmentation through multiple
context information embedding and enhanced feature aggregation. The proposed
network builds a three-stage segmentation system, which integrates feature
information from low resolution to high resolution and achieves feature
refinement progressively. CSRNet contains two critical modules: the Shorted
Pyramid Fusion Module (SPFM) and the Selective Resolution Module (SRM). The
SPFM is a computationally efficient module to incorporate the global context
information and significantly enlarge the receptive field at each stage. The
SRM is designed to fuse multi-resolution feature maps with various receptive
fields, which assigns soft channel attentions across the feature maps and helps
to remedy the problem caused by multi-scale objects. Comprehensive experiments
on two well-known datasets demonstrate that the proposed CSRNet effectively
improves the performance for real-time segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14256">
<div class="article-summary-box-inner">
<span><p>Background: Transrectal ultrasound guided systematic biopsies of the prostate
is a routine procedure to establish a prostate cancer diagnosis. However, the
10-12 prostate core biopsies only sample a relatively small volume of the
prostate, and tumour lesions in regions between biopsy cores can be missed,
leading to a well-known low sensitivity to detect clinically relevant cancer.
As a proof-of-principle, we developed and validated a deep convolutional neural
network model to distinguish between morphological patterns in benign prostate
biopsy whole slide images from men with and without established cancer.
Methods: This study included 14,354 hematoxylin and eosin stained whole slide
images from benign prostate biopsies from 1,508 men in two groups: men without
an established prostate cancer (PCa) diagnosis and men with at least one core
biopsy diagnosed with PCa. 80% of the participants were assigned as training
data and used for model optimization (1,211 men), and the remaining 20% (297
men) as a held-out test set used to evaluate model performance. An ensemble of
10 deep convolutional neural network models was optimized for classification of
biopsies from men with and without established cancer. Hyperparameter
optimization and model selection was performed by cross-validation in the
training data . Results: Area under the receiver operating characteristic curve
(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy
level and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a
specificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:
The developed model has the ability to detect men with risk of missed PCa due
to under-sampling of the prostate. The proposed model has the potential to
reduce the number of false negative cases in routine systematic prostate
biopsies and to indicate men who could benefit from MRI-guided re-biopsy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An overview of mixing augmentation methods and augmentation strategies. (arXiv:2107.09887v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09887">
<div class="article-summary-box-inner">
<span><p>Deep Convolutional Neural Networks have made an incredible progress in many
Computer Vision tasks. This progress, however, often relies on the availability
of large amounts of the training data, required to prevent over-fitting, which
in many domains entails significant cost of manual data labeling. An
alternative approach is application of data augmentation (DA) techniques that
aim at model regularization by creating additional observations from the
available ones. This survey focuses on two DA research streams: image mixing
and automated selection of augmentation strategies. First, the presented
methods are briefly described, and then qualitatively compared with respect to
their key characteristics. Various quantitative comparisons are also included
based on the results reported in recent DA literature. This review mainly
covers the methods published in the materials of top-tier conferences and in
leading journals in the years 2017-2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05021">
<div class="article-summary-box-inner">
<span><p>Red-lesions, microaneurysms (MAs) and hemorrhages (HMs), are the early signs
of diabetic retinopathy (DR). The automatic detection of MAs and HMs on retinal
fundus images is a challenging task. Most of the existing methods detect either
only MAs or only HMs because of the difference in their texture, sizes, and
morphology. Though some methods detect both MAs and HMs, they suffer from the
curse of dimensionality of shape and colors features and fail to detect all
shape variations of HMs such as flame-shaped. Leveraging the progress in deep
learning, we proposed a two-stream red lesions detection system dealing
simultaneously with small and large red lesions. For this system, we introduced
a new ROIs candidates generation method for large red lesions on fundus images;
it is based on blood vessel segmentation and morphological operations, and
reduces the computational complexity, and enhances the detection accuracy by
generating a small number of potential candidates. For detection, we proposed a
framework with two streams. We used pretrained VGGNet as a backbone model and
carried out several extensive experiments to tune it for vessels segmentation
and candidates generation, and finally learning the appropriate mapping, which
yields better detection of the red lesions comparing with the state-of-the-art
methods. The experimental results validated the effectiveness of the system in
the detection of both MAs and HMs; it yields higher performance for per lesion
detection; its sensitivity equals 0.8589 and good FROC score under 8 FPIs on
DiaretDB1-MA reports FROC=0.7518, and with SN=0.7552 and good FROC score under
2,4and 8 FPIs on DiaretDB1-HM, and SN=0.8157 on e-ophtha with overall
FROC=0.4537 and on ROCh dataset with FROC=0.3461 which is higher than the
state-of-the art methods. For DR screening, the system performs well with good
AUC on DiaretDB1-MA, DiaretDB1-HM, and e-ophtha datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines. (arXiv:2109.12925v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12925">
<div class="article-summary-box-inner">
<span><p>Due to its role in many computer vision tasks, image matching has been
subjected to an active investigation by researchers, which has lead to better
and more discriminant feature descriptors and to more robust matching
strategies, also thanks to the advent of the deep learning and the increased
computational power of the modern hardware. Despite of these achievements, the
keypoint extraction process at the base of the image matching pipeline has not
seen equivalent progresses. This paper presents HarrisZ$^+$, an upgrade to the
HarrisZ corner detector, optimized to synergically take advance of the recent
improvements of the other steps of the image matching pipeline. HarrisZ$^+$
does not only consists of a tuning of the setup parameters, but introduces
further refinements to the selection criteria delineated by HarrisZ, so
providing more, yet discriminative, keypoints, which are better distributed on
the image and with higher localization accuracy. The image matching pipeline
including HarrisZ$^+$, together with the other modern components, obtained in
different recent matching benchmarks state-of-the-art results among the classic
image matching pipelines. These results are quite close to those obtained by
the more recent fully deep end-to-end trainable approaches and show that there
is still a proper margin of improvement that can be granted by the research in
classic image matching methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Reasoning for Visual Question Answering. (arXiv:2110.02526v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02526">
<div class="article-summary-box-inner">
<span><p>Bridging the semantic gap between image and question is an important step to
improve the accuracy of the Visual Question Answering (VQA) task. However, most
of the existing VQA methods focus on attention mechanisms or visual relations
for reasoning the answer, while the features at different semantic levels are
not fully utilized. In this paper, we present a new reasoning framework to fill
the gap between visual features and semantic clues in the VQA task. Our method
first extracts the features and predicates from the image and question. We then
propose a new reasoning framework to effectively jointly learn these features
and predicates in a coarse-to-fine manner. The intensively experimental results
on three large-scale VQA datasets show that our proposed approach achieves
superior accuracy comparing with other state-of-the-art methods. Furthermore,
our reasoning framework also provides an explainable way to understand the
decision of the deep neural network when predicting the answer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPSN: Motion-aware Pseudo Siamese Network for Indoor Video Head Detection in Buildings. (arXiv:2110.03302v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03302">
<div class="article-summary-box-inner">
<span><p>Head detection in the indoor video is an essential component of building
occupancy detection. While deep models have achieved remarkable progress in
general object detection, they are not satisfying enough in complex indoor
scenes. The indoor surveillance video often includes cluttered background
objects, among which heads have small scales and diverse poses. In this paper,
we propose Motion-aware Pseudo Siamese Network (MPSN), an end-to-end approach
that leverages head motion information to guide the deep model to extract
effective head features in indoor scenarios. By taking the pixel-wise
difference of adjacent frames as the auxiliary input, MPSN effectively enhances
human head motion information and removes the irrelevant objects in the
background. Compared with prior methods, it achieves superior performance on
the two indoor video datasets. Our experiments show that MPSN successfully
suppresses static background objects and highlights the moving instances,
especially human heads in indoor videos. We also compare different methods to
capture head motion, which demonstrates the simplicity and flexibility of MPSN.
To validate the robustness of MPSN, we conduct adversarial experiments with a
mathematical solution of small perturbations for robust model selection.
Finally, for confirming its potential in building control systems, we apply
MPSN to occupancy counting. Code is available at
https://github.com/pl-share/MPSN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastDOG: Fast Discrete Optimization on GPU. (arXiv:2111.10270v3 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10270">
<div class="article-summary-box-inner">
<span><p>We present a massively parallel Lagrange decomposition method for solving
0--1 integer linear programs occurring in structured prediction. We propose a
new iterative update scheme for solving the Lagrangean dual and a perturbation
technique for decoding primal solutions. For representing subproblems we follow
Lange et al. (2021) and use binary decision diagrams (BDDs). Our primal and
dual algorithms require little synchronization between subproblems and
optimization over BDDs needs only elementary operations without complicated
control flow. This allows us to exploit the parallelism offered by GPUs for all
components of our method. We present experimental results on combinatorial
problems from MAP inference for Markov Random Fields, quadratic assignment and
cell tracking for developmental biology. Our highly parallel GPU implementation
improves upon the running times of the algorithms from Lange et al. (2021) by
up to an order of magnitude. In particular, we come close to or outperform some
state-of-the-art specialized heuristics while being problem agnostic. Our
implementation is available at https://github.com/LPMP/BDD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling. (arXiv:2111.12698v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12698">
<div class="article-summary-box-inner">
<span><p>Open-vocabulary instance segmentation aims at segmenting novel classes
without mask annotations. It is an important step toward reducing laborious
human supervision. Most existing works first pretrain a model on captioned
images covering many novel classes and then finetune it on limited base classes
with mask annotations. However, the high-level textual information learned from
caption pretraining alone cannot effectively encode the details required for
pixel-wise segmentation. To address this, we propose a cross-modal
pseudo-labeling framework, which generates training pseudo masks by aligning
word semantics in captions with visual features of object masks in images.
Thus, our framework is capable of labeling novel classes in captions via their
word semantics to self-train a student model. To account for noises in pseudo
masks, we design a robust student model that selectively distills mask
knowledge by estimating the mask noise levels, hence mitigating the adverse
impact of noisy pseudo masks. By extensive experiments, we show the
effectiveness of our framework, where we significantly improve mAP score by
4.5% on MS-COCO and 5.1% on the large-scale Open Images &amp; Conceptual Captions
datasets compared to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Space Smoothing for Individually Fair Representations. (arXiv:2111.13650v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13650">
<div class="article-summary-box-inner">
<span><p>Fair representation learning transforms user data into a representation that
ensures fairness and utility regardless of the downstream application. However,
learning individually fair representations, i.e., guaranteeing that similar
individuals are treated similarly, remains challenging in high-dimensional
settings such as computer vision. In this work, we introduce LASSI, the first
representation learning method for certifying individual fairness of
high-dimensional data. Our key insight is to leverage recent advances in
generative modeling to capture the set of similar individuals in the generative
latent space. This enables us to learn individually fair representations that
map similar individuals close together by using adversarial training to
minimize the distance between their representations. Finally, we employ
randomized smoothing to provably map similar individuals close together, in
turn ensuring that local robustness verification of the downstream application
results in end-to-end fairness certification. Our experimental evaluation on
challenging real-world image data demonstrates that our method increases
certified individual fairness by up to 90% without significantly affecting task
utility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Symmetry Detection and Shape Matching for Non-Rigid Point Cloud. (arXiv:2112.02713v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02713">
<div class="article-summary-box-inner">
<span><p>Despite the success of deep functional maps in non-rigid 3D shape matching,
there exists no learning framework that models both self-symmetry and shape
matching simultaneously. This is despite the fact that errors due to symmetry
mismatch are a major challenge in non-rigid shape matching. In this paper, we
propose a novel framework that simultaneously learns both self symmetry as well
as a pairwise map between a pair of shapes. Our key idea is to couple a self
symmetry map and a pairwise map through a regularization term that provides a
joint constraint on both of them, thereby, leading to more accurate maps. We
validate our method on several benchmarks where it outperforms many competitive
baselines on both tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I M Avatar: Implicit Morphable Head Avatars from Videos. (arXiv:2112.07471v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07471">
<div class="article-summary-box-inner">
<span><p>Traditional 3D morphable face models (3DMMs) provide fine-grained control
over expression but cannot easily capture geometric and appearance details.
Neural volumetric representations approach photorealism but are hard to animate
and do not generalize well to unseen expressions. To tackle this problem, we
propose IMavatar (Implicit Morphable avatar), a novel method for learning
implicit head avatars from monocular videos. Inspired by the fine-grained
control mechanisms afforded by conventional 3DMMs, we represent the expression-
and pose- related deformations via learned blendshapes and skinning fields.
These attributes are pose-independent and can be used to morph the canonical
geometry and texture fields given novel expression and pose parameters. We
employ ray marching and iterative root-finding to locate the canonical surface
intersection for each pixel. A key contribution is our novel analytical
gradient formulation that enables end-to-end training of IMavatars from videos.
We show quantitatively and qualitatively that our method improves geometry and
covers a more complete expression space compared to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Key Multimodal Backdoors for Visual Question Answering. (arXiv:2112.07668v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07668">
<div class="article-summary-box-inner">
<span><p>The success of deep learning has enabled advances in multimodal tasks that
require non-trivial fusion of multiple input domains. Although multimodal
models have shown potential in many problems, their increased complexity makes
them more vulnerable to attacks. A Backdoor (or Trojan) attack is a class of
security vulnerability wherein an attacker embeds a malicious secret behavior
into a network (e.g. targeted misclassification) that is activated when an
attacker-specified trigger is added to an input. In this work, we show that
multimodal networks are vulnerable to a novel type of attack that we refer to
as Dual-Key Multimodal Backdoors. This attack exploits the complex fusion
mechanisms used by state-of-the-art networks to embed backdoors that are both
effective and stealthy. Instead of using a single trigger, the proposed attack
embeds a trigger in each of the input modalities and activates the malicious
behavior only when both the triggers are present. We present an extensive study
of multimodal backdoors on the Visual Question Answering (VQA) task with
multiple architectures and visual feature backbones. A major challenge in
embedding backdoors in VQA models is that most models use visual features
extracted from a fixed pretrained object detector. This is challenging for the
attacker as the detector can distort or ignore the visual trigger entirely,
which leads to models where backdoors are over-reliant on the language trigger.
We tackle this problem by proposing a visual trigger optimization strategy
designed for pretrained object detectors. Through this method, we create
Dual-Key Backdoors with over a 98% attack success rate while only poisoning 1%
of the training data. Finally, we release TrojVQA, a large collection of clean
and trojan VQA models to enable research in defending against multimodal
backdoors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPTS: Single-Point Text Spotting. (arXiv:2112.07917v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07917">
<div class="article-summary-box-inner">
<span><p>Existing scene text spotting (i.e., end-to-end text detection and
recognition) methods rely on costly bounding box annotations (e.g., text-line,
word-level, or character-level bounding boxes). For the first time, we
demonstrate that training scene text spotting models can be achieved with an
extremely low-cost annotation of a single-point for each instance. We propose
an end-to-end scene text spotting method that tackles scene text spotting as a
sequence prediction task. Given an image as input, we formulate the desired
detection and recognition results as a sequence of discrete tokens and use an
auto-regressive Transformer to predict the sequence. The proposed method is
simple yet effective, which can achieve state-of-the-art results on widely used
benchmarks. Most significantly, we show that the performance is not very
sensitive to the positions of the point annotation, meaning that it can be much
easier to be annotated or even be automatically generated than the bounding box
that requires precise positions. We believe that such a pioneer attempt
indicates a significant opportunity for scene text spotting applications of a
much larger scale than previously possible. The code will be publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Wanderings of Odysseus in 3D Scenes. (arXiv:2112.09251v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09251">
<div class="article-summary-box-inner">
<span><p>Our goal is to populate digital environments, in which digital humans have
diverse body shapes, move perpetually, and have plausible body-scene contact.
The core challenge is to generate realistic, controllable, and infinitely long
motions for diverse 3D bodies. To this end, we propose generative motion
primitives via body surface markers, or GAMMA in short. In our solution, we
decompose the long-term motion into a time sequence of motion primitives. We
exploit body surface markers and conditional variational autoencoder to model
each motion primitive, and generate long-term motion by implementing the
generative model recursively. To control the motion to reach a goal, we apply a
policy network to explore the generative model's latent space and use a
tree-based search to preserve the motion quality during testing. Experiments
show that our method can produce more realistic and controllable motion than
state-of-the-art data-driven methods. With conventional path-finding
algorithms, the generated human bodies can realistically move long distances
for a long period of time in the scene. Code is released for research purposes
at: \url{https://yz-cnsdqz.github.io/eigenmotion/GAMMA/}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Queries for Efficient Local Attention. (arXiv:2112.11435v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11435">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) serve as powerful vision models. Unlike
convolutional neural networks, which dominated vision research in previous
years, vision transformers enjoy the ability to capture long-range dependencies
in the data. Nonetheless, an integral part of any transformer architecture, the
self-attention mechanism, suffers from high latency and inefficient memory
utilization, making it less suitable for high-resolution input images. To
alleviate these shortcomings, hierarchical vision models locally employ
self-attention on non-interleaving windows. This relaxation reduces the
complexity to be linear in the input size; however, it limits the cross-window
interaction, hurting the model performance. In this paper, we propose a new
shift-invariant local attention layer, called query and attend (QnA), that
aggregates the input locally in an overlapping manner, much like convolutions.
The key idea behind QnA is to introduce learned queries, which allow fast and
efficient implementation. We verify the effectiveness of our layer by
incorporating it into a hierarchical vision transformer model. We show
improvements in speed and memory complexity while achieving comparable accuracy
with state-of-the-art models. Finally, our layer scales especially well with
window size, requiring up-to x10 less memory while being up-to x5 faster than
existing methods. The code is publicly available at
\url{https://github.com/moabarar/qna}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross Modal Retrieval with Querybank Normalisation. (arXiv:2112.12777v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12777">
<div class="article-summary-box-inner">
<span><p>Profiting from large-scale training datasets, advances in neural architecture
design and efficient inference, joint embeddings have become the dominant
approach for tackling cross-modal retrieval. In this work we first show that,
despite their effectiveness, state-of-the-art joint embeddings suffer
significantly from the longstanding "hubness problem" in which a small number
of gallery embeddings form the nearest neighbours of many queries. Drawing
inspiration from the NLP literature, we formulate a simple but effective
framework called Querybank Normalisation (QB-Norm) that re-normalises query
similarities to account for hubs in the embedding space. QB-Norm improves
retrieval performance without requiring retraining. Differently from prior
work, we show that QB-Norm works effectively without concurrent access to any
test set queries. Within the QB-Norm framework, we also propose a novel
similarity normalisation method, the Dynamic Inverted Softmax, that is
significantly more robust than existing approaches. We showcase QB-Norm across
a range of cross modal retrieval models and benchmarks where it consistently
enhances strong baselines beyond the state of the art. Code is available at
https://vladbogo.github.io/QB-Norm/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch. (arXiv:2201.09367v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09367">
<div class="article-summary-box-inner">
<span><p>The freeform architectural modeling process often involves two important
stages: concept design and digital modeling. In the first stage, architects
usually sketch the overall 3D shape and the panel layout on a physical or
digital paper briefly. In the second stage, a digital 3D model is created using
the sketch as a reference. The digital model needs to incorporate geometric
requirements for its components, such as the planarity of panels due to
consideration of construction costs, which can make the modeling process more
challenging. In this work, we present a novel sketch-based system to bridge the
concept design and digital modeling of freeform roof-like shapes represented as
planar quadrilateral (PQ) meshes. Our system allows the user to sketch the
surface boundary and contour lines under axonometric projection and supports
the sketching of occluded regions. In addition, the user can sketch feature
lines to provide directional guidance to the PQ mesh layout. Given the 2D
sketch input, we propose a deep neural network to infer in real-time the
underlying surface shape along with a dense conjugate direction field, both of
which are used to extract the final PQ mesh. To train and validate our network,
we generate a large synthetic dataset that mimics architect sketching of
freeform quadrilateral patches. The effectiveness and usability of our system
are demonstrated with quantitative and qualitative evaluation as well as user
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning. (arXiv:2201.09671v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09671">
<div class="article-summary-box-inner">
<span><p>Since frequent severe droughts are lengthening the dry season in the Amazon
Rainforest, it is important to detect wildfires promptly and forecast possible
spread for effective suppression response. Current wildfire detection models
are not versatile enough for the low-technology conditions of South American
hot spots. This deep learning study first trains a Fully Convolutional Neural
Network on Landsat 8 images of Ecuador and the Galapagos, using Green and
Short-wave Infrared bands to predict pixel-level binary fire masks. This model
achieves a 0.962 validation F2 score and a 0.932 F2 score on test data from
Guyana and Suriname. Afterward, image segmentation is conducted on the Cirrus
band using K-Means Clustering to simplify continuous pixel values into three
discrete classes representing differing degrees of cirrus cloud contamination.
Three additional Convolutional Neural Networks are trained to conduct a
sensitivity analysis measuring the effect of simplified features on model
accuracy and train time. The Experimental model trained on the segmented cirrus
images provides a statistically significant decrease in train time compared to
the Control model trained on raw cirrus images, without compromising binary
accuracy. This proof of concept reveals that feature engineering can improve
the performance of wildfire detection models by lowering computational expense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Smart Glasses Dream of Sentimental Visions? Deep Emotionship Analysis for Eyewear Devices. (arXiv:2201.09933v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09933">
<div class="article-summary-box-inner">
<span><p>Emotion recognition in smart eyewear devices is highly valuable but
challenging. One key limitation of previous works is that the
expression-related information like facial or eye images is considered as the
only emotional evidence. However, emotional status is not isolated; it is
tightly associated with people's visual perceptions, especially those
sentimental ones. However, little work has examined such associations to better
illustrate the cause of different emotions. In this paper, we study the
emotionship analysis problem in eyewear systems, an ambitious task that
requires not only classifying the user's emotions but also semantically
understanding the potential cause of such emotions. To this end, we devise
EMOShip, a deep-learning-based eyewear system that can automatically detect the
wearer's emotional status and simultaneously analyze its associations with
semantic-level visual perceptions. Experimental studies with 20 participants
demonstrate that, thanks to the emotionship awareness, EMOShip not only
achieves superior emotion recognition accuracy over existing methods (80.2% vs.
69.4%), but also provides a valuable understanding of the cause of emotions.
Pilot studies with 20 participants further motivate the potential use of
EMOShip to empower emotion-aware applications, such as emotionship
self-reflection and emotionship life-logging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bootstrapped Representation Learning for Skeleton-Based Action Recognition. (arXiv:2202.02232v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02232">
<div class="article-summary-box-inner">
<span><p>In this work, we study self-supervised representation learning for 3D
skeleton-based action recognition. We extend Bootstrap Your Own Latent (BYOL)
for representation learning on skeleton sequence data and propose a new data
augmentation strategy including two asymmetric transformation pipelines. We
also introduce a multi-viewpoint sampling method that leverages multiple
viewing angles of the same action captured by different cameras. In the
semi-supervised setting, we show that the performance can be further improved
by knowledge distillation from wider networks, leveraging once more the
unlabeled samples. We conduct extensive experiments on the NTU-60 and NTU-120
datasets to demonstrate the performance of our proposed method. Our method
consistently outperforms the current state of the art on both linear evaluation
and semi-supervised benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comprehensive benchmark analysis for sand dust image reconstruction. (arXiv:2202.03031v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03031">
<div class="article-summary-box-inner">
<span><p>Numerous sand dust image enhancement algorithms have been proposed in recent
years. To our best acknowledge, however, most methods evaluated their
performance with no-reference way using few selected real-world images from
internet. It is unclear how to quantitatively analysis the performance of the
algorithms in a supervised way and how we could gauge the progress in the
field. Moreover, due to the absence of large-scale benchmark datasets, there
are no well-known reports of data-driven based method for sand dust image
enhancement up till now. To advance the development of deep learning-based
algorithms for sand dust image reconstruction, while enabling supervised
objective evaluation of algorithm performance. In this paper, we presented a
comprehensive perceptual study and analysis of real-world sand dust images,
then constructed a Sand-dust Image Reconstruction Benchmark (SIRB) for training
Convolutional Neural Networks (CNNs) and evaluating algorithms performance. In
addition, we adopted the existing image transformation neural network trained
on SIRB as baseline to illustrate the generalization of SIRB for training CNNs.
Finally, we conducted the qualitative and quantitative evaluation to
demonstrate the performance and limitations of the state-of-the-arts (SOTA),
which shed light on future research in sand dust image reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-Level Region Contrast for Object Detection Pre-Training. (arXiv:2202.04639v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04639">
<div class="article-summary-box-inner">
<span><p>In this work we present point-level region contrast, a self-supervised
pre-training approach for the task of object detection. This approach is
motivated by the two key factors in detection: localization and recognition.
While accurate localization favors models that operate at the pixel- or
point-level, correct recognition typically relies on a more holistic,
region-level view of objects. Incorporating this perspective in pre-training,
our approach performs contrastive learning by directly sampling individual
point pairs from different regions. Compared to an aggregated representation
per region, our approach is more robust to the change in input region quality,
and further enables us to implicitly improve initial region assignments via
online knowledge distillation during training. Both advantages are important
when dealing with imperfect regions encountered in the unsupervised setting.
Experiments show point-level region contrast improves on state-of-the-art
pre-training methods for object detection and segmentation across multiple
tasks and datasets, and we provide extensive ablation studies and
visualizations to aid understanding. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FILM: Frame Interpolation for Large Motion. (arXiv:2202.04901v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04901">
<div class="article-summary-box-inner">
<span><p>We present a frame interpolation algorithm that synthesizes multiple
intermediate frames from two input images with large in-between motion. Recent
methods use multiple networks to estimate optical flow or depth and a separate
network dedicated to frame synthesis. This is often complex and requires scarce
optical flow or depth ground-truth. In this work, we present a single unified
network, distinguished by a multi-scale feature extractor that shares weights
at all scales, and is trainable from frames alone. To synthesize crisp and
pleasing frames, we propose to optimize our network with the Gram matrix loss
that measures the correlation difference between feature maps. Our approach
outperforms state-of-the-art methods on the Xiph large motion benchmark. We
also achieve higher scores on Vimeo-90K, Middlebury and UCF101, when comparing
to methods that use perceptual losses. We study the effect of weight sharing
and of training with datasets of increasing motion range. Finally, we
demonstrate our model's effectiveness in synthesizing high quality and
temporally coherent videos on a challenging near-duplicate photos dataset.
Codes and pre-trained models are available at
https://github.com/google-research/frame-interpolation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAMMA Challenge:Glaucoma grAding from Multi-Modality imAges. (arXiv:2202.06511v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06511">
<div class="article-summary-box-inner">
<span><p>Color fundus photography and Optical Coherence Tomography (OCT) are the two
most cost-effective tools for glaucoma screening. Both two modalities of images
have prominent biomarkers to indicate glaucoma suspected. Clinically, it is
often recommended to take both of the screenings for a more accurate and
reliable diagnosis. However, although numerous algorithms are proposed based on
fundus images or OCT volumes in computer-aided diagnosis, there are still few
methods leveraging both of the modalities for the glaucoma assessment. Inspired
by the success of Retinal Fundus Glaucoma Challenge (REFUGE) we held
previously, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA)
Challenge to encourage the development of fundus \&amp; OCT-based glaucoma grading.
The primary task of the challenge is to grade glaucoma from both the 2D fundus
images and 3D OCT scanning volumes. As part of GAMMA, we have publicly released
a glaucoma annotated dataset with both 2D fundus color photography and 3D OCT
volumes, which is the first multi-modality dataset for glaucoma grading. In
addition, an evaluation framework is also established to evaluate the
performance of the submitted methods. During the challenge, 1272 results were
submitted, and finally, top-10 teams were selected to the final stage. We
analysis their results and summarize their methods in the paper. Since all
these teams submitted their source code in the challenge, a detailed ablation
study is also conducted to verify the effectiveness of the particular modules
proposed. We find many of the proposed techniques are practical for the
clinical diagnosis of glaucoma. As the first in-depth study of fundus \&amp; OCT
multi-modality glaucoma grading, we believe the GAMMA Challenge will be an
essential starting point for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">H4D: Human 4D Modeling by Learning Neural Compositional Representation. (arXiv:2203.01247v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01247">
<div class="article-summary-box-inner">
<span><p>Despite the impressive results achieved by deep learning based 3D
reconstruction, the techniques of directly learning to model 4D human captures
with detailed geometry have been less studied. This work presents a novel
framework that can effectively learn a compact and compositional representation
for dynamic human by exploiting the human body prior from the widely used SMPL
parametric model. Particularly, our representation, named H4D, represents a
dynamic 3D human over a temporal span with the SMPL parameters of shape and
initial pose, and latent codes encoding motion and auxiliary information. A
simple yet effective linear motion model is proposed to provide a rough and
regularized motion estimation, followed by per-frame compensation for pose and
geometry details with the residual encoded in the auxiliary code. Technically,
we introduce novel GRU-based architectures to facilitate learning and improve
the representation capability. Extensive experiments demonstrate our method is
not only efficacy in recovering dynamic human with accurate motion and detailed
geometry, but also amenable to various 4D human related tasks, including motion
retargeting, motion completion and future prediction. Please check out the
project page for video and code: https://boyanjiang.github.io/H4D/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05297">
<div class="article-summary-box-inner">
<span><p>Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11480">
<div class="article-summary-box-inner">
<span><p>Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11692">
<div class="article-summary-box-inner">
<span><p>We describe here the panoptic segmentation method we devised for our
participation in the CoNIC: Colon Nuclei Identification and Counting Challenge
at ISBI 2022. Key features of our method are a weighted loss specifically
engineered for semantic segmentation of highly imbalanced cell types, and a
state-of-the art nuclei instance segmentation model, which we combine in a
Hovernet-like architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R3M: A Universal Visual Representation for Robot Manipulation. (arXiv:2203.12601v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12601">
<div class="article-summary-box-inner">
<span><p>We study how visual representations pre-trained on diverse human video data
can enable data-efficient learning of downstream robotic manipulation tasks.
Concretely, we pre-train a visual representation using the Ego4D human video
dataset using a combination of time-contrastive learning, video-language
alignment, and an L1 penalty to encourage sparse and compact representations.
The resulting representation, R3M, can be used as a frozen perception module
for downstream policy learning. Across a suite of 12 simulated robot
manipulation tasks, we find that R3M improves task success by over 20% compared
to training from scratch and by over 10% compared to state-of-the-art visual
representations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika
Panda arm to learn a range of manipulation tasks in a real, cluttered apartment
given just 20 demonstrations. Code and pre-trained models are available at
https://tinyurl.com/robotr3m.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships. (arXiv:2203.14260v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14260">
<div class="article-summary-box-inner">
<span><p>Understanding realistic visual scene images together with language
descriptions is a fundamental task towards generic visual understanding.
Previous works have shown compelling comprehensive results by building
hierarchical structures for visual scenes (e.g., scene graphs) and natural
languages (e.g., dependency trees), individually. However, how to construct a
joint vision-language (VL) structure has barely been investigated. More
challenging but worthwhile, we introduce a new task that targets on inducing
such a joint VL structure in an unsupervised manner. Our goal is to bridge the
visual scene graphs and linguistic dependency trees seamlessly. Due to the lack
of VL structural data, we start by building a new dataset VLParse. Rather than
using labor-intensive labeling from scratch, we propose an automatic alignment
procedure to produce coarse structures followed by human refinement to produce
high-quality ones. Moreover, we benchmark our dataset by proposing a
contrastive learning (CL)-based framework VLGAE, short for Vision-Language
Graph Autoencoder. Our model obtains superior performance on two derived tasks,
i.e., language grammar induction and VL phrase grounding. Ablations show the
effectiveness of both visual cues and dependency relationships on fine-grained
VL structure construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nested Collaborative Learning for Long-Tailed Visual Recognition. (arXiv:2203.15359v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15359">
<div class="article-summary-box-inner">
<span><p>The networks trained on the long-tailed dataset vary remarkably, despite the
same training settings, which shows the great uncertainty in long-tailed
learning. To alleviate the uncertainty, we propose a Nested Collaborative
Learning (NCL), which tackles the problem by collaboratively learning multiple
experts together. NCL consists of two core components, namely Nested Individual
Learning (NIL) and Nested Balanced Online Distillation (NBOD), which focus on
the individual supervised learning for each single expert and the knowledge
transferring among multiple experts, respectively. To learn representations
more thoroughly, both NIL and NBOD are formulated in a nested way, in which the
learning is conducted on not just all categories from a full perspective but
some hard categories from a partial perspective. Regarding the learning in the
partial perspective, we specifically select the negative categories with high
predicted scores as the hard categories by using a proposed Hard Category
Mining (HCM). In the NCL, the learning from two perspectives is nested, highly
related and complementary, and helps the network to capture not only global and
robust features but also meticulous distinguishing ability. Moreover,
self-supervision is further utilized for feature enhancement. Extensive
experiments manifest the superiority of our method with outperforming the
state-of-the-art whether by using a single model or an ensemble.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ball 3D Localization From A Single Calibrated Image. (arXiv:2204.00003v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00003">
<div class="article-summary-box-inner">
<span><p>Ball 3D localization in team sports has various applications including
automatic offside detection in soccer, or shot release localization in
basketball. Today, this task is either resolved by using expensive multi-views
setups, or by restricting the analysis to ballistic trajectories. In this work,
we propose to address the task on a single image from a calibrated monocular
camera by estimating ball diameter in pixels and use the knowledge of real ball
diameter in meters. This approach is suitable for any game situation where the
ball is (even partly) visible. To achieve this, we use a small neural network
trained on image patches around candidates generated by a conventional ball
detector. Besides predicting ball diameter, our network outputs the confidence
of having a ball in the image patch. Validations on 3 basketball datasets
reveals that our model gives remarkable predictions on ball 3D localization. In
addition, through its confidence output, our model improves the detection rate
by filtering the candidates produced by the detector. The contributions of this
work are (i) the first model to address 3D ball localization on a single image,
(ii) an effective method for ball 3D annotation from single calibrated images,
(iii) a high quality 3D ball evaluation dataset annotated from a single
viewpoint. In addition, the code to reproduce this research is be made freely
available at https://github.com/gabriel-vanzandycke/deepsport.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Learning of Feature Extraction and Cost Aggregation for Semantic Correspondence. (arXiv:2204.02164v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02164">
<div class="article-summary-box-inner">
<span><p>Establishing dense correspondences across semantically similar images is one
of the challenging tasks due to the significant intra-class variations and
background clutters. To solve these problems, numerous methods have been
proposed, focused on learning feature extractor or cost aggregation
independently, which yields sub-optimal performance. In this paper, we propose
a novel framework for jointly learning feature extraction and cost aggregation
for semantic correspondence. By exploiting the pseudo labels from each module,
the networks consisting of feature extraction and cost aggregation modules are
simultaneously learned in a boosting fashion. Moreover, to ignore unreliable
pseudo labels, we present a confidence-aware contrastive loss function for
learning the networks in a weakly-supervised manner. We demonstrate our
competitive results on standard benchmarks for semantic correspondence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M$^2$BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Birds-Eye View Representation. (arXiv:2204.05088v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05088">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose M$^2$BEV, a unified framework that jointly performs
3D object detection and map segmentation in the Birds Eye View~(BEV) space with
multi-camera image inputs. Unlike the majority of previous works which
separately process detection and segmentation, M$^2$BEV infers both tasks with
a unified model and improves efficiency. M$^2$BEV efficiently transforms
multi-view 2D image features into the 3D BEV feature in ego-car coordinates.
Such BEV representation is important as it enables different tasks to share a
single encoder. Our framework further contains four important designs that
benefit both accuracy and efficiency: (1) An efficient BEV encoder design that
reduces the spatial dimension of a voxel feature map. (2) A dynamic box
assignment strategy that uses learning-to-match to assign ground-truth 3D boxes
with anchors. (3) A BEV centerness re-weighting that reinforces with larger
weights for more distant predictions, and (4) Large-scale 2D detection
pre-training and auxiliary supervision. We show that these designs
significantly benefit the ill-posed camera-based 3D perception tasks where
depth information is missing. M$^2$BEV is memory efficient, allowing
significantly higher resolution images as input, with faster inference speed.
Experiments on nuScenes show that M$^2$BEV achieves state-of-the-art results in
both 3D object detection and BEV segmentation, with the best single model
achieving 42.5 mAP and 57.0 mIoU in these two tasks, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transparent Shape from Single Polarization Images. (arXiv:2204.06331v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06331">
<div class="article-summary-box-inner">
<span><p>This paper presents a data-driven approach for transparent shape from
polarization. Due to the inherent high transmittance, the previous shape from
polarization(SfP) methods based on specular reflection model have difficulty in
estimating transparent shape, and the lack of datasets for transparent SfP also
limits the application of the data-driven approach. Hence, we construct the
transparent SfP dataset which consists of both synthetic and real-world
datasets. To determine the reliability of the physics-based reflection model,
we define the physics-based prior confidence by exploiting the inherent fault
of polarization information, then we propose a multi-branch fusion network to
embed the confidence. Experimental results show that our approach outperforms
other SfP methods. Compared with the previous method, the mean and median
angular error of our approach are reduced from $19.00^\circ$ and $14.91^\circ$
to $16.72^\circ$ and $13.36^\circ$, and the accuracy $11.25^\circ, 22.5^\circ,
30^\circ$ are improved from $38.36\%, 77.36\%, 87.48\%$ to $45.51\%, 78.86\%,
89.98\%$, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06718">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. As the core of CNNs, the image
convolution operation helps CNNs to get good performance on image-related
tasks. However, the image convolution is hard to be implemented and
parallelized. This paper proposes a novel neural network model, namely CEMNet,
which can be trained in the frequency domain. The most important motivation of
this research is that we can use the straightforward element-wise
multiplication operation to replace the image convolution in the frequency
domain based on the Cross-Correlation Theorem. We further introduce a Weight
Fixation mechanism to alleviate the problem of over-fitting, and analyze the
working behavior of Batch Normalization, Leaky ReLU, and Dropout in the
frequency domain to design their counterparts for CEMNet. Also, to deal with
complex inputs brought by Discrete Fourier Transform, we design a two-branches
network structure for CEMNet. Experimental results imply that CEMNet achieves
good performance on MNIST and CIFAR-10 databases. To the best of our knowledge,
CEMNet is the first model trained in Fourier Domain that achieves more than
70\% validation accuracy on CIFAR-10 database.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis. (arXiv:2204.06929v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06929">
<div class="article-summary-box-inner">
<span><p>Ultrasound (US) imaging is widely used for anatomical structure inspection in
clinical diagnosis. The training of new sonographers and deep learning based
algorithms for US image analysis usually requires a large amount of data.
However, obtaining and labeling large-scale US imaging data are not easy tasks,
especially for diseases with low incidence. Realistic US image synthesis can
alleviate this problem to a great extent. In this paper, we propose a
generative adversarial network (GAN) based image synthesis framework. Our main
contributions include: 1) we present the first work that can synthesize
realistic B-mode US images with high-resolution and customized texture editing
features; 2) to enhance structural details of generated images, we propose to
introduce auxiliary sketch guidance into a conditional GAN. We superpose the
edge sketch onto the object mask and use the composite mask as the network
input; 3) to generate high-resolution US images, we adopt a progressive
training strategy to gradually generate high-resolution images from
low-resolution images. In addition, a feature loss is proposed to minimize the
difference of high-level features between the generated and real images, which
further improves the quality of generated images; 4) the proposed US image
synthesis method is quite universal and can also be generalized to the US
images of other anatomical structures besides the three ones tested in our
study (lung, hip joint, and ovary); 5) extensive experiments on three large US
image datasets are conducted to validate our method. Ablation studies,
customized texture editing, user studies, and segmentation tests demonstrate
promising results of our method in synthesizing realistic US images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Mechanism based Cognition-level Scene Understanding. (arXiv:2204.08027v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08027">
<div class="article-summary-box-inner">
<span><p>Given a question-image input, the Visual Commonsense Reasoning (VCR) model
can predict an answer with the corresponding rationale, which requires
inference ability from the real world. The VCR task, which calls for exploiting
the multi-source information as well as learning different levels of
understanding and extensive commonsense knowledge, is a cognition-level scene
understanding task. The VCR task has aroused researchers' interest due to its
wide range of applications, including visual question answering, automated
vehicle systems, and clinical decision support. Previous approaches to solving
the VCR task generally rely on pre-training or exploiting memory with long
dependency relationship encoded models. However, these approaches suffer from a
lack of generalizability and losing information in long sequences. In this
paper, we propose a parallel attention-based cognitive VCR network PAVCR, which
fuses visual-textual information efficiently and encodes semantic information
in parallel to enable the model to capture rich information for cognition-level
inference. Extensive experiments show that the proposed model yields
significant improvements over existing methods on the benchmark VCR dataset.
Moreover, the proposed model provides intuitive interpretation into visual
commonsense reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Super-Resolution. (arXiv:2204.08192v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08192">
<div class="article-summary-box-inner">
<span><p>Super-Resolution is the technique to improve the quality of a low-resolution
photo by boosting its plausible resolution. The computer vision community has
extensively explored the area of Super-Resolution. However, previous
Super-Resolution methods require vast amounts of data for training which
becomes problematic in domains where very few low-resolution, high-resolution
pairs might be available. One such area is statistical downscaling, where
super-resolution is increasingly being used to obtain high-resolution climate
information from low-resolution data. Acquiring high-resolution climate data is
extremely expensive and challenging. To reduce the cost of generating
high-resolution climate information, Super-Resolution algorithms should be able
to train with a limited number of low-resolution, high-resolution pairs. This
paper tries to solve the aforementioned problem by introducing a
semi-supervised way to perform super-resolution that can generate sharp,
high-resolution images with as few as 500 paired examples. The proposed
semi-supervised technique can be used as a plug-and-play module with any
supervised GAN-based Super-Resolution method to enhance its performance. We
quantitatively and qualitatively analyze the performance of the proposed model
and compare it with completely supervised methods as well as other unsupervised
techniques. Comprehensive evaluations show the superiority of our method over
other methods on different metrics. We also offer the applicability of our
approach in statistical downscaling to obtain high-resolution climate images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHSCNet: A Multimodal Hierarchical Shot-aware Convolutional Network for Video Summarization. (arXiv:2204.08352v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08352">
<div class="article-summary-box-inner">
<span><p>Video summarization intends to produce a concise video summary by effectively
capturing and combining the most informative parts of the whole content.
Existing approaches for video summarization regard the task as a frame-wise
keyframe selection problem and generally construct the frame-wise
representation by combining the long-range temporal dependency with the
unimodal or bimodal information. However, the optimal video summaries need to
reflect the most valuable keyframe with its own information, and one with
semantic power of the whole content. Thus, it is critical to construct a more
powerful and robust frame-wise representation and predict the frame-level
importance score in a fair and comprehensive manner. To tackle the above
issues, we propose a multimodal hierarchical shot-aware convolutional network,
denoted as MHSCNet, to enhance the frame-wise representation via combining the
comprehensive available multimodal information. Specifically, we design a
hierarchical ShotConv network to incorporate the adaptive shot-aware
frame-level representation by considering the short-range and long-range
temporal dependency. Based on the learned shot-aware representations, MHSCNet
can predict the frame-level importance score in the local and global view of
the video. Extensive experiments on two standard video summarization datasets
demonstrate that our proposed method consistently outperforms state-of-the-art
baselines. Source code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. (arXiv:2204.08387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08387">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-training techniques have achieved remarkable progress in
Document AI. Most multimodal pre-trained models use a masked language modeling
objective to learn bidirectional representations on the text modality, but they
differ in pre-training objectives for the image modality. This discrepancy adds
difficulty to multimodal representation learning. In this paper, we propose
LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified
text and image masking. Additionally, LayoutLMv3 is pre-trained with a
word-patch alignment objective to learn cross-modal alignment by predicting
whether the corresponding image patch of a text word is masked. The simple
unified architecture and training objectives make LayoutLMv3 a general-purpose
pre-trained model for both text-centric and image-centric Document AI tasks.
Experimental results show that LayoutLMv3 achieves state-of-the-art performance
not only in text-centric tasks, including form understanding, receipt
understanding, and document visual question answering, but also in
image-centric tasks such as document image classification and document layout
analysis. The code and models are publicly available at
https://aka.ms/layoutlmv3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Consistency Regularization for Semi-supervised Change Detection in Remote Sensing Images. (arXiv:2204.08454v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08454">
<div class="article-summary-box-inner">
<span><p>Remote-sensing (RS) Change Detection (CD) aims to detect "changes of
interest" from co-registered bi-temporal images. The performance of existing
deep supervised CD methods is attributed to the large amounts of annotated data
used to train the networks. However, annotating large amounts of remote sensing
images is labor-intensive and expensive, particularly with bi-temporal images,
as it requires pixel-wise comparisons by a human expert. On the other hand, we
often have access to unlimited unlabeled multi-temporal RS imagery thanks to
ever-increasing earth observation programs. In this paper, we propose a simple
yet effective way to leverage the information from unlabeled bi-temporal images
to improve the performance of CD approaches. More specifically, we propose a
semi-supervised CD model in which we formulate an unsupervised CD loss in
addition to the supervised Cross-Entropy (CE) loss by constraining the output
change probability map of a given unlabeled bi-temporal image pair to be
consistent under the small random perturbations applied on the deep feature
difference map that is obtained by subtracting their latent feature
representations. Experiments conducted on two publicly available CD datasets
show that the proposed semi-supervised CD method can reach closer to the
performance of supervised CD even with access to as little as 10% of the
annotated training data. Code available at https://github.com/wgcban/SemiCD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Land Cover Classification from Remote Sensing Images Based on Multi-Scale Fully Convolutional Network. (arXiv:2008.00168v2 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.00168">
<div class="article-summary-box-inner">
<span><p>In this paper, a Multi-Scale Fully Convolutional Network (MSFCN) with
multi-scale convolutional kernel is proposed to exploit discriminative
representations from two-dimensional (2D) satellite images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Binary Segmentation of Seismic Facies Using Encoder-Decoder Neural Networks. (arXiv:2012.03675v1 [eess.IV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03675">
<div class="article-summary-box-inner">
<span><p>The interpretation of seismic data is vital for characterizing sediments'
shape in areas of geological study. In seismic interpretation, deep learning
becomes useful for reducing the dependence on handcrafted facies segmentation
geometry and the time required to study geological areas. This work presents a
Deep Neural Network for Facies Segmentation (DNFS) to obtain state-of-the-art
results for seismic facies segmentation. DNFS is trained using a combination of
cross-entropy and Jaccard loss functions. Our results show that DNFS obtains
highly detailed predictions for seismic facies segmentation using fewer
parameters than StNet and U-Net.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-20 23:10:37.987426758 UTC">2022-04-20 23:10:37 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>