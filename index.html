<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-22T01:30:00Z">04-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimize_Prime@DravidianLangTech-ACL2022: Abusive Comment Detection in Tamil. (arXiv:2204.09675v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09675">
<div class="article-summary-box-inner">
<span><p>This paper tries to address the problem of abusive comment detection in
low-resource indic languages. Abusive comments are statements that are
offensive to a person or a group of people. These comments are targeted toward
individuals belonging to specific ethnicities, genders, caste, race, sexuality,
etc. Abusive Comment Detection is a significant problem, especially with the
recent rise in social media users. This paper presents the approach used by our
team - Optimize_Prime, in the ACL 2022 shared task "Abusive Comment Detection
in Tamil." This task detects and classifies YouTube comments in Tamil and
Tamil- English Codemixed format into multiple categories. We have used three
methods to optimize our results: Ensemble models, Recurrent Neural Networks,
and Transformers. In the Tamil data, MuRIL and XLM-RoBERTA were our best
performing models with a macro-averaged f1 score of 0.43. Furthermore, for the
Code-mixed data, MuRIL and M-BERT provided sub-lime results, with a
macro-averaged f1 score of 0.45.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">yosm: A new yoruba sentiment corpus for movie reviews. (arXiv:2204.09711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09711">
<div class="article-summary-box-inner">
<span><p>A movie that is thoroughly enjoyed and recommended by an individual might be
hated by another. One characteristic of humans is the ability to have feelings
which could be positive or negative. To automatically classify and study human
feelings, an aspect of natural language processing, sentiment analysis and
opinion mining were designed to understand human feelings regarding several
issues which could affect a product, a social media platforms, government, or
societal discussions or even movies. Several works on sentiment analysis have
been done on high resource languages while low resources languages like Yoruba
have been sidelined. Due to the scarcity of datasets and linguistic
architectures that will suit low resource languages, African languages "low
resource languages" have been ignored and not fully explored. For this reason,
our attention is placed on Yoruba to explore sentiment analysis on reviews of
Nigerian movies. The data comprised 1500 movie reviews that were sourced from
IMDB, Rotten Tomatoes, Letterboxd, Cinemapointer and Nollyrated. We develop
sentiment classification models using the state-of-the-art pre-trained language
models like mBERT and AfriBERTa to classify the movie reviews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Pre-Trained Transformers for Biologically Inspired Design. (arXiv:2204.09714v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09714">
<div class="article-summary-box-inner">
<span><p>Biological systems in nature have evolved for millions of years to adapt and
survive the environment. Many features they developed can be inspirational and
beneficial for solving technical problems in modern industries. This leads to a
novel form of design-by-analogy called bio-inspired design (BID). Although BID
as a design method has been proven beneficial, the gap between biology and
engineering continuously hinders designers from effectively applying the
method. Therefore, we explore the recent advance of artificial intelligence
(AI) for a computational approach to bridge the gap. This paper proposes a
generative design approach based on the pre-trained language model (PLM) to
automatically retrieve and map biological analogy and generate BID in the form
of natural language. The latest generative pre-trained transformer, namely
GPT-3, is used as the base PLM. Three types of design concept generators are
identified and fine-tuned from the PLM according to the looseness of the
problem space representation. Machine evaluators are also fine-tuned to assess
the correlation between the domains within the generated BID concepts. The
approach is then tested via a case study in which the fine-tuned models are
applied to generate and evaluate light-weighted flying car concepts inspired by
nature. The results show our approach can generate BID concepts with good
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Language Model Size in Cross-Device Federated Learning. (arXiv:2204.09715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09715">
<div class="article-summary-box-inner">
<span><p>Most studies in cross-device federated learning focus on small models, due to
the server-client communication and on-device computation bottlenecks. In this
work, we leverage various techniques for mitigating these bottlenecks to train
larger language models in cross-device federated learning. With systematic
applications of partial model training, quantization, efficient transfer
learning, and communication-efficient optimizers, we are able to train a $21$M
parameter Transformer that achieves the same perplexity as that of a similarly
sized LSTM with $\sim10\times$ smaller client-to-server communication cost and
$11\%$ lower perplexity than smaller LSTMs commonly studied in literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Specific Fine-tuning of Denoising Sequence-to-Sequence Models for Natural Language Summarization. (arXiv:2204.09716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09716">
<div class="article-summary-box-inner">
<span><p>Summarization of long-form text data is a problem especially pertinent in
knowledge economy jobs such as medicine and finance, that require continuously
remaining informed on a sophisticated and evolving body of knowledge. As such,
isolating and summarizing key content automatically using Natural Language
Processing (NLP) techniques holds the potential for extensive time savings in
these industries. We explore applications of a state-of-the-art NLP model
(BART), and explore strategies for tuning it to optimal performance using data
augmentation and various fine-tuning strategies. We show that our end-to-end
fine-tuning approach can result in a 5-6\% absolute ROUGE-1 improvement over an
out-of-the-box pre-trained BART summarizer when tested on domain specific data,
and make available our end-to-end pipeline to achieve these results on finance,
medical, or other user-specified domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LSTM-RASA Based Agri Farm Assistant for Farmers. (arXiv:2204.09717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09717">
<div class="article-summary-box-inner">
<span><p>The application of Deep Learning and Natural Language based ChatBots are
growing rapidly in recent years. They are used in many fields like customer
support, reservation system and as personal assistant. The Enterprises are
using such ChatBots to serve their customers in a better and efficient manner.
Even after such technological advancement, the expert advice does not reach the
farmers on timely manner. The farmers are still largely dependent on their
peers knowledge in solving the problems they face in their field. These
technologies have not been effectively used to give the required information to
farmers on timely manner. This project aims to implement a closed domain
ChatBot for the field of Agriculture Farmers Assistant. Farmers can have
conversation with the Chatbot and get the expert advice in their field. Farmers
Assistant is based on RASA Open Source Framework. The Chatbot identifies the
intent and entity from user utterances and retrieve the remedy from the
database and share it with the user. We tested the Bot with existing data and
it showed promising results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching Writers to Content Writing Tasks. (arXiv:2204.09718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09718">
<div class="article-summary-box-inner">
<span><p>Businesses need content. In various forms and formats and for varied
purposes. In fact, the content marketing industry is set to be worth $412.88
billion by the end of 2021. However, according to the Content Marketing
Institute, creating engaging content is the #1 challenge that marketers face
today. We under-stand that producing great content requires great writers who
understand the business and can weave their message into reader (and search
engine) friendly content. In this project, the team has attempted to bridge the
gap between writers and projects by using AI and ML tools. We used NLP
techniques to analyze thou-sands of publicly available business articles
(corpora) to extract various defining factors for each writing sample. Through
this project we aim to automate the highly time-consuming, and often biased
task of manually shortlisting the most suitable writer for a given content
writing requirement. We believe that a tool like this will have far reaching
positive implications for both parties - businesses looking for suitable talent
for niche writing jobs as well as experienced writers and Subject Matter
Experts (SMEs) wanting to lend their services to content marketing projects.
The business gets the content they need, the content writer/ SME gets a chance
to leverage his or her talent, while the reader gets authentic content that
adds real value.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Progress in Conversational AI. (arXiv:2204.09719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09719">
<div class="article-summary-box-inner">
<span><p>Conversational artificial intelligence (AI) is becoming an increasingly
popular topic among industry and academia. With the fast development of neural
network-based models, a lot of neural-based conversational AI system are
developed. We will provide a brief review of the recent progress in the
Conversational AI, including the commonly adopted techniques, notable works,
famous competitions from academia and industry and widely used datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes. (arXiv:2204.09722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09722">
<div class="article-summary-box-inner">
<span><p>Recent causal probing literature reveals when language models and syntactic
probes use similar representations. Such techniques may yield "false negative"
causality results: models may use representations of syntax, but probes may
have learned to use redundant encodings of the same syntactic information. We
demonstrate that models do encode syntactic information redundantly and
introduce a new probe design that guides probes to consider all syntactic
information present in embeddings. Using these probes, we find evidence for the
use of syntax in models where prior methods did not, allowing us to boost model
performance by injecting syntactic information into representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Res-CNN-BiLSTM Network for overcoming Mental Health Disturbances caused due to Cyberbullying through Social Media. (arXiv:2204.09738v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09738">
<div class="article-summary-box-inner">
<span><p>Mental Health Disturbance has many reasons and cyberbullying is one of the
major causes that does exploitation using social media as an instrument. The
cyberbullying is done on the basis of Religion, Ethnicity, Age and Gender which
is a sensitive psychological issue. This can be addressed using Natural
Language Processing with Deep Learning, since social media is the medium and it
generates massive form of data in textual form. Such data can be leveraged to
find the semantics and derive what type of cyberbullying is done and who are
the people involved for early measures. Since deriving semantics is essential
we proposed a Hybrid Deep Learning Model named 1-Dimensional
CNN-Bidirectional-LSTMs with Residuals shortly known as Res-CNN-BiLSTM. In this
paper we have proposed the architecture and compared its performance with
different approaches of Embedding Deep Learning Algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations. (arXiv:2204.09781v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09781">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has been severely impacting global society since
December 2019. Massive research has been undertaken to understand the
characteristics of the virus and design vaccines and drugs. The related
findings have been reported in biomedical literature at a rate of about 10,000
articles on COVID-19 per month. Such rapid growth significantly challenges
manual curation and interpretation. For instance, LitCovid is a literature
database of COVID-19-related articles in PubMed, which has accumulated more
than 200,000 articles with millions of accesses each month by users worldwide.
One primary curation task is to assign up to eight topics (e.g., Diagnosis and
Treatment) to the articles in LitCovid. Despite the continuing advances in
biomedical text mining methods, few have been dedicated to topic annotations in
COVID-19 literature. To close the gap, we organized the BioCreative LitCovid
track to call for a community effort to tackle automated topic annotation for
COVID-19 literature. The BioCreative LitCovid dataset, consisting of over
30,000 articles with manually reviewed topics, was created for training and
testing. It is one of the largest multilabel classification datasets in
biomedical scientific literature. 19 teams worldwide participated and made 80
submissions in total. Most teams used hybrid systems based on transformers. The
highest performing submissions achieved 0.8875, 0.9181, and 0.9394 for macro
F1-score, micro F1-score, and instance-based F1-score, respectively. The level
of participation and results demonstrate a successful track and help close the
gap between dataset curation and method development. The dataset is publicly
available via https://ftp.ncbi.nlm.nih.gov/pub/lu/LitCovid/biocreative/ for
benchmarking and further development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09817">
<div class="article-summary-box-inner">
<span><p>Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision-language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Masked Image Reconstruction Network for Document-level Relation Extraction. (arXiv:2204.09851v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09851">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction aims to extract relations among entities
within a document. Compared with its sentence-level counterpart, Document-level
relation extraction requires inference over multiple sentences to extract
complex relational triples. Previous research normally complete reasoning
through information propagation on the mention-level or entity-level
document-graphs, regardless of the correlations between the relationships. In
this paper, we propose a novel Document-level Relation Extraction model based
on a Masked Image Reconstruction network (DRE-MIR), which models inference as a
masked image reconstruction problem to capture the correlations between
relationships. Specifically, we first leverage an encoder module to get the
features of entities and construct the entity-pair matrix based on the
features. After that, we look on the entity-pair matrix as an image and then
randomly mask it and restore it through an inference module to capture the
correlations between the relationships. We evaluate our model on three public
document-level relation extraction datasets, i.e. DocRED, CDR, and GDA.
Experimental results demonstrate that our model achieves state-of-the-art
performance on these three datasets and has excellent robustness against the
noises during the inference process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Model-Agnostic Data Manipulation Method for Persona-based Dialogue Generation. (arXiv:2204.09867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09867">
<div class="article-summary-box-inner">
<span><p>Towards building intelligent dialogue agents, there has been a growing
interest in introducing explicit personas in generation models. However, with
limited persona-based dialogue data at hand, it may be difficult to train a
dialogue generation model well. We point out that the data challenges of this
generation task lie in two aspects: first, it is expensive to scale up current
persona-based dialogue datasets; second, each data sample in this task is more
complex to learn with than conventional dialogue data. To alleviate the above
data issues, we propose a data manipulation method, which is model-agnostic to
be packed with any persona-based dialogue generation model to improve its
performance. The original training samples will first be distilled and thus
expected to be fitted more easily. Next, we show various effective ways that
can diversify such easier distilled data. A given base model will then be
trained via the constructed data curricula, i.e. first on augmented distilled
samples and then on original ones. Experiments illustrate the superiority of
our method with two strong base dialogue models (Transformer encoder-decoder
and GPT2).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics. (arXiv:2204.09874v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09874">
<div class="article-summary-box-inner">
<span><p>Recent work incorporates pre-trained word embeddings such as BERT embeddings
into Neural Topic Models (NTMs), generating highly coherent topics. However,
with high-quality contextualized document representations, do we really need
sophisticated neural models to obtain coherent and interpretable topics? In
this paper, we conduct thorough experiments showing that directly clustering
high-quality sentence embeddings with an appropriate word selecting method can
generate more coherent and diverse topics than NTMs, achieving also higher
efficiency and simplicity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Attention-Based Model for Predicting Contextual Informativeness and Curriculum Learning Applications. (arXiv:2204.09885v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09885">
<div class="article-summary-box-inner">
<span><p>Both humans and machines learn the meaning of unknown words through
contextual information in a sentence, but not all contexts are equally helpful
for learning. We introduce an effective method for capturing the level of
contextual informativeness with respect to a given target word. Our study makes
three main contributions. First, we develop models for estimating contextual
informativeness, focusing on the instructional aspect of sentences. Our
attention-based approach using pre-trained embeddings demonstrates
state-of-the-art performance on our single-context dataset and an existing
multi-sentence context dataset. Second, we show how our model identifies key
contextual elements in a sentence that are likely to contribute most to a
reader's understanding of the target word. Third, we examine how our contextual
informativeness model, originally developed for vocabulary learning
applications for students, can be used for developing better training curricula
for word embedding models in batch learning and few-shot machine learning
settings. We believe our results open new possibilities for applications that
support language learning for both human and machine learners
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spurious Correlations in Reference-Free Evaluation of Text Generation. (arXiv:2204.09890v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09890">
<div class="article-summary-box-inner">
<span><p>Model-based, reference-free evaluation metrics have been proposed as a fast
and cost-effective approach to evaluate Natural Language Generation (NLG)
systems. Despite promising recent results, we find evidence that reference-free
evaluation metrics of summarization and dialog generation may be relying on
spurious correlations with measures such as word overlap, perplexity, and
length. We further observe that for text summarization, these metrics have high
error rates when ranking current state-of-the-art abstractive summarization
systems. We demonstrate that these errors can be mitigated by explicitly
designing evaluation metrics to avoid spurious features in reference-free
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task recommendation system for scientific papers with high-way networks. (arXiv:2204.09930v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09930">
<div class="article-summary-box-inner">
<span><p>Finding and selecting the most relevant scientific papers from a large number
of papers written in a research community is one of the key challenges for
researchers these days. As we know, much information around research interest
for scholars and academicians belongs to papers they read. Analysis and
extracting contextual features from these papers could help us to suggest the
most related paper to them. In this paper, we present a multi-task
recommendation system (RS) that predicts a paper recommendation and generates
its meta-data such as keywords. The system is implemented as a three-stage deep
neural network encoder that tries to maps longer sequences of text to an
embedding vector and learns simultaneously to predict the recommendation rate
for a particular user and the paper's keywords. The motivation behind this
approach is that the paper's topics expressed as keywords are a useful
predictor of preferences of researchers. To achieve this goal, we use a system
combination of RNNs, Highway and Convolutional Neural Networks to train
end-to-end a context-aware collaborative matrix. Our application uses Highway
networks to train the system very deep, combine the benefits of RNN and CNN to
find the most important factor and make latent representation. Highway Networks
allow us to enhance the traditional RNN and CNN pipeline by learning more
sophisticated semantic structural representations. Using this method we can
also overcome the cold start problem and learn latent features over large
sequences of text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on Twitter (BEAR). (arXiv:2204.09952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09952">
<div class="article-summary-box-inner">
<span><p>Text mining and information extraction for the medical domain has focused on
scientific text generated by researchers. However, their direct access to
individual patient experiences or patient-doctor interactions can be limited.
Information provided on social media, e.g., by patients and their relatives,
complements the knowledge in scientific text. It reflects the patient's journey
and their subjective perspective on the process of developing symptoms, being
diagnosed and offered a treatment, being cured or learning to live with a
medical condition. The value of this type of data is therefore twofold:
Firstly, it offers direct access to people's perspectives. Secondly, it might
cover information that is not available elsewhere, including self-treatment or
self-diagnoses. Named entity recognition and relation extraction are methods to
structure information that is available in unstructured text. However, existing
medical social media corpora focused on a comparably small set of entities and
relations and particular domains, rather than putting the patient into the
center of analyses. With this paper we contribute a corpus with a rich set of
annotation layers following the motivation to uncover and model patients'
journeys and experiences in more detail. We label 14 entity classes (incl.
environmental factors, diagnostics, biochemical processes, patients'
quality-of-life descriptions, pathogens, medical conditions, and treatments)
and 20 relation classes (e.g., prevents, influences, interactions, causes) most
of which have not been considered before for social media data. The publicly
available dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000
relation annotations. In a corpus analysis we find that over 80 % of documents
contain relevant entities. Over 50 % of tweets express relations which we
consider essential for uncovering patients' narratives about their journeys.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEAM-Atreides at SemEval-2022 Task 11: On leveraging data augmentation and ensemble to recognize complex Named Entities in Bangla. (arXiv:2204.09964v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09964">
<div class="article-summary-box-inner">
<span><p>Many areas, such as the biological and healthcare domain, artistic works, and
organization names, have nested, overlapping, discontinuous entity mentions
that may even be syntactically or semantically ambiguous in practice.
Traditional sequence tagging algorithms are unable to recognize these complex
mentions because they may violate the assumptions upon which sequence tagging
schemes are founded. In this paper, we describe our contribution to SemEval
2022 Task 11 on identifying such complex Named Entities. We have leveraged the
ensemble of multiple ELECTRA-based models that were exclusively pretrained on
the Bangla language with the performance of ELECTRA-based models pretrained on
English to achieve competitive performance on the Track-11. Besides providing a
system description, we will also present the outcomes of our experiments on
architectural decisions, dataset augmentations, and post-competition findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Standing on the Shoulders of Giant Frozen Language Models. (arXiv:2204.10019v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10019">
<div class="article-summary-box-inner">
<span><p>Huge pretrained language models (LMs) have demonstrated surprisingly good
zero-shot capabilities on a wide variety of tasks. This gives rise to the
appealing vision of a single, versatile model with a wide range of
functionalities across disparate applications. However, current leading
techniques for leveraging a "frozen" LM -- i.e., leaving its weights untouched
-- still often underperform fine-tuning approaches which modify these weights
in a task-dependent way. Those, in turn, suffer forgetfulness and compromise
versatility, suggesting a tradeoff between performance and versatility. The
main message of this paper is that current frozen-model techniques such as
prompt tuning are only the tip of the iceberg, and more powerful methods for
leveraging frozen LMs can do just as well as fine tuning in challenging domains
without sacrificing the underlying model's versatility. To demonstrate this, we
introduce three novel methods for leveraging frozen models: input-dependent
prompt tuning, frozen readers, and recursive LMs, each of which vastly improves
on current frozen-model approaches. Indeed, some of our methods even outperform
fine-tuning approaches in domains currently dominated by the latter. The
computational cost of each method is higher than that of existing frozen model
methods, but still negligible relative to a single pass through a huge frozen
LM. Each of these methods constitutes a meaningful contribution in its own
right, but by presenting these contributions together we aim to convince the
reader of a broader message that goes beyond the details of any given method:
that frozen models have untapped potential and that fine-tuning is often
unnecessary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding. (arXiv:2204.10050v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10050">
<div class="article-summary-box-inner">
<span><p>This paper presents the shared task on Multilingual Idiomaticity Detection
and Sentence Embedding, which consists of two subtasks: (a) a binary
classification one aimed at identifying whether a sentence contains an
idiomatic expression, and (b) a task based on semantic text similarity which
requires the model to adequately represent potentially idiomatic expressions in
context. Each subtask includes different settings regarding the amount of
training data. Besides the task description, this paper introduces the datasets
in English, Portuguese, and Galician and their annotation procedure, the
evaluation metrics, and a summary of the participant systems and their results.
The task had close to 100 registered participants organised into twenty five
teams making over 650 and 150 submissions in the practice and evaluation phases
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying and Characterizing Active Citizens who Refute Misinformation in Social Media. (arXiv:2204.10080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10080">
<div class="article-summary-box-inner">
<span><p>The phenomenon of misinformation spreading in social media has developed a
new form of active citizens who focus on tackling the problem by refuting posts
that might contain misinformation. Automatically identifying and characterizing
the behavior of such active citizens in social media is an important task in
computational social science for complementing studies in misinformation
analysis. In this paper, we study this task across different social media
platforms (i.e., Twitter and Weibo) and languages (i.e., English and Chinese)
for the first time. To this end, (1) we develop and make publicly available a
new dataset of Weibo users mapped into one of the two categories (i.e.,
misinformation posters or active citizens); (2) we evaluate a battery of
supervised models on our new Weibo dataset and an existing Twitter dataset
which we repurpose for the task; and (3) we present an extensive analysis of
the differences in language use between the two user categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OTExtSum: Extractive Text Summarisation with Optimal Transport. (arXiv:2204.10086v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10086">
<div class="article-summary-box-inner">
<span><p>Extractive text summarisation aims to select salient sentences from a
document to form a short yet informative summary. While learning-based methods
have achieved promising results, they have several limitations, such as
dependence on expensive training and lack of interpretability. Therefore, in
this paper, we propose a novel non-learning-based method by for the first time
formulating text summarisation as an Optimal Transport (OT) problem, namely
Optimal Transport Extractive Summariser (OTExtSum). Optimal sentence extraction
is conceptualised as obtaining an optimal summary that minimises the
transportation cost to a given document regarding their semantic distributions.
Such a cost is defined by the Wasserstein distance and used to measure the
summary's semantic coverage of the original document. Comprehensive experiments
on four challenging and widely used datasets - MultiNews, PubMed, BillSum, and
CNN/DM demonstrate that our proposed method outperforms the state-of-the-art
non-learning-based methods and several recent learning-based methods in terms
of the ROUGE metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gated Multimodal Fusion with Contrastive Learning for Turn-taking Prediction in Human-robot Dialogue. (arXiv:2204.10172v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10172">
<div class="article-summary-box-inner">
<span><p>Turn-taking, aiming to decide when the next speaker can start talking, is an
essential component in building human-robot spoken dialogue systems. Previous
studies indicate that multimodal cues can facilitate this challenging task.
However, due to the paucity of public multimodal datasets, current methods are
mostly limited to either utilizing unimodal features or simplistic multimodal
ensemble models. Besides, the inherent class imbalance in real scenario, e.g.
sentence ending with short pause will be mostly regarded as the end of turn,
also poses great challenge to the turn-taking decision. In this paper, we first
collect a large-scale annotated corpus for turn-taking with over 5,000 real
human-robot dialogues in speech and text modalities. Then, a novel gated
multimodal fusion mechanism is devised to utilize various information
seamlessly for turn-taking prediction. More importantly, to tackle the data
imbalance issue, we design a simple yet effective data augmentation method to
construct negative instances without supervision and apply contrastive learning
to obtain better feature representations. Extensive experiments are conducted
and the results demonstrate the superiority and competitiveness of our model
over several state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Script Knowledge from Pre-Trained Models. (arXiv:2204.10176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10176">
<div class="article-summary-box-inner">
<span><p>Script knowledge is critical for humans to understand the broad daily tasks
and routine activities in the world. Recently researchers have explored the
large-scale pre-trained language models (PLMs) to perform various script
related tasks, such as story generation, temporal ordering of event, future
event prediction and so on. However, it's still not well studied in terms of
how well the PLMs capture the script knowledge. To answer this question, we
design three probing tasks: inclusive sub-event selection, starting sub-event
selection and temporal ordering to investigate the capabilities of PLMs with
and without fine-tuning. The three probing tasks can be further used to
automatically induce a script for each main event given all the possible
sub-events. Taking BERT as a case study, by analyzing its performance on script
induction as well as each individual probing task, we conclude that the
stereotypical temporal knowledge among the sub-events is well captured in BERT,
however the inclusive or starting sub-event knowledge is barely encoded.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doctor XAvIer: Explainable Diagnosis using Physician-Patient Dialogues and XAI Evaluation. (arXiv:2204.10178v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10178">
<div class="article-summary-box-inner">
<span><p>We introduce Doctor XAvIer, a BERT-based diagnostic system that extracts
relevant clinical data from transcribed patient-doctor dialogues and explains
predictions using feature attribution methods. We present a novel performance
plot and evaluation metric for feature attribution methods: Feature Attribution
Dropping (FAD) curve and its Normalized Area Under the Curve (N-AUC). FAD curve
analysis shows that integrated gradients outperforms Shapley values in
explaining diagnosis classification. Doctor XAvIer outperforms the baseline
with 0.97 F1-score in named entity recognition and symptom pertinence
classification and 0.91 F1-score in diagnosis classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WordAlchemy: A transformer-based Reverse Dictionary. (arXiv:2204.10181v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10181">
<div class="article-summary-box-inner">
<span><p>A reverse dictionary takes a target word's description as input and returns
the words that fit the description. Reverse Dictionaries are useful for new
language learners, anomia patients, and for solving common tip-of-the-tongue
problems (lethologica). Currently, there does not exist any Reverse Dictionary
provider with support for any Indian Language. We present a novel open-source
cross-lingual reverse dictionary system with support for Indian languages. In
this paper, we propose a transformer-based deep learning approach to tackle the
limitations faced by the existing systems using the mT5 model. This
architecture uses the Translation Language Modeling (TLM) technique, rather
than the conventional BERT's Masked Language Modeling (MLM) technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social Media Sentiment Analysis for Cryptocurrency Market Prediction. (arXiv:2204.10185v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10185">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the usability of different natural language
processing models for the sentiment analysis of social media applied to
financial market prediction, using the cryptocurrency domain as a reference. We
study how the different sentiment metrics are correlated with the price
movements of Bitcoin. For this purpose, we explore different methods to
calculate the sentiment metrics from a text finding most of them not very
accurate for this prediction task. We find that one of the models outperforms
more than 20 other public ones and makes it possible to fine-tune it
efficiently given its interpretable nature. Thus we confirm that interpretable
artificial intelligence and natural language processing methods might be more
valuable practically than non-explainable and non-interpretable ones. In the
end, we analyse potential causal connections between the different sentiment
metrics and the price movements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Topic Modeling of Psychotherapy Sessions. (arXiv:2204.10189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10189">
<div class="article-summary-box-inner">
<span><p>In this work, we compare different neural topic modeling methods in learning
the topical propensities of different psychiatric conditions from the
psychotherapy session transcripts parsed from speech recordings. We also
incorporate temporal modeling to put this additional interpretability to action
by parsing out topic similarities as a time series in a turn-level resolution.
We believe this topic modeling framework can offer interpretable insights for
the therapist to optimally decide his or her strategy and improve the
psychotherapy effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating User Radicalization: A Novel Dataset for Identifying Fine-Grained Temporal Shifts in Opinion. (arXiv:2204.10190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10190">
<div class="article-summary-box-inner">
<span><p>There is an increasing need for the ability to model fine-grained opinion
shifts of social media users, as concerns about the potential polarizing social
effects increase. However, the lack of publicly available datasets that are
suitable for the task presents a major challenge. In this paper, we introduce
an innovative annotated dataset for modeling subtle opinion fluctuations and
detecting fine-grained stances. The dataset includes a sufficient amount of
stance polarity and intensity labels per user over time and within entire
conversational threads, thus making subtle opinion fluctuations detectable both
in long term and in short term. All posts are annotated by non-experts and a
significant portion of the data is also annotated by experts. We provide a
strategy for recruiting suitable non-experts. Our analysis of the
inter-annotator agreements shows that the resulting annotations obtained from
the majority vote of the non-experts are of comparable quality to the
annotations of the experts. We provide analyses of the stance evolution in
short term and long term levels, a comparison of language usage between users
with vacillating and resolute attitudes, and fine-grained stance detection
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residue-Based Natural Language Adversarial Attack Detection. (arXiv:2204.10192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10192">
<div class="article-summary-box-inner">
<span><p>Deep learning based systems are susceptible to adversarial attacks, where a
small, imperceptible change at the input alters the model prediction. However,
to date the majority of the approaches to detect these attacks have been
designed for image processing systems. Many popular image adversarial detection
approaches are able to identify adversarial examples from embedding feature
spaces, whilst in the NLP domain existing state of the art detection approaches
solely focus on input text features, without consideration of model embedding
spaces. This work examines what differences result when porting these image
designed strategies to Natural Language Processing (NLP) tasks - these
detectors are found to not port over well. This is expected as NLP systems have
a very different form of input: discrete and sequential in nature, rather than
the continuous and fixed size inputs for images. As an equivalent model-focused
NLP detection approach, this work proposes a simple sentence-embedding
"residue" based detector to identify adversarial examples. On many tasks, it
out-performs ported image domain detectors and recent state of the art NLP
specific detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Structure based Query Graph Prediction for Question Answering over Knowledge Graph. (arXiv:2204.10194v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10194">
<div class="article-summary-box-inner">
<span><p>Building query graphs from natural language questions is an important step in
complex question answering over knowledge graph (Complex KGQA). In general, a
question can be correctly answered if its query graph is built correctly and
the right answer is then retrieved by issuing the query graph against the KG.
Therefore, this paper focuses on query graph generation from natural language
questions. Existing approaches for query graph generation ignore the semantic
structure of a question, resulting in a large number of noisy query graph
candidates that undermine prediction accuracies. In this paper, we define six
semantic structures from common questions in KGQA and develop a novel
Structure-BERT to predict the semantic structure of a question. By doing so, we
can first filter out noisy candidate query graphs by the predicted semantic
structures, and then rank the remaining candidates with a BERT-based ranking
model. Extensive experiments on two popular benchmarks MetaQA and
WebQuestionsSP (WSP) demonstrate the effectiveness of our method as compared to
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IIITDWD-ShankarB@ Dravidian-CodeMixi-HASOC2021: mBERT based model for identification of offensive content in south Indian languages. (arXiv:2204.10195v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10195">
<div class="article-summary-box-inner">
<span><p>In recent years, there has been a lot of focus on offensive content. The
amount of offensive content generated by social media is increasing at an
alarming rate. This created a greater need to address this issue than ever
before. To address these issues, the organizers of "Dravidian-Code Mixed
HASOC-2020" have created two challenges. Task 1 involves identifying offensive
content in Malayalam data, whereas Task 2 includes Malayalam and Tamil Code
Mixed Sentences. Our team participated in Task 2. In our suggested model, we
experiment with multilingual BERT to extract features, and three different
classifiers are used on extracted features. Our model received a weighted F1
score of 0.70 for Malayalam data and was ranked fifth; we also received a
weighted F1 score of 0.573 for Tamil Code Mixed data and were ranked eleventh.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Hate Speech Detection from Bengali Memes and Texts. (arXiv:2204.10196v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10196">
<div class="article-summary-box-inner">
<span><p>Numerous works have been proposed to employ machine learning (ML) and deep
learning (DL) techniques to utilize textual data from social media for
anti-social behavior analysis such as cyberbullying, fake news propagation, and
hate speech mainly for highly resourced languages like English. However,
despite having a lot of diversity and millions of native speakers, some
languages such as Bengali are under-resourced, which is due to a lack of
computational resources for natural language processing (NLP). Like English,
Bengali social media content also includes images along with texts (e.g.,
multimodal contents are posted by embedding short texts into images on
Facebook), only the textual data is not enough to judge them (e.g., to
determine they are hate speech). In those cases, images might give extra
context to properly judge. This paper is about hate speech detection from
multimodal Bengali memes and texts. We prepared the only multimodal hate speech
detection dataset1 for a kind of problem for Bengali. We train several neural
architectures (i.e., neural networks like Bi-LSTM/Conv-LSTM with word
embeddings, EfficientNet + transformer architectures such as monolingual Bangla
BERT, multilingual BERT-cased/uncased, and XLM-RoBERTa) jointly analyze textual
and visual information for hate speech detection. The Conv-LSTM and XLM-RoBERTa
models performed best for texts, yielding F1 scores of 0.78 and 0.82,
respectively. As of memes, ResNet152 and DenseNet201 models yield F1 scores of
0.78 and 0.7, respectively. The multimodal fusion of mBERT-uncased +
EfficientNet-B1 performed the best, yielding an F1 score of 0.80. Our study
suggests that memes are moderately useful for hate speech detection in Bengali,
but none of the multimodal models outperform unimodal models analyzing only
textual data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Aware Language Modeling for Goal-Oriented Dialogue Systems. (arXiv:2204.10198v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10198">
<div class="article-summary-box-inner">
<span><p>Goal-oriented dialogue systems face a trade-off between fluent language
generation and task-specific control. While supervised learning with large
language models is capable of producing realistic text, how to steer such
responses towards completing a specific task without sacrificing language
quality remains an open question. In this work, we formulate goal-oriented
dialogue as a partially observed Markov decision process, interpreting the
language model as a representation of both the dynamics and the policy. This
view allows us to extend techniques from learning-based control, such as task
relabeling, to derive a simple and effective method to finetune language models
in a goal-aware way, leading to significantly improved task performance. We
additionally introduce a number of training strategies that serve to better
focus the model on the task at hand. We evaluate our method, Context-Aware
Language Models (CALM), on a practical flight-booking task using AirDialogue.
Empirically, CALM outperforms the state-of-the-art method by 7% in terms of
task success, matching human-level task performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploratory Study on Code Attention in BERT. (arXiv:2204.10200v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10200">
<div class="article-summary-box-inner">
<span><p>Many recent models in software engineering introduced deep neural models
based on the Transformer architecture or use transformer-based Pre-trained
Language Models (PLM) trained on code. Although these models achieve the state
of the arts results in many downstream tasks such as code summarization and bug
detection, they are based on Transformer and PLM, which are mainly studied in
the Natural Language Processing (NLP) field. The current studies rely on the
reasoning and practices from NLP for these models in code, despite the
differences between natural languages and programming languages. There is also
limited literature on explaining how code is modeled.
</p>
<p>Here, we investigate the attention behavior of PLM on code and compare it
with natural language. We pre-trained BERT, a Transformer based PLM, on code
and explored what kind of information it learns, both semantic and syntactic.
We run several experiments to analyze the attention values of code constructs
on each other and what BERT learns in each layer. Our analyses show that BERT
pays more attention to syntactic entities, specifically identifiers and
separators, in contrast to the most attended token [CLS] in NLP. This
observation motivated us to leverage identifiers to represent the code sequence
instead of the [CLS] token when used for code clone detection. Our results show
that employing embeddings from identifiers increases the performance of BERT by
605% and 4% F1-score in its lower layers and the upper layers, respectively.
When identifiers' embeddings are used in CodeBERT, a code-based PLM, the
performance is improved by 21-24% in the F1-score of clone detection. The
findings can benefit the research community by using code-specific
representations instead of applying the common embeddings used in NLP, and open
new directions for developing smaller models with similar performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Usage-based learning of grammatical categories. (arXiv:2204.10201v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10201">
<div class="article-summary-box-inner">
<span><p>Human languages use a wide range of grammatical categories to constrain which
words or phrases can fill certain slots in grammatical patterns and to express
additional meanings, such as tense or aspect, through morpho-syntactic means.
These grammatical categories, which are most often language-specific and
changing over time, are difficult to define and learn. This paper raises the
question how these categories can be acquired and where they have come from. We
explore a usage-based approach. This means that categories and grammatical
constructions are selected and aligned by their success in language
interactions. We report on a multi-agent experiment in which agents are endowed
with mechanisms for understanding and producing utterances as well as
mechanisms for expanding their inventories using a meta-level learning process
based on pro- and anti-unification. We show that a categorial type network
which has scores based on the success in a language interaction leads to the
spontaneous formation of grammatical categories in tandem with the formation of
grammatical patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical Text by Leveraging External Knowledge. (arXiv:2204.10202v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10202">
<div class="article-summary-box-inner">
<span><p>Extracting phenotypes from clinical text has been shown to be useful for a
variety of clinical use cases such as identifying patients with rare diseases.
However, reasoning with numerical values remains challenging for phenotyping in
clinical text, for example, temperature 102F representing Fever. Current
state-of-the-art phenotyping models are able to detect general phenotypes, but
perform poorly when they detect phenotypes requiring numerical reasoning. We
present a novel unsupervised methodology leveraging external knowledge and
contextualized word embeddings from ClinicalBERT for numerical reasoning in a
variety of phenotypic contexts. Comparing against unsupervised benchmarks, it
shows a substantial performance improvement with absolute gains on generalized
Recall and F1 scores up to 79% and 71%, respectively. In the supervised
setting, it also surpasses the performance of alternative approaches with
absolute gains on generalized Recall and F1 scores up to 70% and 44%,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics. (arXiv:2204.10206v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10206">
<div class="article-summary-box-inner">
<span><p>Question answering-based summarization evaluation metrics must automatically
determine whether the QA model's prediction is correct or not, a task known as
answer verification. In this work, we benchmark the lexical answer verification
methods which have been used by current QA-based metrics as well as two more
sophisticated text comparison methods, BERTScore and LERC. We find that LERC
out-performs the other methods in some settings while remaining statistically
indistinguishable from lexical overlap in others. However, our experiments
reveal that improved verification performance does not necessarily translate to
overall QA-based metric quality: In some scenarios, using a worse verification
method -- or using none at all -- has comparable performance to using the best
verification method, a result that we attribute to properties of the datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics. (arXiv:2204.10216v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10216">
<div class="article-summary-box-inner">
<span><p>How reliably an automatic summarization evaluation metric replicates human
judgments of summary quality is quantified by system-level correlations. We
identify two ways in which the definition of the system-level correlation is
inconsistent with how metrics are used to evaluate systems in practice and
propose changes to rectify this disconnect. First, we calculate the system
score for an automatic metric using the full test set instead of the subset of
summaries judged by humans, which is currently standard practice. We
demonstrate how this small change leads to more precise estimates of
system-level correlations. Second, we propose to calculate correlations only on
pairs of systems that are separated by small differences in automatic scores
which are commonly observed in practice. This allows us to demonstrate that our
best estimate of the correlation of ROUGE to human judgments is near 0 in
realistic scenarios. The results from the analyses point to the need to collect
more high-quality human judgments and to improve automatic metrics when
differences in system scores are small.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Query-Based Summarization of Crisis-Related Social Media: An Abstractive Approach Using Transformers. (arXiv:2204.10230v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10230">
<div class="article-summary-box-inner">
<span><p>Relevant and timely information collected from social media during crises can
be an invaluable resource for emergency management. However, extracting this
information remains a challenging task, particularly when dealing with social
media postings in multiple languages. This work proposes a cross-lingual method
for retrieving and summarizing crisis-relevant information from social media
postings. We describe a uniform way of expressing various information needs
through structured queries and a way of creating summaries answering those
information needs. The method is based on multilingual transformers embeddings.
Queries are written in one of the languages supported by the embeddings, and
the extracted sentences can be in any of the other languages supported.
Abstractive summaries are created by transformers. The evaluation, done by
crowdsourcing evaluators and emergency management experts, and carried out on
collections extracted from Twitter during five large-scale disasters spanning
ten languages, shows the flexibility of our approach. The generated summaries
are regarded as more focused, structured, and coherent than existing
state-of-the-art methods, and experts compare them favorably against summaries
created by existing, state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpaceE: Knowledge Graph Embedding by Relational Linear Transformation in the Entity Space. (arXiv:2204.10245v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10245">
<div class="article-summary-box-inner">
<span><p>Translation distance based knowledge graph embedding (KGE) methods, such as
TransE and RotatE, model the relation in knowledge graphs as translation or
rotation in the vector space. Both translation and rotation are injective; that
is, the translation or rotation of different vectors results in different
results. In knowledge graphs, different entities may have a relation with the
same entity; for example, many actors starred in one movie. Such a
non-injective relation pattern cannot be well modeled by the translation or
rotation operations in existing translation distance based KGE methods. To
tackle the challenge, we propose a translation distance-based KGE method called
SpaceE to model relations as linear transformations. The proposed SpaceE embeds
both entities and relations in knowledge graphs as matrices and SpaceE
naturally models non-injective relations with singular linear transformations.
We theoretically demonstrate that SpaceE is a fully expressive model with the
ability to infer multiple desired relation patterns, including symmetry,
skew-symmetry, inversion, Abelian composition, and non-Abelian composition.
Experimental results on link prediction datasets illustrate that SpaceE
substantially outperforms many previous translation distance based knowledge
graph embedding methods, especially on datasets with many non-injective
relations. The code is available based on the PaddlePaddle deep learning
platform https://www.paddlepaddle.org.cn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns. (arXiv:2204.10281v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10281">
<div class="article-summary-box-inner">
<span><p>Gender-neutral pronouns have recently been introduced in many languages to a)
include non-binary people and b) as a generic singular. Recent results from
psycho-linguistics suggest that gender-neutral pronouns (in Swedish) are not
associated with human processing difficulties. This, we show, is in sharp
contrast with automated processing. We show that gender-neutral pronouns in
Danish, English, and Swedish are associated with higher perplexity, more
dispersed attention patterns, and worse downstream performance. We argue that
such conservativity in language models may limit widespread adoption of
gender-neutral pronouns and must therefore be resolved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Revise References for Faithful Summarization. (arXiv:2204.10290v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10290">
<div class="article-summary-box-inner">
<span><p>In many real-world scenarios with naturally occurring datasets, reference
summaries are noisy and contain information that cannot be inferred from the
source text. On large news corpora, removing low quality samples has been shown
to reduce model hallucinations. Yet, this method is largely untested for
smaller, noisier corpora. To improve reference quality while retaining all
data, we propose a new approach: to revise--not remove--unsupported reference
content. Without ground-truth supervision, we construct synthetic unsupported
alternatives to supported sentences and use contrastive learning to
discourage/encourage (un)faithful revisions. At inference, we vary style codes
to over-generate revisions of unsupported reference sentences and select a
final revision which balances faithfulness and abstraction. We extract a small
corpus from a noisy source--the Electronic Health Record (EHR)--for the task of
summarizing a hospital admission from multiple notes. Training models on
original, filtered, and revised references, we find (1) learning from revised
references reduces the hallucination rate substantially more than filtering
(18.4\% vs 3.8\%), (2) learning from abstractive (vs extractive) revisions
improves coherence, relevance, and faithfulness, (3) beyond redress of noisy
data, the revision task has standalone value for the task: as a pre-training
objective and as a post-hoc editor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hierarchical N-Gram Framework for Zero-Shot Link Prediction. (arXiv:2204.10293v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10293">
<div class="article-summary-box-inner">
<span><p>Due to the incompleteness of knowledge graphs (KGs), zero-shot link
prediction (ZSLP) which aims to predict unobserved relations in KGs has
attracted recent interest from researchers. A common solution is to use textual
features of relations (e.g., surface name or textual descriptions) as auxiliary
information to bridge the gap between seen and unseen relations. Current
approaches learn an embedding for each word token in the text. These methods
lack robustness as they suffer from the out-of-vocabulary (OOV) problem.
Meanwhile, models built on character n-grams have the capability of generating
expressive representations for OOV words. Thus, in this paper, we propose a
Hierarchical N-Gram framework for Zero-Shot Link Prediction (HNZSLP), which
considers the dependencies among character n-grams of the relation surface name
for ZSLP. Our approach works by first constructing a hierarchical n-gram graph
on the surface name to model the organizational structure of n-grams that leads
to the surface name. A GramTransformer, based on the Transformer is then
presented to model the hierarchical n-gram graph to construct the relation
embedding for ZSLP. Experimental results show the proposed HNZSLP achieved
state-of-the-art performance on two ZSLP datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings. (arXiv:2204.10298v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10298">
<div class="article-summary-box-inner">
<span><p>We propose DiffCSE, an unsupervised contrastive learning framework for
learning sentence embeddings. DiffCSE learns sentence embeddings that are
sensitive to the difference between the original sentence and an edited
sentence, where the edited sentence is obtained by stochastically masking out
the original sentence and then sampling from a masked language model. We show
that DiffSCE is an instance of equivariant contrastive learning (Dangovski et
al., 2021), which generalizes contrastive learning and learns representations
that are insensitive to certain types of augmentations and sensitive to other
"harmful" types of augmentations. Our experiments show that DiffCSE achieves
state-of-the-art results among unsupervised sentence representation learning
methods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic
textual similarity tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06022">
<div class="article-summary-box-inner">
<span><p>We propose a parameter sharing method for Transformers (Vaswani et al.,
2017). The proposed approach relaxes a widely used technique, which shares
parameters for one layer with all layers such as Universal Transformers
(Dehghani et al., 2019), to increase the efficiency in the computational time.
We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign
parameters to each layer. Experimental results show that the proposed
strategies are efficient in the parameter size and computational time.
Moreover, we indicate that the proposed strategies are also effective in the
configuration where we use many training data such as the recent WMT
competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out of Context: A New Clue for Context Modeling of Aspect-based Sentiment Analysis. (arXiv:2106.10816v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10816">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) aims to predict the sentiment
expressed in a review with respect to a given aspect. The core of ABSA is to
model the interaction between the context and given aspect to extract the
aspect-related information. In prior work, attention mechanisms and dependency
graph networks are commonly adopted to capture the relations between the
context and given aspect. And the weighted sum of context hidden states is used
as the final representation fed to the classifier. However, the information
related to the given aspect may be already discarded and adverse information
may be retained in the context modeling processes of existing models. This
problem cannot be solved by subsequent modules and there are two reasons:
first, their operations are conducted on the encoder-generated context hidden
states, whose value cannot change after the encoder; second, existing encoders
only consider the context while not the given aspect. To address this problem,
we argue the given aspect should be considered as a new clue out of context in
the context modeling process. As for solutions, we design several aspect-aware
context encoders based on different backbones: an aspect-aware LSTM and three
aspect-aware BERTs. They are dedicated to generate aspect-aware hidden states
which are tailored for ABSA task. In these aspect-aware context encoders, the
semantics of the given aspect is used to regulate the information flow.
Consequently, the aspect-related information can be retained and
aspect-irrelevant information can be excluded in the generated hidden states.
We conduct extensive experiments on several benchmark datasets with empirical
analysis, demonstrating the efficacies and advantages of our proposed
aspect-aware context encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation. (arXiv:2108.11626v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11626">
<div class="article-summary-box-inner">
<span><p>As the use of interactive machines grow, the task of Emotion Recognition in
Conversation (ERC) became more important. If the machine-generated sentences
reflect emotion, more human-like sympathetic conversations are possible. Since
emotion recognition in conversation is inaccurate if the previous utterances
are not taken into account, many studies reflect the dialogue context to
improve the performances. Many recent approaches show performance improvement
by combining knowledge into modules learned from external structured data.
However, structured data is difficult to access in non-English languages,
making it difficult to extend to other languages. Therefore, we extract the
pre-trained memory using the pre-trained language model as an extractor of
external knowledge. We introduce CoMPM, which combines the speaker's
pre-trained memory with the context model, and find that the pre-trained memory
significantly improves the performance of the context model. CoMPM achieves the
first or second performance on all data and is state-of-the-art among systems
that do not leverage structured data. In addition, our method shows that it can
be extended to other languages because structured knowledge is not required,
unlike previous methods. Our code is available on github
(https://github.com/rungjoo/CoMPM).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization. (arXiv:2108.13684v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13684">
<div class="article-summary-box-inner">
<span><p>Despite recent progress in abstractive summarization, systems still suffer
from faithfulness errors. While prior work has proposed models that improve
faithfulness, it is unclear whether the improvement comes from an increased
level of extractiveness of the model outputs as one naive way to improve
faithfulness is to make summarization models more extractive. In this work, we
present a framework for evaluating the effective faithfulness of summarization
systems, by generating a faithfulnessabstractiveness trade-off curve that
serves as a control at different operating points on the abstractiveness
spectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as
well as a recently proposed method for improving faithfulness, are both worse
than the control at the same level of abstractiveness. Finally, we learn a
selector to identify the most faithful and abstractive summary for a given
document, and show that this system can attain higher faithfulness scores in
human evaluations while being more abstractive than the baseline system on two
datasets. Moreover, we show that our system is able to achieve a better
faithfulness-abstractiveness trade-off than the control at the same level of
abstractiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Prompt-Based Models Really Understand the Meaning of their Prompts?. (arXiv:2109.01247v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01247">
<div class="article-summary-box-inner">
<span><p>Recently, a boom of papers has shown extraordinary progress in zero-shot and
few-shot learning with various prompt-based models. It is commonly argued that
prompts help models to learn faster in the same way that humans learn faster
when provided with task instructions expressed in natural language. In this
study, we experiment with over 30 prompt templates manually written for natural
language inference (NLI). We find that models learn just as fast with many
prompts that are intentionally irrelevant or even pathologically misleading as
they do with instructively "good" prompts. Further, such patterns hold even for
models as large as 175 billion parameters (Brown et al., 2020) as well as the
recently proposed instruction-tuned models which are trained on hundreds of
prompts (Sanh et al., 2022). That is, instruction-tuned models often produce
good predictions with irrelevant and misleading prompts even at zero shots. In
sum, notwithstanding prompt-based models' impressive improvement, we find
evidence of serious limitations that question the degree to which such
improvement is derived from models understanding task instructions in ways
analogous to humans' use of task instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Packed Levitated Marker for Entity and Relation Extraction. (arXiv:2109.06067v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06067">
<div class="article-summary-box-inner">
<span><p>Recent entity and relation extraction works focus on investigating how to
obtain a better span representation from the pre-trained encoder. However, a
major limitation of existing works is that they ignore the interrelation
between spans (pairs). In this work, we propose a novel span representation
approach, named Packed Levitated Markers (PL-Marker), to consider the
interrelation between the spans (pairs) by strategically packing the markers in
the encoder. In particular, we propose a neighborhood-oriented packing
strategy, which considers the neighbor spans integrally to better model the
entity boundary information. Furthermore, for those more complicated span pair
classification tasks, we design a subject-oriented packing strategy, which
packs each subject and all its objects to model the interrelation between the
same-subject span pairs. The experimental results show that, with the enhanced
marker feature, our model advances baselines on six NER benchmarks, and obtains
a 4.1%-4.3% strict relation F1 improvement with higher speed over previous
state-of-the-art models on ACE04 and ACE05.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Attention Sparsity in Transformers. (arXiv:2109.12188v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12188">
<div class="article-summary-box-inner">
<span><p>Transformers' quadratic complexity with respect to the input sequence length
has motivated a body of work on efficient sparse approximations to softmax. An
alternative path, used by entmax transformers, consists of having built-in
exact sparse attention; however this approach still requires quadratic
computation. In this paper, we propose Sparsefinder, a simple model trained to
identify the sparsity pattern of entmax attention before computing it. We
experiment with three variants of our method, based on distances, quantization,
and clustering, on two tasks: machine translation (attention in the decoder)
and masked language modeling (encoder-only). Our work provides a new angle to
study model efficiency by doing extensive analysis of the tradeoff between the
sparsity and recall of the predicted attention graph. This allows for detailed
comparison between different models along their Pareto curves, important to
guide future benchmarks for sparse attention models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Sequence Training of Attention Models using Approximative Recombination. (arXiv:2110.09245v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09245">
<div class="article-summary-box-inner">
<span><p>Sequence discriminative training is a great tool to improve the performance
of an automatic speech recognition system. It does, however, necessitate a sum
over all possible word sequences, which is intractable to compute in practice.
Current state-of-the-art systems with unlimited label context circumvent this
problem by limiting the summation to an n-best list of relevant competing
hypotheses obtained from beam search.
</p>
<p>This work proposes to perform (approximative) recombinations of hypotheses
during beam search, if they share a common local history. The error that is
incurred by the approximation is analyzed and it is shown that using this
technique the effective beam size can be increased by several orders of
magnitude without significantly increasing the computational requirements.
Lastly, it is shown that this technique can be used to effectively perform
sequence discriminative training for attention-based encoder-decoder acoustic
models on the LibriSpeech task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surfer100: Generating Surveys From Web Resources on Wikipedia-style. (arXiv:2112.06377v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06377">
<div class="article-summary-box-inner">
<span><p>Fast-developing fields such as Artificial Intelligence (AI) often outpace the
efforts of encyclopedic sources such as Wikipedia, which either do not
completely cover recently-introduced topics or lack such content entirely. As a
result, methods for automatically producing content are valuable tools to
address this information overload. We show that recent advances in pretrained
language modeling can be combined for a two-stage extractive and abstractive
approach for Wikipedia lead paragraph generation. We extend this approach to
generate longer Wikipedia-style summaries with sections and examine how such
methods struggle in this application through detailed studies with 100
reference human-collected surveys. This is the first study on utilizing web
resources for long Wikipedia-style summaries to the best of our knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Curriculum Learning for Emotion Recognition in Conversation. (arXiv:2112.11718v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11718">
<div class="article-summary-box-inner">
<span><p>Emotion recognition in conversation (ERC) aims to detect the emotion label
for each utterance. Motivated by recent studies which have proven that feeding
training examples in a meaningful order rather than considering them randomly
can boost the performance of models, we propose an ERC-oriented hybrid
curriculum learning framework. Our framework consists of two curricula: (1)
conversation-level curriculum (CC); and (2) utterance-level curriculum (UC). In
CC, we construct a difficulty measurer based on "emotion shift" frequency
within a conversation, then the conversations are scheduled in an "easy to
hard" schema according to the difficulty score returned by the difficulty
measurer. For UC, it is implemented from an emotion-similarity perspective,
which progressively strengthens the model's ability in identifying the
confusing emotions. With the proposed model-agnostic hybrid curriculum learning
strategy, we observe significant performance boosts over a wide range of
existing ERC models and we are able to achieve new state-of-the-art results on
four public ERC datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiased Learning from Naturally Imbalanced Pseudo-Labels. (arXiv:2201.01490v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01490">
<div class="article-summary-box-inner">
<span><p>Pseudo-labels are confident predictions made on unlabeled target data by a
classifier trained on labeled source data. They are widely used for adapting a
model to unlabeled data, e.g., in a semi-supervised learning setting.
</p>
<p>Our key insight is that pseudo-labels are naturally imbalanced due to
intrinsic data similarity, even when a model is trained on balanced source data
and evaluated on balanced target data. If we address this previously unknown
imbalanced classification problem arising from pseudo-labels instead of
ground-truth training labels, we could remove model biases towards false
majorities created by pseudo-labels.
</p>
<p>We propose a novel and effective debiased learning method with pseudo-labels,
based on counterfactual reasoning and adaptive margins: The former removes the
classifier response bias, whereas the latter adjusts the margin of each class
according to the imbalance of pseudo-labels. Validated by extensive
experimentation, our simple debiased learning delivers significant accuracy
gains over the state-of-the-art on ImageNet-1K: 26% for semi-supervised
learning with 0.2% annotations and 9% for zero-shot learning. Our code is
available at: https://github.com/frank-xwang/debiased-pseudo-labeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Czech Grammar Error Correction with a Large and Diverse Corpus. (arXiv:2201.05590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05590">
<div class="article-summary-box-inner">
<span><p>We introduce a large and diverse Czech corpus annotated for grammatical error
correction (GEC) with the aim to contribute to the still scarce data resources
in this domain for languages other than English. The Grammar Error Correction
Corpus for Czech (GECCC) offers a variety of four domains, covering error
distributions ranging from high error density essays written by non-native
speakers, to website texts, where errors are expected to be much less common.
We compare several Czech GEC systems, including several Transformer-based ones,
setting a strong baseline to future research. Finally, we meta-evaluate common
GEC metrics against human judgements on our data. We make the new Czech GEC
corpus publicly available under the CC BY-SA 4.0 license at
<a href="http://hdl.handle.net/11234/1-4639">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation. (arXiv:2201.05955v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05955">
<div class="article-summary-box-inner">
<span><p>A recurring challenge of crowdsourcing NLP datasets at scale is that human
writers often rely on repetitive patterns when crafting examples, leading to a
lack of linguistic diversity. We introduce a novel approach for dataset
creation based on worker and AI collaboration, which brings together the
generative strength of language models and the evaluative strength of humans.
Starting with an existing dataset, MultiNLI for natural language inference
(NLI), our approach uses dataset cartography to automatically identify examples
that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose
new examples with similar patterns. Machine generated examples are then
automatically filtered, and finally revised and labeled by human crowdworkers.
The resulting dataset, WANLI, consists of 108,079 NLI examples and presents
unique empirical strengths over existing NLI datasets. Remarkably, training a
model on WANLI instead of MultiNLI (which is $4$ times larger) improves
performance on seven out-of-domain test sets we consider, including by 11% on
HANS and 9% on Adversarial NLI. Moreover, combining MultiNLI with WANLI is more
effective than combining it with other NLI augmentation sets. Our results
demonstrate the potential of natural language generation techniques to curate
NLP datasets of enhanced quality and diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiFSMN: Binary Neural Network for Keyword Spotting. (arXiv:2202.06483v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06483">
<div class="article-summary-box-inner">
<span><p>The deep neural networks, such as the Deep-FSMN, have been widely studied for
keyword spotting (KWS) applications. However, computational resources for these
networks are significantly constrained since they usually run on-call on edge
devices. In this paper, we present BiFSMN, an accurate and extreme-efficient
binary neural network for KWS. We first construct a High-frequency Enhancement
Distillation scheme for the binarization-aware training, which emphasizes the
high-frequency information from the full-precision network's representation
that is more crucial for the optimization of the binarized network. Then, to
allow the instant and adaptive accuracy-efficiency trade-offs at runtime, we
also propose a Thinnable Binarization Architecture to further liberate the
acceleration potential of the binarized network from the topology perspective.
Moreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8
devices which fully utilizes registers and increases instruction throughput to
push the limit of deployment efficiency. Extensive experiments show that BiFSMN
outperforms existing binarization methods by convincing margins on various
datasets and is even comparable with the full-precision counterpart (e.g., less
than 3% drop on Speech Commands V1-12). We highlight that benefiting from the
thinnable architecture and the optimized 1-bit implementation, BiFSMN can
achieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge
hardware.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Extractive Opinion Summarization Using Sparse Coding. (arXiv:2203.07921v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07921">
<div class="article-summary-box-inner">
<span><p>Opinion summarization is the task of automatically generating summaries that
encapsulate information from multiple user reviews. We present Semantic
Autoencoder (SemAE) to perform extractive opinion summarization in an
unsupervised manner. SemAE uses dictionary learning to implicitly capture
semantic information from the review and learns a latent representation of each
sentence over semantic units. A semantic unit is supposed to capture an
abstract semantic concept. Our extractive summarization algorithm leverages the
representations to identify representative opinions among hundreds of reviews.
SemAE is also able to perform controllable summarization to generate
aspect-specific summaries. We report strong performance on SPACE and AMAZON
datasets, and perform experiments to investigate the functioning of our model.
Our code is publicly available at https://github.com/brcsomnath/SemAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses. (arXiv:2203.10750v4 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10750">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop a new multi-singer Chinese neural singing voice
synthesis (SVS) system named WeSinger. To improve the accuracy and naturalness
of synthesized singing voice, we design several specifical modules and
techniques: 1) A deep bi-directional LSTM based duration model with multi-scale
rhythm loss and post-processing step; 2) A Transformer-alike acoustic model
with progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet
neural vocoder to produce high-quality singing waveforms; 4) A novel data
augmentation method with multi-singer pre-training for stronger robustness and
naturalness. To our knowledge, WeSinger is the first SVS system to adopt 24 kHz
LPCNet and multi-singer pre-training simultaneously. Both quantitative and
qualitative evaluation results demonstrate the effectiveness of WeSinger in
terms of accuracy and naturalness, and WeSinger achieves state-of-the-art
performance on the recently public Chinese singing corpus Opencpop. Some
synthesized singing samples are available online
(https://zzw922cn.github.io/wesinger/).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions. (arXiv:2203.12235v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12235">
<div class="article-summary-box-inner">
<span><p>The syntactic categories of categorial grammar formalisms are structured
units made of smaller, indivisible primitives, bound together by the underlying
grammar's category formation rules. In the trending approach of constructive
supertagging, neural models are increasingly made aware of the internal
category structure, which in turn enables them to more reliably predict rare
and out-of-vocabulary categories, with significant implications for grammars
previously deemed too complex to find practical use. In this work, we revisit
constructive supertagging from a graph-theoretic perspective, and propose a
framework based on heterogeneous dynamic graph convolutions aimed at exploiting
the distinctive structure of a supertagger's output space. We test our approach
on a number of categorial grammar datasets spanning different languages and
grammar formalisms, achieving substantial improvements over previous state of
the art scores. Code will be made available at
https://github.com/konstantinosKokos/dynamic-graph-supertagging
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brazilian Court Documents Clustered by Similarity Together Using Natural Language Processing Approaches with Transformers. (arXiv:2204.07182v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07182">
<div class="article-summary-box-inner">
<span><p>Recent advances in Artificial intelligence (AI) have leveraged promising
results in solving complex problems in the area of Natural Language Processing
(NLP), being an important tool to help in the expeditious resolution of
judicial proceedings in the legal area. In this context, this work targets the
problem of detecting the degree of similarity between judicial documents that
can be achieved in the inference group, by applying six NLP techniques based on
transformers, namely BERT, GPT-2 and RoBERTa pre-trained in the Brazilian
Portuguese language and the same specialized using 210,000 legal proceedings.
Documents were pre-processed and had their content transformed into a vector
representation using these NLP techniques. Unsupervised learning was used to
cluster the lawsuits, calculating the quality of the model based on the cosine
of the distance between the elements of the group to its centroid. We noticed
that models based on transformers present better performance when compared to
previous research, highlighting the RoBERTa model specialized in the Brazilian
Portuguese language, making it possible to advance in the current state of the
art in the area of NLP applied to the legal sector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate detection of sepsis at ED triage using machine learning with clinical natural language processing. (arXiv:2204.07657v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07657">
<div class="article-summary-box-inner">
<span><p>Sepsis is a life-threatening condition with organ dysfunction and is a
leading cause of death and critical illness worldwide. Accurate detection of
sepsis during emergency department triage would allow early initiation of lab
analysis, antibiotic administration, and other sepsis treatment protocols. The
purpose of this study was to determine whether EHR data can be extracted and
synthesized with the latest machine learning algorithms (KATE Sepsis) and
clinical natural language processing to produce accurate sepsis models, and
compare KATE Sepsis performance with existing sepsis screening protocols, such
as SIRS and qSOFA. A machine learning model (KATE Sepsis) was developed using
patient encounters with triage data from 16 participating hospitals. KATE
Sepsis, SIRS, standard screening (SIRS with source of infection) and qSOFA were
tested in three settings. Cohort-A was a retrospective analysis on medical
records from a single Site 1. Cohort-B was a prospective analysis of Site 1.
Cohort-C was a retrospective analysis on Site 1 with 15 additional sites.
Across all cohorts, KATE Sepsis demonstrates an AUC of 0.94-0.963 with
73-74.87% TPR and 3.76-7.17% FPR. Standard screening demonstrates an AUC of
0.682-0.726 with 39.39-51.19% TPR and 2.9-6.02% FPR. The qSOFA protocol
demonstrates an AUC of 0.544-0.56, with 10.52-13.18% TPR and 1.22-1.68% FPR.
For severe sepsis, across all cohorts, KATE Sepsis demonstrates an AUC of
0.935-0.972 with 70-82.26% TPR and 4.64-8.62% FPR. For septic shock, across all
cohorts, KATE Sepsis demonstrates an AUC of 0.96-0.981 with 85.71-89.66% TPR
and 4.85-8.8% FPR. SIRS, standard screening, and qSOFA demonstrate low AUC and
TPR for severe sepsis and septic shock detection. KATE Sepsis provided
substantially better sepsis detection performance in triage than commonly used
screening protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persua: A Visual Interactive System to Enhance the Persuasiveness of Arguments in Online Discussion. (arXiv:2204.07741v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07741">
<div class="article-summary-box-inner">
<span><p>Persuading people to change their opinions is a common practice in online
discussion forums on topics ranging from political campaigns to relationship
consultation. Enhancing people's ability to write persuasive arguments could
not only practice their critical thinking and reasoning but also contribute to
the effectiveness and civility in online communication. It is, however, not an
easy task in online discussion settings where written words are the primary
communication channel. In this paper, we derived four design goals for a tool
that helps users improve the persuasiveness of arguments in online discussions
through a survey with 123 online forum users and interviews with five debating
experts. To satisfy these design goals, we analyzed and built a labeled dataset
of fine-grained persuasive strategies (i.e., logos, pathos, ethos, and
evidence) in 164 arguments with high ratings on persuasiveness from
ChangeMyView, a popular online discussion forum. We then designed an
interactive visual system, Persua, which provides example-based guidance on
persuasive strategies to enhance the persuasiveness of arguments. In
particular, the system constructs portfolios of arguments based on different
persuasive strategies applied to a given discussion topic. It then presents
concrete examples based on the difference between the portfolios of user input
and high-quality arguments in the dataset. A between-subjects study shows
suggestive evidence that Persua encourages users to submit more times for
feedback and helps users improve more on the persuasiveness of their arguments
than a baseline system. Finally, a set of design considerations was summarized
to guide future intelligent systems that improve the persuasiveness in text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2204.07955v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07955">
<div class="article-summary-box-inner">
<span><p>As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment
Analysis (MABSA) has attracted increasing attention in recent years. However,
previous approaches either (i) use separately pre-trained visual and textual
models, which ignore the crossmodal alignment or (ii) use vision-language
models pre-trained with general pre-training tasks, which are inadequate to
identify finegrained aspects, opinions, and their alignments across modalities.
To tackle these limitations, we propose a task-specific Vision-Language
Pre-training framework for MABSA (VLPMABSA), which is a unified multimodal
encoder-decoder architecture for all the pretraining and downstream tasks. We
further design three types of task-specific pre-training tasks from the
language, vision, and multimodal modalities, respectively. Experimental results
show that our approach generally outperforms the state-of-the-art approaches on
three MABSA subtasks. Further analysis demonstrates the effectiveness of each
pretraining task. The source code is publicly released at
https://github.com/NUSTM/VLP-MABSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Psycho-linguistic Analysis of BitChute. (arXiv:2204.08078v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08078">
<div class="article-summary-box-inner">
<span><p>In order to better support researchers, journalist, and practitioners in
their use of the MeLa-BitChute dataset for exploration and investigative
reporting, we provide new psycho-linguistic metadata for the videos, comments,
and channels in the dataset using LIWC22. This paper describes that metadata
and methods to filter the data using the metadata. In addition, we provide
basic analysis and comparison of the language on BitChute to other social media
platforms. The MeLa-BitChute dataset and LIWC metadata described in this paper
can be found at:
https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KRD1VS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Execute Actions or Ask Clarification Questions. (arXiv:2204.08373v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08373">
<div class="article-summary-box-inner">
<span><p>Collaborative tasks are ubiquitous activities where a form of communication
is required in order to reach a joint goal. Collaborative building is one of
such tasks. We wish to develop an intelligent builder agent in a simulated
building environment (Minecraft) that can build whatever users wish to build by
just talking to the agent. In order to achieve this goal, such agents need to
be able to take the initiative by asking clarification questions when further
information is needed. Existing works on Minecraft Corpus Dataset only learn to
execute instructions neglecting the importance of asking for clarifications. In
this paper, we extend the Minecraft Corpus Dataset by annotating all builder
utterances into eight types, including clarification questions, and propose a
new builder agent model capable of determining when to ask or execute
instructions. Experimental results show that our model achieves
state-of-the-art performance on the collaborative building task with a
substantial improvement. We also define two new tasks, the learning to ask task
and the joint learning task. The latter consists of solving both collaborating
building and learning to ask tasks jointly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing for the Usage of Grammatical Number. (arXiv:2204.08831v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08831">
<div class="article-summary-box-inner">
<span><p>A central quest of probing is to uncover how pre-trained models encode a
linguistic property within their representations. An encoding, however, might
be spurious-i.e., the model might not rely on it when making predictions. In
this paper, we try to find encodings that the model actually uses, introducing
a usage-based probing setup. We first choose a behavioral task which cannot be
solved without using the linguistic property. Then, we attempt to remove the
property by intervening on the model's representations. We contend that, if an
encoding is used by the model, its removal should harm the performance on the
chosen behavioral task. As a case study, we focus on how BERT encodes
grammatical number, and on how it uses this encoding to solve the number
agreement task. Experimentally, we find that BERT relies on a linear encoding
of grammatical number to produce the correct behavioral output. We also find
that BERT uses a separate encoding of grammatical number for nouns and verbs.
Finally, we identify in which layers information about grammatical number is
transferred from a noun to its head verb.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Situational Perception Guided Image Matting. (arXiv:2204.09276v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09276">
<div class="article-summary-box-inner">
<span><p>Most automatic matting methods try to separate the salient foreground from
the background. However, the insufficient quantity and subjective bias of the
current existing matting datasets make it difficult to fully explore the
semantic association between object-to-object and object-to-environment in a
given image. In this paper, we propose a Situational Perception Guided Image
Matting (SPG-IM) method that mitigates subjective bias of matting annotations
and captures sufficient situational perception information for better global
saliency distilled from the visual-to-textual task. SPG-IM can better associate
inter-objects and object-to-environment saliency, and compensate the subjective
nature of image matting and its expensive annotation. We also introduce a
textual Semantic Transformation (TST) module that can effectively transform and
integrate the semantic feature stream to guide the visual representations. In
addition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed
to adaptively switch multi-scale receptive fields and focal points to enhance
both global and local details. Extensive experiments demonstrate the
effectiveness of situational perception guidance from the visual-to-textual
tasks on image matting, and our model outperforms the state-of-the-art methods.
We also analyze the significance of different components in our model. The code
will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Continuous Integrate-and-Fire for Adaptive Simultaneous Speech Translation. (arXiv:2204.09595v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09595">
<div class="article-summary-box-inner">
<span><p>Simultaneous speech translation (SimulST) is a challenging task aiming to
translate streaming speech before the complete input is observed. A SimulST
system generally includes two components: the pre-decision that aggregates the
speech information and the policy that decides to read or write. While recent
works had proposed various strategies to improve the pre-decision, they mainly
adopt the fixed wait-k policy, leaving the adaptive policies rarely explored.
This paper proposes to model the adaptive policy by adapting the Continuous
Integrate-and-Fire (CIF). Compared with monotonic multihead attention (MMA),
our method has the advantage of simpler computation, superior quality at low
latency, and better generalization to long utterances. We conduct experiments
on the MuST-C V2 dataset and show the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceiving the World: Question-guided Reinforcement Learning for Text-based Games. (arXiv:2204.09597v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09597">
<div class="article-summary-box-inner">
<span><p>Text-based games provide an interactive way to study natural language
processing. While deep reinforcement learning has shown effectiveness in
developing the game playing agent, the low sample efficiency and the large
action space remain to be the two major challenges that hinder the DRL from
being applied in the real world. In this paper, we address the challenges by
introducing world-perceiving modules, which automatically decompose tasks and
prune actions by answering questions about the environment. We then propose a
two-phase training framework to decouple language learning from reinforcement
learning, which further improves the sample efficiency. The experimental
results show that the proposed method significantly improves the performance
and sample efficiency. Besides, it shows robustness against compound error and
limited pre-training data.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatially-Preserving Flattening for Location-Aware Classification of Findings in Chest X-Rays. (arXiv:2204.09676v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09676">
<div class="article-summary-box-inner">
<span><p>Chest X-rays have become the focus of vigorous deep learning research in
recent years due to the availability of large labeled datasets. While
classification of anomalous findings is now possible, ensuring that they are
correctly localized still remains challenging, as this requires recognition of
anomalies within anatomical regions. Existing deep learning networks for
fine-grained anomaly classification learn location-specific findings using
architectures where the location and spatial contiguity information is lost
during the flattening step before classification. In this paper, we present a
new spatially preserving deep learning network that preserves location and
shape information through auto-encoding of feature maps during flattening. The
feature maps, auto-encoder and classifier are then trained in an end-to-end
fashion to enable location aware classification of findings in chest X-rays.
Results are shown on a large multi-hospital chest X-ray dataset indicating a
significant improvement in the quality of finding classification over
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FS-NCSR: Increasing Diversity of the Super-Resolution Space via Frequency Separation and Noise-Conditioned Normalizing Flow. (arXiv:2204.09679v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09679">
<div class="article-summary-box-inner">
<span><p>Super-resolution suffers from an innate ill-posed problem that a single
low-resolution (LR) image can be from multiple high-resolution (HR) images.
Recent studies on the flow-based algorithm solve this ill-posedness by learning
the super-resolution space and predicting diverse HR outputs. Unfortunately,
the diversity of the super-resolution outputs is still unsatisfactory, and the
outputs from the flow-based model usually suffer from undesired artifacts which
causes low-quality outputs. In this paper, we propose FS-NCSR which produces
diverse and high-quality super-resolution outputs using frequency separation
and noise conditioning compared to the existing flow-based approaches. As the
sharpness and high-quality detail of the image rely on its high-frequency
information, FS-NCSR only estimates the high-frequency information of the
high-resolution outputs without redundant low-frequency components. Through
this, FS-NCSR significantly improves the diversity score without significant
image quality degradation compared to the NCSR, the winner of the previous
NTIRE 2021 challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complete identification of complex salt-geometries from inaccurate migrated images using Deep Learning. (arXiv:2204.09710v1 [physics.geo-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09710">
<div class="article-summary-box-inner">
<span><p>Delimiting salt inclusions from migrated images is a time-consuming activity
that relies on highly human-curated analysis and is subject to interpretation
errors or limitations of the methods available. We propose to use migrated
images produced from an inaccurate velocity model (with a reasonable
approximation of sediment velocity, but without salt inclusions) to predict the
correct salt inclusions shape using a Convolutional Neural Network (CNN). Our
approach relies on subsurface Common Image Gathers to focus the sediments'
reflections around the zero offset and to spread the energy of salt reflections
over large offsets. Using synthetic data, we trained a U-Net to use
common-offset subsurface images as input channels for the CNN and the correct
salt-masks as network output. The network learned to predict the salt
inclusions masks with high accuracy; moreover, it also performed well when
applied to synthetic benchmark data sets that were not previously introduced.
Our training process tuned the U-Net to successfully learn the shape of complex
salt bodies from partially focused subsurface offset images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval. (arXiv:2204.09730v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09730">
<div class="article-summary-box-inner">
<span><p>Cross-modal image-recipe retrieval has gained significant attention in recent
years. Most work focuses on improving cross-modal embeddings using unimodal
encoders, that allow for efficient retrieval in large-scale databases, leaving
aside cross-attention between modalities which is more computationally
expensive. We propose a new retrieval framework, T-Food (Transformer Decoders
with MultiModal Regularization for Cross-Modal Food Retrieval) that exploits
the interaction between modalities in a novel regularization scheme, while
using only unimodal encoders at test time for efficient retrieval. We also
capture the intra-dependencies between recipe entities with a dedicated recipe
encoder, and propose new variants of triplet losses with dynamic margins that
adapt to the difficulty of the task. Finally, we leverage the power of the
recent Vision and Language Pretraining (VLP) models such as CLIP for the image
encoder. Our approach outperforms existing approaches by a large margin on the
Recipe1M dataset. Specifically, we achieve absolute improvements of 8.1 % (72.6
R@1) and +10.9 % (44.6 R@1) on the 1k and 10k test sets respectively. The code
is available here:https://github.com/mshukor/TFood
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time-based Self-supervised Learning for Wireless Capsule Endoscopy. (arXiv:2204.09773v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09773">
<div class="article-summary-box-inner">
<span><p>State-of-the-art machine learning models, and especially deep learning ones,
are significantly data-hungry; they require vast amounts of manually labeled
samples to function correctly. However, in most medical imaging fields,
obtaining said data can be challenging. Not only the volume of data is a
problem, but also the imbalances within its classes; it is common to have many
more images of healthy patients than of those with pathology. Computer-aided
diagnostic systems suffer from these issues, usually over-designing their
models to perform accurately. This work proposes using self-supervised learning
for wireless endoscopy videos by introducing a custom-tailored method that does
not initially need labels or appropriate balance. We prove that using the
inferred inherent structure learned by our method, extracted from the temporal
axis, improves the detection rate on several domain-specific applications even
under severe imbalance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention in Reasoning: Dataset, Analysis, and Modeling. (arXiv:2204.09774v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09774">
<div class="article-summary-box-inner">
<span><p>While attention has been an increasingly popular component in deep neural
networks to both interpret and boost the performance of models, little work has
examined how attention progresses to accomplish a task and whether it is
reasonable. In this work, we propose an Attention with Reasoning capability
(AiR) framework that uses attention to understand and improve the process
leading to task outcomes. We first define an evaluation metric based on a
sequence of atomic reasoning operations, enabling a quantitative measurement of
attention that considers the reasoning process. We then collect human
eye-tracking and answer correctness data, and analyze various machine and human
attention mechanisms on their reasoning capability and how they impact task
performance. To improve the attention and reasoning ability of visual question
answering models, we propose to supervise the learning of attention
progressively along the reasoning process and to differentiate the correct and
incorrect attention patterns. We demonstrate the effectiveness of the proposed
framework in analyzing and modeling attention with better reasoning capability
and task performance. The code and data are available at
https://github.com/szzexpoi/AiR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Focus Image Fusion based on Gradient Transform. (arXiv:2204.09777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09777">
<div class="article-summary-box-inner">
<span><p>Multi-focus image fusion is a challenging field of study that aims to provide
a completely focused image by integrating focused and un-focused pixels. Most
existing methods suffer from shift variance, misregistered images, and
data-dependent. In this study, we introduce a novel gradient information-based
multi-focus image fusion method that is robust for the aforementioned problems.
The proposed method first generates gradient images from original images by
using Halftoning-Inverse Halftoning (H-IH) transform. Then, Energy of Gradient
(EOG) and Standard Deviation functions are used as the focus measurement on the
gradient images to form a fused image. Finally, in order to enhance the fused
image a decision fusion approach is applied with the majority voting method.
The proposed method is compared with 17 different novel and conventional
techniques both visually and objectively. For objective evaluation, 6 different
quantitative metrics are used. It is observed that the proposed method is
promising according to visual evaluation and 83.3% success is achieved by being
first in five out of six metrics according to objective evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Scale Features and Parallel Transformers Based Image Quality Assessment. (arXiv:2204.09779v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09779">
<div class="article-summary-box-inner">
<span><p>With the increase in multimedia content, the type of distortions associated
with multimedia is also increasing. This problem of image quality assessment is
expanded well in the PIPAL dataset, which is still an open problem to solve for
researchers. Although, recently proposed transformers networks have already
been used in the literature for image quality assessment. At the same time, we
notice that multi-scale feature extraction has proven to be a promising
approach for image quality assessment. However, the way transformer networks
are used for image quality assessment until now lacks these properties of
multi-scale feature extraction. We utilized this fact in our approach and
proposed a new architecture by integrating these two promising quality
assessment techniques of images. Our experimentation on various datasets,
including the PIPAL dataset, demonstrates that the proposed integration
technique outperforms existing algorithms. The source code of the proposed
algorithm is available online: https://github.com/KomalPal9610/IQA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiPathGAN: Structure Preserving Stain Normalization using Unsupervised Multi-domain Adversarial Network with Perception Loss. (arXiv:2204.09782v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09782">
<div class="article-summary-box-inner">
<span><p>Histopathology relies on the analysis of microscopic tissue images to
diagnose disease. A crucial part of tissue preparation is staining whereby a
dye is used to make the salient tissue components more distinguishable.
However, differences in laboratory protocols and scanning devices result in
significant confounding appearance variation in the corresponding images. This
variation increases both human error and the inter-rater variability, as well
as hinders the performance of automatic or semi-automatic methods. In the
present paper we introduce an unsupervised adversarial network to translate
(and hence normalize) whole slide images across multiple data acquisition
domains. Our key contributions are: (i) an adversarial architecture which
learns across multiple domains with a single generator-discriminator network
using an information flow branch which optimizes for perceptual loss, and (ii)
the inclusion of an additional feature extraction network during training which
guides the transformation network to keep all the structural features in the
tissue image intact. We: (i) demonstrate the effectiveness of the proposed
method firstly on H\&amp;E slides of 120 cases of kidney cancer, as well as (ii)
show the benefits of the approach on more general problems, such as flexible
illumination based natural image enhancement and light source adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SELMA: SEmantic Large-scale Multimodal Acquisitions in Variable Weather, Daytime and Viewpoints. (arXiv:2204.09788v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09788">
<div class="article-summary-box-inner">
<span><p>Accurate scene understanding from multiple sensors mounted on cars is a key
requirement for autonomous driving systems. Nowadays, this task is mainly
performed through data-hungry deep learning techniques that need very large
amounts of data to be trained. Due to the high cost of performing segmentation
labeling, many synthetic datasets have been proposed. However, most of them
miss the multi-sensor nature of the data, and do not capture the significant
changes introduced by the variation of daytime and weather conditions. To fill
these gaps, we introduce SELMA, a novel synthetic dataset for semantic
segmentation that contains more than 30K unique waypoints acquired from 24
different sensors including RGB, depth, semantic cameras and LiDARs, in 27
different atmospheric and daytime conditions, for a total of more than 20M
samples. SELMA is based on CARLA, an open-source simulator for generating
synthetic data in autonomous driving scenarios, that we modified to increase
the variability and the diversity in the scenes and class sets, and to align it
with other benchmark datasets. As shown by the experimental evaluation, SELMA
allows the efficient training of standard and multi-modal deep learning
architectures, and achieves remarkable results on real-world data. SELMA is
free and publicly available, thus supporting open science and research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Gaussian Mixture Model for Realtime Roadside LiDAR Object Detection. (arXiv:2204.09804v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09804">
<div class="article-summary-box-inner">
<span><p>Background modeling is widely used for intelligent surveillance systems to
detect the moving targets by subtracting the static background components. Most
roadside LiDAR object detection methods filter out foreground points by
comparing new points to pre-trained background references based on descriptive
statistics over many frames (e.g., voxel density, slopes, maximum distance).
These solutions are not efficient under heavy traffic, and parameter values are
hard to transfer from one scenario to another. In early studies, the
video-based background modeling methods were considered not suitable for
roadside LiDAR surveillance systems due to the sparse and unstructured point
clouds data. In this paper, the raw LiDAR data were transformed into a
multi-dimensional tensor structure based on the elevation and azimuth value of
each LiDAR point. With this high-order data representation, we break the
barrier to allow the efficient Gaussian Mixture Model (GMM) method for roadside
LiDAR background modeling. The probabilistic GMM is built with superior agility
and real-time capability. The proposed Method was compared against two
state-of-the-art roadside LiDAR background models and evaluated based on point
level, object level, and path level, demonstrating better robustness under
heavy traffic and challenging weather. This multimodal GMM method is capable of
handling dynamic backgrounds with noisy measurements and substantially enhances
the infrastructure-based LiDAR object detection, whereby various 3D modeling
for smart city applications could be created
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parametric Level-sets Enhanced To Improve Reconstruction (PaLEnTIR). (arXiv:2204.09815v1 [math.NA])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09815">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the restoration and reconstruction of piecewise
constant objects in two and three dimensions using PaLEnTIR, a significantly
enhanced Parametric level set (PaLS) model relative to the current
state-of-the-art. The primary contribution of this paper is a new PaLS
formulation which requires only a single level set function to recover a scene
with piecewise constant objects possessing multiple unknown contrasts. Our
model offers distinct advantages over current approaches to the multi-contrast,
multi-object problem, all of which require multiple level sets and explicit
estimation of the contrast magnitudes. Given upper and lower bounds on the
contrast, our approach is able to recover objects with any distribution of
contrasts and eliminates the need to know either the number of contrasts in a
given scene or their values. We provide an iterative process for finding these
space-varying contrast limits. Relative to most PaLS methods which employ
radial basis functions (RBFs), our model makes use of non-isotropic basis
functions, thereby expanding the class of shapes that a PaLS model of a given
complexity can approximate. Finally, PaLEnTIR improves the conditioning of the
Jacobian matrix required as part of the parameter identification process and
consequently accelerates the optimization methods by controlling the magnitude
of the PaLS expansion coefficients, fixing the centers of the basis functions,
and the uniqueness of parametric to image mappings provided by the new
parameterization. We demonstrate the performance of the new approach using both
2D and 3D variants of X-ray computed tomography, diffuse optical tomography
(DOT), denoising, deconvolution problems. Application to experimental sparse CT
data and simulated data with different types of noise are performed to further
validate the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09817">
<div class="article-summary-box-inner">
<span><p>Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision-language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimMC: Simple Masked Contrastive Learning of Skeleton Representations for Unsupervised Person Re-Identification. (arXiv:2204.09826v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09826">
<div class="article-summary-box-inner">
<span><p>Recent advances in skeleton-based person re-identification (re-ID) obtain
impressive performance via either hand-crafted skeleton descriptors or skeleton
representation learning with deep learning paradigms. However, they typically
require skeletal pre-modeling and label information for training, which leads
to limited applicability of these methods. In this paper, we focus on
unsupervised skeleton-based person re-ID, and present a generic Simple Masked
Contrastive learning (SimMC) framework to learn effective representations from
unlabeled 3D skeletons for person re-ID. Specifically, to fully exploit
skeleton features within each skeleton sequence, we first devise a masked
prototype contrastive learning (MPC) scheme to cluster the most typical
skeleton features (skeleton prototypes) from different subsequences randomly
masked from raw sequences, and contrast the inherent similarity between
skeleton features and different prototypes to learn discriminative skeleton
representations without using any label. Then, considering that different
subsequences within the same sequence usually enjoy strong correlations due to
the nature of motion continuity, we propose the masked intra-sequence
contrastive learning (MIC) to capture intra-sequence pattern consistency
between subsequences, so as to encourage learning more effective skeleton
representations for person re-ID. Extensive experiments validate that the
proposed SimMC outperforms most state-of-the-art skeleton-based methods. We
further show its scalability and efficiency in enhancing the performance of
existing models. Our codes are available at https://github.com/Kali-Hac/SimMC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast AdvProp. (arXiv:2204.09838v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09838">
<div class="article-summary-box-inner">
<span><p>Adversarial Propagation (AdvProp) is an effective way to improve recognition
models, leveraging adversarial examples. Nonetheless, AdvProp suffers from the
extremely slow training speed, mainly because: a) extra forward and backward
passes are required for generating adversarial examples; b) both original
samples and their adversarial counterparts are used for training (i.e.,
2$\times$ data). In this paper, we introduce Fast AdvProp, which aggressively
revamps AdvProp's costly training components, rendering the method nearly as
cheap as the vanilla training. Specifically, our modifications in Fast AdvProp
are guided by the hypothesis that disentangled learning with adversarial
examples is the key for performance improvements, while other training recipes
(e.g., paired clean and adversarial training samples, multi-step adversarial
attackers) could be largely simplified.
</p>
<p>Our empirical results show that, compared to the vanilla training baseline,
Fast AdvProp is able to further model performance on a spectrum of visual
benchmarks, without incurring extra training cost. Additionally, our ablations
find Fast AdvProp scales better if larger models are used, is compatible with
existing data augmentation methods (i.e., Mixup and CutMix), and can be easily
adapted to other recognition tasks like object detection. The code is available
here: https://github.com/meijieru/fast_advprop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiscale Analysis for Improving Texture Classification. (arXiv:2204.09841v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09841">
<div class="article-summary-box-inner">
<span><p>Information from an image occurs over multiple and distinct spatial scales.
Image pyramid multiresolution representations are a useful data structure for
image analysis and manipulation over a spectrum of spatial scales. This paper
employs the Gaussian-Laplacian pyramid to treat different spatial frequency
bands of a texture separately. First, we generate three images corresponding to
three levels of the Gaussian-Laplacian pyramid for an input image to capture
intrinsic details. Then we aggregate features extracted from gray and color
texture images using bio-inspired texture descriptors, information-theoretic
measures, gray-level co-occurrence matrix features, and Haralick statistical
features into a single feature vector. Such an aggregation aims at producing
features that characterize textures to their maximum extent, unlike employing
each descriptor separately, which may lose some relevant textural information
and reduce the classification performance. The experimental results on texture
and histopathologic image datasets have shown the advantages of the proposed
method compared to state-of-the-art approaches. Such findings emphasize the
importance of multiscale image analysis and corroborate that the descriptors
mentioned above are complementary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unseen Object Instance Segmentation with Fully Test-time RGB-D Embeddings Adaptation. (arXiv:2204.09847v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09847">
<div class="article-summary-box-inner">
<span><p>Segmenting unseen objects is a crucial ability for the robot since it may
encounter new environments during the operation. Recently, a popular solution
is leveraging RGB-D features of large-scale synthetic data and directly
applying the model to unseen real-world scenarios. However, even though depth
data have fair generalization ability, the domain shift due to the Sim2Real gap
is inevitable, which presents a key challenge to the unseen object instance
segmentation (UOIS) model. To tackle this problem, we re-emphasize the
adaptation process across Sim2Real domains in this paper. Specifically, we
propose a framework to conduct the Fully Test-time RGB-D Embeddings Adaptation
(FTEA) based on parameters of the BatchNorm layer. To construct the learning
objective for test-time back-propagation, we propose a novel non-parametric
entropy objective that can be implemented without explicit classification
layers. Moreover, we design a cross-modality knowledge distillation module to
encourage the information transfer during test time. The proposed method can be
efficiently conducted with test-time images, without requiring annotations or
revisiting the large-scale synthetic training data. Besides significant time
savings, the proposed method consistently improves segmentation results on both
overlap and boundary metrics, achieving state-of-the-art performances on two
real-world RGB-D image datasets. We hope our work could draw attention to the
test-time adaptation and reveal a promising direction for robot perception in
unseen environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Aligned Feature Fusion for Multimodal Object Detection. (arXiv:2204.09848v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09848">
<div class="article-summary-box-inner">
<span><p>To achieve accurate and robust object detection in the real-world scenario,
various forms of images are incorporated, such as color, thermal, and depth.
However, multimodal data often suffer from the position shift problem, i.e.,
the image pair is not strictly aligned, making one object has different
positions in different modalities. For the deep learning method, this problem
makes it difficult to fuse multimodal features and puzzles the convolutional
neural network (CNN) training. In this article, we propose a general multimodal
detector named aligned region CNN (AR-CNN) to tackle the position shift
problem. First, a region feature (RF) alignment module with adjacent similarity
constraint is designed to consistently predict the position shift between two
modalities and adaptively align the cross-modal RFs. Second, we propose a novel
region of interest (RoI) jitter strategy to improve the robustness to
unexpected shift patterns. Third, we present a new multimodal feature fusion
method that selects the more reliable feature and suppresses the less useful
one via feature reweighting. In addition, by locating bounding boxes in both
modalities and building their relationships, we provide novel multimodal
labeling named KAIST-Paired. Extensive experiments on 2-D and 3-D object
detection, RGB-T, and RGB-D datasets demonstrate the effectiveness and
robustness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning to Guide Scientifically Relevant Categorization of Martian Terrain Images. (arXiv:2204.09854v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09854">
<div class="article-summary-box-inner">
<span><p>Automatic terrain recognition in Mars rover images is an important problem
not just for navigation, but for scientists interested in studying rock types,
and by extension, conditions of the ancient Martian paleoclimate and
habitability. Existing approaches to label Martian terrain either involve the
use of non-expert annotators producing taxonomies of limited granularity (e.g.
soil, sand, bedrock, float rock, etc.), or rely on generic class discovery
approaches that tend to produce perceptual classes such as rover parts and
landscape, which are irrelevant to geologic analysis. Expert-labeled datasets
containing granular geological/geomorphological terrain categories are rare or
inaccessible to public, and sometimes require the extraction of relevant
categorical information from complex annotations. In order to facilitate the
creation of a dataset with detailed terrain categories, we present a
self-supervised method that can cluster sedimentary textures in images captured
from the Mast camera onboard the Curiosity rover (Mars Science Laboratory). We
then present a qualitative analysis of these clusters and describe their
geologic significance via the creation of a set of granular terrain categories.
The precision and geologic validation of these automatically discovered
clusters suggest that our methods are promising for the rapid classification of
important geologic features and will therefore facilitate our long-term goal of
producing a large, granular, and publicly available dataset for Mars terrain
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Remote Sensing Cross-Modal Text-Image Retrieval Based on Global and Local Information. (arXiv:2204.09860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09860">
<div class="article-summary-box-inner">
<span><p>Cross-modal remote sensing text-image retrieval (RSCTIR) has recently become
an urgent research hotspot due to its ability of enabling fast and flexible
information extraction on remote sensing (RS) images. However, current RSCTIR
methods mainly focus on global features of RS images, which leads to the
neglect of local features that reflect target relationships and saliency. In
this article, we first propose a novel RSCTIR framework based on global and
local information (GaLR), and design a multi-level information dynamic fusion
(MIDF) module to efficaciously integrate features of different levels. MIDF
leverages local information to correct global information, utilizes global
information to supplement local information, and uses the dynamic addition of
the two to generate prominent visual representation. To alleviate the pressure
of the redundant targets on the graph convolution network (GCN) and to improve
the model s attention on salient instances during modeling local features, the
de-noised representation matrix and the enhanced adjacency matrix (DREA) are
devised to assist GCN in producing superior local representations. DREA not
only filters out redundant features with high similarity, but also obtains more
powerful local features by enhancing the features of prominent objects.
Finally, to make full use of the information in the similarity matrix during
inference, we come up with a plug-and-play multivariate rerank (MR) algorithm.
The algorithm utilizes the k nearest neighbors of the retrieval results to
perform a reverse search, and improves the performance by combining multiple
components of bidirectional retrieval. Extensive experiments on public datasets
strongly demonstrate the state-of-the-art performance of GaLR methods on the
RSCTIR task. The code of GaLR method, MR algorithm, and corresponding files
have been made available at https://github.com/xiaoyuan1996/GaLR .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pixel2Mesh++: 3D Mesh Generation and Refinement from Multi-View Images. (arXiv:2204.09866v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09866">
<div class="article-summary-box-inner">
<span><p>We study the problem of shape generation in 3D mesh representation from a
small number of color images with or without camera poses. While many previous
works learn to hallucinate the shape directly from priors, we adopt to further
improve the shape quality by leveraging cross-view information with a graph
convolution network. Instead of building a direct mapping function from images
to 3D shape, our model learns to predict series of deformations to improve a
coarse shape iteratively. Inspired by traditional multiple view geometry
methods, our network samples nearby area around the initial mesh's vertex
locations and reasons an optimal deformation using perceptual feature
statistics built from multiple input images. Extensive experiments show that
our model produces accurate 3D shapes that are not only visually plausible from
the input perspectives, but also well aligned to arbitrary viewpoints. With the
help of physically driven architecture, our model also exhibits generalization
capability across different semantic categories, and the number of input
images. Model analysis experiments show that our model is robust to the quality
of the initial mesh and the error of camera pose, and can be combined with a
differentiable renderer for test-time optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring a Fine-Grained Multiscale Method for Cross-Modal Remote Sensing Image Retrieval. (arXiv:2204.09868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09868">
<div class="article-summary-box-inner">
<span><p>Remote sensing (RS) cross-modal text-image retrieval has attracted extensive
attention for its advantages of flexible input and efficient query. However,
traditional methods ignore the characteristics of multi-scale and redundant
targets in RS image, leading to the degradation of retrieval accuracy. To cope
with the problem of multi-scale scarcity and target redundancy in RS multimodal
retrieval task, we come up with a novel asymmetric multimodal feature matching
network (AMFMN). Our model adapts to multi-scale feature inputs, favors
multi-source retrieval methods, and can dynamically filter redundant features.
AMFMN employs the multi-scale visual self-attention (MVSA) module to extract
the salient features of RS image and utilizes visual features to guide the text
representation. Furthermore, to alleviate the positive samples ambiguity caused
by the strong intraclass similarity in RS image, we propose a triplet loss
function with dynamic variable margin based on prior similarity of sample
pairs. Finally, unlike the traditional RS image-text dataset with coarse text
and higher intraclass similarity, we construct a fine-grained and more
challenging Remote sensing Image-Text Match dataset (RSITMD), which supports RS
image retrieval through keywords and sentence separately and jointly.
Experiments on four RS text-image datasets demonstrate that the proposed model
can achieve state-of-the-art performance in cross-modal RS text-image retrieval
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics vs. Learned Priors: Rethinking Camera and Algorithm Design for Task-Specific Imaging. (arXiv:2204.09871v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09871">
<div class="article-summary-box-inner">
<span><p>Cameras were originally designed using physics-based heuristics to capture
aesthetic images. In recent years, there has been a transformation in camera
design from being purely physics-driven to increasingly data-driven and
task-specific. In this paper, we present a framework to understand the building
blocks of this nascent field of end-to-end design of camera hardware and
algorithms. As part of this framework, we show how methods that exploit both
physics and data have become prevalent in imaging and computer vision,
underscoring a key trend that will continue to dominate the future of
task-specific camera design. Finally, we share current barriers to progress in
end-to-end design, and hypothesize how these barriers can be overcome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persistent-Transient Duality in Human Behavior Modeling. (arXiv:2204.09875v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09875">
<div class="article-summary-box-inner">
<span><p>We propose to model the persistent-transient duality in human behavior using
a parent-child multi-channel neural network, which features a parent persistent
channel that manages the global dynamics and children transient channels that
are initiated and terminated on-demand to handle detailed interactive actions.
The short-lived transient sessions are managed by a proposed Transient Switch.
The neural framework is trained to discover the structure of the duality
automatically. Our model shows superior performances in human-object
interaction motion prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNLL: A Semi-supervised Approach For Continual Noisy Label Learning. (arXiv:2204.09881v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09881">
<div class="article-summary-box-inner">
<span><p>The task of continual learning requires careful design of algorithms that can
tackle catastrophic forgetting. However, the noisy label, which is inevitable
in a real-world scenario, seems to exacerbate the situation. While very few
studies have addressed the issue of continual learning under noisy labels, long
training time and complicated training schemes limit their applications in most
cases. In contrast, we propose a simple purification technique to effectively
cleanse the online data stream that is both cost-effective and more accurate.
After purification, we perform fine-tuning in a semi-supervised fashion that
ensures the participation of all available samples. Training in this fashion
helps us learn a better representation that results in state-of-the-art (SOTA)
performance. Through extensive experimentation on 3 benchmark datasets, MNIST,
CIFAR10 and CIFAR100, we show the effectiveness of our proposed approach. We
achieve a 24.8% performance gain for CIFAR10 with 20% noise over previous SOTA
methods. Our code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Color Invariant Skin Segmentation. (arXiv:2204.09882v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09882">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of automatically detecting human skin in
images without reliance on color information. A primary motivation of the work
has been to achieve results that are consistent across the full range of skin
tones, even while using a training dataset that is significantly biased toward
lighter skin tones. Previous skin-detection methods have used color cues almost
exclusively, and we present a new approach that performs well in the absence of
such information. A key aspect of the work is dataset repair through
augmentation that is applied strategically during training, with the goal of
color invariant feature learning to enhance generalization. We have
demonstrated the concept using two architectures, and experimental results show
improvements in both precision and recall for most Fitzpatrick skin tones in
the benchmark ECU dataset. We further tested the system with the RFW dataset to
show that the proposed method performs much more consistently across different
ethnicities, thereby reducing the chance of bias based on skin color. To
demonstrate the effectiveness of our work, extensive experiments were performed
on grayscale images as well as images obtained under unconstrained illumination
and with artificial filters. Source code:
https://github.com/HanXuMartin/Color-Invariant-Skin-Segmentation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Video Interpolation by Learning Multilayered 2.5D Motion Fields. (arXiv:2204.09900v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09900">
<div class="article-summary-box-inner">
<span><p>The problem of video frame interpolation is to increase the temporal
resolution of a low frame-rate video, by interpolating novel frames between
existing temporally sparse frames. This paper presents a self-supervised
approach to video frame interpolation that requires only a single video. We
pose the video as a set of layers. Each layer is parameterized by two implicit
neural networks -- one for learning a static frame and the other for a
time-varying motion field corresponding to video dynamics. Together they
represent an occlusion-free subset of the scene with a pseudo-depth channel. To
model inter-layer occlusions, all layers are lifted to the 2.5D space so that
the frontal layer occludes distant layers. This is done by assigning each layer
a depth channel, which we call `pseudo-depth', whose partial order defines the
occlusion between layers. The pseudo-depths are converted to visibility values
through a fully differentiable SoftMin function so that closer layers are more
visible than layers in a distance. On the other hand, we parameterize the video
motions by solving an ordinary differentiable equation (ODE) defined on a
time-varying neural velocity field that guarantees valid motions. This implicit
neural representation learns the video as a space-time continuum, allowing
frame interpolation at any temporal resolution. We demonstrate the
effectiveness of our method on real-world datasets, where our method achieves
comparable performance to state-of-the-arts that require ground truth labels
for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation. (arXiv:2204.09903v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09903">
<div class="article-summary-box-inner">
<span><p>Few-shot segmentation, which aims to segment unseen-class objects given only
a handful of densely labeled samples, has received widespread attention from
the community. Existing approaches typically follow the prototype learning
paradigm to perform meta-inference, which fails to fully exploit the underlying
information from support image-mask pairs, resulting in various segmentation
failures, e.g., incomplete objects, ambiguous boundaries, and distractor
activation. To this end, we propose a simple yet versatile framework in the
spirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is
first implemented on the annotated support image, and then the coarse
segmentation mask is divided into multiple regions with different properties.
Leveraging effective masked average pooling operations, a series of
support-induced proxies are thus derived, each playing a specific role in
conquering the above challenges. Moreover, we devise a unique parallel decoder
structure that integrates proxies with similar attributes to boost the
discrimination power. Our proposed approach, named divide-and-conquer proxies
(DCP), allows for the development of appropriate and reliable information as a
guide at the "episode" level, not just about the object cues themselves.
Extensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of
DCP over conventional prototype-based approaches (up to 5~10% on average),
which also establishes a new state-of-the-art. Code is available at
github.com/chunbolang/DCP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infographics Wizard: Flexible Infographics Authoring and Design Exploration. (arXiv:2204.09904v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09904">
<div class="article-summary-box-inner">
<span><p>Infographics are an aesthetic visual representation of information following
specific design principles of human perception. Designing infographics can be a
tedious process for non-experts and time-consuming, even for professional
designers. With the help of designers, we propose a semi-automated infographic
framework for general structured and flow-based infographic design generation.
For novice designers, our framework automatically creates and ranks infographic
designs for a user-provided text with no requirement for design input. However,
expert designers can still provide custom design inputs to customize the
infographics. We will also contribute an individual visual group (VG) designs
dataset (in SVG), along with a 1k complete infographic image dataset with
segmented VGs in this work. Evaluation results confirm that by using our
framework, designers from all expertise levels can generate generic infographic
designs faster than existing methods while maintaining the same quality as
hand-designed infographics templates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient End-to-End Deep Neural Network for Interstitial Lung Disease Recognition and Classification. (arXiv:2204.09909v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09909">
<div class="article-summary-box-inner">
<span><p>The automated Interstitial Lung Diseases (ILDs) classification technique is
essential for assisting clinicians during the diagnosis process. Detecting and
classifying ILDs patterns is a challenging problem. This paper introduces an
end-to-end deep convolution neural network (CNN) for classifying ILDs patterns.
The proposed model comprises four convolutional layers with different kernel
sizes and Rectified Linear Unit (ReLU) activation function, followed by batch
normalization and max-pooling with a size equal to the final feature map size
well as four dense layers. We used the ADAM optimizer to minimize categorical
cross-entropy. A dataset consisting of 21328 image patches of 128 CT scans with
five classes is taken to train and assess the proposed model. A comparison
study showed that the presented model outperformed pre-trained CNNs and
five-fold cross-validation on the same dataset. For ILDs pattern
classification, the proposed approach achieved the accuracy scores of 99.09%
and the average F score of 97.9%, outperforming three pre-trained CNNs. These
outcomes show that the proposed model is relatively state-of-the-art in
precision, recall, f score, and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation. (arXiv:2204.09914v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09914">
<div class="article-summary-box-inner">
<span><p>LiDAR semantic segmentation essential for advanced autonomous driving is
required to be accurate, fast, and easy-deployed on mobile platforms. Previous
point-based or sparse voxel-based methods are far away from real-time
applications since time-consuming neighbor searching or sparse 3D convolution
are employed. Recent 2D projection-based methods, including range view and
multi-view fusion, can run in real time, but suffer from lower accuracy due to
information loss during the 2D projection. Besides, to improve the performance,
previous methods usually adopt test time augmentation (TTA), which further
slows down the inference process. To achieve a better speed-accuracy trade-off,
we propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both
effectiveness and efficiency mainly by the following two techniques: 1) the
novel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D
projected grid for efficiency, while summarizes both 2D and 3D features on 3D
point for minimal information loss; 2) the proposed transformation consistency
loss narrows the gap between the single-time model inference and TTA. The
experiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the
CPGNet without ensemble models or TTA is comparable with the state-of-the-art
RPVNet, while it runs 4.7 times faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perception Visualization: Seeing Through the Eyes of a DNN. (arXiv:2204.09920v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09920">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) systems power the world we live in. Deep neural
networks (DNNs) are able to solve tasks in an ever-expanding landscape of
scenarios, but our eagerness to apply these powerful models leads us to focus
on their performance and deprioritises our ability to understand them. Current
research in the field of explainable AI tries to bridge this gap by developing
various perturbation or gradient-based explanation techniques. For images,
these techniques fail to fully capture and convey the semantic information
needed to elucidate why the model makes the predictions it does. In this work,
we develop a new form of explanation that is radically different in nature from
current explanation methods, such as Grad-CAM. Perception visualization
provides a visual representation of what the DNN perceives in the input image
by depicting what visual patterns the latent representation corresponds to.
Visualizations are obtained through a reconstruction model that inverts the
encoded features, such that the parameters and predictions of the original
models are not modified. Results of our user study demonstrate that humans can
better understand and predict the system's decisions when perception
visualizations are available, thus easing the debugging and deployment of deep
models as trusted systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Training of A Two-Stage Framework for Video Restoration. (arXiv:2204.09924v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09924">
<div class="article-summary-box-inner">
<span><p>As a widely studied task, video restoration aims to enhance the quality of
the videos with multiple potential degradations, such as noises, blurs and
compression artifacts. Among video restorations, compressed video quality
enhancement and video super-resolution are two of the main tacks with
significant values in practical scenarios. Recently, recurrent neural networks
and transformers attract increasing research interests in this field, due to
their impressive capability in sequence-to-sequence modeling. However, the
training of these models is not only costly but also relatively hard to
converge, with gradient exploding and vanishing problems. To cope with these
problems, we proposed a two-stage framework including a multi-frame recurrent
network and a single-frame transformer. Besides, multiple training strategies,
such as transfer learning and progressive training, are developed to shorten
the training time and improve the model performance. Benefiting from the above
technical contributions, our solution wins two champions and a runner-up in the
NTIRE 2022 super-resolution and quality enhancement of compressed video
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale Knowledge Distillation for Unsupervised Person Re-Identification. (arXiv:2204.09931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09931">
<div class="article-summary-box-inner">
<span><p>Unsupervised person re-identification is a challenging and promising task in
the computer vision. Nowadays unsupervised person re-identification methods
have achieved great improvements by training with pseudo labels. However, the
appearance and label noise are less explicitly studied in the unsupervised
manner. To relieve the effects of appearance noise the global features
involved, we also take into account the features from two local views and
produce multi-scale features. We explore the knowledge distillation to filter
label noise, Specifically, we first train a teacher model from noisy pseudo
labels in a iterative way, and then use the teacher model to guide the learning
of our student model. In our setting, the student model could converge fast in
the supervision of the teacher model thus reduce the interference of noisy
labels as the teacher model greatly suffered. After carefully handling the
noises in the feature learning, Our multi-scale knowledge distillation are
proven to be very effective in the unsupervised re-identification. Extensive
experiments on three popular person re-identification datasets demonstrate the
superiority of our method. Especially, our approach achieves a state-of-the-art
accuracy 85.7% @mAP or 94.3% @Rank-1 on the challenging Market-1501 benchmark
with ResNet-50 under the fully unsupervised setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Invariant Model with Graph Convolutional Network for Mammogram Classification. (arXiv:2204.09954v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09954">
<div class="article-summary-box-inner">
<span><p>Due to its safety-critical property, the image-based diagnosis is desired to
achieve robustness on out-of-distribution (OOD) samples. A natural way towards
this goal is capturing only clinically disease-related features, which is
composed of macroscopic attributes (e.g., margins, shapes) and microscopic
image-based features (e.g., textures) of lesion-related areas. However, such
disease-related features are often interweaved with data-dependent (but disease
irrelevant) biases during learning, disabling the OOD generalization. To
resolve this problem, we propose a novel framework, namely Domain Invariant
Model with Graph Convolutional Network (DIM-GCN), which only exploits invariant
disease-related features from multiple domains. Specifically, we first propose
a Bayesian network, which explicitly decomposes the latent variables into
disease-related and other disease-irrelevant parts that are provable to be
disentangled from each other. Guided by this, we reformulate the objective
function based on Variational Auto-Encoder, in which the encoder in each domain
has two branches: the domain-independent and -dependent ones, which
respectively encode disease-related and -irrelevant features. To better capture
the macroscopic features, we leverage the observed clinical attributes as a
goal for reconstruction, via Graph Convolutional Network (GCN). Finally, we
only implement the disease-related features for prediction. The effectiveness
and utility of our method are demonstrated by the superior OOD generalization
performance over others on mammogram benign/malignant diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Referring Expression Comprehension via Cross-Level Multi-Modal Fusion. (arXiv:2204.09957v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09957">
<div class="article-summary-box-inner">
<span><p>As an important and challenging problem in vision-language tasks, referring
expression comprehension (REC) aims to localize the target object specified by
a given referring expression. Recently, most of the state-of-the-art REC
methods mainly focus on multi-modal fusion while overlooking the inherent
hierarchical information contained in visual and language encoders. Considering
that REC requires visual and textual hierarchical information for accurate
target localization, and encoders inherently extract features in a hierarchical
fashion, we propose to effectively utilize the rich hierarchical information
contained in different layers of visual and language encoders. To this end, we
design a Cross-level Multi-modal Fusion (CMF) framework, which gradually
integrates visual and textual features of multi-layer through intra- and
inter-modal. Experimental results on RefCOCO, RefCOCO+, RefCOCOg, and
ReferItGame datasets demonstrate the proposed framework achieves significant
performance improvements over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChildPredictor: A Child Face Prediction Framework with Disentangled Learning. (arXiv:2204.09962v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09962">
<div class="article-summary-box-inner">
<span><p>The appearances of children are inherited from their parents, which makes it
feasible to predict them. Predicting realistic children's faces may help settle
many social problems, such as age-invariant face recognition, kinship
verification, and missing child identification. It can be regarded as an
image-to-image translation task. Existing approaches usually assume domain
information in the image-to-image translation can be interpreted by "style",
i.e., the separation of image content and style. However, such separation is
improper for the child face prediction, because the facial contours between
children and parents are not the same. To address this issue, we propose a new
disentangled learning strategy for children's face prediction. We assume that
children's faces are determined by genetic factors (compact family features,
e.g., face contour), external factors (facial attributes irrelevant to
prediction, such as moustaches and glasses), and variety factors (individual
properties for each child). On this basis, we formulate predictions as a
mapping from parents' genetic factors to children's genetic factors, and
disentangle them from external and variety factors. In order to obtain accurate
genetic factors and perform the mapping, we propose a ChildPredictor framework.
It transfers human faces to genetic factors by encoders and back by generators.
Then, it learns the relationship between the genetic factors of parents and
children through a mapping function. To ensure the generated faces are
realistic, we collect a large Family Face Database to train ChildPredictor and
evaluate it on the FF-Database validation set. Experimental results demonstrate
that ChildPredictor is superior to other well-known image-to-image translation
methods in predicting realistic and diverse child faces. Implementation codes
can be found at https://github.com/zhaoyuzhi/ChildPredictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-Guided Convolutional Neural Network for Cross-View Geolocalization. (arXiv:2204.09967v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09967">
<div class="article-summary-box-inner">
<span><p>Ground-to-aerial geolocalization refers to localizing a ground-level query
image by matching it to a reference database of geo-tagged aerial imagery. This
is very challenging due to the huge perspective differences in visual
appearances and geometric configurations between these two views. In this work,
we propose a novel Transformer-guided convolutional neural network (TransGCNN)
architecture, which couples CNN-based local features with Transformer-based
global representations for enhanced representation learning. Specifically, our
TransGCNN consists of a CNN backbone extracting feature map from an input image
and a Transformer head modeling global context from the CNN map. In particular,
our Transformer head acts as a spatial-aware importance generator to select
salient CNN features as the final feature representation. Such a coupling
procedure allows us to leverage a lightweight Transformer network to greatly
enhance the discriminative capability of the embedded features. Furthermore, we
design a dual-branch Transformer head network to combine image features from
multi-scale windows in order to improve details of the global feature
representation. Extensive experiments on popular benchmark datasets demonstrate
that our model achieves top-1 accuracy of 94.12\% and 84.92\% on CVUSA and
CVACT_val, respectively, which outperforms the second-performing baseline with
less than 50% parameters and almost 2x higher frame rate, therefore achieving a
preferable accuracy-efficiency tradeoff.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DGECN: A Depth-Guided Edge Convolutional Network for End-to-End 6D Pose Estimation. (arXiv:2204.09983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09983">
<div class="article-summary-box-inner">
<span><p>Monocular 6D pose estimation is a fundamental task in computer vision.
Existing works often adopt a two-stage pipeline by establishing correspondences
and utilizing a RANSAC algorithm to calculate 6 degrees-of-freedom (6DoF) pose.
Recent works try to integrate differentiable RANSAC algorithms to achieve an
end-to-end 6D pose estimation. However, most of them hardly consider the
geometric features in 3D space, and ignore the topology cues when performing
differentiable RANSAC algorithms. To this end, we proposed a Depth-Guided Edge
Convolutional Network (DGECN) for 6D pose estimation task. We have made efforts
from the following three aspects: 1) We take advantages ofestimated depth
information to guide both the correspondences-extraction process and the
cascaded differentiable RANSAC algorithm with geometric information. 2)We
leverage the uncertainty ofthe estimated depth map to improve accuracy and
robustness ofthe output 6D pose. 3) We propose a differentiable
Perspective-n-Point(PnP) algorithm via edge convolution to explore the topology
relations between 2D-3D correspondences. Experiments demonstrate that our
proposed network outperforms current works on both effectiveness and
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach. (arXiv:2204.09992v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09992">
<div class="article-summary-box-inner">
<span><p>Conventional model quantization methods use a fixed quantization scheme to
different data samples, which ignores the inherent "recognition difficulty"
differences between various samples. We propose to feed different data samples
with varying quantization schemes to achieve a data-dependent dynamic
inference, at a fine-grained layer level. However, enabling this adaptive
inference with changeable layer-wise quantization schemes is challenging
because the combination of bit-widths and layers is growing exponentially,
making it extremely difficult to train a single model in such a vast searching
space and use it in practice. To solve this problem, we present the Arbitrary
Bit-width Network (ABN), where the bit-widths of a single deep network can
change at runtime for different data samples, with a layer-wise granularity.
Specifically, first we build a weight-shared layer-wise quantizable
"super-network" in which each layer can be allocated with multiple bit-widths
and thus quantized differently on demand. The super-network provides a
considerably large number of combinations of bit-widths and layers, each of
which can be used during inference without retraining or storing myriad models.
Second, based on the well-trained super-network, each layer's runtime bit-width
selection decision is modeled as a Markov Decision Process (MDP) and solved by
an adaptive inference strategy accordingly. Experiments show that the
super-network can be built without accuracy degradation, and the bit-widths
allocation of each layer can be adjusted to deal with various inputs on the
fly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement
while saving 36.2% BitOps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Learning the Invisible in Photoacoustic Tomography with Flat Directionally Sensitive Detector. (arXiv:2204.10001v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10001">
<div class="article-summary-box-inner">
<span><p>In photoacoustic tomography (PAT) with flat sensor, we routinely encounter
two types of limited data. The first is due to using a finite sensor and is
especially perceptible if the region of interest is large relatively to the
sensor or located farther away from the sensor. In this paper, we focus on the
second type caused by a varying sensitivity of the sensor to the incoming
wavefront direction which can be modelled as binary i.e. by a cone of
sensitivity. Such visibility conditions result, in Fourier domain, in a
restriction of both the image and the data to a bowtie, akin to the one
corresponding to the range of the forward operator. The visible ranges, in
image and data domains, are related by the wavefront direction mapping. We
adapt the wedge restricted Curvelet decomposition, we previously proposed for
the representation of the full PAT data, to separate the visible and invisible
wavefronts in the image. We optimally combine fast approximate operators with
tailored deep neural network architectures into efficient learned
reconstruction methods which perform reconstruction of the visible coefficients
and the invisible coefficients are learned from a training set of similar data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fluctuation-based Outlier Detection. (arXiv:2204.10007v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10007">
<div class="article-summary-box-inner">
<span><p>Outlier detection is an important topic in machine learning and has been used
in a wide range of applications. Outliers are objects that are few in number
and deviate from the majority of objects. As a result of these two properties,
we show that outliers are susceptible to a mechanism called fluctuation. This
article proposes a method called fluctuation-based outlier detection (FBOD)
that achieves a low linear time complexity and detects outliers purely based on
the concept of fluctuation without employing any distance, density or isolation
measure. Fundamentally different from all existing methods. FBOD first converts
the Euclidean structure datasets into graphs by using random links, then
propagates the feature value according to the connection of the graph. Finally,
by comparing the difference between the fluctuation of an object and its
neighbors, FBOD determines the object with a larger difference as an outlier.
The results of experiments comparing FBOD with seven state-of-the-art
algorithms on eight real-world tabular datasets and three video datasets show
that FBOD outperforms its competitors in the majority of cases and that FBOD
has only 5% of the execution time of the fastest algorithm. The experiment
codes are available at:
https://github.com/FluctuationOD/Fluctuation-based-Outlier-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Fewer Labels: Support Pair Active Learning for Person Re-identification. (arXiv:2204.10008v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10008">
<div class="article-summary-box-inner">
<span><p>Supervised-learning based person re-identification (re-id) require a large
amount of manual labeled data, which is not applicable in practical re-id
deployment. In this work, we propose a Support Pair Active Learning (SPAL)
framework to lower the manual labeling cost for large-scale person
reidentification. The support pairs can provide the most informative
relationships and support the discriminative feature learning. Specifically, we
firstly design a dual uncertainty selection strategy to iteratively discover
support pairs and require human annotations. Afterwards, we introduce a
constrained clustering algorithm to propagate the relationships of labeled
support pairs to other unlabeled samples. Moreover, a hybrid learning strategy
consisting of an unsupervised contrastive loss and a supervised support pair
loss is proposed to learn the discriminative re-id feature representation. The
proposed overall framework can effectively lower the labeling cost by mining
and leveraging the critical support pairs. Extensive experiments demonstrate
the superiority of the proposed method over state-of-the-art active learning
methods on large-scale person re-id benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Domain Gap in LiDAR Object Detection Networks. (arXiv:2204.10024v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10024">
<div class="article-summary-box-inner">
<span><p>In order to make autonomous driving a reality, artificial neural networks
have to work reliably in the open-world. However, the open-world is vast and
continuously changing, so it is not technically feasible to collect and
annotate training datasets which accurately represent this domain. Therefore,
there are always domain gaps between training datasets and the open-world which
must be understood. In this work, we investigate the domain gaps between
high-resolution and low-resolution LiDAR sensors in object detection networks.
Using a unique dataset, which enables us to study sensor resolution domain gaps
independent of other effects, we show two distinct domain gaps - an inference
domain gap and a training domain gap. The inference domain gap is characterised
by a strong dependence on the number of LiDAR points per object, while the
training gap shows no such dependence. These fndings show that different
approaches are required to close these inference and training domain gaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Neuron Coverage Needed to Make Person Detection More Robust?. (arXiv:2204.10027v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10027">
<div class="article-summary-box-inner">
<span><p>The growing use of deep neural networks (DNNs) in safety- and
security-critical areas like autonomous driving raises the need for their
systematic testing. Coverage-guided testing (CGT) is an approach that applies
mutation or fuzzing according to a predefined coverage metric to find inputs
that cause misbehavior. With the introduction of a neuron coverage metric, CGT
has also recently been applied to DNNs. In this work, we apply CGT to the task
of person detection in crowded scenes. The proposed pipeline uses YOLOv3 for
person detection and includes finding DNN bugs via sampling and mutation, and
subsequent DNN retraining on the updated training set. To be a bug, we require
a mutated image to cause a significant performance drop compared to a clean
input. In accordance with the CGT, we also consider an additional requirement
of increased coverage in the bug definition. In order to explore several types
of robustness, our approach includes natural image transformations,
corruptions, and adversarial examples generated with the Daedalus attack. The
proposed framework has uncovered several thousand cases of incorrect DNN
behavior. The relative change in mAP performance of the retrained models
reached on average between 26.21\% and 64.24\% for different robustness types.
However, we have found no evidence that the investigated coverage metrics can
be advantageously used to improve robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Dataset and Transformer for Stereoscopic Video Super-Resolution. (arXiv:2204.10039v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10039">
<div class="article-summary-box-inner">
<span><p>Stereo video super-resolution (SVSR) aims to enhance the spatial resolution
of the low-resolution video by reconstructing the high-resolution video. The
key challenges in SVSR are preserving the stereo-consistency and
temporal-consistency, without which viewers may experience 3D fatigue. There
are several notable works on stereoscopic image super-resolution, but there is
little research on stereo video super-resolution. In this paper, we propose a
novel Transformer-based model for SVSR, namely Trans-SVSR. Trans-SVSR comprises
two key novel components: a spatio-temporal convolutional self-attention layer
and an optical flow-based feed-forward layer that discovers the correlation
across different video frames and aligns the features. The parallax attention
mechanism (PAM) that uses the cross-view information to consider the
significant disparities is used to fuse the stereo views. Due to the lack of a
benchmark dataset suitable for the SVSR task, we collected a new stereoscopic
video dataset, SVSR-Set, containing 71 full high-definition (HD) stereo videos
captured using a professional stereo camera. Extensive experiments on the
collected dataset, along with two other datasets, demonstrate that the
Trans-SVSR can achieve competitive performance compared to the state-of-the-art
methods. Project code and additional results are available at
https://github.com/H-deep/Trans-SVSR/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Shape Completion via Adversarial Shape Priors. (arXiv:2204.10060v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10060">
<div class="article-summary-box-inner">
<span><p>We present a novel neural implicit shape method for partial point cloud
completion. To that end, we combine a conditional Deep-SDF architecture with
learned, adversarial shape priors. More specifically, our network converts
partial inputs into a global latent code and then recovers the full geometry
via an implicit, signed distance generator. Additionally, we train a PointNet++
discriminator that impels the generator to produce plausible, globally
consistent reconstructions. In that way, we effectively decouple the challenges
of predicting shapes that are both realistic, i.e. imitate the training set's
pose distribution, and accurate in the sense that they replicate the partial
input observations. In our experiments, we demonstrate state-of-the-art
performance for completing partial shapes, considering both man-made objects
(e.g. airplanes, chairs, ...) and deformable shape categories (human bodies).
Finally, we show that our adversarial training approach leads to visually
plausible reconstructions that are highly consistent in recovering missing
parts of a given object.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Absolute Wrong Makes Better: Boosting Weakly Supervised Object Detection via Negative Deterministic Information. (arXiv:2204.10068v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10068">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object detection (WSOD) is a challenging task, in which
image-level labels (e.g., categories of the instances in the whole image) are
used to train an object detector. Many existing methods follow the standard
multiple instance learning (MIL) paradigm and have achieved promising
performance. However, the lack of deterministic information leads to part
domination and missing instances. To address these issues, this paper focuses
on identifying and fully exploiting the deterministic information in WSOD. We
discover that negative instances (i.e. absolutely wrong instances), ignored in
most of the previous studies, normally contain valuable deterministic
information. Based on this observation, we here propose a negative
deterministic information (NDI) based method for improving WSOD, namely
NDI-WSOD. Specifically, our method consists of two stages: NDI collecting and
exploiting. In the collecting stage, we design several processes to identify
and distill the NDI from negative instances online. In the exploiting stage, we
utilize the extracted NDI to construct a novel negative contrastive learning
mechanism and a negative guided instance selection strategy for dealing with
the issues of part domination and missing instances, respectively. Experimental
results on several public benchmarks including VOC 2007, VOC 2012 and MS COCO
show that our method achieves satisfactory performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn from Unpaired Data for Image Restoration: A Variational Bayes Approach. (arXiv:2204.10090v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10090">
<div class="article-summary-box-inner">
<span><p>Collecting paired training data is difficult in practice, but the unpaired
samples broadly exist. Current approaches aim at generating synthesized
training data from the unpaired samples by exploring the relationship between
the corrupted and clean data. This work proposes LUD-VAE, a deep generative
method to learn the joint probability density function from data sampled from
marginal distributions. Our approach is based on a carefully designed
probabilistic graphical model in which the clean and corrupted data domains are
conditionally independent. Using variational inference, we maximize the
evidence lower bound (ELBO) to estimate the joint probability density function.
Furthermore, we show that the ELBO is computable without paired samples under
the inference invariant assumption. This property provides the mathematical
rationale of our approach in the unpaired setting. Finally, we apply our method
to real-world image denoising and super-resolution tasks and train the models
using the synthetic data generated by the LUD-VAE. Experimental results
validate the advantages of our method over other learnable approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R2-Trans:Fine-Grained Visual Categorization with Redundancy Reduction. (arXiv:2204.10095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10095">
<div class="article-summary-box-inner">
<span><p>Fine-grained visual categorization (FGVC) aims to discriminate similar
subcategories, whose main challenge is the large intraclass diversities and
subtle inter-class differences. Existing FGVC methods usually select
discriminant regions found by a trained model, which is prone to neglect other
potential discriminant information. On the other hand, the massive interactions
between the sequence of image patches in ViT make the resulting class-token
contain lots of redundant information, which may also impacts FGVC performance.
In this paper, we present a novel approach for FGVC, which can simultaneously
make use of partial yet sufficient discriminative information in environmental
cues and also compress the redundant information in class-token with respect to
the target. Specifically, our model calculates the ratio of high-weight regions
in a batch, adaptively adjusts the masking threshold and achieves moderate
extraction of background information in the input space. Moreover, we also use
the Information Bottleneck~(IB) approach to guide our network to learn a
minimum sufficient representations in the feature space. Experimental results
on three widely-used benchmark datasets verify that our approach can achieve
outperforming performance than other state-of-the-art approaches and baseline
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAF-NAU: Gramian Angular Field encoded Neighborhood Attention U-Net for Pixel-Wise Hyperspectral Image Classification. (arXiv:2204.10099v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10099">
<div class="article-summary-box-inner">
<span><p>Hyperspectral image (HSI) classification is the most vibrant area of research
in the hyperspectral community due to the rich spectral information contained
in HSI can greatly aid in identifying objects of interest. However, inherent
non-linearity between materials and the corresponding spectral profiles brings
two major challenges in HSI classification: interclass similarity and
intraclass variability. Many advanced deep learning methods have attempted to
address these issues from the perspective of a region/patch-based approach,
instead of a pixel-based alternate. However, the patch-based approaches
hypothesize that neighborhood pixels of a target pixel in a fixed spatial
window belong to the same class. And this assumption is not always true. To
address this problem, we herein propose a new deep learning architecture,
namely Gramian Angular Field encoded Neighborhood Attention U-Net (GAF-NAU),
for pixel-based HSI classification. The proposed method does not require
regions or patches centered around a raw target pixel to perform 2D-CNN based
classification, instead, our approach transforms 1D pixel vector in HSI into 2D
angular feature space using Gramian Angular Field (GAF) and then embed it to a
new neighborhood attention network to suppress irrelevant angular feature while
emphasizing on pertinent features useful for HSI classification task.
Evaluation results on three publicly available HSI datasets demonstrate the
superior performance of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Working memory inspired hierarchical video decomposition with transformative representations. (arXiv:2204.10105v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10105">
<div class="article-summary-box-inner">
<span><p>Video decomposition is very important to extract moving foreground objects
from complex backgrounds in computer vision, machine learning, and medical
imaging, e.g., extracting moving contrast-filled vessels from the complex and
noisy backgrounds of X-ray coronary angiography (XCA). However, the challenges
caused by dynamic backgrounds, overlapping heterogeneous environments and
complex noises still exist in video decomposition. To solve these problems,
this study is the first to introduce a flexible visual working memory model in
video decomposition tasks to provide interpretable and high-performance
hierarchical deep architecture, integrating the transformative representations
between sensory and control layers from the perspective of visual and cognitive
neuroscience. Specifically, robust PCA unrolling networks acting as a
structure-regularized sensor layer decompose XCA into sparse/low-rank
structured representations to separate moving contrast-filled vessels from
noisy and complex backgrounds. Then, patch recurrent convolutional LSTM
networks with a backprojection module embody unstructured random
representations of the control layer in working memory, recurrently projecting
spatiotemporally decomposed nonlocal patches into orthogonal subspaces for
heterogeneous vessel retrieval and interference suppression. This video
decomposition deep architecture effectively restores the heterogeneous profiles
of intensity and the geometries of moving objects against the complex
background interferences. Experiments show that the proposed method
significantly outperforms state-of-the-art methods in accurate moving
contrast-filled vessel extraction with excellent flexibility and computational
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Model-Based Super-Resolution with Non-uniform Blur. (arXiv:2204.10109v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10109">
<div class="article-summary-box-inner">
<span><p>We propose a state-of-the-art method for super-resolution with non-uniform
blur. Single-image super-resolution methods seek to restore a high-resolution
image from blurred, subsampled, and noisy measurements. Despite their
impressive performance, existing techniques usually assume a uniform blur
kernel. Hence, these techniques do not generalize well to the more general case
of non-uniform blur. Instead, in this paper, we address the more realistic and
computationally challenging case of spatially-varying blur. To this end, we
first propose a fast deep plug-and-play algorithm, based on linearized ADMM
splitting techniques, which can solve the super-resolution problem with
spatially-varying blur. Second, we unfold our iterative algorithm into a single
network and train it end-to-end. In this way, we overcome the intricacy of
manually tuning the parameters involved in the optimization scheme. Our
algorithm presents remarkable performance and generalizes well after a single
training to a large family of spatially-varying blur kernels, noise levels and
scale factors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSSO: Obtaining Skeletal Shape from Outside. (arXiv:2204.10129v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10129">
<div class="article-summary-box-inner">
<span><p>We address the problem of inferring the anatomic skeleton of a person, in an
arbitrary pose, from the 3D surface of the body; i.e. we predict the inside
(bones) from the outside (skin). This has many applications in medicine and
biomechanics. Existing state-of-the-art biomechanical skeletons are detailed
but do not easily generalize to new subjects. Additionally, computer vision and
graphics methods that predict skeletons are typically heuristic, not learned
from data, do not leverage the full 3D body surface, and are not validated
against ground truth. To our knowledge, our system, called OSSO (Obtaining
Skeletal Shape from Outside), is the first to learn the mapping from the 3D
body surface to the internal skeleton from real data. We do so using 1000 male
and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit
a parametric 3D body shape model (STAR) to capture the body surface and a novel
part-based 3D skeleton model to capture the bones. This provides inside/outside
training pairs. We model the statistical variation of full skeletons using PCA
in a pose-normalized space. We then train a regressor from body shape
parameters to skeleton shape parameters and refine the skeleton to satisfy
constraints on physical plausibility. Given an arbitrary 3D body shape and
pose, OSSO predicts a realistic skeleton inside. In contrast to previous work,
we evaluate the accuracy of the skeleton shape quantitatively on held-out DXA
scans, outperforming the state-of-the-art. We also show 3D skeleton prediction
from varied and challenging 3D bodies. The code to infer a skeleton from a body
shape is available for research at https://osso.is.tue.mpg.de/, and the dataset
of paired outer surface (skin) and skeleton (bone) meshes is available as a
Biobank Returned Dataset. This research has been conducted using the UK Biobank
Resource.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Fast, Flexible, and Robust Low-Light Image Enhancement. (arXiv:2204.10137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10137">
<div class="article-summary-box-inner">
<span><p>Existing low-light image enhancement techniques are mostly not only difficult
to deal with both visual quality and computational efficiency but also commonly
invalid in unknown complex scenarios. In this paper, we develop a new
Self-Calibrated Illumination (SCI) learning framework for fast, flexible, and
robust brightening images in real-world low-light scenarios. To be specific, we
establish a cascaded illumination learning process with weight sharing to
handle this task. Considering the computational burden of the cascaded pattern,
we construct the self-calibrated module which realizes the convergence between
results of each stage, producing the gains that only use the single basic block
for inference (yet has not been exploited in previous works), which drastically
diminishes computation cost. We then define the unsupervised training loss to
elevate the model capability that can adapt to general scenes. Further, we make
comprehensive explorations to excavate SCI's inherent properties (lacking in
existing works) including operation-insensitive adaptability (acquiring stable
performance under the settings of different simple operations) and
model-irrelevant generality (can be applied to illumination-based existing
works to improve performance). Finally, plenty of experiments and ablation
studies fully indicate our superiority in both quality and efficiency.
Applications on low-light face detection and nighttime semantic segmentation
fully reveal the latent practical values for SCI. The source code is available
at https://github.com/vis-opt-group/SCI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple EffNet/ResNet Architectures for Melanoma Classification. (arXiv:2204.10142v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10142">
<div class="article-summary-box-inner">
<span><p>Melanoma is the most malignant skin tumor and usually cancerates from normal
moles, which is difficult to distinguish benign from malignant in the early
stage. Therefore, many machine learning methods are trying to make auxiliary
prediction. However, these methods attach more attention to the image data of
suspected tumor, and focus on improving the accuracy of image classification,
but ignore the significance of patient-level contextual information for disease
diagnosis in actual clinical diagnosis. To make more use of patient information
and improve the accuracy of diagnosis, we propose a new melanoma classification
model based on EffNet and Resnet. Our model not only uses images within the
same patient but also consider patient-level contextual information for better
cancer prediction. The experimental results demonstrated that the proposed
model achieved 0.981 ACC. Furthermore, we note that the overall ROC value of
the model is 0.976 which is better than the previous state-of-the-art
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A case for using rotation invariant features in state of the art feature matchers. (arXiv:2204.10144v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10144">
<div class="article-summary-box-inner">
<span><p>The aim of this paper is to demonstrate that a state of the art feature
matcher (LoFTR) can be made more robust to rotations by simply replacing the
backbone CNN with a steerable CNN which is equivariant to translations and
image rotations. It is experimentally shown that this boost is obtained without
reducing performance on ordinary illumination and viewpoint matching sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebFace260M: A Benchmark for Million-Scale Deep Face Recognition. (arXiv:2204.10149v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10149">
<div class="article-summary-box-inner">
<span><p>Face benchmarks empower the research community to train and evaluate
high-performance face recognition systems. In this paper, we contribute a new
million-scale recognition benchmark, containing uncurated 4M identities/260M
faces (WebFace260M) and cleaned 2M identities/42M faces (WebFace42M) training
data, as well as an elaborately designed time-constrained evaluation protocol.
Firstly, we collect 4M name lists and download 260M faces from the Internet.
Then, a Cleaning Automatically utilizing Self-Training (CAST) pipeline is
devised to purify the tremendous WebFace260M, which is efficient and scalable.
To the best of our knowledge, the cleaned WebFace42M is the largest public face
recognition training set and we expect to close the data gap between academia
and industry. Referring to practical deployments, Face Recognition Under
Inference Time conStraint (FRUITS) protocol and a new test set with rich
attributes are constructed. Besides, we gather a large-scale masked face
sub-set for biometrics assessment under COVID-19. For a comprehensive
evaluation of face matchers, three recognition tasks are performed under
standard, masked and unbiased settings, respectively. Equipped with this
benchmark, we delve into million-scale face recognition problems. A distributed
framework is developed to train face recognition models efficiently without
tampering with the performance. Enabled by WebFace42M, we reduce 40% failure
rate on the challenging IJB-C set and rank 3rd among 430 entries on NIST-FRVT.
Even 10% data (WebFace4M) shows superior performance compared with the public
training sets. Furthermore, comprehensive baselines are established under the
FRUITS-100/500/1000 milliseconds protocols. The proposed benchmark shows
enormous potential on standard, masked and unbiased face recognition scenarios.
Our WebFace260M website is https://www.face-benchmark.org.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Person Video Dataset Annotation Method of Spatio-Temporally Actions. (arXiv:2204.10160v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10160">
<div class="article-summary-box-inner">
<span><p>Spatio-temporal action detection is an important and challenging problem in
video understanding. However, the application of the existing large-scale
spatio-temporal action datasets in specific fields is limited, and there is
currently no public tool for making spatio-temporal action datasets, it takes a
lot of time and effort for researchers to customize the spatio-temporal action
datasets, so we propose a multi-Person video dataset Annotation Method of
spatio-temporally actions.First, we use ffmpeg to crop the videos and frame the
videos; then use yolov5 to detect human in the video frame, and then use deep
sort to detect the ID of the human in the video frame. By processing the
detection results of yolov5 and deep sort, we can get the annotation file of
the spatio-temporal action dataset to complete the work of customizing the
spatio-temporal action dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated analysis of fibrous cap in intravascular optical coherence tomography images of coronary arteries. (arXiv:2204.10162v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10162">
<div class="article-summary-box-inner">
<span><p>Thin-cap fibroatheroma (TCFA) and plaque rupture have been recognized as the
most frequent risk factor for thrombosis and acute coronary syndrome.
Intravascular optical coherence tomography (IVOCT) can identify TCFA and assess
cap thickness, which provides an opportunity to assess plaque vulnerability. We
developed an automated method that can detect lipidous plaque and assess
fibrous cap thickness in IVOCT images. This study analyzed a total of 4,360
IVOCT image frames of 77 lesions among 41 patients. To improve segmentation
performance, preprocessing included lumen segmentation, pixel-shifting, and
noise filtering on the raw polar (r, theta) IVOCT images. We used the
DeepLab-v3 plus deep learning model to classify lipidous plaque pixels. After
lipid detection, we automatically detected the outer border of the fibrous cap
using a special dynamic programming algorithm and assessed the cap thickness.
Our method provided excellent discriminability of lipid plaque with a
sensitivity of 85.8% and A-line Dice coefficient of 0.837. By comparing lipid
angle measurements between two analysts following editing of our automated
software, we found good agreement by Bland-Altman analysis (difference 6.7+/-17
degree; mean 196 degree). Our method accurately detected the fibrous cap from
the detected lipid plaque. Automated analysis required a significant
modification for only 5.5% frames. Furthermore, our method showed a good
agreement of fibrous cap thickness between two analysts with Bland-Altman
analysis (4.2+/-14.6 micron; mean 175 micron), indicating little bias between
users and good reproducibility of the measurement. We developed a fully
automated method for fibrous cap quantification in IVOCT images, resulting in
good agreement with determinations by analysts. The method has great potential
to enable highly automated, repeatable, and comprehensive evaluations of TCFAs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BTranspose: Bottleneck Transformers for Human Pose Estimation with Self-Supervised Pre-Training. (arXiv:2204.10209v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10209">
<div class="article-summary-box-inner">
<span><p>The task of 2D human pose estimation is challenging as the number of
keypoints is typically large (~ 17) and this necessitates the use of robust
neural network architectures and training pipelines that can capture the
relevant features from the input image. These features are then aggregated to
make accurate heatmap predictions from which the final keypoints of human body
parts can be inferred. Many papers in literature use CNN-based architectures
for the backbone, and/or combine it with a transformer, after which the
features are aggregated to make the final keypoint predictions [1]. In this
paper, we consider the recently proposed Bottleneck Transformers [2], which
combine CNN and multi-head self attention (MHSA) layers effectively, and we
integrate it with a Transformer encoder and apply it to the task of 2D human
pose estimation. We consider different backbone architectures and pre-train
them using the DINO self-supervised learning method [3], this pre-training is
found to improve the overall prediction accuracy. We call our model BTranspose,
and experiments show that on the COCO validation set, our model achieves an AP
of 76.4, which is competitive with other methods such as [1] and has fewer
network parameters. Furthermore, we also present the dependencies of the final
predicted keypoints on both the MHSA block and the Transformer encoder layers,
providing clues on the image sub-regions the network attends to at the mid and
high levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SmartPortraits: Depth Powered Handheld Smartphone Dataset of Human Portraits for State Estimation, Reconstruction and Synthesis. (arXiv:2204.10211v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10211">
<div class="article-summary-box-inner">
<span><p>We present a dataset of 1000 video sequences of human portraits recorded in
real and uncontrolled conditions by using a handheld smartphone accompanied by
an external high-quality depth camera. The collected dataset contains 200
people captured in different poses and locations and its main purpose is to
bridge the gap between raw measurements obtained from a smartphone and
downstream applications, such as state estimation, 3D reconstruction, view
synthesis, etc. The sensors employed in data collection are the smartphone's
camera and Inertial Measurement Unit (IMU), and an external Azure Kinect DK
depth camera software synchronized with sub-millisecond precision to the
smartphone system. During the recording, the smartphone flash is used to
provide a periodic secondary source of lightning. Accurate mask of the foremost
person is provided as well as its impact on the camera alignment accuracy. For
evaluation purposes, we compare multiple state-of-the-art camera alignment
methods by using a Motion Capture system. We provide a smartphone
visual-inertial benchmark for portrait capturing, where we report results for
multiple methods and motivate further use of the provided trajectories,
available in the dataset, in view synthesis and 3D reconstruction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OCTOPUS -- optical coherence tomography plaque and stent analysis software. (arXiv:2204.10212v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10212">
<div class="article-summary-box-inner">
<span><p>Compared with other imaging modalities, intravascular optical coherence
tomography (IVOCT) has significant advantages for guiding percutaneous coronary
interventions. To aid IVOCT research studies, we developed the Optical
Coherence TOmography PlaqUe and Stent (OCTOPUS) analysis software. To automate
image analysis results, the software includes several important algorithmic
steps: pre-processing, deep learning plaque segmentation, machine learning
identification of stent struts, and registration of pullbacks. Interactive
visualization and manual editing of segmentations were included in the
software. Quantifications include stent deployment characteristics (e.g., stent
strut malapposition), strut level analysis, calcium angle, and calcium
thickness measurements. Interactive visualizations include (x,y) anatomical, en
face, and longitudinal views with optional overlays. Underlying plaque
segmentation algorithm yielded excellent pixel-wise results (86.2% sensitivity
and 0.781 F1 score). Using OCTOPUS on 34 new pullbacks, we determined that
following automated segmentation, only 13% and 23% of frames needed any manual
touch up for detailed lumen and calcification labeling, respectively. Only up
to 3.8% of plaque pixels were modified, leading to an average editing time of
only 7.5 seconds/frame, an approximately 80% reduction compared to manual
analysis. Regarding stent analysis, sensitivity and precision were both greater
than 90%, and each strut was successfully classified as either covered or
uncovered with high sensitivity (94%) and specificity (90%). We introduced and
evaluated the clinical application of a highly automated software package,
OCTOPUS, for quantitative plaque and stent analysis in IVOCT images. The
software is currently used as an offline tool for research purposes; however,
the software's embedded algorithms may also be useful for real-time treatment
planning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Planes vs. Chairs: Category-guided 3D shape learning without any 3D cues. (arXiv:2204.10235v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10235">
<div class="article-summary-box-inner">
<span><p>We present a novel 3D shape reconstruction method which learns to predict an
implicit 3D shape representation from a single RGB image. Our approach uses a
set of single-view images of multiple object categories without viewpoint
annotation, forcing the model to learn across multiple object categories
without 3D supervision. To facilitate learning with such minimal supervision,
we use category labels to guide shape learning with a novel categorical metric
learning approach. We also utilize adversarial and viewpoint regularization
techniques to further disentangle the effects of viewpoint and shape. We obtain
the first results for large-scale (more than 50 categories) single-viewpoint
shape prediction using a single model without any 3D cues. We are also the
first to examine and quantify the benefit of class information in single-view
supervised 3D shape reconstruction. Our method achieves superior performance
over state-of-the-art methods on ShapeNet-13, ShapeNet-55 and Pascal3D+.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HEATGait: Hop-Extracted Adjacency Technique in Graph Convolution based Gait Recognition. (arXiv:2204.10238v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10238">
<div class="article-summary-box-inner">
<span><p>Biometric authentication using gait has become a promising field due to its
unobtrusive nature. Recent approaches in model-based gait recognition
techniques utilize spatio-temporal graphs for the elegant extraction of gait
features. However, existing methods often rely on multi-scale operators for
extracting long-range relationships among joints resulting in biased weighting.
In this paper, we present HEATGait, a gait recognition system that improves the
existing multi-scale graph convolution by efficient hop-extraction technique to
alleviate the issue. Combined with preprocessing and augmentation techniques,
we propose a powerful feature extractor that utilizes ResGCN to achieve
state-of-the-art performance in model-based gait recognition on the CASIA-B
gait dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Examination of Bias of Facial Analysis based BMI Prediction Models. (arXiv:2204.10262v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10262">
<div class="article-summary-box-inner">
<span><p>Obesity is one of the most important public health problems that the world is
facing today. A recent trend is in the development of intervention tools that
predict BMI using facial images for weight monitoring and management to combat
obesity. Most of these studies used BMI annotated facial image datasets that
mainly consisted of Caucasian subjects. Research on bias evaluation of
face-based gender-, age-classification, and face recognition systems suggest
that these technologies perform poorly for women, dark-skinned people, and
older adults. The bias of facial analysis-based BMI prediction tools has not
been studied until now. This paper evaluates the bias of facial-analysis-based
BMI prediction models across Caucasian and African-American Males and Females.
Experimental investigations on the gender, race, and BMI balanced version of
the modified MORPH-II dataset suggested that the error rate in BMI prediction
was least for Black Males and highest for White Females. Further, the
psychology-related facial features correlated with weight suggested that as the
BMI increases, the changes in the facial region are more prominent for Black
Males and the least for White Females. This is the reason for the least error
rate of the facial analysis-based BMI prediction tool for Black Males and
highest for White Females.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DooDLeNet: Double DeepLab Enhanced Feature Fusion for Thermal-color Semantic Segmentation. (arXiv:2204.10266v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10266">
<div class="article-summary-box-inner">
<span><p>In this paper we present a new approach for feature fusion between RGB and
LWIR Thermal images for the task of semantic segmentation for driving
perception. We propose DooDLeNet, a double DeepLab architecture with
specialized encoder-decoders for thermal and color modalities and a shared
decoder for final segmentation. We combine two strategies for feature fusion:
confidence weighting and correlation weighting. We report state-of-the-art mean
IoU results on the MF dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency. (arXiv:2204.10310v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10310">
<div class="article-summary-box-inner">
<span><p>Approaches to single-view reconstruction typically rely on viewpoint
annotations, silhouettes, the absence of background, multiple views of the same
instance, a template shape, or symmetry. We avoid all of these supervisions and
hypotheses by leveraging explicitly the consistency between images of different
object instances. As a result, our method can learn from large collections of
unlabelled images depicting the same object category. Our main contributions
are two approaches to leverage cross-instance consistency: (i) progressive
conditioning, a training strategy to gradually specialize the model from
category to instances in a curriculum learning fashion; (ii) swap
reconstruction, a loss enforcing consistency between instances having similar
shape or texture. Critical to the success of our method are also: our
structured autoencoding architecture decomposing an image into explicit shape,
texture, pose, and background; an adapted formulation of differential
rendering, and; a new optimization scheme alternating between 3D and pose
learning. We compare our approach, UNICORN, both on the diverse synthetic
ShapeNet dataset - the classical benchmark for methods requiring multiple views
as supervision - and on standard real-image benchmarks (Pascal3D+ Car, CUB-200)
for which most methods require known templates and silhouette annotations. We
also showcase applicability to more challenging real-world collections
(CompCars, LSUN), where silhouettes are not available and images are not
cropped around the object.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Human Action Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance. (arXiv:2204.10312v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10312">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel end-to-end method for the problem of
skeleton-based unsupervised human action recognition. We propose a new
architecture with a convolutional autoencoder that uses graph Laplacian
regularization to model the skeletal geometry across the temporal dynamics of
actions. Our approach is robust towards viewpoint variations by including a
self-supervised gradient reverse layer that ensures generalization across
camera views. The proposed method is validated on NTU-60 and NTU-120
large-scale datasets in which it outperforms all prior unsupervised
skeleton-based approaches on the cross-subject, cross-view, and cross-setup
protocols. Although unsupervised, our learnable representation allows our
method even to surpass a few supervised skeleton-based action recognition
methods. The code is available in:
www.github.com/IIT-PAVIS/UHAR_Skeletal_Laplacian
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Contrastive Learning by Permuting Cluster Assignments. (arXiv:2204.10314v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10314">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has gained popularity as an effective self-supervised
representation learning technique. Several research directions improve
traditional contrastive approaches, e.g., prototypical contrastive methods
better capture the semantic similarity among instances and reduce the
computational burden by considering cluster prototypes or cluster assignments,
while adversarial instance-wise contrastive methods improve robustness against
a variety of attacks. To the best of our knowledge, no prior work jointly
considers robustness, cluster-wise semantic similarity and computational
efficiency. In this work, we propose SwARo, an adversarial contrastive
framework that incorporates cluster assignment permutations to generate
representative adversarial samples. We evaluate SwARo on multiple benchmark
datasets and against various white-box and black-box attacks, obtaining
consistent improvements over state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature anomaly detection system (FADS) for intelligent manufacturing. (arXiv:2204.10318v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10318">
<div class="article-summary-box-inner">
<span><p>Anomaly detection is important for industrial automation and part quality
assurance, and while humans can easily detect anomalies in components given a
few examples, designing a generic automated system that can perform at human or
above human capabilities remains a challenge. In this work, we present a simple
new anomaly detection algorithm called FADS (feature-based anomaly detection
system) which leverages pretrained convolutional neural networks (CNN) to
generate a statistical model of nominal inputs by observing the activation of
the convolutional filters. During inference the system compares the
convolutional filter activation of the new input to the statistical model and
flags activations that are outside the expected range of values and therefore
likely an anomaly. By using a pretrained network, FADS demonstrates excellent
performance similar to or better than other machine learning approaches to
anomaly detection while at the same time FADS requires no tuning of the CNN
weights. We demonstrate FADS ability by detecting process parameter changes on
a custom dataset of additively manufactured lattices. The FADS localization
algorithm shows that textural differences that are visible on the surface can
be used to detect process parameter changes. In addition, we test FADS on
benchmark datasets, such as the MVTec Anomaly Detection dataset, and report
good results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TorchSparse: Efficient Point Cloud Inference Engine. (arXiv:2204.10319v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10319">
<div class="article-summary-box-inner">
<span><p>Deep learning on point clouds has received increased attention thanks to its
wide applications in AR/VR and autonomous driving. These applications require
low latency and high accuracy to provide real-time user experience and ensure
user safety. Unlike conventional dense workloads, the sparse and irregular
nature of point clouds poses severe challenges to running sparse CNNs
efficiently on the general-purpose hardware. Furthermore, existing sparse
acceleration techniques for 2D images do not translate to 3D point clouds. In
this paper, we introduce TorchSparse, a high-performance point cloud inference
engine that accelerates the sparse convolution computation on GPUs. TorchSparse
directly optimizes the two bottlenecks of sparse convolution: irregular
computation and data movement. It applies adaptive matrix multiplication
grouping to trade computation for better regularity, achieving 1.4-1.5x speedup
for matrix multiplication. It also optimizes the data movement by adopting
vectorized, quantized and fused locality-aware memory access, reducing the
memory movement cost by 2.7x. Evaluated on seven representative models across
three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured
end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfD: Self-Learning Large-Scale Driving Policies From the Web. (arXiv:2204.10320v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10320">
<div class="article-summary-box-inner">
<span><p>Effectively utilizing the vast amounts of ego-centric navigation data that is
freely available on the internet can advance generalized intelligent systems,
i.e., to robustly scale across perspectives, platforms, environmental
conditions, scenarios, and geographical locations. However, it is difficult to
directly leverage such large amounts of unlabeled and highly diverse data for
complex 3D reasoning and planning tasks. Consequently, researchers have
primarily focused on its use for various auxiliary pixel- and image-level
computer vision tasks that do not consider an ultimate navigational objective.
In this work, we introduce SelfD, a framework for learning scalable driving by
utilizing large amounts of online monocular images. Our key idea is to leverage
iterative semi-supervised training when learning imitative agents from
unlabeled data. To handle unconstrained viewpoints, scenes, and camera
parameters, we train an image-based model that directly learns to plan in the
Bird's Eye View (BEV) space. Next, we use unlabeled data to augment the
decision-making knowledge and robustness of an initially trained model via
self-training. In particular, we propose a pseudo-labeling step which enables
making full use of highly diverse demonstration data through "hypothetical"
planning-based data augmentation. We employ a large dataset of publicly
available YouTube videos to train SelfD and comprehensively analyze its
generalization benefits across challenging navigation scenarios. Without
requiring any additional data collection or annotation efforts, SelfD
demonstrates consistent improvements (by up to 24%) in driving performance
evaluation on nuScenes, Argoverse, Waymo, and CARLA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Future Object Prediction with a Spatiotemporal Detection Transformer. (arXiv:2204.10321v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10321">
<div class="article-summary-box-inner">
<span><p>We explore future object prediction -- a challenging problem where all
objects visible in a future video frame are to be predicted. We propose to
tackle this problem end-to-end by training a detection transformer to directly
output future objects. In order to make accurate predictions about the future,
it is necessary to capture the dynamics in the scene, both of other objects and
of the ego-camera. We extend existing detection transformers in two ways to
capture the scene dynamics. First, we experiment with three different
mechanisms that enable the model to spatiotemporally process multiple frames.
Second, we feed ego-motion information to the model via cross-attention. We
show that both of these cues substantially improve future object prediction
performance. Our final approach learns to capture the dynamics and make
predictions on par with an oracle for 100 ms prediction horizons, and
outperform baselines for longer prediction horizons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Deep Hashing Methods. (arXiv:2003.03369v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.03369">
<div class="article-summary-box-inner">
<span><p>Nearest neighbor search aims to obtain the samples in the database with the
smallest distances from them to the queries, which is a fundamental problem in
various domains, such as computer vision, recommendation systems and machine
learning. Hashing is one of the most widely used methods for its computational
and storage efficiency. With the development of deep learning, deep hashing
methods show more advantages than traditional methods. In this survey, we
detailedly investigate current deep hashing algorithms including deep
supervised hashing and deep unsupervised hashing. Specifically, we categorize
deep supervised hashing methods into pairwise methods, ranking-based methods,
pointwise methods as well as quantization according to how measuring the
similarities of the learned hash codes. Moreover, deep unsupervised hashing is
categorized into similarity reconstruction-based methods, pseudo-label-based
methods and prediction-free self-supervised learning-based methods based on
their semantic learning manners. We also introduce three related important
topics including semi-supervised deep hashing, domain adaption deep hashing and
multi-modal deep hashing. Meanwhile, we present some commonly used public
datasets and the scheme to measure the performance of deep hashing algorithms.
Finally, we discuss some potential research directions in conclusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAM: Self-supervised Learning of Pixel-wise Anatomical Embeddings in Radiological Images. (arXiv:2012.02383v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02383">
<div class="article-summary-box-inner">
<span><p>Radiological images such as computed tomography (CT) and X-rays render
anatomy with intrinsic structures. Being able to reliably locate the same
anatomical structure across varying images is a fundamental task in medical
image analysis. In principle it is possible to use landmark detection or
semantic segmentation for this task, but to work well these require large
numbers of labeled data for each anatomical structure and sub-structure of
interest. A more universal approach would learn the intrinsic structure from
unlabeled images. We introduce such an approach, called Self-supervised
Anatomical eMbedding (SAM). SAM generates semantic embeddings for each image
pixel that describes its anatomical location or body part. To produce such
embeddings, we propose a pixel-level contrastive learning framework. A
coarse-to-fine strategy ensures both global and local anatomical information
are encoded. Negative sample selection strategies are designed to enhance the
embedding's discriminability. Using SAM, one can label any point of interest on
a template image and then locate the same body part in other images by simple
nearest neighbor searching. We demonstrate the effectiveness of SAM in multiple
tasks with 2D and 3D image modalities. On a chest CT dataset with 19 landmarks,
SAM outperforms widely-used registration algorithms while only taking 0.23
seconds for inference. On two X-ray datasets, SAM, with only one labeled
template image, surpasses supervised methods trained on 50 labeled images. We
also apply SAM on whole-body follow-up lesion matching in CT and obtain an
accuracy of 91%. SAM can also be applied for improving image registration and
initializing CNN weights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">F-SIOL-310: A Robotic Dataset and Benchmark for Few-Shot Incremental Object Learning. (arXiv:2103.12242v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12242">
<div class="article-summary-box-inner">
<span><p>Deep learning has achieved remarkable success in object recognition tasks
through the availability of large scale datasets like ImageNet. However, deep
learning systems suffer from catastrophic forgetting when learning
incrementally without replaying old data. For real-world applications, robots
also need to incrementally learn new objects. Further, since robots have
limited human assistance available, they must learn from only a few examples.
However, very few object recognition datasets and benchmarks exist to test
incremental learning capability for robotic vision. Further, there is no
dataset or benchmark specifically designed for incremental object learning from
a few examples. To fill this gap, we present a new dataset termed F-SIOL-310
(Few-Shot Incremental Object Learning) which is specifically captured for
testing few-shot incremental object learning capability for robotic vision. We
also provide benchmarks and evaluations of 8 incremental learning algorithms on
F-SIOL-310 for future comparisons. Our results demonstrate that the few-shot
incremental object learning problem for robotic vision is far from being
solved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discrete Cosine Transform Network for Guided Depth Map Super-Resolution. (arXiv:2104.06977v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06977">
<div class="article-summary-box-inner">
<span><p>Guided depth super-resolution (GDSR) is an essential topic in multi-modal
image processing, which reconstructs high-resolution (HR) depth maps from
low-resolution ones collected with suboptimal conditions with the help of HR
RGB images of the same scene. To solve the challenges in interpreting the
working mechanism, extracting cross-modal features and RGB texture
over-transferred, we propose a novel Discrete Cosine Transform Network (DCTNet)
to alleviate the problems from three aspects. First, the Discrete Cosine
Transform (DCT) module reconstructs the multi-channel HR depth features by
using DCT to solve the channel-wise optimization problem derived from the image
domain. Second, we introduce a semi-coupled feature extraction module that uses
shared convolutional kernels to extract common information and private kernels
to extract modality-specific information. Third, we employ an edge attention
mechanism to highlight the contours informative for guided upsampling.
Extensive quantitative and qualitative evaluations demonstrate the
effectiveness of our DCTNet, which outperforms previous state-of-the-art
methods with a relatively small number of parameters. The code is available at
\url{https://github.com/Zhaozixiang1228/GDSR-DCTNet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning on Monocular Object Pose Detection and Tracking: A Comprehensive Overview. (arXiv:2105.14291v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14291">
<div class="article-summary-box-inner">
<span><p>Object pose detection and tracking has recently attracted increasing
attention due to its wide applications in many areas, such as autonomous
driving, robotics, and augmented reality. Among methods for object pose
detection and tracking, deep learning is the most promising one that has shown
better performance than others. However, survey study about the latest
development of deep learning-based methods is lacking. Therefore, this study
presents a comprehensive review of recent progress in object pose detection and
tracking that belongs to the deep learning technical route. To achieve a more
thorough introduction, the scope of this study is limited to methods taking
monocular RGB/RGBD data as input and covering three kinds of major tasks:
instance-level monocular object pose detection, category-level monocular object
pose detection, and monocular object pose tracking. In our work, metrics,
datasets, and methods of both detection and tracking are presented in detail.
Comparative results of current state-of-the-art methods on several publicly
available datasets are also presented, together with insightful observations
and inspiring future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image2Point: 3D Point-Cloud Understanding with Pretrained 2D ConvNets. (arXiv:2106.04180v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04180">
<div class="article-summary-box-inner">
<span><p>3D point-clouds and 2D images are different visual representations of the
physical world. While human vision can understand both representations,
computer vision models designed for 2D image and 3D point-cloud understanding
are quite different. Our paper explores the potential of transferring 2D model
architectures and weights to understand 3D point-clouds, by empirically
investigating the feasibility of the transfer, the benefits of the transfer,
and shedding light on why the transfer works. We discover that we can indeed
use the same architecture and pretrained weights of a neural net model to
understand both images and point-clouds. Specifically, we transfer the
image-pretrained model to a point-cloud model by copying or inflating the
weights. We find that \textbf{f}inetuning the transformed
\textbf{i}mage-\textbf{p}retrained models (FIP) with minimal efforts -- only on
input, output, and normalization layers -- can achieve competitive performance
on 3D point-cloud classification, beating a wide range of point-cloud models
that adopt task-specific architectures and use a variety of tricks. When
finetuning the whole model, the performance improves even further. Meanwhile,
FIP improves data efficiency, reaching up to 10.0 top-1 accuracy percent on
few-shot classification. It also speeds up the training of point-cloud models
by up to 11.1x for a target accuracy (e.g., 90 \% accuracy). Lastly, we provide
an explanation of the image to point-cloud transfer from the aspect of
\textit{neural collapse}. The code is available at:
\url{https://github.com/chenfengxu714/image2point}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface. (arXiv:2107.06393v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06393">
<div class="article-summary-box-inner">
<span><p>Modeling complex phenomena typically involves the use of both discrete and
continuous variables. Such a setting applies across a wide range of problems,
from identifying trends in time-series data to performing effective
compositional scene understanding in images. Here, we propose Hybrid Memoised
Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid
discrete-continuous models. Prior approaches to learning suffer as they need to
perform repeated expensive inner-loop discrete inference. We build on a recent
approach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by
memoising discrete variables, and extend it to allow for a principled and
effective way to handle continuous variables by learning a separate recognition
model used for importance-sampling based approximate inference and
marginalization. We evaluate HMWS in the GP-kernel learning and 3D scene
understanding domains, and show that it outperforms current state-of-the-art
inference methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Certified Robustness for Ensemble Models and Beyond. (arXiv:2107.10873v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10873">
<div class="article-summary-box-inner">
<span><p>Recent studies show that deep neural networks (DNN) are vulnerable to
adversarial examples, which aim to mislead DNNs by adding perturbations with
small magnitude. To defend against such attacks, both empirical and theoretical
defense approaches have been extensively studied for a single ML model. In this
work, we aim to analyze and provide the certified robustness for ensemble ML
models, together with the sufficient and necessary conditions of robustness for
different ensemble protocols. Although ensemble models are shown more robust
than a single model empirically; surprisingly, we find that in terms of the
certified robustness the standard ensemble models only achieve marginal
improvement compared to a single model. Thus, to explore the conditions that
guarantee to provide certifiably robust ensemble ML models, we first prove that
diversified gradient and large confidence margin are sufficient and necessary
conditions for certifiably robust ensemble models under the model-smoothness
assumption. We then provide the bounded model-smoothness analysis based on the
proposed Ensemble-before-Smoothing strategy. We also prove that an ensemble
model can always achieve higher certified robustness than a single base model
under mild conditions. Inspired by the theoretical findings, we propose the
lightweight Diversity Regularized Training (DRT) to train certifiably robust
ensemble ML models. Extensive experiments show that our DRT enhanced ensembles
can consistently achieve higher certified robustness than existing single and
ensemble ML models, demonstrating the state-of-the-art certified L2-robustness
on MNIST, CIFAR-10, and ImageNet datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The CORSMAL benchmark for the prediction of the properties of containers. (arXiv:2107.12719v3 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12719">
<div class="article-summary-box-inner">
<span><p>The contactless estimation of the weight of a container and the amount of its
content manipulated by a person are key pre-requisites for safe human-to-robot
handovers. However, opaqueness and transparencies of the container and the
content, and variability of materials, shapes, and sizes, make this estimation
difficult. In this paper, we present a range of methods and an open framework
to benchmark acoustic and visual perception for the estimation of the capacity
of a container, and the type, mass, and amount of its content. The framework
includes a dataset, specific tasks and performance measures. We conduct an
in-depth comparative analysis of methods that used this framework and
audio-only or vision-only baselines designed from related works. Based on this
analysis, we can conclude that audio-only and audio-visual classifiers are
suitable for the estimation of the type and amount of the content using
different types of convolutional neural networks, combined with either
recurrent neural networks or a majority voting strategy, whereas computer
vision methods are suitable to determine the capacity of the container using
regression and geometric approaches. Classifying the content type and level
using only audio achieves a weighted average F1-score up to 81% and 97%,
respectively. Estimating the container capacity with vision-only approaches and
estimating the filling mass with audio-visual multi-stage approaches reach up
to 65% weighted average capacity and mass scores. These results show that there
is still room for improvement on the design of new methods. These new methods
can be ranked and compared on the individual leaderboards provided by our open
framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Raising context awareness in motion forecasting. (arXiv:2109.08048v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08048">
<div class="article-summary-box-inner">
<span><p>Learning-based trajectory prediction models have encountered great success,
with the promise of leveraging contextual information in addition to motion
history. Yet, we find that state-of-the-art forecasting methods tend to overly
rely on the agent's current dynamics, failing to exploit the semantic
contextual cues provided at its input. To alleviate this issue, we introduce
CAB, a motion forecasting model equipped with a training procedure designed to
promote the use of semantic contextual information. We also introduce two novel
metrics - dispersion and convergence-to-range - to measure the temporal
consistency of successive forecasts, which we found missing in standard
metrics. Our method is evaluated on the widely adopted nuScenes Prediction
benchmark as well as on a subset of the most difficult examples from this
benchmark. The code is available at github.com/valeoai/CAB
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Random Dilated Shapelet Transform: A New Approach for Time Series Shapelets. (arXiv:2109.13514v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13514">
<div class="article-summary-box-inner">
<span><p>Shapelet-based algorithms are widely used for time series classification
because of their ease of interpretation, but they are currently outperformed by
recent state-of-the-art approaches. We present a new formulation of time series
shapelets including the notion of dilation, and we introduce a new shapelet
feature to enhance their discriminative power for classification. Experiments
performed on 112 datasets show that our method improves on the state-of-the-art
shapelet algorithm, and achieves comparable accuracy to recent state-of-the-art
approaches, without sacrificing neither scalability, nor interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole brain MRI. (arXiv:2110.03588v6 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03588">
<div class="article-summary-box-inner">
<span><p>Treatment decisions for brain metastatic disease rely on knowledge of the
primary organ site, and currently made with biopsy and histology. Here we
develop a novel deep learning approach for accurate non-invasive digital
histology with whole-brain MRI data. Our IRB-approved single-site retrospective
study was comprised of patients (n=1,399) referred for MRI treatment-planning
and gamma knife radiosurgery over 21 years. Contrast-enhanced T1-weighted and
T2-weighted Fluid-Attenuated Inversion Recovery brain MRI exams (n=1,582) were
preprocessed and input to the proposed deep learning workflow for tumor
segmentation, modality transfer, and primary site classification into one of
five classes. Ten-fold cross-validation generated overall AUC of 0.878
(95%CI:0.873,0.883), lung class AUC of 0.889 (95%CI:0.883,0.895), breast class
AUC of 0.873 (95%CI:0.860,0.886), melanoma class AUC of 0.852
(95%CI:0.842,0.862), renal class AUC of 0.830 (95%CI:0.809,0.851), and other
class AUC of 0.822 (95%CI:0.805,0.839). These data establish that whole-brain
imaging features are discriminative to allow accurate diagnosis of the primary
organ site of malignancy. Our end-to-end deep radiomic approach has great
potential for classifying metastatic tumor types from whole-brain MRI images.
Further refinement may offer an invaluable clinical tool to expedite primary
cancer site identification for precision treatment and improved outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Synchronicity. (arXiv:2111.05329v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05329">
<div class="article-summary-box-inner">
<span><p>We present CrissCross, a self-supervised framework for learning audio-visual
representations. A novel notion is introduced in our framework whereby in
addition to learning the intra-modal and standard synchronous cross-modal
relations, CrissCross also learns asynchronous cross-modal relationships. We
show that by relaxing the temporal synchronicity between the audio and visual
modalities, the network learns strong generalized representations. Our
experiments show that strong augmentations for both audio and visual modalities
with relaxation of cross-modal temporal synchronicity optimize performance. To
pretrain our proposed framework, we use 3 different datasets with varying
sizes, Kinetics-Sound, Kinetics400, and AudioSet. The learned representations
are evaluated on a number of downstream tasks namely action recognition, sound
classification, and retrieval. CrissCross shows state-of-the-art performances
on action recognition (UCF101 and HMDB51) and sound classification (ESC50 and
DCASE). The codes and pretrained models will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Domain Adaptation for Pavement Crack Detection. (arXiv:2111.10101v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10101">
<div class="article-summary-box-inner">
<span><p>Deep learning-based pavement cracks detection methods often require
large-scale labels with detailed crack location information to learn accurate
predictions. In practice, however, crack locations are very difficult to be
manually annotated due to various visual patterns of pavement crack. In this
paper, we propose a Deep Domain Adaptation-based Crack Detection Network
(DDACDN), which learns to take advantage of the source domain knowledge to
predict the multi-category crack location information in the target domain,
where only image-level labels are available. Specifically, DDACDN first
extracts crack features from both the source and target domain by a two-branch
weights-shared backbone network. And in an effort to achieve the cross-domain
adaptation, an intermediate domain is constructed by aggregating the
three-scale features from the feature space of each domain to adapt the crack
features from the source domain to the target domain. Finally, the network
involves the knowledge of both domains and is trained to recognize and localize
pavement cracks. To facilitate accurate training and validation for domain
adaptation, we use two challenging pavement crack datasets CQU-BPDD and
RDD2020. Furthermore, we construct a new large-scale Bituminous Pavement
Multi-label Disease Dataset named CQU-BPMDD, which contains 38994
high-resolution pavement disease images to further evaluate the robustness of
our model. Extensive experiments demonstrate that DDACDN outperforms
state-of-the-art pavement crack detection methods in predicting the crack
location on the target domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Well Do Sparse Imagenet Models Transfer?. (arXiv:2111.13445v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13445">
<div class="article-summary-box-inner">
<span><p>Transfer learning is a classic paradigm by which models pretrained on large
"upstream" datasets are adapted to yield good results on "downstream"
specialized datasets. Generally, more accurate models on the "upstream" dataset
tend to provide better transfer accuracy "downstream". In this work, we perform
an in-depth investigation of this phenomenon in the context of convolutional
neural networks (CNNs) trained on the ImageNet dataset, which have been pruned
- that is, compressed by sparsifying their connections. We consider transfer
using unstructured pruned models obtained by applying several state-of-the-art
pruning methods, including magnitude-based, second-order, re-growth,
lottery-ticket, and regularization approaches, in the context of twelve
standard transfer tasks. In a nutshell, our study shows that sparse models can
match or even outperform the transfer performance of dense models, even at high
sparsities, and, while doing so, can lead to significant inference and even
training speedups. At the same time, we observe and analyze significant
differences in the behaviour of different pruning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer. (arXiv:2111.13824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13824">
<div class="article-summary-box-inner">
<span><p>Network quantization significantly reduces model inference complexity and has
been widely used in real-world deployments. However, most existing quantization
methods have been developed mainly on Convolutional Neural Networks (CNN), and
suffer severe degradation when applied to fully quantized vision transformers.
In this work, we demonstrate that many of these difficulties arise because of
serious inter-channel variation in LayerNorm inputs, and present, Power-of-Two
Factor (PTF), a systematic method to reduce the performance degradation and
inference complexity of fully quantized vision transformers. In addition,
observing an extreme non-uniform distribution in attention maps, we propose
Log-Int-Softmax (LIS) to sustain that and simplify inference by using 4-bit
quantization and the BitShift operator. Comprehensive experiments on various
transformer-based architectures and benchmarks show that our Fully Quantized
Vision Transformer (FQ-ViT) outperforms previous works while even using lower
bit-width on attention maps. For instance, we reach 84.89% top-1 accuracy with
ViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our
knowledge, we are the first to achieve lossless accuracy degradation (~1%) on
fully quantized vision transformers. Code is available at
https://github.com/linyang-zhh/FQ-ViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Cluster Contrastive learning for Object Re-Identification. (arXiv:2112.04662v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04662">
<div class="article-summary-box-inner">
<span><p>Recently, cluster contrastive learning has been proven effective for object
ReID by computing the contrastive loss between the individual features and the
cluster memory. However, existing methods that use the individual features to
momentum update the cluster memory will fluctuate over the training examples,
especially for the outlier samples. Unlike the individual-based updating
mechanism, the centroid-based updating mechanism that applies the mean feature
of each cluster to update the cluster memory can reduce the impact of
individual samples. Therefore, we formulate the individual-based updating and
centroid-based updating mechanisms in a unified cluster contrastive framework,
named Dual Cluster Contrastive framework (DCC), which maintains two types of
memory banks: individual and centroid cluster memory banks. Significantly, the
individual cluster memory considers just one individual at a time to take a
single step for updating. The centroid cluster memory applies the mean feature
of each cluster to update the corresponding cluster memory. During
optimization, besides the vallina contrastive loss of each memory, a cross-view
consistency constraint is applied to exchange the benefits of two memories for
generating a discriminative description for the object ReID. Note that DCC can
be easily applied for unsupervised or supervised object ReID by using
ground-truth labels or the generated pseudo-labels. Extensive experiments on
three benchmarks, \emph{e.g.,} Market-1501, MSMT17, and VeRi-776, under
\textbf{supervised Object ReID} and \textbf{unsupervised Object ReID}
demonstrate the superiority of the proposed DCC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MantissaCam: Learning Snapshot High-dynamic-range Imaging with Perceptually-based In-pixel Irradiance Encoding. (arXiv:2112.05221v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05221">
<div class="article-summary-box-inner">
<span><p>The ability to image high-dynamic-range (HDR) scenes is crucial in many
computer vision applications. The dynamic range of conventional sensors,
however, is fundamentally limited by their well capacity, resulting in
saturation of bright scene parts. To overcome this limitation, emerging sensors
offer in-pixel processing capabilities to encode the incident irradiance. Among
the most promising encoding schemes is modulo wrapping, which results in a
computational photography problem where the HDR scene is computed by an
irradiance unwrapping algorithm from the wrapped low-dynamic-range (LDR) sensor
image. Here, we design a neural network--based algorithm that outperforms
previous irradiance unwrapping methods and we design a perceptually inspired
"mantissa" encoding scheme that more efficiently wraps an HDR scene into an LDR
sensor. Combined with our reconstruction framework, MantissaCam achieves
state-of-the-art results among modulo-type snapshot HDR imaging approaches. We
demonstrate the efficacy of our method in simulation and show benefits of our
algorithm on modulo images captured with a prototype implemented with a
programmable sensor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-vehicle Cooperative Visual Perception for Autonomous Driving under Complex Road and Traffic Scenarios. (arXiv:2112.09298v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09298">
<div class="article-summary-box-inner">
<span><p>Human-vehicle cooperative driving has become the critical technology of
autonomous driving, which reduces the workload of human drivers. However, the
complex and uncertain road environments bring great challenges to the visual
perception of cooperative systems. And the perception characteristics of
autonomous driving differ from manual driving a lot. To enhance the visual
perception capability of human-vehicle cooperative driving, this paper proposed
a cooperative visual perception model. 506 images of complex road and traffic
scenarios were collected as the data source. Then this paper improved the
object detection algorithm of autonomous vehicles. The mean perception accuracy
of traffic elements reached 75.52%. By the image fusion method, the gaze points
of human drivers were fused with vehicles' monitoring screens. Results revealed
that cooperative visual perception could reflect the riskiest zone and predict
the trajectory of conflict objects more precisely. The findings can be applied
in improving the visual perception algorithms and providing accurate data for
planning and control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NICE-SLAM: Neural Implicit Scalable Encoding for SLAM. (arXiv:2112.12130v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12130">
<div class="article-summary-box-inner">
<span><p>Neural implicit representations have recently shown encouraging results in
various domains, including promising progress in simultaneous localization and
mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene
reconstructions and have difficulty scaling up to large scenes. These
limitations are mainly due to their simple fully-connected network architecture
that does not incorporate local information in the observations. In this paper,
we present NICE-SLAM, a dense SLAM system that incorporates multi-level local
information by introducing a hierarchical scene representation. Optimizing this
representation with pre-trained geometric priors enables detailed
reconstruction on large indoor scenes. Compared to recent neural implicit SLAM
systems, our approach is more scalable, efficient, and robust. Experiments on
five challenging datasets demonstrate competitive results of NICE-SLAM in both
mapping and tracking quality. Project page:
https://pengsongyou.github.io/nice-slam
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiased Learning from Naturally Imbalanced Pseudo-Labels. (arXiv:2201.01490v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01490">
<div class="article-summary-box-inner">
<span><p>Pseudo-labels are confident predictions made on unlabeled target data by a
classifier trained on labeled source data. They are widely used for adapting a
model to unlabeled data, e.g., in a semi-supervised learning setting.
</p>
<p>Our key insight is that pseudo-labels are naturally imbalanced due to
intrinsic data similarity, even when a model is trained on balanced source data
and evaluated on balanced target data. If we address this previously unknown
imbalanced classification problem arising from pseudo-labels instead of
ground-truth training labels, we could remove model biases towards false
majorities created by pseudo-labels.
</p>
<p>We propose a novel and effective debiased learning method with pseudo-labels,
based on counterfactual reasoning and adaptive margins: The former removes the
classifier response bias, whereas the latter adjusts the margin of each class
according to the imbalance of pseudo-labels. Validated by extensive
experimentation, our simple debiased learning delivers significant accuracy
gains over the state-of-the-art on ImageNet-1K: 26% for semi-supervised
learning with 0.2% annotations and 9% for zero-shot learning. Our code is
available at: https://github.com/frank-xwang/debiased-pseudo-labeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling the Class Imbalance Problem of Deep Learning Based Head and Neck Organ Segmentation. (arXiv:2201.01636v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01636">
<div class="article-summary-box-inner">
<span><p>The segmentation of organs at risk (OAR) is a required precondition for the
cancer treatment with image guided radiation therapy. The automation of the
segmentation task is therefore of high clinical relevance. Deep Learning (DL)
based medical image segmentation is currently the most successful approach, but
suffers from the over-presence of the background class and the anatomically
given organ size difference, which is most severe in the head and neck (HAN)
area. To tackle the HAN area specific class imbalance problem we first optimize
the patch-size of the currently best performing general purpose segmentation
framework, the nnU-Net, based on the introduced class imbalance measurement,
and second, introduce the class adaptive Dice loss to further compensate for
the highly imbalanced setting. Both the patch-size and the loss function are
parameters with direct influence on the class imbalance and their optimization
leads to a 3\% increase of the Dice score and 22% reduction of the 95%
Hausdorff distance compared to the baseline, finally reaching $0.8\pm0.15$ and
$3.17\pm1.7$ mm for the segmentation of seven HAN organs using a single and
simple neural network. The patch-size optimization and the class adaptive Dice
loss are both simply integrable in current DL based segmentation approaches and
allow to increase the performance for class imbalanced segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SRVIO: Super Robust Visual Inertial Odometry for dynamic environments and challenging Loop-closure conditions. (arXiv:2201.05386v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05386">
<div class="article-summary-box-inner">
<span><p>There has been extensive research on visual localization and odometry for
autonomous robots and virtual reality during the past decades. Traditionally,
this problem has been solved with the help of expensive sensors, such as
lidars. Nowadays, the focus of the leading research in this field is on robust
localization using more economic sensors, such as cameras and IMUs.
Consequently, geometric visual localization methods have become more accurate
in time. However, these methods still suffer from significant loss and
divergence in challenging environments, such as a room full of moving people.
Scientists started using deep neural networks (DNNs) to mitigate this problem.
The main idea behind using DNNs is to better understand challenging aspects of
the data and overcome complex conditions such as the movement of a dynamic
object in front of the camera that covers the full view of the camera, extreme
lighting conditions, and high speed of the camera. Prior end-to-end DNN methods
have overcome some of these challenges. However, no general and robust
framework is available to overcome all challenges together. In this paper, we
have combined geometric and DNN-based methods to have the generality and speed
of geometric SLAM frameworks and overcome most of these challenging conditions
with the help of DNNs and deliver the most robust framework so far. To do so,
we have designed a framework based on Vins-Mono, and show that it is able to
achieve state-of-the-art results on TUM-Dynamic, TUM-VI, ADVIO, and EuRoC
datasets compared to geometric and end-to-end DNN based SLAMs. Our proposed
framework could also achieve outstanding results on extreme simulated cases
resembling the aforementioned challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection. (arXiv:2201.06493v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06493">
<div class="article-summary-box-inner">
<span><p>Object detection through either RGB images or the LiDAR point clouds has been
extensively explored in autonomous driving. However, it remains challenging to
make these two data sources complementary and beneficial to each other. In this
paper, we propose \textit{AutoAlign}, an automatic feature fusion strategy for
3D object detection. Instead of establishing deterministic correspondence with
camera projection matrix, we model the mapping relationship between the image
and point clouds with a learnable alignment map. This map enables our model to
automate the alignment of non-homogenous features in a dynamic and data-driven
manner. Specifically, a cross-attention feature alignment module is devised to
adaptively aggregate \textit{pixel-level} image features for each voxel. To
enhance the semantic consistency during feature alignment, we also design a
self-supervised cross-modal feature interaction module, through which the model
can learn feature aggregation with \textit{instance-level} feature guidance.
Extensive experimental results show that our approach can lead to 2.3 mAP and
7.0 mAP improvements on the KITTI and nuScenes datasets, respectively. Notably,
our best model reaches 70.9 NDS on the nuScenes testing leaderboard, achieving
competitive performance among various state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resistance Training using Prior Bias: toward Unbiased Scene Graph Generation. (arXiv:2201.06794v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06794">
<div class="article-summary-box-inner">
<span><p>Scene Graph Generation (SGG) aims to build a structured representation of a
scene using objects and pairwise relationships, which benefits downstream
tasks. However, current SGG methods usually suffer from sub-optimal scene graph
generation because of the long-tailed distribution of training data. To address
this problem, we propose Resistance Training using Prior Bias (RTPB) for the
scene graph generation. Specifically, RTPB uses a distributed-based prior bias
to improve models' detecting ability on less frequent relationships during
training, thus improving the model generalizability on tail categories. In
addition, to further explore the contextual information of objects and
relationships, we design a contextual encoding backbone network, termed as Dual
Transformer (DTrans). We perform extensive experiments on a very popular
benchmark, VG150, to demonstrate the effectiveness of our method for the
unbiased scene graph generation. In specific, our RTPB achieves an improvement
of over 10% under the mean recall when applied to current SGG methods.
Furthermore, DTrans with RTPB outperforms nearly all state-of-the-art methods
with a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Video Representation Learning with Cascade Positive Retrieval. (arXiv:2201.07989v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07989">
<div class="article-summary-box-inner">
<span><p>Self-supervised video representation learning has been shown to effectively
improve downstream tasks such as video retrieval and action recognition. In
this paper, we present the Cascade Positive Retrieval (CPR) that successively
mines positive examples w.r.t. the query for contrastive learning in a cascade
of stages. Specifically, CPR exploits multiple views of a query example in
different modalities, where an alternative view may help find another positive
example dissimilar in the query view. We explore the effects of possible CPR
configurations in ablations including the number of mining stages, the top
similar example selection ratio in each stage, and progressive training with an
incremental number of the final Top-k selection. The overall mining quality is
measured to reflect the recall across training set classes. CPR reaches a
median class mining recall of 83.3%, outperforming previous work by 5.5%.
Implementation-wise, CPR is complementary to pretext tasks and can be easily
applied to previous work. In the evaluation of pretraining on UCF101, CPR
consistently improves existing work and even achieves state-of-the-art R@1 of
56.7% and 24.4% in video retrieval as well as 83.8% and 54.8% in action
recognition on UCF101 and HMDB51. The code is available at
https://github.com/necla-ml/CPR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch. (arXiv:2201.09367v3 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09367">
<div class="article-summary-box-inner">
<span><p>The freeform architectural modeling process often involves two important
stages: concept design and digital modeling. In the first stage, architects
usually sketch the overall 3D shape and the panel layout on a physical or
digital paper briefly. In the second stage, a digital 3D model is created using
the sketch as a reference. The digital model needs to incorporate geometric
requirements for its components, such as the planarity of panels due to
consideration of construction costs, which can make the modeling process more
challenging. In this work, we present a novel sketch-based system to bridge the
concept design and digital modeling of freeform roof-like shapes represented as
planar quadrilateral (PQ) meshes. Our system allows the user to sketch the
surface boundary and contour lines under axonometric projection and supports
the sketching of occluded regions. In addition, the user can sketch feature
lines to provide directional guidance to the PQ mesh layout. Given the 2D
sketch input, we propose a deep neural network to infer in real-time the
underlying surface shape along with a dense conjugate direction field, both of
which are used to extract the final PQ mesh. To train and validate our network,
we generate a large synthetic dataset that mimics architect sketching of
freeform quadrilateral patches. The effectiveness and usability of our system
are demonstrated with quantitative and qualitative evaluation as well as user
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Hash Naturally Sorts. (arXiv:2201.13322v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13322">
<div class="article-summary-box-inner">
<span><p>Learning to hash pictures a list-wise sorting problem. Its testing metrics,
e.g., mean-average precision, count on a sorted candidate list ordered by
pair-wise code similarity. However, scarcely does one train a deep hashing
model with the sorted results end-to-end because of the non-differentiable
nature of the sorting operation. This inconsistency in the objectives of
training and test may lead to sub-optimal performance since the training loss
often fails to reflect the actual retrieval metric. In this paper, we tackle
this problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming
distances of samples' hash codes and accordingly gather their latent
representations for self-supervised training. Thanks to the recent advances in
differentiable sorting approximations, the hash head receives gradients from
the sorter so that the hash encoder can be optimized along with the training
procedure. Additionally, we describe a novel Sorted Noise-Contrastive
Estimation (SortedNCE) loss that selectively picks positive and negative
samples for contrastive learning, which allows NSH to mine data semantic
relations during training in an unsupervised manner. Our extensive experiments
show the proposed NSH model significantly outperforms the existing unsupervised
hashing methods on three benchmarked datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Scene BERT: Improving object detection by searching for challenging groups of data. (arXiv:2202.03651v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03651">
<div class="article-summary-box-inner">
<span><p>Modern computer vision applications rely on learning-based perception modules
parameterized with neural networks for tasks like object detection. These
modules frequently have low expected error overall but high error on atypical
groups of data due to biases inherent in the training process. In building
autonomous vehicles (AV), this problem is an especially important challenge
because their perception modules are crucial to the overall system performance.
After identifying failures in AV, a human team will comb through the associated
data to group perception failures that share common causes. More data from
these groups is then collected and annotated before retraining the model to fix
the issue. In other words, error groups are found and addressed in hindsight.
Our main contribution is a pseudo-automatic method to discover such groups in
foresight by performing causal interventions on simulated scenes. To keep our
interventions on the data manifold, we utilize masked language models. We
verify that the prioritized groups found via intervention are challenging for
the object detector and show that retraining with data collected from these
groups helps inordinately compared to adding more IID data. We also plan to
release software to run interventions in simulated scenes, which we hope will
benefit the causality community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Convolutional Networks for Multi-modality Medical Imaging: Methods, Architectures, and Clinical Applications. (arXiv:2202.08916v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08916">
<div class="article-summary-box-inner">
<span><p>Image-based characterization and disease understanding involve integrative
analysis of morphological, spatial, and topological information across
biological scales. The development of graph convolutional networks (GCNs) has
created the opportunity to address this information complexity via graph-driven
architectures, since GCNs can perform feature aggregation, interaction, and
reasoning with remarkable flexibility and efficiency. These GCNs capabilities
have spawned a new wave of research in medical imaging analysis with the
overarching goal of improving quantitative disease understanding, monitoring,
and diagnosis. Yet daunting challenges remain for designing the important
image-to-graph transformation for multi-modality medical imaging and gaining
insights into model interpretation and enhanced clinical decision support. In
this review, we present recent GCNs developments in the context of medical
image analysis including imaging data from radiology and histopathology. We
discuss the fast-growing use of graph network architectures in medical image
analysis to improve disease diagnosis and patient outcomes in clinical
practice. To foster cross-disciplinary research, we present GCNs technical
advancements, emerging medical applications, identify common challenges in the
use of image-based GCNs and their extensions in model interpretation,
large-scale benchmarks that promise to transform the scope of medical image
studies and related graph-driven medical research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-Efficient Parking Analytics System using Deep Reinforcement Learning. (arXiv:2202.08973v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08973">
<div class="article-summary-box-inner">
<span><p>Advances in deep vision techniques and ubiquity of smart cameras will drive
the next generation of video analytics. However, video analytics applications
consume vast amounts of energy as both deep learning techniques and cameras are
power-hungry. In this paper, we focus on a parking video analytics platform and
propose RL-CamSleep, a deep reinforcement learning-based technique, to actuate
the cameras to reduce the energy footprint while retaining the system's
utility. Our key insight is that many video-analytics applications do not
always need to be operational, and we can design policies to activate video
analytics only when necessary. Moreover, our work is complementary to existing
work that focuses on improving hardware and software efficiency. We evaluate
our approach on a city-scale parking dataset having 76 streets spread across
the city. Our analysis demonstrates how streets have various parking patterns,
highlighting the importance of an adaptive policy. Our approach can learn such
an adaptive policy that can reduce the average energy consumption by 76.38% and
achieve an average accuracy of more than 98% in performing video analytics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OUR-GAN: One-shot Ultra-high-Resolution Generative Adversarial Networks. (arXiv:2202.13799v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13799">
<div class="article-summary-box-inner">
<span><p>We propose OUR-GAN, the first one-shot ultra-high-resolution (UHR) image
synthesis framework that generates non-repetitive images with 4K or higher
resolution from a single training image. OUR-GAN generates a visually coherent
image at low resolution and then gradually increases the resolution by
super-resolution. Since OUR-GAN learns from a real UHR image, it can synthesize
large-scale shapes with fine details while maintaining long-range coherence,
which is difficult with conventional generative models that generate large
images based on the patch distribution learned from relatively small images.
OUR-GAN applies seamless subregion-wise super-resolution that synthesizes 4k or
higher UHR images with limited memory, preventing discontinuity at the
boundary. Additionally, OUR-GAN improves visual coherence maintaining diversity
by adding vertical positional embeddings to the feature maps. In experiments on
the ST4K and RAISE datasets, OUR-GAN exhibited improved fidelity, visual
coherency, and diversity compared with existing methods. The synthesized images
are presented at https://anonymous-62348.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Update Compression for Deep Neural Networks on the Edge. (arXiv:2203.04516v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04516">
<div class="article-summary-box-inner">
<span><p>An increasing number of artificial intelligence (AI) applications involve the
execution of deep neural networks (DNNs) on edge devices. Many practical
reasons motivate the need to update the DNN model on the edge device
post-deployment, such as refining the model, concept drift, or outright change
in the learning task. In this paper, we consider the scenario where retraining
can be done on the server side based on a copy of the DNN model, with only the
necessary data transmitted to the edge to update the deployed model. However,
due to bandwidth constraints, we want to minimise the transmission required to
achieve the update. We develop a simple approach based on matrix factorisation
to compress the model update -- this differs from compressing the model itself.
The key idea is to preserve existing knowledge in the current model and
optimise only small additional parameters for the update which can be used to
reconstitute the model on the edge. We compared our method to similar
techniques used in federated learning; our method usually requires less than
half of the update size of existing methods to achieve the same accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Bracket High Dynamic Range Imaging with Event Cameras. (arXiv:2203.06622v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06622">
<div class="article-summary-box-inner">
<span><p>Modern high dynamic range (HDR) imaging pipelines align and fuse multiple low
dynamic range (LDR) images captured at different exposure times. While these
methods work well in static scenes, dynamic scenes remain a challenge since the
LDR images still suffer from saturation and noise. In such scenarios, event
cameras would be a valid complement, thanks to their higher temporal resolution
and dynamic range. In this paper, we propose the first multi-bracket HDR
pipeline combining a standard camera with an event camera. Our results show
better overall robustness when using events, with improvements in PSNR by up to
5dB on synthetic data and up to 0.7dB on real-world data. We also introduce a
new dataset containing bracketed LDR images with aligned events and HDR ground
truth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sat-NeRF: Learning Multi-View Satellite Photogrammetry With Transient Objects and Shadow Modeling Using RPC Cameras. (arXiv:2203.08896v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08896">
<div class="article-summary-box-inner">
<span><p>We introduce the Satellite Neural Radiance Field (Sat-NeRF), a new end-to-end
model for learning multi-view satellite photogrammetry in the wild. Sat-NeRF
combines some of the latest trends in neural rendering with native satellite
camera models, represented by rational polynomial coefficient (RPC) functions.
The proposed method renders new views and infers surface models of similar
quality to those obtained with traditional state-of-the-art stereo pipelines.
Multi-date images exhibit significant changes in appearance, mainly due to
varying shadows and transient objects (cars, vegetation). Robustness to these
challenges is achieved by a shadow-aware irradiance model and uncertainty
weighting to deal with transient phenomena that cannot be explained by the
position of the sun. We evaluate Sat-NeRF using WorldView-3 images from
different locations and stress the advantages of applying a bundle adjustment
to the satellite camera models prior to training. This boosts the network
performance and can optionally be used to extract additional cues for depth
supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation. (arXiv:2203.15865v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15865">
<div class="article-summary-box-inner">
<span><p>Supervised approaches to 3D pose estimation from single images are remarkably
effective when labeled data is abundant. Therefore, much of the recent
attention has shifted towards semi and (or) weakly supervised learning.
Generating an effective form of supervision with little annotations still poses
major challenges in crowded scenes. However, since it is easy to observe a
scene from multiple cameras, we propose to impose multi-view geometrical
constraints by means of a differentiable triangulation and to use it as form of
self-supervision during training when no labels are available. We therefore
train a 2D pose estimator in such a way that its predictions correspond to the
re-projection of the triangulated 3D one and train an auxiliary network on them
to produce the final 3D poses. We complement the triangulation with a weighting
mechanism that nullify the impact of noisy predictions caused by self-occlusion
or occlusion from other subjects. Our experimental results on Human3.6M and
MPI-INF-3DHP substantiate the significance of our weighting strategy where we
obtain state-of-the-art results in the semi and weakly supervised learning
setup. We also contribute a new multi-player sports dataset that features
occlusion, and show the effectiveness of our algorithm over baseline
triangulation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Linear Attention for Fast and Accurate Keypoint Matching. (arXiv:2204.07731v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07731">
<div class="article-summary-box-inner">
<span><p>Recently Transformers have provided state-of-the-art performance in sparse
matching, crucial to realize high-performance 3D vision applications. Yet,
these Transformers lack efficiency due to the quadratic computational
complexity of their attention mechanism. To solve this problem, we employ an
efficient linear attention for the linear computational complexity. Then, we
propose a new attentional aggregation that achieves high accuracy by
aggregating both the global and local information from sparse keypoints. To
further improve the efficiency, we propose the joint learning of feature
matching and description. Our learning enables simpler and faster matching than
Sinkhorn, often used in matching the learned descriptors from Transformers. Our
method achieves competitive performance with only 0.84M learnable parameters
against the bigger SOTAs, SuperGlue (12M parameters) and SGMNet (30M
parameters), on three benchmarks, HPatch, ETH, and Aachen Day-Night.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Attention Methods in Deep Learning: An In-Depth Survey. (arXiv:2204.07756v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07756">
<div class="article-summary-box-inner">
<span><p>Inspired by the human cognitive system, attention is a mechanism that
imitates the human cognitive awareness about specific information, amplifying
critical details to focus more on the essential aspects of data. Deep learning
has employed attention to boost performance for many applications.
Interestingly, the same attention design can suit processing different data
modalities and can easily be incorporated into large networks. Furthermore,
multiple complementary attention mechanisms can be incorporated in one network.
Hence, attention techniques have become extremely attractive. However, the
literature lacks a comprehensive survey specific to attention techniques to
guide researchers in employing attention in their deep models. Note that,
besides being demanding in terms of training data and computational resources,
transformers only cover a single category in self-attention out of the many
categories available. We fill this gap and provide an in-depth survey of 50
attention techniques categorizing them by their most prominent features. We
initiate our discussion by introducing the fundamental concepts behind the
success of attention mechanism. Next, we furnish some essentials such as the
strengths and limitations of each attention category, describe their
fundamental building blocks, basic formulations with primary usage, and
applications specifically for computer vision. We also discuss the challenges
and open questions related to attention mechanism in general. Finally, we
recommend possible future research directions for deep attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping LiDAR and Camera Measurements in a Dual Top-View Grid Representation Tailored for Automated Vehicles. (arXiv:2204.07887v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07887">
<div class="article-summary-box-inner">
<span><p>We present a generic evidential grid mapping pipeline designed for imaging
sensors such as LiDARs and cameras. Our grid-based evidential model contains
semantic estimates for cell occupancy and ground separately. We specify the
estimation steps for input data represented by point sets, but mainly focus on
input data represented by images such as disparity maps or LiDAR range images.
Instead of relying on an external ground segmentation only, we deduce occupancy
evidence by analyzing the surface orientation around measurements. We conduct
experiments and evaluate the presented method using LiDAR and stereo camera
data recorded in real traffic scenarios. Our method estimates cell occupancy
robustly and with a high level of detail while maximizing efficiency and
minimizing the dependency to external processing modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2204.07955v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07955">
<div class="article-summary-box-inner">
<span><p>As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment
Analysis (MABSA) has attracted increasing attention in recent years. However,
previous approaches either (i) use separately pre-trained visual and textual
models, which ignore the crossmodal alignment or (ii) use vision-language
models pre-trained with general pre-training tasks, which are inadequate to
identify finegrained aspects, opinions, and their alignments across modalities.
To tackle these limitations, we propose a task-specific Vision-Language
Pre-training framework for MABSA (VLPMABSA), which is a unified multimodal
encoder-decoder architecture for all the pretraining and downstream tasks. We
further design three types of task-specific pre-training tasks from the
language, vision, and multimodal modalities, respectively. Experimental results
show that our approach generally outperforms the state-of-the-art approaches on
three MABSA subtasks. Further analysis demonstrates the effectiveness of each
pretraining task. The source code is publicly released at
https://github.com/NUSTM/VLP-MABSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NICO++: Towards Better Benchmarking for Domain Generalization. (arXiv:2204.08040v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08040">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable performance that modern deep neural networks have
achieved on independent and identically distributed (I.I.D.) data, they can
crash under distribution shifts. Most current evaluation methods for domain
generalization (DG) adopt the leave-one-out strategy as a compromise on the
limited number of domains. We propose a large-scale benchmark with extensive
labeled domains named NICO++ along with more rational evaluation methods for
comprehensively evaluating DG algorithms. To evaluate DG datasets, we propose
two metrics to quantify covariate shift and concept shift, respectively. Two
novel generalization bounds from the perspective of data construction are
proposed to prove that limited concept shift and significant covariate shift
favor the evaluation capability for generalization. Through extensive
experiments, NICO++ shows its superior evaluation capability compared with
current DG datasets and its contribution in alleviating unfairness caused by
the leak of oracle knowledge in model selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration. (arXiv:2204.08058v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08058">
<div class="article-summary-box-inner">
<span><p>Multimodal video-audio-text understanding and generation can benefit from
datasets that are narrow but rich. The narrowness allows bite-sized challenges
that the research community can make progress on. The richness ensures we are
making progress along the core challenges. To this end, we present a
large-scale video-audio-text dataset MUGEN, collected using the open-sourced
platform game CoinRun [11]. We made substantial modifications to make the game
richer by introducing audio and enabling new interactions. We trained RL agents
with different objectives to navigate the game and interact with 13 objects and
characters. This allows us to automatically extract a large collection of
diverse videos and associated audio. We sample 375K video clips (3.2s each) and
collect text descriptions from human annotators. Each video has additional
annotations that are extracted automatically from the game engine, such as
accurate semantic maps for each frame and templated textual descriptions.
Altogether, MUGEN can help progress research in many tasks in multimodal
understanding and generation. We benchmark representative approaches on tasks
involving video-audio-text retrieval and generation. Our dataset and code are
released at: https://mugen-org.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Consistency Regularization for Semi-supervised Change Detection in Remote Sensing Images. (arXiv:2204.08454v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08454">
<div class="article-summary-box-inner">
<span><p>Remote-sensing (RS) Change Detection (CD) aims to detect "changes of
interest" from co-registered bi-temporal images. The performance of existing
deep supervised CD methods is attributed to the large amounts of annotated data
used to train the networks. However, annotating large amounts of remote sensing
images is labor-intensive and expensive, particularly with bi-temporal images,
as it requires pixel-wise comparisons by a human expert. On the other hand, we
often have access to unlimited unlabeled multi-temporal RS imagery thanks to
ever-increasing earth observation programs. In this paper, we propose a simple
yet effective way to leverage the information from unlabeled bi-temporal images
to improve the performance of CD approaches. More specifically, we propose a
semi-supervised CD model in which we formulate an unsupervised CD loss in
addition to the supervised Cross-Entropy (CE) loss by constraining the output
change probability map of a given unlabeled bi-temporal image pair to be
consistent under the small random perturbations applied on the deep feature
difference map that is obtained by subtracting their latent feature
representations. Experiments conducted on two publicly available CD datasets
show that the proposed semi-supervised CD method can reach closer to the
performance of supervised CD even with access to as little as 10% of the
annotated training data. Code available at https://github.com/wgcban/SemiCD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer. (arXiv:2204.08680v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08680">
<div class="article-summary-box-inner">
<span><p>Vision transformers have achieved great successes in many computer vision
tasks. Most methods generate vision tokens by splitting an image into a regular
and fixed grid and treating each cell as a token. However, not all regions are
equally important in human-centric vision tasks, e.g., the human body needs a
fine representation with many tokens, while the image background can be modeled
by a few tokens. To address this problem, we propose a novel Vision
Transformer, called Token Clustering Transformer (TCFormer), which merges
tokens by progressive clustering, where the tokens can be merged from different
locations with flexible shapes and sizes. The tokens in TCFormer can not only
focus on important areas but also adjust the token shapes to fit the semantic
concept and adopt a fine resolution for regions containing critical details,
which is beneficial to capturing detailed information. Extensive experiments
show that TCFormer consistently outperforms its counterparts on different
challenging human-centric tasks and datasets, including whole-body pose
estimation on COCO-WholeBody and 3D human mesh reconstruction on 3DPW. Code is
available at https://github.com/zengwang430521/TCFormer.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Missing Annotations for Incremental Learning in Object Detection. (arXiv:2204.08766v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08766">
<div class="article-summary-box-inner">
<span><p>Despite the recent advances in the field of object detection, common
architectures are still ill-suited to incrementally detect new categories over
time. They are vulnerable to catastrophic forgetting: they forget what has been
already learned while updating their parameters in absence of the original
training data. Previous works extended standard classification methods in the
object detection task, mainly adopting the knowledge distillation framework.
However, we argue that object detection introduces an additional problem, which
has been overlooked. While objects belonging to new classes are learned thanks
to their annotations, if no supervision is provided for other objects that may
still be present in the input, the model learns to associate them to background
regions. We propose to handle these missing annotations by revisiting the
standard knowledge distillation framework. Our approach outperforms current
state-of-the-art methods in every setting of the Pascal-VOC dataset. We further
propose an extension to instance segmentation, outperforming the other
baselines. Code can be found here: https://github.com/fcdl94/MMA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Domain-Incremental Learning Approach to Drive in All Weather Conditions. (arXiv:2204.08817v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08817">
<div class="article-summary-box-inner">
<span><p>Although deep neural networks enable impressive visual perception performance
for autonomous driving, their robustness to varying weather conditions still
requires attention. When adapting these models for changed environments, such
as different weather conditions, they are prone to forgetting previously
learned information. This catastrophic forgetting is typically addressed via
incremental learning approaches which usually re-train the model by either
keeping a memory bank of training samples or keeping a copy of the entire model
or model parameters for each scenario. While these approaches show impressive
results, they can be prone to scalability issues and their applicability for
autonomous driving in all weather conditions has not been shown. In this paper
we propose DISC -- Domain Incremental through Statistical Correction -- a
simple online zero-forgetting approach which can incrementally learn new tasks
(i.e weather conditions) without requiring re-training or expensive memory
banks. The only information we store for each task are the statistical
parameters as we categorize each domain by the change in first and second order
statistics. Thus, as each task arrives, we simply 'plug and play' the
statistical vectors for the corresponding task into the model and it
immediately starts to perform well on that task. We show the efficacy of our
approach by testing it for object detection in a challenging domain-incremental
autonomous driving scenario where we encounter different adverse weather
conditions, such as heavy rain, fog, and snow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment. (arXiv:2204.08958v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08958">
<div class="article-summary-box-inner">
<span><p>No-Reference Image Quality Assessment (NR-IQA) aims to assess the perceptual
quality of images in accordance with human subjective perception.
Unfortunately, existing NR-IQA methods are far from meeting the needs of
predicting accurate quality scores on GAN-based distortion images. To this end,
we propose Multi-dimension Attention Network for no-reference Image Quality
Assessment (MANIQA) to improve the performance on GAN-based distortion. We
firstly extract features via ViT, then to strengthen global and local
interactions, we propose the Transposed Attention Block (TAB) and the Scale
Swin Transformer Block (SSTB). These two modules apply attention mechanisms
across the channel and spatial dimension, respectively. In this
multi-dimensional manner, the modules cooperatively increase the interaction
among different regions of images globally and locally. Finally, a dual branch
structure for patch-weighted quality prediction is applied to predict the final
score depending on the weight of each patch's score. Experimental results
demonstrate that MANIQA outperforms state-of-the-art methods on four standard
datasets (LIVE, TID2013, CSIQ, and KADID-10K) by a large margin. Besides, our
method ranked first place in the final testing phase of the NTIRE 2022
Perceptual Image Quality Assessment Challenge Track 2: No-Reference. Codes and
models are available at https://github.com/IIGROUP/MANIQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rendering Nighttime Image Via Cascaded Color and Brightness Compensation. (arXiv:2204.08970v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08970">
<div class="article-summary-box-inner">
<span><p>Image signal processing (ISP) is crucial for camera imaging, and neural
networks (NN) solutions are extensively deployed for daytime scenes. The lack
of sufficient nighttime image dataset and insights on nighttime illumination
characteristics poses a great challenge for high-quality rendering using
existing NN ISPs. To tackle it, we first built a high-resolution nighttime
RAW-RGB (NR2R) dataset with white balance and tone mapping annotated by expert
professionals. Meanwhile, to best capture the characteristics of nighttime
illumination light sources, we develop the CBUnet, a two-stage NN ISP to
cascade the compensation of color and brightness attributes. Experiments show
that our method has better visual quality compared to traditional ISP pipeline,
and is ranked at the second place in the NTIRE 2022 Night Photography Rendering
Challenge for two tracks by respective People's and Professional Photographer's
choices. The code and relevant materials are avaiable on our website:
https://njuvision.github.io/CBUnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstruction-Aware Prior Distillation for Semi-supervised Point Cloud Completion. (arXiv:2204.09186v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09186">
<div class="article-summary-box-inner">
<span><p>Point clouds scanned by real-world sensors are always incomplete, irregular,
and noisy, making the point cloud completion task become increasingly more
important. Though many point cloud completion methods have been proposed, most
of them require a large number of paired complete-incomplete point clouds for
training, which is labor exhausted. In contrast, this paper proposes a novel
Reconstruction-Aware Prior Distillation semi-supervised point cloud completion
method named RaPD, which takes advantage of a two-stage training scheme to
reduce the dependence on a large-scale paired dataset. In training stage 1, the
so-called deep semantic prior is learned from both unpaired complete and
unpaired incomplete point clouds using a reconstruction-aware pretraining
process. While in training stage 2, we introduce a semi-supervised prior
distillation process, where an encoder-decoder-based completion network is
trained by distilling the prior into the network utilizing only a small number
of paired training samples. A self-supervised completion module is further
introduced, excavating the value of a large number of unpaired incomplete point
clouds, leading to an increase in the network's performance. Extensive
experiments on several widely used datasets demonstrate that RaPD, the first
semi-supervised point cloud completion method, achieves superior performance to
previous methods on both homologous and heterologous scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sound-Guided Semantic Video Generation. (arXiv:2204.09273v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09273">
<div class="article-summary-box-inner">
<span><p>The recent success in StyleGAN demonstrates that pre-trained StyleGAN latent
space is useful for realistic video generation. However, the generated motion
in the video is usually not semantically meaningful due to the difficulty of
determining the direction and magnitude in the StyleGAN latent space. In this
paper, we propose a framework to generate realistic videos by leveraging
multimodal (sound-image-text) embedding space. As sound provides the temporal
contexts of the scene, our framework learns to generate a video that is
semantically consistent with sound. First, our sound inversion module maps the
audio directly into the StyleGAN latent space. We then incorporate the
CLIP-based multimodal embedding space to further provide the audio-visual
relationships. Finally, the proposed frame generator learns to find the
trajectory in the latent space which is coherent with the corresponding sound
and generates a video in a hierarchical manner. We provide the new
high-resolution landscape video dataset (audio-visual pair) for the
sound-guided video generation task. The experiments show that our model
outperforms the state-of-the-art methods in terms of video quality. We further
show several applications including image and video editing to verify the
effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Situational Perception Guided Image Matting. (arXiv:2204.09276v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09276">
<div class="article-summary-box-inner">
<span><p>Most automatic matting methods try to separate the salient foreground from
the background. However, the insufficient quantity and subjective bias of the
current existing matting datasets make it difficult to fully explore the
semantic association between object-to-object and object-to-environment in a
given image. In this paper, we propose a Situational Perception Guided Image
Matting (SPG-IM) method that mitigates subjective bias of matting annotations
and captures sufficient situational perception information for better global
saliency distilled from the visual-to-textual task. SPG-IM can better associate
inter-objects and object-to-environment saliency, and compensate the subjective
nature of image matting and its expensive annotation. We also introduce a
textual Semantic Transformation (TST) module that can effectively transform and
integrate the semantic feature stream to guide the visual representations. In
addition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed
to adaptively switch multi-scale receptive fields and focal points to enhance
both global and local details. Extensive experiments demonstrate the
effectiveness of situational perception guidance from the visual-to-textual
tasks on image matting, and our model outperforms the state-of-the-art methods.
We also analyze the significance of different components in our model. The code
will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Point Clouds: A Survey. (arXiv:2204.09337v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09337">
<div class="article-summary-box-inner">
<span><p>Point cloud has drawn more and more research attention as well as real-world
applications. However, many of these applications (e.g. autonomous driving and
robotic manipulation) are actually based on sequential point clouds (i.e. four
dimensions) because the information of the static point cloud data could
provide is still limited. Recently, researchers put more and more effort into
sequential point clouds. This paper presents an extensive review of the deep
learning-based methods for sequential point cloud research including dynamic
flow estimation, object detection \&amp; tracking, point cloud segmentation, and
point cloud forecasting. This paper further summarizes and compares the
quantitative results of the reviewed methods over the public benchmark
datasets. Finally, this paper is concluded by discussing the challenges in the
current sequential point cloud research and pointing out insightful potential
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESS: Learning Event-based Semantic Segmentation from Still Images. (arXiv:2203.10016v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10016">
<div class="article-summary-box-inner">
<span><p>Retrieving accurate semantic information in challenging high dynamic range
(HDR) and high-speed conditions remains an open challenge for image-based
algorithms due to severe image degradations. Event cameras promise to address
these challenges since they feature a much higher dynamic range and are
resilient to motion blur. Nonetheless, semantic segmentation with event cameras
is still in its infancy which is chiefly due to the novelty of the sensor, and
the lack of high-quality, labeled datasets. In this work, we introduce ESS,
which tackles this problem by directly transferring the semantic segmentation
task from existing labeled image datasets to unlabeled events via unsupervised
domain adaptation (UDA). Compared to existing UDA methods, our approach aligns
recurrent, motion-invariant event embeddings with image embeddings. For this
reason, our method neither requires video data nor per-pixel alignment between
images and events and, crucially, does not need to hallucinate motion from
still images. Additionally, to spur further research in event-based semantic
segmentation, we introduce DSEC-Semantic, the first large-scale event-based
dataset with fine-grained labels. We show that using image labels alone, ESS
outperforms existing UDA approaches, and when combined with event labels, it
even outperforms state-of-the-art supervised approaches on both DDD17 and
DSEC-Semantic. Finally, ESS is general-purpose, which unlocks the vast amount
of existing labeled image datasets and paves the way for new and exciting
research directions in new fields previously inaccessible for event cameras.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-23 23:08:16.077864701 UTC">2022-04-23 23:08:16 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>