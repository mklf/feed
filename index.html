<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-09T01:30:00Z">05-09</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CompactIE: Compact Facts in Open Information Extraction. (arXiv:2205.02880v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02880">
<div class="article-summary-box-inner">
<span><p>A major drawback of modern neural OpenIE systems and benchmarks is that they
prioritize high coverage of information in extractions over compactness of
their constituents. This severely limits the usefulness of OpenIE extractions
in many downstream tasks. The utility of extractions can be improved if
extractions are compact and share constituents. To this end, we study the
problem of identifying compact extractions with neural-based methods. We
propose CompactIE, an OpenIE system that uses a novel pipelined approach to
produce compact extractions with overlapping constituents. It first detects
constituents of the extractions and then links them to build extractions. We
train our system on compact extractions obtained by processing existing
benchmarks. Our experiments on CaRB and Wire57 datasets indicate that CompactIE
finds 1.5x-2x more compact extractions than previous systems, with high
precision, establishing a new state-of-the-art performance in OpenIE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ontology Reuse: the Real Test of Ontological Design. (arXiv:2205.02892v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02892">
<div class="article-summary-box-inner">
<span><p>Reusing ontologies in practice is still very challenging, especially when
multiple ontologies are involved. Moreover, despite recent advances, systematic
ontology quality assurance remains a difficult problem. In this work, the
quality of thirty biomedical ontologies, and the Computer Science Ontology, are
investigated from the perspective of a practical use case. Special scrutiny is
given to cross-ontology references, which are vital for combining ontologies.
Diverse methods to detect the issues are proposed, including natural language
processing and network analysis. Moreover, several suggestions for improving
ontologies and their quality assurance processes are presented. It is argued
that while the advancing automatic tools for ontology quality assurance are
crucial for ontology improvement, they will not solve the problem entirely. It
is ontology reuse that is the ultimate method for continuously verifying and
improving ontology quality, as well as for guiding its future development. Many
issues can be found and fixed only through practical and diverse ontology reuse
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Model Cards: A Human-Centered Approach to Model Documentation. (arXiv:2205.02894v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02894">
<div class="article-summary-box-inner">
<span><p>Deep learning models for natural language processing (NLP) are increasingly
adopted and deployed by analysts without formal training in NLP or machine
learning (ML). However, the documentation intended to convey the model's
details and appropriate use is tailored primarily to individuals with ML or NLP
expertise. To address this gap, we conduct a design inquiry into interactive
model cards, which augment traditionally static model cards with affordances
for exploring model documentation and interacting with the models themselves.
Our investigation consists of an initial conceptual study with experts in ML,
NLP, and AI Ethics, followed by a separate evaluative study with non-expert
analysts who use ML models in their work. Using a semi-structured interview
format coupled with a think-aloud protocol, we collected feedback from a total
of 30 participants who engaged with different versions of standard and
interactive model cards. Through a thematic analysis of the collected data, we
identified several conceptual dimensions that summarize the strengths and
limitations of standard and interactive model cards, including: stakeholders;
design; guidance; understandability &amp; interpretability; sensemaking &amp;
skepticism; and trust &amp; safety. Our findings demonstrate the importance of
carefully considered design and interactivity for orienting and supporting
non-expert analysts using deep learning models, along with a need for
consideration of broader sociotechnical contexts and organizational dynamics.
We have also identified design elements, such as language, visual cues, and
warnings, among others, that support interactivity and make non-interactive
content accessible. We summarize our findings as design guidelines and discuss
their implications for a human-centered approach towards AI/ML documentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Analysis of Daily Dialog Data using Polite Emotional Dialogue Acts. (arXiv:2205.02921v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02921">
<div class="article-summary-box-inner">
<span><p>Many socio-linguistic cues are used in conversational analysis, such as
emotion, sentiment, and dialogue acts. One of the fundamental cues is
politeness, which linguistically possesses properties such as social manners
useful in conversational analysis. This article presents findings of polite
emotional dialogue act associations, where we can correlate the relationships
between the socio-linguistic cues. We confirm our hypothesis that the
utterances with the emotion classes Anger and Disgust are more likely to be
impolite. At the same time, Happiness and Sadness are more likely to be polite.
A less expectable phenomenon occurs with dialogue acts Inform and Commissive
which contain more polite utterances than Question and Directive. Finally, we
conclude on the future work of these findings to extend the learning of social
behaviours using politeness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Propaganda Techniques in Visuo-Lingual Metaphor in Memes. (arXiv:2205.02937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02937">
<div class="article-summary-box-inner">
<span><p>The exponential rise of social media networks has allowed the production,
distribution, and consumption of data at a phenomenal rate. Moreover, the
social media revolution has brought a unique phenomenon to social media
platforms called Internet memes. Internet memes are one of the most popular
contents used on social media, and they can be in the form of images with a
witty, catchy, or satirical text description. In this paper, we are dealing
with propaganda that is often seen in Internet memes in recent times.
Propaganda is communication, which frequently includes psychological and
rhetorical techniques to manipulate or influence an audience to act or respond
as the propagandist wants. To detect propaganda in Internet memes, we propose a
multimodal deep learning fusion system that fuses the text and image feature
representations and outperforms individual models based solely on either text
or image modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining the Effectiveness of Multi-Task Learning for Efficient Knowledge Extraction from Spine MRI Reports. (arXiv:2205.02979v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02979">
<div class="article-summary-box-inner">
<span><p>Pretrained Transformer based models finetuned on domain specific corpora have
changed the landscape of NLP. However, training or fine-tuning these models for
individual tasks can be time consuming and resource intensive. Thus, a lot of
current research is focused on using transformers for multi-task learning
(Raffel et al.,2020) and how to group the tasks to help a multi-task model to
learn effective representations that can be shared across tasks (Standley et
al., 2020; Fifty et al., 2021). In this work, we show that a single
multi-tasking model can match the performance of task specific models when the
task specific models show similar representations across all of their hidden
layers and their gradients are aligned, i.e. their gradients follow the same
direction. We hypothesize that the above observations explain the effectiveness
of multi-task learning. We validate our observations on our internal
radiologist-annotated datasets on the cervical and lumbar spine. Our method is
simple and intuitive, and can be used in a wide range of NLP problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Noisy Label Correction for Fine-Grained Entity Typing. (arXiv:2205.03011v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03011">
<div class="article-summary-box-inner">
<span><p>Fine-grained entity typing (FET) aims to assign proper semantic types to
entity mentions according to their context, which is a fundamental task in
various entity-leveraging applications. Current FET systems usually establish
on large-scale weakly-supervised/distantly annotation data, which may contain
abundant noise and thus severely hinder the performance of the FET task.
Although previous studies have made great success in automatically identifying
the noisy labels in FET, they usually rely on some auxiliary resources which
may be unavailable in real-world applications (e.g. pre-defined hierarchical
type structures, human-annotated subsets). In this paper, we propose a novel
approach to automatically correct noisy labels for FET without external
resources. Specifically, it first identifies the potentially noisy labels by
estimating the posterior probability of a label being positive or negative
according to the logits output by the model, and then relabel candidate noisy
labels by training a robust model over the remaining clean labels. Experiments
on two popular benchmarks prove the effectiveness of our method. Our source
code can be obtained from \url{https://github.com/CCIIPLab/DenoiseFET}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aksharantar: Towards building open transliteration tools for the next billion users. (arXiv:2205.03018v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03018">
<div class="article-summary-box-inner">
<span><p>We introduce Aksharantar, the largest publicly available transliteration
dataset for 21 Indic languages containing 26 million transliteration pairs. We
build this dataset by mining transliteration pairs from large monolingual and
parallel corpora, as well as collecting transliterations from human annotators
to ensure diversity of words and representation of low-resource languages. We
introduce a new, large, diverse testset for Indic language transliteration
containing 103k words pairs spanning 19 languages that enables fine-grained
analysis of transliteration models.
</p>
<p>We train the IndicXlit model on the Aksharantar training set. IndicXlit is a
single transformer-based multilingual transliteration model for roman to Indic
script conversion supporting 21 Indic languages. It achieves state-of-the art
results on the Dakshina testset, and establishes strong baselines on the
Aksharantar testset released along with this work.
</p>
<p>All the datasets and models are publicly available at
https://indicnlp.ai4bharat.org/aksharantar. We hope the availability of these
large-scale, open resources will spur innovation for Indic language
transliteration and downstream applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hearing voices at the National Library -- a speech corpus and acoustic model for the Swedish language. (arXiv:2205.03026v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03026">
<div class="article-summary-box-inner">
<span><p>This paper explains our work in developing new acoustic models for automated
speech recognition (ASR) at KBLab, the infrastructure for data-driven research
at the National Library of Sweden (KB). We evaluate different approaches for a
viable speech-to-text pipeline for audiovisual resources in Swedish, using the
wav2vec 2.0 architecture in combination with speech corpuses created from KB's
collections. These approaches include pretraining an acoustic model for Swedish
from the ground up, and fine-tuning existing monolingual and multilingual
models. The collections-based corpuses we use have been sampled from millions
of hours of speech, with a conscious attempt to balance regional dialects to
produce a more representative, and thus more democratic, model. The acoustic
model this enabled, "VoxRex", outperforms existing models for Swedish ASR. We
also evaluate combining this model with various pretrained language models,
which further enhanced performance. We conclude by highlighting the potential
of such technology for cultural heritage institutions with vast collections of
previously unlabelled audiovisual data. Our models are released for further
exploration and research here: https://huggingface.co/KBLab.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing Multi-Domain False News and Underlying User Effects on Chinese Weibo. (arXiv:2205.03068v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03068">
<div class="article-summary-box-inner">
<span><p>False news that spreads on social media has proliferated over the past years
and has led to multi-aspect threats in the real world. While there are studies
of false news on specific domains (like politics or health care), little work
is found comparing false news across domains. In this article, we investigate
false news across nine domains on Weibo, the largest Twitter-like social media
platform in China, from 2009 to 2019. The newly collected data comprise 44,728
posts in the nine domains, published by 40,215 users, and reposted over 3.4
million times. Based on the distributions and spreads of the multi-domain
dataset, we observe that false news in domains that are close to daily life
like health and medicine generated more posts but diffused less effectively
than those in other domains like politics, and that political false news had
the most effective capacity for diffusion. The widely diffused false news posts
on Weibo were associated strongly with certain types of users -- by gender,
age, etc. Further, these posts provoked strong emotions in the reposts and
diffused further with the active engagement of false-news starters. Our
findings have the potential to help design false news detection systems in
suspicious news discovery, veracity prediction, and display and explanation.
The comparison of the findings on Weibo with those of existing work
demonstrates nuanced patterns, suggesting the need for more research on data
from diverse platforms, countries, or languages to tackle the global issue of
false news. The code and new anonymized dataset are available at
https://github.com/ICTMCG/Characterizing-Weibo-Multi-Domain-False-News.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering. (arXiv:2205.03071v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03071">
<div class="article-summary-box-inner">
<span><p>Extractive Question Answering (EQA) is one of the most important tasks in
Machine Reading Comprehension (MRC), which can be solved by fine-tuning the
span selecting heads of Pre-trained Language Models (PLMs). However, most
existing approaches for MRC may perform poorly in the few-shot learning
scenario. To solve this issue, we propose a novel framework named Knowledge
Enhanced Contrastive Prompt-tuning (KECP). Instead of adding pointer heads to
PLMs, we introduce a seminal paradigm for EQA that transform the task into a
non-autoregressive Masked Language Modeling (MLM) generation problem.
Simultaneously, rich semantics from the external knowledge base (KB) and the
passage context are support for enhancing the representations of the query. In
addition, to boost the performance of PLMs, we jointly train the model by the
MLM and contrastive learning objectives. Experiments on multiple benchmarks
demonstrate that our method consistently outperforms state-of-the-art
approaches in few-shot settings by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QLEVR: A Diagnostic Dataset for Quantificational Language and Elementary Visual Reasoning. (arXiv:2205.03075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03075">
<div class="article-summary-box-inner">
<span><p>Synthetic datasets have successfully been used to probe visual
question-answering datasets for their reasoning abilities. CLEVR
(johnson2017clevr), for example, tests a range of visual reasoning abilities.
The questions in CLEVR focus on comparisons of shapes, colors, and sizes,
numerical reasoning, and existence claims. This paper introduces a minimally
biased, diagnostic visual question-answering dataset, QLEVR, that goes beyond
existential and numerical quantification and focus on more complex quantifiers
and their combinations, e.g., asking whether there are more than two red balls
that are smaller than at least three blue balls in an image. We describe how
the dataset was created and present a first evaluation of state-of-the-art
visual question-answering models, showing that QLEVR presents a formidable
challenge to our current models. Code and Dataset are available at
https://github.com/zechenli03/QLEVR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning with Noisy User Feedback. (arXiv:2205.03092v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03092">
<div class="article-summary-box-inner">
<span><p>Machine Learning (ML) systems are getting increasingly popular, and drive
more and more applications and services in our daily life. This has led to
growing concerns over user privacy, since human interaction data typically
needs to be transmitted to the cloud in order to train and improve such
systems. Federated learning (FL) has recently emerged as a method for training
ML models on edge devices using sensitive user data and is seen as a way to
mitigate concerns over data privacy. However, since ML models are most commonly
trained with label supervision, we need a way to extract labels on edge to make
FL viable. In this work, we propose a strategy for training FL models using
positive and negative user feedback. We also design a novel framework to study
different noise patterns in user feedback, and explore how well standard
noise-robust objectives can help mitigate this noise when training models in a
federated setting. We evaluate our proposed training setup through detailed
experiments on two text classification datasets and analyze the effects of
varying levels of user reliability and feedback noise on model performance. We
show that our method improves substantially over a self-training baseline,
achieving performance closer to models trained with full supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emp-RFT: Empathetic Response Generation via Recognizing Feature Transitions between Utterances. (arXiv:2205.03112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03112">
<div class="article-summary-box-inner">
<span><p>Each utterance in multi-turn empathetic dialogues has features such as
emotion, keywords, and utterance-level meaning. Feature transitions between
utterances occur naturally. However, existing approaches fail to perceive the
transitions because they extract features for the context at the coarse-grained
level. To solve the above issue, we propose a novel approach of recognizing
feature transitions between utterances, which helps understand the dialogue
flow and better grasp the features of utterance that needs attention. Also, we
introduce a response generation strategy to help focus on emotion and keywords
related to appropriate features when generating responses. Experimental results
show that our approach outperforms baselines and especially, achieves
significant improvements on multi-turn dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arabic Fake News Detection Based on Deep Contextualized Embedding Models. (arXiv:2205.03114v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03114">
<div class="article-summary-box-inner">
<span><p>Social media is becoming a source of news for many people due to its ease and
freedom of use. As a result, fake news has been spreading quickly and easily
regardless of its credibility, especially in the last decade. Fake news
publishers take advantage of critical situations such as the Covid-19 pandemic
and the American presidential elections to affect societies negatively. Fake
news can seriously impact society in many fields including politics, finance,
sports, etc. Many studies have been conducted to help detect fake news in
English, but research conducted on fake news detection in the Arabic language
is scarce. Our contribution is twofold: first, we have constructed a large and
diverse Arabic fake news dataset. Second, we have developed and evaluated
transformer-based classifiers to identify fake news while utilizing eight
state-of-the-art Arabic contextualized embedding models. The majority of these
models had not been previously used for Arabic fake news detection. We conduct
a thorough analysis of the state-of-the-art Arabic contextualized embedding
models as well as comparison with similar fake news detection systems.
Experimental results confirm that these state-of-the-art models are robust,
with accuracy exceeding 98%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Domain Gap for Stance Detection for the Zulu language. (arXiv:2205.03153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03153">
<div class="article-summary-box-inner">
<span><p>Misinformation has become a major concern in recent last years given its
spread across our information sources. In the past years, many NLP tasks have
been introduced in this area, with some systems reaching good results on
English language datasets. Existing AI based approaches for fighting
misinformation in literature suggest automatic stance detection as an integral
first step to success. Our paper aims at utilizing this progress made for
English to transfers that knowledge into other languages, which is a
non-trivial task due to the domain gap between English and the target
languages. We propose a black-box non-intrusive method that utilizes techniques
from Domain Adaptation to reduce the domain gap, without requiring any human
expertise in the target language, by leveraging low-quality data in both a
supervised and unsupervised manner. This allows us to rapidly achieve similar
results for stance detection for the Zulu language, the target language in this
work, as are found for English. We also provide a stance detection dataset in
the Zulu language. Our experimental results show that by leveraging English
datasets and machine translation we can increase performances on both English
data along with other languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review on Text-Based Emotion Detection -- Techniques, Applications, Datasets, and Future Directions. (arXiv:2205.03235v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03235">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence (AI) has been used for processing data to make
decisions, interact with humans, and understand their feelings and emotions.
With the advent of the internet, people share and express their thoughts on
day-to-day activities and global and local events through text messaging
applications. Hence, it is essential for machines to understand emotions in
opinions, feedback, and textual dialogues to provide emotionally aware
responses to users in today's online world. The field of text-based emotion
detection (TBED) is advancing to provide automated solutions to various
applications, such as businesses, and finances, to name a few. TBED has gained
a lot of attention in recent times. The paper presents a systematic literature
review of the existing literature published between 2005 to 2021 in TBED. This
review has meticulously examined 63 research papers from IEEE, Science Direct,
Scopus, and Web of Science databases to address four primary research
questions. It also reviews the different applications of TBED across various
research domains and highlights its use. An overview of various emotion models,
techniques, feature extraction methods, datasets, and research challenges with
future directions has also been represented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collective Relevance Labeling for Passage Retrieval. (arXiv:2205.03273v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03273">
<div class="article-summary-box-inner">
<span><p>Deep learning for Information Retrieval (IR) requires a large amount of
high-quality query-document relevance labels, but such labels are inherently
sparse. Label smoothing redistributes some observed probability mass over
unobserved instances, often uniformly, uninformed of the true distribution. In
contrast, we propose knowledge distillation for informed labeling, without
incurring high computation overheads at evaluation time. Our contribution is
designing a simple but efficient teacher model which utilizes collective
knowledge, to outperform state-of-the-arts distilled from a more complex
teacher model. Specifically, we train up to x8 faster than the state-of-the-art
teacher, while distilling the rankings better. Our code is publicly available
at https://github.com/jihyukkim-nlp/CollectiveKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers. (arXiv:2205.03286v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03286">
<div class="article-summary-box-inner">
<span><p>There has been a growing interest in interpreting the underlying dynamics of
Transformers. While self-attention patterns were initially deemed as the
primary option, recent studies have shown that integrating other components can
yield more accurate explanations. This paper introduces a novel token
attribution analysis method that incorporates all the components in the encoder
block and aggregates this throughout layers. Through extensive quantitative and
qualitative experiments, we demonstrate that our method can produce faithful
and meaningful global token attributions. Our experiments reveal that
incorporating almost every encoder component results in increasingly more
accurate analysis in both local (single layer) and global (the whole model)
settings. Our global attribution analysis significantly outperforms previous
methods on various tasks regarding correlation with gradient-based saliency
scores. Our code is freely available at
https://github.com/mohsenfayyaz/GlobEnc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude Detection in Social Media. (arXiv:2205.03296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03296">
<div class="article-summary-box-inner">
<span><p>Building models to detect vaccine attitudes on social media is challenging
because of the composite, often intricate aspects involved, and the limited
availability of annotated data. Existing approaches have relied heavily on
supervised training that requires abundant annotations and pre-defined aspect
categories. Instead, with the aim of leveraging the large amount of unannotated
data now available on vaccination, we propose a novel semi-supervised approach
for vaccine attitude detection, called VADet. A variational autoencoding
architecture based on language models is employed to learn from unlabelled data
the topical information of the domain. Then, the model is fine-tuned with a few
manually annotated examples of user attitudes. We validate the effectiveness of
VADet on our annotated data and also on an existing vaccination corpus
annotated with opinions on vaccines. Our results show that VADet is able to
learn disentangled stance and aspect topics, and outperforms existing
aspect-based sentiment analysis models on both stance detection and tweet
clustering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Necessity and Sufficiency for Explaining Text Classifiers: A Case Study in Hate Speech Detection. (arXiv:2205.03302v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03302">
<div class="article-summary-box-inner">
<span><p>We present a novel feature attribution method for explaining text
classifiers, and analyze it in the context of hate speech detection. Although
feature attribution models usually provide a single importance score for each
token, we instead provide two complementary and theoretically-grounded scores
-- necessity and sufficiency -- resulting in more informative explanations. We
propose a transparent method that calculates these values by generating
explicit perturbations of the input text, allowing the importance scores
themselves to be explainable. We employ our method to explain the predictions
of different hate speech detection models on the same set of curated examples
from a test suite, and show that different values of necessity and sufficiency
for identity terms correspond to different kinds of false positive errors,
exposing sources of classifier bias against marginalized groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Humor and Sarcasm for Improving Political Parody Detection. (arXiv:2205.03313v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03313">
<div class="article-summary-box-inner">
<span><p>Parody is a figurative device used for mimicking entities for comedic or
critical purposes. Parody is intentionally humorous and often involves sarcasm.
This paper explores jointly modelling these figurative tropes with the goal of
improving performance of political parody detection in tweets. To this end, we
present a multi-encoder model that combines three parallel encoders to enrich
parody-specific representations with humor and sarcasm information. Experiments
on a publicly available data set of political parody tweets demonstrate that
our approach outperforms previous state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Example-Based Machine Translation from Text to a Hierarchical Representation of Sign Language. (arXiv:2205.03314v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03314">
<div class="article-summary-box-inner">
<span><p>This article presents an original method for Text-to-Sign Translation. It
compensates data scarcity using a domain-specific parallel corpus of alignments
between text and hierarchical formal descriptions of Sign Language videos in
AZee. Based on the detection of similarities present in the source text, the
proposed algorithm recursively exploits matches and substitutions of aligned
segments to build multiple candidate translations for a novel statement. This
helps preserving Sign Language structures as much as possible before falling
back on literal translations too quickly, in a generative way. The resulting
translations are in the form of AZee expressions, designed to be used as input
to avatar synthesis systems. We present a test set tailored to showcase its
potential for expressiveness and generation of idiomatic target language, and
observed limitations. This work finally opens prospects on how to evaluate
translation and linguistic aspects, such as accuracy and grammatical fluency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Synthesis and Fusion and their Impact on Machine Translation. (arXiv:2205.03369v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03369">
<div class="article-summary-box-inner">
<span><p>Theoretical work in morphological typology offers the possibility of
measuring morphological diversity on a continuous scale. However, literature in
Natural Language Processing (NLP) typically labels a whole language with a
strict type of morphology, e.g. fusional or agglutinative. In this work, we
propose to reduce the rigidity of such claims, by quantifying morphological
typology at the word and segment level. We consider Payne (2017)'s approach to
classify morphology using two indices: synthesis (e.g. analytic to
polysynthetic) and fusion (agglutinative to fusional). For computing synthesis,
we test unsupervised and supervised morphological segmentation methods for
English, German and Turkish, whereas for fusion, we propose a semi-automatic
method using Spanish as a case study. Then, we analyse the relationship between
machine translation quality and the degree of synthesis and fusion at word
(nouns and verbs for English-Turkish, and verbs in English-Spanish) and segment
level (previous language pairs plus English-German in both directions). We
complement the word-level analysis with human evaluation, and overall, we
observe a consistent impact of both indexes on machine translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Unreliability of Explanations in Few-Shot In-Context Learning. (arXiv:2205.03401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03401">
<div class="article-summary-box-inner">
<span><p>How can prompting a large language model like GPT-3 with explanations improve
in-context learning? We focus specifically on two NLP tasks that involve
reasoning over text, namely question answering and natural language inference.
Including explanations in the prompt and having the model generate them does
not consistently improve performance in the settings we study, contrary to
recent results on symbolic reasoning tasks (Nye et al., 2021; Wei et al.,
2022). Despite careful prompting, explanations generated by GPT-3 may not even
be factually grounded in the input, even on simple tasks with straightforward
extractive explanations. However, these flawed explanations can still be useful
as a way to verify GPT-3's predictions post-hoc. Through analysis in three
settings, we show that explanations judged as good by humans--those that are
logically consistent with the input and the prediction--usually indicate more
accurate predictions. Following these observations, we present a framework for
calibrating model predictions based on the reliability of the explanations. Our
framework trains calibrators using automatically extracted scores that
approximately assess the reliability of explanations, which helps improve
performance across three different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data Cartography based MixUp for Pre-trained Language Models. (arXiv:2205.03403v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03403">
<div class="article-summary-box-inner">
<span><p>MixUp is a data augmentation strategy where additional samples are generated
during training by combining random pairs of training samples and their labels.
However, selecting random pairs is not potentially an optimal choice. In this
work, we propose TDMixUp, a novel MixUp strategy that leverages Training
Dynamics and allows more informative samples to be combined for generating new
data samples. Our proposed TDMixUp first measures confidence, variability,
(Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al.,
2020) to identify the characteristics of training samples (e.g., as
easy-to-learn or ambiguous samples), and then interpolates these characterized
samples. We empirically validate that our method not only achieves competitive
performance using a smaller subset of the training data compared with strong
baselines, but also yields lower expected calibration error on the pre-trained
language model, BERT, on both in-domain and out-of-domain settings in a wide
range of NLP tasks. We publicly release our code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adjusting for Confounders with Text: Challenges and an Empirical Evaluation Framework for Causal Inference. (arXiv:2009.09961v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09961">
<div class="article-summary-box-inner">
<span><p>Causal inference studies using textual social media data can provide
actionable insights on human behavior. Making accurate causal inferences with
text requires controlling for confounding which could otherwise impart bias.
Recently, many different methods for adjusting for confounders have been
proposed, and we show that these existing methods disagree with one another on
two datasets inspired by previous social media studies. Evaluating causal
methods is challenging, as ground truth counterfactuals are almost never
available. Presently, no empirical evaluation framework for causal methods
using text exists, and as such, practitioners must select their methods without
guidance. We contribute the first such framework, which consists of five tasks
drawn from real world studies. Our framework enables the evaluation of any
casual inference method using text. Across 648 experiments and two datasets, we
evaluate every commonly used causal inference method and identify their
strengths and weaknesses to inform social media researchers seeking to use such
methods, and guide future improvements. We make all tasks, data, and models
public to inform applications and encourage additional research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks. (arXiv:2104.08815v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08815">
<div class="article-summary-box-inner">
<span><p>Increasing concerns and regulations about data privacy and sparsity
necessitate the study of privacy-preserving, decentralized learning methods for
natural language processing (NLP) tasks. Federated learning (FL) provides
promising approaches for a large number of clients (e.g., personal devices or
organizations) to collaboratively learn a shared global model to benefit all
clients while allowing users to keep their data locally. Despite interest in
studying FL methods for NLP tasks, a systematic comparison and analysis is
lacking in the literature. Herein, we present the FedNLP, a benchmarking
framework for evaluating federated learning methods on four different task
formulations: text classification, sequence tagging, question answering, and
seq2seq. We propose a universal interface between Transformer-based language
models (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) under
various non-IID partitioning strategies. Our extensive experiments with FedNLP
provide empirical comparisons between FL methods and helps us better understand
the inherent challenges of this direction. The comprehensive analysis points to
intriguing and exciting future research aimed at developing FL methods for NLP
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-based Hate. (arXiv:2108.05921v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05921">
<div class="article-summary-box-inner">
<span><p>Detecting online hate is a complex task, and low-performing models have
harmful consequences when used for sensitive applications such as content
moderation. Emoji-based hate is an emerging challenge for automated detection.
We present HatemojiCheck, a test suite of 3,930 short-form statements that
allows us to evaluate performance on hateful language expressed with emoji.
Using the test suite, we expose weaknesses in existing hate detection models.
To address these weaknesses, we create the HatemojiBuild dataset using a
human-and-model-in-the-loop approach. Models built with these 5,912 adversarial
examples perform substantially better at detecting emoji-based hate, while
retaining strong performance on text-only hate. Both HatemojiCheck and
HatemojiBuild are made publicly available. See our Github Repository
(https://github.com/HannahKirk/Hatemoji). HatemojiCheck, HatemojiBuild, and the
final Hatemoji Model are also available on HuggingFace
(https://huggingface.co/datasets/HannahRoseKirk/).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Enhanced Span-based Decomposition Method for Few-Shot Sequence Labeling. (arXiv:2109.13023v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13023">
<div class="article-summary-box-inner">
<span><p>Few-Shot Sequence Labeling (FSSL) is a canonical paradigm for the tagging
models, e.g., named entity recognition and slot filling, to generalize on an
emerging, resource-scarce domain. Recently, the metric-based meta-learning
framework has been recognized as a promising approach for FSSL. However, most
prior works assign a label to each token based on the token-level similarities,
which ignores the integrality of named entities or slots. To this end, in this
paper, we propose ESD, an Enhanced Span-based Decomposition method for FSSL.
ESD formulates FSSL as a span-level matching problem between test query and
supporting instances. Specifically, ESD decomposes the span matching problem
into a series of span-level procedures, mainly including enhanced span
representation, class prototype aggregation and span conflicts resolution.
Extensive experiments show that ESD achieves the new state-of-the-art results
on two popular FSSL benchmarks, FewNERD and SNIPS, and is proven to be more
robust in the nested and noisy tagging scenarios. Our code is available at
https://github.com/Wangpeiyi9979/ESD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PARE: A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction. (arXiv:2110.07415v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07415">
<div class="article-summary-box-inner">
<span><p>Neural models for distantly supervised relation extraction (DS-RE) encode
each sentence in an entity-pair bag separately. These are then aggregated for
bag-level relation prediction. Since, at encoding time, these approaches do not
allow information to flow from other sentences in the bag, we believe that they
do not utilize the available bag data to the fullest. In response, we explore a
simple baseline approach (PARE) in which all sentences of a bag are
concatenated into a passage of sentences, and encoded jointly using BERT. The
contextual embeddings of tokens are aggregated using attention with the
candidate relation as query -- this summary of whole passage predicts the
candidate relation. We find that our simple baseline solution outperforms
existing state-of-the-art DS-RE models in both monolingual and multilingual
DS-RE datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding. (arXiv:2110.14170v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14170">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs (KGs) consisting of a large number of triples have become
widespread recently, and many knowledge graph embedding (KGE) methods are
proposed to embed entities and relations of a KG into continuous vector spaces.
Such embedding methods simplify the operations of conducting various in-KG
tasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering).
They can be viewed as general solutions for representing KGs. However, existing
KGE methods are not applicable to inductive settings, where a model trained on
source KGs will be tested on target KGs with entities unseen during model
training. Existing works focusing on KGs in inductive settings can only solve
the inductive relation prediction task. They can not handle other out-of-KG
tasks as general as KGE methods since they don't produce embeddings for
entities. In this paper, to achieve inductive knowledge graph embedding, we
propose a model MorsE, which does not learn embeddings for entities but learns
transferable meta-knowledge that can be used to produce entity embeddings. Such
meta-knowledge is modeled by entity-independent modules and learned by
meta-learning. Experimental results show that our model significantly
outperforms corresponding baselines for in-KG and out-of-KG tasks in inductive
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offense Detection in Dravidian Languages using Code-Mixing Index based Focal Loss. (arXiv:2111.06916v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06916">
<div class="article-summary-box-inner">
<span><p>Over the past decade, we have seen exponential growth in online content
fueled by social media platforms. Data generation of this scale comes with the
caveat of insurmountable offensive content in it. The complexity of identifying
offensive content is exacerbated by the usage of multiple modalities (image,
language, etc.), code-mixed language and more. Moreover, even after careful
sampling and annotation of offensive content, there will always exist a
significant class imbalance between offensive and non-offensive content. In
this paper, we introduce a novel Code-Mixing Index (CMI) based focal loss which
circumvents two challenges (1) code-mixing in languages (2) class imbalance
problem for Dravidian language offense detection. We also replace the
conventional dot product-based classifier with the cosine-based classifier
which results in a boost in performance. Further, we use multilingual models
that help transfer characteristics learnt across languages to work effectively
with low resourced languages. It is also important to note that our model
handles instances of mixed script (say usage of Latin and Dravidian-Tamil
script) as well. To summarize, our model can handle offensive language
detection in a low-resource, class imbalanced, multilingual and code-mixed
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long Context Question Answering via Supervised Contrastive Learning. (arXiv:2112.08777v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08777">
<div class="article-summary-box-inner">
<span><p>Long-context question answering (QA) tasks require reasoning over a long
document or multiple documents. Addressing these tasks often benefits from
identifying a set of evidence spans (e.g., sentences), which provide supporting
evidence for answering the question. In this work, we propose a novel method
for equipping long-context QA models with an additional sequence-level
objective for better identification of the supporting evidence. We achieve this
via an additional contrastive supervision signal in finetuning, where the model
is encouraged to explicitly discriminate supporting evidence sentences from
negative ones by maximizing question-evidence similarity. The proposed
additional loss exhibits consistent improvements on three different strong
long-context transformer models, across two challenging question answering
benchmarks -- HotpotQA and QAsper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based Collaborative Filtering. (arXiv:2204.04179v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04179">
<div class="article-summary-box-inner">
<span><p>Content-based collaborative filtering (CCF) predicts user-item interactions
based on both users' interaction history and items' content information.
Recently, pre-trained language models (PLM) have been used to extract
high-quality item encodings for CCF. However, it is resource-intensive to train
a PLM-based CCF model in an end-to-end (E2E) manner, since optimization
involves back-propagating through every content encoding within a given user
interaction sequence. To tackle this issue, we propose GRAM (GRadient
Accumulation for Multi-modality in CCF), which exploits the fact that a given
item often appears multiple times within a batch of interaction histories.
Specifically, Single-step GRAM aggregates each item encoding's gradients for
back-propagation, with theoretic equivalence to the standard E2E training. As
an extension of Single-step GRAM, we propose Multi-step GRAM, which increases
the gradient update latency, achieving a further speedup with drastically less
GPU memory. GRAM significantly improves training efficiency (up to 146x) on
five datasets from two task domains of Knowledge Tracing and News
Recommendation. Our code is available at https://github.com/yoonseok312/GRAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Climate and Weather: Inspecting Depression Detection via Emotion Recognition. (arXiv:2204.14099v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14099">
<div class="article-summary-box-inner">
<span><p>Automatic depression detection has attracted increasing amount of attention
but remains a challenging task. Psychological research suggests that depressive
mood is closely related with emotion expression and perception, which motivates
the investigation of whether knowledge of emotion recognition can be
transferred for depression detection. This paper uses pretrained features
extracted from the emotion recognition model for depression detection, further
fuses emotion modality with audio and text to form multimodal depression
detection. The proposed emotion transfer improves depression detection
performance on DAIC-WOZ as well as increases the training stability. The
analysis of how the emotion expressed by depressed individuals is further
perceived provides clues for further understanding of the relationship between
depression and emotion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation. (arXiv:2205.01133v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01133">
<div class="article-summary-box-inner">
<span><p>Multi-modal Machine Translation (MMT) enables the use of visual information
to enhance the quality of translations. The visual information can serve as a
valuable piece of context information to decrease the ambiguity of input
sentences. Despite the increasing popularity of such a technique, good and
sizeable datasets are scarce, limiting the full extent of their potential.
Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It
is estimated that about 100 to 150 million people speak the language, with more
than 80 million indigenous speakers. This is more than any of the other Chadic
languages. Despite a large number of speakers, the Hausa language is considered
low-resource in natural language processing (NLP). This is due to the absence
of sufficient resources to implement most NLP tasks. While some datasets exist,
they are either scarce, machine-generated, or in the religious domain.
Therefore, there is a need to create training and evaluation data for
implementing machine learning tasks and bridging the research gap in the
language. This work presents the Hausa Visual Genome (HaVG), a dataset that
contains the description of an image or a section within the image in Hausa and
its equivalent in English. To prepare the dataset, we started by translating
the English description of the images in the Hindi Visual Genome (HVG) into
Hausa automatically. Afterward, the synthetic Hausa data was carefully
post-edited considering the respective images. The dataset comprises 32,923
images and their descriptions that are divided into training, development,
test, and challenge test set. The Hausa Visual Genome is the first dataset of
its kind and can be used for Hausa-English machine translation, multi-modal
research, and image description, among various other natural language
processing and generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reproducibility Beyond the Research Community: Experience from NLP Beginners. (arXiv:2205.02182v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02182">
<div class="article-summary-box-inner">
<span><p>As NLP research attracts public attention and excitement, it becomes
increasingly important for it to be accessible to a broad audience. As the
research community works to democratize NLP, it remains unclear whether
beginners to the field can easily apply the latest developments. To understand
their needs, we conducted a study with 93 students in an introductory NLP
course, where students reproduced results of recent NLP papers. Surprisingly,
our results suggest that their technical skill (i.e., programming experience)
has limited impact on their effort spent completing the exercise. Instead, we
find accessibility efforts by research authors to be key to a successful
experience, including thorough documentation and easy access to required models
and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction. (arXiv:2205.02225v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02225">
<div class="article-summary-box-inner">
<span><p>Unsupervised relation extraction aims to extract the relationship between
entities from natural language sentences without prior information on
relational scope or distribution. Existing works either utilize self-supervised
schemes to refine relational feature signals by iteratively leveraging adaptive
clustering and classification that provoke gradual drift problems, or adopt
instance-wise contrastive learning which unreasonably pushes apart those
sentence pairs that are semantically similar. To overcome these defects, we
propose a novel contrastive learning framework named HiURE, which has the
capability to derive hierarchical signals from relational feature space using
cross hierarchy attention and effectively optimize relation representation of
sentences under exemplar-wise contrastive learning. Experimental results on two
public datasets demonstrate the advanced effectiveness and robustness of HiURE
on unsupervised relation extraction when compared with state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">User-Driven Research of Medical Note Generation Software. (arXiv:2205.02549v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02549">
<div class="article-summary-box-inner">
<span><p>A growing body of work uses Natural Language Processing (NLP) methods to
automatically generate medical notes from audio recordings of doctor-patient
consultations. However, there are very few studies on how such systems could be
used in clinical practice, how clinicians would adjust to using them, or how
system design should be influenced by such considerations. In this paper, we
present three rounds of user studies, carried out in the context of developing
a medical note generation system. We present, analyse and discuss the
participating clinicians' impressions and views of how the system ought to be
adapted to be of value to them. Next, we describe a three-week test run of the
system in a live telehealth clinical practice. Major findings include (i) the
emergence of five different note-taking behaviours; (ii) the importance of the
system generating notes in real time during the consultation; and (iii) the
identification of a number of clinical use cases that could prove challenging
for automatic note generation systems.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">GAN Inversion for Data Augmentation to Improve Colonoscopy Lesion Classification. (arXiv:2205.02840v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02840">
<div class="article-summary-box-inner">
<span><p>A major challenge in applying deep learning to medical imaging is the paucity
of annotated data. This study demonstrates that synthetic colonoscopy images
generated by Generative Adversarial Network (GAN) inversion can be used as
training data to improve the lesion classification performance of deep learning
models. This approach inverts pairs of images with the same label to a
semantically rich &amp; disentangled latent space and manipulates latent
representations to produce new synthetic images with the same label. We perform
image modality translation (style transfer) between white light and narrowband
imaging (NBI). We also generate realistic-looking synthetic lesion images by
interpolating between original training images to increase the variety of
lesion shapes in the training dataset. We show that these approaches outperform
comparative colonoscopy data augmentation techniques without the need to
re-train multiple generative models. This approach also leverages information
from datasets that may not have been designed for the specific colonoscopy
downstream task. E.g. using a bowel prep grading dataset for a polyp
classification task. Our experiments show this approach can perform multiple
colonoscopy data augmentations, which improve the downstream polyp
classification performance over baseline and comparison methods by up to 6%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Transfer Learning for Chest Radiograph Clinical Report Generation with Modified Transformer Architectures. (arXiv:2205.02841v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02841">
<div class="article-summary-box-inner">
<span><p>The image captioning task is increasingly prevalent in artificial
intelligence applications for medicine. One important application is clinical
report generation from chest radiographs. The clinical writing of unstructured
reports is time consuming and error-prone. An automated system would improve
standardization, error reduction, time consumption, and medical accessibility.
In this paper we demonstrate the importance of domain specific pre-training and
propose a modified transformer architecture for the medical image captioning
task. To accomplish this, we train a series of modified transformers to
generate clinical reports from chest radiograph image input. These modified
transformers include: a meshed-memory augmented transformer architecture with
visual extractor using ImageNet pre-trained weights, a meshed-memory augmented
transformer architecture with visual extractor using CheXpert pre-trained
weights, and a meshed-memory augmented transformer whose encoder is passed the
concatenated embeddings using both ImageNet pre-trained weights and CheXpert
pre-trained weights. We use BLEU(1-4), ROUGE-L, CIDEr, and the clinical
CheXbert F1 scores to validate our models and demonstrate competitive scores
with state of the art models. We provide evidence that ImageNet pre-training is
ill-suited for the medical image captioning task, especially for less frequent
conditions (eg: enlarged cardiomediastinum, lung lesion, pneumothorax).
Furthermore, we demonstrate that the double feature model improves performance
for specific medical conditions (edema, consolidation, pneumothorax, support
devices) and overall CheXbert F1 score, and should be further developed in
future work. Such a double feature model, including both ImageNet pre-training
as well as domain specific pre-training, could be used in a wide range of image
captioning models in medicine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InvNorm: Domain Generalization for Object Detection in Gastrointestinal Endoscopy. (arXiv:2205.02842v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02842">
<div class="article-summary-box-inner">
<span><p>Domain Generalization is a challenging topic in computer vision, especially
in Gastrointestinal Endoscopy image analysis. Due to several device limitations
and ethical reasons, current open-source datasets are typically collected on a
limited number of patients using the same brand of sensors. Different brands of
devices and individual differences will significantly affect the model's
generalizability. Therefore, to address the generalization problem in
GI(Gastrointestinal) endoscopy, we propose a multi-domain GI dataset and a
light, plug-in block called InvNorm(Invertible Normalization), which could
achieve a better generalization performance in any structure. Previous
DG(Domain Generalization) methods fail to achieve invertible transformation,
which would lead to some misleading augmentation. Moreover, these models would
be more likely to lead to medical ethics issues. Our method utilizes
normalizing flow to achieve invertible and explainable style normalization to
address the problem. The effectiveness of InvNorm is demonstrated on a wide
range of tasks, including GI recognition, GI object detection, and natural
image recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Network Based Synthetic Learning and a Novel Domain Relevant Loss Term for Spine Radiographs. (arXiv:2205.02843v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02843">
<div class="article-summary-box-inner">
<span><p>Problem: There is a lack of big data for the training of deep learning models
in medicine, characterized by the time cost of data collection and privacy
concerns. Generative adversarial networks (GANs) offer both the potential to
generate new data, as well as to use this newly generated data, without
inclusion of patients' real data, for downstream applications.
</p>
<p>Approach: A series of GANs were trained and applied for a downstream computer
vision spine radiograph abnormality classification task. Separate classifiers
were trained with either access or no access to the original imaging. Trained
GANs included a conditional StyleGAN2 with adaptive discriminator augmentation,
a conditional StyleGAN2 with adaptive discriminator augmentation to generate
spine radiographs conditional on lesion type, and using a novel clinical loss
term for the generator a StyleGAN2 with adaptive discriminator augmentation
conditional on abnormality (SpineGAN). Finally, a differential privacy imposed
StyleGAN2 with adaptive discriminator augmentation conditional on abnormality
was trained and an ablation study was performed on its differential privacy
impositions.
</p>
<p>Key Results: We accomplish GAN generation of synthetic spine radiographs
without meaningful input for the first time from a literature review. We
further demonstrate the success of synthetic learning for the spine domain with
a downstream clinical classification task (AUC of 0.830 using synthetic data
compared to AUC of 0.886 using the real data). Importantly, the introduction of
a new clinical loss term for the generator was found to increase generation
recall as well as accelerate model training. Lastly, we demonstrate that, in a
limited size medical dataset, differential privacy impositions severely impede
GAN training, finding that this is specifically due to the requirement for
gradient perturbation with noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invariant Content Synergistic Learning for Domain Generalization of Medical Image Segmentation. (arXiv:2205.02845v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02845">
<div class="article-summary-box-inner">
<span><p>While achieving remarkable success for medical image segmentation, deep
convolution neural networks (DCNNs) often fail to maintain their robustness
when confronting test data with the novel distribution. To address such a
drawback, the inductive bias of DCNNs is recently well-recognized.
Specifically, DCNNs exhibit an inductive bias towards image style (e.g.,
superficial texture) rather than invariant content (e.g., object shapes). In
this paper, we propose a method, named Invariant Content Synergistic Learning
(ICSL), to improve the generalization ability of DCNNs on unseen datasets by
controlling the inductive bias. First, ICSL mixes the style of training
instances to perturb the training distribution. That is to say, more diverse
domains or styles would be made available for training DCNNs. Based on the
perturbed distribution, we carefully design a dual-branches invariant content
synergistic learning strategy to prevent style-biased predictions and focus
more on the invariant content. Extensive experimental results on two typical
medical image segmentation tasks show that our approach performs better than
state-of-the-art domain generalization methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation with Super Images: A New 2D Perspective on 3D Medical Image Analysis. (arXiv:2205.02847v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02847">
<div class="article-summary-box-inner">
<span><p>Deep learning is showing an increasing number of audience in medical imaging
research. In the segmentation task of medical images, we oftentimes rely on
volumetric data, and thus require the use of 3D architectures which are praised
for their ability to capture more features from the depth dimension. Yet, these
architectures are generally more ineffective in time and compute compared to
their 2D counterpart on account of 3D convolutions, max pooling,
up-convolutions, and other operations used in these networks. Moreover, there
are limited to no 3D pretrained model weights, and pretraining is generally
challenging. To alleviate these issues, we propose to cast volumetric data to
2D super images and use 2D networks for the segmentation task. The method
processes the 3D image by stitching slices side-by-side to generate a super
resolution image. While the depth information is lost, we expect that deep
neural networks can still capture and learn these features. Our goal in this
work is to introduce a new perspective when dealing with volumetric data, and
test our hypothesis using vanilla networks. We hope that this approach, while
achieving close enough results to 3D networks using only 2D counterparts, can
attract more related research in the future, especially in medical image
analysis since volumetric data is comparably limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Brains: Subvolume Recombination for Data Augmentation in Large Vessel Occlusion Detection. (arXiv:2205.02848v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02848">
<div class="article-summary-box-inner">
<span><p>Ischemic strokes are often caused by large vessel occlusions (LVOs), which
can be visualized and diagnosed with Computed Tomography Angiography scans. As
time is brain, a fast, accurate and automated diagnosis of these scans is
desirable. Human readers compare the left and right hemispheres in their
assessment of strokes. A large training data set is required for a standard
deep learning-based model to learn this strategy from data. As labeled medical
data in this field is rare, other approaches need to be developed. To both
include the prior knowledge of side comparison and increase the amount of
training data, we propose an augmentation method that generates artificial
training samples by recombining vessel tree segmentations of the hemispheres or
hemisphere subregions from different patients. The subregions cover vessels
commonly affected by LVOs, namely the internal carotid artery (ICA) and middle
cerebral artery (MCA). In line with the augmentation scheme, we use a
3D-DenseNet fed with task-specific input, fostering a side-by-side comparison
between the hemispheres. Furthermore, we propose an extension of that
architecture to process the individual hemisphere subregions. All
configurations predict the presence of an LVO, its side, and the affected
subregion. We show the effect of recombination as an augmentation strategy in a
5-fold cross validated ablation study. We enhanced the AUC for patient-wise
classification regarding the presence of an LVO of all investigated
architectures. For one variant, the proposed method improved the AUC from 0.73
without augmentation to 0.89. The best configuration detects LVOs with an AUC
of 0.91, LVOs in the ICA with an AUC of 0.96, and in the MCA with 0.91 while
accurately predicting the affected side.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching. (arXiv:2205.02849v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02849">
<div class="article-summary-box-inner">
<span><p>This paper tackles the challenge of forensic medical image matching (FMIM)
using deep neural networks (DNNs). FMIM is a particular case of content-based
image retrieval (CBIR). The main challenge in FMIM compared to the general case
of CBIR, is that the subject to whom a query image belongs may be affected by
aging and progressive degenerative disorders, making it difficult to match data
on a subject level. CBIR with DNNs is generally solved by minimizing a ranking
loss, such as Triplet loss (TL), computed on image representations extracted by
a DNN from the original data. TL, in particular, operates on triplets: anchor,
positive (similar to anchor) and negative (dissimilar to anchor). Although TL
has been shown to perform well in many CBIR tasks, it still has limitations,
which we identify and analyze in this work. In this paper, we introduce (i) the
AdaTriplet loss -- an extension of TL whose gradients adapt to different
difficulty levels of negative samples, and (ii) the AutoMargin method -- a
technique to adjust hyperparameters of margin-based losses such as TL and our
proposed loss dynamically. Our results are evaluated on two large-scale
benchmarks for FMIM based on the Osteoarthritis Initiative and Chest X-ray-14
datasets. The codes allowing replication of this study have been made publicly
available at \url{https://github.com/Oulu-IMEDS/AdaTriplet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Reinforcement Learning Framework for Rapid Diagnosis of Whole Slide Pathological Images. (arXiv:2205.02850v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02850">
<div class="article-summary-box-inner">
<span><p>The deep neural network is a research hotspot for histopathological image
analysis, which can improve the efficiency and accuracy of diagnosis for
pathologists or be used for disease screening. The whole slide pathological
image can reach one gigapixel and contains abundant tissue feature information,
which needs to be divided into a lot of patches in the training and inference
stages. This will lead to a long convergence time and large memory consumption.
Furthermore, well-annotated data sets are also in short supply in the field of
digital pathology. Inspired by the pathologist's clinical diagnosis process, we
propose a weakly supervised deep reinforcement learning framework, which can
greatly reduce the time required for network inference. We use neural network
to construct the search model and decision model of reinforcement learning
agent respectively. The search model predicts the next action through the image
features of different magnifications in the current field of view, and the
decision model is used to return the predicted probability of the current field
of view image. In addition, an expert-guided model is constructed by
multi-instance learning, which not only provides rewards for search model, but
also guides decision model learning by the knowledge distillation method.
Experimental results show that our proposed method can achieve fast inference
and accurate prediction of whole slide images without any pixel-level
annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Context for Deep Object Detectors. (arXiv:2205.02887v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02887">
<div class="article-summary-box-inner">
<span><p>Which object detector is suitable for your context sensitive task? Deep
object detectors exploit scene context for recognition differently. In this
paper, we group object detectors into 3 categories in terms of context use: no
context by cropping the input (RCNN), partial context by cropping the
featuremap (two-stage methods) and full context without any cropping
(single-stage methods). We systematically evaluate the effect of context for
each deep detector category. We create a fully controlled dataset for varying
context and investigate the context for deep detectors. We also evaluate
gradually removing the background context and the foreground object on MS COCO.
We demonstrate that single-stage and two-stage object detectors can and will
use the context by virtue of their large receptive field. Thus, choosing the
best object detector may depend on the application context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Jacobian Fields: Learning Intrinsic Mappings of Arbitrary Meshes. (arXiv:2205.02904v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02904">
<div class="article-summary-box-inner">
<span><p>This paper introduces a framework designed to accurately predict piecewise
linear mappings of arbitrary meshes via a neural network, enabling training and
evaluating over heterogeneous collections of meshes that do not share a
triangulation, as well as producing highly detail-preserving maps whose
accuracy exceeds current state of the art. The framework is based on reducing
the neural aspect to a prediction of a matrix for a single given point,
conditioned on a global shape descriptor. The field of matrices is then
projected onto the tangent bundle of the given mesh, and used as candidate
jacobians for the predicted map. The map is computed by a standard Poisson
solve, implemented as a differentiable layer with cached pre-factorization for
efficient training. This construction is agnostic to the triangulation of the
input, thereby enabling applications on datasets with varying triangulations.
At the same time, by operating in the intrinsic gradient domain of each
individual mesh, it allows the framework to predict highly-accurate mappings.
We validate these properties by conducting experiments over a broad range of
scenarios, from semantic ones such as morphing, registration, and deformation
transfer, to optimization-based ones, such as emulating elastic deformations
and contact correction, as well as being the first work, to our knowledge, to
tackle the task of learning to compute UV parameterizations of arbitrary
meshes. The results exhibit the high accuracy of the method as well as its
versatility, as it is readily applied to the above scenarios without any
changes to the framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Representative Samples for Few-Shot Classification. (arXiv:2205.02918v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02918">
<div class="article-summary-box-inner">
<span><p>Few-shot learning (FSL) aims to learn new categories with a few visual
samples per class. Few-shot class representations are often biased due to data
scarcity. To mitigate this issue, we propose to generate visual samples based
on semantic embeddings using a conditional variational autoencoder (CVAE)
model. We train this CVAE model on base classes and use it to generate features
for novel classes. More importantly, we guide this VAE to strictly generate
representative samples by removing non-representative samples from the base
training set when training the CVAE model. We show that this training scheme
enhances the representativeness of the generated samples and therefore,
improves the few-shot classification results. Experimental results show that
our method improves three FSL baseline methods by substantial margins,
achieving state-of-the-art few-shot classification performance on miniImageNet
and tieredImageNet datasets for both 1-shot and 5-shot settings. Code is
available at: https://github.com/cvlab-stonybrook/fsl-rsvae.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FisheyeDistill: Self-Supervised Monocular Depth Estimation with Ordinal Distillation for Fisheye Cameras. (arXiv:2205.02930v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02930">
<div class="article-summary-box-inner">
<span><p>In this paper, we deal with the problem of monocular depth estimation for
fisheye cameras in a self-supervised manner. A known issue of self-supervised
depth estimation is that it suffers in low-light/over-exposure conditions and
in large homogeneous regions. To tackle this issue, we propose a novel ordinal
distillation loss that distills the ordinal information from a large teacher
model. Such a teacher model, since having been trained on a large amount of
diverse data, can capture the depth ordering information well, but lacks in
preserving accurate scene geometry. Combined with self-supervised losses, we
show that our model can not only generate reasonable depth maps in challenging
environments but also better recover the scene geometry. We further leverage
the fisheye cameras of an AR-Glasses device to collect an indoor dataset to
facilitate evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Urban Water Consumption using Remotely Sensed Data. (arXiv:2205.02932v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02932">
<div class="article-summary-box-inner">
<span><p>Urban metabolism is an active field of research that deals with the
estimation of emissions and resource consumption from urban regions. The
analysis could be carried out through a manual surveyor by the implementation
of elegant machine learning algorithms. In this exploratory work, we estimate
the water consumption by the buildings in the region captured by satellite
imagery. To this end, we break our analysis into three parts: i) Identification
of building pixels, given a satellite image, followed by ii) identification of
the building type (residential/non-residential) from the building pixels, and
finally iii) using the building pixels along with their type to estimate the
water consumption using the average per unit area consumption for different
building types as obtained from municipal surveys.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biometric Signature Verification Using Recurrent Neural Networks. (arXiv:2205.02934v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02934">
<div class="article-summary-box-inner">
<span><p>Architectures based on Recurrent Neural Networks (RNNs) have been
successfully applied to many different tasks such as speech or handwriting
recognition with state-of-the-art results. The main contribution of this work
is to analyse the feasibility of RNNs for on-line signature verification in
real practical scenarios. We have considered a system based on Long Short-Term
Memory (LSTM) with a Siamese architecture whose goal is to learn a similarity
metric from pairs of signatures. For the experimental work, the BiosecurID
database comprised of 400 users and 4 separated acquisition sessions are
considered. Our proposed LSTM RNN system has outperformed the results of recent
published works on the BiosecurID benchmark in figures ranging from 17.76% to
28.00% relative verification performance improvement for skilled forgeries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Propaganda Techniques in Visuo-Lingual Metaphor in Memes. (arXiv:2205.02937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02937">
<div class="article-summary-box-inner">
<span><p>The exponential rise of social media networks has allowed the production,
distribution, and consumption of data at a phenomenal rate. Moreover, the
social media revolution has brought a unique phenomenon to social media
platforms called Internet memes. Internet memes are one of the most popular
contents used on social media, and they can be in the form of images with a
witty, catchy, or satirical text description. In this paper, we are dealing
with propaganda that is often seen in Internet memes in recent times.
Propaganda is communication, which frequently includes psychological and
rhetorical techniques to manipulate or influence an audience to act or respond
as the propagandist wants. To detect propaganda in Internet memes, we propose a
multimodal deep learning fusion system that fuses the text and image feature
representations and outperforms individual models based solely on either text
or image modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Immiscible Color Flows in Optimal Transport Networks for Image Classification. (arXiv:2205.02938v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02938">
<div class="article-summary-box-inner">
<span><p>In classification tasks, it is crucial to meaningfully exploit information
contained in data. Here, we propose a physics-inspired dynamical system that
adapts Optimal Transport principles to effectively leverage color distributions
of images. Our dynamics regulates immiscible fluxes of colors traveling on a
network built from images. Instead of aggregating colors together, it treats
them as different commodities that interact with a shared capacity on edges.
Our method outperforms competitor algorithms on image classification tasks in
datasets where color information matters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN-Augmented Visual-Inertial SLAM with Planar Constraints. (arXiv:2205.02940v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02940">
<div class="article-summary-box-inner">
<span><p>We present a robust visual-inertial SLAM system that combines the benefits of
Convolutional Neural Networks (CNNs) and planar constraints. Our system
leverages a CNN to predict the depth map and the corresponding uncertainty map
for each image. The CNN depth effectively bootstraps the back-end optimization
of SLAM and meanwhile the CNN uncertainty adaptively weighs the contribution of
each feature point to the back-end optimization. Given the gravity direction
from the inertial sensor, we further present a fast plane detection method that
detects horizontal planes via one-point RANSAC and vertical planes via
two-point RANSAC. Those stably detected planes are in turn used to regularize
the back-end optimization of SLAM. We evaluate our system on a public dataset,
\ie, EuRoC, and demonstrate improved results over a state-of-the-art SLAM
system, \ie, ORB-SLAM3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn-to-Race Challenge 2022: Benchmarking Safe Learning and Cross-domain Generalisation in Autonomous Racing. (arXiv:2205.02953v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02953">
<div class="article-summary-box-inner">
<span><p>We present the results of our autonomous racing virtual challenge, based on
the newly-released Learn-to-Race (L2R) simulation framework, which seeks to
encourage interdisciplinary research in autonomous driving and to help advance
the state of the art on a realistic benchmark. Analogous to racing being used
to test cutting-edge vehicles, we envision autonomous racing to serve as a
particularly challenging proving ground for autonomous agents as: (i) they need
to make sub-second, safety-critical decisions in a complex, fast-changing
environment; and (ii) both perception and control must be robust to
distribution shifts, novel road features, and unseen obstacles. Thus, the main
goal of the challenge is to evaluate the joint safety, performance, and
generalisation capabilities of reinforcement learning agents on multi-modal
perception, through a two-stage process. In the first stage of the challenge,
we evaluate an autonomous agent's ability to drive as fast as possible, while
adhering to safety constraints. In the second stage, we additionally require
the agent to adapt to an unseen racetrack through safe exploration. In this
paper, we describe the new L2R Task 2.0 benchmark, with refined metrics and
baseline approaches. We also provide an overview of deployment, evaluation, and
rankings for the inaugural instance of the L2R Autonomous Racing Virtual
Challenge (supported by Carnegie Mellon University, Arrival Ltd., AICrowd,
Amazon Web Services, and Honda Research), which officially used the new L2R
Task 2.0 benchmark and received over 20,100 views, 437 active participants, 46
teams, and 733 model submissions -- from 88 unique institutions, in 28
different countries. Finally, we release leaderboard results from the challenge
and provide description of the two top-ranking approaches in cross-domain model
transfer, across multiple sensor configurations and simulated races.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Graph Expansion for Semantics-Guided Image Outpainting. (arXiv:2205.02958v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02958">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the task of semantics-guided image outpainting,
which is to complete an image by generating semantically practical content.
Different from most existing image outpainting works, we approach the above
task by understanding and completing image semantics at the scene graph level.
In particular, we propose a novel network of Scene Graph Transformer (SGT),
which is designed to take node and edge features as inputs for modeling the
associated structural information. To better understand and process graph-based
inputs, our SGT uniquely performs feature attention at both node and edge
levels. While the former views edges as relationship regularization, the latter
observes the co-occurrence of nodes for guiding the attention process. We
demonstrate that, given a partial input image with its layout and scene graph,
our SGT can be applied for scene graph expansion and its conversion to a
complete layout. Following state-of-the-art layout-to-image conversions works,
the task of image outpainting can be completed with sufficient and practical
semantics introduced. Extensive experiments are conducted on the datasets of
MS-COCO and Visual Genome, which quantitatively and qualitatively confirm the
effectiveness of our proposed SGT and outpainting frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approximate Convex Decomposition for 3D Meshes with Collision-Aware Concavity and Tree Search. (arXiv:2205.02961v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02961">
<div class="article-summary-box-inner">
<span><p>Approximate convex decomposition aims to decompose a 3D shape into a set of
almost convex components, whose convex hulls can then be used to represent the
input shape. It thus enables efficient geometry processing algorithms
specifically designed for convex shapes and has been widely used in game
engines, physics simulations, and animation. While prior works can capture the
global structure of input shapes, they may fail to preserve fine-grained
details (e.g., filling a toaster's slots), which are critical for retaining the
functionality of objects in interactive environments. In this paper, we propose
a novel method that addresses the limitations of existing approaches from three
perspectives: (a) We introduce a novel collision-aware concavity metric that
examines the distance between a shape and its convex hull from both the
boundary and the interior. The proposed concavity preserves collision
conditions and is more robust to detect various approximation errors. (b) We
decompose shapes by directly cutting meshes with 3D planes. It ensures
generated convex hulls are intersection-free and avoids voxelization errors.
(c) Instead of using a one-step greedy strategy, we propose employing a
multi-step tree search to determine the cutting planes, which leads to a
globally better solution and avoids unnecessary cuttings. Through extensive
evaluation on a large-scale articulated object dataset, we show that our method
generates decompositions closer to the original shape with fewer components. It
thus supports delicate and efficient object interaction in downstream
applications. We will release our implementation to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Scale Transfer Learning for Differentially Private Image Classification. (arXiv:2205.02973v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02973">
<div class="article-summary-box-inner">
<span><p>Differential Privacy (DP) provides a formal framework for training machine
learning models with individual example level privacy. Training models with DP
protects the model against leakage of sensitive data in a potentially
adversarial setting. In the field of deep learning, Differentially Private
Stochastic Gradient Descent (DP-SGD) has emerged as a popular private training
algorithm. Private training using DP-SGD protects against leakage by injecting
noise into individual example gradients, such that the trained model weights
become nearly independent of the use any particular training example. While
this result is quite appealing, the computational cost of training large-scale
models with DP-SGD is substantially higher than non-private training. This is
further exacerbated by the fact that increasing the number of parameters leads
to larger degradation in utility with DP. In this work, we zoom in on the
ImageNet dataset and demonstrate that similar to the non-private case,
pre-training over-parameterized models on a large public dataset can lead to
substantial gains when the model is finetuned privately. Moreover, by
systematically comparing private and non-private models across a range of huge
batch sizes, we find that similar to non-private setting, choice of optimizer
can further improve performance substantially with DP. By switching from DP-SGD
to DP-LAMB we saw improvement of up to 20$\%$ points (absolute). Finally, we
show that finetuning just the last layer for a \emph{single step} in the full
batch setting leads to both SOTA results of 81.7 $\%$ under a wide privacy
budget range of $\epsilon \in [4, 10]$ and $\delta$ = $10^{-6}$ while
minimizing the computational overhead substantially.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generate and Edit Your Own Character in a Canonical View. (arXiv:2205.02974v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02974">
<div class="article-summary-box-inner">
<span><p>Recently, synthesizing personalized characters from a single user-given
portrait has received remarkable attention as a drastic popularization of
social media and the metaverse. The input image is not always in frontal view,
thus it is important to acquire or predict canonical view for 3D modeling or
other applications. Although the progress of generative models enables the
stylization of a portrait, obtaining the stylized image in canonical view is
still a challenging task. There have been several studies on face
frontalization but their performance significantly decreases when input is not
in the real image domain, e.g., cartoon or painting. Stylizing after
frontalization also results in degenerated output. In this paper, we propose a
novel and unified framework which generates stylized portraits in canonical
view. With a proposed latent mapper, we analyze and discover frontalization
mapping in a latent space of StyleGAN to stylize and frontalize at once. In
addition, our model can be trained with unlabelled 2D image sets, without any
3D supervision. The effectiveness of our method is demonstrated by experimental
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-view Point Cloud Registration based on Evolutionary Multitasking with Bi-Channel Knowledge Sharing Mechanism. (arXiv:2205.02996v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02996">
<div class="article-summary-box-inner">
<span><p>Registration of multi-view point clouds is fundamental in 3D reconstruction.
Since there are close connections between point clouds captured from different
viewpoints, registration performance can be enhanced if these connections be
harnessed properly. Therefore, this paper models the registration problem as
multi-task optimization, and proposes a novel bi-channel knowledge sharing
mechanism for effective and efficient problem solving. The modeling of
multi-view point cloud registration as multi-task optimization are twofold. By
simultaneously considering the local accuracy of two point clouds as well as
the global consistency posed by all the point clouds involved, a fitness
function with an adaptive threshold is derived. Also a framework of the
co-evolutionary search process is defined for the concurrent optimization of
multiple fitness functions belonging to related tasks. To enhance solution
quality and convergence speed, the proposed bi-channel knowledge sharing
mechanism plays its role. The intra-task knowledge sharing introduces aiding
tasks that are much simpler to solve, and useful information is shared within
tasks, accelerating the search process. The inter-task knowledge sharing
explores commonalities buried among tasks, aiming to prevent tasks from getting
stuck to local optima. Comprehensive experiments conducted on model object as
well as scene point clouds show the efficacy of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Pretraining for Semi-Supervised Learning in the Low-Label Regime. (arXiv:2205.03001v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03001">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) addresses the lack of labeled data by
exploiting large unlabeled data through pseudolabeling. However, in the
extremely low-label regime, pseudo labels could be incorrect, a.k.a. the
confirmation bias, and the pseudo labels will in turn harm the network
training. Recent studies combined finetuning (FT) from pretrained weights with
SSL to mitigate the challenges and claimed superior results in the low-label
regime. In this work, we first show that the better pretrained weights brought
in by FT account for the state-of-the-art performance, and importantly that
they are universally helpful to off-the-shelf semi-supervised learners. We
further argue that direct finetuning from pretrained weights is suboptimal due
to covariate shift and propose a contrastive target pretraining step to adapt
model weights towards target dataset. We carried out extensive experiments on
both classification and segmentation tasks by doing target pretraining then
followed by semi-supervised finetuning. The promising results validate the
efficacy of target pretraining for SSL, in particular in the low-label regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Fingerprint Detection Method by Fingerprint Ridge Orientation Check. (arXiv:2205.03019v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03019">
<div class="article-summary-box-inner">
<span><p>Fingerprints are popular among the biometric based systems due to ease of
acquisition, uniqueness and availability. Nowadays it is used in smart phone
security, digital payment and digital locker. Fingerprint recognition
technology has been studied for a long time, and its recognition rate has
recently risen to a high level. In particular, with the introduction of Deep
Neural Network technologies, the recognition rate that could not be reached
before was reached. In this paper, we propose a fingerprint detection algorithm
used in a fingerprint recognition system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantification of Robotic Surgeries with Vision-Based Deep Learning. (arXiv:2205.03028v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03028">
<div class="article-summary-box-inner">
<span><p>Surgery is a high-stakes domain where surgeons must navigate critical
anatomical structures and actively avoid potential complications while
achieving the main task at hand. Such surgical activity has been shown to
affect long-term patient outcomes. To better understand this relationship,
whose mechanics remain unknown for the majority of surgical procedures, we
hypothesize that the core elements of surgery must first be quantified in a
reliable, objective, and scalable manner. We believe this is a prerequisite for
the provision of surgical feedback and modulation of surgeon performance in
pursuit of improved patient outcomes. To holistically quantify surgeries, we
propose a unified deep learning framework, entitled Roboformer, which operates
exclusively on videos recorded during surgery to independently achieve multiple
tasks: surgical phase recognition (the what of surgery), gesture classification
and skills assessment (the how of surgery). We validated our framework on four
video-based datasets of two commonly-encountered types of steps (dissection and
suturing) within minimally-invasive robotic surgeries. We demonstrated that our
framework can generalize well to unseen videos, surgeons, medical centres, and
surgical procedures. We also found that our framework, which naturally lends
itself to explainable findings, identified relevant information when achieving
a particular task. These findings are likely to instill surgeons with more
confidence in our framework's behaviour, increasing the likelihood of clinical
adoption, and thus paving the way for more targeted surgical feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Level Decoupled Transformer for Video Captioning. (arXiv:2205.03039v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03039">
<div class="article-summary-box-inner">
<span><p>Video captioning aims to understand the spatio-temporal semantic concept of
the video and generate descriptive sentences. The de-facto approach to this
task dictates a text generator to learn from \textit{offline-extracted} motion
or appearance features from \textit{pre-trained} vision models. However, these
methods may suffer from the so-called \textbf{\textit{"couple"}} drawbacks on
both \textit{video spatio-temporal representation} and \textit{sentence
generation}. For the former, \textbf{\textit{"couple"}} means learning
spatio-temporal representation in a single model(3DCNN), resulting the problems
named \emph{disconnection in task/pre-train domain} and \emph{hard for
end-to-end training}. As for the latter, \textbf{\textit{"couple"}} means
treating the generation of visual semantic and syntax-related words equally. To
this end, we present $\mathcal{D}^{2}$ - a dual-level decoupled transformer
pipeline to solve the above drawbacks: \emph{(i)} for video spatio-temporal
representation, we decouple the process of it into
"first-spatial-then-temporal" paradigm, releasing the potential of using
dedicated model(\textit{e.g.} image-text pre-training) to connect the
pre-training and downstream tasks, and makes the entire model end-to-end
trainable. \emph{(ii)} for sentence generation, we propose \emph{Syntax-Aware
Decoder} to dynamically measure the contribution of visual semantic and
syntax-related words. Extensive experiments on three widely-used benchmarks
(MSVD, MSR-VTT and VATEX) have shown great potential of the proposed
$\mathcal{D}^{2}$ and surpassed the previous methods by a large margin in the
task of video captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Object Detection via Prototypical Task Correlation Guided Gating Mechanism. (arXiv:2205.03055v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03055">
<div class="article-summary-box-inner">
<span><p>Continual learning is a challenging real-world problem for constructing a
mature AI system when data are provided in a streaming fashion. Despite recent
progress in continual classification, the researches of continual object
detection are impeded by the diverse sizes and numbers of objects in each
image. Different from previous works that tune the whole network for all tasks,
in this work, we present a simple and flexible framework for continual object
detection via pRotOtypical taSk corrElaTion guided gaTing mechAnism (ROSETTA).
Concretely, a unified framework is shared by all tasks while task-aware gates
are introduced to automatically select sub-models for specific tasks. In this
way, various knowledge can be successively memorized by storing their
corresponding sub-model weights in this system. To make ROSETTA automatically
determine which experience is available and useful, a prototypical task
correlation guided Gating Diversity Controller(GDC) is introduced to adaptively
adjust the diversity of gates for the new task based on class-specific
prototypes. GDC module computes class-to-class correlation matrix to depict the
cross-task correlation, and hereby activates more exclusive gates for the new
task if a significant domain gap is observed. Comprehensive experiments on
COCO-VOC, KITTI-Kitchen, class-incremental detection on VOC and sequential
learning of four tasks show that ROSETTA yields state-of-the-art performance on
both task-based and class-based continual object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Data-Uploading for Full-Quantum Classification. (arXiv:2205.03057v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03057">
<div class="article-summary-box-inner">
<span><p>The data representation in a machine-learning model strongly influences its
performance. This becomes even more important for quantum machine learning
models implemented on noisy intermediate scale quantum (NISQ) devices. Encoding
high dimensional data into a quantum circuit for a NISQ device without any loss
of information is not trivial and brings a lot of challenges. While simple
encoding schemes (like single qubit rotational gates to encode high dimensional
data) often lead to information loss within the circuit, complex encoding
schemes with entanglement and data re-uploading lead to an increase in the
encoding gate count. This is not well-suited for NISQ devices. This work
proposes 'incremental data-uploading', a novel encoding pattern for high
dimensional data that tackles these challenges. We spread the encoding gates
for the feature vector of a given data point throughout the quantum circuit
with parameterized gates in between them. This encoding pattern results in a
better representation of data in the quantum circuit with a minimal
pre-processing requirement. We show the efficiency of our encoding pattern on a
classification task using the MNIST and Fashion-MNIST datasets, and compare
different encoding methods via classification accuracy and the effective
dimension of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QLEVR: A Diagnostic Dataset for Quantificational Language and Elementary Visual Reasoning. (arXiv:2205.03075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03075">
<div class="article-summary-box-inner">
<span><p>Synthetic datasets have successfully been used to probe visual
question-answering datasets for their reasoning abilities. CLEVR
(johnson2017clevr), for example, tests a range of visual reasoning abilities.
The questions in CLEVR focus on comparisons of shapes, colors, and sizes,
numerical reasoning, and existence claims. This paper introduces a minimally
biased, diagnostic visual question-answering dataset, QLEVR, that goes beyond
existential and numerical quantification and focus on more complex quantifiers
and their combinations, e.g., asking whether there are more than two red balls
that are smaller than at least three blue balls in an image. We describe how
the dataset was created and present a first evaluation of state-of-the-art
visual question-answering models, showing that QLEVR presents a formidable
challenge to our current models. Code and Dataset are available at
https://github.com/zechenli03/QLEVR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crop Type Identification for Smallholding Farms: Analyzing Spatial, Temporal and Spectral Resolutions in Satellite Imagery. (arXiv:2205.03104v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03104">
<div class="article-summary-box-inner">
<span><p>The integration of the modern Machine Learning (ML) models into remote
sensing and agriculture has expanded the scope of the application of satellite
images in the agriculture domain. In this paper, we present how the accuracy of
crop type identification improves as we move from
medium-spatiotemporal-resolution (MSTR) to high-spatiotemporal-resolution
(HSTR) satellite images. We further demonstrate that high spectral resolution
in satellite imagery can improve prediction performance for low spatial and
temporal resolutions (LSTR) images. The F1-score is increased by 7% when using
multispectral data of MSTR images as compared to the best results obtained from
HSTR images. Similarly, when crop season based time series of multispectral
data is used we observe an increase of 1.2% in the F1-score. The outcome
motivates further advancements in the field of synthetic band generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled Dropout for Uncertainty Estimation. (arXiv:2205.03109v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03109">
<div class="article-summary-box-inner">
<span><p>Uncertainty quantification in a neural network is one of the most discussed
topics for safety-critical applications. Though Neural Networks (NNs) have
achieved state-of-the-art performance for many applications, they still provide
unreliable point predictions, which lack information about uncertainty
estimates. Among various methods to enable neural networks to estimate
uncertainty, Monte Carlo (MC) dropout has gained much popularity in a short
period due to its simplicity. In this study, we present a new version of the
traditional dropout layer where we are able to fix the number of dropout
configurations. As such, each layer can take and apply the new dropout layer in
the MC method to quantify the uncertainty associated with NN predictions. We
conduct experiments on both toy and realistic datasets and compare the results
with the MC method using the traditional dropout layer. Performance analysis
utilizing uncertainty evaluation metrics corroborates that our dropout layer
offers better performance in most cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A High-Accuracy Unsupervised Person Re-identification Method Using Auxiliary Information Mined from Datasets. (arXiv:2205.03124v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03124">
<div class="article-summary-box-inner">
<span><p>Supervised person re-identification methods rely heavily on high-quality
cross-camera training label. This significantly hinders the deployment of re-ID
models in real-world applications. The unsupervised person re-ID methods can
reduce the cost of data annotation, but their performance is still far lower
than the supervised ones. In this paper, we make full use of the auxiliary
information mined from the datasets for multi-modal feature learning, including
camera information, temporal information and spatial information. By analyzing
the style bias of cameras, the characteristics of pedestrians' motion
trajectories and the positions of camera network, this paper designs three
modules: Time-Overlapping Constraint (TOC), Spatio-Temporal Similarity (STS)
and Same-Camera Penalty (SCP) to exploit the auxiliary information. Auxiliary
information can improve the model performance and inference accuracy by
constructing association constraints or fusing with visual features. In
addition, this paper proposes three effective training tricks, including
Restricted Label Smoothing Cross Entropy Loss (RLSCE), Weight Adaptive Triplet
Loss (WATL) and Dynamic Training Iterations (DTI). The tricks achieve mAP of
72.4% and 81.1% on MARS and DukeMTMC-VideoReID, respectively. Combined with
auxiliary information exploiting modules, our methods achieve mAP of 89.9% on
DukeMTMC, where TOC, STS and SCP all contributed considerable performance
improvements. The method proposed by this paper outperforms most existing
unsupervised re-ID methods and narrows the gap between unsupervised and
supervised re-ID methods. Our code is at
https://github.com/tenghehan/AuxUSLReID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BDIS: Bayesian Dense Inverse Searching Method for Real-Time Stereo Surgical Image Matching. (arXiv:2205.03133v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03133">
<div class="article-summary-box-inner">
<span><p>In stereoscope-based Minimally Invasive Surgeries (MIS), dense stereo
matching plays an indispensable role in 3D shape recovery, AR, VR, and
navigation tasks. Although numerous Deep Neural Network (DNN) approaches are
proposed, the conventional prior-free approaches are still popular in the
industry because of the lack of open-source annotated data set and the
limitation of the task-specific pre-trained DNNs. Among the prior-free stereo
matching algorithms, there is no successful real-time algorithm in none GPU
environment for MIS. This paper proposes the first CPU-level real-time
prior-free stereo matching algorithm for general MIS tasks. We achieve an
average 17 Hz on 640*480 images with a single-core CPU (i5-9400) for surgical
images. Meanwhile, it achieves slightly better accuracy than the popular ELAS.
The patch-based fast disparity searching algorithm is adopted for the rectified
stereo images. A coarse-to-fine Bayesian probability and a spatial Gaussian
mixed model were proposed to evaluate the patch probability at different
scales. An optional probability density function estimation algorithm was
adopted to quantify the prediction variance. Extensive experiments demonstrated
the proposed method's capability to handle ambiguities introduced by the
textureless surfaces and the photometric inconsistency from the non-Lambertian
reflectance and dark illumination. The estimated probability managed to balance
the confidences of the patches for stereo images at different scales. It has
similar or higher accuracy and fewer outliers than the baseline ELAS in MIS,
while it is 4-5 times faster. The code and the synthetic data sets are
available at https://github.com/JingweiSong/BDIS-v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised 3D Point Cloud Segmentation via Multi-Prototype Learning. (arXiv:2205.03137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03137">
<div class="article-summary-box-inner">
<span><p>Addressing the annotation challenge in 3D Point Cloud segmentation has
inspired research into weakly supervised learning. Existing approaches mainly
focus on exploiting manifold and pseudo-labeling to make use of large unlabeled
data points. A fundamental challenge here lies in the large intra-class
variations of local geometric structure, resulting in subclasses within a
semantic class. In this work, we leverage this intuition and opt for
maintaining an individual classifier for each subclass. Technically, we design
a multi-prototype classifier, each prototype serves as the classifier weights
for one subclass. To enable effective updating of multi-prototype classifier
weights, we propose two constraints respectively for updating the prototypes
w.r.t. all point features and for encouraging the learning of diverse
prototypes. Experiments on weakly supervised 3D point cloud segmentation tasks
validate the efficacy of proposed method in particular at low-label regime. Our
hypothesis is also verified given the consistent discovery of semantic
subclasses at no cost of additional annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-CLOP: CLIP-Guided Collage and Photomontage. (arXiv:2205.03146v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03146">
<div class="article-summary-box-inner">
<span><p>The unabated mystique of large-scale neural networks, such as the CLIP dual
image-and-text encoder, popularized automatically generated art. Increasingly
more sophisticated generators enhanced the artworks' realism and visual
appearance, and creative prompt engineering enabled stylistic expression.
Guided by an artist-in-the-loop ideal, we design a gradient-based generator to
produce collages. It requires the human artist to curate libraries of image
patches and to describe (with prompts) the whole image composition, with the
option to manually adjust the patches' positions during generation, thereby
allowing humans to reclaim some control of the process and achieve greater
creative freedom. We explore the aesthetic potentials of high-resolution
collages, and provide an open-source Google Colab as an artistic tool.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Easy to Hard: Learning Language-guided Curriculum for Visual Question Answering on Remote Sensing Data. (arXiv:2205.03147v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03147">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) for remote sensing scene has great potential
in intelligent human-computer interaction system. Although VQA in computer
vision has been widely researched, VQA for remote sensing data (RSVQA) is still
in its infancy. There are two characteristics that need to be specially
considered for the RSVQA task. 1) No object annotations are available in RSVQA
datasets, which makes it difficult for models to exploit informative region
representation; 2) There are questions with clearly different difficulty levels
for each image in the RSVQA task. Directly training a model with questions in a
random order may confuse the model and limit the performance. To address these
two problems, in this paper, a multi-level visual feature learning method is
proposed to jointly extract language-guided holistic and regional image
features. Besides, a self-paced curriculum learning (SPCL)-based VQA model is
developed to train networks with samples in an easy-to-hard way. To be more
specific, a language-guided SPCL method with a soft weighting strategy is
explored in this work. The proposed model is evaluated on three public
datasets, and extensive experimental results show that the proposed RSVQA
framework can achieve promising performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating and Explaining the Frequency Bias in Image Classification. (arXiv:2205.03154v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03154">
<div class="article-summary-box-inner">
<span><p>CNNs exhibit many behaviors different from humans, one of which is the
capability of employing high-frequency components. This paper discusses the
frequency bias phenomenon in image classification tasks: the high-frequency
components are actually much less exploited than the low- and mid-frequency
components. We first investigate the frequency bias phenomenon by presenting
two observations on feature discrimination and learning priority. Furthermore,
we hypothesize that (i) the spectral density, (ii) class consistency directly
affect the frequency bias. Specifically, our investigations verify that the
spectral density of datasets mainly affects the learning priority, while the
class consistency mainly affects the feature discrimination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantics-Guided Moving Object Segmentation with 3D LiDAR. (arXiv:2205.03186v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03186">
<div class="article-summary-box-inner">
<span><p>Moving object segmentation (MOS) is a task to distinguish moving objects,
e.g., moving vehicles and pedestrians, from the surrounding static environment.
The segmentation accuracy of MOS can have an influence on odometry, map
construction, and planning tasks. In this paper, we propose a semantics-guided
convolutional neural network for moving object segmentation. The network takes
sequential LiDAR range images as inputs. Instead of segmenting the moving
objects directly, the network conducts single-scan-based semantic segmentation
and multiple-scan-based moving object segmentation in turn. The semantic
segmentation module provides semantic priors for the MOS module, where we
propose an adjacent scan association (ASA) module to convert the semantic
features of adjacent scans into the same coordinate system to fully exploit the
cross-scan semantic features. Finally, by analyzing the difference between the
transformed features, reliable MOS result can be obtained quickly. Experimental
results on the SemanticKITTI MOS dataset proves the effectiveness of our work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Atlas-powered deep learning (ADL) -- application to diffusion weighted MRI. (arXiv:2205.03210v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03210">
<div class="article-summary-box-inner">
<span><p>Deep learning has a great potential for estimating biomarkers in diffusion
weighted magnetic resonance imaging (dMRI). Atlases, on the other hand, are a
unique tool for modeling the spatio-temporal variability of biomarkers. In this
paper, we propose the first framework to exploit both deep learning and atlases
for biomarker estimation in dMRI. Our framework relies on non-linear diffusion
tensor registration to compute biomarker atlases and to estimate atlas
reliability maps. We also use nonlinear tensor registration to align the atlas
to a subject and to estimate the error of this alignment. We use the biomarker
atlas, atlas reliability map, and alignment error map, in addition to the dMRI
signal, as inputs to a deep learning model for biomarker estimation. We use our
framework to estimate fractional anisotropy and neurite orientation dispersion
from down-sampled dMRI data on a test cohort of 70 newborn subjects. Results
show that our method significantly outperforms standard estimation methods as
well as recent deep learning techniques. Our method is also more robust to
stronger measurement down-sampling factors. Our study shows that the advantages
of deep learning and atlases can be synergistically combined to achieve
unprecedented accuracy in biomarker estimation from dMRI data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Future Occupancy Grids in Dynamic Environment with Spatio-Temporal Learning. (arXiv:2205.03212v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03212">
<div class="article-summary-box-inner">
<span><p>Reliably predicting future occupancy of highly dynamic urban environments is
an important precursor for safe autonomous navigation. Common challenges in the
prediction include forecasting the relative position of other vehicles,
modelling the dynamics of vehicles subjected to different traffic conditions,
and vanishing surrounding objects. To tackle these challenges, we propose a
spatio-temporal prediction network pipeline that takes the past information
from the environment and semantic labels separately for generating future
occupancy predictions. Compared to the current SOTA, our approach predicts
occupancy for a longer horizon of 3 seconds and in a relatively complex
environment from the nuScenes dataset. Our experimental results demonstrate the
ability of spatio-temporal networks to understand scene dynamics without the
need for HD-Maps and explicit modeling dynamic objects. We publicly release our
occupancy grid dataset based on nuScenes to support further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forget Less, Count Better: A Domain-Incremental Self-Distillation Learning Benchmark for Lifelong Crowd Counting. (arXiv:2205.03307v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03307">
<div class="article-summary-box-inner">
<span><p>Crowd Counting has important applications in public safety and pandemic
control. A robust and practical crowd counting system has to be capable of
continuously learning with the new-coming domain data in real-world scenarios
instead of fitting one domain only. Off-the-shelf methods have some drawbacks
to handle multiple domains. 1) The models will achieve limited performance
(even drop dramatically) among old domains after training images from new
domains due to the discrepancies of intrinsic data distributions from various
domains, which is called catastrophic forgetting. 2) The well-trained model in
a specific domain achieves imperfect performance among other unseen domains
because of the domain shift. 3) It leads to linearly-increased storage overhead
either mixing all the data for training or simply training dozens of separate
models for different domains when new ones are available. To overcome these
issues, we investigate a new task of crowd counting under the incremental
domains training setting, namely, Lifelong Crowd Counting. It aims at
alleviating the catastrophic forgetting and improving the generalization
ability using a single model updated by the incremental domains. To be more
specific, we propose a self-distillation learning framework as a
benchmark~(Forget Less, Count Better, FLCB) for lifelong crowd counting, which
helps the model sustainably leverage previous meaningful knowledge for better
crowd counting to mitigate the forgetting when the new data arrive. Meanwhile,
a new quantitative metric, normalized backward transfer~(nBwT), is developed to
evaluate the forgetting degree of the model in the lifelong learning process.
Extensive experimental results demonstrate the superiority of our proposed
benchmark in achieving a low catastrophic forgetting degree and strong
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Distribution Learning. (arXiv:2205.03340v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03340">
<div class="article-summary-box-inner">
<span><p>We present prompt distribution learning for effectively adapting a
pre-trained vision-language model to address downstream recognition tasks. Our
method not only learns low-bias prompts from a few samples but also captures
the distribution of diverse prompts to handle the varying visual
representations. In this way, we provide high-quality task-related content for
facilitating recognition. This prompt distribution learning is realized by an
efficient approach that learns the output embeddings of prompts instead of the
input embeddings. Thus, we can employ a Gaussian distribution to model them
effectively and derive a surrogate loss for efficient training. Extensive
experiments on 12 datasets demonstrate that our method consistently and
significantly outperforms existing methods. For example, with 1 sample per
category, it relatively improves the average result by 9.1% compared to
human-crafted prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitask AET with Orthogonal Tangent Regularity for Dark Object Detection. (arXiv:2205.03346v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03346">
<div class="article-summary-box-inner">
<span><p>Dark environment becomes a challenge for computer vision algorithms owing to
insufficient photons and undesirable noise. To enhance object detection in a
dark environment, we propose a novel multitask auto encoding transformation
(MAET) model which is able to explore the intrinsic pattern behind illumination
translation. In a self-supervision manner, the MAET learns the intrinsic visual
structure by encoding and decoding the realistic illumination-degrading
transformation considering the physical noise model and image signal processing
(ISP).
</p>
<p>Based on this representation, we achieve the object detection task by
decoding the bounding box coordinates and classes. To avoid the
over-entanglement of two tasks, our MAET disentangles the object and degrading
features by imposing an orthogonal tangent regularity. This forms a parametric
manifold along which multitask predictions can be geometrically formulated by
maximizing the orthogonality between the tangents along the outputs of
respective tasks. Our framework can be implemented based on the mainstream
object detection architecture and directly trained end-to-end using normal
target detection datasets, such as VOC and COCO. We have achieved the
state-of-the-art performance using synthetic and real-world datasets. Code is
available at https://github.com/cuiziteng/MAET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All Grains, One Scheme (AGOS): Learning Multi-grain Instance Representation for Aerial Scene Classification. (arXiv:2205.03371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03371">
<div class="article-summary-box-inner">
<span><p>Aerial scene classification remains challenging as: 1) the size of key
objects in determining the scene scheme varies greatly; 2) many objects
irrelevant to the scene scheme are often flooded in the image. Hence, how to
effectively perceive the region of interests (RoIs) from a variety of sizes and
build more discriminative representation from such complicated object
distribution is vital to understand an aerial scene. In this paper, we propose
a novel all grains, one scheme (AGOS) framework to tackle these challenges. To
the best of our knowledge, it is the first work to extend the classic multiple
instance learning into multi-grain formulation. Specially, it consists of a
multi-grain perception module (MGP), a multi-branch multi-instance
representation module (MBMIR) and a self-aligned semantic fusion (SSF) module.
Firstly, our MGP preserves the differential dilated convolutional features from
the backbone, which magnifies the discriminative information from multi-grains.
Then, our MBMIR highlights the key instances in the multi-grain representation
under the MIL formulation. Finally, our SSF allows our framework to learn the
same scene scheme from multi-grain instance representations and fuses them, so
that the entire framework is optimized as a whole. Notably, our AGOS is
flexible and can be easily adapted to existing CNNs in a plug-and-play manner.
Extensive experiments on UCM, AID and NWPU benchmarks demonstrate that our AGOS
achieves a comparable performance against the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-mode Tensor Train Factorization with Spatial-spectral Regularization for Remote Sensing Images Recovery. (arXiv:2205.03380v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03380">
<div class="article-summary-box-inner">
<span><p>Tensor train (TT) factorization and corresponding TT rank, which can well
express the low-rankness and mode correlations of higher-order tensors, have
attracted much attention in recent years. However, TT factorization based
methods are generally not sufficient to characterize low-rankness along each
mode of third-order tensor. Inspired by this, we generalize the tensor train
factorization to the mode-k tensor train factorization and introduce a
corresponding multi-mode tensor train (MTT) rank. Then, we proposed a novel
low-MTT-rank tensor completion model via multi-mode TT factorization and
spatial-spectral smoothness regularization. To tackle the proposed model, we
develop an efficient proximal alternating minimization (PAM) algorithm.
Extensive numerical experiment results on visual data demonstrate that the
proposed MTTD3R method outperforms compared methods in terms of visual and
quantitative measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MINI: Mining Implicit Novel Instances for Few-Shot Object Detection. (arXiv:2205.03381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03381">
<div class="article-summary-box-inner">
<span><p>Learning from a few training samples is a desirable ability of an object
detector, inspiring the explorations of Few-Shot Object Detection (FSOD). Most
existing approaches employ a pretrain-transfer paradigm. The model is first
pre-trained on base classes with abundant data and then transferred to novel
classes with a few annotated samples. Despite the substantial progress, the
FSOD performance is still far behind satisfactory. During pre-training, due to
the co-occurrence between base and novel classes, the model is learned to treat
the co-occurred novel classes as backgrounds. During transferring, given scarce
samples of novel classes, the model suffers from learning discriminative
features to distinguish novel instances from backgrounds and base classes. To
overcome the obstacles, we propose a novel framework, Mining Implicit Novel
Instances (MINI), to mine the implicit novel instances as auxiliary training
samples, which widely exist in abundant base data but are not annotated. MINI
comprises an offline mining mechanism and an online mining mechanism. The
offline mining mechanism leverages a self-supervised discriminative model to
collaboratively mine implicit novel instances with a trained FSOD network.
Taking the mined novel instances as auxiliary training samples, the online
mining mechanism takes a teacher-student framework to simultaneously update the
FSOD network and the mined implicit novel instances on the fly. Extensive
experiments on PASCAL VOC and MS-COCO datasets show MINI achieves new
state-of-the-art performance on any shot and split. The significant performance
improvements demonstrate the superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Domain-Independent Feature Decomposition Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2003.09869v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.09869">
<div class="article-summary-box-inner">
<span><p>Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal
retrieval task for searching natural images given free-hand sketches under the
zero-shot scenario. Most existing methods solve this problem by simultaneously
projecting visual features and semantic supervision into a low-dimensional
common space for efficient retrieval. However, such low-dimensional projection
destroys the completeness of semantic knowledge in original semantic space, so
that it is unable to transfer useful knowledge well when learning semantic from
different modalities. Moreover, the domain information and semantic information
are entangled in visual features, which is not conducive for cross-modal
matching since it will hinder the reduction of domain gap between sketch and
image. In this paper, we propose a Progressive Domain-independent Feature
Decomposition (PDFD) network for ZS-SBIR. Specifically, with the supervision of
original semantic knowledge, PDFD decomposes visual features into domain
features and semantic ones, and then the semantic features are projected into
common space as retrieval features for ZS-SBIR. The progressive projection
strategy maintains strong semantic supervision. Besides, to guarantee the
retrieval features to capture clean and complete semantic information, the
cross-reconstruction loss is introduced to encourage that any combinations of
retrieval features and domain features can reconstruct the visual features.
Extensive experiments demonstrate the superiority of our PDFD over
state-of-the-art competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Quantification for Hyperspectral Image Denoising Frameworks based on Low-rank Matrix Approximation. (arXiv:2004.10959v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.10959">
<div class="article-summary-box-inner">
<span><p>Sliding-window based low-rank matrix approximation (LRMA) is a technique
widely used in hyperspectral images (HSIs) denoising or completion. However,
the uncertainty quantification of the restored HSI has not been addressed to
date. Accurate uncertainty quantification of the denoised HSI facilitates to
applications such as multi-source or multi-scale data fusion, data
assimilation, and product uncertainty quantification, since these applications
require an accurate approach to describe the statistical distributions of the
input data. Therefore, we propose a prior-free closed-form element-wise
uncertainty quantification method for LRMA-based HSI restoration. Our
closed-form algorithm overcomes the difficulty of the HSI patch mixing problem
caused by the sliding-window strategy used in the conventional LRMA process.
The proposed approach only requires the uncertainty of the observed HSI and
provides the uncertainty result relatively rapidly and with similar
computational complexity as the LRMA technique. We conduct extensive
experiments to validate the estimation accuracy of the proposed closed-form
uncertainty approach. The method is robust to at least 10% random impulse noise
at the cost of 10-20% of additional processing time compared to the LRMA. The
experiments indicate that the proposed closed-form uncertainty quantification
method is more applicable to real-world applications than the baseline Monte
Carlo test, which is computationally expensive. The code is available in the
attachment and will be released after the acceptance of this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction. (arXiv:2105.09711v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09711">
<div class="article-summary-box-inner">
<span><p>Joint relation modeling is a curial component in human motion prediction.
Most existing methods tend to design skeletal-based graphs to build the
relations among joints, where local interactions between joint pairs are well
learned. However, the global coordination of all joints, which reflects human
motion's balance property, is usually weakened because it is learned from part
to whole progressively and asynchronously. Thus, the final predicted motions
are sometimes unnatural. To tackle this issue, we learn a medium, called
balance attractor (BA), from the spatiotemporal features of motion to
characterize the global motion features, which is subsequently used to build
new joint relations. Through the BA, all joints are related synchronously, and
thus the global coordination of all joints can be better learned. Based on the
BA, we propose our framework, referred to Attractor-Guided Neural Network,
mainly including Attractor-Based Joint Relation Extractor (AJRE) and
Multi-timescale Dynamics Extractor (MTDE). The AJRE mainly includes Global
Coordination Extractor (GCE) and Local Interaction Extractor (LIE). The former
presents the global coordination of all joints, and the latter encodes local
interactions between joint pairs. The MTDE is designed to extract dynamic
information from raw position information for effective prediction. Extensive
experiments show that the proposed framework outperforms state-of-the-art
methods in both short and long-term predictions in H3.6M, CMU-Mocap, and 3DPW.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Adversarial Driving Scenario Generation with Explicit Knowledge Integration. (arXiv:2106.04066v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04066">
<div class="article-summary-box-inner">
<span><p>Generating adversarial scenarios, which have the potential to fail autonomous
driving systems, provides an effective way to improve the robustness. Extending
purely data-driven generative models, recent specialized models satisfy
additional controllable requirements such as embedding a traffic sign in a
driving scene by manipulating patterns implicitly in the neuron level. In this
paper, we introduce a method to incorporate domain knowledge explicitly in the
generation process to achieve the Semantically Adversarial Generation (SAG). To
be consistent with the composition of driving scenes, we first categorize the
knowledge into two types, the property of objects and the relationship among
objects. We then propose a tree-structured variational auto-encoder (T-VAE) to
learn hierarchical scene representation. By imposing semantic rules on the
properties of nodes and edges in the tree structure, explicit knowledge
integration enables controllable generation. We construct a synthetic example
to illustrate the controllability and explainability of our method in a
succinct setting. We further extend to realistic environments for autonomous
vehicles: our method efficiently identifies adversarial driving scenes against
different state-of-the-art 3D point cloud segmentation models and satisfies the
traffic rules specified as the explicit knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Total Recall in Industrial Anomaly Detection. (arXiv:2106.08265v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08265">
<div class="article-summary-box-inner">
<span><p>Being able to spot defective parts is a critical component in large-scale
industrial manufacturing. A particular challenge that we address in this work
is the cold-start problem: fit a model using nominal (non-defective) example
images only. While handcrafted solutions per class are possible, the goal is to
build systems that work well simultaneously on many different tasks
automatically. The best performing approaches combine embeddings from ImageNet
models with an outlier detection model. In this paper, we extend on this line
of work and propose \textbf{PatchCore}, which uses a maximally representative
memory bank of nominal patch-features. PatchCore offers competitive inference
times while achieving state-of-the-art performance for both detection and
localization. On the challenging, widely used MVTec AD benchmark PatchCore
achieves an image-level anomaly detection AUROC score of up to $99.6\%$, more
than halving the error compared to the next best competitor. We further report
competitive results on two additional datasets and also find competitive
results in the few samples regime.\freefootnote{$^*$ Work done during a
research internship at Amazon AWS.} Code:
github.com/amazon-research/patchcore-inspection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Remote Blood Oxygen Estimation From Videos Using Neural Networks. (arXiv:2107.05087v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05087">
<div class="article-summary-box-inner">
<span><p>Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory
functionality and is receiving increasing attention during the COVID-19
pandemic. Clinical findings show that it is possible for COVID-19 patients to
have significantly low SpO$_2$ before any obvious symptoms. The prevalence of
cameras has motivated researchers to investigate methods for monitoring SpO$_2$
using videos. Most prior schemes involving smartphones are contact-based: They
require a fingertip to cover the phone's camera and the nearby light source to
capture re-emitted light from the illuminated tissue. In this paper, we propose
the first convolutional neural network based noncontact SpO$_2$ estimation
scheme using smartphone cameras. The scheme analyzes the videos of a
participant's hand for physiological sensing, which is convenient and
comfortable, and can protect their privacy and allow for keeping face masks on.
We design our neural network architectures inspired by the optophysiological
models for SpO$_2$ measurement and demonstrate the explainability by
visualizing the weights for channel combination. Our proposed models outperform
the state-of-the-art model that is designed for contact-based SpO$_2$
measurement, showing the potential of our proposed method to contribute to
public health. We also analyze the impact of skin type and the side of a hand
on SpO$_2$ estimation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Optimal Conformal Classifiers. (arXiv:2110.09192v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09192">
<div class="article-summary-box-inner">
<span><p>Modern deep learning based classifiers show very high accuracy on test data
but this does not provide sufficient guarantees for safe deployment, especially
in high-stake AI applications such as medical diagnosis. Usually, predictions
are obtained without a reliable uncertainty estimate or a formal guarantee.
Conformal prediction (CP) addresses these issues by using the classifier's
predictions, e.g., its probability estimates, to predict confidence sets
containing the true class with a user-specified probability. However, using CP
as a separate processing step after training prevents the underlying model from
adapting to the prediction of confidence sets. Thus, this paper explores
strategies to differentiate through CP during training with the goal of
training model with the conformal wrapper end-to-end. In our approach,
conformal training (ConfTr), we specifically "simulate" conformalization on
mini-batches during training. Compared to standard training, ConfTr reduces the
average confidence set size (inefficiency) of state-of-the-art CP methods
applied after training. Moreover, it allows to "shape" the confidence sets
predicted at test time, which is difficult for standard CP. On experiments with
several datasets, we show ConfTr can influence how inefficiency is distributed
across classes, or guide the composition of confidence sets in terms of the
included classes, while retaining the guarantees offered by CP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAGLETS: A System for Automatic Semi-Supervised Learning with Auxiliary Data. (arXiv:2111.04798v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04798">
<div class="article-summary-box-inner">
<span><p>Machine learning practitioners often have access to a spectrum of data:
labeled data for the target task (which is often limited), unlabeled data, and
auxiliary data, the many available labeled datasets for other tasks. We
describe TAGLETS, a system built to study techniques for automatically
exploiting all three types of data and creating high-quality, servable
classifiers. The key components of TAGLETS are: (1) auxiliary data organized
according to a knowledge graph, (2) modules encapsulating different methods for
exploiting auxiliary and unlabeled data, and (3) a distillation stage in which
the ensembled modules are combined into a servable model. We compare TAGLETS
with state-of-the-art transfer learning and semi-supervised learning methods on
four image classification tasks. Our study covers a range of settings, varying
the amount of labeled data and the semantic relatedness of the auxiliary data
to the target task. We find that the intelligent incorporation of auxiliary and
unlabeled data into multiple learning techniques enables TAGLETS to match-and
most often significantly surpass-these alternatives. TAGLETS is available as an
open-source system at github.com/BatsResearch/taglets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CYBORG: Blending Human Saliency Into the Loss Improves Deep Learning. (arXiv:2112.00686v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00686">
<div class="article-summary-box-inner">
<span><p>Can deep learning models achieve greater generalization if their training is
guided by reference to human perceptual abilities? And how can we implement
this in a practical manner? This paper proposes a training strategy to ConveY
Brain Oversight to Raise Generalization (CYBORG). This new approach
incorporates human-annotated saliency maps into a CYBORG loss function that
guides the model's learning towards features from image regions that humans
find salient for the task. The Class Activation Mapping (CAM) mechanism is used
to probe the model's current saliency in each training batch, juxtapose this
model saliency with human saliency, and penalize large differences. Results on
the task of synthetic face detection, selected to illustrate the effectiveness
of the approach, show that CYBORG leads to significant improvement in accuracy
on unseen samples consisting of face images generated from six Generative
Adversarial Networks across multiple classification network architectures. We
also show that scaling to even seven times the training data with standard loss
cannot beat CYBORG accuracy. As a side effect, we observe that the addition of
explicit region annotation to the task of synthetic face detection increased
human classification performance. This work opens a new area of research on how
to incorporate human visual saliency into loss functions in practice. All data,
code and pre-trained models used in this work are offered with this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Echocardiography Segmentation with Enforced Temporal Consistency. (arXiv:2112.02102v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02102">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNN) have demonstrated their ability to
segment 2D cardiac ultrasound images. However, despite recent successes
according to which the intra-observer variability on end-diastole and
end-systole images has been reached, CNNs still struggle to leverage temporal
information to provide accurate and temporally consistent segmentation maps
across the whole cycle. Such consistency is required to accurately describe the
cardiac function, a necessary step in diagnosing many cardiovascular diseases.
In this paper, we propose a framework to learn the 2D+time apical long-axis
cardiac shape such that the segmented sequences can benefit from temporal and
anatomical consistency constraints. Our method is a post-processing that takes
as input segmented echocardiographic sequences produced by any state-of-the-art
method and processes it in two steps to (i) identify spatio-temporal
inconsistencies according to the overall dynamics of the cardiac sequence and
(ii) correct the inconsistencies. The identification and correction of cardiac
inconsistencies relies on a constrained autoencoder trained to learn a
physiologically interpretable embedding of cardiac shapes, where we can both
detect and fix anomalies. We tested our framework on 98 full-cycle sequences
from the CAMUS dataset, which are available alongside this paper. Our temporal
regularization method not only improves the accuracy of the segmentation across
the whole sequences, but also enforces temporal and anatomical consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DPICT: Deep Progressive Image Compression Using Trit-Planes. (arXiv:2112.06334v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06334">
<div class="article-summary-box-inner">
<span><p>We propose the deep progressive image compression using trit-planes (DPICT)
algorithm, which is the first learning-based codec supporting fine granular
scalability (FGS). First, we transform an image into a latent tensor using an
analysis network. Then, we represent the latent tensor in ternary digits
(trits) and encode it into a compressed bitstream trit-plane by trit-plane in
the decreasing order of significance. Moreover, within each trit-plane, we sort
the trits according to their rate-distortion priorities and transmit more
important information first. Since the compression network is less optimized
for the cases of using fewer trit-planes, we develop a postprocessing network
for refining reconstructed images at low rates. Experimental results show that
DPICT outperforms conventional progressive codecs significantly, while enabling
FGS transmission. Codes are available at
https://github.com/jaehanlee-mcl/DPICT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Image Denoising Algorithm Using Concepts of Quantum Many-Body Theory. (arXiv:2112.09254v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09254">
<div class="article-summary-box-inner">
<span><p>Sparse representation of real-life images is a very effective approach in
imaging applications, such as denoising. In recent years, with the growth of
computing power, data-driven strategies exploiting the redundancy within
patches extracted from one or several images to increase sparsity have become
more prominent. This paper presents a novel image denoising algorithm
exploiting such an image-dependent basis inspired by the quantum many-body
theory. Based on patch analysis, the similarity measures in a local image
neighborhood are formalized through a term akin to interaction in quantum
mechanics that can efficiently preserve the local structures of real images.
The versatile nature of this adaptive basis extends the scope of its
application to image-independent or image-dependent noise scenarios without any
adjustment. We carry out a rigorous comparison with contemporary methods to
demonstrate the denoising capability of the proposed algorithm regardless of
the image characteristics, noise statistics and intensity. We illustrate the
properties of the hyperparameters and their respective effects on the denoising
performance, together with automated rules of selecting their values close to
the optimal one in experimental setups with ground truth not available.
Finally, we show the ability of our approach to deal with practical images
denoising problems such as medical ultrasound image despeckling applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the Visible: A Survey on Cross-spectral Face Recognition. (arXiv:2201.04435v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04435">
<div class="article-summary-box-inner">
<span><p>Cross-spectral face recognition (CFR) refers to recognizing individuals using
face images stemming from different spectral bands, such as infrared vs.
visible. While CFR is inherently more challenging than classical face
recognition due to significant variation in facial appearance caused by the
modality gap, it is useful in many scenarios including night-vision biometrics
and detecting presentation attacks. Recent advances in convolutional neural
networks (CNNs) have resulted in significant improvement in the performance of
CFR systems. Given these developments, the contributions of this survey are
three-fold. First, we provide an overview of CFR, by formalizing the CFR
problem and presenting related applications. Secondly, we discuss the
appropriate spectral bands for face recognition and discuss recent CFR methods,
placing emphasis on deep neural networks. In particular we describe techniques
that have been proposed to extract and compare heterogeneous features emerging
from different spectral bands. We also discuss the datasets that have been used
for evaluating CFR methods. Finally, we discuss the challenges and future lines
of research on this topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImpliCity: City Modeling from Satellite Images with Deep Implicit Occupancy Fields. (arXiv:2201.09968v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09968">
<div class="article-summary-box-inner">
<span><p>High-resolution optical satellite sensors, combined with dense stereo
algorithms, have made it possible to reconstruct 3D city models from space.
However, these models are, in practice, rather noisy and tend to miss small
geometric features that are clearly visible in the images. We argue that one
reason for the limited quality may be a too early, heuristic reduction of the
triangulated 3D point cloud to an explicit height field or surface mesh. To
make full use of the point cloud and the underlying images, we introduce
ImpliCity, a neural representation of the 3D scene as an implicit, continuous
occupancy field, driven by learned embeddings of the point cloud and a stereo
pair of ortho-photos. We show that this representation enables the extraction
of high-quality DSMs: with image resolution 0.5$\,$m, ImpliCity reaches a
median height error of $\approx\,$0.7$\,$m and outperforms competing methods,
especially w.r.t. building reconstruction, featuring intricate roof details,
smooth surfaces, and straight, regular outlines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration. (arXiv:2201.12716v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12716">
<div class="article-summary-box-inner">
<span><p>Promising results have been achieved recently in category-level manipulation
that generalizes across object instances. Nevertheless, it often requires
expensive real-world data collection and manual specification of semantic
keypoints for each object category and task. Additionally, coarse keypoint
predictions and ignoring intermediate action sequences hinder adoption in
complex manipulation tasks beyond pick-and-place. This work proposes a novel,
category-level manipulation framework that leverages an object-centric,
category-level representation and model-free 6 DoF motion tracking. The
canonical object representation is learned solely in simulation and then used
to parse a category-level, task trajectory from a single demonstration video.
The demonstration is reprojected to a target trajectory tailored to a novel
object via the canonical representation. During execution, the manipulation
horizon is decomposed into longrange, collision-free motion and last-inch
manipulation. For the latter part, a category-level behavior cloning (CatBC)
method leverages motion tracking to perform closed-loop control. CatBC follows
the target trajectory, projected from the demonstration and anchored to a
dynamically selected category-level coordinate frame. The frame is
automatically selected along the manipulation horizon by a local attention
mechanism. This framework allows to teach different manipulation strategies by
solely providing a single demonstration, without complicated manual
programming. Extensive experiments demonstrate its efficacy in a range of
challenging industrial tasks in highprecision assembly, which involve learning
complex, long-horizon policies. The process exhibits robustness against
uncertainty due to dynamics as well as generalization across object instances
and scene configurations. The supplementary video is available at
https://www.youtube.com/watch?v=WAr8ZY3mYyw
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NIMBLE: A Non-rigid Hand Model with Bones and Muscles. (arXiv:2202.04533v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04533">
<div class="article-summary-box-inner">
<span><p>Emerging Metaverse applications demand reliable, accurate, and photorealistic
reproductions of human hands to perform sophisticated operations as if in the
physical world. While real human hand represents one of the most intricate
coordination between bones, muscle, tendon, and skin, state-of-the-art
techniques unanimously focus on modeling only the skeleton of the hand. In this
paper, we present NIMBLE, a novel parametric hand model that includes the
missing key components, bringing 3D hand model to a new level of realism. We
first annotate muscles, bones and skins on the recent Magnetic Resonance
Imaging hand (MRI-Hand) dataset and then register a volumetric template hand
onto individual poses and subjects within the dataset. NIMBLE consists of 20
bones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin
mesh. Via iterative shape registration and parameter learning, it further
produces shape blend shapes, pose blend shapes, and a joint regressor. We
demonstrate applying NIMBLE to modeling, rendering, and visual inference tasks.
By enforcing the inner bones and muscles to match anatomic and kinematic rules,
NIMBLE can animate 3D hands to new poses at unprecedented realism. To model the
appearance of skin, we further construct a photometric HandStage to acquire
high-quality textures and normal maps to model wrinkles and palm print.
Finally, NIMBLE also benefits learning-based hand pose and shape estimation by
either synthesizing rich data or acting directly as a differentiable layer in
the inference network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Beneath the Ink: Synthetic Data and Tattoo Removal with Application to Face Recognition. (arXiv:2202.05297v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05297">
<div class="article-summary-box-inner">
<span><p>Systems that analyse faces have seen significant improvements in recent years
and are today used in numerous application scenarios. However, these systems
have been found to be negatively affected by facial alterations such as
tattoos. To better understand and mitigate the effect of facial tattoos in
facial analysis systems, large datasets of images of individuals with and
without tattoos are needed. To this end, we propose a generator for
automatically adding realistic tattoos to facial images. Moreover, we
demonstrate the feasibility of the generation by using a deep learning-based
model for removing tattoos from face images. The experimental results show that
it is possible to remove facial tattoos from real images without degrading the
quality of the image. Additionally, we show that it is possible to improve face
recognition accuracy by using the proposed deep learning-based tattoo removal
before extracting and comparing facial features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADAM Challenge: Detecting Age-related Macular Degeneration from Fundus Images. (arXiv:2202.07983v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07983">
<div class="article-summary-box-inner">
<span><p>Age-related macular degeneration (AMD) is the leading cause of visual
impairment among elderly in the world. Early detection of AMD is of great
importance, as the vision loss caused by this disease is irreversible and
permanent. Color fundus photography is the most cost-effective imaging modality
to screen for retinal disorders. Cutting edge deep learning based algorithms
have been recently developed for automatically detecting AMD from fundus
images. However, there are still lack of a comprehensive annotated dataset and
standard evaluation benchmarks. To deal with this issue, we set up the
Automatic Detection challenge on Age-related Macular degeneration (ADAM), which
was held as a satellite event of the ISBI 2020 conference. The ADAM challenge
consisted of four tasks which cover the main aspects of detecting and
characterizing AMD from fundus images, including detection of AMD, detection
and segmentation of optic disc, localization of fovea, and detection and
segmentation of lesions. As part of the challenge, we have released a
comprehensive dataset of 1200 fundus images with AMD diagnostic labels,
pixel-wise segmentation masks for both optic disc and AMD-related lesions
(drusen, exudates, hemorrhages and scars, among others), as well as the
coordinates corresponding to the location of the macular fovea. A uniform
evaluation framework has been built to make a fair comparison of different
models using this dataset. During the challenge, 610 results were submitted for
online evaluation, with 11 teams finally participating in the onsite challenge.
This paper introduces the challenge, the dataset and the evaluation methods, as
well as summarizes the participating methods and analyzes their results for
each task. In particular, we observed that the ensembling strategy and the
incorporation of clinical domain knowledge were the key to improve the
performance of the deep learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferring Adversarial Robustness Through Robust Representation Matching. (arXiv:2202.09994v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09994">
<div class="article-summary-box-inner">
<span><p>With the widespread use of machine learning, concerns over its security and
reliability have become prevalent. As such, many have developed defenses to
harden neural networks against adversarial examples, imperceptibly perturbed
inputs that are reliably misclassified. Adversarial training in which
adversarial examples are generated and used during training is one of the few
known defenses able to reliably withstand such attacks against neural networks.
However, adversarial training imposes a significant training overhead and
scales poorly with model complexity and input dimension. In this paper, we
propose Robust Representation Matching (RRM), a low-cost method to transfer the
robustness of an adversarially trained model to a new model being trained for
the same task irrespective of architectural differences. Inspired by
student-teacher learning, our method introduces a novel training loss that
encourages the student to learn the teacher's robust representations. Compared
to prior works, RRM is superior with respect to both model performance and
adversarial training time. On CIFAR-10, RRM trains a robust model $\sim
1.8\times$ faster than the state-of-the-art. Furthermore, RRM remains effective
on higher-dimensional datasets. On Restricted-ImageNet, RRM trains a ResNet50
model $\sim 18\times$ faster than standard adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polarity Sampling: Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values. (arXiv:2203.01993v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01993">
<div class="article-summary-box-inner">
<span><p>We present Polarity Sampling, a theoretically justified plug-and-play method
for controlling the generation quality and diversity of pre-trained deep
generative networks DGNs). Leveraging the fact that DGNs are, or can be
approximated by, continuous piecewise affine splines, we derive the analytical
DGN output space distribution as a function of the product of the DGN's
Jacobian singular values raised to a power $\rho$. We dub $\rho$ the
$\textbf{polarity}$ parameter and prove that $\rho$ focuses the DGN sampling on
the modes ($\rho &lt; 0$) or anti-modes ($\rho &gt; 0$) of the DGN output-space
distribution. We demonstrate that nonzero polarity values achieve a better
precision-recall (quality-diversity) Pareto frontier than standard methods,
such as truncation, for a number of state-of-the-art DGNs. We also present
quantitative and qualitative results on the improvement of overall generation
quality (e.g., in terms of the Frechet Inception Distance) for a number of
state-of-the-art DGNs, including StyleGAN3, BigGAN-deep, NVAE, for different
conditional and unconditional image generation tasks. In particular, Polarity
Sampling redefines the state-of-the-art for StyleGAN2 on the FFHQ Dataset to
FID 2.57, StyleGAN2 on the LSUN Car Dataset to FID 2.27 and StyleGAN3 on the
AFHQv2 Dataset to FID 3.95. Demo: bit.ly/polarity-samp
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AEGNN: Asynchronous Event-based Graph Neural Networks. (arXiv:2203.17149v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17149">
<div class="article-summary-box-inner">
<span><p>The best performing learning algorithms devised for event cameras work by
first converting events into dense representations that are then processed
using standard CNNs. However, these steps discard both the sparsity and high
temporal resolution of events, leading to high computational burden and
latency. For this reason, recent works have adopted Graph Neural Networks
(GNNs), which process events as "static" spatio-temporal graphs, which are
inherently "sparse". We take this trend one step further by introducing
Asynchronous, Event-based Graph Neural Networks (AEGNNs), a novel
event-processing paradigm that generalizes standard GNNs to process events as
"evolving" spatio-temporal graphs. AEGNNs follow efficient update rules that
restrict recomputation of network activations only to the nodes affected by
each new event, thereby significantly reducing both computation and latency for
event-by-event processing. AEGNNs are easily trained on synchronous inputs and
can be converted to efficient, "asynchronous" networks at test time. We
thoroughly validate our method on object classification and detection tasks,
where we show an up to a 200-fold reduction in computational complexity
(FLOPs), with similar or even better performance than state-of-the-art
asynchronous methods. This reduction in computation directly translates to an
8-fold reduction in computational latency when compared to standard GNNs, which
opens the door to low-latency event-based processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SymForce: Symbolic Computation and Code Generation for Robotics. (arXiv:2204.07889v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07889">
<div class="article-summary-box-inner">
<span><p>We present SymForce, a library for fast symbolic computation, code
generation, and nonlinear optimization for robotics applications like computer
vision, motion planning, and controls. SymForce combines the development speed
and flexibility of symbolic math with the performance of autogenerated, highly
optimized code in C++ or any target runtime language. SymForce provides
geometry and camera types, Lie group operations, and branchless singularity
handling for creating and analyzing complex symbolic expressions in Python,
built on top of SymPy. Generated functions can be integrated as factors into
our tangent-space nonlinear optimizer, which is highly optimized for real-time
production use. We introduce novel methods to automatically compute
tangent-space Jacobians, eliminating the need for bug-prone handwritten
derivatives. This workflow enables faster runtime code, faster development
time, and fewer lines of handwritten code versus the state-of-the-art. Our
experiments demonstrate that our approach can yield order of magnitude speedups
on computational tasks core to robotics. Code is available at
https://github.com/symforce-org/symforce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Working memory inspired hierarchical video decomposition with transformative representations. (arXiv:2204.10105v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10105">
<div class="article-summary-box-inner">
<span><p>Video decomposition is very important to extract moving foreground objects
from complex backgrounds in computer vision, machine learning, and medical
imaging, e.g., extracting moving contrast-filled vessels from the complex and
noisy backgrounds of X-ray coronary angiography (XCA). However, the challenges
caused by dynamic backgrounds, overlapping heterogeneous environments and
complex noises still exist in video decomposition. To solve these problems,
this study is the first to introduce a flexible visual working memory model in
video decomposition tasks to provide interpretable and high-performance
hierarchical deep architecture, integrating the transformative representations
between sensory and control layers from the perspective of visual and cognitive
neuroscience. Specifically, robust PCA unrolling networks acting as a
structure-regularized sensor layer decompose XCA into sparse/low-rank
structured representations to separate moving contrast-filled vessels from
noisy and complex backgrounds. Then, patch recurrent convolutional LSTM
networks with a backprojection module embody unstructured random
representations of the control layer in working memory, recurrently projecting
spatiotemporally decomposed nonlocal patches into orthogonal subspaces for
heterogeneous vessel retrieval and interference suppression. This video
decomposition deep architecture effectively restores the heterogeneous profiles
of intensity and the geometries of moving objects against the complex
background interferences. Experiments show that the proposed method
significantly outperforms state-of-the-art methods in accurate moving
contrast-filled vessel extraction with excellent flexibility and computational
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Poly-CAM: High resolution class activation map for convolutional neural networks. (arXiv:2204.13359v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13359">
<div class="article-summary-box-inner">
<span><p>The need for Explainable AI is increasing with the development of deep
learning. The saliency maps derived from convolutional neural networks
generally fail in localizing with accuracy the image features justifying the
network prediction. This is because those maps are either low-resolution as for
CAM [Zhou et al., 2016], or smooth as for perturbation-based methods [Zeiler
and Fergus, 2014], or do correspond to a large number of widespread peaky spots
as for gradient-based approaches [Sundararajan et al., 2017, Smilkov et al.,
2017]. In contrast, our work proposes to combine the information from earlier
network layers with the one from later layers to produce a high resolution
Class Activation Map that is competitive with the previous art in term of
insertion-deletion faithfulness metrics, while outperforming it in term of
precision of class-specific features localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual networks based 3D Multi-Person Pose Estimation from Monocular Video. (arXiv:2205.00748v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00748">
<div class="article-summary-box-inner">
<span><p>Monocular 3D human pose estimation has made progress in recent years. Most of
the methods focus on single persons, which estimate the poses in the
person-centric coordinates, i.e., the coordinates based on the center of the
target person. Hence, these methods are inapplicable for multi-person 3D pose
estimation, where the absolute coordinates (e.g., the camera coordinates) are
required. Moreover, multi-person pose estimation is more challenging than
single pose estimation, due to inter-person occlusion and close human
interactions. Existing top-down multi-person methods rely on human detection
(i.e., top-down approach), and thus suffer from the detection errors and cannot
produce reliable pose estimation in multi-person scenes. Meanwhile, existing
bottom-up methods that do not use human detection are not affected by detection
errors, but since they process all persons in a scene at once, they are prone
to errors, particularly for persons in small scales. To address all these
challenges, we propose the integration of top-down and bottom-up approaches to
exploit their strengths. Our top-down network estimates human joints from all
persons instead of one in an image patch, making it robust to possible
erroneous bounding boxes. Our bottom-up network incorporates human-detection
based normalized heatmaps, allowing the network to be more robust in handling
scale variations. Finally, the estimated 3D poses from the top-down and
bottom-up networks are fed into our integration network for final 3D poses. To
address the common gaps between training and testing data, we do optimization
during the test time, by refining the estimated 3D human poses using high-order
temporal constraint, re-projection loss, and bone length regularizations. Our
evaluations demonstrate the effectiveness of the proposed method. Code and
models are available: https://github.com/3dpose/3D-Multi-Person-Pose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation. (arXiv:2205.01133v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01133">
<div class="article-summary-box-inner">
<span><p>Multi-modal Machine Translation (MMT) enables the use of visual information
to enhance the quality of translations. The visual information can serve as a
valuable piece of context information to decrease the ambiguity of input
sentences. Despite the increasing popularity of such a technique, good and
sizeable datasets are scarce, limiting the full extent of their potential.
Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It
is estimated that about 100 to 150 million people speak the language, with more
than 80 million indigenous speakers. This is more than any of the other Chadic
languages. Despite a large number of speakers, the Hausa language is considered
low-resource in natural language processing (NLP). This is due to the absence
of sufficient resources to implement most NLP tasks. While some datasets exist,
they are either scarce, machine-generated, or in the religious domain.
Therefore, there is a need to create training and evaluation data for
implementing machine learning tasks and bridging the research gap in the
language. This work presents the Hausa Visual Genome (HaVG), a dataset that
contains the description of an image or a section within the image in Hausa and
its equivalent in English. To prepare the dataset, we started by translating
the English description of the images in the Hindi Visual Genome (HVG) into
Hausa automatically. Afterward, the synthetic Hausa data was carefully
post-edited considering the respective images. The dataset comprises 32,923
images and their descriptions that are divided into training, development,
test, and challenge test set. The Hausa Visual Genome is the first dataset of
its kind and can be used for Hausa-English machine translation, multi-modal
research, and image description, among various other natural language
processing and generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Loose-Fitting Garment Deformations Using Bone-Driven Motion Networks. (arXiv:2205.01355v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01355">
<div class="article-summary-box-inner">
<span><p>We present a learning algorithm that uses bone-driven motion networks to
predict the deformation of loose-fitting garment meshes at interactive rates.
Given a garment, we generate a simulation database and extract virtual bones
from simulated mesh sequences using skin decomposition. At runtime, we
separately compute low- and high-frequency deformations in a sequential manner.
The low-frequency deformations are predicted by transferring body motions to
virtual bones' motions, and the high-frequency deformations are estimated
leveraging the global information of virtual bones' motions and local
information extracted from low-frequency meshes. In addition, our method can
estimate garment deformations caused by variations of the simulation parameters
(e.g., fabric's bending stiffness) using an RBF kernel ensembling trained
networks for different sets of simulation parameters. Through extensive
comparisons, we show that our method outperforms state-of-the-art methods in
terms of prediction accuracy of mesh deformations by about 20% in RMSE and 10%
in Hausdorff distance and STED. The code and data are available at
https://github.com/non-void/VirtualBones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuroevolutionary Multi-objective approaches to Trajectory Prediction in Autonomous Vehicles. (arXiv:2205.02105v3 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02105">
<div class="article-summary-box-inner">
<span><p>The incentive for using Evolutionary Algorithms (EAs) for the automated
optimization and training of deep neural networks (DNNs), a process referred to
as neuroevolution, has gained momentum in recent years. The configuration and
training of these networks can be posed as optimization problems. Indeed, most
of the recent works on neuroevolution have focused their attention on
single-objective optimization. Moreover, from the little research that has been
done at the intersection of neuroevolution and evolutionary multi-objective
optimization (EMO), all the research that has been carried out has focused
predominantly on the use of one type of DNN: convolutional neural networks
(CNNs), using well-established standard benchmark problems such as MNIST. In
this work, we make a leap in the understanding of these two areas
(neuroevolution and EMO), regarded in this work as neuroevolutionary
multi-objective, by using and studying a rich DNN composed of a CNN and
Long-short Term Memory network. Moreover, we use a robust and challenging
vehicle trajectory prediction problem. By using the well-known Non-dominated
Sorting Genetic Algorithm-II, we study the effects of five different
objectives, tested in categories of three, allowing us to show how these
objectives have either a positive or detrimental effect in neuroevolution for
trajectory prediction in autonomous vehicles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Octree Graph Networks for Learning Adaptive Volumetric Shape Representations. (arXiv:2205.02825v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02825">
<div class="article-summary-box-inner">
<span><p>We present an adaptive deep representation of volumetric fields of 3D shapes
and an efficient approach to learn this deep representation for high-quality 3D
shape reconstruction and auto-encoding. Our method encodes the volumetric field
of a 3D shape with an adaptive feature volume organized by an octree and
applies a compact multilayer perceptron network for mapping the features to the
field value at each 3D position. An encoder-decoder network is designed to
learn the adaptive feature volume based on the graph convolutions over the dual
graph of octree nodes. The core of our network is a new graph convolution
operator defined over a regular grid of features fused from irregular
neighboring octree nodes at different levels, which not only reduces the
computational and memory cost of the convolutions over irregular neighboring
octree nodes, but also improves the performance of feature learning. Our method
effectively encodes shape details, enables fast 3D shape reconstruction, and
exhibits good generality for modeling 3D shapes out of training categories. We
evaluate our method on a set of reconstruction tasks of 3D shapes and scenes
and validate its superiority over other existing approaches. Our code, data,
and trained models are available at https://wang-ps.github.io/dualocnn.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-09 23:08:30.796519639 UTC">2022-05-09 23:08:30 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>