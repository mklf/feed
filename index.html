<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-11T01:30:00Z">05-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Behind the Mask: Demographic bias in name detection for PII masking. (arXiv:2205.04505v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04505">
<div class="article-summary-box-inner">
<span><p>Many datasets contain personally identifiable information, or PII, which
poses privacy risks to individuals. PII masking is commonly used to redact
personal information such as names, addresses, and phone numbers from text
data. Most modern PII masking pipelines involve machine learning algorithms.
However, these systems may vary in performance, such that individuals from
particular demographic groups bear a higher risk for having their personal
information exposed. In this paper, we evaluate the performance of three
off-the-shelf PII masking systems on name detection and redaction. We generate
data using names and templates from the customer service domain. We find that
an open-source RoBERTa-based system shows fewer disparities than the commercial
models we test. However, all systems demonstrate significant differences in
error rate based on demographics. In particular, the highest error rates
occurred for names associated with Black and Asian/Pacific Islander
individuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Slot Schema Induction for Task-oriented Dialog. (arXiv:2205.04515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04515">
<div class="article-summary-box-inner">
<span><p>Carefully-designed schemas describing how to collect and annotate dialog
corpora are a prerequisite towards building task-oriented dialog systems. In
practical applications, manually designing schemas can be error-prone,
laborious, iterative, and slow, especially when the schema is complicated. To
alleviate this expensive and time consuming process, we propose an unsupervised
approach for slot schema induction from unlabeled dialog corpora. Leveraging
in-domain language models and unsupervised parsing structures, our data-driven
approach extracts candidate slots without constraints, followed by
coarse-to-fine clustering to induce slot types. We compare our method against
several strong supervised baselines, and show significant performance
improvement in slot schema induction on MultiWoz and SGD datasets. We also
demonstrate the effectiveness of induced schemas on downstream applications
including dialog state tracking and response generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Song of (Dis)agreement: Evaluating the Evaluation of Explainable Artificial Intelligence in Natural Language Processing. (arXiv:2205.04559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04559">
<div class="article-summary-box-inner">
<span><p>There has been significant debate in the NLP community about whether or not
attention weights can be used as an explanation - a mechanism for interpreting
how important each input token is for a particular prediction. The validity of
"attention as explanation" has so far been evaluated by computing the rank
correlation between attention-based explanations and existing feature
attribution explanations using LSTM-based models. In our work, we (i) compare
the rank correlation between five more recent feature attribution methods and
two attention-based methods, on two types of NLP tasks, and (ii) extend this
analysis to also include transformer-based models. We find that attention-based
explanations do not correlate strongly with any recent feature attribution
methods, regardless of the model or task. Furthermore, we find that none of the
tested explanations correlate strongly with one another for the
transformer-based model, leading us to question the underlying assumption that
we should measure the validity of attention-based explanations based on how
well they correlate with existing feature attribution explanation methods.
After conducting experiments on five datasets using two different models, we
argue that the community should stop using rank correlation as an evaluation
metric for attention-based explanations. We suggest that researchers and
practitioners should instead test various explanation methods and employ a
human-in-the-loop process to determine if the explanations align with human
intuition for the particular use case at hand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Model for Reverse Dictionary and Definition Modelling. (arXiv:2205.04602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04602">
<div class="article-summary-box-inner">
<span><p>We train a dual-way neural dictionary to guess words from definitions
(reverse dictionary), and produce definitions given words (definition
modelling). Our method learns the two tasks simultaneously, and handles unknown
words via embeddings. It casts a word or a definition to the same
representation space through a shared layer, then generates the other form from
there, in a multi-task fashion. The model achieves promising automatic scores
without extra resources. Human annotators prefer the proposed model's outputs
in both reference-less and reference-based evaluation, which indicates its
practicality. Analysis suggests that multiple objectives benefit learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence-level Privacy for Document Embeddings. (arXiv:2205.04605v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04605">
<div class="article-summary-box-inner">
<span><p>User language data can contain highly sensitive personal content. As such, it
is imperative to offer users a strong and interpretable privacy guarantee when
learning from their data. In this work, we propose SentDP: pure local
differential privacy at the sentence level for a single user document. We
propose a novel technique, DeepCandidate, that combines concepts from robust
statistics and language modeling to produce high-dimensional, general-purpose
$\epsilon$-SentDP document embeddings. This guarantees that any single sentence
in a document can be substituted with any other sentence while keeping the
embedding $\epsilon$-indistinguishable. Our experiments indicate that these
private document embeddings are useful for downstream tasks like sentiment
analysis and topic classification and even outperform baseline methods with
weaker guarantees like word-level Metric DP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParaCotta: Synthetic Multilingual Paraphrase Corpora from the Most Diverse Translation Sample Pair. (arXiv:2205.04651v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04651">
<div class="article-summary-box-inner">
<span><p>We release our synthetic parallel paraphrase corpus across 17 languages:
Arabic, Catalan, Czech, German, English, Spanish, Estonian, French, Hindi,
Indonesian, Italian, Dutch, Romanian, Russian, Swedish, Vietnamese, and
Chinese. Our method relies only on monolingual data and a neural machine
translation system to generate paraphrases, hence simple to apply. We generate
multiple translation samples using beam search and choose the most lexically
diverse pair according to their sentence BLEU. We compare our generated corpus
with the \texttt{ParaBank2}. According to our evaluation, our synthetic
paraphrase pairs are semantically similar and lexically diverse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuMe: A Dataset Towards Summarizing Biomedical Mechanisms. (arXiv:2205.04652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04652">
<div class="article-summary-box-inner">
<span><p>Can language models read biomedical texts and explain the biomedical
mechanisms discussed? In this work we introduce a biomedical mechanism
summarization task. Biomedical studies often investigate the mechanisms behind
how one entity (e.g., a protein or a chemical) affects another in a biological
context. The abstracts of these publications often include a focused set of
sentences that present relevant supporting statements regarding such
relationships, associated experimental evidence, and a concluding sentence that
summarizes the mechanism underlying the relationship. We leverage this
structure and create a summarization task, where the input is a collection of
sentences and the main entities in an abstract, and the output includes the
relationship and a sentence that summarizes the mechanism. Using a small amount
of manually labeled mechanism sentences, we train a mechanism sentence
classifier to filter a large biomedical abstract collection and create a
summarization dataset with 22k instances. We also introduce conclusion sentence
generation as a pretraining task with 611k instances. We benchmark the
performance of large bio-domain language models. We find that while the
pretraining task help improves performance, the best model produces acceptable
mechanism outputs in only 32% of the instances, which shows the task presents
significant challenges in biomedical language understanding and summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdMix: A Mixed Sample Data Augmentation Method for Neural Machine Translation. (arXiv:2205.04686v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04686">
<div class="article-summary-box-inner">
<span><p>In Neural Machine Translation (NMT), data augmentation methods such as
back-translation have proven their effectiveness in improving translation
performance. In this paper, we propose a novel data augmentation approach for
NMT, which is independent of any additional training data. Our approach, AdMix,
consists of two parts: 1) introduce faint discrete noise (word replacement,
word dropping, word swapping) into the original sentence pairs to form
augmented samples; 2) generate new synthetic training data by softly mixing the
augmented samples with their original samples in training corpus. Experiments
on three translation datasets of different scales show that AdMix achieves
signifi cant improvements (1.0 to 2.7 BLEU points) over strong Transformer
baseline. When combined with other data augmentation techniques (e.g.,
back-translation), our approach can obtain further improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting. (arXiv:2205.04692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04692">
<div class="article-summary-box-inner">
<span><p>We study the knowledge extrapolation problem to embed new components (i.e.,
entities and relations) that come with emerging knowledge graphs (KGs) in the
federated setting. In this problem, a model trained on an existing KG needs to
embed an emerging KG with unseen entities and relations. To solve this problem,
we introduce the meta-learning setting, where a set of tasks are sampled on the
existing KG to mimic the link prediction task on the emerging KG. Based on
sampled tasks, we meta-train a graph neural network framework that can
construct features for unseen components based on structural information and
output embeddings for them. Experimental results show that our proposed method
can effectively embed unseen components and outperforms models that consider
inductive settings for KGs and baselines that directly use conventional KG
embedding methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective. (arXiv:2205.04733v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04733">
<div class="article-summary-box-inner">
<span><p>Neural retrievers based on dense representations combined with Approximate
Nearest Neighbors search have recently received a lot of attention, owing their
success to distillation and/or better sampling of examples for training --
while still relying on the same backbone architecture. In the meantime, sparse
representation learning fueled by traditional inverted indexing techniques has
seen a growing interest, inheriting from desirable IR priors such as explicit
lexical matching. While some architectural variants have been proposed, a
lesser effort has been put in the training of such models. In this work, we
build on SPLADE -- a sparse expansion-based retriever -- and show to which
extent it is able to benefit from the same training improvements as dense
models, by studying the effect of distillation, hard-negative mining as well as
the Pre-trained Language Model initialization. We furthermore study the link
between effectiveness and efficiency, on in-domain and zero-shot settings,
leading to state-of-the-art results in both scenarios for sufficiently
expressive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlling Extra-Textual Attributes about Dialogue Participants: A Case Study of English-to-Polish Neural Machine Translation. (arXiv:2205.04747v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04747">
<div class="article-summary-box-inner">
<span><p>Unlike English, morphologically rich languages can reveal characteristics of
speakers or their conversational partners, such as gender and number, via
pronouns, morphological endings of words and syntax. When translating from
English to such languages, a machine translation model needs to opt for a
certain interpretation of textual context, which may lead to serious
translation errors if extra-textual information is unavailable. We investigate
this challenge in the English-to-Polish language direction. We focus on the
underresearched problem of utilising external metadata in automatic translation
of TV dialogue, proposing a case study where a wide range of approaches for
controlling attributes in translation is employed in a multi-attribute
scenario. The best model achieves an improvement of +5.81 chrF++/+6.03 BLEU,
with other models achieving competitive performance. We additionally contribute
a novel attribute-annotated dataset of Polish TV dialogue and a morphological
analysis script used to evaluate attribute control in models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Importance of Context in Very Low Resource Language Modeling. (arXiv:2205.04810v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04810">
<div class="article-summary-box-inner">
<span><p>This paper investigates very low resource language model pretraining, when
less than 100 thousand sentences are available. We find that, in very low
resource scenarios, statistical n-gram language models outperform
state-of-the-art neural models. Our experiments show that this is mainly due to
the focus of the former on a local context. As such, we introduce three methods
to improve a neural model's performance in the low-resource setting, finding
that limiting the model's self-attention is the most effective one, improving
on downstream tasks such as NLI and POS tagging by up to 5% for the languages
we test on: English, Hindi, and Turkish.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the prosody GAP: Genetic Algorithm with People to efficiently sample emotional prosody. (arXiv:2205.04820v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04820">
<div class="article-summary-box-inner">
<span><p>The human voice effectively communicates a range of emotions with nuanced
variations in acoustics. Existing emotional speech corpora are limited in that
they are either (a) highly curated to induce specific emotions with predefined
categories that may not capture the full extent of emotional experiences, or
(b) entangled in their semantic and prosodic cues, limiting the ability to
study these cues separately. To overcome this challenge, we propose a new
approach called 'Genetic Algorithm with People' (GAP), which integrates human
decision and production into a genetic algorithm. In our design, we allow
creators and raters to jointly optimize the emotional prosody over generations.
We demonstrate that GAP can efficiently sample from the emotional speech space
and capture a broad range of emotions, and show comparable results to
state-of-the-art emotional speech corpora. GAP is language-independent and
supports large crowd-sourcing, thus can support future large-scale
cross-cultural research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALLSH: Active Learning Guided by Local Sensitivity and Hardness. (arXiv:2205.04980v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04980">
<div class="article-summary-box-inner">
<span><p>Active learning, which effectively collects informative unlabeled data for
annotation, reduces the demand for labeled data. In this work, we propose to
retrieve unlabeled samples with a local sensitivity and hardness-aware
acquisition function. The proposed method generates data copies through local
perturbations and selects data points whose predictive likelihoods diverge the
most from their copies. We further empower our acquisition function by
injecting the select-worst case perturbation. Our method achieves consistent
gains over the commonly used active learning strategies in various
classification tasks. Furthermore, we observe consistent improvements over the
baselines on the study of prompt selection in prompt-based few-shot learning.
These experiments demonstrate that our acquisition guided by local sensitivity
and hardness can be effective and beneficial for many NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Answer Visual Questions from Web Videos. (arXiv:2205.05019v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05019">
<div class="article-summary-box-inner">
<span><p>Recent methods for visual question answering rely on large-scale annotated
datasets. Manual annotation of questions and answers for videos, however, is
tedious, expensive and prevents scalability. In this work, we propose to avoid
manual annotation and generate a large-scale training dataset for video
question answering making use of automatic cross-modal supervision. We leverage
a question generation transformer trained on text data and use it to generate
question-answer pairs from transcribed video narrations. Given narrated videos,
we then automatically generate the HowToVQA69M dataset with 69M
video-question-answer triplets. To handle the open vocabulary of diverse
answers in this dataset, we propose a training procedure based on a contrastive
loss between a video-question multi-modal transformer and an answer
transformer. We introduce the zero-shot VideoQA task and the VideoQA feature
probe evaluation setting and show excellent results, in particular for rare
answers. Furthermore, our method achieves competitive results on MSRVTT-QA,
ActivityNet-QA, MSVD-QA and How2QA datasets. We also show that our VideoQA
dataset generation approach generalizes to another source of web video and text
data. We use our method to generate the \webdataname{} dataset from the WebVid
dataset, i.e., videos with alt-text annotations, and show its benefits for
training VideoQA models. Finally, for a detailed evaluation we introduce
\smalldatasetname{}, a new VideoQA dataset with reduced language bias and
high-quality manual annotations. Code, datasets and trained models are
available at https://antoyang.github.io/just-ask.html
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">White-box Testing of NLP models with Mask Neuron Coverage. (arXiv:2205.05050v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05050">
<div class="article-summary-box-inner">
<span><p>Recent literature has seen growing interest in using black-box strategies
like CheckList for testing the behavior of NLP models. Research on white-box
testing has developed a number of methods for evaluating how thoroughly the
internal behavior of deep models is tested, but they are not applicable to NLP
models. We propose a set of white-box testing methods that are customized for
transformer-based NLP models. These include Mask Neuron Coverage (MNCOVER) that
measures how thoroughly the attention layers in models are exercised during
testing. We show that MNCOVER can refine testing suites generated by CheckList
by substantially reduce them in size, for more than 60\% on average, while
retaining failing tests -- thereby concentrating the fault detection power of
the test suite. Further we show how MNCOVER can be used to guide CheckList
input generation, evaluate alternative NLP testing methods, and drive data
augmentation to improve accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers. (arXiv:2205.05055v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05055">
<div class="article-summary-box-inner">
<span><p>Large transformer-based language models are able to perform few-shot learning
(also known as in-context learning), without having been explicitly trained for
it. We hypothesized that specific distributional properties of natural language
might drive this emergent phenomenon, as these characteristics might lead to a
kind of interpolation between few-shot meta-training (designed to elicit rapid
few-shot learning) and standard supervised training (designed to elicit gradual
in-weights learning). We also hypothesized that these distributional properties
could lead to emergent few-shot learning in domains outside of language.
Inspired by this idea, we ran a series of experiments on a standard image-based
few-shot dataset. We discovered that a number of data properties did indeed
promote the emergence of few-shot learning in transformer models. All of these
properties are present in natural language -- burstiness, long-tailedness, and
many-to-one or one-to-many label mappings. The data influenced whether models
were biased towards either few-shot learning vs. memorizing information in
their weights; models could generally perform well at only one or the other.
However, we discovered that an additional distributional property could allow
the two capabilities to co-exist in the same model -- a skewed, Zipfian
distribution over classes -- which occurs in language as well. Notably,
training data that could elicit few-shot learning in transformers were unable
to elicit few-shot learning in recurrent models. In sum, we find that few-shot
learning emerges only from applying the right architecture to the right data
distribution; neither component is sufficient on its own.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Climate Awareness in NLP Research. (arXiv:2205.05071v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05071">
<div class="article-summary-box-inner">
<span><p>The climate impact of AI, and NLP research in particular, has become a
serious issue given the enormous amount of energy that is increasingly being
used for training and running computational models. Consequently, increasing
focus is placed on efficient NLP. However, this important initiative lacks
simple guidelines that would allow for systematic climate reporting of NLP
research. We argue that this deficiency is one of the reasons why very few
publications in NLP report key figures that would allow a more thorough
examination of environmental impact. As a remedy, we propose a climate
performance model card with the primary purpose of being practically usable
with only limited information about experiments and the underlying computer
hardware. We describe why this step is essential to increase awareness about
the environmental impact of NLP research and, thereby, paving the way for more
thorough discussions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrated Node Encoder for Labelled Textual Networks. (arXiv:2005.11694v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.11694">
<div class="article-summary-box-inner">
<span><p>Voluminous works have been implemented to exploit content-enhanced network
embedding models, with little focus on the labelled information of nodes.
Although TriDNR leverages node labels by treating them as node attributes, it
fails to enrich unlabelled node vectors with the labelled information, which
leads to the weaker classification result on the test set in comparison to
existing unsupervised textual network embedding models. In this study, we
design an integrated node encoder (INE) for textual networks which is jointly
trained on the structure-based and label-based objectives. As a result, the
node encoder preserves the integrated knowledge of not only the network text
and structure, but also the labelled information. Furthermore, INE allows the
creation of label-enhanced vectors for unlabelled nodes by entering their node
contents. Our node embedding achieves state-of-the-art performances in the
classification task on two public citation networks, namely Cora and DBLP,
pushing benchmarks up by 10.0\% and 12.1\%, respectively, with the 70\%
training ratio. Additionally, a feasible solution that generalizes our model
from textual networks to a broader range of networks is proposed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla. (arXiv:2101.00204v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00204">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce BanglaBERT, a BERT-based Natural Language
Understanding (NLU) model pretrained in Bangla, a widely spoken yet
low-resource language in the NLP literature. To pretrain BanglaBERT, we collect
27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular
Bangla sites. We introduce two downstream task datasets on natural language
inference and question answering and benchmark on four diverse NLU tasks
covering text classification, sequence labeling, and span prediction. In the
process, we bring them under the first-ever Bangla Language Understanding
Benchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming
multilingual and monolingual models. We are making the models, datasets, and a
leaderboard publicly available at https://github.com/csebuetnlp/banglabert to
advance Bangla NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MirrorAlign: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning. (arXiv:2102.04009v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04009">
<div class="article-summary-box-inner">
<span><p>Word alignment is essential for the downstream cross-lingual language
understanding and generation tasks. Recently, the performance of the neural
word alignment models has exceeded that of statistical models. However, they
heavily rely on sophisticated translation models. In this study, we propose a
super lightweight unsupervised word alignment model named MirrorAlign, in which
bidirectional symmetric attention trained with a contrastive learning objective
is introduced, and an agreement loss is employed to bind the attention maps,
such that the alignments follow mirror-like symmetry hypothesis. Experimental
results on several public benchmarks demonstrate that our model achieves
competitive, if not better, performance compared to the state of the art in
word alignment while significantly reducing the training and decoding time on
average. Further ablation analysis and case studies show the superiority of our
proposed MirrorAlign. Notably, we recognize our model as a pioneer attempt to
unify bilingual word embedding and word alignments. Encouragingly, our approach
achieves {16.4X speedup} against GIZA++, and {50X parameter compression}
compared with the Transformer-based alignment methods. We release our code to
facilitate the community: https://github.com/moore3930/MirrorAlign.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Restoring Hebrew Diacritics Without a Dictionary. (arXiv:2105.05209v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05209">
<div class="article-summary-box-inner">
<span><p>We demonstrate that it is feasible to diacritize Hebrew script without any
human-curated resources other than plain diacritized text. We present NAKDIMON,
a two-layer character level LSTM, that performs on par with much more
complicated curation-dependent systems, across a diverse array of modern Hebrew
sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Abusive Albanian. (arXiv:2107.13592v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13592">
<div class="article-summary-box-inner">
<span><p>The ever growing usage of social media in the recent years has had a direct
impact on the increased presence of hate speech and offensive speech in online
platforms. Research on effective detection of such content has mainly focused
on English and a few other widespread languages, while the leftover majority
fail to have the same work put into them and thus cannot benefit from the
steady advancements made in the field. In this paper we present \textsc{Shaj},
an annotated Albanian dataset for hate speech and offensive speech that has
been constructed from user-generated content on various social media platforms.
Its annotation follows the hierarchical schema introduced in OffensEval. The
dataset is tested using three different classification models, the best of
which achieves an F1 score of 0.77 for the identification of offensive
language, 0.64 F1 score for the automatic categorization of offensive types and
lastly, 0.52 F1 score for the offensive language target identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WiC = TSV = WSD: On the Equivalence of Three Semantic Tasks. (arXiv:2107.14352v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14352">
<div class="article-summary-box-inner">
<span><p>The Word-in-Context (WiC) task has attracted considerable attention in the
NLP community, as demonstrated by the popularity of the recent MCL-WiC SemEval
shared task. Systems and lexical resources from word sense disambiguation (WSD)
are often used for the WiC task and WiC dataset construction. In this paper, we
establish the exact relationship between WiC and WSD, as well as the related
task of target sense verification (TSV). Building upon a novel hypothesis on
the equivalence of sense and meaning distinctions, we demonstrate through the
application of tools from theoretical computer science that these three
semantic classification problems can be pairwise reduced to each other, and
therefore are equivalent. The results of experiments that involve systems and
datasets for both WiC and WSD provide strong empirical evidence that our
problem reductions work in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Linking and Discovery via Arborescence-based Supervised Clustering. (arXiv:2109.01242v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01242">
<div class="article-summary-box-inner">
<span><p>Previous work has shown promising results in performing entity linking by
measuring not only the affinities between mentions and entities but also those
amongst mentions. In this paper, we present novel training and inference
procedures that fully utilize mention-to-mention affinities by building minimum
arborescences (i.e., directed spanning trees) over mentions and entities across
documents in order to make linking decisions. We also show that this method
gracefully extends to entity discovery, enabling the clustering of mentions
that do not have an associated entity in the knowledge base. We evaluate our
approach on the Zero-Shot Entity Linking dataset and MedMentions, the largest
publicly available biomedical dataset, and show significant improvements in
performance for both entity linking and discovery compared to identically
parameterized models. We further show significant efficiency improvements with
only a small loss in accuracy over previous work, which use more
computationally expensive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Free Prosody-Aware Generative Spoken Language Modeling. (arXiv:2109.03264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03264">
<div class="article-summary-box-inner">
<span><p>Speech pre-training has primarily demonstrated efficacy on classification
tasks, while its capability of generating novel speech, similar to how GPT-2
can generate coherent paragraphs, has barely been explored. Generative Spoken
Language Modeling (GSLM) \cite{Lakhotia2021} is the only prior work addressing
the generative aspects of speech pre-training, which replaces text with
discovered phone-like units for language modeling and shows the ability to
generate meaningful novel sentences. Unfortunately, despite eliminating the
need of text, the units used in GSLM discard most of the prosodic information.
Hence, GSLM fails to leverage prosody for better comprehension, and does not
generate expressive speech. In this work, we present a prosody-aware generative
spoken language model (pGSLM). It is composed of a multi-stream transformer
language model (MS-TLM) of speech, represented as discovered unit and prosodic
feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to
waveforms. We devise a series of metrics for prosody modeling and generation,
and re-use metrics from GSLM for content modeling. Experimental results show
that the pGSLM can utilize prosody to improve both prosody and content
modeling, and also generate natural, meaningful, and coherent speech given a
spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.
Codes and models are available at
https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrollsWithOpinion: A Dataset for Predicting Domain-specific Opinion Manipulation in Troll Memes. (arXiv:2109.03571v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03571">
<div class="article-summary-box-inner">
<span><p>Research into the classification of Image with Text (IWT) troll memes has
recently become popular. Since the online community utilizes the refuge of
memes to express themselves, there is an abundance of data in the form of
memes. These memes have the potential to demean, harras, or bully targeted
individuals. Moreover, the targeted individual could fall prey to opinion
manipulation. To comprehend the use of memes in opinion manipulation, we define
three specific domains (product, political or others) which we classify into
troll or not-troll, with or without opinion manipulation. To enable this
analysis, we enhanced an existing dataset by annotating the data with our
defined classes, resulting in a dataset of 8,881 IWT or multimodal memes in the
English language (TrollsWithOpinion dataset). We perform baseline experiments
on the annotated dataset, and our result shows that existing state-of-the-art
techniques could only reach a weighted-average F1-score of 0.37. This shows the
need for a development of a specific technique to deal with multimodal troll
memes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04947">
<div class="article-summary-box-inner">
<span><p>Large-scale, pre-trained language models (LMs) have achieved human-level
performance on a breadth of language understanding tasks. However, evaluations
only based on end task performance shed little light on machines' true ability
in language understanding and reasoning. In this paper, we highlight the
importance of evaluating the underlying reasoning process in addition to end
performance. Toward this goal, we introduce Tiered Reasoning for Intuitive
Physics (TRIP), a novel commonsense reasoning dataset with dense annotations
that enable multi-tiered evaluation of machines' reasoning process. Our
empirical results show that while large LMs can achieve high end performance,
they struggle to support their predictions with valid supporting evidence. The
TRIP dataset and our baseline results will motivate verifiable evaluation of
commonsense reasoning and facilitate future research toward developing better
language understanding and reasoning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Control Prefixes for Parameter-Efficient Text Generation. (arXiv:2110.08329v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08329">
<div class="article-summary-box-inner">
<span><p>Prefix-tuning is a powerful lightweight technique for adapting a large
pre-trained language model to a downstream application. However, it uses the
same dataset-level tuned prompt for all examples in the dataset. We extend this
idea and propose a dynamic method, Control Prefixes, which allows for the
inclusion of conditional input-dependent information, combining the benefits of
prompt tuning and controlled generation. The method incorporates
attribute-level learnable representations into different layers of a
pre-trained transformer, allowing for the generated text to be guided in a
particular direction. We provide a systematic evaluation of the technique and
apply it to five datasets from the GEM benchmark for natural language
generation (NLG). Although the aim is to develop a parameter-efficient model,
we show Control Prefixes can even outperform full fine-tuning methods. We
present state-of-the-art results on several data-to-text datasets, including
WebNLG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hey AI, Can You Solve Complex Tasks by Talking to Agents?. (arXiv:2110.08542v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08542">
<div class="article-summary-box-inner">
<span><p>Training giant models from scratch for each complex task is resource- and
data-inefficient. To help develop models that can leverage existing systems, we
propose a new challenge: Learning to solve complex tasks by communicating with
existing agents (or models) in natural language. We design a synthetic
benchmark, CommaQA, with three complex reasoning tasks (explicit, implicit,
numeric) designed to be solved by communicating with existing QA agents. For
instance, using text and table QA agents to answer questions such as "Who had
the longest javelin throw from USA?". We show that black-box models struggle to
learn this task from scratch (accuracy under 50\%) even with access to each
agent's knowledge and gold facts supervision. In contrast, models that learn to
communicate with agents outperform black-box models, reaching scores of 100\%
when given gold decomposition supervision. However, we show that the challenge
of learning to solve complex tasks by communicating with existing agents
\emph{without relying on any auxiliary supervision or data} still remains
highly elusive. We release CommaQA, along with a compositional generalization
test split, to advance research in this direction. Dataset and Code available
at https://github.com/allenai/commaqa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Cross-Utterance Language Modeling for Conversational Speech Recognition. (arXiv:2111.03333v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03333">
<div class="article-summary-box-inner">
<span><p>Conversational speech normally is embodied with loose syntactic structures at
the utterance level but simultaneously exhibits topical coherence relations
across consecutive utterances. Prior work has shown that capturing longer
context information with a recurrent neural network or long short-term memory
language model (LM) may suffer from the recent bias while excluding the
long-range context. In order to capture the long-term semantic interactions
among words and across utterances, we put forward disparate conversation
history fusion methods for language modeling in automatic speech recognition
(ASR) of conversational speech. Furthermore, a novel audio-fusion mechanism is
introduced, which manages to fuse and utilize the acoustic embeddings of a
current utterance and the semantic content of its corresponding conversation
history in a cooperative way. To flesh out our ideas, we frame the ASR N-best
hypothesis rescoring task as a prediction problem, leveraging BERT, an iconic
pre-trained LM, as the ingredient vehicle to facilitate selection of the oracle
hypothesis from a given N-best hypothesis list. Empirical experiments conducted
on the AMI benchmark dataset seem to demonstrate the feasibility and efficacy
of our methods in relation to some current top-of-line methods. The proposed
methods not only achieve significant inference time reduction but also improve
the ASR performance for conversational speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Jargon: Combining Extraction and Generation for Definition Modeling. (arXiv:2111.07267v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07267">
<div class="article-summary-box-inner">
<span><p>Can machines know what twin prime is? From the composition of this phrase,
machines may guess twin prime is a certain kind of prime, but it is still
difficult to deduce exactly what twin stands for without additional knowledge.
Here, twin prime is a jargon - a specialized term used by experts in a
particular field. Explaining jargon is challenging since it usually requires
domain knowledge to understand. Recently, there is an increasing interest in
extracting and generating definitions of words automatically. However, existing
approaches, either extraction or generation, perform poorly on jargon. In this
paper, we propose to combine extraction and generation for jargon definition
modeling: first extract self- and correlative definitional information of
target jargon from the Web and then generate the final definitions by
incorporating the extracted definitional information. Our framework is
remarkably simple but effective: experiments demonstrate our method can
generate high-quality definitions for jargon and outperform state-of-the-art
models significantly, e.g., BLEU score from 8.76 to 22.66 and human-annotated
score from 2.34 to 4.04.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection. (arXiv:2111.07997v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07997">
<div class="article-summary-box-inner">
<span><p>The perceived toxicity of language can vary based on someone's identity and
beliefs, but this variation is often ignored when collecting toxic language
datasets, resulting in dataset and model biases. We seek to understand the who,
why, and what behind biases in toxicity annotations. In two online studies with
demographically and politically diverse participants, we investigate the effect
of annotator identities (who) and beliefs (why), drawing from social psychology
research about hate speech, free speech, racist beliefs, political leaning, and
more. We disentangle what is annotated as toxic by considering posts with three
characteristics: anti-Black language, African American English (AAE) dialect,
and vulgarity. Our results show strong associations between annotator identity
and beliefs and their ratings of toxicity. Notably, more conservative
annotators and those who scored highly on our scale for racist beliefs were
less likely to rate anti-Black language as toxic, but more likely to rate AAE
as toxic. We additionally present a case study illustrating how a popular
toxicity detection system's ratings inherently reflect only specific beliefs
and perspectives. Our findings call for contextualizing toxicity labels in
social variables, which raises immense implications for toxic language
annotation and detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiVerS: Improving scientific claim verification with weak supervision and full-document context. (arXiv:2112.01640v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01640">
<div class="article-summary-box-inner">
<span><p>The scientific claim verification task requires an NLP system to label
scientific documents which Support or Refute an input claim, and to select
evidentiary sentences (or rationales) justifying each predicted label. In this
work, we present MultiVerS, which predicts a fact-checking label and identifies
rationales in a multitask fashion based on a shared encoding of the claim and
full document context. This approach accomplishes two key modeling goals.
First, it ensures that all relevant contextual information is incorporated into
each labeling decision. Second, it enables the model to learn from instances
annotated with a document-level fact-checking label, but lacking sentence-level
rationales. This allows MultiVerS to perform weakly-supervised domain
adaptation by training on scientific documents labeled using high-precision
heuristics. Our approach outperforms two competitive baselines on three
scientific claim verification datasets, with particularly strong performance in
zero / few-shot domain adaptation experiments. Our code and data are available
at https://github.com/dwadden/multivers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measure and Improve Robustness in NLP Models: A Survey. (arXiv:2112.08313v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08313">
<div class="article-summary-box-inner">
<span><p>As NLP models achieved state-of-the-art performances over benchmarks and
gained wide applications, it has been increasingly important to ensure the safe
deployment of these models in the real world, e.g., making sure the models are
robust against unseen or challenging scenarios. Despite robustness being an
increasingly studied topic, it has been separately explored in applications
like vision and NLP, with various definitions, evaluation and mitigation
strategies in multiple lines of research. In this paper, we aim to provide a
unifying survey of how to define, measure and improve robustness in NLP. We
first connect multiple definitions of robustness, then unify various lines of
work on identifying robustness failures and evaluating models' robustness.
Correspondingly, we present mitigation strategies that are data-driven,
model-driven, and inductive-prior-based, with a more systematic view of how to
effectively improve robustness in NLP models. Finally, we conclude by outlining
open challenges and future directions to motivate further research in this
area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Repair: Repairing model output errors after deployment using a dynamic memory of feedback. (arXiv:2112.09737v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09737">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs), while powerful, are not immune to mistakes, but
can be difficult to retrain. Our goal is for an LM to continue to improve after
deployment, without retraining, using feedback from the user. Our approach
pairs an LM with (i) a growing memory of cases where the user identified an
output error and provided general feedback on how to correct it (ii) a
corrector model, trained to translate this general feedback into specific edits
to repair the model output. Given a new, unseen input, our model can then use
feedback from similar, past cases to repair output errors that may occur. We
instantiate our approach using an existing, fixed model for script generation,
that takes a goal (e.g., "bake a cake") and generates a partially ordered
sequence of actions to achieve that goal, sometimes containing errors. Our
memory-enhanced system, FBNet, learns to apply user feedback to repair such
errors (up to 30 points improvement), while making a start at avoiding similar
past mistakes on new, unseen examples (up to 7 points improvement in a
controlled setting). This is a first step towards strengthening deployed
models, potentially broadening their utility. Our code and data is available at
https://github.com/allenai/interscript/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotating the Tweebank Corpus on Named Entity Recognition and Building NLP Models for Social Media Analysis. (arXiv:2201.07281v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07281">
<div class="article-summary-box-inner">
<span><p>Social media data such as Twitter messages ("tweets") pose a particular
challenge to NLP systems because of their short, noisy, and colloquial nature.
Tasks such as Named Entity Recognition (NER) and syntactic parsing require
highly domain-matched training data for good performance. To date, there is no
complete training corpus for both NER and syntactic analysis (e.g., part of
speech tagging, dependency parsing) of tweets. While there are some publicly
available annotated NLP datasets of tweets, they are only designed for
individual tasks. In this study, we aim to create Tweebank-NER, an English NER
corpus based on Tweebank V2 (TB2), train state-of-the-art (SOTA) Tweet NLP
models on TB2, and release an NLP pipeline called Twitter-Stanza. We annotate
named entities in TB2 using Amazon Mechanical Turk and measure the quality of
our annotations. We train the Stanza pipeline on TB2 and compare with
alternative NLP frameworks (e.g., FLAIR, spaCy) and transformer-based models.
The Stanza tokenizer and lemmatizer achieve SOTA performance on TB2, while the
Stanza NER tagger, part-of-speech (POS) tagger, and dependency parser achieve
competitive performance against non-transformer models. The transformer-based
models establish a strong baseline in Tweebank-NER and achieve the new SOTA
performance in POS tagging and dependency parsing on TB2. We release the
dataset and make both the Stanza pipeline and BERTweet-based models available
"off-the-shelf" for use in future Tweet NLP research. Our source code, data,
and pre-trained models are available at:
\url{https://github.com/social-machines/TweebankNLP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Generative Pretraining for Multimodal Video Captioning. (arXiv:2201.08264v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08264">
<div class="article-summary-box-inner">
<span><p>Recent video and language pretraining frameworks lack the ability to generate
sentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new
pretraining framework for learning from unlabelled videos which can be
effectively used for generative tasks such as multimodal video captioning.
Unlike recent video-language pretraining frameworks, our framework trains both
a multimodal video encoder and a sentence decoder jointly. To overcome the lack
of captions in unlabelled videos, we leverage the future utterance as an
additional text source and propose a bidirectional generation objective -- we
generate future utterances given the present mulitmodal context, and also the
present utterance given future observations. With this objective, we train an
encoder-decoder model end-to-end to generate a caption from raw pixels and
transcribed speech directly. Our model achieves state-of-the-art performance
for multimodal video captioning on four standard benchmarks, as well as for
other video understanding tasks such as VideoQA, video retrieval and action
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey of Hallucination in Natural Language Generation. (arXiv:2202.03629v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03629">
<div class="article-summary-box-inner">
<span><p>Natural Language Generation (NLG) has improved exponentially in recent years
thanks to the development of sequence-to-sequence deep learning technologies
such as Transformer-based language models. This advancement has led to more
fluent and coherent NLG, leading to improved development in downstream tasks
such as abstractive summarization, dialogue generation and data-to-text
generation. However, it is also apparent that deep learning based generation is
prone to hallucinate unintended text, which degrades the system performance and
fails to meet user expectations in many real-world scenarios. To address this
issue, many studies have been presented in measuring and mitigating
hallucinated texts, but these have never been reviewed in a comprehensive
manner before. In this survey, we thus provide a broad overview of the research
progress and challenges in the hallucination problem in NLG. The survey is
organized into two parts: (1) a general overview of metrics, mitigation
methods, and future directions; and (2) an overview of task-specific research
progress on hallucinations in the following downstream tasks, namely
abstractive summarization, dialogue generation, generative question answering,
data-to-text generation, and machine translation. This survey serves to
facilitate collaborative efforts among researchers in tackling the challenge of
hallucinated texts in NLG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic features of object concepts generated with GPT-3. (arXiv:2202.03753v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03753">
<div class="article-summary-box-inner">
<span><p>Semantic features have been playing a central role in investigating the
nature of our conceptual representations. Yet the enormous time and effort
required to empirically sample and norm features from human raters has
restricted their use to a limited set of manually curated concepts. Given
recent promising developments with transformer-based language models, here we
asked whether it was possible to use such models to automatically generate
meaningful lists of properties for arbitrary object concepts and whether these
models would produce features similar to those found in humans. To this end, we
probed a GPT-3 model to generate semantic features for 1,854 objects and
compared automatically-generated features to existing human feature norms.
GPT-3 generated many more features than humans, yet showed a similar
distribution in the types of generated features. Generated feature norms
rivaled human norms in predicting similarity, relatedness, and category
membership, while variance partitioning demonstrated that these predictions
were driven by similar variance in humans and GPT-3. Together, these results
highlight the potential of large language models to capture important facets of
human knowledge and yield a new approach for automatically generating
interpretable feature sets, thus drastically expanding the potential use of
semantic features in psychological and linguistic studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building and curating conversational corpora for diversity-aware language science and technology. (arXiv:2203.03399v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03399">
<div class="article-summary-box-inner">
<span><p>We present an analysis pipeline and best practice guidelines for building and
curating corpora of everyday conversation in diverse languages. Surveying
language documentation corpora and other resources that cover 67 languages and
varieties from 28 phyla, we describe the compilation and curation process,
specify minimal properties of a unified format for interactional data, and
develop methods for quality control that take into account turn-taking and
timing. Two case studies show the broad utility of conversational data for (i)
charting human interactional infrastructure and (ii) tracing challenges and
opportunities for current ASR solutions. Linguistically diverse conversational
corpora can provide new insights for the language sciences and stronger
empirical foundations for language technology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection. (arXiv:2203.09509v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09509">
<div class="article-summary-box-inner">
<span><p>Toxic language detection systems often falsely flag text that contains
minority group mentions as toxic, as those groups are often the targets of
online hate. Such over-reliance on spurious correlations also causes systems to
struggle with detecting implicitly toxic language. To help mitigate these
issues, we create ToxiGen, a new large-scale and machine-generated dataset of
274k toxic and benign statements about 13 minority groups. We develop a
demonstration-based prompting framework and an adversarial
classifier-in-the-loop decoding method to generate subtly toxic and benign text
with a massive pretrained language model. Controlling machine generation in
this way allows ToxiGen to cover implicitly toxic text at a larger scale, and
about more demographic groups, than previous resources of human-written text.
We conduct a human evaluation on a challenging subset of ToxiGen and find that
annotators struggle to distinguish machine-generated text from human-written
language. We also find that 94.5% of toxic examples are labeled as hate speech
by human annotators. Using three publicly-available datasets, we show that
finetuning a toxicity classifier on our data improves its performance on
human-written data substantially. We also demonstrate that ToxiGen can be used
to fight machine-generated toxicity as finetuning improves the classifier
significantly on our evaluation subset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Converse: A Tree-Based Modular Task-Oriented Dialogue System. (arXiv:2203.12187v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12187">
<div class="article-summary-box-inner">
<span><p>Creating a system that can have meaningful conversations with humans to help
accomplish tasks is one of the ultimate goals of Artificial Intelligence (AI).
It has defined the meaning of AI since the beginning. A lot has been
accomplished in this area recently, with voice assistant products entering our
daily lives and chat bot systems becoming commonplace in customer service. At
first glance there seems to be no shortage of options for dialogue systems.
However, the frequently deployed dialogue systems today seem to all struggle
with a critical weakness - they are hard to build and harder to maintain. At
the core of the struggle is the need to script every single turn of
interactions between the bot and the human user. This makes the dialogue
systems more difficult to maintain as the tasks become more complex and more
tasks are added to the system. In this paper, we propose Converse, a flexible
tree-based modular task-oriented dialogue system. Converse uses an and-or tree
structure to represent tasks and offers powerful multi-task dialogue
management. Converse supports task dependency and task switching, which are
unique features compared to other open-source dialogue frameworks. At the same
time, Converse aims to make the bot building process easy and simple, for both
professional and non-professional software developers. The code is available at
https://github.com/salesforce/Converse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01171">
<div class="article-summary-box-inner">
<span><p>Current language generation models suffer from issues such as repetition,
incoherence, and hallucinations. An often-repeated hypothesis is that this
brittleness of generation models is caused by the training and the generation
procedure mismatch, also referred to as exposure bias. In this paper, we verify
this hypothesis by analyzing exposure bias from an imitation learning
perspective. We show that exposure bias leads to an accumulation of errors,
analyze why perplexity fails to capture this accumulation, and empirically show
that this accumulation results in poor generation quality. Source code to
reproduce these experiments is available at
https://github.com/kushalarora/quantifying_exposure_bias
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Noisy Label Correction for Fine-Grained Entity Typing. (arXiv:2205.03011v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03011">
<div class="article-summary-box-inner">
<span><p>Fine-grained entity typing (FET) aims to assign proper semantic types to
entity mentions according to their context, which is a fundamental task in
various entity-leveraging applications. Current FET systems usually establish
on large-scale weakly-supervised/distantly annotation data, which may contain
abundant noise and thus severely hinder the performance of the FET task.
Although previous studies have made great success in automatically identifying
the noisy labels in FET, they usually rely on some auxiliary resources which
may be unavailable in real-world applications (e.g. pre-defined hierarchical
type structures, human-annotated subsets). In this paper, we propose a novel
approach to automatically correct noisy labels for FET without external
resources. Specifically, it first identifies the potentially noisy labels by
estimating the posterior probability of a label being positive or negative
according to the logits output by the model, and then relabel candidate noisy
labels by training a robust model over the remaining clean labels. Experiments
on two popular benchmarks prove the effectiveness of our method. Our source
code can be obtained from https://github.com/CCIIPLab/DenoiseFET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniMorph 4.0: Universal Morphology. (arXiv:2205.03608v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03608">
<div class="article-summary-box-inner">
<span><p>The Universal Morphology (UniMorph) project is a collaborative effort
providing broad-coverage instantiated normalized morphological inflection
tables for hundreds of diverse world languages. The project comprises two major
thrusts: a language-independent feature schema for rich morphological
annotation and a type-level resource of annotated data in diverse languages
realizing that schema. This paper presents the expansions and improvements made
on several fronts over the last couple of years (since McCarthy et al. (2020)).
Collaborative efforts by numerous linguists have added 67 new languages,
including 30 endangered languages. We have implemented several improvements to
the extraction pipeline to tackle some issues, e.g. missing gender and macron
information. We have also amended the schema to use a hierarchical structure
that is needed for morphological phenomena like multiple-argument agreement and
case stacking, while adding some missing morphological features to make the
schema more inclusive. In light of the last UniMorph release, we also augmented
the database with morpheme segmentation for 16 languages. Lastly, this new
release makes a push towards inclusion of derivational morphology in UniMorph
by enriching the data and annotation schema with instances representing
derivational processes from MorphyNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scheduled Multi-task Learning for Neural Chat Translation. (arXiv:2205.03766v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03766">
<div class="article-summary-box-inner">
<span><p>Neural Chat Translation (NCT) aims to translate conversational text into
different languages. Existing methods mainly focus on modeling the bilingual
dialogue characteristics (e.g., coherence) to improve chat translation via
multi-task learning on small-scale chat translation data. Although the NCT
models have achieved impressive success, it is still far from satisfactory due
to insufficient chat translation data and simple joint training manners. To
address the above issues, we propose a scheduled multi-task learning framework
for NCT. Specifically, we devise a three-stage training framework to
incorporate the large-scale in-domain chat translation data into training by
adding a second pre-training stage between the original pre-training and
fine-tuning stages. Further, we investigate where and how to schedule the
dialogue-related auxiliary tasks in multiple training stages to effectively
enhance the main chat translation task. Extensive experiments in four language
directions (English-Chinese and English-German) verify the effectiveness and
superiority of the proposed approach. Additionally, we have made the
large-scale in-domain paired bilingual dialogue dataset publicly available to
the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Aware Abbreviation Expansion Using Large Language Models. (arXiv:2205.03767v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03767">
<div class="article-summary-box-inner">
<span><p>Motivated by the need for accelerating text entry in augmentative and
alternative communication (AAC) for people with severe motor impairments, we
propose a paradigm in which phrases are abbreviated aggressively as primarily
word-initial letters. Our approach is to expand the abbreviations into
full-phrase options by leveraging conversation context with the power of
pretrained large language models (LLMs). Through zero-shot, few-shot, and
fine-tuning experiments on four public conversation datasets, we show that for
replies to the initial turn of a dialog, an LLM with 64B parameters is able to
exactly expand over 70% of phrases with abbreviation length up to 10, leading
to an effective keystroke saving rate of up to about 77% on these exact
expansions. Including a small amount of context in the form of a single
conversation turn more than doubles abbreviation expansion accuracies compared
to having no context, an effect that is more pronounced for longer phrases.
Additionally, the robustness of models against typo noise can be enhanced
through fine-tuning on noisy data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality. (arXiv:2205.04421v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04421">
<div class="article-summary-box-inner">
<span><p>Text to speech (TTS) has made rapid progress in both academia and industry in
recent years. Some questions naturally arise that whether a TTS system can
achieve human-level quality, how to define/judge that quality and how to
achieve it. In this paper, we answer these questions by first defining the
human-level quality based on the statistical significance of subjective measure
and introducing appropriate guidelines to judge it, and then developing a TTS
system called NaturalSpeech that achieves human-level quality on a benchmark
dataset. Specifically, we leverage a variational autoencoder (VAE) for
end-to-end text to waveform generation, with several key modules to enhance the
capacity of the prior from text and reduce the complexity of the posterior from
speech, including phoneme pre-training, differentiable duration modeling,
bidirectional prior/posterior modeling, and a memory mechanism in VAE.
Experiment evaluations on popular LJSpeech dataset show that our proposed
NaturalSpeech achieves -0.01 CMOS (comparative mean opinion score) to human
recordings at the sentence level, with Wilcoxon signed rank test at p-level p
&gt;&gt; 0.05, which demonstrates no statistically significant difference from human
recordings for the first time on this dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Simplification by Tagging. (arXiv:2103.05070v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05070">
<div class="article-summary-box-inner">
<span><p>Edit-based approaches have recently shown promising results on multiple
monolingual sequence transduction tasks. In contrast to conventional
sequence-to-sequence (Seq2Seq) models, which learn to generate text from
scratch as they are trained on parallel corpora, these methods have proven to
be much more effective since they are able to learn to make fast and accurate
transformations while leveraging powerful pre-trained language models. Inspired
by these ideas, we present TST, a simple and efficient Text Simplification
system based on sequence Tagging, leveraging pre-trained Transformer-based
encoders. Our system makes simplistic data augmentations and tweaks in training
and inference on a pre-existing system, which makes it less reliant on large
amounts of parallel training data, provides more control over the outputs and
enables faster inference speeds. Our best model achieves near state-of-the-art
performance on benchmark test datasets for the task. Since it is fully
non-autoregressive, it achieves faster inference speeds by over 11 times than
the current state-of-the-art text simplification system.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization. (arXiv:2205.04464v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04464">
<div class="article-summary-box-inner">
<span><p>We propose a new microscopy simulation system that can depict atomistic
models in a micrograph visual style, similar to results of physical electron
microscopy imaging. This system is scalable, able to represent simulation of
electron microscopy of tens of viral particles and synthesizes the image faster
than previous methods. On top of that, the simulator is differentiable, both
its deterministic as well as stochastic stages that form signal and noise
representations in the micrograph. This notable property has the capability for
solving inverse problems by means of optimization and thus allows for
generation of microscopy simulations using the parameter settings estimated
from real data. We demonstrate this learning capability through two
applications: (1) estimating the parameters of the modulation transfer function
defining the detector properties of the simulated and real micrographs, and (2)
denoising the real data based on parameters trained from the simulated
examples. While current simulators do not support any parameter estimation due
to their forward design, we show that the results obtained using estimated
parameters are very similar to the results of real micrographs. Additionally,
we evaluate the denoising capabilities of our approach and show that the
results showed an improvement over state-of-the-art methods. Denoised
micrographs exhibit less noise in the tilt-series tomography reconstructions,
ultimately reducing the visual dominance of noise in direct volume rendering of
microscopy tomograms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skin disease diagnosis using image analysis and natural language processing. (arXiv:2205.04468v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04468">
<div class="article-summary-box-inner">
<span><p>In Zambia, there is a serious shortage of medical staff where each
practitioner attends to about 17000 patients in a given district while still,
other patients travel over 10 km to access the basic medical services. In this
research, we implement a deep learning model that can perform the clinical
diagnosis process. The study will prove whether image analysis is capable of
performing clinical diagnosis. It will also enable us to understand if we can
use image analysis to lessen the workload on medical practitioners by
delegating some tasks to an AI. The success of this study has the potential to
increase the accessibility of medical services to Zambians, which is one of the
national goals of Vision 2030.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiview Stereo with Cascaded Epipolar RAFT. (arXiv:2205.04502v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04502">
<div class="article-summary-box-inner">
<span><p>We address multiview stereo (MVS), an important 3D vision task that
reconstructs a 3D model such as a dense point cloud from multiple calibrated
images. We propose CER-MVS (Cascaded Epipolar RAFT Multiview Stereo), a new
approach based on the RAFT (Recurrent All-Pairs Field Transforms) architecture
developed for optical flow. CER-MVS introduces five new changes to RAFT:
epipolar cost volumes, cost volume cascading, multiview fusion of cost volumes,
dynamic supervision, and multiresolution fusion of depth maps. CER-MVS is
significantly different from prior work in multiview stereo. Unlike prior work,
which operates by updating a 3D cost volume, CER-MVS operates by updating a
disparity field. Furthermore, we propose an adaptive thresholding method to
balance the completeness and accuracy of the reconstructed point clouds.
Experiments show that our approach achieves competitive performance on DTU (the
second best among known results) and state-of-the-art performance on the
Tanks-and-Temples benchmark (both the intermediate and advanced set). Code is
available at https://github.com/princeton-vl/CER-MVS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image2Gif: Generating Continuous Realistic Animations with Warping NODEs. (arXiv:2205.04519v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04519">
<div class="article-summary-box-inner">
<span><p>Generating smooth animations from a limited number of sequential observations
has a number of applications in vision. For example, it can be used to increase
number of frames per second, or generating a new trajectory only based on first
and last frames, e.g. a motion of face emotions. Despite the discrete observed
data (frames), the problem of generating a new trajectory is a continues
problem. In addition, to be perceptually realistic, the domain of an image
should not alter drastically through the trajectory of changes. In this paper,
we propose a new framework, Warping Neural ODE, for generating a smooth
animation (video frame interpolation) in a continuous manner, given two
("farther apart") frames, denoting the start and the end of the animation. The
key feature of our framework is utilizing the continuous spatial transformation
of the image based on the vector field, derived from a system of differential
equations. This allows us to achieve the smoothness and the realism of an
animation with infinitely small time steps between the frames. We show the
application of our work in generating an animation given two frames, in
different training settings, including Generative Adversarial Network (GAN) and
with $L_2$ loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surreal-GAN:Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns. (arXiv:2205.04523v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04523">
<div class="article-summary-box-inner">
<span><p>A plethora of machine learning methods have been applied to imaging data,
enabling the construction of clinically relevant imaging signatures of
neurological and neuropsychiatric diseases. Oftentimes, such methods don't
explicitly model the heterogeneity of disease effects, or approach it via
nonlinear models that are not interpretable. Moreover, unsupervised methods may
parse heterogeneity that is driven by nuisance confounding factors that affect
brain structure or function, rather than heterogeneity relevant to a pathology
of interest. On the other hand, semi-supervised clustering methods seek to
derive a dichotomous subtype membership, ignoring the truth that disease
heterogeneity spatially and temporally extends along a continuum. To address
the aforementioned limitations, herein, we propose a novel method, termed
Surreal-GAN (Semi-SUpeRvised ReprEsentAtion Learning via GAN). Using
cross-sectional imaging data, Surreal-GAN dissects underlying disease-related
heterogeneity under the principle of semi-supervised clustering (cluster
mappings from normal control to patient), proposes a continuously dimensional
representation, and infers the disease severity of patients at individual level
along each dimension. The model first learns a transformation function from
normal control (CN) domain to the patient (PT) domain with latent variables
controlling transformation directions. An inverse mapping function together
with regularization on function continuity, pattern orthogonality and
monotonicity was also imposed to make sure that the transformation function
captures necessarily meaningful imaging patterns with clinical significance. We
first validated the model through extensive semi-synthetic experiments, and
then demonstrate its potential in capturing biologically plausible imaging
patterns in Alzheimer's disease (AD).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Frequency Bias Affect the Robustness of Neural Image Classifiers against Common Corruption and Adversarial Perturbations?. (arXiv:2205.04533v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04533">
<div class="article-summary-box-inner">
<span><p>Model robustness is vital for the reliable deployment of machine learning
models in real-world applications. Recent studies have shown that data
augmentation can result in model over-relying on features in the low-frequency
domain, sacrificing performance against low-frequency corruptions, highlighting
a connection between frequency and robustness. Here, we take one step further
to more directly study the frequency bias of a model through the lens of its
Jacobians and its implication to model robustness. To achieve this, we propose
Jacobian frequency regularization for models' Jacobians to have a larger ratio
of low-frequency components. Through experiments on four image datasets, we
show that biasing classifiers towards low (high)-frequency components can bring
performance gain against high (low)-frequency corruption and adversarial
perturbation, albeit with a tradeoff in performance for low (high)-frequency
corruption. Our approach elucidates a more direct connection between the
frequency bias and robustness of deep learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is my Depth Ground-Truth Good Enough? HAMMER -- Highly Accurate Multi-Modal Dataset for DEnse 3D Scene Regression. (arXiv:2205.04565v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04565">
<div class="article-summary-box-inner">
<span><p>Depth estimation is a core task in 3D computer vision. Recent methods
investigate the task of monocular depth trained with various depth sensor
modalities. Every sensor has its advantages and drawbacks caused by the nature
of estimates. In the literature, mostly mean average error of the depth is
investigated and sensor capabilities are typically not discussed. Especially
indoor environments, however, pose challenges for some devices. Textureless
regions pose challenges for structure from motion, reflective materials are
problematic for active sensing, and distances for translucent material are
intricate to measure with existing sensors. This paper proposes HAMMER, a
dataset comprising depth estimates from multiple commonly used sensors for
indoor depth estimation, namely ToF, stereo, structured light together with
monocular RGB+P data. We construct highly reliable ground truth depth maps with
the help of 3D scanners and aligned renderings. A popular depth estimators is
trained on this data and typical depth senosors. The estimates are extensively
analyze on different scene structures. We notice generalization issues arising
from various sensor technologies in household environments with challenging but
everyday scene content. HAMMER, which we make publicly available, provides a
reliable base to pave the way to targeted depth improvements and sensor fusion
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When does dough become a bagel? Analyzing the remaining mistakes on ImageNet. (arXiv:2205.04596v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04596">
<div class="article-summary-box-inner">
<span><p>Image classification accuracy on the ImageNet dataset has been a barometer
for progress in computer vision over the last decade. Several recent papers
have questioned the degree to which the benchmark remains useful to the
community, yet innovations continue to contribute gains to performance, with
today's largest models achieving 90%+ top-1 accuracy. To help contextualize
progress on ImageNet and provide a more meaningful evaluation for today's
state-of-the-art models, we manually review and categorize every remaining
mistake that a few top models make in order to provide insight into the
long-tail of errors on one of the most benchmarked datasets in computer vision.
We focus on the multi-label subset evaluation of ImageNet, where today's best
models achieve upwards of 97% top-1 accuracy. Our analysis reveals that nearly
half of the supposed mistakes are not mistakes at all, and we uncover new valid
multi-labels, demonstrating that, without careful review, we are significantly
underestimating the performance of these models. On the other hand, we also
find that today's best models still make a significant number of mistakes (40%)
that are obviously wrong to human reviewers. To calibrate future progress on
ImageNet, we provide an updated multi-label evaluation set, and we curate
ImageNet-Major: a 68-example "major error" slice of the obvious mistakes made
by today's top models -- a slice where models should achieve near perfection,
but today are far from doing so.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoDo: Contrastive Learning with Downstream Background Invariance for Detection. (arXiv:2205.04617v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04617">
<div class="article-summary-box-inner">
<span><p>The prior self-supervised learning researches mainly select image-level
instance discrimination as pretext task. It achieves a fantastic classification
performance that is comparable to supervised learning methods. However, with
degraded transfer performance on downstream tasks such as object detection. To
bridge the performance gap, we propose a novel object-level self-supervised
learning method, called Contrastive learning with Downstream background
invariance (CoDo). The pretext task is converted to focus on instance location
modeling for various backgrounds, especially for downstream datasets. The
ability of background invariance is considered vital for object detection.
Firstly, a data augmentation strategy is proposed to paste the instances onto
background images, and then jitter the bounding box to involve background
information. Secondly, we implement architecture alignment between our
pretraining network and the mainstream detection pipelines. Thirdly,
hierarchical and multi views contrastive learning is designed to improve
performance of visual representation learning. Experiments on MSCOCO
demonstrate that the proposed CoDo with common backbones, ResNet50-FPN, yields
strong transfer learning results for object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KEMP: Keyframe-Based Hierarchical End-to-End Deep Model for Long-Term Trajectory Prediction. (arXiv:2205.04624v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04624">
<div class="article-summary-box-inner">
<span><p>Predicting future trajectories of road agents is a critical task for
autonomous driving. Recent goal-based trajectory prediction methods, such as
DenseTNT and PECNet, have shown good performance on prediction tasks on public
datasets. However, they usually require complicated goal-selection algorithms
and optimization. In this work, we propose KEMP, a hierarchical end-to-end deep
learning framework for trajectory prediction. At the core of our framework is
keyframe-based trajectory prediction, where keyframes are representative states
that trace out the general direction of the trajectory. KEMP first predicts
keyframes conditioned on the road context, and then fills in intermediate
states conditioned on the keyframes and the road context. Under our general
framework, goal-conditioned methods are special cases in which the number of
keyframes equal to one. Unlike goal-conditioned methods, our keyframe predictor
is learned automatically and does not require hand-crafted goal-selection
algorithms. We evaluate our model on public benchmarks and our model ranked 1st
on Waymo Open Motion Dataset Leaderboard (as of September 1, 2021).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using frequency attention to make adversarial patch powerful against person detector. (arXiv:2205.04638v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04638">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are vulnerable to adversarial attacks. In
particular, object detectors may be attacked by applying a particular
adversarial patch to the image. However, because the patch shrinks during
preprocessing, most existing approaches that employ adversarial patches to
attack object detectors would diminish the attack success rate on small and
medium targets. This paper proposes a Frequency Module(FRAN), a
frequency-domain attention module for guiding patch generation. This is the
first study to introduce frequency domain attention to optimize the attack
capabilities of adversarial patches. Our method increases the attack success
rates of small and medium targets by 4.18% and 3.89%, respectively, over the
state-of-the-art attack method for fooling the human detector while assaulting
YOLOv3 without reducing the attack success rate of big targets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STDC-MA Network for Semantic Segmentation. (arXiv:2205.04639v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04639">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is applied extensively in autonomous driving and
intelligent transportation with methods that highly demand spatial and semantic
information. Here, an STDC-MA network is proposed to meet these demands. First,
the STDC-Seg structure is employed in STDC-MA to ensure a lightweight and
efficient structure. Subsequently, the feature alignment module (FAM) is
applied to understand the offset between high-level and low-level features,
solving the problem of pixel offset related to upsampling on the high-level
feature map. Our approach implements the effective fusion between high-level
features and low-level features. A hierarchical multiscale attention mechanism
is adopted to reveal the relationship among attention regions from two
different input sizes of one image. Through this relationship, regions
receiving much attention are integrated into the segmentation results, thereby
reducing the unfocused regions of the input image and improving the effective
utilization of multiscale features. STDC- MA maintains the segmentation speed
as an STDC-Seg network while improving the segmentation accuracy of small
objects. STDC-MA was verified on the verification set of Cityscapes. The
segmentation result of STDC-MA attained 76.81% mIOU with the input of 0.5x
scale, 3.61% higher than STDC-Seg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Monitoring and Insect Behavioural Analysis Using Computer Vision for Precision Pollination. (arXiv:2205.04675v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04675">
<div class="article-summary-box-inner">
<span><p>Insects are the most important global pollinator of crops and play a key role
in maintaining the sustainability of natural ecosystems. Insect pollination
monitoring and management are therefore essential for improving crop production
and food security. Computer vision facilitated pollinator monitoring can
intensify data collection over what is feasible using manual approaches. The
new data it generates may provide a detailed understanding of insect
distributions and facilitate fine-grained analysis sufficient to predict their
pollination efficacy and underpin precision pollination. Current computer
vision facilitated insect tracking in complex outdoor environments is
restricted in spatial coverage and often constrained to a single insect
species. This limits its relevance to agriculture. Therefore, in this article
we introduce a novel system to facilitate markerless data capture for insect
counting, insect motion tracking, behaviour analysis and pollination prediction
across large agricultural areas. Our system is comprised of Edge Computing
multi-point video recording, offline automated multi-species insect counting,
tracking and behavioural analysis. We implement and test our system on a
commercial berry farm to demonstrate its capabilities. Our system successfully
tracked four insect varieties, at nine monitoring stations within a
poly-tunnel, obtaining an F-score above 0.8 for each variety. The system
enabled calculation of key metrics to assess the relative pollination impact of
each insect variety. With this technological advancement, detailed, ongoing
data collection for precision pollination becomes achievable. This is important
to inform growers and apiarists managing crop pollination, as it allows
data-driven decisions to be made to improve food production and food security.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNITS: Unsupervised Intermediate Training Stage for Scene Text Detection. (arXiv:2205.04683v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04683">
<div class="article-summary-box-inner">
<span><p>Recent scene text detection methods are almost based on deep learning and
data-driven. Synthetic data is commonly adopted for pre-training due to
expensive annotation cost. However, there are obvious domain discrepancies
between synthetic data and real-world data. It may lead to sub-optimal
performance to directly adopt the model initialized by synthetic data in the
fine-tuning stage. In this paper, we propose a new training paradigm for scene
text detection, which introduces an \textbf{UN}supervised \textbf{I}ntermediate
\textbf{T}raining \textbf{S}tage (UNITS) that builds a buffer path to
real-world data and can alleviate the gap between the pre-training stage and
fine-tuning stage. Three training strategies are further explored to perceive
information from real-world data in an unsupervised way. With UNITS, scene text
detectors are improved without introducing any parameters and computations
during inference. Extensive experimental results show consistent performance
improvements on three public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OTFPF: Optimal Transport-Based Feature Pyramid Fusion Network for Brain Age Estimation with 3D Overlapped ConvNeXt. (arXiv:2205.04684v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04684">
<div class="article-summary-box-inner">
<span><p>Chronological age of healthy brain is able to be predicted using deep neural
networks from T1-weighted magnetic resonance images (T1 MRIs), and the
predicted brain age could serve as an effective biomarker for detecting
aging-related diseases or disorders. In this paper, we propose an end-to-end
neural network architecture, referred to as optimal transport based feature
pyramid fusion (OTFPF) network, for the brain age estimation with T1 MRIs. The
OTFPF consists of three types of modules: Optimal Transport based Feature
Pyramid Fusion (OTFPF) module, 3D overlapped ConvNeXt (3D OL-ConvNeXt) module
and fusion module. These modules strengthen the OTFPF network's understanding
of each brain's semi-multimodal and multi-level feature pyramid information,
and significantly improve its estimation performances. Comparing with recent
state-of-the-art models, the proposed OTFPF converges faster and performs
better. The experiments with 11,728 MRIs aged 3-97 years show that OTFPF
network could provide accurate brain age estimation, yielding mean absolute
error (MAE) of 2.097, Pearson's correlation coefficient (PCC) of 0.993 and
Spearman's rank correlation coefficient (SRCC) of 0.989, between the estimated
and chronological ages. Widespread quantitative experiments and ablation
experiments demonstrate the superiority and rationality of OTFPF network. The
codes and implement details will be released on GitHub:
https://github.com/ZJU-Brain/OTFPF after final decision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An asynchronous event-based algorithm for periodic signals. (arXiv:2205.04691v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04691">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a simple event-oriented algorithm for detection of
pixel-size signals with a known frequency, by the novel technology of an event
camera. In addition, we analyze the ability of the algorithm to filter out the
desired periodic signals from random fluctuations. We demonstrate this ability
and show how the algorithm can distinguish, during twilight, between the
signals of a streetlight that flicker with frequency of 100 Hz, and sun glitter
originating from windows in far-away buildings in the field of view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Detection of Microaneurysms in OCT Images Using Bag of Features. (arXiv:2205.04695v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04695">
<div class="article-summary-box-inner">
<span><p>Diabetic Retinopathy (DR) caused by diabetes occurs as a result of changes in
the retinal vessels and causes visual impairment. Microaneurysms (MAs) are the
early clinical signs of DR, whose timely diagnosis can help detecting DR in the
early stages of its development. It has been observed that MAs are more common
in the inner retinal layers compared to the outer retinal layers in eyes
suffering from DR. Optical Coherence Tomography (OCT) is a noninvasive imaging
technique that provides a cross-sectional view of the retina and it has been
used in recent years to diagnose many eye diseases. As a result, in this paper
has attempted to identify areas with MA from normal areas of the retina using
OCT images. This work is done using the dataset collected from FA and OCT
images of 20 patients with DR. In this regard, firstly Fluorescein Angiography
(FA) and OCT images were registered. Then the MA and normal areas were
separated and the features of each of these areas were extracted using the Bag
of Features (BOF) approach with Speeded-Up Robust Feature (SURF) descriptor.
Finally, the classification process was performed using a multilayer perceptron
network. For each of the criteria of accuracy, sensitivity, specificity, and
precision, the obtained results were 96.33%, 97.33%, 95.4%, and 95.28%,
respectively. Utilizing OCT images to detect MAsautomatically is a new idea and
the results obtained as preliminary research in this field are promising .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network. (arXiv:2205.04721v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04721">
<div class="article-summary-box-inner">
<span><p>With the growing popularity of smartphones, capturing high-quality images is
of vital importance to smartphones. The cameras of smartphones have small
apertures and small sensor cells, which lead to the noisy images in low light
environment. Denoising based on a burst of multiple frames generally
outperforms single frame denoising but with the larger compututional cost. In
this paper, we propose an efficient yet effective burst denoising system. We
adopt a three-stage design: noise prior integration, multi-frame alignment and
multi-frame denoising. First, we integrate noise prior by pre-processing raw
signals into a variance-stabilization space, which allows using a small-scale
network to achieve competitive performance. Second, we observe that it is
essential to adopt an explicit alignment for burst denoising, but it is not
necessary to integrate a learning-based method to perform multi-frame
alignment. Instead, we resort to a conventional and efficient alignment method
and combine it with our multi-frame denoising network. At last, we propose a
denoising strategy that processes multiple frames sequentially. Sequential
denoising avoids filtering a large number of frames by decomposing multiple
frames denoising into several efficient sub-network denoising. As for each
sub-network, we propose an efficient multi-frequency denoising network to
remove noise of different frequencies. Our three-stage design is efficient and
shows strong performance on burst denoising. Experiments on synthetic and real
raw datasets demonstrate that our method outperforms state-of-the-art methods,
with less computational cost. Furthermore, the low complexity and high-quality
performance make deployment on smartphones possible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Medical Image Classification from Noisy Labeled Data with Global and Local Representation Guided Co-training. (arXiv:2205.04723v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04723">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have achieved remarkable success in a wide variety of
natural image and medical image computing tasks. However, these achievements
indispensably rely on accurately annotated training data. If encountering some
noisy-labeled images, the network training procedure would suffer from
difficulties, leading to a sub-optimal classifier. This problem is even more
severe in the medical image analysis field, as the annotation quality of
medical images heavily relies on the expertise and experience of annotators. In
this paper, we propose a novel collaborative training paradigm with global and
local representation learning for robust medical image classification from
noisy-labeled data to combat the lack of high quality annotated medical data.
Specifically, we employ the self-ensemble model with a noisy label filter to
efficiently select the clean and noisy samples. Then, the clean samples are
trained by a collaborative training strategy to eliminate the disturbance from
imperfect labeled samples. Notably, we further design a novel global and local
representation learning scheme to implicitly regularize the networks to utilize
noisy samples in a self-supervised manner. We evaluated our proposed robust
learning strategy on four public medical image classification datasets with
three types of label noise,ie,random noise, computer-generated label noise, and
inter-observer variability noise. Our method outperforms other learning from
noisy label methods and we also conducted extensive experiments to analyze each
component of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-supervised segmentation of referring expressions. (arXiv:2205.04725v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04725">
<div class="article-summary-box-inner">
<span><p>Visual grounding localizes regions (boxes or segments) in the image
corresponding to given referring expressions. In this work we address image
segmentation from referring expressions, a problem that has so far only been
addressed in a fully-supervised setting. A fully-supervised setup, however,
requires pixel-wise supervision and is hard to scale given the expense of
manual annotation. We therefore introduce a new task of weakly-supervised image
segmentation from referring expressions and propose Text grounded semantic
SEGgmentation (TSEG) that learns segmentation masks directly from image-level
referring expressions without pixel-level annotations. Our transformer-based
method computes patch-text similarities and guides the classification objective
during training with a new multi-label patch assignment mechanism. The
resulting visual grounding model segments image regions corresponding to given
natural language expressions. Our approach TSEG demonstrates promising results
for weakly-supervised referring expression segmentation on the challenging
PhraseCut and RefCOCO datasets. TSEG also shows competitive performance when
evaluated in a zero-shot setting for semantic segmentation on Pascal VOC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild. (arXiv:2205.04749v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04749">
<div class="article-summary-box-inner">
<span><p>Previous methods for dynamic facial expression in the wild are mainly based
on Convolutional Neural Networks (CNNs), whose local operations ignore the
long-range dependencies in videos. To solve this problem, we propose the
spatio-temporal Transformer (STT) to capture discriminative features within
each frame and model contextual relationships among frames. Spatio-temporal
dependencies are captured and integrated by our unified Transformer.
Specifically, given an image sequence consisting of multiple frames as input,
we utilize the CNN backbone to translate each frame into a visual feature
sequence. Subsequently, the spatial attention and the temporal attention within
each block are jointly applied for learning spatio-temporal representations at
the sequence level. In addition, we propose the compact softmax cross entropy
loss to further encourage the learned features have the minimum intra-class
distance and the maximum inter-class distance. Experiments on two in-the-wild
dynamic facial expression datasets (i.e., DFEW and AFEW) indicate that our
method provides an effective way to make use of the spatial and temporal
dependencies for dynamic facial expression recognition. The source code and the
training logs will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WG-VITON: Wearing-Guide Virtual Try-On for Top and Bottom Clothes. (arXiv:2205.04759v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04759">
<div class="article-summary-box-inner">
<span><p>Studies of virtual try-on (VITON) have been shown their effectiveness in
utilizing the generative neural network for virtually exploring fashion
products, and some of recent researches of VITON attempted to synthesize human
image wearing given multiple types of garments (e.g., top and bottom clothes).
However, when replacing the top and bottom clothes of the target human,
numerous wearing styles are possible with a certain combination of the clothes.
In this paper, we address the problem of variation in wearing style when
simultaneously replacing the top and bottom clothes of the model. We introduce
Wearing-Guide VITON (i.e., WG-VITON) which utilizes an additional input binary
mask to control the wearing styles of the generated image. Our experiments show
that WG-VITON effectively generates an image of the model wearing given top and
bottom clothes, and create complicated wearing styles such as partly tucking in
the top to the bottom
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Deep Learning Methods in Medical Diagnosis: A Survey. (arXiv:2205.04766v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04766">
<div class="article-summary-box-inner">
<span><p>The remarkable success of deep learning has prompted interest in its
application to medical diagnosis. Even tough state-of-the-art deep learning
models have achieved human-level accuracy on the classification of different
types of medical data, these models are hardly adopted in clinical workflows,
mainly due to their lack of interpretability. The black-box-ness of deep
learning models has raised the need for devising strategies to explain the
decision process of these models, leading to the creation of the topic of
eXplainable Artificial Intelligence (XAI). In this context, we provide a
thorough survey of XAI applied to medical diagnosis, including visual, textual,
and example-based explanation methods. Moreover, this work reviews the existing
medical imaging datasets and the existing metrics for evaluating the quality of
the explanations . Complementary to most existing surveys, we include a
performance comparison among a set of report generation-based methods. Finally,
the major challenges in applying XAI to medical imaging are also discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Invariant Masked Autoencoders for Self-supervised Learning from Multi-domains. (arXiv:2205.04771v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04771">
<div class="article-summary-box-inner">
<span><p>Generalizing learned representations across significantly different visual
domains is a fundamental yet crucial ability of the human visual system. While
recent self-supervised learning methods have achieved good performances with
evaluation set on the same domain as the training set, they will have an
undesirable performance decrease when tested on a different domain. Therefore,
the self-supervised learning from multiple domains task is proposed to learn
domain-invariant features that are not only suitable for evaluation on the same
domain as the training set but also can be generalized to unseen domains. In
this paper, we propose a Domain-invariant Masked AutoEncoder (DiMAE) for
self-supervised learning from multi-domains, which designs a new pretext task,
\emph{i.e.,} the cross-domain reconstruction task, to learn domain-invariant
features. The core idea is to augment the input image with style noise from
different domains and then reconstruct the image from the embedding of the
augmented image, regularizing the encoder to learn domain-invariant features.
To accomplish the idea, DiMAE contains two critical designs, 1)
content-preserved style mix, which adds style information from other domains to
input while persevering the content in a parameter-free manner, and 2) multiple
domain-specific decoders, which recovers the corresponding domain style of
input to the encoded domain-invariant features for reconstruction. Experiments
on PACS and DomainNet illustrate that DiMAE achieves considerable gains
compared with recent state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Isometric Shape Matching via Functional Maps on Landmark-Adapted Bases. (arXiv:2205.04800v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04800">
<div class="article-summary-box-inner">
<span><p>We propose a principled approach for non-isometric landmark-preserving
non-rigid shape matching. Our method is based on the functional maps framework,
but rather than promoting isometries we focus instead on near-conformal maps
that preserve landmarks exactly. We achieve this, first, by introducing a novel
landmark-adapted basis using an intrinsic Dirichlet-Steklov eigenproblem.
Second, we establish the functional decomposition of conformal maps expressed
in this basis. Finally, we formulate a conformally-invariant energy that
promotes high-quality landmark-preserving maps, and show how it can be solved
via a variant of the recently proposed ZoomOut method that we extend to our
setting. Our method is descriptor-free, efficient and robust to significant
mesh variability. We evaluate our approach on a range of benchmark datasets and
demonstrate state-of-the-art performance on non-isometric benchmarks and near
state-of-the-art performance on isometric ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Partial Occlusion on Pedestrian Detectability. (arXiv:2205.04812v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04812">
<div class="article-summary-box-inner">
<span><p>Robust detection of vulnerable road users is a safety critical requirement
for the deployment of autonomous vehicles in heterogeneous traffic. One of the
most complex outstanding challenges is that of partial occlusion where a target
object is only partially available to the sensor due to obstruction by another
foreground object. A number of leading pedestrian detection benchmarks provide
annotation for partial occlusion, however each benchmark varies greatly in
their definition of the occurrence and severity of occlusion. Recent research
demonstrates that a high degree of subjectivity is used to classify occlusion
level in these cases and occlusion is typically categorized into 2 to 3 broad
categories such as partially and heavily occluded. This can lead to inaccurate
or inconsistent reporting of pedestrian detection model performance depending
on which benchmark is used. This research introduces a novel, objective
benchmark for partially occluded pedestrian detection to facilitate the
objective characterization of pedestrian detection models. Characterization is
carried out on seven popular pedestrian detection models for a range of
occlusion levels from 0-99%. Results demonstrate that pedestrian detection
performance degrades, and the number of false negative detections increase as
pedestrian occlusion level increases. Of the seven popular pedestrian detection
routines characterized, CenterNet has the greatest overall performance,
followed by SSDlite. RetinaNet has the lowest overall detection performance
across the range of occlusion levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised regression learning using domain knowledge: Applications to improving self-supervised denoising in imaging. (arXiv:2205.04821v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04821">
<div class="article-summary-box-inner">
<span><p>Regression that predicts continuous quantity is a central part of
applications using computational imaging and computer vision technologies. Yet,
studying and understanding self-supervised learning for regression tasks -
except for a particular regression task, image denoising - have lagged behind.
This paper proposes a general self-supervised regression learning (SSRL)
framework that enables learning regression neural networks with only input data
(but without ground-truth target data), by using a designable pseudo-predictor
that encapsulates domain knowledge of a specific application. The paper
underlines the importance of using domain knowledge by showing that under
different settings, the better pseudo-predictor can lead properties of SSRL
closer to those of ordinary supervised learning. Numerical experiments for
low-dose computational tomography denoising and camera image denoising
demonstrate that proposed SSRL significantly improves the denoising quality
over several existing self-supervised denoising methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Detection in Indian Food Platters using Transfer Learning with YOLOv4. (arXiv:2205.04841v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04841">
<div class="article-summary-box-inner">
<span><p>Object detection is a well-known problem in computer vision. Despite this,
its usage and pervasiveness in the traditional Indian food dishes has been
limited. Particularly, recognizing Indian food dishes present in a single photo
is challenging due to three reasons: 1. Lack of annotated Indian food datasets
2. Non-distinct boundaries between the dishes 3. High intra-class variation. We
solve these issues by providing a comprehensively labelled Indian food dataset-
IndianFood10, which contains 10 food classes that appear frequently in a staple
Indian meal and using transfer learning with YOLOv4 object detector model. Our
model is able to achieve an overall mAP score of 91.8% and f1-score of 0.90 for
our 10 class dataset. We also provide an extension of our 10 class dataset-
IndianFood20, which contains 10 more traditional Indian food classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Streamline Plausibility Through Randomized Iterative Spherical-Deconvolution Informed Tractogram Filtering. (arXiv:2205.04843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04843">
<div class="article-summary-box-inner">
<span><p>Tractography has become an indispensable part of brain connectivity studies.
However, it is currently facing problems with reliability. In particular, a
substantial amount of nerve fiber reconstructions (streamlines) in tractograms
produced by state-of-the-art tractography methods are anatomically implausible.
To address this problem, tractogram filtering methods have been developed to
remove faulty connections in a postprocessing step. This study takes a closer
look at one such method, \textit{Spherical-deconvolution Informed Filtering of
Tractograms} (SIFT), which uses a global optimization approach to improve the
agreement between the remaining streamlines after filtering and the underlying
diffusion magnetic resonance imaging data. SIFT is not suitable to judge the
plausibility of individual streamlines since its results depend on the size and
composition of the surrounding tractogram. To tackle this problem, we propose
applying SIFT to randomly selected tractogram subsets in order to retrieve
multiple assessments for each streamline. This approach makes it possible to
identify streamlines with very consistent filtering results, which were used as
pseudo ground truths for training classifiers. The trained classifier is able
to distinguish the obtained groups of plausible and implausible streamlines
with accuracy above 80%. The software code used in the paper and pretrained
weights of the classifier are distributed freely via the Github repository
https://github.com/djoerch/randomised_filtering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MNet: Rethinking 2D/3D Networks for Anisotropic Medical Image Segmentation. (arXiv:2205.04846v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04846">
<div class="article-summary-box-inner">
<span><p>The nature of thick-slice scanning causes severe inter-slice discontinuities
of 3D medical images, and the vanilla 2D/3D convolutional neural networks
(CNNs) fail to represent sparse inter-slice information and dense intra-slice
information in a balanced way, leading to severe underfitting to inter-slice
features (for vanilla 2D CNNs) and overfitting to noise from long-range slices
(for vanilla 3D CNNs). In this work, a novel mesh network (MNet) is proposed to
balance the spatial representation inter axes via learning. 1) Our MNet
latently fuses plenty of representation processes by embedding
multi-dimensional convolutions deeply into basic modules, making the selections
of representation processes flexible, thus balancing representation for sparse
inter-slice information and dense intra-slice information adaptively. 2) Our
MNet latently fuses multi-dimensional features inside each basic module,
simultaneously taking the advantages of 2D (high segmentation accuracy of the
easily recognized regions in 2D view) and 3D (high smoothness of 3D organ
contour) representations, thus obtaining more accurate modeling for target
regions. Comprehensive experiments are performed on four public datasets
(CT\&amp;MR), the results consistently demonstrate the proposed MNet outperforms
the other methods. The code and datasets are available at:
https://github.com/zfdong-code/MNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperparameter optimization of hybrid quantum neural networks for car classification. (arXiv:2205.04878v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04878">
<div class="article-summary-box-inner">
<span><p>Image recognition is one of the primary applications of machine learning
algorithms. Nevertheless, machine learning models used in modern image
recognition systems consist of millions of parameters that usually require
significant computational time to be adjusted. Moreover, adjustment of model
hyperparameters leads to additional overhead. Because of this, new developments
in machine learning models and hyperparameter optimization techniques are
required. This paper presents a quantum-inspired hyperparameter optimization
technique and a hybrid quantum-classical machine learning model for supervised
learning. We benchmark our hyperparameter optimization method over standard
black-box objective functions and observe performance improvements in the form
of reduced expected run times and fitness in response to the growth in the size
of the search space. We test our approaches in a car image classification task,
and demonstrate a full-scale implementation of the hybrid quantum neural
network model with the tensor train hyperparameter optimization. Our tests show
a qualitative and quantitative advantage over the corresponding standard
classical tabular grid search approach used with a deep neural network
ResNet34. A classification accuracy of 0.97 was obtained by the hybrid model
after 18 iterations, whereas the classical model achieved an accuracy of 0.92
after 75 iterations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identical Image Retrieval using Deep Learning. (arXiv:2205.04883v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04883">
<div class="article-summary-box-inner">
<span><p>In recent years, we know that the interaction with images has increased.
Image similarity involves fetching similar-looking images abiding by a given
reference image. The target is to find out whether the image searched as a
query can result in similar pictures. We are using the BigTransfer Model, which
is a state-of-art model itself. BigTransfer(BiT) is essentially a ResNet but
pre-trained on a larger dataset like ImageNet and ImageNet-21k with additional
modifications. Using the fine-tuned pre-trained Convolution Neural Network
Model, we extract the key features and train on the K-Nearest Neighbor model to
obtain the nearest neighbor. The application of our model is to find similar
images, which are hard to achieve through text queries within a low inference
time. We analyse the benchmark of our model based on this application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Non-target Knowledge for Few-shot Semantic Segmentation. (arXiv:2205.04903v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04903">
<div class="article-summary-box-inner">
<span><p>Existing studies in few-shot semantic segmentation only focus on mining the
target object information, however, often are hard to tell ambiguous regions,
especially in non-target regions, which include background (BG) and Distracting
Objects (DOs). To alleviate this problem, we propose a novel framework, namely
Non-Target Region Eliminating (NTRE) network, to explicitly mine and eliminate
BG and DO regions in the query. First, a BG Mining Module (BGMM) is proposed to
extract the BG region via learning a general BG prototype. To this end, we
design a BG loss to supervise the learning of BGMM only using the known target
object segmentation ground truth. Then, a BG Eliminating Module and a DO
Eliminating Module are proposed to successively filter out the BG and DO
information from the query feature, based on which we can obtain a BG and
DO-free target object segmentation result. Furthermore, we propose a
prototypical contrastive learning algorithm to improve the model ability of
distinguishing the target object from DOs. Extensive experiments on both
PASCAL-5i and COCO-20i datasets show that our approach is effective despite its
simplicity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shadow-Aware Dynamic Convolution for Shadow Removal. (arXiv:2205.04908v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04908">
<div class="article-summary-box-inner">
<span><p>With a wide range of shadows in many collected images, shadow removal has
aroused increasing attention since uncontaminated images are of vital
importance for many downstream multimedia tasks. Current methods consider the
same convolution operations for both shadow and non-shadow regions while
ignoring the large gap between the color mappings for the shadow region and the
non-shadow region, leading to poor quality of reconstructed images and a heavy
computation burden. To solve this problem, this paper introduces a novel
plug-and-play Shadow-Aware Dynamic Convolution (SADC) module to decouple the
interdependence between the shadow region and the non-shadow region. Inspired
by the fact that the color mapping of the non-shadow region is easier to learn,
our SADC processes the non-shadow region with a lightweight convolution module
in a computationally cheap manner and recovers the shadow region with a more
complicated convolution module to ensure the quality of image reconstruction.
Given that the non-shadow region often contains more background color
information, we further develop a novel intra-convolution distillation loss to
strengthen the information flow from the non-shadow region to the shadow
region. Extensive experiments on the ISTD and SRD datasets show our method
achieves better performance in shadow removal over many state-of-the-arts. Our
code is available at https://github.com/xuyimin0926/SADC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Blind Super-Resolution: Degradation Models, Baselines, and Performance Upper Bounds. (arXiv:2205.04910v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04910">
<div class="article-summary-box-inner">
<span><p>Degradation models play an important role in Blind super-resolution (SR). The
classical degradation model, which mainly involves blur degradation, is too
simple to simulate real-world scenarios. The recently proposed practical
degradation model includes a full spectrum of degradation types, but only
considers complex cases that use all degradation types in the degradation
process, while ignoring many important corner cases that are common in the real
world. To address this problem, we propose a unified gated degradation model to
generate a broad set of degradation cases using a random gate controller. Based
on the gated degradation model, we propose simple baseline networks that can
effectively handle non-blind, classical, practical degradation cases as well as
many other corner cases. To fairly evaluate the performance of our baseline
networks against state-of-the-art methods and understand their limits, we
introduce the performance upper bound of an SR network for every degradation
type. Our empirical analysis shows that with the unified gated degradation
model, the proposed baselines can achieve much better performance than existing
methods in quantitative and qualitative results, which are close to the
performance upper bounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training. (arXiv:2205.04948v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04948">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a cross-modal recipe retrieval framework,
Transformer-based Network for Large Batch Training (TNLBT), which is inspired
by ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).
TNLBT aims to accomplish retrieval tasks while generating images from recipe
embeddings. We apply the Hierarchical Transformer-based recipe text encoder,
the Vision Transformer~(ViT)-based recipe image encoder, and an adversarial
network architecture to enable better cross-modal embedding learning for recipe
texts and images. In addition, we use self-supervised learning to exploit the
rich information in the recipe texts having no corresponding images. Since
contrastive learning could benefit from a larger batch size according to the
recent literature on self-supervised learning, we adopt a large batch size
during training and have validated its effectiveness. In the experiments, the
proposed framework significantly outperformed the current state-of-the-art
frameworks in both cross-modal recipe retrieval and image generation tasks on
the benchmark Recipe1M. This is the first work which confirmed the
effectiveness of large batch training on cross-modal recipe embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRF-Editing: Geometry Editing of Neural Radiance Fields. (arXiv:2205.04978v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04978">
<div class="article-summary-box-inner">
<span><p>Implicit neural rendering, especially Neural Radiance Field (NeRF), has shown
great potential in novel view synthesis of a scene. However, current NeRF-based
methods cannot enable users to perform user-controlled shape deformation in the
scene. While existing works have proposed some approaches to modify the
radiance field according to the user's constraints, the modification is limited
to color editing or object translation and rotation. In this paper, we propose
a method that allows users to perform controllable shape deformation on the
implicit representation of the scene, and synthesizes the novel view images of
the edited scene without re-training the network. Specifically, we establish a
correspondence between the extracted explicit mesh representation and the
implicit neural representation of the target scene. Users can first utilize
well-developed mesh-based deformation methods to deform the mesh representation
of the scene. Our method then utilizes user edits from the mesh representation
to bend the camera rays by introducing a tetrahedra mesh as a proxy, obtaining
the rendering results of the edited scene. Extensive experiments demonstrate
that our framework can achieve ideal editing results not only on synthetic
data, but also on real scenes captured by users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling A Single MR Modality. (arXiv:2205.04982v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04982">
<div class="article-summary-box-inner">
<span><p>Disentangling anatomical and contrast information from medical images has
gained attention recently, demonstrating benefits for various image analysis
tasks. Current methods learn disentangled representations using either paired
multi-modal images with the same underlying anatomy or auxiliary labels (e.g.,
manual delineations) to provide inductive bias for disentanglement. However,
these requirements could significantly increase the time and cost in data
collection and limit the applicability of these methods when such data are not
available. Moreover, these methods generally do not guarantee disentanglement.
In this paper, we present a novel framework that learns theoretically and
practically superior disentanglement from single modality magnetic resonance
images. Moreover, we propose a new information-based metric to quantitatively
evaluate disentanglement. Comparisons over existing disentangling methods
demonstrate that the proposed method achieves superior performance in both
disentanglement and cross-domain image-to-image translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints. (arXiv:2205.04992v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04992">
<div class="article-summary-box-inner">
<span><p>Image-based volumetric avatars using pixel-aligned features promise
generalization to unseen poses and identities. Prior work leverages global
spatial encodings and multi-view geometric consistency to reduce spatial
ambiguity. However, global encodings often suffer from overfitting to the
distribution of the training data, and it is difficult to learn multi-view
consistent reconstruction from sparse views. In this work, we investigate
common issues with existing spatial encodings and propose a simple yet highly
effective approach to modeling high-fidelity volumetric avatars from sparse
views. One of the key ideas is to encode relative spatial 3D information via
sparse 3D keypoints. This approach is robust to the sparsity of viewpoints and
cross-dataset domain gap. Our approach outperforms state-of-the-art methods for
head reconstruction. On human body reconstruction for unseen subjects, we also
achieve performance comparable to prior work that uses a parametric human body
model and temporal feature aggregation. Our experiments show that a majority of
errors in prior work stem from an inappropriate choice of spatial encoding and
thus we suggest a new direction for high-fidelity image-based avatar modeling.
https://markomih.github.io/KeypointNeRF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Deep Learning-based Features Extracted from CT scans to Predict Outcomes in COVID-19 Patients. (arXiv:2205.05009v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05009">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has had a considerable impact on day-to-day life.
Tackling the disease by providing the necessary resources to the affected is of
paramount importance. However, estimation of the required resources is not a
trivial task given the number of factors which determine the requirement. This
issue can be addressed by predicting the probability that an infected patient
requires Intensive Care Unit (ICU) support and the importance of each of the
factors that influence it. Moreover, to assist the doctors in determining the
patients at high risk of fatality, the probability of death is also calculated.
For determining both the patient outcomes (ICU admission and death), a novel
methodology is proposed by combining multi-modal features, extracted from
Computed Tomography (CT) scans and Electronic Health Record (EHR) data. Deep
learning models are leveraged to extract quantitative features from CT scans.
These features combined with those directly read from the EHR database are fed
into machine learning models to eventually output the probabilities of patient
outcomes. This work demonstrates both the ability to apply a broad set of deep
learning methods for general quantification of Chest CT scans and the ability
to link these quantitative metrics to patient outcomes. The effectiveness of
the proposed method is shown by testing it on an internally curated dataset,
achieving a mean area under Receiver operating characteristic curve (AUC) of
0.77 on ICU admission prediction and a mean AUC of 0.73 on death prediction
using the best performing classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Answer Visual Questions from Web Videos. (arXiv:2205.05019v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05019">
<div class="article-summary-box-inner">
<span><p>Recent methods for visual question answering rely on large-scale annotated
datasets. Manual annotation of questions and answers for videos, however, is
tedious, expensive and prevents scalability. In this work, we propose to avoid
manual annotation and generate a large-scale training dataset for video
question answering making use of automatic cross-modal supervision. We leverage
a question generation transformer trained on text data and use it to generate
question-answer pairs from transcribed video narrations. Given narrated videos,
we then automatically generate the HowToVQA69M dataset with 69M
video-question-answer triplets. To handle the open vocabulary of diverse
answers in this dataset, we propose a training procedure based on a contrastive
loss between a video-question multi-modal transformer and an answer
transformer. We introduce the zero-shot VideoQA task and the VideoQA feature
probe evaluation setting and show excellent results, in particular for rare
answers. Furthermore, our method achieves competitive results on MSRVTT-QA,
ActivityNet-QA, MSVD-QA and How2QA datasets. We also show that our VideoQA
dataset generation approach generalizes to another source of web video and text
data. We use our method to generate the \webdataname{} dataset from the WebVid
dataset, i.e., videos with alt-text annotations, and show its benefits for
training VideoQA models. Finally, for a detailed evaluation we introduce
\smalldatasetname{}, a new VideoQA dataset with reduced language bias and
high-quality manual annotations. Code, datasets and trained models are
available at https://antoyang.github.io/just-ask.html
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification and mapping of low-statured 'shrubland' cover types in post-agricultural landscapes of the US Northeast. (arXiv:2205.05047v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05047">
<div class="article-summary-box-inner">
<span><p>Context: Novel plant communities reshape landscapes and pose challenges for
land cover classification and mapping that can constrain research and
stewardship efforts. In the US Northeast, emergence of low-statured woody
vegetation, or 'shrublands', instead of secondary forests in post-agricultural
landscapes is well-documented by field studies, but poorly understood from a
landscape perspective, which limits the ability to systematically study and
manage these lands. Objectives: To address gaps in classification/mapping of
low-statured cover types where they have been historically rare, we developed
models to predict 'shrubland' distributions at 30m resolution across New York
State (NYS), using machine learning and model ensembling techniques to
integrate remote sensing of structural (airborne LIDAR) and optical (satellite
imagery) properties of vegetation cover. We first classified a 1m canopy height
model (CHM), derived from a "patchwork" of available LIDAR coverages, to define
shrubland presence/absence. Next, these non-contiguous maps were used to train
a model ensemble based on temporally-segmented imagery to predict 'shrubland'
probability for the entire study landscape (NYS). Results: Approximately 2.5%
of the CHM coverage area was classified as shrubland. Models using Landsat
predictors trained on the classified CHM were effective at identifying
shrubland (test set AUC=0.893, real-world AUC=0.904), in discriminating between
shrub/young forest and other cover classes, and produced qualitatively sensible
maps, even when extending beyond the original training data. Conclusions: After
ground-truthing, we expect these shrubland maps and models will have many
research and stewardship applications including wildlife conservation, invasive
species mitigation and natural climate solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metric Learning based Interactive Modulation for Real-World Super-Resolution. (arXiv:2205.05065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05065">
<div class="article-summary-box-inner">
<span><p>Interactive image restoration aims to restore images by adjusting several
controlling coefficients, which determine the restoration strength. Existing
methods are restricted in learning the controllable functions under the
supervision of known degradation types and levels. They usually suffer from a
severe performance drop when the real degradation is different from their
assumptions. Such a limitation is due to the complexity of real-world
degradations, which can not provide explicit supervision to the interactive
modulation during training. However, how to realize the interactive modulation
in real-world super-resolution has not yet been studied. In this work, we
present a Metric Learning based Interactive Modulation for Real-World
Super-Resolution (MM-RealSR). Specifically, we propose an unsupervised
degradation estimation strategy to estimate the degradation level in real-world
scenarios. Instead of using known degradation levels as explicit supervision to
the interactive mechanism, we propose a metric learning strategy to map the
unquantifiable degradation levels in real-world scenarios to a metric space,
which is trained in an unsupervised manner. Moreover, we introduce an anchor
point strategy in the metric learning process to normalize the distribution of
metric space. Extensive experiments demonstrate that the proposed MM-RealSR
achieves excellent modulation and restoration performance in real-world
super-resolution. Codes are available at
https://github.com/TencentARC/MM-RealSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating the Training of Video Super-Resolution. (arXiv:2205.05069v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05069">
<div class="article-summary-box-inner">
<span><p>Despite that convolution neural networks (CNN) have recently demonstrated
high-quality reconstruction for video super-resolution (VSR), efficiently
training competitive VSR models remains a challenging problem. It usually takes
an order of magnitude more time than training their counterpart image models,
leading to long research cycles. Existing VSR methods typically train models
with fixed spatial and temporal sizes from beginning to end. The fixed sizes
are usually set to large values for good performance, resulting to slow
training. However, is such a rigid training strategy necessary for VSR? In this
work, we show that it is possible to gradually train video models from small to
large spatial/temporal sizes, i.e., in an easy-to-hard manner. In particular,
the whole training is divided into several stages and the earlier stage has
smaller training spatial shape. Inside each stage, the temporal size also
varies from short to long while the spatial size remains unchanged. Training is
accelerated by such a multigrid training strategy, as most of computation is
performed on smaller spatial and shorter temporal shapes. For further
acceleration with GPU parallelization, we also investigate the large minibatch
training without the loss in accuracy. Extensive experiments demonstrate that
our method is capable of largely speeding up training (up to $6.2\times$
speedup in wall-clock training time) without performance drop for various VSR
models. The code is available at
https://github.com/TencentARC/Efficient-VSR-Training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Visual Styles from Audio-Visual Associations. (arXiv:2205.05072v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05072">
<div class="article-summary-box-inner">
<span><p>From the patter of rain to the crunch of snow, the sounds we hear often
convey the visual textures that appear within a scene. In this paper, we
present a method for learning visual styles from unlabeled audio-visual data.
Our model learns to manipulate the texture of a scene to match a sound, a
problem we term audio-driven image stylization. Given a dataset of paired
audio-visual data, we learn to modify input images such that, after
manipulation, they are more likely to co-occur with a given input sound. In
quantitative and qualitative evaluations, our sound-based model outperforms
label-based approaches. We also show that audio can be an intuitive
representation for manipulating images, as adjusting a sound's volume or mixing
two sounds together results in predictable changes to visual style. Project
webpage: https://tinglok.netlify.app/files/avstyle
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reduce Information Loss in Transformers for Pluralistic Image Inpainting. (arXiv:2205.05076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05076">
<div class="article-summary-box-inner">
<span><p>Transformers have achieved great success in pluralistic image inpainting
recently. However, we find existing transformer based solutions regard each
pixel as a token, thus suffer from information loss issue from two aspects: 1)
They downsample the input image into much lower resolutions for efficiency
consideration, incurring information loss and extra misalignment for the
boundaries of masked regions. 2) They quantize $256^3$ RGB pixels to a small
number (such as 512) of quantized pixels. The indices of quantized pixels are
used as tokens for the inputs and prediction targets of transformer. Although
an extra CNN network is used to upsample and refine the low-resolution results,
it is difficult to retrieve the lost information back.To keep input information
as much as possible, we propose a new transformer based framework "PUT".
Specifically, to avoid input downsampling while maintaining the computation
efficiency, we design a patch-based auto-encoder P-VQVAE, where the encoder
converts the masked image into non-overlapped patch tokens and the decoder
recovers the masked regions from inpainted tokens while keeping the unmasked
regions unchanged. To eliminate the information loss caused by quantization, an
Un-Quantized Transformer (UQ-Transformer) is applied, which directly takes the
features from P-VQVAE encoder as input without quantization and regards the
quantized tokens only as prediction targets. Extensive experiments show that
PUT greatly outperforms state-of-the-art methods on image fidelity, especially
for large masked regions and complex large-scale datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Dynamic Vision Sensors. (arXiv:2006.00422v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.00422">
<div class="article-summary-box-inner">
<span><p>As an alternative sensing paradigm, dynamic vision sensors (DVS) have been
recently explored to tackle scenarios where conventional sensors result in high
data rate and processing time. This paper presents a hybrid event-frame
approach for detecting and tracking objects recorded by a stationary
neuromorphic sensor, thereby exploiting the sparse DVS output in a low-power
setting for traffic monitoring. Specifically, we propose a hardware efficient
processing pipeline that optimizes memory and computational needs that enable
long-term battery powered usage for IoT applications. To exploit the background
removal property of a static DVS, we propose an event-based binary image
creation that signals presence or absence of events in a frame duration. This
reduces memory requirement and enables usage of simple algorithms like median
filtering and connected component labeling for denoise and region proposal
respectively. To overcome the fragmentation issue, a YOLO inspired neural
network based detector and classifier to merge fragmented region proposals has
been proposed. Finally, a new overlap based tracker was implemented, exploiting
overlap between detections and tracks is proposed with heuristics to overcome
occlusion. The proposed pipeline is evaluated with more than 5 hours of traffic
recording spanning three different locations on two different neuromorphic
sensors (DVS and CeleX) and demonstrate similar performance. Compared to
existing event-based feature trackers, our method provides similar accuracy
while needing approx 6 times less computes. To the best of our knowledge, this
is the first time a stationary DVS based traffic monitoring solution is
extensively compared to simultaneously recorded RGB frame-based methods while
showing tremendous promise by outperforming state-of-the-art deep learning
solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting. (arXiv:2010.04456v6 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04456">
<div class="article-summary-box-inner">
<span><p>Forecasting complex dynamical phenomena in settings where only partial
knowledge of their dynamics is available is a prevalent problem across various
scientific fields. While purely data-driven approaches are arguably
insufficient in this context, standard physical modeling based approaches tend
to be over-simplistic, inducing non-negligible errors. In this work, we
introduce the APHYNITY framework, a principled approach for augmenting
incomplete physical dynamics described by differential equations with deep
data-driven models. It consists in decomposing the dynamics into two
components: a physical component accounting for the dynamics for which we have
some prior knowledge, and a data-driven component accounting for errors of the
physical model. The learning problem is carefully formulated such that the
physical model explains as much of the data as possible, while the data-driven
component only describes information that cannot be captured by the physical
model, no more, no less. This not only provides the existence and uniqueness
for this decomposition, but also ensures interpretability and benefits
generalization. Experiments made on three important use cases, each
representative of a different family of phenomena, i.e. reaction-diffusion
equations, wave equations and the non-linear damped pendulum, show that
APHYNITY can efficiently leverage approximate physical models to accurately
forecast the evolution of the system and correctly identify relevant physical
parameters. Code is available at https://github.com/yuan-yin/APHYNITY .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResNet-LDDMM: Advancing the LDDMM Framework using Deep Residual Networks. (arXiv:2102.07951v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07951">
<div class="article-summary-box-inner">
<span><p>In deformable registration, the geometric framework - large deformation
diffeomorphic metric mapping or LDDMM, in short - has inspired numerous
techniques for comparing, deforming, averaging and analyzing shapes or images.
Grounded in flows, which are akin to the equations of motion used in fluid
dynamics, LDDMM algorithms solve the flow equation in the space of plausible
deformations, i.e. diffeomorphisms. In this work, we make use of deep residual
neural networks to solve the non-stationary ODE (flow equation) based on a
Euler's discretization scheme. The central idea is to represent time-dependent
velocity fields as fully connected ReLU neural networks (building blocks) and
derive optimal weights by minimizing a regularized loss function. Computing
minimizing paths between deformations, thus between shapes, turns to find
optimal network parameters by back-propagating over the intermediate building
blocks. Geometrically, at each time step, ResNet-LDDMM searches for an optimal
partition of the space into multiple polytopes, and then computes optimal
velocity vectors as affine transformations on each of these polytopes. As a
result, different parts of the shape, even if they are close (such as two
fingers of a hand), can be made to belong to different polytopes, and therefore
be moved in different directions without costing too much energy. Importantly,
we show how diffeomorphic transformations, or more precisely bilipshitz
transformations, are predicted by our algorithm. We illustrate these ideas on
diverse registration problems of 3D shapes under complex topology-preserving
transformations. We thus provide essential foundations for more advanced shape
variability analysis under a novel joint geometric-neural networks
Riemannian-like framework, i.e. ResNet-LDDMM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization: A Survey. (arXiv:2103.02503v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02503">
<div class="article-summary-box-inner">
<span><p>Generalization to out-of-distribution (OOD) data is a capability natural to
humans yet challenging for machines to reproduce. This is because most learning
algorithms strongly rely on the i.i.d.~assumption on source/target data, which
is often violated in practice due to domain shift. Domain generalization (DG)
aims to achieve OOD generalization by using only source data for model
learning. Over the last ten years, research in DG has made great progress,
leading to a broad spectrum of methodologies, e.g., those based on domain
alignment, meta-learning, data augmentation, or ensemble learning, to name a
few; DG has also been studied in various application areas including computer
vision, speech recognition, natural language processing, medical imaging, and
reinforcement learning. In this paper, for the first time a comprehensive
literature review in DG is provided to summarize the developments over the past
decade. Specifically, we first cover the background by formally defining DG and
relating it to other relevant fields like domain adaptation and transfer
learning. Then, we conduct a thorough review into existing methods and
theories. Finally, we conclude this survey with insights and discussions on
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TubeR: Tubelet Transformer for Video Action Detection. (arXiv:2104.00969v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00969">
<div class="article-summary-box-inner">
<span><p>We propose TubeR: a simple solution for spatio-temporal video action
detection. Different from existing methods that depend on either an off-line
actor detector or hand-designed actor-positional hypotheses like proposals or
anchors, we propose to directly detect an action tubelet in a video by
simultaneously performing action localization and recognition from a single
representation. TubeR learns a set of tubelet-queries and utilizes a
tubelet-attention module to model the dynamic spatio-temporal nature of a video
clip, which effectively reinforces the model capacity compared to using
actor-positional hypotheses in the spatio-temporal space. For videos containing
transitional states or scene changes, we propose a context aware classification
head to utilize short-term and long-term context to strengthen action
classification, and an action switch regression head for detecting the precise
temporal action extent. TubeR directly produces action tubelets with variable
lengths and even maintains good results for long video clips. TubeR outperforms
the previous state-of-the-art on commonly used action detection datasets AVA,
UCF101-24 and JHMDB51-21.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepTag: A General Framework for Fiducial Marker Design and Detection. (arXiv:2105.13731v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13731">
<div class="article-summary-box-inner">
<span><p>A fiducial marker system usually consists of markers, a detection algorithm,
and a coding system. The appearance of markers and the detection robustness are
generally limited by the existing detection algorithms, which are hand-crafted
with traditional low-level image processing techniques. Furthermore, a
sophisticatedly designed coding system is required to overcome the shortcomings
of both markers and detection algorithms. To improve the flexibility and
robustness in various applications, we propose a general deep learning based
framework, DeepTag, for fiducial marker design and detection. DeepTag not only
supports detection of a wide variety of existing marker families, but also
makes it possible to design new marker families with customized local patterns.
Moreover, we propose an effective procedure to synthesize training data on the
fly without manual annotations. Thus, DeepTag can easily adapt to existing and
newly-designed marker families. To validate DeepTag and existing methods,
beside existing datasets, we further collect a new large and challenging
dataset where markers are placed in different view distances and angles.
Experiments show that DeepTag well supports different marker families and
greatly outperforms the existing methods in terms of both detection robustness
and pose accuracy. Both code and dataset are available at
https://herohuyongtao.github.io/research/publications/deep-tag/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Streaming Egocentric Action Anticipation. (arXiv:2110.05386v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05386">
<div class="article-summary-box-inner">
<span><p>Egocentric action anticipation is the task of predicting the future actions a
camera wearer will likely perform based on past video observations. While in a
real-world system it is fundamental to output such predictions before the
action begins, past works have not generally paid attention to model runtime
during evaluation. Indeed, current evaluation schemes assume that predictions
can be made offline, and hence that computational resources are not limited. In
contrast, in this paper, we propose a "streaming" egocentric action
anticipation evaluation protocol which explicitly considers model runtime for
performance assessment, assuming that predictions will be available only after
the current video segment is processed, which depends on the processing time of
a method. Following the proposed evaluation scheme, we benchmark different
state-of-the-art approaches for egocentric action anticipation on two popular
datasets. Our analysis shows that models with a smaller runtime tend to
outperform heavier models in the considered streaming scenario, thus changing
the rankings generally observed in standard offline evaluations. Based on this
observation, we propose a lightweight action anticipation model consisting in a
simple feed-forward 3D CNN, which we propose to optimize using knowledge
distillation techniques and a custom loss. The results show that the proposed
approach outperforms prior art in the streaming scenario, also in combination
with other lightweight models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skeleton-Based Mutually Assisted Interacted Object Localization and Human Action Recognition. (arXiv:2110.14994v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14994">
<div class="article-summary-box-inner">
<span><p>Skeleton data carries valuable motion information and is widely explored in
human action recognition. However, not only the motion information but also the
interaction with the environment provides discriminative cues to recognize the
action of persons. In this paper, we propose a joint learning framework for
mutually assisted "interacted object localization" and "human action
recognition" based on skeleton data. The two tasks are serialized together and
collaborate to promote each other, where preliminary action type derived from
skeleton alone helps improve interacted object localization, which in turn
provides valuable cues for the final human action recognition. Besides, we
explore the temporal consistency of interacted object as constraint to better
localize the interacted object with the absence of ground-truth labels.
Extensive experiments on the datasets of SYSU-3D, NTU60 RGB+D,
Northwestern-UCLA and UAV-Human show that our method achieves the best or
competitive performance with the state-of-the-art methods for human action
recognition. Visualization results show that our method can also provide
reasonable interacted object localization results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Authentication Attacks on Projection-based Cancelable Biometric Schemes (long version). (arXiv:2110.15163v4 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15163">
<div class="article-summary-box-inner">
<span><p>Cancelable biometric schemes aim at generating secure biometric templates by
combining user specific tokens, such as password, stored secret or salt, along
with biometric data. This type of transformation is constructed as a
composition of a biometric transformation with a feature extraction algorithm.
The security requirements of cancelable biometric schemes concern the
irreversibility, unlinkability and revocability of templates, without losing in
accuracy of comparison. While several schemes were recently attacked regarding
these requirements, full reversibility of such a composition in order to
produce colliding biometric characteristics, and specifically presentation
attacks, were never demonstrated to the best of our knowledge. In this paper,
we formalize these attacks for a traditional cancelable scheme with the help of
integer linear programming (ILP) and quadratically constrained quadratic
programming (QCQP). Solving these optimization problems allows an adversary to
slightly alter its fingerprint image in order to impersonate any individual.
Moreover, in an even more severe scenario, it is possible to simultaneously
impersonate several individuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization. (arXiv:2111.07898v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07898">
<div class="article-summary-box-inner">
<span><p>Recurrent neural networks (RNNs) have been shown to perform better than
feedforward architectures in visual object categorization tasks, especially in
challenging conditions such as cluttered images. However, little is known about
the exact computational role of recurrent information flow in these conditions.
Here we test RNNs trained for object categorization on the hypothesis that
recurrence iteratively aids object categorization via the communication of
category-orthogonal auxiliary variables (the location, orientation, and scale
of the object). Using diagnostic linear readouts, we find that: (a) information
about auxiliary variables increases across time in all network layers, (b) this
information is indeed present in the recurrent information flow, and (c) its
manipulation significantly affects task performance. These observations confirm
the hypothesis that category-orthogonal auxiliary variable information is
conveyed through recurrent connectivity and is used to optimize category
inference in cluttered environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Neural Light Fields with Ray-Space Embedding Networks. (arXiv:2112.01523v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01523">
<div class="article-summary-box-inner">
<span><p>Neural radiance fields (NeRFs) produce state-of-the-art view synthesis
results. However, they are slow to render, requiring hundreds of network
evaluations per pixel to approximate a volume rendering integral. Baking NeRFs
into explicit data structures enables efficient rendering, but results in a
large increase in memory footprint and, in many cases, a quality reduction. In
this paper, we propose a novel neural light field representation that, in
contrast, is compact and directly predicts integrated radiance along rays. Our
method supports rendering with a single network evaluation per pixel for small
baseline light field datasets and can also be applied to larger baselines with
only a few evaluations per pixel. At the core of our approach is a ray-space
embedding network that maps the 4D ray-space manifold into an intermediate,
interpolable latent space. Our method achieves state-of-the-art quality on
dense forward-facing datasets such as the Stanford Light Field dataset. In
addition, for forward-facing scenes with sparser inputs we achieve results that
are competitive with NeRF-based approaches in terms of quality while providing
a better speed/quality/memory trade-off with far fewer network evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GETAM: Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation. (arXiv:2112.02841v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02841">
<div class="article-summary-box-inner">
<span><p>Weakly Supervised Semantic Segmentation (WSSS) is challenging, particularly
when image-level labels are used to supervise pixel level prediction. To bridge
their gap, a Class Activation Map (CAM) is usually generated to provide pixel
level pseudo labels. CAMs in Convolutional Neural Networks suffer from partial
activation ie, only the most discriminative regions are activated. Transformer
based methods, on the other hand, are highly effective at exploring global
context with long range dependency modeling, potentially alleviating the
"partial activation" issue. In this paper, we propose the first transformer
based WSSS approach, and introduce the Gradient weighted Element wise
Transformer Attention Map (GETAM). GETAM shows fine scale activation for all
feature map elements, revealing different parts of the object across
transformer layers. Further, we propose an activation aware label completion
module to generate high quality pseudo labels. Finally, we incorporate our
methods into an end to end framework for WSSS using double backward
propagation. Extensive experiments on PASCAL VOC and COCO demonstrate that our
results beat the state-of-the-art end-to-end approaches by a significant
margin, and outperform most multi-stage methods.m most multi-stage methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SVIP: Sequence VerIfication for Procedures in Videos. (arXiv:2112.06447v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06447">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel sequence verification task that aims to
distinguish positive video pairs performing the same action sequence from
negative ones with step-level transformations but still conducting the same
task. Such a challenging task resides in an open-set setting without prior
action detection or segmentation that requires event-level or even frame-level
annotations. To that end, we carefully reorganize two publicly available
action-related datasets with step-procedure-task structure. To fully
investigate the effectiveness of any method, we collect a scripted video
dataset enumerating all kinds of step-level transformations in chemical
experiments. Besides, a novel evaluation metric Weighted Distance Ratio is
introduced to ensure equivalence for different step-level transformations
during evaluation. In the end, a simple but effective baseline based on the
transformer encoder with a novel sequence alignment loss is introduced to
better characterize long-term dependency between steps, which outperforms other
action recognition methods. Codes and data will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Paced Deep Regression Forests with Consideration on Ranking Fairness. (arXiv:2112.06455v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06455">
<div class="article-summary-box-inner">
<span><p>Deep discriminative models (DDMs), such as deep regression forests, deep
neural decision forests, have been extensively studied recently to solve
problems like facial age estimation, head pose estimation, gaze estimation and
so forth. Such problems are challenging in part because a large amount of
effective training data without noise and bias is often not available. While
some progress has been achieved through learning more discriminative features,
or reweighting samples, we argue what is more desirable is to learn gradually
to discriminate like human beings. Then, we resort to self-paced learning
(SPL). But a natural question arises: can self-paced regime lead DDMs to
achieve more robust and less biased solutions? A serious problem with SPL,
which is firstly discussed by this work, is it tends to aggravate the bias of
solutions, especially for obvious imbalanced data. To this end, this paper
proposes a new self-paced paradigm for deep discriminative model, which
distinguishes noisy and underrepresented examples according to the output
likelihood and entropy associated with each example, and tackle the fundamental
ranking problem in SPL from a new perspective: fairness. This paradigm is
fundamental, and could be easily combined with a variety of DDMs. Extensive
experiments on three computer vision tasks, such as facial age estimation, head
pose estimation and gaze estimation, demonstrate the efficacy of our paradigm.
To the best of our knowledge, our work is the first paper in the literature of
SPL that considers ranking fairness for self-paced regime construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoencoder-based background reconstruction and foreground segmentation with background noise estimation. (arXiv:2112.08001v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08001">
<div class="article-summary-box-inner">
<span><p>Even after decades of research, dynamic scene background reconstruction and
foreground object segmentation are still considered as open problems due
various challenges such as illumination changes, camera movements, or
background noise caused by air turbulence or moving trees. We propose in this
paper to model the background of a frame sequence as a low dimensional manifold
using an autoencoder and compare the reconstructed background provided by this
autoencoder with the original image to compute the foreground/background
segmentation masks. The main novelty of the proposed model is that the
autoencoder is also trained to predict the background noise, which allows to
compute for each frame a pixel-dependent threshold to perform the foreground
segmentation. Although the proposed model does not use any temporal or motion
information, it exceeds the state of the art for unsupervised background
subtraction on the CDnet 2014 and LASIESTA datasets, with a significant
improvement on videos where the camera is moving. It is also able to perform
background reconstruction on some non-video image datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Domain Adversarial Adaptation for Photon-efficient Imaging. (arXiv:2201.02475v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02475">
<div class="article-summary-box-inner">
<span><p>Photon-efficient imaging with the single-photon LiDAR captures the 3D
structure of a scene by only a few detected signal photons per pixel. However,
the existing computational methods for photon-efficient imaging are pre-tuned
on a restricted scenario or trained on simulated datasets. When applied to
realistic scenarios whose signal-to-background ratios (SBR) and other
hardware-specific properties differ from those of the original task, the model
performance often significantly deteriorates. In this Letter, we present a
domain adversarial adaptation design to alleviate this domain shift problem by
exploiting unlabeled real-world data, with significant resource savings. This
method demonstrates superior performance on simulated and real-world
experiments using our home-built up-conversion single-photon imaging system,
which provides an efficient approach to bypass the lack of ground-truth depth
information in implementing computational imaging algorithms for realistic
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Generative Pretraining for Multimodal Video Captioning. (arXiv:2201.08264v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08264">
<div class="article-summary-box-inner">
<span><p>Recent video and language pretraining frameworks lack the ability to generate
sentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new
pretraining framework for learning from unlabelled videos which can be
effectively used for generative tasks such as multimodal video captioning.
Unlike recent video-language pretraining frameworks, our framework trains both
a multimodal video encoder and a sentence decoder jointly. To overcome the lack
of captions in unlabelled videos, we leverage the future utterance as an
additional text source and propose a bidirectional generation objective -- we
generate future utterances given the present mulitmodal context, and also the
present utterance given future observations. With this objective, we train an
encoder-decoder model end-to-end to generate a caption from raw pixels and
transcribed speech directly. Our model achieves state-of-the-art performance
for multimodal video captioning on four standard benchmarks, as well as for
other video understanding tasks such as VideoQA, video retrieval and action
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition. (arXiv:2201.10394v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10394">
<div class="article-summary-box-inner">
<span><p>We address the problem of capturing temporal information for video
classification in 2D networks, without increasing computational cost. Existing
approaches focus on modifying the architecture of 2D networks (e.g. by
including filters in the temporal dimension to turn them into 3D networks, or
using optical flow, etc.), which increases computation cost. Instead, we
propose a novel sampling strategy, where we re-order the channels of the input
video, to capture short-term frame-to-frame changes. We observe that without
bells and whistles, the proposed sampling strategy improves performance on
multiple architectures (e.g. TSN, TRN, and TSM) and datasets (CATER,
Something-Something-V1 and V2), up to 24% over the baseline of using the
standard video input. In addition, our sampling strategies do not require
training from scratch and do not increase the computational cost of training
and testing. Given the generality of the results and the flexibility of the
approach, we hope this can be widely useful to the video understanding
community. Code is available at https://github.com/kiyoon/PyVideoAI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System. (arXiv:2201.12604v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12604">
<div class="article-summary-box-inner">
<span><p>Humans excel at continually learning from an ever-changing environment
whereas it remains a challenge for deep neural networks which exhibit
catastrophic forgetting. The complementary learning system (CLS) theory
suggests that the interplay between rapid instance-based learning and slow
structured learning in the brain is crucial for accumulating and retaining
knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)
method which maintains short-term and long-term semantic memories that interact
with the episodic memory. Our method employs an effective replay mechanism
whereby new knowledge is acquired while aligning the decision boundaries with
the semantic memories. CLS-ER does not utilize the task boundaries or make any
assumption about the distribution of the data which makes it versatile and
suited for "general continual learning". Our approach achieves state-of-the-art
performance on standard benchmarks as well as more realistic general continual
learning settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VOS: Learning What You Don't Know by Virtual Outlier Synthesis. (arXiv:2202.01197v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01197">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection has received much attention lately due to
its importance in the safe deployment of neural networks. One of the key
challenges is that models lack supervision signals from unknown data, and as a
result, can produce overconfident predictions on OOD data. Previous approaches
rely on real outlier datasets for model regularization, which can be costly and
sometimes infeasible to obtain in practice. In this paper, we present VOS, a
novel framework for OOD detection by adaptively synthesizing virtual outliers
that can meaningfully regularize the model's decision boundary during training.
Specifically, VOS samples virtual outliers from the low-likelihood region of
the class-conditional distribution estimated in the feature space. Alongside,
we introduce a novel unknown-aware training objective, which contrastively
shapes the uncertainty space between the ID data and synthesized outlier data.
VOS achieves competitive performance on both object detection and image
classification models, reducing the FPR95 by up to 9.36% compared to the
previous best method on object detectors. Code is available at
https://github.com/deeplearning-wisc/vos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Smooth Neural Functions via Lipschitz Regularization. (arXiv:2202.08345v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08345">
<div class="article-summary-box-inner">
<span><p>Neural implicit fields have recently emerged as a useful representation for
3D shapes. These fields are commonly represented as neural networks which map
latent descriptors and 3D coordinates to implicit function values. The latent
descriptor of a neural field acts as a deformation handle for the 3D shape it
represents. Thus, smoothness with respect to this descriptor is paramount for
performing shape-editing operations. In this work, we introduce a novel
regularization designed to encourage smooth latent spaces in neural fields by
penalizing the upper bound on the field's Lipschitz constant. Compared with
prior Lipschitz regularized networks, ours is computationally fast, can be
implemented in four lines of code, and requires minimal hyperparameter tuning
for geometric applications. We demonstrate the effectiveness of our approach on
shape interpolation and extrapolation as well as partial shape reconstruction
from 3D point clouds, showing both qualitative and quantitative improvements
over existing state-of-the-art and non-regularized baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker Extraction with Co-Speech Gestures Cue. (arXiv:2203.16840v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16840">
<div class="article-summary-box-inner">
<span><p>Speaker extraction seeks to extract the clean speech of a target speaker from
a multi-talker mixture speech. There have been studies to use a pre-recorded
speech sample or face image of the target speaker as the speaker cue. In human
communication, co-speech gestures that are naturally timed with speech also
contribute to speech perception. In this work, we explore the use of co-speech
gestures sequence, e.g. hand and body movements, as the speaker cue for speaker
extraction, which could be easily obtained from low-resolution video
recordings, thus more available than face recordings. We propose two networks
using the co-speech gestures cue to perform attentive listening on the target
speaker, one that implicitly fuses the co-speech gestures cue in the speaker
extraction process, the other performs speech separation first, followed by
explicitly using the co-speech gestures cue to associate a separated speech to
the target speaker. The experimental results show that the co-speech gestures
cue is informative in associating with the target speaker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel stereo matching pipeline with robustness and unfixed disparity search range. (arXiv:2204.04865v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04865">
<div class="article-summary-box-inner">
<span><p>Stereo matching is an essential basis for various applications, but most
stereo matching methods have poor generalization performance and require a
fixed disparity search range. Moreover, current stereo matching methods focus
on the scenes that only have positive disparities, but ignore the scenes that
contain both positive and negative disparities, such as 3D movies. In this
paper, we present a new stereo matching pipeline that first computes semi-dense
disparity maps based on binocular disparity, and then completes the rest
depending on monocular cues. The new stereo matching pipeline have the
following advantages: It 1) has better generalization performance than most of
the current stereo matching methods; 2) relaxes the limitation of a fixed
disparity search range; 3) can handle the scenes that involve both positive and
negative disparities, which has more potential applications, such as view
synthesis in 3D multimedia and VR/AR. Experimental results demonstrate the
effectiveness of our new stereo matching pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Image Relational Knowledge Distillation for Semantic Segmentation. (arXiv:2204.06986v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06986">
<div class="article-summary-box-inner">
<span><p>Current Knowledge Distillation (KD) methods for semantic segmentation often
guide the student to mimic the teacher's structured information generated from
individual data samples. However, they ignore the global semantic relations
among pixels across various images that are valuable for KD. This paper
proposes a novel Cross-Image Relational KD (CIRKD), which focuses on
transferring structured pixel-to-pixel and pixel-to-region relations among the
whole images. The motivation is that a good teacher network could construct a
well-structured feature space in terms of global pixel dependencies. CIRKD
makes the student mimic better structured semantic relations from the teacher,
thus improving the segmentation performance. Experimental results over
Cityscapes, CamVid and Pascal VOC datasets demonstrate the effectiveness of our
proposed approach against state-of-the-art distillation methods. The code is
available at https://github.com/winycg/CIRKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResT V2: Simpler, Faster and Stronger. (arXiv:2204.07366v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07366">
<div class="article-summary-box-inner">
<span><p>This paper proposes ResTv2, a simpler, faster, and stronger multi-scale
vision Transformer for visual recognition. ResTv2 simplifies the EMSA structure
in ResTv1 (i.e., eliminating the multi-head interaction part) and employs an
upsample operation to reconstruct the lost medium- and high-frequency
information caused by the downsampling operation. In addition, we explore
different techniques for better apply ResTv2 backbones to downstream tasks. We
found that although combining EMSAv2 and window attention can greatly reduce
the theoretical matrix multiply FLOPs, it may significantly decrease the
computation density, thus causing lower actual speed. We comprehensively
validate ResTv2 on ImageNet classification, COCO detection, and ADE20K semantic
segmentation. Experimental results show that the proposed ResTv2 can outperform
the recently state-of-the-art backbones by a large margin, demonstrating the
potential of ResTv2 as solid backbones. The code and models will be made
publicly available at \url{https://github.com/wofmanaf/ResT}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of Transfer Learning and Ensemble Learning in Image-level Classification for Breast Histopathology. (arXiv:2204.08311v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08311">
<div class="article-summary-box-inner">
<span><p>Background: Breast cancer has the highest prevalence in women globally. The
classification and diagnosis of breast cancer and its histopathological images
have always been a hot spot of clinical concern. In Computer-Aided Diagnosis
(CAD), traditional classification models mostly use a single network to extract
features, which has significant limitations. On the other hand, many networks
are trained and optimized on patient-level datasets, ignoring the application
of lower-level data labels.
</p>
<p>Method: This paper proposes a deep ensemble model based on image-level labels
for the binary classification of benign and malignant lesions of breast
histopathological images. First, the BreaKHis dataset is randomly divided into
a training, validation and test set. Then, data augmentation techniques are
used to balance the number of benign and malignant samples. Thirdly,
considering the performance of transfer learning and the complementarity
between each network, VGG16, Xception, ResNet50, DenseNet201 are selected as
the base classifiers.
</p>
<p>Result: In the ensemble network model with accuracy as the weight, the
image-level binary classification achieves an accuracy of $98.90\%$. In order
to verify the capabilities of our method, the latest Transformer and Multilayer
Perception (MLP) models have been experimentally compared on the same dataset.
Our model wins with a $5\%-20\%$ advantage, emphasizing the ensemble model's
far-reaching significance in classification tasks.
</p>
<p>Conclusion: This research focuses on improving the model's classification
performance with an ensemble algorithm. Transfer learning plays an essential
role in small datasets, improving training speed and accuracy. Our model has
outperformed many existing approaches in accuracy, providing a method for the
field of auxiliary medical diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Model-Based Super-Resolution with Non-uniform Blur. (arXiv:2204.10109v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10109">
<div class="article-summary-box-inner">
<span><p>We propose a state-of-the-art method for super-resolution with non-uniform
blur. Single-image super-resolution methods seek to restore a high-resolution
image from blurred, subsampled, and noisy measurements. Despite their
impressive performance, existing techniques usually assume a uniform blur
kernel. Hence, these techniques do not generalize well to the more general case
of non-uniform blur. Instead, in this paper, we address the more realistic and
computationally challenging case of spatially-varying blur. To this end, we
first propose a fast deep plug-and-play algorithm, based on linearized ADMM
splitting techniques, which can solve the super-resolution problem with
spatially-varying blur. Second, we unfold our iterative algorithm into a single
network and train it end-to-end. In this way, we overcome the intricacy of
manually tuning the parameters involved in the optimization scheme. Our
algorithm presents remarkable performance and generalizes well after a single
training to a large family of spatially-varying blur kernels, noise levels and
scale factors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix. (arXiv:2204.11425v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11425">
<div class="article-summary-box-inner">
<span><p>The evaluation of human epidermal growth factor receptor 2 (HER2) expression
is essential to formulate a precise treatment for breast cancer. The routine
evaluation of HER2 is conducted with immunohistochemical techniques (IHC),
which is very expensive. Therefore, for the first time, we propose a breast
cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data
directly with the paired hematoxylin and eosin (HE) stained images. The dataset
contains 4870 registered image pairs, covering a variety of HER2 expression
levels. Based on BCI, as a minor contribution, we further build a pyramid
pix2pix image generation method, which achieves better HE to IHC translation
results than the other current popular algorithms. Extensive experiments
demonstrate that BCI poses new challenges to the existing image translation
research. Besides, BCI also opens the door for future pathology studies in HER2
expression evaluation based on the synthesized IHC images. BCI dataset can be
downloaded from https://bupt-ai-cz.github.io/BCI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching. (arXiv:2205.02849v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02849">
<div class="article-summary-box-inner">
<span><p>This paper tackles the challenge of forensic medical image matching (FMIM)
using deep neural networks (DNNs). FMIM is a particular case of content-based
image retrieval (CBIR). The main challenge in FMIM compared to the general case
of CBIR, is that the subject to whom a query image belongs may be affected by
aging and progressive degenerative disorders, making it difficult to match data
on a subject level. CBIR with DNNs is generally solved by minimizing a ranking
loss, such as Triplet loss (TL), computed on image representations extracted by
a DNN from the original data. TL, in particular, operates on triplets: anchor,
positive (similar to anchor) and negative (dissimilar to anchor). Although TL
has been shown to perform well in many CBIR tasks, it still has limitations,
which we identify and analyze in this work. In this paper, we introduce (i) the
AdaTriplet loss -- an extension of TL whose gradients adapt to different
difficulty levels of negative samples, and (ii) the AutoMargin method -- a
technique to adjust hyperparameters of margin-based losses such as TL and our
proposed loss dynamically. Our results are evaluated on two large-scale
benchmarks for FMIM based on the Osteoarthritis Initiative and Chest X-ray-14
datasets. The codes allowing replication of this study have been made publicly
available at \url{https://github.com/Oulu-IMEDS/AdaTriplet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn-to-Race Challenge 2022: Benchmarking Safe Learning and Cross-domain Generalisation in Autonomous Racing. (arXiv:2205.02953v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02953">
<div class="article-summary-box-inner">
<span><p>We present the results of our autonomous racing virtual challenge, based on
the newly-released Learn-to-Race (L2R) simulation framework, which seeks to
encourage interdisciplinary research in autonomous driving and to help advance
the state of the art on a realistic benchmark. Analogous to racing being used
to test cutting-edge vehicles, we envision autonomous racing to serve as a
particularly challenging proving ground for autonomous agents as: (i) they need
to make sub-second, safety-critical decisions in a complex, fast-changing
environment; and (ii) both perception and control must be robust to
distribution shifts, novel road features, and unseen obstacles. Thus, the main
goal of the challenge is to evaluate the joint safety, performance, and
generalisation capabilities of reinforcement learning agents on multi-modal
perception, through a two-stage process. In the first stage of the challenge,
we evaluate an autonomous agent's ability to drive as fast as possible, while
adhering to safety constraints. In the second stage, we additionally require
the agent to adapt to an unseen racetrack through safe exploration. In this
paper, we describe the new L2R Task 2.0 benchmark, with refined metrics and
baseline approaches. We also provide an overview of deployment, evaluation, and
rankings for the inaugural instance of the L2R Autonomous Racing Virtual
Challenge (supported by Carnegie Mellon University, Arrival Ltd., AICrowd,
Amazon Web Services, and Honda Research), which officially used the new L2R
Task 2.0 benchmark and received over 20,100 views, 437 active participants, 46
teams, and 733 model submissions -- from 88+ unique institutions, in 58+
different countries. Finally, we release leaderboard results from the challenge
and provide description of the two top-ranking approaches in cross-domain model
transfer, across multiple sensor configurations and simulated races.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement. (arXiv:2205.03569v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03569">
<div class="article-summary-box-inner">
<span><p>Compressed video action recognition has recently drawn growing attention,
since it remarkably reduces the storage and computational cost via replacing
raw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,
motion vectors and residuals). However, this task severely suffers from the
coarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB
and motion modalities. To address the two issues above, this paper proposes a
novel framework, namely Attentive Cross-modal Interaction Network with Motion
Enhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for
the RGB modality and the other for the motion modality. Particularly, the
motion stream employs a multi-scale block embedded with a denoising module to
enhance representation learning. The interaction between the two streams is
then strengthened by introducing the Selective Motion Complement (SMC) and
Cross-Modality Augment (CMA) modules, where SMC complements the RGB modality
with spatio-temporally attentive local motion features and CMA further combines
the two modalities with selective feature augmentation. Extensive experiments
on the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the
effectiveness and efficiency of MEACI-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HierAttn: Effectively Learn Representations from Stage Attention and Branch Attention for Skin Lesions Diagnosis. (arXiv:2205.04326v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04326">
<div class="article-summary-box-inner">
<span><p>An accurate and unbiased examination of skin lesions is critical for the
early diagnosis and treatment of skin cancers. The visual feature of the skin
lesions varies significantly because skin images are collected from patients
with different skin colours by using various devices. Recent studies have
developed ensembled convolutional neural networks (CNNs) to classify the images
for early diagnosis. However, the practical use of CNNs is limited because
their network structures are heavyweight and neglect contextual information.
Vision transformers (ViTs) learn the global features by self-attention
mechanisms, but they also have comparatively large model sizes (more than
100M). To address these limitations, we introduce HierAttn, a lite and
effective neural network with hierarchical and self attention. HierAttn applies
a novel strategy based on learning local and global features by a multi-stage
and hierarchical network. The efficacy of HierAttn was evaluated by using the
dermoscopy images dataset ISIC2019 and smartphone photos dataset PAD-UFES-20.
The experimental results show that HierAttn achieves the best top-1 accuracy
and AUC among state-of-the-art mobile networks, including MobileNetV3 and
MobileViT. The code is available at https://github.com/anthonyweidai/HierAttn.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-11 23:08:33.719148557 UTC">2022-05-11 23:08:33 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>