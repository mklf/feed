<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-29T01:30:00Z">09-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually Grounded Reasoning across Languages and Cultures. (arXiv:2109.13238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13238">
<div class="article-summary-box-inner">
<span><p>The design of widespread vision-and-language datasets and pre-trained
encoders directly adopts, or draws inspiration from, the concepts and images of
ImageNet. While one can hardly overestimate how much this benchmark contributed
to progress in computer vision, it is mostly derived from lexical databases and
image queries in English, resulting in source material with a North American or
Western European bias. Therefore, we devise a new protocol to construct an
ImageNet-style hierarchy representative of more languages and cultures. In
particular, we let the selection of both concepts and images be entirely driven
by native speakers, rather than scraping them automatically. Specifically, we
focus on a typologically diverse set of languages, namely, Indonesian, Mandarin
Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images
obtained through this new protocol, we create a multilingual dataset for
{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting
statements from native speaker annotators about pairs of images. The task
consists of discriminating whether each grounded statement is true or false. We
establish a series of baselines using state-of-the-art models and find that
their cross-lingual transfer performance lags dramatically behind supervised
performance in English. These results invite us to reassess the robustness and
accuracy of current state-of-the-art models beyond a narrow domain, but also
open up new exciting challenges for the development of truly multilingual and
multicultural systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation. (arXiv:2109.13296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13296">
<div class="article-summary-box-inner">
<span><p>Recent progress in generative language models has enabled machines to
generate astonishingly realistic texts. While there are many legitimate
applications of such models, there is also a rising need to distinguish
machine-generated texts from human-written ones (e.g., fake news detection).
However, to our best knowledge, there is currently no benchmark environment
with datasets and tasks to systematically study the so-called "Turing Test"
problem for neural text generation methods. In this work, we present the
TuringBench benchmark environment, which is comprised of (1) a dataset with
200K human- or machine-generated samples across 20 labels {Human, GPT-1,
GPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3,
GROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large,
FAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two
benchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and
(3) a website with leaderboards. Our preliminary experimental results using
TuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all
language models tested, in generating the most human-like indistinguishable
texts with the lowest F1 score by five state-of-the-art TT detection models.
The TuringBench is available at: https://turingbench.ist.psu.edu/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Isotropy Calibration of Transformers. (arXiv:2109.13304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13304">
<div class="article-summary-box-inner">
<span><p>Different studies of the embedding space of transformer models suggest that
the distribution of contextual representations is highly anisotropic - the
embeddings are distributed in a narrow cone. Meanwhile, static word
representations (e.g., Word2Vec or GloVe) have been shown to benefit from
isotropic spaces. Therefore, previous work has developed methods to calibrate
the embedding space of transformers in order to ensure isotropy. However, a
recent study (Cai et al. 2021) shows that the embedding space of transformers
is locally isotropic, which suggests that these models are already capable of
exploiting the expressive capacity of their embedding space. In this work, we
conduct an empirical evaluation of state-of-the-art methods for isotropy
calibration on transformers and find that they do not provide consistent
improvements across models and tasks. These results support the thesis that,
given the local isotropy, transformers do not benefit from additional isotropy
calibration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Transformer Networks with Linear Competing Units: Application to end-to-end SL Translation. (arXiv:2109.13318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13318">
<div class="article-summary-box-inner">
<span><p>Automating sign language translation (SLT) is a challenging real world
application. Despite its societal importance, though, research progress in the
field remains rather poor. Crucially, existing methods that yield viable
performance necessitate the availability of laborious to obtain gloss sequence
groundtruth. In this paper, we attenuate this need, by introducing an
end-to-end SLT model that does not entail explicit use of glosses; the model
only needs text groundtruth. This is in stark contrast to existing end-to-end
models that use gloss sequence groundtruth, either in the form of a modality
that is recognized at an intermediate model stage, or in the form of a parallel
output process, jointly trained with the SLT model. Our approach constitutes a
Transformer network with a novel type of layers that combines: (i) local
winner-takes-all (LWTA) layers with stochastic winner sampling, instead of
conventional ReLU layers, (ii) stochastic weights with posterior distributions
estimated via variational inference, and (iii) a weight compression technique
at inference time that exploits estimated posterior variance to perform
massive, almost lossless compression. We demonstrate that our approach can
reach the currently best reported BLEU-4 score on the PHOENIX 2014T benchmark,
but without making use of glosses for model training, and with a memory
footprint reduced by more than 70%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Biomedical BERT Models for Vocabulary Alignment at Scale in the UMLS Metathesaurus. (arXiv:2109.13348v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13348">
<div class="article-summary-box-inner">
<span><p>The current UMLS (Unified Medical Language System) Metathesaurus construction
process for integrating over 200 biomedical source vocabularies is expensive
and error-prone as it relies on the lexical algorithms and human editors for
deciding if the two biomedical terms are synonymous. Recent advances in Natural
Language Processing such as Transformer models like BERT and its biomedical
variants with contextualized word embeddings have achieved state-of-the-art
(SOTA) performance on downstream tasks. We aim to validate if these approaches
using the BERT models can actually outperform the existing approaches for
predicting synonymy in the UMLS Metathesaurus. In the existing Siamese Networks
with LSTM and BioWordVec embeddings, we replace the BioWordVec embeddings with
the biomedical BERT embeddings extracted from each BERT model using different
ways of extraction. In the Transformer architecture, we evaluate the use of the
different biomedical BERT models that have been pre-trained using different
datasets and tasks. Given the SOTA performance of these BERT models for other
downstream tasks, our experiments yield surprisingly interesting results: (1)
in both model architectures, the approaches employing these biomedical
BERT-based models do not outperform the existing approaches using Siamese
Network with BioWordVec embeddings for the UMLS synonymy prediction task, (2)
the original BioBERT large model that has not been pre-trained with the UMLS
outperforms the SapBERT models that have been pre-trained with the UMLS, and
(3) using the Siamese Networks yields better performance for synonymy
prediction when compared to using the biomedical BERT models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SYGMA: System for Generalizable Modular Question Answering OverKnowledge Bases. (arXiv:2109.13430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13430">
<div class="article-summary-box-inner">
<span><p>Knowledge Base Question Answering (KBQA) tasks that in-volve complex
reasoning are emerging as an important re-search direction. However, most KBQA
systems struggle withgeneralizability, particularly on two dimensions: (a)
acrossmultiple reasoning types where both datasets and systems haveprimarily
focused on multi-hop reasoning, and (b) across mul-tiple knowledge bases, where
KBQA approaches are specif-ically tuned to a single knowledge base. In this
paper, wepresent SYGMA, a modular approach facilitating general-izability
across multiple knowledge bases and multiple rea-soning types. Specifically,
SYGMA contains three high levelmodules: 1) KB-agnostic question understanding
module thatis common across KBs 2) Rules to support additional reason-ing types
and 3) KB-specific question mapping and answeringmodule to address the
KB-specific aspects of the answer ex-traction. We demonstrate effectiveness of
our system by evalu-ating on datasets belonging to two distinct knowledge
bases,DBpedia and Wikidata. In addition, to demonstrate extensi-bility to
additional reasoning types we evaluate on multi-hopreasoning datasets and a new
Temporal KBQA benchmarkdataset on Wikidata, namedTempQA-WD1, introduced in
thispaper. We show that our generalizable approach has bettercompetetive
performance on multiple datasets on DBpediaand Wikidata that requires both
multi-hop and temporal rea-soning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When in Doubt: Improving Classification Performance with Alternating Normalization. (arXiv:2109.13449v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13449">
<div class="article-summary-box-inner">
<span><p>We introduce Classification with Alternating Normalization (CAN), a
non-parametric post-processing step for classification. CAN improves
classification accuracy for challenging examples by re-adjusting their
predicted class probability distribution using the predicted class
distributions of high-confidence validation examples. CAN is easily applicable
to any probabilistic classifier, with minimal computation overhead. We analyze
the properties of CAN using simulated experiments, and empirically demonstrate
its effectiveness across a diverse set of classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Teacher-Student Learning Approach for Multi-lingual Speech-to-Intent Classification. (arXiv:2109.13486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13486">
<div class="article-summary-box-inner">
<span><p>End-to-end speech-to-intent classification has shown its advantage in
harvesting information from both text and speech. In this paper, we study a
technique to develop such an end-to-end system that supports multiple
languages. To overcome the scarcity of multi-lingual speech corpus, we exploit
knowledge from a pre-trained multi-lingual natural language processing model.
Multi-lingual bidirectional encoder representations from transformers (mBERT)
models are trained on multiple languages and hence expected to perform well in
the multi-lingual scenario. In this work, we employ a teacher-student learning
approach to sufficiently extract information from an mBERT model to train a
multi-lingual speech model. In particular, we use synthesized speech generated
from an English-Mandarin text corpus for analysis and training of a
multi-lingual intent classification model. We also demonstrate that the
teacher-student learning approach obtains an improved performance (91.02%) over
the traditional end-to-end (89.40%) intent classification approach in a
practical multi-lingual scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"How Robust r u?": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations. (arXiv:2109.13489v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13489">
<div class="article-summary-box-inner">
<span><p>Most prior work in dialogue modeling has been on written conversations mostly
because of existing data sets. However, written dialogues are not sufficient to
fully capture the nature of spoken conversations as well as the potential
speech recognition errors in practical spoken dialogue systems. This work
presents a new benchmark on spoken task-oriented conversations, which is
intended to study multi-domain dialogue state tracking and knowledge-grounded
dialogue modeling. We report that the existing state-of-the-art models trained
on written conversations are not performing well on our spoken data, as
expected. Furthermore, we observe improvements in task performances when
leveraging n-best speech recognition hypotheses such as by combining
predictions based on individual hypotheses. Our data set enables speech-based
benchmarking of task-oriented dialogue systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance-Based Neural Dependency Parsing. (arXiv:2109.13497v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13497">
<div class="article-summary-box-inner">
<span><p>Interpretable rationales for model predictions are crucial in practical
applications. We develop neural models that possess an interpretable inference
process for dependency parsing. Our models adopt instance-based inference,
where dependency edges are extracted and labeled by comparing them to edges in
a training set. The training edges are explicitly used for the predictions;
thus, it is easy to grasp the contribution of each edge to the predictions. Our
experiments show that our instance-based models achieve competitive accuracy
with standard neural models and have the reasonable plausibility of
instance-based explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoxCeleb Enrichment for Age and Gender Recognition. (arXiv:2109.13510v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13510">
<div class="article-summary-box-inner">
<span><p>VoxCeleb datasets are widely used in speaker recognition studies. Our work
serves two purposes. First, we provide speaker age labels and (an alternative)
annotation of speaker gender. Second, we demonstrate the use of this metadata
by constructing age and gender recognition models with different features and
classifiers. We query different celebrity databases and apply consensus rules
to derive age and gender labels. We also compare the original VoxCeleb gender
labels with our labels to identify records that might be mislabeled in the
original VoxCeleb data. On modeling side, we design a comprehensive study of
multiple features and models for recognizing gender and age. Our best system,
using i-vector features, achieved an F1-score of 0.9829 for gender recognition
task using logistic regression, and the lowest mean absolute error (MAE) in age
regression, 9.443 years, is obtained with ridge regression. This indicates
challenge in age estimation from in-the-wild style speech data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Template-free Prompt Tuning for Few-shot NER. (arXiv:2109.13532v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13532">
<div class="article-summary-box-inner">
<span><p>Prompt-based methods have been successfully applied in sentence-level
few-shot learning tasks, mostly owing to the sophisticated design of templates
and label words. However, when applied to token-level labeling tasks such as
NER, it would be time-consuming to enumerate the template queries over all
potential entity spans. In this work, we propose a more elegant method to
reformulate NER tasks as LM problems without any templates. Specifically, we
discard the template construction process while maintaining the word prediction
paradigm of pre-training models to predict a class-related pivot word (or label
word) at the entity position. Meanwhile, we also explore principled ways to
automatically search for appropriate label words that the pre-trained models
can easily adapt to. While avoiding complicated template-based process, the
proposed LM objective also reduces the gap between different objectives used in
pre-training and fine-tuning, thus it can better benefit the few-shot
performance. Experimental results demonstrate the effectiveness of the proposed
method over bert-tagger and template-based method under few-shot setting.
Moreover, the decoding speed of the proposed method is up to 1930.12 times
faster than the template-based method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators' Disagreement. (arXiv:2109.13563v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13563">
<div class="article-summary-box-inner">
<span><p>Since state-of-the-art approaches to offensive language detection rely on
supervised learning, it is crucial to quickly adapt them to the continuously
evolving scenario of social media. While several approaches have been proposed
to tackle the problem from an algorithmic perspective, so to reduce the need
for annotated data, less attention has been paid to the quality of these data.
Following a trend that has emerged recently, we focus on the level of agreement
among annotators while selecting data to create offensive language datasets, a
task involving a high level of subjectivity. Our study comprises the creation
of three novel datasets of English tweets covering different topics and having
five crowd-sourced judgments each. We also present an extensive set of
experiments showing that selecting training and test data according to
different levels of annotators' agreement has a strong effect on classifiers
performance and robustness. Our findings are further validated in cross-domain
experiments and studied using a popular benchmark dataset. We show that such
hard cases, where low agreement is present, are not necessarily due to
poor-quality annotation and we advocate for a higher presence of ambiguous
cases in future datasets, particularly in test sets, to better account for the
different points of view expressed online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating texts under constraint through discriminator-guided MCTS. (arXiv:2109.13582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13582">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (LM) based on Transformers allow to
generate very plausible long texts. In this paper, we explore how this
generation can be further controlled to satisfy certain constraints (eg. being
non-toxic, positive or negative, convey certain emotions, etc.) without
fine-tuning the LM. Precisely, we formalize constrained generation as a tree
exploration process guided by a discriminator according to how well the
associated sequence respects the constraint. Using a discriminator to guide
this generation, rather than fine-tuning the LM, in addition to be easier and
cheaper to train, allows to apply the constraint more finely and dynamically.
We propose several original methods to search this generation tree, notably the
Monte Carlo Tree Search (MCTS) which provides theoretical guarantees on the
search efficiency, but also simpler methods based on re-ranking a pool of
diverse sequences using the discriminator scores. We evaluate these methods on
two types of constraints and languages: review polarity and emotion control in
French and English. We show that MCTS achieves state-of-the-art results in
constrained generation, without having to tune the language model, in both
tasks and languages. We also demonstrate that our other proposed methods based
on re-ranking can be really effective when diversity among the generated
propositions is encouraged.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Argument Mining: A Practical Approach. (arXiv:2109.13611v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13611">
<div class="article-summary-box-inner">
<span><p>Despite considerable recent progress, the creation of well-balanced and
diverse resources remains a time-consuming and costly challenge in Argument
Mining. Active Learning reduces the amount of data necessary for the training
of machine learning models by querying the most informative samples for
annotation and therefore is a promising method for resource creation. In a
large scale comparison of several Active Learning methods, we show that Active
Learning considerably decreases the effort necessary to get good deep learning
performance on the task of Argument Unit Recognition and Classification (AURC).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking. (arXiv:2109.13620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13620">
<div class="article-summary-box-inner">
<span><p>Recent progress in task-oriented neural dialogue systems is largely focused
on a handful of languages, as annotation of training data is tedious and
expensive. Machine translation has been used to make systems multilingual, but
this can introduce a pipeline of errors. Another promising solution is using
cross-lingual transfer learning through pretrained multilingual models.
Existing methods train multilingual models with additional code-mixed task data
or refine the cross-lingual representations through parallel ontologies. In
this work, we enhance the transfer learning process by intermediate fine-tuning
of pretrained multilingual models, where the multilingual models are fine-tuned
with different but related data and/or tasks. Specifically, we use parallel and
conversational movie subtitles datasets to design cross-lingual intermediate
tasks suitable for downstream dialogue tasks. We use only 200K lines of
parallel data for intermediate fine-tuning which is already available for 1782
language pairs. We test our approach on the cross-lingual dialogue state
tracking task for the parallel MultiWoZ (English -&gt; Chinese, Chinese -&gt;
English) and Multilingual WoZ (English -&gt; German, English -&gt; Italian) datasets.
We achieve impressive improvements (&gt; 20% on joint goal accuracy) on the
parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla
baseline with only 10% of the target language task data and zero-shot setup
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPSL: End-to-end perception and reasoning with applications to zero shot learning. (arXiv:2109.13662v1 [eess.SY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13662">
<div class="article-summary-box-inner">
<span><p>We introduce DeepPSL a variant of Probabilistic Soft Logic (PSL) to produce
an end-to-end trainable system that integrates reasoning and perception. PSL
represents first-order logic in terms of a convex graphical model -- Hinge Loss
Markov random fields (HL-MRFs). PSL stands out among probabilistic logic
frameworks due to its tractability having been applied to systems of more than
1 billion ground rules. The key to our approach is to represent predicates in
first-order logic using deep neural networks and then to approximately
back-propagate through the HL-MRF and thus train every aspect of the
first-order system being represented. We believe that this approach represents
an interesting direction for the integration of deep learning and reasoning
techniques with applications to knowledge base learning, multi-task learning,
and explainability. We evaluate DeepPSL on a zero shot learning problem in
image classification. State of the art results demonstrate the utility and
flexibility of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Counter Narrative Type Classification. (arXiv:2109.13664v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13664">
<div class="article-summary-box-inner">
<span><p>The growing interest in employing counter narratives for hatred intervention
brings with it a focus on dataset creation and automation strategies. In this
scenario, learning to recognize counter narrative types from natural text is
expected to be useful for applications such as hate speech countering, where
operators from non-governmental organizations are supposed to answer to hate
with several and diverse arguments that can be mined from online sources. This
paper presents the first multilingual work on counter narrative type
classification, evaluating SoTA pre-trained language models in monolingual,
multilingual and cross-lingual settings. When considering a fine-grained
annotation of counter narrative classes, we report strong baseline
classification results for the majority of the counter narrative types,
especially if we translate every language to English before cross-lingual
prediction. This suggests that knowledge about counter narratives can be
successfully transferred across languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nana-HDR: A Non-attentive Non-autoregressive Hybrid Model for TTS. (arXiv:2109.13673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13673">
<div class="article-summary-box-inner">
<span><p>This paper presents Nana-HDR, a new non-attentive non-autoregressive model
with hybrid Transformer-based Dense-fuse encoder and RNN-based decoder for TTS.
It mainly consists of three parts: Firstly, a novel Dense-fuse encoder with
dense connections between basic Transformer blocks for coarse feature fusion
and a multi-head attention layer for fine feature fusion. Secondly, a
single-layer non-autoregressive RNN-based decoder. Thirdly, a duration
predictor instead of an attention model that connects the above hybrid encoder
and decoder. Experiments indicate that Nana-HDR gives full play to the
advantages of each component, such as strong text encoding ability of
Transformer-based encoder, stateful decoding without being bothered by exposure
bias and local information preference, and stable alignment provided by
duration predictor. Due to these advantages, Nana-HDR achieves competitive
performance in naturalness and robustness on two Mandarin corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CIDEr-R: Robust Consensus-based Image Description Evaluation. (arXiv:2109.13701v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13701">
<div class="article-summary-box-inner">
<span><p>This paper shows that CIDEr-D, a traditional evaluation metric for image
description, does not work properly on datasets where the number of words in
the sentence is significantly greater than those in the MS COCO Captions
dataset. We also show that CIDEr-D has performance hampered by the lack of
multiple reference sentences and high variance of sentence length. To bypass
this problem, we introduce CIDEr-R, which improves CIDEr-D, making it more
flexible in dealing with datasets with high sentence length variance. We
demonstrate that CIDEr-R is more accurate and closer to human judgment than
CIDEr-D; CIDEr-R is more robust regarding the number of available references.
Our results reveal that using Self-Critical Sequence Training to optimize
CIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized,
the generated captions' length tends to be similar to the reference length.
However, the models also repeat several times the same word to increase the
sentence length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One to rule them all: Towards Joint Indic Language Hate Speech Detection. (arXiv:2109.13711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13711">
<div class="article-summary-box-inner">
<span><p>This paper is a contribution to the Hate Speech and Offensive Content
Identification in Indo-European Languages (HASOC) 2021 shared task. Social
media today is a hotbed of toxic and hateful conversations, in various
languages. Recent news reports have shown that current models struggle to
automatically identify hate posted in minority languages. Therefore,
efficiently curbing hate speech is a critical challenge and problem of
interest. We present a multilingual architecture using state-of-the-art
transformer language models to jointly learn hate and offensive speech
detection across three languages namely, English, Hindi, and Marathi. On the
provided testing corpora, we achieve Macro F1 scores of 0.7996, 0.7748, 0.8651
for sub-task 1A and 0.6268, 0.5603 during the fine-grained classification of
sub-task 1B. These results show the efficacy of exploiting a multilingual
training scheme.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Use of Character-Level Translation with Sparse and Noisy Datasets. (arXiv:2109.13723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13723">
<div class="article-summary-box-inner">
<span><p>This paper provides an analysis of character-level machine translation models
used in pivot-based translation when applied to sparse and noisy datasets, such
as crowdsourced movie subtitles. In our experiments, we find that such
character-level models cut the number of untranslated words by over 40% and are
especially competitive (improvements of 2-3 BLEU points) in the case of limited
training data. We explore the impact of character alignment, phrase table
filtering, bitext size and the choice of pivot language on translation quality.
We further compare cascaded translation models to the use of synthetic training
data via multiple pivots, and we find that the latter works significantly
better. Finally, we demonstrate that neither word-nor character-BLEU correlate
perfectly with human judgments, due to BLEU's sensitivity to length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translating from Morphologically Complex Languages: A Paraphrase-Based Approach. (arXiv:2109.13724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13724">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach to translating from a morphologically complex
language. Unlike previous research, which has targeted word inflections and
concatenations, we focus on the pairwise relationship between morphologically
related words, which we treat as potential paraphrases and handle using
paraphrasing techniques at the word, phrase, and sentence level. An important
advantage of this framework is that it can cope with derivational morphology,
which has so far remained largely beyond the capabilities of statistical
machine translation systems. Our experiments translating from Malay, whose
morphology is mostly derivational, into English show significant improvements
over rivaling approaches based on five automatic evaluation measures (for
320,000 sentence pairs; 9.5 million English word tokens).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis in Twitter for Macedonian. (arXiv:2109.13725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13725">
<div class="article-summary-box-inner">
<span><p>We present work on sentiment analysis in Twitter for Macedonian. As this is
pioneering work for this combination of language and genre, we created suitable
resources for training and evaluating a system for sentiment analysis of
Macedonian tweets. In particular, we developed a corpus of tweets annotated
with tweet-level sentiment polarity (positive, negative, and neutral), as well
as with phrase-level sentiment, which we made freely available for research
purposes. We further bootstrapped several large-scale sentiment lexicons for
Macedonian, motivated by previous work for English. The impact of several
different pre-processing steps as well as of various features is shown in
experiments that represent the first attempt to build a system for sentiment
analysis in Twitter for the morphologically rich Macedonian language. Overall,
our experimental results show an F1-score of 92.16, which is very strong and is
on par with the best results for English, which were achieved in recent SemEval
competitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exposing Paid Opinion Manipulation Trolls. (arXiv:2109.13726v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13726">
<div class="article-summary-box-inner">
<span><p>Recently, Web forums have been invaded by opinion manipulation trolls. Some
trolls try to influence the other users driven by their own convictions, while
in other cases they can be organized and paid, e.g., by a political party or a
PR agency that gives them specific instructions what to write. Finding paid
trolls automatically using machine learning is a hard task, as there is no
enough training data to train a classifier; yet some test data is possible to
obtain, as these trolls are sometimes caught and widely exposed. In this paper,
we solve the training data problem by assuming that a user who is called a
troll by several different people is likely to be such, and one who has never
been called a troll is unlikely to be such. We compare the profiles of (i) paid
trolls vs. (ii)"mentioned" trolls vs. (iii) non-trolls, and we further show
that a classifier trained to distinguish (ii) from (iii) does quite well also
at telling apart (i) from (iii).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Triplet Loss for Named Entity Recognition using Supplementary Text. (arXiv:2109.13736v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13736">
<div class="article-summary-box-inner">
<span><p>Retail item data contains many different forms of text like the title of an
item, the description of an item, item name and reviews. It is of interest to
identify the item name in the other forms of text using a named entity tagger.
However, the title of an item and its description are syntactically different
(but semantically similar) in that the title is not necessarily a well formed
sentence while the description is made up of well formed sentences. In this
work, we use a triplet loss to contrast the embeddings of the item title with
the description to establish a proof of concept. We find that using the triplet
loss in a multi-task NER algorithm improves both the precision and recall by a
small percentage. While the improvement is small, we think it is a step in the
right direction of using various forms of text in a multi-task algorithm. In
addition to precision and recall, the multi task triplet loss method is also
found to significantly improve the exact match accuracy i.e. the accuracy of
tagging the entire set of tokens in the text with correct tags.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Homophony and R\'enyi Entropy. (arXiv:2109.13766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13766">
<div class="article-summary-box-inner">
<span><p>Homophony's widespread presence in natural languages is a controversial
topic. Recent theories of language optimality have tried to justify its
prevalence, despite its negative effects on cognitive processing time; e.g.,
Piantadosi et al. (2012) argued homophony enables the reuse of efficient
wordforms and is thus beneficial for languages. This hypothesis has recently
been challenged by Trott and Bergen (2020), who posit that good wordforms are
more often homophonous simply because they are more phonotactically probable.
In this paper, we join in on the debate. We first propose a new
information-theoretic quantification of a language's homophony: the sample
R\'enyi entropy. Then, we use this quantification to revisit Trott and Bergen's
claims. While their point is theoretically sound, a specific methodological
issue in their experiments raises doubts about their results. After addressing
this issue, we find no clear pressure either towards or against homophony -- a
much more nuanced result than either Piantadosi et al.'s or Trott and Bergen's
findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying and Mitigating Gender Bias in Hyperbolic Word Embeddings. (arXiv:2109.13767v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13767">
<div class="article-summary-box-inner">
<span><p>Euclidean word embedding models such as GloVe and Word2Vec have been shown to
reflect human-like gender biases. In this paper, we extend the study of gender
bias to the recently popularized hyperbolic word embeddings. We propose
gyrocosine bias, a novel measure for quantifying gender bias in hyperbolic word
representations and observe a significant presence of gender bias. To address
this problem, we propose Poincar\'e Gender Debias (PGD), a novel debiasing
procedure for hyperbolic word representations. Experiments on a suit of
evaluation tests show that PGD effectively reduces bias while adding a minimal
semantic offset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health. (arXiv:2109.13770v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13770">
<div class="article-summary-box-inner">
<span><p>Many statistical models have high accuracy on test benchmarks, but are not
explainable, struggle in low-resource scenarios, cannot be reused for multiple
tasks, and cannot easily integrate domain expertise. These factors limit their
use, particularly in settings such as mental health, where it is difficult to
annotate datasets and model outputs have significant impact. We introduce a
micromodel architecture to address these challenges. Our approach allows
researchers to build interpretable representations that embed domain knowledge
and provide explanations throughout the model's decision process. We
demonstrate the idea on multiple mental health tasks: depression
classification, PTSD classification, and suicidal risk assessment. Our systems
consistently produce strong results, even in low-resource scenarios, and are
more interpretable than alternative methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chekhov's Gun Recognition. (arXiv:2109.13855v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13855">
<div class="article-summary-box-inner">
<span><p>Chekhov's gun is a dramatic principle stating that every element in a story
must be necessary, and irrelevant elements should be removed. This paper
presents a new natural language processing task - Chekhov's gun recognition or
(CGR) - recognition of entities that are pivotal for the development of the
plot. Though similar to classical Named Entity Recognition (NER) it has
profound differences and is crucial for the tasks of narrative processing,
since Chekhov's guns have a profound impact on the causal relationship in a
story. The paper presents a new benchmark dataset for the CGR task that
includes 5550 descriptions with one or more Chekhov's Gun in each and validates
the task on two more datasets available in the natural language processing
(NLP) literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expectation-based Minimalist Grammars. (arXiv:2109.13871v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13871">
<div class="article-summary-box-inner">
<span><p>Expectation-based Minimalist Grammars (e-MGs) are simplified versions of the
(Conflated) Minimalist Grammars, (C)MGs, formalized by Stabler (Stabler, 2011,
2013, 1997) and Phase-based Minimalist Grammars, PMGs (Chesi, 2005, 2007;
Stabler, 2011). The crucial simplification consists of driving structure
building only by relying on lexically encoded categorial top-down expectations.
The commitment on a top-down derivation (as in e-MGs and PMGs, as opposed to
(C)MGs, Chomsky, 1995; Stabler, 2011) allows us to define a core derivation
that should be the same in both parsing and generation (Momma &amp; Phillips,
2018).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-dataset Experts for Multi-dataset Question Answering. (arXiv:2109.13880v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13880">
<div class="article-summary-box-inner">
<span><p>Many datasets have been created for training reading comprehension models,
and a natural question is whether we can combine them to build models that (1)
perform better on all of the training datasets and (2) generalize and transfer
better to new datasets. Prior work has addressed this goal by training one
network simultaneously on multiple datasets, which works well on average but is
prone to over- or under-fitting different sub-distributions and might transfer
worse compared to source models with more overlap with the target dataset. Our
approach is to model multi-dataset question answering with a collection of
single-dataset experts, by training a collection of lightweight,
dataset-specific adapter modules (Houlsby et al., 2019) that share an
underlying Transformer model. We find that these Multi-Adapter Dataset Experts
(MADE) outperform all our baselines in terms of in-distribution accuracy, and
simple methods based on parameter-averaging lead to better zero-shot
generalization and few-shot transfer performance, offering a strong and
versatile starting point for building new reading comprehension systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Different Text-preprocessing Techniques Using The BERT Model Affect The Gender Profiling of Authors. (arXiv:2109.13890v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13890">
<div class="article-summary-box-inner">
<span><p>Forensic author profiling plays an important role in indicating possible
profiles for suspects. Among the many automated solutions recently proposed for
author profiling, transfer learning outperforms many other state-of-the-art
techniques in natural language processing. Nevertheless, the sophisticated
technique has yet to be fully exploited for author profiling. At the same time,
whereas current methods of author profiling, all largely based on features
engineering, have spawned significant variation in each model used, transfer
learning usually requires a preprocessed text to be fed into the model. We
reviewed multiple references in the literature and determined the most common
preprocessing techniques associated with authors' genders profiling.
Considering the variations in potential preprocessing techniques, we conducted
an experimental study that involved applying five such techniques to measure
each technique's effect while using the BERT model, chosen for being one of the
most-used stock pretrained models. We used the Hugging face transformer library
to implement the code for each preprocessing case. In our five experiments, we
found that BERT achieves the best accuracy in predicting the gender of the
author when no preprocessing technique is applied. Our best case achieved
86.67% accuracy in predicting the gender of authors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Information and Event Markup Language: TIE-ML Markup Process and Schema Version 1.0. (arXiv:2109.13892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13892">
<div class="article-summary-box-inner">
<span><p>Temporal Information and Event Markup Language (TIE-ML) is a markup strategy
and annotation schema to improve the productivity and accuracy of temporal and
event related annotation of corpora to facilitate machine learning based model
training. For the annotation of events, temporal sequencing, and durations, it
is significantly simpler by providing an extremely reduced tag set for just
temporal relations and event enumeration. In comparison to other standards, as
for example the Time Markup Language (TimeML), it is much easier to use by
dropping sophisticated formalisms, theoretical concepts, and annotation
approaches. Annotations of corpora using TimeML can be mapped to TIE-ML with a
loss, and TIE-ML annotations can be fully mapped to TimeML with certain
under-specification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsolved Problems in ML Safety. (arXiv:2109.13916v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13916">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards ("Robustness"),
identifying hazards ("Monitoring"), steering ML systems ("Alignment"), and
reducing risks to how ML systems are handled ("External Safety"). Throughout,
we clarify each problem's motivation and provide concrete research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13847">
<div class="article-summary-box-inner">
<span><p>Word embeddings are a powerful natural language processing technique, but
they are extremely difficult to interpret. To enable interpretable NLP models,
we create vectors where each dimension is inherently interpretable. By
inherently interpretable, we mean a system where each dimension is associated
with some human understandable hint that can describe the meaning of that
dimension. In order to create more interpretable word embeddings, we transform
pretrained dense word embeddings into sparse embeddings. These new embeddings
are inherently interpretable: each of their dimensions is created from and
represents a natural language word or specific grammatical concept. We
construct these embeddings through sparse coding, where each vector in the
basis set is itself a word embedding. Therefore, each dimension of our sparse
vectors corresponds to a natural language word. We also show that models
trained using these sparse embeddings can achieve good performance and are more
interpretable in practice, including through human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Knowledge Graphs Canonicalization using Variational Autoencoders. (arXiv:2012.04780v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04780">
<div class="article-summary-box-inner">
<span><p>Noun phrases and Relation phrases in open knowledge graphs are not
canonicalized, leading to an explosion of redundant and ambiguous
subject-relation-object triples. Existing approaches to solve this problem take
a two-step approach. First, they generate embedding representations for both
noun and relation phrases, then a clustering algorithm is used to group them
using the embeddings as features. In this work, we propose Canonicalizing Using
Variational Autoencoders (CUVA), a joint model to learn both embeddings and
cluster assignments in an end-to-end approach, which leads to a better vector
representation for the noun and relation phrases. Our evaluation over multiple
benchmarks shows that CUVA outperforms the existing state-of-the-art
approaches. Moreover, we introduce CanonicNell, a novel dataset to evaluate
entity canonicalization systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models. (arXiv:2102.07988v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07988">
<div class="article-summary-box-inner">
<span><p>Model parallelism has become a necessity for training modern large-scale deep
language models. In this work, we identify a new and orthogonal dimension from
existing model parallel approaches: it is possible to perform pipeline
parallelism within a single training sequence for Transformer-based language
models thanks to its autoregressive property. This enables a more fine-grained
pipeline compared with previous work. With this key idea, we design TeraPipe, a
high-performance token-level pipeline parallel algorithm for synchronous
model-parallel training of Transformer-based language models. We develop a
novel dynamic programming-based algorithm to calculate the optimal pipelining
execution scheme given a specific model and cluster configuration. We show that
TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175
billion parameters on an AWS cluster with 48 p3.16xlarge instances compared
with state-of-the-art model-parallel methods. The code for reproduction can be
found at https://github.com/zhuohan123/terapipe
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving and Simplifying Pattern Exploiting Training. (arXiv:2103.11955v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11955">
<div class="article-summary-box-inner">
<span><p>Recently, pre-trained language models (LMs) have achieved strong performance
when fine-tuned on difficult benchmarks like SuperGLUE. However, performance
can suffer when there are very few labeled examples available for fine-tuning.
Pattern Exploiting Training (PET) is a recent approach that leverages patterns
for few-shot learning. However, PET uses task-specific unlabeled data. In this
paper, we focus on few-shot learning without any unlabeled data and introduce
ADAPET, which modifies PET's objective to provide denser supervision during
fine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any
task-specific unlabeled data. Our code can be found at
https://github.com/rrmenon10/ADAPET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intent Recognition and Unsupervised Slot Identification for Low Resourced Spoken Dialog Systems. (arXiv:2104.01287v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01287">
<div class="article-summary-box-inner">
<span><p>Intent Recognition and Slot Identification are crucial components in spoken
language understanding (SLU) systems. In this paper, we present a novel
approach towards both these tasks in the context of low resourced and unwritten
languages. We present an acoustic based SLU system that converts speech to its
phonetic transcription using a universal phone recognition system. We build a
word-free natural language understanding module that does intent recognition
and slot identification from these phonetic transcription. Our proposed SLU
system performs competitively for resource rich scenarios and significantly
outperforms existing approaches as the amount of available data reduces. We
observe more than 10% improvement for intent classification in Tamil and more
than 5% improvement for intent classification in Sinhala. We also present a
novel approach towards unsupervised slot identification using normalized
attention scores. This approach can be used for unsupervised slot labelling,
data augmentation and to generate data for a new slot in a one-shot way with
only one speech recording
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hidden Backdoors in Human-Centric Language Models. (arXiv:2105.00164v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00164">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) systems have been proven to be vulnerable
to backdoor attacks, whereby hidden features (backdoors) are trained into a
language model and may only be activated by specific inputs (called triggers),
to trick the model into producing unexpected behaviors. In this paper, we
create covert and natural triggers for textual backdoor attacks, \textit{hidden
backdoors}, where triggers can fool both modern language models and human
inspection. We deploy our hidden backdoors through two state-of-the-art trigger
embedding methods. The first approach via homograph replacement, embeds the
trigger into deep neural networks through the visual spoofing of lookalike
character replacement. The second approach uses subtle differences between text
generated by language models and real natural text to produce trigger sentences
with correct grammar and high fluency. We demonstrate that the proposed hidden
backdoors can be effective across three downstream security-critical NLP tasks,
representative of modern human-centric NLP systems, including toxic comment
detection, neural machine translation (NMT), and question answering (QA). Our
two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at
least $97\%$ with an injection rate of only $3\%$ in toxic comment detection,
$95.1\%$ ASR in NMT with less than $0.5\%$ injected data, and finally $91.12\%$
ASR against QA updated with only 27 poisoning data samples on a model
previously trained with 92,024 samples (0.029\%). We are able to demonstrate
the adversary's high success rate of attacks, while maintaining functionality
for regular users, with triggers inconspicuous by the human administrators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking. (arXiv:2107.05002v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05002">
<div class="article-summary-box-inner">
<span><p>Extractive Reading Comprehension (ERC) has made tremendous advances enabled
by the availability of large-scale high-quality ERC training data. Despite of
such rapid progress and widespread application, the datasets in languages other
than high-resource languages such as English remain scarce. To address this
issue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by
modelling existing high-quality extractive reading comprehension datasets in a
multilingual environment. To be specific, we present multilingual adaptive
attention (MAA) to combine intra-attention and inter-attention to learn more
general generalizable semantic and lexical knowledge from each pair of language
families. Furthermore, to make full use of existing datasets, we adopt a new
training framework to train our model by calculating task-level similarities
between each existing dataset and target dataset. The experimental results show
that our XLTT model surpasses six baselines on two multilingual ERC benchmarks,
especially more effective for low-resource languages with 3.9 and 4.1 average
improvement in F1 and EM, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arabic aspect based sentiment analysis using BERT. (arXiv:2107.13290v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13290">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that
defines the polarity of opinions on certain aspects related to specific
targets. The majority of research on ABSA is in English, with a small amount of
work available in Arabic. Most previous Arabic research has relied on deep
learning models that depend primarily on context-independent word embeddings
(e.g.word2vec), where each word has a fixed representation independent of its
context. This article explores the modeling capabilities of contextual
embeddings from pre-trained language models, such as BERT, and making use of
sentence pair input on Arabic ABSA tasks. In particular, we are building a
simple but effective BERT-based neural baseline to handle this task. Our BERT
architecture with a simple linear classification layer surpassed the
state-of-the-art works, according to the experimental results on the
benchmarked Arabic hotel reviews dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeqScore: Addressing Barriers to Reproducible Named Entity Recognition Evaluation. (arXiv:2107.14154v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14154">
<div class="article-summary-box-inner">
<span><p>To address a looming crisis of unreproducible evaluation for named entity
recognition, we propose guidelines and introduce SeqScore, a software package
to improve reproducibility. The guidelines we propose are extremely simple and
center around transparency regarding how chunks are encoded and scored. We
demonstrate that despite the apparent simplicity of NER evaluation, unreported
differences in the scoring procedure can result in changes to scores that are
both of noticeable magnitude and statistically significant. We describe
SeqScore, which addresses many of the issues that cause replication failures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HeadlineCause: A Dataset of News Headlines for Detecting Causalities. (arXiv:2108.12626v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12626">
<div class="article-summary-box-inner">
<span><p>Detecting implicit causal relations in texts is a task that requires both
common sense and world knowledge. Existing datasets are focused either on
commonsense causal reasoning or explicit causal relations. In this work, we
present HeadlineCause, a dataset for detecting implicit causal relations
between pairs of news headlines. The dataset includes over 5000 headline pairs
from English news and over 9000 headline pairs from Russian news labeled
through crowdsourcing. The pairs vary from totally unrelated or belonging to
the same general topic to the ones including causation and refutation
relations. We also present a set of models and experiments that demonstrates
the dataset validity, including a multilingual XLM-RoBERTa based model for
causality detection and a GPT-2 based model for possible effects prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05473">
<div class="article-summary-box-inner">
<span><p>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by
learning with merely a handful of annotated instances. Meta-learning has been
widely adopted for such a task, which trains on randomly generated few-shot
tasks to learn generic data representations. Despite impressive results
achieved, existing models still perform suboptimally when handling hard FSRE
tasks, where the relations are fine-grained and similar to each other. We argue
this is largely because existing models do not distinguish hard tasks from easy
ones in the learning process. In this paper, we introduce a novel approach
based on contrastive learning that learns better representations by exploiting
relation label information. We further design a method that allows the model to
adaptively learn how to focus on hard tasks. Experiments on two standard
datasets demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Something Old, Something New: Grammar-based CCG Parsing with Transformer Models. (arXiv:2109.10044v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10044">
<div class="article-summary-box-inner">
<span><p>This report describes the parsing problem for Combinatory Categorial Grammar
(CCG), showing how a combination of Transformer-based neural models and a
symbolic CCG grammar can lead to substantial gains over existing approaches.
The report also documents a 20-year research program, showing how NLP methods
have evolved over this time. The staggering accuracy improvements provided by
neural models for CCG parsing can be seen as a reflection of the improvements
seen in NLP more generally. The report provides a minimal introduction to CCG
and CCG parsing, with many pointers to the relevant literature. It then
describes the CCG supertagging problem, and some recent work from Tian et al.
(2020) which applies Transformer-based models to supertagging with great
effect. I use this existing model to develop a CCG multitagger, which can serve
as a front-end to an existing CCG parser. Simply using this new multitagger
provides substantial gains in parsing accuracy. I then show how a
Transformer-based model from the parsing literature can be combined with the
grammar-based CCG parser, setting a new state-of-the-art for the CCGbank
parsing task of almost 93% F-score for labelled dependencies, with complete
sentence accuracies of over 50%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recursively Summarizing Books with Human Feedback. (arXiv:2109.10862v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10862">
<div class="article-summary-box-inner">
<span><p>A major challenge for scaling machine learning is training models to perform
tasks that are very difficult or time-consuming for humans to evaluate. We
present progress on this problem on the task of abstractive summarization of
entire fiction novels. Our method combines learning from human feedback with
recursive task decomposition: we use models trained on smaller parts of the
task to assist humans in giving feedback on the broader task. We collect a
large volume of demonstrations and comparisons from human labelers, and
fine-tune GPT-3 using behavioral cloning and reward modeling to do
summarization recursively. At inference time, the model first summarizes small
sections of the book and then recursively summarizes these summaries to produce
a summary of the entire book. Our human labelers are able to supervise and
evaluate the models quickly, despite not having read the entire books
themselves. Our resulting model generates sensible summaries of entire books,
even matching the quality of human-written summaries in a few cases ($\sim5\%$
of books). We achieve state-of-the-art results on the recent BookSum dataset
for book-length summarization. A zero-shot question-answering model using these
summaries achieves state-of-the-art results on the challenging NarrativeQA
benchmark for answering questions about books and movie scripts. We release
datasets of samples from our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AES Systems Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (arXiv:2109.11728v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11728">
<div class="article-summary-box-inner">
<span><p>Deep-learning based Automatic Essay Scoring (AES) systems are being actively
used by states and language testing agencies alike to evaluate millions of
candidates for life-changing decisions ranging from college applications to
visa approvals. However, little research has been put to understand and
interpret the black-box nature of deep-learning based scoring algorithms.
Previous studies indicate that scoring models can be easily fooled. In this
paper, we explore the reason behind their surprising adversarial brittleness.
We utilize recent advances in interpretability to find the extent to which
features such as coherence, content, vocabulary, and relevance are important
for automated scoring mechanisms. We use this to investigate the
oversensitivity i.e., large change in output score with a little change in
input essay content) and overstability i.e., little change in output scores
with large changes in input essay content) of AES. Our results indicate that
autoscoring models, despite getting trained as "end-to-end" models with rich
contextual embeddings such as BERT, behave like bag-of-words models. A few
words determine the essay score without the requirement of any context making
the model largely overstable. This is in stark contrast to recent probing
studies on pre-trained representation learning models, which show that rich
linguistic features such as parts-of-speech and morphology are encoded by them.
Further, we also find that the models have learnt dataset biases, making them
oversensitive. To deal with these issues, we propose detection-based protection
models that can detect oversensitivity and overstability causing samples with
high accuracies. We find that our proposed models are able to detect unusual
attribution patterns and flag adversarial samples successfully.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts. (arXiv:2109.12761v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12761">
<div class="article-summary-box-inner">
<span><p>In order to better simulate the real human conversation process, models need
to generate dialogue utterances based on not only preceding textual contexts
but also visual contexts. However, with the development of multi-modal dialogue
learning, the dataset scale gradually becomes a bottleneck. In this report, we
release OpenViDial 2.0, a larger-scale open-domain multi-modal dialogue dataset
compared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a
total number of 5.6 million dialogue turns extracted from either movies or TV
series from different resources, and each dialogue turn is paired with its
corresponding visual context. We hope this large-scale dataset can help
facilitate future researches on open-domain multi-modal dialog generation,
e.g., multi-modal pretraining for dialogue generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations. (arXiv:2109.13059v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13059">
<div class="article-summary-box-inner">
<span><p>In NLP, a large volume of tasks involve pairwise comparison between two
sequences (e.g. sentence similarity and paraphrase identification).
Predominantly, two formulations are used for sentence-pair tasks: bi-encoders
and cross-encoders. Bi-encoders produce fixed-dimensional sentence
representations and are computationally efficient, however, they usually
underperform cross-encoders. Cross-encoders can leverage their attention heads
to exploit inter-sentence interactions for better performance but they require
task fine-tuning and are computationally more expensive. In this paper, we
present a completely unsupervised sentence representation model termed as
Trans-Encoder that combines the two learning paradigms into an iterative joint
framework to simultaneously learn enhanced bi- and cross-encoders.
Specifically, on top of a pre-trained Language Model (PLM), we start with
converting it to an unsupervised bi-encoder, and then alternate between the bi-
and cross-encoder task formulations. In each alternation, one task formulation
will produce pseudo-labels which are used as learning signals for the other
task formulation. We then propose an extension to conduct such
self-distillation approach on multiple PLMs in parallel and use the average of
their pseudo-labels for mutual-distillation. Trans-Encoder creates, to the best
of our knowledge, the first completely unsupervised cross-encoder and also a
state-of-the-art unsupervised bi-encoder for sentence similarity. Both the
bi-encoder and cross-encoder formulations of Trans-Encoder outperform recently
proposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT
and SimCSE by up to 5% on the sentence similarity benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prefix-to-SQL: Text-to-SQL Generation from Incomplete User Questions. (arXiv:2109.13066v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13066">
<div class="article-summary-box-inner">
<span><p>Existing text-to-SQL research only considers complete questions as the input,
but lay-users might strive to formulate a complete question. To build a smarter
natural language interface to database systems (NLIDB) that also processes
incomplete questions, we propose a new task, prefix-to-SQL which takes question
prefix from users as the input and predicts the intended SQL. We construct a
new benchmark called PAGSAS that contains 124K user question prefixes and the
intended SQL for 5 sub-tasks Advising, GeoQuery, Scholar, ATIS, and Spider.
Additionally, we propose a new metric SAVE to measure how much effort can be
saved by users. Experimental results show that PAGSAS is challenging even for
strong baseline models such as T5. As we observe the difficulty of
prefix-to-SQL is related to the number of omitted tokens, we incorporate
curriculum learning of feeding examples with an increasing number of omitted
tokens. This improves scores on various sub-tasks by as much as 9% recall
scores on sub-task GeoQuery in PAGSAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Generation of Word Problems for Academic Education via Natural Language Processing (NLP). (arXiv:2109.13123v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13123">
<div class="article-summary-box-inner">
<span><p>Digital learning platforms enable students to learn on a flexible and
individual schedule as well as providing instant feedback mechanisms. The field
of STEM education requires students to solve numerous training exercises to
grasp underlying concepts. It is apparent that there are restrictions in
current online education in terms of exercise diversity and individuality. Many
exercises show little variance in structure and content, hindering the adoption
of abstraction capabilities by students. This thesis proposes an approach to
generate diverse, context rich word problems. In addition to requiring the
generated language to be grammatically correct, the nature of word problems
implies additional constraints on the validity of contents. The proposed
approach is proven to be effective in generating valid word problems for
mathematical statistics. The experimental results present a tradeoff between
generation time and exercise validity. The system can easily be parametrized to
handle this tradeoff according to the requirements of specific use cases.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Domain Shift on Left and Right Ventricle Segmentation in Short Axis Cardiac MR Images. (arXiv:2109.13230v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13230">
<div class="article-summary-box-inner">
<span><p>Domain shift refers to the difference in the data distribution of two
datasets, normally between the training set and the test set for machine
learning algorithms. Domain shift is a serious problem for generalization of
machine learning models and it is well-established that a domain shift between
the training and test sets may cause a drastic drop in the model's performance.
In medical imaging, there can be many sources of domain shift such as different
scanners or scan protocols, different pathologies in the patient population,
anatomical differences in the patient population (e.g. men vs women) etc.
Therefore, in order to train models that have good generalization performance,
it is important to be aware of the domain shift problem, its potential causes
and to devise ways to address it. In this paper, we study the effect of domain
shift on left and right ventricle blood pool segmentation in short axis cardiac
MR images. Our dataset contains short axis images from 4 different MR scanners
and 3 different pathology groups. The training is performed with nnUNet. The
results show that scanner differences cause a greater drop in performance
compared to changing the pathology group, and that the impact of domain shift
is greater on right ventricle segmentation compared to left ventricle
segmentation. Increasing the number of training subjects increased
cross-scanner performance more than in-scanner performance at small training
set sizes, but this difference in improvement decreased with larger training
set sizes. Training models using data from multiple scanners improved
cross-domain performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DOODLER: Determining Out-Of-Distribution Likelihood from Encoder Reconstructions. (arXiv:2109.13237v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13237">
<div class="article-summary-box-inner">
<span><p>Deep Learning models possess two key traits that, in combination, make their
use in the real world a risky prospect. One, they do not typically generalize
well outside of the distribution for which they were trained, and two, they
tend to exhibit confident behavior regardless of whether or not they are
producing meaningful outputs. While Deep Learning possesses immense power to
solve realistic, high-dimensional problems, these traits in concert make it
difficult to have confidence in their real-world applications. To overcome this
difficulty, the task of Out-Of-Distribution (OOD) Detection has been defined,
to determine when a model has received an input from outside of the
distribution for which it is trained to operate.
</p>
<p>This paper introduces and examines a novel methodology, DOODLER, for OOD
Detection, which directly leverages the traits which result in its necessity.
By training a Variational Auto-Encoder (VAE) on the same data as another Deep
Learning model, the VAE learns to accurately reconstruct In-Distribution (ID)
inputs, but not to reconstruct OOD inputs, meaning that its failure state can
be used to perform OOD Detection. Unlike other work in the area, DOODLER
requires only very weak assumptions about the existence of an OOD dataset,
allowing for more realistic application. DOODLER also enables pixel-wise
segmentations of input images by OOD likelihood, and experimental results show
that it matches or outperforms methodologies that operate under the same
constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually Grounded Reasoning across Languages and Cultures. (arXiv:2109.13238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13238">
<div class="article-summary-box-inner">
<span><p>The design of widespread vision-and-language datasets and pre-trained
encoders directly adopts, or draws inspiration from, the concepts and images of
ImageNet. While one can hardly overestimate how much this benchmark contributed
to progress in computer vision, it is mostly derived from lexical databases and
image queries in English, resulting in source material with a North American or
Western European bias. Therefore, we devise a new protocol to construct an
ImageNet-style hierarchy representative of more languages and cultures. In
particular, we let the selection of both concepts and images be entirely driven
by native speakers, rather than scraping them automatically. Specifically, we
focus on a typologically diverse set of languages, namely, Indonesian, Mandarin
Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images
obtained through this new protocol, we create a multilingual dataset for
{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting
statements from native speaker annotators about pairs of images. The task
consists of discriminating whether each grounded statement is true or false. We
establish a series of baselines using state-of-the-art models and find that
their cross-lingual transfer performance lags dramatically behind supervised
performance in English. These results invite us to reassess the robustness and
accuracy of current state-of-the-art models beyond a narrow domain, but also
open up new exciting challenges for the development of truly multilingual and
multicultural systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Urban Driver: Learning to Drive from Real-world Demonstrations Using Policy Gradients. (arXiv:2109.13333v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13333">
<div class="article-summary-box-inner">
<span><p>In this work we are the first to present an offline policy gradient method
for learning imitative policies for complex urban driving from a large corpus
of real-world demonstrations. This is achieved by building a differentiable
data-driven simulator on top of perception outputs and high-fidelity HD maps of
the area. It allows us to synthesize new driving experiences from existing
demonstrations using mid-level representations. Using this simulator we then
train a policy network in closed-loop employing policy gradients. We train our
proposed method on 100 hours of expert demonstrations on urban roads and show
that it learns complex driving policies that generalize well and can perform a
variety of driving maneuvers. We demonstrate this in simulation as well as
deploy our model to self-driving vehicles in the real-world. Our method
outperforms previously demonstrated state-of-the-art for urban driving
scenarios -- all this without the need for complex state perturbations or
collecting additional on-policy data during training. We make code and data
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Computer Vision on Edge Devices with Pipeline-Parallel Hierarchical Neural Networks. (arXiv:2109.13356v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13356">
<div class="article-summary-box-inner">
<span><p>Computer vision on low-power edge devices enables applications including
search-and-rescue and security. State-of-the-art computer vision algorithms,
such as Deep Neural Networks (DNNs), are too large for inference on low-power
edge devices. To improve efficiency, some existing approaches parallelize DNN
inference across multiple edge devices. However, these techniques introduce
significant communication and synchronization overheads or are unable to
balance workloads across devices. This paper demonstrates that the hierarchical
DNN architecture is well suited for parallel processing on multiple edge
devices. We design a novel method that creates a parallel inference pipeline
for computer vision problems that use hierarchical DNNs. The method balances
loads across the collaborating devices and reduces communication costs to
facilitate the processing of multiple video frames simultaneously with higher
throughput. Our experiments consider a representative computer vision problem
where image recognition is performed on each video frame, running on multiple
Raspberry Pi 4Bs. With four collaborating low-power edge devices, our approach
achieves 3.21X higher throughput, 68% less energy consumption per device per
frame, and 58% decrease in memory when compared with existing single-device
hierarchical DNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WarpedGANSpace: Finding non-linear RBF paths in GAN latent space. (arXiv:2109.13357v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13357">
<div class="article-summary-box-inner">
<span><p>This work addresses the problem of discovering, in an unsupervised manner,
interpretable paths in the latent space of pretrained GANs, so as to provide an
intuitive and easy way of controlling the underlying generative factors. In
doing so, it addresses some of the limitations of the state-of-the-art works,
namely, a) that they discover directions that are independent of the latent
code, i.e., paths that are linear, and b) that their evaluation relies either
on visual inspection or on laborious human labeling. More specifically, we
propose to learn non-linear warpings on the latent space, each one parametrized
by a set of RBF-based latent space warping functions, and where each warping
gives rise to a family of non-linear paths via the gradient of the function.
Building on the work of Voynov and Babenko, that discovers linear paths, we
optimize the trainable parameters of the set of RBFs, so as that images that
are generated by codes along different paths, are easily distinguishable by a
discriminator network. This leads to easily distinguishable image
transformations, such as pose and facial expressions in facial images. We show
that linear paths can be derived as a special case of our method, and show
experimentally that non-linear paths in the latent space lead to steeper, more
disentangled and interpretable changes in the image space than in state-of-the
art methods, both qualitatively and quantitatively. We make the code and the
pretrained models publicly available at:
https://github.com/chi0tzp/WarpedGANSpace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IGAN: Inferent and Generative Adversarial Networks. (arXiv:2109.13360v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13360">
<div class="article-summary-box-inner">
<span><p>I present IGAN (Inferent Generative Adversarial Networks), a neural
architecture that learns both a generative and an inference model on a complex
high dimensional data distribution, i.e. a bidirectional mapping between data
samples and a simpler low-dimensional latent space. It extends the traditional
GAN framework with inference by rewriting the adversarial strategy in both the
image and the latent space with an entangled game between data-latent encoded
posteriors and priors. It brings a measurable stability and convergence to the
classical GAN scheme, while keeping its generative quality and remaining simple
and frugal in order to run on a lab PC. IGAN fosters the encoded latents to
span the full prior space: this enables the exploitation of an enlarged and
self-organised latent space in an unsupervised manner. An analysis of
previously published articles sets the theoretical ground for the proposed
algorithm. A qualitative demonstration of potential applications like
self-supervision or multi-modal data translation is given on common image
datasets including SAR and optical imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D. (arXiv:2109.13410v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13410">
<div class="article-summary-box-inner">
<span><p>For the last few decades, several major subfields of artificial intelligence
including computer vision, graphics, and robotics have progressed largely
independently from each other. Recently, however, the community has realized
that progress towards robust intelligent systems such as self-driving cars
requires a concerted effort across the different fields. This motivated us to
develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a
suburban driving dataset which comprises richer input modalities, comprehensive
semantic instance annotations and accurate localization to facilitate research
at the intersection of vision, graphics and robotics. For efficient annotation,
we created a tool to label 3D scenes with bounding primitives and developed a
model that transfers this information into the 2D image domain, resulting in
over 150k semantic and instance annotated images and 1B annotated 3D points.
Moreover, we established benchmarks and baselines for several tasks relevant to
mobile perception, encompassing problems from computer vision, graphics, and
robotics on the same dataset. KITTI-360 will enable progress at the
intersection of these research areas and thus contributing towards solving one
of our grand challenges: the development of fully autonomous self-driving
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminative Attribution from Counterfactuals. (arXiv:2109.13412v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13412">
<div class="article-summary-box-inner">
<span><p>We present a method for neural network interpretability by combining feature
attribution with counterfactual explanations to generate attribution maps that
highlight the most discriminative features between pairs of classes. We show
that this method can be used to quantitatively evaluate the performance of
feature attribution methods in an objective manner, thus preventing potential
observer bias. We evaluate the proposed method on three diverse datasets,
including a challenging artificial dataset and real-world biological data. We
show quantitatively and qualitatively that the highlighted features are
substantially more discriminative than those extracted using conventional
attribution methods and argue that this type of explanation is better suited
for understanding fine grained class differences as learned by a deep neural
network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Deep Neural Network Domain Adaptation Techniques for Image Recognition. (arXiv:2109.13420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13420">
<div class="article-summary-box-inner">
<span><p>It has been well proved that deep networks are efficient at extracting
features from a given (source) labeled dataset. However, it is not always the
case that they can generalize well to other (target) datasets which very often
have a different underlying distribution. In this report, we evaluate four
different domain adaptation techniques for image classification tasks:
DeepCORAL, DeepDomainConfusion, CDAN and CDAN+E. These techniques are
unsupervised given that the target dataset dopes not carry any labels during
training phase. We evaluate model performance on the office-31 dataset. A link
to the github repository of this report can be found here:
https://github.com/agrija9/Deep-Unsupervised-Domain-Adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Keypoint Discovery. (arXiv:2109.13423v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13423">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a method for keypoint discovery from a 2D image
using image-level supervision. Recent works on unsupervised keypoint discovery
reliably discover keypoints of aligned instances. However, when the target
instances have high viewpoint or appearance variation, the discovered keypoints
do not match the semantic correspondences over different images. Our work aims
to discover keypoints even when the target instances have high viewpoint and
appearance variation by using image-level supervision. Motivated by the
weakly-supervised learning approach, our method exploits image-level
supervision to identify discriminative parts and infer the viewpoint of the
target instance. To discover diverse parts, we adopt a conditional image
generation approach using a pair of images with structural deformation.
Finally, we enforce a viewpoint-based equivariance constraint using the
keypoints from the image-level supervision to resolve the spatial correlation
problem that consistently appears in the images taken from various viewpoints.
Our approach achieves state-of-the-art performance for the task of keypoint
estimation on the limited supervision scenarios. Furthermore, the discovered
keypoints are directly applicable to downstream tasks without requiring any
keypoint labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Warp-Refine Propagation: Semi-Supervised Auto-labeling via Cycle-consistency. (arXiv:2109.13432v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13432">
<div class="article-summary-box-inner">
<span><p>Deep learning models for semantic segmentation rely on expensive,
large-scale, manually annotated datasets. Labelling is a tedious process that
can take hours per image. Automatically annotating video sequences by
propagating sparsely labeled frames through time is a more scalable
alternative. In this work, we propose a novel label propagation method, termed
Warp-Refine Propagation, that combines semantic cues with geometric cues to
efficiently auto-label videos. Our method learns to refine geometrically-warped
labels and infuse them with learned semantic priors in a semi-supervised
setting by leveraging cycle consistency across time. We quantitatively show
that our method improves label-propagation by a noteworthy margin of 13.1 mIoU
on the ApolloScape dataset. Furthermore, by training with the auto-labelled
frames, we achieve competitive results on three semantic-segmentation
benchmarks, improving the state-of-the-art by a large margin of 1.8 and 3.61
mIoU on NYU-V2 and KITTI, while matching the current best results on
Cityscapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Which Out-Of-Distribution Object Orientations Are DNNs Capable of Generalizing?. (arXiv:2109.13445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13445">
<div class="article-summary-box-inner">
<span><p>The capability of Deep Neural Networks (DNNs) to recognize objects in
orientations outside the distribution of the training data, ie.
out-of-distribution (OoD) orientations, is not well understood. For humans,
behavioral studies showed that recognition accuracy varies across OoD
orientations, where generalization is much better for some orientations than
for others. In contrast, for DNNs, it remains unknown how generalization
abilities are distributed among OoD orientations. In this paper, we investigate
the limitations of DNNs' generalization capacities by systematically inspecting
patterns of success and failure of DNNs across OoD orientations. We use an
intuitive and controlled, yet challenging learning paradigm, in which some
instances of an object category are seen at only a few geometrically restricted
orientations, while other instances are seen at all orientations. The effect of
data diversity is also investigated by increasing the number of instances seen
at all orientations in the training set. We present a comprehensive analysis of
DNNs' generalization abilities and limitations for representative architectures
(ResNet, Inception, DenseNet and CORnet). Our results reveal an intriguing
pattern -- DNNs are only capable of generalizing to instances of objects that
appear like 2D, ie. in-plane, rotations of in-distribution orientations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When in Doubt: Improving Classification Performance with Alternating Normalization. (arXiv:2109.13449v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13449">
<div class="article-summary-box-inner">
<span><p>We introduce Classification with Alternating Normalization (CAN), a
non-parametric post-processing step for classification. CAN improves
classification accuracy for challenging examples by re-adjusting their
predicted class probability distribution using the predicted class
distributions of high-confidence validation examples. CAN is easily applicable
to any probabilistic classifier, with minimal computation overhead. We analyze
the properties of CAN using simulated experiments, and empirically demonstrate
its effectiveness across a diverse set of classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SiamEvent: Event-based Object Tracking via Edge-aware Similarity Learning with Siamese Networks. (arXiv:2109.13456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13456">
<div class="article-summary-box-inner">
<span><p>Event cameras are novel sensors that perceive the per-pixel intensity changes
and output asynchronous event streams, showing lots of advantages over
traditional cameras, such as high dynamic range (HDR) and no motion blur. It
has been shown that events alone can be used for object tracking by motion
compensation or prediction. However, existing methods assume that the target
always moves and is the stand-alone object. Moreover, they fail to track the
stopped non-independent moving objects on fixed scenes. In this paper, we
propose a novel event-based object tracking framework, called SiamEvent, using
Siamese networks via edge-aware similarity learning. Importantly, to find the
part having the most similar edge structure of target, we propose to correlate
the embedded events at two timestamps to compute the target edge similarity.
The Siamese network enables tracking arbitrary target edge by finding the part
with the highest similarity score. This extends the possibility of event-based
object tracking applied not only for the independent stand-alone moving
objects, but also for various settings of the camera and scenes. In addition,
target edge initialization and edge detector are also proposed to prevent
SiamEvent from the drifting problem. Lastly, we built an open dataset including
various synthetic and real scenes to train and evaluate SiamEvent. Extensive
experiments demonstrate that SiamEvent achieves up to 15% tracking performance
enhancement than the baselines on the real-world scenes and more robust
tracking performance in the challenging HDR and motion blur conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delve into the Performance Degradation of Differentiable Architecture Search. (arXiv:2109.13466v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13466">
<div class="article-summary-box-inner">
<span><p>Differentiable architecture search (DARTS) is widely considered to be easy to
overfit the validation set which leads to performance degradation. We first
employ a series of exploratory experiments to verify that neither high-strength
architecture parameters regularization nor warmup training scheme can
effectively solve this problem. Based on the insights from the experiments, we
conjecture that the performance of DARTS does not depend on the well-trained
supernet weights and argue that the architecture parameters should be trained
by the gradients which are obtained in the early stage rather than the final
stage of training. This argument is then verified by exchanging the learning
rate schemes of weights and parameters. Experimental results show that the
simple swap of the learning rates can effectively solve the degradation and
achieve competitive performance. Further empirical evidence suggests that the
degradation is not a simple problem of the validation set overfitting but
exhibit some links between the degradation and the operation selection bias
within bilevel optimization dynamics. We demonstrate the generalization of this
bias and propose to utilize this bias to achieve an operation-magnitude-based
selective stop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metal Artifact Reduction in 2D CT Images with Self-supervised Cross-domain Learning. (arXiv:2109.13483v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13483">
<div class="article-summary-box-inner">
<span><p>The presence of metallic implants often introduces severe metal artifacts in
the X-ray CT images, which could adversely influence clinical diagnosis or dose
calculation in radiation therapy. In this work, we present a novel
deep-learning-based approach for metal artifact reduction (MAR). In order to
alleviate the need for anatomically identical CT image pairs (i.e., metal
artifact-corrupted CT image and metal artifact-free CT image) for network
learning, we propose a self-supervised cross-domain learning framework.
Specifically, we train a neural network to restore the metal trace region
values in the given metal-free sinogram, where the metal trace is identified by
the forward projection of metal masks. We then design a novel FBP
reconstruction loss to encourage the network to generate more perfect
completion results and a residual-learning-based image refinement module to
reduce the secondary artifacts in the reconstructed CT images. To preserve the
fine structure details and fidelity of the final MAR image, instead of directly
adopting CNN-refined images as output, we incorporate the metal trace
replacement into our framework and replace the metal-affected projections of
the original sinogram with the prior sinogram generated by the forward
projection of the CNN output. We then use the filtered backward projection
(FBP) algorithms for final MAR image reconstruction. We conduct an extensive
evaluation on simulated and real artifact data to show the effectiveness of our
design. Our method produces superior MAR results and outperforms other
compelling methods. We also demonstrate the potential of our framework for
other organ sites.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Rotation Invariance in Object Detection. (arXiv:2109.13488v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13488">
<div class="article-summary-box-inner">
<span><p>Rotation augmentations generally improve a model's invariance/equivariance to
rotation - except in object detection. In object detection the shape is not
known, therefore rotation creates a label ambiguity. We show that the de-facto
method for bounding box label rotation, the Largest Box Method, creates very
large labels, leading to poor performance and in many cases worse performance
than using no rotation at all. We propose a new method of rotation augmentation
that can be implemented in a few lines of code. First, we create a
differentiable approximation of label accuracy and show that axis-aligning the
bounding box around an ellipse is optimal. We then introduce Rotation
Uncertainty (RU) Loss, allowing the model to adapt to the uncertainty of the
labels. On five different datasets (including COCO, PascalVOC, and Transparent
Object Bin Picking), this approach improves the rotational invariance of both
one-stage and two-stage architectures when measured with AP, AP50, and AP75.
The code is available at \url{https://github.com/akasha-imaging/ICCV2021}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modelling Neighbor Relation in Joint Space-Time Graph for Video Correspondence Learning. (arXiv:2109.13499v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13499">
<div class="article-summary-box-inner">
<span><p>This paper presents a self-supervised method for learning reliable visual
correspondence from unlabeled videos. We formulate the correspondence as
finding paths in a joint space-time graph, where nodes are grid patches sampled
from frames, and are linked by two types of edges: (i) neighbor relations that
determine the aggregation strength from intra-frame neighbors in space, and
(ii) similarity relations that indicate the transition probability of
inter-frame paths across time. Leveraging the cycle-consistency in videos, our
contrastive learning objective discriminates dynamic objects from both their
neighboring views and temporal views. Compared with prior works, our approach
actively explores the neighbor relations of central instances to learn a latent
association between center-neighbor pairs (e.g., "hand -- arm") across time,
thus improving the instance discrimination. Without fine-tuning, our learned
representation outperforms the state-of-the-art self-supervised methods on a
variety of visual tasks including video object propagation, part propagation,
and pose keypoint tracking. Our self-supervised method also surpasses some
fully supervised algorithms designed for the specific tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Shapelet Transform: A new approach for time series shapelets. (arXiv:2109.13514v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13514">
<div class="article-summary-box-inner">
<span><p>Shapelet-based algorithms are widely used for time series classification
because of their ease of interpretation, but they are currently outperformed,
notably by methods using convolutional kernels, capable of reaching
state-of-the-art performance while being highly scalable. We present a new
formulation of time series shapelets including the notion of dilation, and a
shapelet extraction method based on convolutional kernels, which is able to
target the discriminant information identified by convolutional kernels.
Experiments performed on 108 datasets show that our method improves on the
state-of-the-art for shapelet algorithms, and we show that it can be used to
interpret results from convolutional kernels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Semantic Image Recognition Model and Evaluating Index for explaining the deep learning models. (arXiv:2109.13531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13531">
<div class="article-summary-box-inner">
<span><p>Although deep learning models are powerful among various applications, most
deep learning models are still a black box, lacking verifiability and
interpretability, which means the decision-making process that human beings
cannot understand. Therefore, how to evaluate deep neural networks with
explanations is still an urgent task. In this paper, we first propose a
multi-semantic image recognition model, which enables human beings to
understand the decision-making process of the neural network. Then, we presents
a new evaluation index, which can quantitatively assess the model
interpretability. We also comprehensively summarize the semantic information
that affects the image classification results in the judgment process of neural
networks. Finally, this paper also exhibits the relevant baseline performance
with current state-of-the-art deep learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A hierarchical residual network with compact triplet-center loss for sketch recognition. (arXiv:2109.13536v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13536">
<div class="article-summary-box-inner">
<span><p>With the widespread use of touch-screen devices, it is more and more
convenient for people to draw sketches on screen. This results in the demand
for automatically understanding the sketches. Thus, the sketch recognition task
becomes more significant than before. To accomplish this task, it is necessary
to solve the critical issue of improving the distinction of the sketch
features. To this end, we have made efforts in three aspects. First, a novel
multi-scale residual block is designed. Compared with the conventional basic
residual block, it can better perceive multi-scale information and reduce the
number of parameters during training. Second, a hierarchical residual structure
is built by stacking multi-scale residual blocks in a specific way. In contrast
with the single-level residual structure, the learned features from this
structure are more sufficient. Last but not least, the compact triplet-center
loss is proposed specifically for the sketch recognition task. It can solve the
problem that the triplet-center loss does not fully consider too large
intra-class space and too small inter-class space in sketch field. By studying
the above modules, a hierarchical residual network as a whole is proposed for
sketch recognition and evaluated on Tu-Berlin benchmark thoroughly. The
experimental results show that the proposed network outperforms most of
baseline methods and it is excellent among non-sequential models at present.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Strong Baseline for the VIPriors Data-Efficient Image Classification Challenge. (arXiv:2109.13561v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13561">
<div class="article-summary-box-inner">
<span><p>Learning from limited amounts of data is the hallmark of intelligence,
requiring strong generalization and abstraction skills. In a machine learning
context, data-efficient methods are of high practical importance since data
collection and annotation are prohibitively expensive in many domains. Thus,
coordinated efforts to foster progress in this area emerged recently, e.g., in
the form of dedicated workshops and competitions. Besides a common benchmark,
measuring progress requires strong baselines. We present such a strong baseline
for data-efficient image classification on the VIPriors challenge dataset,
which is a sub-sampled version of ImageNet-1k with 100 images per class. We do
not use any methods tailored to data-efficient classification but only standard
models and techniques as well as common competition tricks and thorough
hyper-parameter tuning. Our baseline achieves 69.7% accuracy on the VIPriors
image classification dataset and outperforms 50% of submissions to the VIPriors
2021 challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Elevation Network for Fast Online Action Detection. (arXiv:2109.13572v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13572">
<div class="article-summary-box-inner">
<span><p>Online action detection (OAD) is a task that receives video segments within a
streaming video as inputs and identifies ongoing actions within them. It is
important to retain past information associated with a current action. However,
long short-term memory (LSTM), a popular recurrent unit for modeling temporal
information from videos, accumulates past information from the previous hidden
and cell states and the extracted visual features at each timestep without
considering the relationships between the past and current information.
Consequently, the forget gate of the original LSTM can lose the accumulated
information relevant to the current action because it determines which
information to forget without considering the current action. We introduce a
novel information elevation unit (IEU) that lifts up and accumulate the past
information relevant to the current action in order to model the past
information that is especially relevant to the current action. To the best of
our knowledge, our IEN is the first attempt that considers the computational
overhead for the practical use of OAD. Through ablation studies, we design an
efficient and effective OAD network using IEUs, called an information elevation
network (IEN). Our IEN uses visual features extracted by a fast action
recognition network taking only RGB frames because extracting optical flows
requires heavy computation overhead. On two OAD benchmark datasets, THUMOS-14
and TVSeries, our IEN outperforms state-of-the-art OAD methods using only RGB
frames. Furthermore, on the THUMOS-14 dataset, our IEN outperforms the
state-of-the-art OAD methods using two-stream features based on RGB frames and
optical flows.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Curiosity Explicit in Vision-based RL. (arXiv:2109.13588v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13588">
<div class="article-summary-box-inner">
<span><p>Vision-based reinforcement learning (RL) is a promising technique to solve
control tasks involving images as the main observation. State-of-the-art RL
algorithms still struggle in terms of sample efficiency, especially when using
image observations. This has led to an increased attention on integrating state
representation learning (SRL) techniques into the RL pipeline. Work in this
field demonstrates a substantial improvement in sample efficiency among other
benefits. However, to take full advantage of this paradigm, the quality of
samples used for training plays a crucial role. More importantly, the diversity
of these samples could affect the sample efficiency of vision-based RL, but
also its generalization capability. In this work, we present an approach to
improve the sample diversity. Our method enhances the exploration capability of
the RL algorithms by taking advantage of the SRL setup. Our experiments show
that the presented approach outperforms the baseline for all tested
environments. These results are most apparent for environments where the
baseline method struggles. Even in simple environments, our method stabilizes
the training, reduces the reward variance and boosts sample efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Global-Local Memory for Real-time Instrument Segmentation of Robotic Surgical Video. (arXiv:2109.13593v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13593">
<div class="article-summary-box-inner">
<span><p>Performing a real-time and accurate instrument segmentation from videos is of
great significance for improving the performance of robotic-assisted surgery.
We identify two important clues for surgical instrument perception, including
local temporal dependency from adjacent frames and global semantic correlation
in long-range duration. However, most existing works perform segmentation
purely using visual cues in a single frame. Optical flow is just used to model
the motion between only two frames and brings heavy computational cost. We
propose a novel dual-memory network (DMNet) to wisely relate both global and
local spatio-temporal knowledge to augment the current features, boosting the
segmentation performance and retaining the real-time prediction capability. We
propose, on the one hand, an efficient local memory by taking the complementary
advantages of convolutional LSTM and non-local mechanisms towards the relating
reception field. On the other hand, we develop an active global memory to
gather the global semantic correlation in long temporal range to current one,
in which we gather the most informative frames derived from model uncertainty
and frame similarity. We have extensively validated our method on two public
benchmark surgical video datasets. Experimental results demonstrate that our
method largely outperforms the state-of-the-art works on segmentation accuracy
while maintaining a real-time speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SafetyNet: Safe planning for real-world self-driving vehicles using machine-learned policies. (arXiv:2109.13602v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13602">
<div class="article-summary-box-inner">
<span><p>In this paper we present the first safe system for full control of
self-driving vehicles trained from human demonstrations and deployed in
challenging, real-world, urban environments. Current industry-standard
solutions use rule-based systems for planning. Although they perform reasonably
well in common scenarios, the engineering complexity renders this approach
incompatible with human-level performance. On the other hand, the performance
of machine-learned (ML) planning solutions can be improved by simply adding
more exemplar data. However, ML methods cannot offer safety guarantees and
sometimes behave unpredictably. To combat this, our approach uses a simple yet
effective rule-based fallback layer that performs sanity checks on an ML
planner's decisions (e.g. avoiding collision, assuring physical feasibility).
This allows us to leverage ML to handle complex situations while still assuring
the safety, reducing ML planner-only collisions by 95%. We train our ML planner
on 300 hours of expert driving demonstrations using imitation learning and
deploy it along with the fallback layer in downtown San Francisco, where it
takes complete control of a real vehicle and navigates a wide variety of
challenging urban driving scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Glaucoma Detection from Digital Fundus Images using Self-ONNs. (arXiv:2109.13604v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13604">
<div class="article-summary-box-inner">
<span><p>Glaucoma leads to permanent vision disability by damaging the optical nerve
that transmits visual images to the brain. The fact that glaucoma does not show
any symptoms as it progresses and cannot be stopped at the later stages, makes
it critical to be diagnosed in its early stages. Although various deep learning
models have been applied for detecting glaucoma from digital fundus images, due
to the scarcity of labeled data, their generalization performance was limited
along with high computational complexity and special hardware requirements. In
this study, compact Self-Organized Operational Neural Networks (Self- ONNs) are
proposed for early detection of glaucoma in fundus images and their performance
is compared against the conventional (deep) Convolutional Neural Networks
(CNNs) over three benchmark datasets: ACRIMA, RIM-ONE, and ESOGU. The
experimental results demonstrate that Self-ONNs not only achieve superior
detection performance but can also significantly reduce the computational
complexity making it a potentially suitable network model for biomedical
datasets especially when the data is scarce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Network Design for Face Video Super-resolution. (arXiv:2109.13626v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13626">
<div class="article-summary-box-inner">
<span><p>Face video super-resolution algorithm aims to reconstruct realistic face
details through continuous input video sequences. However, existing video
processing algorithms usually contain redundant parameters to guarantee
different super-resolution scenes. In this work, we focus on super-resolution
of face areas in original video scenes, while rest areas are interpolated. This
specific super-resolved task makes it possible to cut redundant parameters in
general video super-resolution networks. We construct a dataset consisting
entirely of face video sequences for network training and evaluation, and
conduct hyper-parameter optimization in our experiments. We use three combined
strategies to optimize the network parameters with a simultaneous
train-evaluation method to accelerate optimization process. Results show that
simultaneous train-evaluation method improves the training speed and
facilitates the generation of efficient networks. The generated network can
reduce at least 52.4% parameters and 20.7% FLOPs, achieve better performance on
PSNR, SSIM compared with state-of-art video super-resolution algorithms. When
processing 36x36x1x3 input video frame sequences, the efficient network
provides 47.62 FPS real-time processing performance. We name our proposal as
hyper-parameter optimization for face Video Super-Resolution (HO-FVSR), which
is open-sourced at https://github.com/yphone/efficient-network-for-face-VSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Diffeomorphic Surface Registration and Non-Linear Modelling. (arXiv:2109.13630v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13630">
<div class="article-summary-box-inner">
<span><p>Registration is an essential tool in image analysis. Deep learning based
alternatives have recently become popular, achieving competitive performance at
a faster speed. However, many contemporary techniques are limited to volumetric
representations, despite increased popularity of 3D surface and shape data in
medical image analysis. We propose a one-step registration model for 3D
surfaces that internalises a lower dimensional probabilistic deformation model
(PDM) using conditional variational autoencoders (CVAE). The deformations are
constrained to be diffeomorphic using an exponentiation layer. The one-step
registration model is benchmarked against iterative techniques, trading in a
slightly lower performance in terms of shape fit for a higher compactness. We
experiment with two distance metrics, Chamfer distance (CD) and Sinkhorn
divergence (SD), as specific distance functions for surface data in real-world
registration scenarios. The internalised deformation model is benchmarked
against linear principal component analysis (PCA) achieving competitive results
and improved generalisability from lower dimensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fail-Safe Human Detection for Drones Using a Multi-Modal Curriculum Learning Approach. (arXiv:2109.13666v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13666">
<div class="article-summary-box-inner">
<span><p>Drones are currently being explored for safety-critical applications where
human agents are expected to evolve in their vicinity. In such applications,
robust people avoidance must be provided by fusing a number of sensing
modalities in order to avoid collisions. Currently however, people detection
systems used on drones are solely based on standard cameras besides an emerging
number of works discussing the fusion of imaging and event-based cameras. On
the other hand, radar-based systems provide up-most robustness towards
environmental conditions but do not provide complete information on their own
and have mainly been investigated in automotive contexts, not for drones. In
order to enable the fusion of radars with both event-based and standard
cameras, we present KUL-UAVSAFE, a first-of-its-kind dataset for the study of
safety-critical people detection by drones. In addition, we propose a baseline
CNN architecture with cross-fusion highways and introduce a curriculum learning
strategy for multi-modal data termed SAUL, which greatly enhances the
robustness of the system towards hard RGB failures and provides a significant
gain of 15% in peak F1 score compared to the use of BlackIn, previously
proposed for cross-fusion networks. We demonstrate the real-time performance
and feasibility of the approach by implementing the system in an edge-computing
unit. We release our dataset and additional material in the project home page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion Deblurring with Real Events. (arXiv:2109.13695v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13695">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an end-to-end learning framework for event-based
motion deblurring in a self-supervised manner, where real-world events are
exploited to alleviate the performance degradation caused by data
inconsistency. To achieve this end, optical flows are predicted from events,
with which the blurry consistency and photometric consistency are exploited to
enable self-supervision on the deblurring network with real-world data.
Furthermore, a piece-wise linear motion model is proposed to take into account
motion non-linearities and thus leads to an accurate model for the physical
formation of motion blurs in the real-world scenario. Extensive evaluation on
both synthetic and real motion blur datasets demonstrates that the proposed
algorithm bridges the gap between simulated and real-world motion blurs and
shows remarkable performance for event-based motion deblurring in real-world
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CIDEr-R: Robust Consensus-based Image Description Evaluation. (arXiv:2109.13701v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13701">
<div class="article-summary-box-inner">
<span><p>This paper shows that CIDEr-D, a traditional evaluation metric for image
description, does not work properly on datasets where the number of words in
the sentence is significantly greater than those in the MS COCO Captions
dataset. We also show that CIDEr-D has performance hampered by the lack of
multiple reference sentences and high variance of sentence length. To bypass
this problem, we introduce CIDEr-R, which improves CIDEr-D, making it more
flexible in dealing with datasets with high sentence length variance. We
demonstrate that CIDEr-R is more accurate and closer to human judgment than
CIDEr-D; CIDEr-R is more robust regarding the number of available references.
Our results reveal that using Self-Critical Sequence Training to optimize
CIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized,
the generated captions' length tends to be similar to the reference length.
However, the models also repeat several times the same word to increase the
sentence length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compound eye inspired flat lensless imaging with spatially-coded Voronoi-Fresnel phase. (arXiv:2109.13703v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13703">
<div class="article-summary-box-inner">
<span><p>Lensless cameras are a class of imaging devices that shrink the physical
dimensions to the very close vicinity of the image sensor by integrating flat
optics and computational algorithms. Here we report a flat lensless camera with
spatially-coded Voronoi-Fresnel phase, partly inspired by biological apposition
compound eye, to achieve superior image quality. We propose a design principle
of maximizing the information in optics to facilitate the computational
reconstruction. By introducing a Fourier domain metric, Modulation Transfer
Function volume (MTFv), we devise an optimization framework to guide the
optimal design of the optical element. The resulting Voronoi-Fresnel phase
features an irregular array of quasi-Centroidal Voronoi cells containing a base
first-order Fresnel phase function. We demonstrate and verify the imaging
performance with a prototype Voronoi-Fresnel lensless camera on a 1.6-megapixel
image sensor in various illumination conditions. The proposed design could
benefit the development of compact imaging systems working in extreme physical
conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Attribute and Structure Subspace Clustering Network. (arXiv:2109.13742v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13742">
<div class="article-summary-box-inner">
<span><p>Deep self-expressiveness-based subspace clustering methods have demonstrated
effectiveness. However, existing works only consider the attribute information
to conduct the self-expressiveness, which may limit the clustering performance.
In this paper, we propose a novel adaptive attribute and structure subspace
clustering network (AASSC-Net) to simultaneously consider the attribute and
structure information in an adaptive graph fusion manner. Specifically, we
first exploit an auto-encoder to represent input data samples with latent
features for the construction of an attribute matrix. We also construct a mixed
signed and symmetric structure matrix to capture the local geometric structure
underlying data samples. Then, we perform self-expressiveness on the
constructed attribute and structure matrices to learn their affinity graphs
separately. Finally, we design a novel attention-based fusion module to
adaptively leverage these two affinity graphs to construct a more
discriminative affinity graph. Extensive experimental results on commonly used
benchmark datasets demonstrate that our AASSC-Net significantly outperforms
state-of-the-art methods. In addition, we conduct comprehensive ablation
studies to discuss the effectiveness of the designed modules. The code will be
publicly available at https://github.com/ZhihaoPENG-CityU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stable training of autoencoders for hyperspectral unmixing. (arXiv:2109.13748v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13748">
<div class="article-summary-box-inner">
<span><p>Neural networks, autoencoders in particular, are one of the most promising
solutions for unmixing hyperspectral data, i.e. reconstructing the spectra of
observed substances (endmembers) and their relative mixing fractions
(abundances). Unmixing is needed for effective hyperspectral analysis and
classification. However, as we show in this paper, the training of autoencoders
for unmixing is highly dependent on weights initialisation. Some sets of
weights lead to degenerate or low performance solutions, introducing negative
bias in expected performance. In this work we present the results of
experiments investigating autoencoders' stability, verifying the dependence of
reconstruction error on initial weights and exploring conditions needed for
successful optimisation of autoencoder parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StereoSpike: Depth Learning with a Spiking Neural Network. (arXiv:2109.13751v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13751">
<div class="article-summary-box-inner">
<span><p>Depth estimation is an important computer vision task, useful in particular
for navigation in autonomous vehicles, or for object manipulation in robotics.
Here we solved it using an end-to-end neuromorphic approach, combining two
event-based cameras and a Spiking Neural Network (SNN) with a slightly modified
U-Net-like encoder-decoder architecture, that we named StereoSpike. More
specifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It
provides a depth ground-truth, which was used to train StereoSpike in a
supervised manner, using surrogate gradient descent. We propose a novel readout
paradigm to obtain a dense analog prediction -- the depth of each pixel -- from
the spikes of the decoder. We demonstrate that this architecture generalizes
very well, even better than its non-spiking counterparts, leading to
state-of-the-art test accuracy. To the best of our knowledge, it is the first
time that such a large-scale regression problem is solved by a fully spiking
network. Finally, we show that low firing rates (&lt;10%) can be obtained via
regularization, with a minimal cost in accuracy. This means that StereoSpike
could be efficiently implemented on neuromorphic chips, opening the door for
low power and real time embedded systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PFENet++: Boosting Few-shot Semantic Segmentation with the Noise-filtered Context-aware Prior Mask. (arXiv:2109.13788v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13788">
<div class="article-summary-box-inner">
<span><p>In this work, we revisit the prior mask guidance proposed in "Prior Guided
Feature Enrichment Network for Few-Shot Segmentation". The prior mask serves as
an indicator that highlights the region of interests of unseen categories, and
it is effective in achieving better performance on different frameworks of
recent studies.
</p>
<p>However, the current method directly takes the maximum element-to-element
correspondence between the query and support features to indicate the
probability of belonging to the target class, thus the broader contextual
information is seldom exploited during the prior mask generation. To address
this issue, first, we propose the Context-aware Prior Mask (CAPM) that
leverages additional nearby semantic cues for better locating the objects in
query images. Second, since the maximum correlation value is vulnerable to
noisy features, we take one step further by incorporating a lightweight Noise
Suppression Module (NSM) to screen out the unnecessary responses, yielding
high-quality masks for providing the prior knowledge.
</p>
<p>Both two contributions are experimentally shown to have substantial practical
merit, and the new model named PFENet++ significantly outperforms the baseline
PFENet as well as all other competitors on three challenging benchmarks
PASCAL-5$^i$, COCO-20$^i$ and FSS-1000.
</p>
<p>The new state-of-the-art performance is achieved without compromising the
efficiency, manifesting the potential for being a new strong baseline in
few-shot semantic segmentation.
</p>
<p>Our code will be available at https://github.com/dvlab-research/PFENet++.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The VVAD-LRS3 Dataset for Visual Voice Activity Detection. (arXiv:2109.13789v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13789">
<div class="article-summary-box-inner">
<span><p>Robots are becoming everyday devices, increasing their interaction with
humans. To make human-machine interaction more natural, cognitive features like
Visual Voice Activity Detection (VVAD), which can detect whether a person is
speaking or not, given visual input of a camera, need to be implemented. Neural
networks are state of the art for tasks in Image Processing, Time Series
Prediction, Natural Language Processing and other domains. Those Networks
require large quantities of labeled data. Currently there are not many datasets
for the task of VVAD. In this work we created a large scale dataset called the
VVAD-LRS3 dataset, derived by automatic annotations from the LRS3 dataset. The
VVAD-LRS3 dataset contains over 44K samples, over three times the next
competitive dataset (WildVVAD). We evaluate different baselines on four kinds
of features: facial and lip images, and facial and lip landmark features. With
a Convolutional Neural Network Long Short Term Memory (CNN LSTM) on facial
images an accuracy of 92% was reached on the test set. A study with humans
showed that they reach an accuracy of 87.93% on the test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not Color Blind: AI Predicts Racial Identity from Black and White Retinal Vessel Segmentations. (arXiv:2109.13845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13845">
<div class="article-summary-box-inner">
<span><p>Background: Artificial intelligence (AI) may demonstrate racial bias when
skin or choroidal pigmentation is present in medical images. Recent studies
have shown that convolutional neural networks (CNNs) can predict race from
images that were not previously thought to contain race-specific features. We
evaluate whether grayscale retinal vessel maps (RVMs) of patients screened for
retinopathy of prematurity (ROP) contain race-specific features.
</p>
<p>Methods: 4095 retinal fundus images (RFIs) were collected from 245 Black and
White infants. A U-Net generated RVMs from RFIs, which were subsequently
thresholded, binarized, or skeletonized. To determine whether RVM differences
between Black and White eyes were physiological, CNNs were trained to predict
race from color RFIs, raw RVMs, and thresholded, binarized, or skeletonized
RVMs. Area under the precision-recall curve (AUC-PR) was evaluated.
</p>
<p>Findings: CNNs predicted race from RFIs near perfectly (image-level AUC-PR:
0.999, subject-level AUC-PR: 1.000). Raw RVMs were almost as informative as
color RFIs (image-level AUC-PR: 0.938, subject-level AUC-PR: 0.995).
Ultimately, CNNs were able to detect whether RFIs or RVMs were from Black or
White babies, regardless of whether images contained color, vessel segmentation
brightness differences were nullified, or vessel segmentation widths were
normalized.
</p>
<p>Interpretation: AI can detect race from grayscale RVMs that were not thought
to contain racial information. Two potential explanations for these findings
are that: retinal vessels physiologically differ between Black and White babies
or the U-Net segments the retinal vasculature differently for various fundus
pigmentations. Either way, the implications remain the same: AI algorithms have
potential to demonstrate racial bias in practice, even when preliminary
attempts to remove such information from the underlying images appear to be
successful.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViT Cane: Visual Assistant for the Visually Impaired. (arXiv:2109.13857v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13857">
<div class="article-summary-box-inner">
<span><p>Blind and visually challenged face multiple issues with navigating the world
independently. Some of these challenges include finding the shortest path to a
destination and detecting obstacles from a distance. To tackle this issue, this
paper proposes ViT Cane, which leverages a vision transformer model in order to
detect obstacles in real-time. Our entire system consists of a Pi Camera Module
v2, Raspberry Pi 4B with 8GB Ram and 4 motors. Based on tactile input using the
4 motors, the obstacle detection model is highly efficient in helping visually
impaired navigate unknown terrain and is designed to be easily reproduced. The
paper discusses the utility of a Visual Transformer model in comparison to
other CNN based models for this specific application. Through rigorous testing,
the proposed obstacle detection model has achieved higher performance on the
Common Object in Context (COCO) data set than its CNN counterpart.
Comprehensive field tests were conducted to verify the effectiveness of our
system for holistic indoor understanding and obstacle avoidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization for Vision-based Driving Trajectory Generation. (arXiv:2109.13858v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13858">
<div class="article-summary-box-inner">
<span><p>One of the challenges in vision-based driving trajectory generation is
dealing with out-of-distribution scenarios. In this paper, we propose a domain
generalization method for vision-based driving trajectory generation for
autonomous vehicles in urban environments, which can be seen as a solution to
extend the Invariant Risk Minimization (IRM) method in complex problems. We
leverage an adversarial learning approach to train a trajectory generator as
the decoder. Based on the pre-trained decoder, we infer the latent variables
corresponding to the trajectories, and pre-train the encoder by regressing the
inferred latent variable. Finally, we fix the decoder but fine-tune the encoder
with the final trajectory loss. We compare our proposed method with the
state-of-the-art trajectory generation method and some recent domain
generalization methods on both datasets and simulation, demonstrating that our
method has better generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NudgeSeg: Zero-Shot Object Segmentation by Repeated Physical Interaction. (arXiv:2109.13859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13859">
<div class="article-summary-box-inner">
<span><p>Recent advances in object segmentation have demonstrated that deep neural
networks excel at object segmentation for specific classes in color and depth
images. However, their performance is dictated by the number of classes and
objects used for training, thereby hindering generalization to never seen
objects or zero-shot samples. To exacerbate the problem further, object
segmentation using image frames rely on recognition and pattern matching cues.
Instead, we utilize the 'active' nature of a robot and their ability to
'interact' with the environment to induce additional geometric constraints for
segmenting zero-shot samples.
</p>
<p>In this paper, we present the first framework to segment unknown objects in a
cluttered scene by repeatedly 'nudging' at the objects and moving them to
obtain additional motion cues at every step using only a monochrome monocular
camera. We call our framework NudgeSeg. These motion cues are used to refine
the segmentation masks. We successfully test our approach to segment novel
objects in various cluttered scenes and provide an extensive study with image
and motion segmentation methods. We show an impressive average detection rate
of over 86% on zero-shot objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introduce the Result Into Self-Attention. (arXiv:2109.13860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13860">
<div class="article-summary-box-inner">
<span><p>Traditional self-attention mechanisms in convolutional networks tend to use
only the output of the previous layer as input to the attention network, such
as SENet, CBAM, etc. In this paper, we propose a new attention modification
method that tries to get the output of the classification network in advance
and use it as a part of the input of the attention network. We used the
auxiliary classifier proposed in GoogLeNet to obtain the results in advance and
pass them into attention networks. we added this mechanism to SE-ResNet for our
experiments and achieved a classification accuracy improvement of at most 1.94%
on cifar100.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual resemblance and communicative context constrain the emergence of graphical conventions. (arXiv:2109.13861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13861">
<div class="article-summary-box-inner">
<span><p>From photorealistic sketches to schematic diagrams, drawing provides a
versatile medium for communicating about the visual world. How do images
spanning such a broad range of appearances reliably convey meaning? Do viewers
understand drawings based solely on their ability to resemble the entities they
refer to (i.e., as images), or do they understand drawings based on shared but
arbitrary associations with these entities (i.e., as symbols)? In this paper,
we provide evidence for a cognitive account of pictorial meaning in which both
visual and social information is integrated to support effective visual
communication. To evaluate this account, we used a communication task where
pairs of participants used drawings to repeatedly communicate the identity of a
target object among multiple distractor objects. We manipulated social cues
across three experiments and a full internal replication, finding pairs of
participants develop referent-specific and interaction-specific strategies for
communicating more efficiently over time, going beyond what could be explained
by either task practice or a pure resemblance-based account alone. Using a
combination of model-based image analyses and crowdsourced sketch annotations,
we further determined that drawings did not drift toward arbitrariness, as
predicted by a pure convention-based account, but systematically preserved
those visual features that were most distinctive of the target object. Taken
together, these findings advance theories of pictorial meaning and have
implications for how successful graphical conventions emerge via complex
interactions between visual perception, communicative experience, and social
context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3N-GAN: Semi-Supervised Classification of X-Ray Images with a 3-Player Adversarial Framework. (arXiv:2109.13862v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13862">
<div class="article-summary-box-inner">
<span><p>The success of deep learning for medical imaging tasks, such as
classification, is heavily reliant on the availability of large-scale datasets.
However, acquiring datasets with large quantities of labeled data is
challenging, as labeling is expensive and time-consuming. Semi-supervised
learning (SSL) is a growing alternative to fully-supervised learning, but
requires unlabeled samples for training. In medical imaging, many datasets lack
unlabeled data entirely, so SSL can't be conventionally utilized. We propose
3N-GAN, or 3 Network Generative Adversarial Networks, to perform
semi-supervised classification of medical images in fully-supervised settings.
We incorporate a classifier into the adversarial relationship such that the
generator trains adversarially against both the classifier and discriminator.
Our preliminary results show improved classification performance and GAN
generations over various algorithms. Our work can seamlessly integrate with
numerous other medical imaging model architectures and SSL methods for greater
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Hand Pose and Shape Estimation from RGB Images for Improved Keypoint-Based Hand-Gesture Recognition. (arXiv:2109.13879v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13879">
<div class="article-summary-box-inner">
<span><p>Estimating the 3D hand pose from a 2D image is a well-studied problem and a
requirement for several real-life applications such as virtual reality,
augmented reality, and hand-gesture recognition. Currently, good estimations
can be computed starting from single RGB images, especially when forcing the
system to also consider, through a multi-task learning approach, the hand shape
when the pose is determined. However, when addressing the aforementioned
real-life tasks, performances can drop considerably depending on the hand
representation, thus suggesting that stable descriptions are required to
achieve satisfactory results. As a consequence, in this paper we present a
keypoint-based end-to-end framework for the 3D hand and pose estimation, and
successfully apply it to the hand-gesture recognition task as a study case.
Specifically, after a pre-processing step where the images are normalized, the
proposed pipeline comprises a multi-task semantic feature extractor generating
2D heatmaps and hand silhouettes from RGB images; a viewpoint encoder
predicting hand and camera view parameters; a stable hand estimator producing
the 3D hand pose and shape; and a loss function designed to jointly guide all
of the components during the learning phase. To assess the proposed framework,
tests were performed on a 3D pose and shape estimation benchmark dataset,
obtaining state-of-the-art performances. What is more, the devised system was
also evaluated on 2 hand-gesture recognition benchmark datasets, where the
framework significantly outperforms other keypoint-based approaches; indicating
that the presented method is an effective solution able to generate stable 3D
estimates for the hand pose and shape.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Turning old models fashion again: Recycling classical CNN networks using the Lattice Transformation. (arXiv:2109.13885v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13885">
<div class="article-summary-box-inner">
<span><p>In the early 1990s, the first signs of life of the CNN era were given: LeCun
et al. proposed a CNN model trained by the backpropagation algorithm to
classify low-resolution images of handwritten digits. Undoubtedly, it was a
breakthrough in the field of computer vision. But with the rise of other
classification methods, it fell out fashion. That was until 2012, when
Krizhevsky et al. revived the interest in CNNs by exhibiting considerably
higher image classification accuracy on the ImageNet challenge. Since then, the
complexity of the architectures are exponentially increasing and many
structures are rapidly becoming obsolete. Using multistream networks as a base
and the feature infusion precept, we explore the proposed LCNN cross-fusion
strategy to use the backbones of former state-of-the-art networks on image
classification in order to discover if the technique is able to put these
designs back in the game. In this paper, we showed that we can obtain an
increase of accuracy up to 63.21% on the NORB dataset we comparing with the
original structure. However, no technique is definitive. While our goal is to
try to reuse previous state-of-the-art architectures with few modifications, we
also expose the disadvantages of our explored strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image scaling by de la Vall\'ee-Poussin filtered interpolation. (arXiv:2109.13897v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13897">
<div class="article-summary-box-inner">
<span><p>We present a new image scaling method both for downscaling and upscaling,
running with any scale factor or desired size. It is based on the sampling of
an approximating bivariate polynomial, which globally interpolates the data and
is defined by a filter of de la Vall\'ee Poussin type whose action ray is
suitable regulated to improve the approximation. The method has been tested on
a significant number of different image datasets. The results are evaluated in
qualitative and quantitative terms and compared with other available
competitive methods. The perceived quality of the resulting scaled images is
such that important details are preserved, and the appearance of artifacts is
low. Very high-quality measure values in downscaling and the competitive ones
in upscaling evidence the effectiveness of the method. Good visual quality,
limited computational effort, and moderate memory demanding make the method
suitable for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Contrastive Learning Approach to Auroral Identification and Classification. (arXiv:2109.13899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13899">
<div class="article-summary-box-inner">
<span><p>Unsupervised learning algorithms are beginning to achieve accuracies
comparable to their supervised counterparts on benchmark computer vision tasks,
but their utility for practical applications has not yet been demonstrated. In
this work, we present a novel application of unsupervised learning to the task
of auroral image classification. Specifically, we modify and adapt the Simple
framework for Contrastive Learning of Representations (SimCLR) algorithm to
learn representations of auroral images in a recently released auroral image
dataset constructed using image data from Time History of Events and Macroscale
Interactions during Substorms (THEMIS) all-sky imagers. We demonstrate that (a)
simple linear classifiers fit to the learned representations of the images
achieve state-of-the-art classification performance, improving the
classification accuracy by almost 10 percentage points over the current
benchmark; and (b) the learned representations naturally cluster into more
clusters than exist manually assigned categories, suggesting that existing
categorizations are overly coarse and may obscure important connections between
auroral types, near-earth solar wind conditions, and geomagnetic disturbances
at the earth's surface. Moreover, our model is much lighter than the previous
benchmark on this dataset, requiring in the area of fewer than 25\% of the
number of parameters. Our approach exceeds an established threshold for
operational purposes, demonstrating readiness for deployment and utilization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PDC-Net+: Enhanced Probabilistic Dense Correspondence Network. (arXiv:2109.13912v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13912">
<div class="article-summary-box-inner">
<span><p>Establishing robust and accurate correspondences between a pair of images is
a long-standing computer vision problem with numerous applications. While
classically dominated by sparse methods, emerging dense approaches offer a
compelling alternative paradigm that avoids the keypoint detection step.
However, dense flow estimation is often inaccurate in the case of large
displacements, occlusions, or homogeneous regions. In order to apply dense
methods to real-world applications, such as pose estimation, image
manipulation, or 3D reconstruction, it is therefore crucial to estimate the
confidence of the predicted matches.
</p>
<p>We propose the Enhanced Probabilistic Dense Correspondence Network, PDC-Net+,
capable of estimating accurate dense correspondences along with a reliable
confidence map. We develop a flexible probabilistic approach that jointly
learns the flow prediction and its uncertainty. In particular, we parametrize
the predictive distribution as a constrained mixture model, ensuring better
modelling of both accurate flow predictions and outliers. Moreover, we develop
an architecture and an enhanced training strategy tailored for robust and
generalizable uncertainty prediction in the context of self-supervised
training. Our approach obtains state-of-the-art results on multiple challenging
geometric matching and optical flow datasets. We further validate the
usefulness of our probabilistic confidence estimation for the tasks of pose
estimation, 3D reconstruction, image-based localization, and image retrieval.
Code and models are available at https://github.com/PruneTruong/DenseMatching.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$f$-Cal: Calibrated aleatoric uncertainty estimation from neural networks for robot perception. (arXiv:2109.13913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13913">
<div class="article-summary-box-inner">
<span><p>While modern deep neural networks are performant perception modules,
performance (accuracy) alone is insufficient, particularly for safety-critical
robotic applications such as self-driving vehicles. Robot autonomy stacks also
require these otherwise blackbox models to produce reliable and calibrated
measures of confidence on their predictions. Existing approaches estimate
uncertainty from these neural network perception stacks by modifying network
architectures, inference procedure, or loss functions. However, in general,
these methods lack calibration, meaning that the predictive uncertainties do
not faithfully represent the true underlying uncertainties (process noise). Our
key insight is that calibration is only achieved by imposing constraints across
multiple examples, such as those in a mini-batch; as opposed to existing
approaches which only impose constraints per-sample, often leading to
overconfident (thus miscalibrated) uncertainty estimates. By enforcing the
distribution of outputs of a neural network to resemble a target distribution
by minimizing an $f$-divergence, we obtain significantly better-calibrated
models compared to prior approaches. Our approach, $f$-Cal, outperforms
existing uncertainty calibration approaches on robot perception tasks such as
object detection and monocular depth estimation over multiple real-world
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsolved Problems in ML Safety. (arXiv:2109.13916v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13916">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards ("Robustness"),
identifying hazards ("Monitoring"), steering ML systems ("Alignment"), and
reducing risks to how ML systems are handled ("External Safety"). Throughout,
we clarify each problem's motivation and provide concrete research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics-guided Neural Networks (PGNN): An Application in Lake Temperature Modeling. (arXiv:1710.11431v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1710.11431">
<div class="article-summary-box-inner">
<span><p>This paper introduces a framework for combining scientific knowledge of
physics-based models with neural networks to advance scientific discovery. This
framework, termed physics-guided neural networks (PGNN), leverages the output
of physics-based model simulations along with observational features in a
hybrid modeling setup to generate predictions using a neural network
architecture. Further, this framework uses physics-based loss functions in the
learning objective of neural networks to ensure that the model predictions not
only show lower errors on the training set but are also scientifically
consistent with the known physics on the unlabeled set. We illustrate the
effectiveness of PGNN for the problem of lake temperature modeling, where
physical relationships between the temperature, density, and depth of water are
used to design a physics-based loss function. By using scientific knowledge to
guide the construction and learning of neural networks, we are able to show
that the proposed framework ensures better generalizability as well as
scientific consistency of results. All the code and datasets used in this study
have been made available on this link \url{https://github.com/arkadaw9/PGNN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding. (arXiv:1911.00232v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.00232">
<div class="article-summary-box-inner">
<span><p>Videos capture events that typically contain multiple sequential, and
simultaneous, actions even in the span of only a few seconds. However, most
large-scale datasets built to train models for action recognition in video only
provide a single label per video. Consequently, models can be incorrectly
penalized for classifying actions that exist in the videos but are not
explicitly labeled and do not learn the full spectrum of information present in
each video in training. Towards this goal, we present the Multi-Moments in Time
dataset (M-MiT) which includes over two million action labels for over one
million three second videos. This multi-label dataset introduces novel
challenges on how to train and analyze models for multi-action detection. Here,
we present baseline results for multi-action recognition using loss functions
adapted for long tail multi-label learning, provide improved methods for
visualizing and interpreting models trained for multi-label action detection
and show the strength of transferring models trained on M-MiT to smaller
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Degenerative Adversarial NeuroImage Nets for Brain Scan Simulations: Application in Ageing and Dementia. (arXiv:1912.01526v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.01526">
<div class="article-summary-box-inner">
<span><p>Accurate and realistic simulation of high-dimensional medical images has
become an important research area relevant to many AI-enabled healthcare
applications. However, current state-of-the-art approaches lack the ability to
produce satisfactory high-resolution and accurate subject-specific images. In
this work, we present a deep learning framework, namely 4D-Degenerative
Adversarial NeuroImage Net (4D-DANI-Net), to generate high-resolution,
longitudinal MRI scans that mimic subject-specific neurodegeneration in ageing
and dementia. 4D-DANI-Net is a modular framework based on adversarial training
and a set of novel spatiotemporal, biologically-informed constraints. To ensure
efficient training and overcome memory limitations affecting such
high-dimensional problems, we rely on three key technological advances: i) a
new 3D training consistency mechanism called Profile Weight Functions (PWFs),
ii) a 3D super-resolution module and iii) a transfer learning strategy to
fine-tune the system for a given individual. To evaluate our approach, we
trained the framework on 9852 T1-weighted MRI scans from 876 participants in
the Alzheimer's Disease Neuroimaging Initiative dataset and held out a separate
test set of 1283 MRI scans from 170 participants for quantitative and
qualitative assessment of the personalised time series of synthetic images. We
performed three evaluations: i) image quality assessment; ii) quantifying the
accuracy of regional brain volumes over and above benchmark models; and iii)
quantifying visual perception of the synthetic images by medical experts.
Overall, both quantitative and qualitative results show that 4D-DANI-Net
produces realistic, low-artefact, personalised time series of synthetic T1 MRI
that outperforms benchmark models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Head2Head++: Deep Facial Attributes Re-Targeting. (arXiv:2006.10199v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10199">
<div class="article-summary-box-inner">
<span><p>Facial video re-targeting is a challenging problem aiming to modify the
facial attributes of a target subject in a seamless manner by a driving
monocular sequence. We leverage the 3D geometry of faces and Generative
Adversarial Networks (GANs) to design a novel deep learning architecture for
the task of facial and head reenactment. Our method is different to purely 3D
model-based approaches, or recent image-based methods that use Deep
Convolutional Neural Networks (DCNNs) to generate individual frames. We manage
to capture the complex non-rigid facial motion from the driving monocular
performances and synthesise temporally consistent videos, with the aid of a
sequential Generator and an ad-hoc Dynamics Discriminator network. We conduct a
comprehensive set of quantitative and qualitative tests and demonstrate
experimentally that our proposed method can successfully transfer facial
expressions, head pose and eye gaze from a source video to a target subject, in
a photo-realistic and faithful fashion, better than other state-of-the-art
methods. Most importantly, our system performs end-to-end reenactment in nearly
real-time speed (18 fps).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v6 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.08637">
<div class="article-summary-box-inner">
<span><p>Coronaviruses constitute a family of viruses that gives rise to respiratory
diseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is
crucial for an effective treatment strategy. However, the RT-PCR test which is
considered to be a gold standard in the diagnosis of COVID-19 suffers from a
high false-negative rate. Chest X-ray (CXR) image analysis has emerged as a
feasible and effective diagnostic technique towards this objective. In this
work, we propose the COVID-19 classification problem as a three-class
classification problem to distinguish between COVID-19, normal, and pneumonia
classes. We propose a three-stage framework, named COV-ELM. Stage one deals
with preprocessing and transformation while stage two deals with feature
extraction. These extracted features are passed as an input to the ELM at the
third stage, resulting in the identification of COVID-19. The choice of ELM in
this work has been motivated by its faster convergence, better generalization
capability, and shorter training time in comparison to the conventional
gradient-based learning algorithms. As bigger and diverse datasets become
available, ELM can be quickly retrained as compared to its gradient-based
competitor models. The proposed model achieved a macro average F1-score of 0.95
and the overall sensitivity of ${0.94 \pm 0.02} at a 95% confidence interval.
When compared to state-of-the-art machine learning algorithms, the COV-ELM is
found to outperform its competitors in this three-class classification
scenario. Further, LIME has been integrated with the proposed COV-ELM model to
generate annotated CXR images. The annotations are based on the superpixels
that have contributed to distinguish between the different classes. It was
observed that the superpixels correspond to the regions of the human lungs that
are clinically observed in COVID-19 and Pneumonia cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Retinal Vessel Segmentation from a Data Augmentation Perspective. (arXiv:2007.15883v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.15883">
<div class="article-summary-box-inner">
<span><p>Retinal vessel segmentation is a fundamental step in screening, diagnosis,
and treatment of various cardiovascular and ophthalmic diseases. Robustness is
one of the most critical requirements for practical utilization, since the test
images may be captured using different fundus cameras, or be affected by
various pathological changes. We investigate this problem from a data
augmentation perspective, with the merits of no additional training data or
inference time. In this paper, we propose two new data augmentation modules,
namely, channel-wise random Gamma correction and channel-wise random vessel
augmentation. Given a training color fundus image, the former applies random
gamma correction on each color channel of the entire image, while the latter
intentionally enhances or decreases only the fine-grained blood vessel regions
using morphological transformations. With the additional training samples
generated by applying these two modules sequentially, a model could learn more
invariant and discriminating features against both global and local
disturbances. Experimental results on both real-world and synthetic datasets
demonstrate that our method can improve the performance and robustness of a
classic convolutional neural network architecture. The source code is available
at
\url{https://github.com/PaddlePaddle/Research/tree/master/CV/robust_vessel_segmentation}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles. (arXiv:2009.02755v8 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.02755">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose POTATOES (Partitioning OverfiTting AuTOencoder
EnSemble), a new method for unsupervised outlier detection (UOD). More
precisely, given any autoencoder for UOD, this technique can be used to improve
its accuracy while at the same time removing the burden of tuning its
regularization. The idea is to not regularize at all, but to rather randomly
partition the data into sufficiently many equally sized parts, overfit each
part with its own autoencoder, and to use the maximum over all autoencoder
reconstruction errors as the anomaly score. We apply our model to various
realistic datasets and show that if the set of inliers is dense enough, our
method indeed improves the UOD performance of a given autoencoder
significantly. For reproducibility, the code is made available on github so the
reader can recreate the results in this paper as well as apply the method to
other autoencoders and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Once Quantization-Aware Training: High Performance Extremely Low-bit Architecture Search. (arXiv:2010.04354v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04354">
<div class="article-summary-box-inner">
<span><p>Quantization Neural Networks (QNN) have attracted a lot of attention due to
their high efficiency. To enhance the quantization accuracy, prior works mainly
focus on designing advanced quantization algorithms but still fail to achieve
satisfactory results under the extremely low-bit case. In this work, we take an
architecture perspective to investigate the potential of high-performance QNN.
Therefore, we propose to combine Network Architecture Search methods with
quantization to enjoy the merits of the two sides. However, a naive combination
inevitably faces unacceptable time consumption or unstable training problem. To
alleviate these problems, we first propose the joint training of architecture
and quantization with a shared step size to acquire a large number of quantized
models. Then a bit-inheritance scheme is introduced to transfer the quantized
models to the lower bit, which further reduces the time cost and meanwhile
improves the quantization accuracy. Equipped with this overall framework,
dubbed as Once Quantization-Aware Training~(OQAT), our searched model family,
OQATNets, achieves a new state-of-the-art compared with various architectures
under different bit-widths. In particular, OQAT-2bit-M achieves 61.6% ImageNet
Top-1 accuracy, outperforming 2-bit counterpart MobileNetV3 by a large margin
of 9% with 10% less computation cost. A series of quantization-friendly
architectures are identified easily and extensive analysis can be made to
summarize the interaction between quantization and neural architectures. Codes
and models are released at https://github.com/LaVieEnRoseSMZ/OQA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANIMC: A Soft Framework for Auto-weighted Noisy and Incomplete Multi-view Clustering. (arXiv:2011.10331v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10331">
<div class="article-summary-box-inner">
<span><p>Multi-view clustering has wide applications in many image processing
scenarios. In these scenarios, original image data often contain missing
instances and noises, which is ignored by most multi-view clustering methods.
However, missing instances may make these methods difficult to use directly and
noises will lead to unreliable clustering results. In this paper, we propose a
novel Auto-weighted Noisy and Incomplete Multi-view Clustering framework
(ANIMC) via a soft auto-weighted strategy and a doubly soft regular regression
model. Firstly, by designing adaptive semi-regularized nonnegative matrix
factorization (adaptive semi-RNMF), the soft auto-weighted strategy assigns a
proper weight to each view and adds a soft boundary to balance the influence of
noises and incompleteness. Secondly, by proposing{\theta}-norm, the doubly soft
regularized regression model adjusts the sparsity of our model by choosing
different{\theta}. Compared with existing methods, ANIMC has three unique
advantages: 1) it is a soft algorithm to adjust our framework in different
scenarios, thereby improving its generalization ability; 2) it automatically
learns a proper weight for each view, thereby reducing the influence of noises;
3) it performs doubly soft regularized regression that aligns the same
instances in different views, thereby decreasing the impact of missing
instances. Extensive experimental results demonstrate its superior advantages
over other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Camera Convolutional Color Constancy. (arXiv:2011.11890v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11890">
<div class="article-summary-box-inner">
<span><p>We present "Cross-Camera Convolutional Color Constancy" (C5), a
learning-based method, trained on images from multiple cameras, that accurately
estimates a scene's illuminant color from raw images captured by a new camera
previously unseen during training. C5 is a hypernetwork-like extension of the
convolutional color constancy (CCC) approach: C5 learns to generate the weights
of a CCC model that is then evaluated on the input image, with the CCC weights
dynamically adapted to different input content. Unlike prior cross-camera color
constancy models, which are usually designed to be agnostic to the spectral
properties of test-set images from unobserved cameras, C5 approaches this
problem through the lens of transductive inference: additional unlabeled images
are provided as input to the model at test time, which allows the model to
calibrate itself to the spectral properties of the test-set camera during
inference. C5 achieves state-of-the-art accuracy for cross-camera color
constancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on
a GPU or CPU, respectively), and requires little memory (~2 MB), and thus is a
practical solution to the problem of calibration-free automatic white balance
for mobile photography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v10 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01338">
<div class="article-summary-box-inner">
<span><p>Training deep learning models in technical domains is often accompanied by
the challenge that although the task is clear, insufficient data for training
is available. In this work, we propose a novel approach based on the
combination of Siamese networks and radial basis function networks to perform
data-efficient classification without pretraining by measuring the distance
between images in semantic space in a data-efficient manner. We develop the
models using three technical datasets, the NEU dataset, the BSD dataset, and
the TEX dataset. In addition to the technical domain, we show the general
applicability to classical datasets (cifar10 and MNIST) as well. The approach
is tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise
reduction of the number of samples available for training. The authors show
that the proposed approach outperforms the state-of-the-art models in the low
data regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mDALU: Multi-Source Domain Adaptation and Label Unification with Partial Datasets. (arXiv:2012.08385v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08385">
<div class="article-summary-box-inner">
<span><p>One challenge of object recognition is to generalize to new domains, to more
classes and/or to new modalities. This necessitates methods to combine and
reuse existing datasets that may belong to different domains, have partial
annotations, and/or have different data modalities. This paper formulates this
as a multi-source domain adaptation and label unification problem, and proposes
a novel method for it. Our method consists of a partially-supervised adaptation
stage and a fully-supervised adaptation stage. In the former, partial knowledge
is transferred from multiple source domains to the target domain and fused
therein. Negative transfer between unmatching label spaces is mitigated via
three new modules: domain attention, uncertainty maximization and
attention-guided adversarial alignment. In the latter, knowledge is transferred
in the unified label space after a label completion process with pseudo-labels.
Extensive experiments on three different tasks - image classification, 2D
semantic image segmentation, and joint 2D-3D semantic segmentation - show that
our method outperforms all competing methods significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond. (arXiv:2101.11517v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11517">
<div class="article-summary-box-inner">
<span><p>Bi-Level Optimization (BLO) is originated from the area of economic game
theory and then introduced into the optimization community. BLO is able to
handle problems with a hierarchical structure, involving two levels of
optimization tasks, where one task is nested inside the other. In machine
learning and computer vision fields, despite the different motivations and
mechanisms, a lot of complex problems, such as hyper-parameter optimization,
multi-task and meta-learning, neural architecture search, adversarial learning
and deep reinforcement learning, actually all contain a series of closely
related subproblms. In this paper, we first uniformly express these complex
learning and vision problems from the perspective of BLO. Then we construct a
best-response-based single-level reformulation and establish a unified
algorithmic framework to understand and formulate mainstream gradient-based BLO
methodologies, covering aspects ranging from fundamental automatic
differentiation schemes to various accelerations, simplifications, extensions
and their convergence and complexity properties. Last but not least, we discuss
the potentials of our unified BLO framework for designing new algorithms and
point out some promising directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards clinically applicable automated aneurysm detection in TOF-MRA: weak labels, anatomical knowledge, and open data. (arXiv:2103.06168v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06168">
<div class="article-summary-box-inner">
<span><p>Purpose: 1) Develop a deep learning algorithm for brain aneurysm detection
exploiting weak labels and prior anatomical knowledge. 2) Describe and release
the largest Time-Of-Flight Magnetic Resonance Angiography (TOF-MRA) dataset to
the community.
</p>
<p>Materials and Methods: In this retrospective study we retrieved TOF-MRA
images of 284 subjects (170 females) scanned between 2010 and 2015. Out of
these, 157 are patients with a total of 198 aneurysms, while 127 are controls.
We used spherical weak labels as detection ground truth, thus making data
annotation, a major bottleneck for medical AI, noticeably faster. Since
aneurysms mainly occur in specific locations, we built our deep neural network
leveraging the anatomy of the brain vasculature. To assess model robustness, we
participated in the first public challenge for TOF-MRA data (93 patients, 20
controls, 125 aneurysms). We stratified results according to aneurysm
risk-of-rupture, location, and size.
</p>
<p>Results: Our network achieves a sensitivity of 80% on the in-house data, with
False Positive (FP) rate of 1.2 per patient. On the public challenge data,
sensitivity was 68% (FP rate = 2.5), ranking 4th/16 on the open leaderboard. We
found no significant difference in sensitivity between risk groups (p = 0.75),
locations (p = 0.72), or sizes (p = 0.15).
</p>
<p>Conclusion: Competitive results can be obtained using fast weak labels and
anatomical knowledge for automated aneurysm detection. Our open-source code and
open access dataset can foster reproducibility, and bring us closer to clinical
application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning in Multi-Task Graphs through Iterative Consensus Shift. (arXiv:2103.14417v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14417">
<div class="article-summary-box-inner">
<span><p>The human ability to synchronize the feedback from all their senses inspired
recent works in multi-task and multi-modal learning. While these works rely on
expensive supervision, our multi-task graph requires only pseudo-labels from
expert models. Every graph node represents a task, and each edge learns between
tasks transformations. Once initialized, the graph learns self-supervised,
based on a novel consensus shift algorithm that intelligently exploits the
agreement between graph pathways to generate new pseudo-labels for the next
learning cycle. We demonstrate significant improvement from one unsupervised
learning iteration to the next, outperforming related recent methods in
extensive multi-task learning experiments on two challenging datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple and Strong Baseline for Universal Targeted Attacks on Siamese Visual Tracking. (arXiv:2105.02480v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02480">
<div class="article-summary-box-inner">
<span><p>Siamese trackers are shown to be vulnerable to adversarial attacks recently.
However, the existing attack methods craft the perturbations for each video
independently, which comes at a non-negligible computational cost. In this
paper, we show the existence of universal perturbations that can enable the
targeted attack, e.g., forcing a tracker to follow the ground-truth trajectory
with specified offsets, to be video-agnostic and free from inference in a
network. Specifically, we attack a tracker by adding a universal imperceptible
perturbation to the template image and adding a fake target, i.e., a small
universal adversarial patch, into the search images adhering to the predefined
trajectory, so that the tracker outputs the location and size of the fake
target instead of the real target. Our approach allows perturbing a novel video
to come at no additional cost except the mere addition operations -- and not
require gradient optimization or network inference. Experimental results on
several datasets demonstrate that our approach can effectively fool the Siamese
trackers in a targeted attack manner. We show that the proposed perturbations
are not only universal across videos, but also generalize well across different
trackers. Such perturbations are therefore doubly universal, both with respect
to the data and the network architectures. We will make our code publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-augmented Spatio-Temporal Segmentation for Land Cover Mapping. (arXiv:2105.02963v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02963">
<div class="article-summary-box-inner">
<span><p>The availability of massive earth observing satellite data provide huge
opportunities for land use and land cover mapping. However, such mapping effort
is challenging due to the existence of various land cover classes, noisy data,
and the lack of proper labels. Also, each land cover class typically has its
own unique temporal pattern and can be identified only during certain periods.
In this article, we introduce a novel architecture that incorporates the UNet
structure with Bidirectional LSTM and Attention mechanism to jointly exploit
the spatial and temporal nature of satellite data and to better identify the
unique temporal patterns of each land cover. We evaluate this method for
mapping crops in multiple regions over the world. We compare our method with
other state-of-the-art methods both quantitatively and qualitatively on two
real-world datasets which involve multiple land cover classes. We also
visualise the attention weights to study its effectiveness in mitigating noise
and identifying discriminative time period.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey of Visual-Semantic Embedding Methods for Zero-Shot Image Retrieval. (arXiv:2105.07391v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07391">
<div class="article-summary-box-inner">
<span><p>Visual-semantic embedding is an interesting research topic because it is
useful for various tasks, such as visual question answering (VQA), image-text
retrieval, image captioning, and scene graph generation. In this paper, we
focus on zero-shot image retrieval using sentences as queries and present a
survey of the technological trends in this area. First, we provide a
comprehensive overview of the history of the technology, starting with a
discussion of the early studies of image-to-text matching and how the
technology has evolved over time. In addition, a description of the datasets
commonly used in experiments and a comparison of the evaluation results of each
method are presented. We also introduce the implementation available on github
for use in confirming the accuracy of experiments and for further improvement.
We hope that this survey paper will encourage researchers to further develop
their research on bridging images and languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalisable and distinctive 3D local deep descriptors for point cloud registration. (arXiv:2105.10382v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10382">
<div class="article-summary-box-inner">
<span><p>An effective 3D descriptor should be invariant to different geometric
transformations, such as scale and rotation, repeatable in the case of
occlusions and clutter, and generalisable in different contexts when data is
captured with different sensors. We present a simple but yet effective method
to learn generalisable and distinctive 3D local descriptors that can be used to
register point clouds captured in different contexts with different sensors.
Point cloud patches are extracted, canonicalised with respect to their local
reference frame, and encoded into scale and rotation-invariant compact
descriptors by a point permutation-invariant deep neural network. Our
descriptors can effectively generalise across sensor modalities from locally
and randomly sampled points. We evaluate and compare our descriptors with
alternative handcrafted and deep learning-based descriptors on several indoor
and outdoor datasets reconstructed using both RGBD sensors and laser scanners.
Our descriptors outperform most recent descriptors by a large margin in terms
of generalisation, and become the state of the art also in benchmarks where
training and testing are performed in the same scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy. (arXiv:2106.01100v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01100">
<div class="article-summary-box-inner">
<span><p>During lung cancer radiotherapy, the position of infrared reflective objects
on the chest can be recorded to estimate the tumor location. However,
radiotherapy systems usually have a latency inherent to robot control
limitations that impedes the radiation delivery precision. Not taking this
phenomenon into account may cause unwanted damage to healthy tissues and lead
to side effects such as radiation pneumonitis. In this research, we use nine
observation records of the three-dimensional position of three external markers
on the chest and abdomen of healthy individuals breathing during intervals from
73s to 222s. The sampling frequency is equal to 10Hz and the amplitudes of the
recorded trajectories range from 6mm to 40mm in the superior-inferior
direction. We forecast the location of each marker simultaneously with a
horizon value (the time interval in advance for which the prediction is made)
between 0.1s and 2.0s, using a recurrent neural network (RNN) trained with
unbiased online recurrent optimization (UORO). We compare its performance with
an RNN trained with real-time recurrent learning, least mean squares (LMS), and
offline linear regression. Training and cross-validation are performed during
the first minute of each sequence. On average, UORO achieves the lowest
root-mean-square (RMS) and maximum error, equal respectively to 1.3mm and
8.8mm, with a prediction time per time step lower than 2.8ms (Dell Intel core
i9-9900K 3.60Ghz). Linear regression has the lowest RMS error for the horizon
values 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,
and UORO for horizon values greater than 0.6s.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inter Extreme Points Geodesics for Weakly Supervised Image Segmentation. (arXiv:2107.00583v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00583">
<div class="article-summary-box-inner">
<span><p>We introduce $\textit{InExtremIS}$, a weakly supervised 3D approach to train
a deep image segmentation network using particularly weak train-time
annotations: only 6 extreme clicks at the boundary of the objects of interest.
Our fully-automatic method is trained end-to-end and does not require any
test-time annotations. From the extreme points, 3D bounding boxes are extracted
around objects of interest. Then, deep geodesics connecting extreme points are
generated to increase the amount of "annotated" voxels within the bounding
boxes. Finally, a weakly supervised regularised loss derived from a Conditional
Random Field formulation is used to encourage prediction consistency over
homogeneous regions. Extensive experiments are performed on a large open
dataset for Vestibular Schwannoma segmentation. $\textit{InExtremIS}$ obtained
competitive performance, approaching full supervision and outperforming
significantly other weakly supervised techniques based on bounding boxes.
Moreover, given a fixed annotation time budget, $\textit{InExtremIS}$
outperforms full supervision. Our code and data are available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation. (arXiv:2107.00781v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00781">
<div class="article-summary-box-inner">
<span><p>Transformer architecture has emerged to be successful in a number of natural
language processing tasks. However, its applications to medical vision remain
largely unexplored. In this study, we present UTNet, a simple yet powerful
hybrid Transformer architecture that integrates self-attention into a
convolutional neural network for enhancing medical image segmentation. UTNet
applies self-attention modules in both encoder and decoder for capturing
long-range dependency at different scales with minimal overhead. To this end,
we propose an efficient self-attention mechanism along with relative position
encoding that reduces the complexity of self-attention operation significantly
from $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also
proposed to recover fine-grained details from the skipped connections in the
encoder. Our approach addresses the dilemma that Transformer requires huge
amounts of data to learn vision inductive bias. Our hybrid layer design allows
the initialization of Transformer into convolutional networks without a need of
pre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac
magnetic resonance imaging cohort. UTNet demonstrates superior segmentation
performance and robustness against the state-of-the-art approaches, holding the
promise to generalize well on other medical image segmentations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00394">
<div class="article-summary-box-inner">
<span><p>Graph matching is an important problem that has received widespread
attention, especially in the field of computer vision. Recently,
state-of-the-art methods seek to incorporate graph matching with deep learning.
However, there is no research to explain what role the graph matching algorithm
plays in the model. Therefore, we propose an approach integrating a MILP
formulation of the graph matching problem. This formulation is solved to
optimal and it provides inherent baseline. Meanwhile, similar approaches are
derived by releasing the optimal guarantee of the graph matching solver and by
introducing a quality level. This quality level controls the quality of the
solutions provided by the graph matching solver. In addition, several
relaxations of the graph matching problem are put to the test. Our experimental
evaluation gives several theoretical insights and guides the direction of deep
graph matching methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refractive Geometry for Underwater Domes. (arXiv:2108.06575v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06575">
<div class="article-summary-box-inner">
<span><p>Underwater cameras are typically placed behind glass windows to protect them
from the water. Spherical glass, a dome port, is well suited for high water
pressures at great depth, allows for a large field of view, and avoids
refraction if a pinhole camera is positioned exactly at the sphere's center.
Adjusting a real lens perfectly to the dome center is a challenging task, both
in terms of how to actually guide the centering process (e.g. visual servoing)
and how to measure the alignment quality, but also, how to mechanically perform
the alignment. Consequently, such systems are prone to being decentered by some
offset, leading to challenging refraction patterns at the sphere that
invalidate the pinhole camera model. We show that the overall camera system
becomes an axial camera, even for thick domes as used for deep sea exploration
and provide a non-iterative way to compute the center of refraction without
requiring knowledge of exact air, glass or water properties. We also analyze
the refractive geometry at the sphere, looking at effects such as forward- vs.
backward decentering, iso-refraction curves and obtain a 6th-degree polynomial
equation for forward projection of 3D points in thin domes. We then propose a
pure underwater calibration procedure to estimate the decentering from multiple
images. This estimate can either be used during adjustment to guide the
mechanical position of the lens, or can be considered in photogrammetric
underwater applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FSNet: A Failure Detection Framework for Semantic Segmentation. (arXiv:2108.08748v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08748">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is an important task that helps autonomous vehicles
understand their surroundings and navigate safely. During deployment, even the
most mature segmentation models are vulnerable to various external factors that
can degrade the segmentation performance with potentially catastrophic
consequences for the vehicle and its surroundings. To address this issue, we
propose a failure detection framework to identify pixel-level
misclassification. We do so by exploiting internal features of the segmentation
model and training it simultaneously with a failure detection network. During
deployment, the failure detector can flag areas in the image where the
segmentation model have failed to segment correctly. We evaluate the proposed
approach against state-of-the-art methods and achieve 12.30%, 9.46%, and 9.65%
performance improvement in the AUPR-Error metric for Cityscapes, BDD100K, and
Mapillary semantic segmentation datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fiducial marker recovery and detection from severely truncated data in navigation assisted spine surgery. (arXiv:2108.13844v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13844">
<div class="article-summary-box-inner">
<span><p>Fiducial markers are commonly used in navigation assisted minimally invasive
spine surgery (MISS) and they help transfer image coordinates into real world
coordinates. In practice, these markers might be located outside the
field-of-view (FOV), due to the limited detector sizes of C-arm cone-beam
computed tomography (CBCT) systems used in intraoperative surgeries. As a
consequence, reconstructed markers in CBCT volumes suffer from artifacts and
have distorted shapes, which sets an obstacle for navigation. In this work, we
propose two fiducial marker detection methods: direct detection from distorted
markers (direct method) and detection after marker recovery (recovery method).
For direct detection from distorted markers in reconstructed volumes, an
efficient automatic marker detection method using two neural networks and a
conventional circle detection algorithm is proposed. For marker recovery, a
task-specific learning strategy is proposed to recover markers from severely
truncated data. Afterwards, a conventional marker detection algorithm is
applied for position detection. The two methods are evaluated on simulated data
and real data, both achieving a marker registration error smaller than 0.2 mm.
Our experiments demonstrate that the direct method is capable of detecting
distorted markers accurately and the recovery method with task-specific
learning has high robustness and generalizability on various data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-step Domain Adaptation for Mitosis Cell Detection in Histopathology Images. (arXiv:2109.00109v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00109">
<div class="article-summary-box-inner">
<span><p>We propose a two-step domain shift-invariant mitosis cell detection method
based on Faster RCNN and a convolutional neural network (CNN). We generate
various domain-shifted versions of existing histopathology images using a stain
augmentation technique, enabling our method to effectively learn various stain
domains and achieve better generalization. The performance of our method is
evaluated on the preliminary test data set of the MIDOG-2021 challenge. The
experimental results demonstrate that the proposed mitosis detection method can
achieve promising performance for domain-shifted histopathology images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Extreme Value Theory for Open Set Video Domain Adaptation. (arXiv:2109.00522v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00522">
<div class="article-summary-box-inner">
<span><p>With the advent of media streaming, video action recognition has become
progressively important for various applications, yet at the high expense of
requiring large-scale data labelling. To overcome the problem of expensive data
labelling, domain adaptation techniques have been proposed that transfers
knowledge from fully labelled data (i.e., source domain) to unlabelled data
(i.e., target domain). The majority of video domain adaptation algorithms are
proposed for closed-set scenarios in which all the classes are shared among the
domains. In this work, we propose an open-set video domain adaptation approach
to mitigate the domain discrepancy between the source and target data, allowing
the target data to contain additional classes that do not belong to the source
domain. Different from previous works, which only focus on improving accuracy
for shared classes, we aim to jointly enhance the alignment of shared classes
and recognition of unknown samples. Towards this goal, class-conditional
extreme value theory is applied to enhance the unknown recognition.
Specifically, the entropy values of target samples are modelled as generalised
extreme value distributions, which allows separating unknown samples lying in
the tail of the distribution. To alleviate the negative transfer issue, weights
computed by the distance from the sample entropy to the threshold are leveraged
in adversarial learning in the sense that confident source and target samples
are aligned, and unconfident samples are pushed away. The proposed method has
been thoroughly evaluated on both small-scale and large-scale cross-domain
video datasets and achieved the state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Human Deformation Transfer. (arXiv:2109.01588v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01588">
<div class="article-summary-box-inner">
<span><p>We consider the problem of human deformation transfer, where the goal is to
retarget poses between different characters. Traditional methods that tackle
this problem require a clear definition of the pose, and use this definition to
transfer poses between characters. In this work, we take a different approach
and transform the identity of a character into a new identity without modifying
the character's pose. This offers the advantage of not having to define
equivalences between 3D human poses, which is not straightforward as poses tend
to change depending on the identity of the character performing them, and as
their meaning is highly contextual. To achieve the deformation transfer, we
propose a neural encoder-decoder architecture where only identity information
is encoded and where the decoder is conditioned on the pose. We use pose
independent representations, such as isometry-invariant shape characteristics,
to represent identity features. Our model uses these features to supervise the
prediction of offsets from the deformed pose to the result of the transfer. We
show experimentally that our method outperforms state-of-the-art methods both
quantitatively and qualitatively, and generalises better to poses not seen
during training. We also introduce a fine-tuning step that allows to obtain
competitive results for extreme identities, and allows to transfer simple
clothing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit 3D Representations. (arXiv:2109.02123v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02123">
<div class="article-summary-box-inner">
<span><p>Neural Radiance Fields (NeRF) has become a popular framework for learning
implicit 3D representations and addressing different tasks such as novel-view
synthesis or depth-map estimation. However, in downstream applications where
decisions need to be made based on automatic predictions, it is critical to
leverage the confidence associated with the model estimations. Whereas
uncertainty quantification is a long-standing problem in Machine Learning, it
has been largely overlooked in the recent NeRF literature. In this context, we
propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of
standard NeRF that learns a probability distribution over all the possible
radiance fields modeling the scene. This distribution allows to quantify the
uncertainty associated with the scene information provided by the model. S-NeRF
optimization is posed as a Bayesian learning problem which is efficiently
addressed using the Variational Inference framework. Exhaustive experiments
over benchmark datasets demonstrate that S-NeRF is able to provide more
reliable predictions and confidence values than generic approaches previously
proposed for uncertainty estimation in other domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paint4Poem: A Dataset for Artistic Visualization of Classical Chinese Poems. (arXiv:2109.11682v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11682">
<div class="article-summary-box-inner">
<span><p>In this work we propose a new task: artistic visualization of classical
Chinese poems, where the goal is to generatepaintings of a certain artistic
style for classical Chinese poems. For this purpose, we construct a new dataset
called Paint4Poem. Thefirst part of Paint4Poem consists of 301 high-quality
poem-painting pairs collected manually from an influential modern Chinese
artistFeng Zikai. As its small scale poses challenges for effectively training
poem-to-painting generation models, we introduce the secondpart of Paint4Poem,
which consists of 3,648 caption-painting pairs collected manually from Feng
Zikai's paintings and 89,204 poem-painting pairs collected automatically from
the web. We expect the former to help learning the artist painting style as it
containshis most paintings, and the latter to help learning the semantic
relevance between poems and paintings. Further, we analyze Paint4Poem regarding
poem diversity, painting style, and the semantic relevance between poems and
paintings. We create abenchmark for Paint4Poem: we train two representative
text-to-image generation models: AttnGAN and MirrorGAN, and evaluate
theirperformance regarding painting pictorial quality, painting stylistic
relevance, and semantic relevance between poems and paintings.The results
indicate that the models are able to generate paintings that have good
pictorial quality and mimic Feng Zikai's style, but thereflection of poem
semantics is limited. The dataset also poses many interesting research
directions on this task, including transferlearning, few-shot learning,
text-to-image generation for low-resource data etc. The dataset is publicly
available.(https://github.com/paint4poem/paint4poem)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harrisz+: Harris Corner Selection for Next-Gen Image Matching Pipelines. (arXiv:2109.12925v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12925">
<div class="article-summary-box-inner">
<span><p>Due to its role in many computer vision tasks, image matching has been
subjected to an active investigation by researchers, which has lead to better
and more discriminant feature descriptors and to more robust matching
strategies, also thanks to the advent of the deep learning and the increased
computational power of the modern hardware. Despite of these achievements, the
keypoint extraction process at the base of the image matching pipeline has not
seen equivalent progresses. This paper presents Harrisz$^{+}$, an upgrade to
the HarrisZ corner detector, optimized to synergically take advance of the
recent improvements of the other steps of the image matching pipeline.
Harrisz$^{+}$ does not only consists of a tuning of the setup parameters, but
introduces further refinements to the selection criteria delineated by HarrisZ,
so providing more, yet discriminative, keypoints, which are better distributed
on the image and with higher localization accuracy. The image matching pipeline
including Harrisz$^{+}$, together with the other modern components, obtained in
different recent matching benchmarks state-of-the-art results among the classic
image matching pipelines, closely following results of the more recent fully
deep end-to-end trainable approaches.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-29 23:02:09.653737742 UTC">2021-09-29 23:02:09 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>