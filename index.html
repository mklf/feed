<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-21T01:30:00Z">07-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding. (arXiv:2207.09514v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09514">
<div class="article-summary-box-inner">
<span><p>This paper presents recent progress on integrating speech separation and
enhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE
work, numerous features have been added, including recent state-of-the-art
speech enhancement models with their respective training and evaluation
recipes. Importantly, a new interface has been designed to flexibly combine
speech enhancement front-ends with other tasks, including automatic speech
recognition (ASR), speech translation (ST), and spoken language understanding
(SLU). To showcase such integration, we performed experiments on carefully
designed synthetic datasets for noisy-reverberant multi-channel ST and SLU
tasks, which can be used as benchmark corpora for future research. In addition
to these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi- and
single-channel SE approaches. Results show that the integration of SE
front-ends with back-end tasks is a promising research direction even for tasks
besides ASR, especially in the multi-channel scenario. The code is available
online at https://github.com/ESPnet/ESPnet. The multi-channel ST and SLU
datasets, which are another contribution of this work, are released on
HuggingFace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification. (arXiv:2207.09519v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09519">
<div class="article-summary-box-inner">
<span><p>Contrastive Vision-Language Pre-training, known as CLIP, has provided a new
paradigm for learning visual representations using large-scale image-text
pairs. It shows impressive performance on downstream tasks by zero-shot
knowledge transfer. To further enhance CLIP's adaption capability, existing
methods proposed to fine-tune additional learnable modules, which significantly
improves the few-shot performance but introduces extra training time and
computational resources. In this paper, we propose a training-free adaption
method for CLIP to conduct few-shot classification, termed as Tip-Adapter,
which not only inherits the training-free advantage of zero-shot CLIP but also
performs comparably to those training-required approaches. Tip-Adapter
constructs the adapter via a key-value cache model from the few-shot training
set, and updates the prior knowledge encoded in CLIP by feature retrieval. On
top of that, the performance of Tip-Adapter can be further boosted to be
state-of-the-art on ImageNet by fine-tuning the cache model for 10$\times$
fewer epochs than existing methods, which is both effective and efficient. We
conduct extensive experiments of few-shot classification on 11 datasets to
demonstrate the superiority of our proposed methods. Code is released at
https://github.com/gaopengcuhk/Tip-Adapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QuoteKG: A Multilingual Knowledge Graph of Quotes. (arXiv:2207.09562v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09562">
<div class="article-summary-box-inner">
<span><p>Quotes of public figures can mark turning points in history. A quote can
explain its originator's actions, foreshadowing political or personal decisions
and revealing character traits. Impactful quotes cross language barriers and
influence the general population's reaction to specific stances, always facing
the risk of being misattributed or taken out of context. The provision of a
cross-lingual knowledge graph of quotes that establishes the authenticity of
quotes and their contexts is of great importance to allow the exploration of
the lives of important people as well as topics from the perspective of what
was actually said. In this paper, we present QuoteKG, the first multilingual
knowledge graph of quotes. We propose the QuoteKG creation pipeline that
extracts quotes from Wikiquote, a free and collaboratively created collection
of quotes in many languages, and aligns different mentions of the same quote.
QuoteKG includes nearly one million quotes in $55$ languages, said by more than
$69,000$ people of public interest across a wide range of topics. QuoteKG is
publicly available and can be accessed via a SPARQL endpoint.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-guided Collaborative Problem Solving: A Natural Language based Framework. (arXiv:2207.09566v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09566">
<div class="article-summary-box-inner">
<span><p>We consider the problem of human-machine collaborative problem solving as a
planning task coupled with natural language communication. Our framework
consists of three components -- a natural language engine that parses the
language utterances to a formal representation and vice-versa, a concept
learner that induces generalized concepts for plans based on limited
interactions with the user, and an HTN planner that solves the task based on
human interaction. We illustrate the ability of this framework to address the
key challenges of collaborative problem solving by demonstrating it on a
collaborative building task in a Minecraft-based blocksworld domain. The
accompanied demo video is available at https://youtu.be/q1pWe4aahF0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doge Tickets: Uncovering Domain-general Language Models by Playing Lottery Tickets. (arXiv:2207.09638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09638">
<div class="article-summary-box-inner">
<span><p>Over-parameterized models, typically pre-trained language models (LMs), have
shown an appealing expressive power due to their small learning bias. However,
the huge learning capacity of LMs can also lead to large learning variance. In
a pilot study, we find that, when faced with multiple domains, a critical
portion of parameters behave unexpectedly in a domain-specific manner while
others behave in a domain-general one. Motivated by this phenomenon, we for the
first time posit that domain-general parameters can underpin a domain-general
LM that can be derived from the original LM. To uncover the domain-general LM,
we propose to identify domain-general parameters by playing lottery tickets
(dubbed doge tickets). In order to intervene the lottery, we propose a
domain-general score, which depicts how domain-invariant a parameter is by
associating it with the variance. Comprehensive experiments are conducted on
the Amazon, Mnli and OntoNotes datasets. The results show that the doge tickets
obtains an improved out-of-domain generalization in comparison with a range of
competitive baselines. Analysis results further hint the existence of
domain-general parameters and the performance consistency of doge tickets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Linguistic Theory and Neural Language Models. (arXiv:2207.09643v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09643">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models have recently achieved remarkable results
in many natural language tasks. However, performance on leaderboards is
generally achieved by leveraging massive amounts of training data, and rarely
by encoding explicit linguistic knowledge into neural models. This has led many
to question the relevance of linguistics for modern natural language
processing. In this dissertation, I present several case studies to illustrate
how theoretical linguistics and neural language models are still relevant to
each other. First, language models are useful to linguists by providing an
objective tool to measure semantic distance, which is difficult to do using
traditional methods. On the other hand, linguistic theory contributes to
language modelling research by providing frameworks and sources of data to
probe our language models for specific aspects of language understanding.
</p>
<p>This thesis contributes three studies that explore different aspects of the
syntax-semantics interface in language models. In the first part of my thesis,
I apply language models to the problem of word class flexibility. Using mBERT
as a source of semantic distance measurements, I present evidence in favour of
analyzing word class flexibility as a directional process. In the second part
of my thesis, I propose a method to measure surprisal at intermediate layers of
language models. My experiments show that sentences containing morphosyntactic
anomalies trigger surprisals earlier in language models than semantic and
commonsense anomalies. Finally, in the third part of my thesis, I adapt several
psycholinguistic studies to show that language models contain knowledge of
argument structure constructions. In summary, my thesis develops new
connections between natural language processing, linguistic theory, and
psycholinguistics to provide fresh perspectives for the interpretation of
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features. (arXiv:2207.09666v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09666">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art methods for image captioning employ region-based
features, as they provide object-level information that is essential to
describe the content of images; they are usually extracted by an object
detector such as Faster R-CNN. However, they have several issues, such as lack
of contextual information, the risk of inaccurate detection, and the high
computational cost. The first two could be resolved by additionally using
grid-based features. However, how to extract and fuse these two types of
features is uncharted. This paper proposes a Transformer-only neural
architecture, dubbed GRIT (Grid- and Region-based Image captioning
Transformer), that effectively utilizes the two visual features to generate
better captions. GRIT replaces the CNN-based detector employed in previous
methods with a DETR-based one, making it computationally faster. Moreover, its
monolithic design consisting only of Transformers enables end-to-end training
of the model. This innovative design and the integration of the dual visual
features bring about significant performance improvement. The experimental
results on several image captioning benchmarks show that GRIT outperforms
previous methods in inference accuracy and speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Data Driven Inverse Text Normalization using Data Augmentation. (arXiv:2207.09674v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09674">
<div class="article-summary-box-inner">
<span><p>Inverse text normalization (ITN) is used to convert the spoken form output of
an automatic speech recognition (ASR) system to a written form. Traditional
handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile
neural modeling approaches require quality large-scale spoken-written pair
examples in the same or similar domain as the ASR system (in-domain data), to
train. Both these approaches require costly and complex annotations. In this
paper, we present a data augmentation technique that effectively generates rich
spoken-written numeric pairs from out-of-domain textual data with minimal human
annotation. We empirically demonstrate that ITN model trained using our data
augmentation technique consistently outperform ITN model trained using only
in-domain data across all numeric surfaces like cardinal, currency, and
fraction, by an overall accuracy of 14.44%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing Auxiliary Text Query-modifier to Content-based Audio Retrieval. (arXiv:2207.09732v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09732">
<div class="article-summary-box-inner">
<span><p>The amount of audio data available on public websites is growing rapidly, and
an efficient mechanism for accessing the desired data is necessary. We propose
a content-based audio retrieval method that can retrieve a target audio that is
similar to but slightly different from the query audio by introducing auxiliary
textual information which describes the difference between the query and target
audio. While the range of conventional content-based audio retrieval is limited
to audio that is similar to the query audio, the proposed method can adjust the
retrieval range by adding an embedding of the auxiliary text query-modifier to
the embedding of the query sample audio in a shared latent space. To evaluate
our method, we built a dataset comprising two different audio clips and the
text that describes the difference. The experimental results show that the
proposed method retrieves the paired audio more accurately than the baseline.
We also confirmed based on visualization that the proposed method obtains the
shared latent space in which the audio difference and the corresponding text
are represented as similar embedding vectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Word Learning in Children from the Performance of Computer Vision Systems. (arXiv:2207.09847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09847">
<div class="article-summary-box-inner">
<span><p>For human children as well as machine learning systems, a key challenge in
learning a word is linking the word to the visual phenomena it describes. We
explore this aspect of word learning by using the performance of computer
vision systems as a proxy for the difficulty of learning a word from visual
cues. We show that the age at which children acquire different categories of
words is predicted by the performance of visual classification and captioning
systems, over and above the expected effects of word frequency. The performance
of the computer vision systems is related to human judgments of the
concreteness of words, supporting the idea that we are capturing the
relationship between words and visual phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Is TTS Augmentation Through a Pivot Language Useful?. (arXiv:2207.09889v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09889">
<div class="article-summary-box-inner">
<span><p>Developing Automatic Speech Recognition (ASR) for low-resource languages is a
challenge due to the small amount of transcribed audio data. For many such
languages, audio and text are available separately, but not audio with
transcriptions. Using text, speech can be synthetically produced via
text-to-speech (TTS) systems. However, many low-resource languages do not have
quality TTS systems either. We propose an alternative: produce synthetic audio
by running text from the target language through a trained TTS system for a
higher-resource pivot language. We investigate when and how this technique is
most effective in low-resource settings. In our experiments, using several
thousand synthetic TTS text-speech pairs and duplicating authentic data to
balance yields optimal results. Our findings suggest that searching over a set
of candidate pivot languages can lead to marginal improvements and that,
surprisingly, ASR performance can by harmed by increases in measured TTS
quality. Application of these findings improves ASR by 64.5\% and 45.0\%
character error reduction rate (CERR) respectively for two low-resource
languages: Guaran\'i and Suba.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REFACTOR GNNS: Revisiting Factorisation-based Models from a Message-Passing Perspective. (arXiv:2207.09980v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09980">
<div class="article-summary-box-inner">
<span><p>Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring
success for Knowledge Graph Completion (KGC) tasks, often outperforming Graph
Neural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node
features and to generalise to unseen nodes in inductive settings. Our work
bridges the gap between FMs and GNNs by proposing REFACTOR GNNS. This new
architecture draws upon both modelling paradigms, which previously were largely
thought of as disjoint. Concretely, using a message-passing formalism, we show
how FMs can be cast as GNNs by reformulating the gradient descent procedure as
message-passing operations, which forms the basis of our REFACTOR GNNS. Across
a multitude of well-established KGC benchmarks, our REFACTOR GNNS achieve
comparable transductive performance to FMs, and state-of-the-art inductive
performance while using an order of magnitude fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Harmful Online Conversational Content towards LGBTQIA+ Individuals. (arXiv:2207.10032v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10032">
<div class="article-summary-box-inner">
<span><p>Online discussions, panels, talk page edits, etc., often contain harmful
conversational content i.e., hate speech, death threats and offensive language,
especially towards certain demographic groups. For example, individuals who
identify as members of the LGBTQIA+ community and/or BIPOC (Black, Indigenous,
People of Color) are at higher risk for abuse and harassment online. In this
work, we first introduce a real-world dataset that will enable us to study and
understand harmful online conversational content. Then, we conduct several
exploratory data analysis experiments to gain deeper insights from the dataset.
We later describe our approach for detecting harmful online Anti-LGBTQIA+
conversational content, and finally, we implement two baseline machine learning
models (i.e., Support Vector Machine and Logistic Regression), and fine-tune 3
pre-trained large language models (BERT, RoBERTa, and HateBERT). Our findings
verify that large language models can achieve very promising performance on
detecting online Anti-LGBTQIA+ conversational content detection tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transgender Community Sentiment Analysis from Social Media Data: A Natural Language Processing Approach. (arXiv:2010.13062v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13062">
<div class="article-summary-box-inner">
<span><p>Transgender community is experiencing a huge disparity in mental health
conditions compared with the general population. Interpreting the social medial
data posted by transgender people may help us understand the sentiments of
these sexual minority groups better and apply early interventions. In this
study, we manually categorize 300 social media comments posted by transgender
people to the sentiment of negative, positive, and neutral. 5 machine learning
algorithms and 2 deep neural networks are adopted to build sentiment analysis
classifiers based on the annotated data. Results show that our annotations are
reliable with a high Cohen's Kappa score over 0.8 across all three classes.
LSTM model yields an optimal performance of accuracy over 0.85 and AUC of
0.876. Our next step will focus on using advanced natural language processing
algorithms on a larger annotated dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Textual Adversarial Examples through Randomized Substitution and Vote. (arXiv:2109.05698v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05698">
<div class="article-summary-box-inner">
<span><p>A line of work has shown that natural text processing models are vulnerable
to adversarial examples. Correspondingly, various defense methods are proposed
to mitigate the threat of textual adversarial examples, eg, adversarial
training, input transformations, detection, etc. In this work, we treat the
optimization process for synonym substitution based textual adversarial attacks
as a specific sequence of word replacement, in which each word mutually
influences other words. We identify that we could destroy such mutual
interaction and eliminate the adversarial perturbation by randomly substituting
a word with its synonyms. Based on this observation, we propose a novel textual
adversarial example detection method, termed Randomized Substitution and Vote
(RS&amp;V), which votes the prediction label by accumulating the logits of k
samples generated by randomly substituting the words in the input text with
synonyms. The proposed RS&amp;V is generally applicable to any existing neural
networks without modification on the architecture or extra training, and it is
orthogonal to prior work on making the classification network itself more
robust. Empirical evaluations on three benchmark datasets demonstrate that our
RS&amp;V could detect the textual adversarial examples more successfully than the
existing detection methods while maintaining the high classification accuracy
on benign samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Omni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming E2E ASR via Supernet. (arXiv:2110.08352v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08352">
<div class="article-summary-box-inner">
<span><p>From wearables to powerful smart devices, modern automatic speech recognition
(ASR) models run on a variety of edge devices with different computational
budgets. To navigate the Pareto front of model accuracy vs model size,
researchers are trapped in a dilemma of optimizing model accuracy by training
and fine-tuning models for each individual edge device while keeping the
training GPU-hours tractable. In this paper, we propose Omni-sparsity DNN,
where a single neural network can be pruned to generate optimized model for a
large range of model sizes. We develop training strategies for Omni-sparsity
DNN that allows it to find models along the Pareto front of word-error-rate
(WER) vs model size while keeping the training GPU-hours to no more than that
of training one singular model. We demonstrate the Omni-sparsity DNN with
streaming E2E ASR models. Our results show great saving on training time and
resources with similar or better accuracy on LibriSpeech compared to
individually pruned sparse models: 2%-6.6% better WER on Test-other.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Topic Transition in Dialogue. (arXiv:2111.14188v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14188">
<div class="article-summary-box-inner">
<span><p>Transitioning between topics is a natural component of human-human dialog.
Although topic transition has been studied in dialogue for decades, only a
handful of corpora based studies have been performed to investigate the
subtleties of topic transitions. Thus, this study annotates 215 conversations
from the switchboard corpus and investigates how variables such as length,
number of topic transitions, topic transitions share by participants and
turns/topic are related. This work presents an empirical study on topic
transition in switchboard corpus followed by modelling topic transition with a
precision of 83% for in-domain(id) test set and 82% on 10 out-of-domain}(ood)
test set. It is envisioned that this work will help in emulating human-human
like topic transition in open-domain dialog systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A pragmatic account of the weak evidence effect. (arXiv:2112.03799v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03799">
<div class="article-summary-box-inner">
<span><p>Language is not only used to inform. We often seek to persuade by arguing in
favor of a particular view. Persuasion raises a number of challenges for
classical accounts of belief updating, as information cannot be taken at face
value. How should listeners account for a speaker's "hidden agenda" when
incorporating new information? Here, we extend recent probabilistic models of
recursive social reasoning to allow for persuasive goals and show that our
model provides a new pragmatic explanation for why weakly favorable arguments
may backfire, a phenomenon known as the weak evidence effect. Critically, our
model predicts a relationship between belief updating and speaker expectations:
weak evidence should only backfire when speakers are expected to act under
persuasive goals, implying the absence of stronger evidence. We introduce a
simple experimental paradigm called the Stick Contest to measure the extent to
which the weak evidence effect depends on speaker expectations, and show that a
pragmatic listener model accounts for the empirical data better than
alternative models. Our findings suggest potential avenues for rational models
of social reasoning to further illuminate decision-making phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes. (arXiv:2203.05203v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05203">
<div class="article-summary-box-inner">
<span><p>3D dense captioning is a recently-proposed novel task, where point clouds
contain more geometric information than the 2D counterpart. However, it is also
more challenging due to the higher complexity and wider variety of inter-object
relations contained in point clouds. Existing methods only treat such relations
as by-products of object feature learning in graphs without specifically
encoding them, which leads to sub-optimal results. In this paper, aiming at
improving 3D dense captioning via capturing and utilizing the complex relations
in the 3D scene, we propose MORE, a Multi-Order RElation mining model, to
support generating more descriptive and comprehensive captions. Technically,
our MORE encodes object relations in a progressive manner since complex
relations can be deduced from a limited number of basic ones. We first devise a
novel Spatial Layout Graph Convolution (SLGC), which semantically encodes
several first-order relations as edges of a graph constructed over 3D object
proposals. Next, from the resulting graph, we further extract multiple triplets
which encapsulate basic first-order relations as the basic unit, and construct
several Object-centric Triplet Attention Graphs (OTAG) to infer multi-order
relations for every target object. The updated node features from OTAG are
aggregated and fed into the caption decoder to provide abundant relational
cues, so that captions including diverse relations with context objects can be
generated. Extensive experiments on the Scan2Cap dataset prove the
effectiveness of our proposed MORE and its components, and we also outperform
the current state-of-the-art method. Our code is available at
https://github.com/SxJyJay/MORE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12886">
<div class="article-summary-box-inner">
<span><p>Preschool evaluation is crucial because it gives teachers and parents
influential knowledge about children's growth and development. The COVID-19
pandemic has highlighted the necessity of online assessment for preschool
children. One of the areas that should be tested is their ability to speak.
Employing an Automatic Speech Recognition(ASR) system is useless since they are
pre-trained on voices that are different from children's voices in terms of
frequency and amplitude. We constructed an ASR for our cognitive test system to
solve this issue using the Wav2Vec 2.0 model with a new pre-training objective
called Random Frequency Pitch(RFP). In addition, we used our new dataset to
fine-tune our model for Meaningless Words(MW) and Rapid Automatic Naming(RAN)
tests. Our new approach reaches a Word Error Rate(WER) of 6.45 on the Persian
section of the CommonVoice dataset. Furthermore, our novel methodology produces
positive outcomes in zero- and few-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech. (arXiv:2203.17190v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17190">
<div class="article-summary-box-inner">
<span><p>Recently, leveraging BERT pre-training to improve the phoneme encoder in text
to speech (TTS) has drawn increasing attention. However, the works apply
pre-training with character-based units to enhance the TTS phoneme encoder,
which is inconsistent with the TTS fine-tuning that takes phonemes as input.
Pre-training only with phonemes as input can alleviate the input mismatch but
lack the ability to model rich representations and semantic information due to
limited phoneme vocabulary. In this paper, we propose MixedPhoneme BERT, a
novel variant of the BERT model that uses mixed phoneme and sup-phoneme
representations to enhance the learning capability. Specifically, we merge the
adjacent phonemes into sup-phonemes and combine the phoneme sequence and the
merged sup-phoneme sequence as the model input, which can enhance the model
capacity to learn rich contextual representations. Experiment results
demonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS
performance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The
Mixed-Phoneme BERT achieves 3x inference speedup and similar voice quality to
the previous TTS pre-trained model PnG BERT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner. (arXiv:2205.09224v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09224">
<div class="article-summary-box-inner">
<span><p>Large language models have achieved high performance on various question
answering (QA) benchmarks, but the explainability of their output remains
elusive. Structured explanations, called entailment trees, were recently
suggested as a way to explain and inspect a QA system's answer. In order to
better generate such entailment trees, we propose an architecture called
Iterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a
given hypothesis by systematically generating a step-by-step explanation from
textual premises. The IRGR model iteratively searches for suitable premises,
constructing a single entailment step at a time. Contrary to previous
approaches, our method combines generation steps and retrieval of premises,
allowing the model to leverage intermediate conclusions, and mitigating the
input size limit of baseline encoder-decoder models. We conduct experiments
using the EntailmentBank dataset, where we outperform existing benchmarks on
premise retrieval and entailment tree generation, with around 300% gain in
overall correctness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELECTRA is a Zero-Shot Learner, Too. (arXiv:2207.08141v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08141">
<div class="article-summary-box-inner">
<span><p>Recently, for few-shot or even zero-shot learning, the new paradigm
"pre-train, prompt, and predict" has achieved remarkable achievements compared
with the "pre-train, fine-tune" paradigm. After the success of prompt-based
GPT-3, a series of masked language model (MLM)-based (e.g., BERT, RoBERTa)
prompt learning methods became popular and widely used. However, another
efficient pre-trained discriminative model, ELECTRA, has probably been
neglected. In this paper, we attempt to accomplish several NLP tasks in the
zero-shot scenario using a novel our proposed replaced token detection
(RTD)-based prompt learning method. Experimental results show that ELECTRA
model based on RTD-prompt learning achieves surprisingly state-of-the-art
zero-shot performance. Numerically, compared to MLM-RoBERTa-large and
MLM-BERT-large, our RTD-ELECTRA-large has an average of about 8.4% and 13.7%
improvement on all 15 tasks. Especially on the SST-2 task, our
RTD-ELECTRA-large achieves an astonishing 90.1% accuracy without any training
data. Overall, compared to the pre-trained masked language models, the
pre-trained replaced token detection model performs better in zero-shot
learning. The source code is available at:
https://github.com/nishiwen1214/RTD-ELECTRA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search. (arXiv:2207.09068v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09068">
<div class="article-summary-box-inner">
<span><p>Since BERT (Devlin et al., 2018), learning contextualized word embeddings has
been a de-facto standard in NLP. However, the progress of learning
contextualized phrase embeddings is hindered by the lack of a human-annotated,
phrase-in-context benchmark. To fill this gap, we propose PiC - a dataset of
~28K of noun phrases accompanied by their contextual Wikipedia pages and a
suite of three tasks of increasing difficulty for evaluating the quality of
phrase embeddings. We find that training on our dataset improves ranking
models' accuracy and remarkably pushes Question Answering (QA) models to
near-human accuracy which is 95% Exact Match (EM) on semantic search given a
query phrase and a passage. Interestingly, we find evidence that such
impressive performance is because the QA models learn to better capture the
common meaning of a phrase regardless of its actual context. That is, on our
Phrase Sense Disambiguation (PSD) task, SotA model accuracy drops substantially
(60% EM), failing to differentiate between two different senses of the same
phrase under two different contexts. Further results on our 3-task PiC
benchmark reveal that learning contextualized phrase embeddings remains an
interesting, open challenge.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of automatic prostate zones segmentation models in MRI images using U-net-like architectures. (arXiv:2207.09483v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09483">
<div class="article-summary-box-inner">
<span><p>Prostate cancer is the second-most frequently diagnosed cancer and the sixth
leading cause of cancer death in males worldwide. The main problem that
specialists face during the diagnosis of prostate cancer is the localization of
Regions of Interest (ROI) containing a tumor tissue. Currently, the
segmentation of this ROI in most cases is carried out manually by expert
doctors, but the procedure is plagued with low detection rates (of about
27-44%) or overdiagnosis in some patients. Therefore, several research works
have tackled the challenge of automatically segmenting and extracting features
of the ROI from magnetic resonance images, as this process can greatly
facilitate many diagnostic and therapeutic applications. However, the lack of
clear prostate boundaries, the heterogeneity inherent to the prostate tissue,
and the variety of prostate shapes makes this process very difficult to
automate.In this work, six deep learning models were trained and analyzed with
a dataset of MRI images obtained from the Centre Hospitalaire de Dijon and
Universitat Politecnica de Catalunya. We carried out a comparison of multiple
deep learning models (i.e. U-Net, Attention U-Net, Dense-UNet, Attention
Dense-UNet, R2U-Net, and Attention R2U-Net) using categorical cross-entropy
loss function. The analysis was performed using three metrics commonly used for
image segmentation: Dice score, Jaccard index, and mean squared error. The
model that give us the best result segmenting all the zones was R2U-Net, which
achieved 0.869, 0.782, and 0.00013 for Dice, Jaccard and mean squared error,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Analysis of Visual Product Reviews. (arXiv:2207.09499v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09499">
<div class="article-summary-box-inner">
<span><p>With the proliferation of the e-commerce industry, analyzing customer
feedback is becoming indispensable to a service provider. In recent days, it
can be noticed that customers upload the purchased product images with their
review scores. In this paper, we undertake the task of analyzing such visual
reviews, which is very new of its kind. In the past, the researchers worked on
analyzing language feedback, but here we do not take any assistance from
linguistic reviews that may be absent, since a recent trend can be observed
where customers prefer to quickly upload the visual feedback instead of typing
language feedback. We propose a hierarchical architecture, where the
higher-level model engages in product categorization, and the lower-level model
pays attention to predicting the review score from a customer-provided product
image. We generated a database by procuring real visual product reviews, which
was quite challenging. Our architecture obtained some promising results by
performing extensive experiments on the employed database. The proposed
hierarchical architecture attained a 57.48% performance improvement over the
single-level best comparable architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invariant Feature Learning for Generalized Long-Tailed Classification. (arXiv:2207.09504v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09504">
<div class="article-summary-box-inner">
<span><p>Existing long-tailed classification (LT) methods only focus on tackling the
class-wise imbalance that head classes have more samples than tail classes, but
overlook the attribute-wise imbalance. In fact, even if the class is balanced,
samples within each class may still be long-tailed due to the varying
attributes. Note that the latter is fundamentally more ubiquitous and
challenging than the former because attributes are not just implicit for most
datasets, but also combinatorially complex, thus prohibitively expensive to be
balanced. Therefore, we introduce a novel research problem: Generalized
Long-Tailed classification (GLT), to jointly consider both kinds of imbalances.
By "generalized", we mean that a GLT method should naturally solve the
traditional LT, but not vice versa. Not surprisingly, we find that most
class-wise LT methods degenerate in our proposed two benchmarks: ImageNet-GLT
and MSCOCO-GLT. We argue that it is because they over-emphasize the adjustment
of class distribution while neglecting to learn attribute-invariant features.
To this end, we propose an Invariant Feature Learning (IFL) method as the first
strong baseline for GLT. IFL first discovers environments with divergent
intra-class distributions from the imperfect predictions and then learns
invariant features across them. Promisingly, as an improved feature backbone,
IFL boosts all the LT line-up: one/two-stage re-balance, augmentation, and
ensemble. Codes and benchmarks are available on Github:
https://github.com/KaihuaTang/Generalized-Long-Tailed-Benchmarks.pytorch
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Method for Face Quality Assessment on the Edge. (arXiv:2207.09505v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09505">
<div class="article-summary-box-inner">
<span><p>Face recognition applications in practice are composed of two main steps:
face detection and feature extraction. In a sole vision-based solution, the
first step generates multiple detection for a single identity by ingesting a
camera stream. A practical approach on edge devices should prioritize these
detection of identities according to their conformity to recognition. In this
perspective, we propose a face quality score regression by just appending a
single layer to a face landmark detection network. With almost no additional
cost, face quality scores are obtained by training this single layer to regress
recognition scores with surveillance like augmentations. We implemented the
proposed approach on edge GPUs with all face detection pipeline steps,
including detection, tracking, and alignment. Comprehensive experiments show
the proposed approach's efficiency through comparison with SOTA face quality
regression models on different data sets and real-life scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeasoNet: A Seasonal Scene Classification, segmentation and Retrieval dataset for satellite Imagery over Germany. (arXiv:2207.09507v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09507">
<div class="article-summary-box-inner">
<span><p>This work presents SeasoNet, a new large-scale multi-label land cover and
land use scene understanding dataset. It includes $1\,759\,830$ images from
Sentinel-2 tiles, with 12 spectral bands and patch sizes of up to $ 120 \
\mathrm{px} \times 120 \ \mathrm{px}$. Each image is annotated with large scale
pixel level labels from the German land cover model LBM-DE2018 with land cover
classes based on the CORINE Land Cover database (CLC) 2018 and a five times
smaller minimum mapping unit (MMU) than the original CLC maps. We provide pixel
synchronous examples from all four seasons, plus an additional snowy set. These
properties make SeasoNet the currently most versatile and biggest remote
sensing scene understanding dataset with possible applications ranging from
scene classification over land cover mapping to content-based cross season
image retrieval and self-supervised feature learning. We provide baseline
results by evaluating state-of-the-art deep networks on the new dataset in
scene classification and semantic segmentation scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition and Learning from Synthetic Images. (arXiv:2207.09508v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09508">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the results of the HSE-NN team in the 4th
competition on Affective Behavior Analysis in-the-wild (ABAW). The novel
multi-task EfficientNet model is trained for simultaneous recognition of facial
expressions and prediction of valence and arousal on static photos. The
resulting MT-EmotiEffNet extracts visual features that are fed into simple
feed-forward neural networks in the multi-task learning challenge. We obtain
performance measure 1.3 on the validation set, which is significantly greater
when compared to either performance of baseline (0.3) or existing models that
are trained only on the s-Aff-Wild2 database. In the learning from synthetic
data challenge, the quality of the original synthetic training set is increased
by using the super-resolution techniques, such as Real-ESRGAN. Next, the
MT-EmotiEffNet is fine-tuned on the new training set. The final prediction is a
simple blending ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our
average validation F1 score is 18% greater than the baseline convolutional
neural network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contributions of Shape, Texture, and Color in Visual Recognition. (arXiv:2207.09510v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09510">
<div class="article-summary-box-inner">
<span><p>We investigate the contributions of three important features of the human
visual system (HVS)~ -- ~shape, texture, and color ~ -- ~to object
classification. We build a humanoid vision engine (HVE) that explicitly and
separately computes shape, texture, and color features from images. The
resulting feature vectors are then concatenated to support the final
classification. We show that HVE can summarize and rank-order the contributions
of the three features to object recognition. We use human experiments to
confirm that both HVE and humans predominantly use some specific features to
support the classification of specific classes (e.g., texture is the dominant
feature to distinguish a zebra from other quadrupeds, both for humans and HVE).
With the help of HVE, given any environment (dataset), we can summarize the
most important features for the whole task (task-specific; e.g., color is the
most important feature overall for classification with the CUB dataset), and
for each class (class-specific; e.g., shape is the most important feature to
recognize boats in the iLab-20M dataset). To demonstrate more usefulness of
HVE, we use it to simulate the open-world zero-shot learning ability of humans
with no attribute labeling. Finally, we show that HVE can also simulate human
imagination ability with the combination of different features. We will
open-source the HVE engine and corresponding datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification. (arXiv:2207.09519v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09519">
<div class="article-summary-box-inner">
<span><p>Contrastive Vision-Language Pre-training, known as CLIP, has provided a new
paradigm for learning visual representations using large-scale image-text
pairs. It shows impressive performance on downstream tasks by zero-shot
knowledge transfer. To further enhance CLIP's adaption capability, existing
methods proposed to fine-tune additional learnable modules, which significantly
improves the few-shot performance but introduces extra training time and
computational resources. In this paper, we propose a training-free adaption
method for CLIP to conduct few-shot classification, termed as Tip-Adapter,
which not only inherits the training-free advantage of zero-shot CLIP but also
performs comparably to those training-required approaches. Tip-Adapter
constructs the adapter via a key-value cache model from the few-shot training
set, and updates the prior knowledge encoded in CLIP by feature retrieval. On
top of that, the performance of Tip-Adapter can be further boosted to be
state-of-the-art on ImageNet by fine-tuning the cache model for 10$\times$
fewer epochs than existing methods, which is both effective and efficient. We
conduct extensive experiments of few-shot classification on 11 datasets to
demonstrate the superiority of our proposed methods. Code is released at
https://github.com/gaopengcuhk/Tip-Adapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Dice loss in the context of missing or empty labels: Introducing $\Phi$ and $\epsilon$. (arXiv:2207.09521v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09521">
<div class="article-summary-box-inner">
<span><p>Albeit the Dice loss is one of the dominant loss functions in medical image
segmentation, most research omits a closer look at its derivative, i.e. the
real motor of the optimization when using gradient descent. In this paper, we
highlight the peculiar action of the Dice loss in the presence of missing or
empty labels. First, we formulate a theoretical basis that gives a general
description of the Dice loss and its derivative. It turns out that the choice
of the reduction dimensions $\Phi$ and the smoothing term $\epsilon$ is
non-trivial and greatly influences its behavior. We find and propose heuristic
combinations of $\Phi$ and $\epsilon$ that work in a segmentation setting with
either missing or empty labels. Second, we empirically validate these findings
in a binary and multiclass segmentation setting using two publicly available
datasets. We confirm that the choice of $\Phi$ and $\epsilon$ is indeed
pivotal. With $\Phi$ chosen such that the reductions happen over a single batch
(and class) element and with a negligible $\epsilon$, the Dice loss deals with
missing labels naturally and performs similarly compared to recent adaptations
specific for missing labels. With $\Phi$ chosen such that the reductions happen
over multiple batch elements or with a heuristic value for $\epsilon$, the Dice
loss handles empty labels correctly. We believe that this work highlights some
essential perspectives and hope that it encourages researchers to better
describe their exact implementation of the Dice loss in future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge distillation with a class-aware loss for endoscopic disease detection. (arXiv:2207.09530v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09530">
<div class="article-summary-box-inner">
<span><p>Prevalence of gastrointestinal (GI) cancer is growing alarmingly every year
leading to a substantial increase in the mortality rate. Endoscopic detection
is providing crucial diagnostic support, however, subtle lesions in upper and
lower GI are quite hard to detect and cause considerable missed detection. In
this work, we leverage deep learning to develop a framework to improve the
localization of difficult to detect lesions and minimize the missed detection
rate. We propose an end to end student-teacher learning setup where class
probabilities of a trained teacher model on one class with larger dataset are
used to penalize multi-class student network. Our model achieves higher
performance in terms of mean average precision (mAP) on both endoscopic disease
detection (EDD2020) challenge and Kvasir-SEG datasets. Additionally, we show
that using such learning paradigm, our model is generalizable to unseen test
set giving higher APs for clinically crucial neoplastic and polyp categories
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Block-based Convolutional Neural Network for Low-Resolution Image Classification. (arXiv:2207.09531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09531">
<div class="article-summary-box-inner">
<span><p>The success of CNN-based architecture on image classification in learning and
extracting features made them so popular these days, but the task of image
classification becomes more challenging when we use state of art models to
classify noisy and low-quality images. To solve this problem, we proposed a
novel image classification architecture that learns subtle details in
low-resolution images that are blurred and noisy. In order to build our new
blocks, we used the idea of Res Connections and the Inception module ideas.
Using the MNIST datasets, we have conducted extensive experiments that show
that the introduced architecture is more accurate and faster than other
state-of-the-art Convolutional neural networks. As a result of the special
characteristics of our model, it can achieve a better result with fewer
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation of 3D Dental Images Using Deep Learning. (arXiv:2207.09582v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09582">
<div class="article-summary-box-inner">
<span><p>3D image segmentation is a recent and crucial step in many medical analysis
and recognition schemes. In fact, it represents a relevant research subject and
a fundamental challenge due to its importance and influence. This paper
provides a multi-phase Deep Learning-based system that hybridizes various
efficient methods in order to get the best 3D segmentation output. First, to
reduce the amount of data and accelerate the processing time, the application
of Decimate compression technique is suggested and justified. We then use a CNN
model to segment dental images into fifteen separated classes. In the end, a
special KNN-based transformation is applied for the purpose of removing
isolated meshes and of correcting dental forms. Experimentations demonstrate
the precision and the robustness of the selected framework applied to 3D dental
images within a private clinical benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICRICS: Iterative Compensation Recovery for Image Compressive Sensing. (arXiv:2207.09594v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09594">
<div class="article-summary-box-inner">
<span><p>Closed-loop architecture is widely utilized in automatic control systems and
attain distinguished performance. However, classical compressive sensing
systems employ open-loop architecture with separated sampling and
reconstruction units. Therefore, a method of iterative compensation recovery
for image compressive sensing (ICRICS) is proposed by introducing closed-loop
framework into traditional compresses sensing systems. The proposed method
depends on any existing approaches and upgrades their reconstruction
performance by adding negative feedback structure. Theory analysis on negative
feedback of compressive sensing systems is performed. An approximate
mathematical proof of the effectiveness of the proposed method is also
provided. Simulation experiments on more than 3 image datasets show that the
proposed method is superior to 10 competition approaches in reconstruction
performance. The maximum increment of average peak signal-to-noise ratio is
4.36 dB and the maximum increment of average structural similarity is 0.034 on
one dataset. The proposed method based on negative feedback mechanism can
efficiently correct the recovery error in the existing systems of image
compressive sensing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AiATrack: Attention in Attention for Transformer Visual Tracking. (arXiv:2207.09603v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09603">
<div class="article-summary-box-inner">
<span><p>Transformer trackers have achieved impressive advancements recently, where
the attention mechanism plays an important role. However, the independent
correlation computation in the attention mechanism could result in noisy and
ambiguous attention weights, which inhibits further performance improvement. To
address this issue, we propose an attention in attention (AiA) module, which
enhances appropriate correlations and suppresses erroneous ones by seeking
consensus among all correlation vectors. Our AiA module can be readily applied
to both self-attention blocks and cross-attention blocks to facilitate feature
aggregation and information propagation for visual tracking. Moreover, we
propose a streamlined Transformer tracking framework, dubbed AiATrack, by
introducing efficient feature reuse and target-background embeddings to make
full use of temporal references. Experiments show that our tracker achieves
state-of-the-art performance on six tracking benchmarks while running at a
real-time speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Accurate and Robust Classification in Continuously Transitioning Industrial Sprays with Mixup. (arXiv:2207.09609v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09609">
<div class="article-summary-box-inner">
<span><p>Image classification with deep neural networks has seen a surge of
technological breakthroughs with promising applications in areas such as face
recognition, medical imaging, and autonomous driving. In engineering problems,
however, such as high-speed imaging of engine fuel injector sprays or body
paint sprays, deep neural networks face a fundamental challenge related to the
availability of adequate and diverse data. Typically, only thousands or
sometimes even hundreds of samples are available for training. In addition, the
transition between different spray classes is a continuum and requires a high
level of domain expertise to label the images accurately. In this work, we used
Mixup as an approach to systematically deal with the data scarcity and
ambiguous class boundaries found in industrial spray applications. We show that
data augmentation can mitigate the over-fitting problem of large neural
networks on small data sets, to a certain level, but cannot fundamentally
resolve the issue. We discuss how a convex linear interpolation of different
classes naturally aligns with the continuous transition between different
classes in our application. Our experiments demonstrate Mixup as a simple yet
effective method to train an accurate and robust deep neural network classifier
with only a few hundred samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Deep Multi-Shape Matching. (arXiv:2207.09610v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09610">
<div class="article-summary-box-inner">
<span><p>3D shape matching is a long-standing problem in computer vision and computer
graphics. While deep neural networks were shown to lead to state-of-the-art
results in shape matching, existing learning-based approaches are limited in
the context of multi-shape matching: (i) either they focus on matching pairs of
shapes only and thus suffer from cycle-inconsistent multi-matchings, or (ii)
they require an explicit template shape to address the matching of a collection
of shapes. In this paper, we present a novel approach for deep multi-shape
matching that ensures cycle-consistent multi-matchings while not depending on
an explicit template shape. To this end, we utilise a shape-to-universe
multi-matching representation that we combine with powerful functional map
regularisation, so that our multi-shape matching neural network can be trained
in a fully unsupervised manner. While the functional map regularisation is only
considered during training time, functional maps are not computed for
predicting correspondences, thereby allowing for fast inference. We demonstrate
that our method achieves state-of-the-art results on several challenging
benchmark datasets, and, most remarkably, that our unsupervised method even
outperforms recent supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Domain Transferability for Collaborative Inter-level Domain Adaptive Object Detection. (arXiv:2207.09613v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09613">
<div class="article-summary-box-inner">
<span><p>Domain adaptation for object detection (DAOD) has recently drawn much
attention owing to its capability of detecting target objects without any
annotations. To tackle the problem, previous works focus on aligning features
extracted from partial levels (e.g., image-level, instance-level, RPN-level) in
a two-stage detector via adversarial training. However, individual levels in
the object detection pipeline are closely related to each other and this
inter-level relation is unconsidered yet. To this end, we introduce a novel
framework for DAOD with three proposed components: Multi-scale-aware
Uncertainty Attention (MUA), Transferable Region Proposal Network (TRPN), and
Dynamic Instance Sampling (DIS). With these modules, we seek to reduce the
negative transfer effect during training while maximizing transferability as
well as discriminability in both domains. Finally, our framework implicitly
learns domain invariant regions for object detection via exploiting the
transferable information and enhances the complementarity between different
detection levels by collaboratively utilizing their domain information. Through
ablation studies and experiments, we show that the proposed modules contribute
to the performance improvement in a synergic way, demonstrating the
effectiveness of our method. Moreover, our model achieves a new
state-of-the-art performance on various benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overlooked factors in concept-based explanations: Dataset choice, concept salience, and human capability. (arXiv:2207.09615v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09615">
<div class="article-summary-box-inner">
<span><p>Concept-based interpretability methods aim to explain deep neural network
model predictions using a predefined set of semantic concepts. These methods
evaluate a trained model on a new, "probe" dataset and correlate model
predictions with the visual concepts labeled in that dataset. Despite their
popularity, they suffer from limitations that are not well-understood and
articulated by the literature. In this work, we analyze three commonly
overlooked factors in concept-based explanations. First, the choice of the
probe dataset has a profound impact on the generated explanations. Our analysis
reveals that different probe datasets may lead to very different explanations,
and suggests that the explanations are not generalizable outside the probe
dataset. Second, we find that concepts in the probe dataset are often less
salient and harder to learn than the classes they claim to explain, calling
into question the correctness of the explanations. We argue that only visually
salient concepts should be used in concept-based explanations. Finally, while
existing methods use hundreds or even thousands of concepts, our human studies
reveal a much stricter upper bound of 32 concepts or less, beyond which the
explanations are much less practically useful. We make suggestions for future
development and analysis of concept-based interpretability methods. Code for
our analysis and user interface can be found at
\url{https://github.com/princetonvisualai/OverlookedFactors}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from few examples: Classifying sex from retinal images via deep learning. (arXiv:2207.09624v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09624">
<div class="article-summary-box-inner">
<span><p>Deep learning has seen tremendous interest in medical imaging, particularly
in the use of convolutional neural networks (CNNs) for developing automated
diagnostic tools. The facility of its non-invasive acquisition makes retinal
fundus imaging amenable to such automated approaches. Recent work in analyzing
fundus images using CNNs relies on access to massive data for training and
validation - hundreds of thousands of images. However, data residency and data
privacy restrictions stymie the applicability of this approach in medical
settings where patient confidentiality is a mandate. Here, we showcase results
for the performance of DL on small datasets to classify patient sex from fundus
images - a trait thought not to be present or quantifiable in fundus images
until recently. We fine-tune a Resnet-152 model whose last layer has been
modified for binary classification. In several experiments, we assess
performance in the small dataset context using one private (DOVS) and one
public (ODIR) data source. Our models, developed using approximately 2500
fundus images, achieved test AUC scores of up to 0.72 (95% CI: [0.67, 0.77]).
This corresponds to a mere 25% decrease in performance despite a nearly
1000-fold decrease in the dataset size compared to prior work in the
literature. Even with a hard task like sex categorization from retinal images,
we find that classification is possible with very small datasets. Additionally,
we perform domain adaptation experiments between DOVS and ODIR; explore the
effect of data curation on training and generalizability; and investigate model
ensembling to maximize CNN classifier performance in the context of small
development datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explicit Image Caption Editing. (arXiv:2207.09625v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09625">
<div class="article-summary-box-inner">
<span><p>Given an image and a reference caption, the image caption editing task aims
to correct the misalignment errors and generate a refined caption. However, all
existing caption editing works are implicit models, ie, they directly produce
the refined captions without explicit connections to the reference captions. In
this paper, we introduce a new task: Explicit Caption Editing (ECE). ECE models
explicitly generate a sequence of edit operations, and this edit operation
sequence can translate the reference caption into a refined one. Compared to
the implicit editing, ECE has multiple advantages: 1) Explainable: it can trace
the whole editing path. 2) Editing Efficient: it only needs to modify a few
words. 3) Human-like: it resembles the way that humans perform caption editing,
and tries to keep original sentence structures. To solve this new task, we
propose the first ECE model: TIger. TIger is a non-autoregressive
transformer-based model, consisting of three modules: Tagger_del, Tagger_add,
and Inserter. Specifically, Tagger_del decides whether each word should be
preserved or not, Tagger_add decides where to add new words, and Inserter
predicts the specific word for adding. To further facilitate ECE research, we
propose two new ECE benchmarks by re-organizing two existing datasets, dubbed
COCO-EE and Flickr30K-EE, respectively. Extensive ablations on both two
benchmarks have demonstrated the effectiveness of TIger.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVHA: Explainable Vision System for Hardware Testing and Assurance -- An Overview. (arXiv:2207.09627v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09627">
<div class="article-summary-box-inner">
<span><p>Due to the ever-growing demands for electronic chips in different sectors the
semiconductor companies have been mandated to offshore their manufacturing
processes. This unwanted matter has made security and trustworthiness of their
fabricated chips concerning and caused creation of hardware attacks. In this
condition, different entities in the semiconductor supply chain can act
maliciously and execute an attack on the design computing layers, from devices
to systems. Our attack is a hardware Trojan that is inserted during mask
generation/fabrication in an untrusted foundry. The Trojan leaves a footprint
in the fabricated through addition, deletion, or change of design cells. In
order to tackle this problem, we propose Explainable Vision System for Hardware
Testing and Assurance (EVHA) in this work that can detect the smallest possible
change to a design in a low-cost, accurate, and fast manner. The inputs to this
system are Scanning Electron Microscopy (SEM) images acquired from the
Integrated Circuits (ICs) under examination. The system output is determination
of IC status in terms of having any defect and/or hardware Trojan through
addition, deletion, or change in the design cells at the cell-level. This
article provides an overview on the design, development, implementation, and
analysis of our defense system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective Phase Angle Model for Polarimetric 3D Reconstruction. (arXiv:2207.09629v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09629">
<div class="article-summary-box-inner">
<span><p>Current polarimetric 3D reconstruction methods, including those in the
well-established shape from polarization literature, are all developed under
the orthographic projection assumption. In the case of a large field of view,
however, this assumption does not hold and may result in significant
reconstruction errors in methods that make this assumption. To address this
problem, we present the perspective phase angle (PPA) model that is applicable
to perspective cameras. Compared with the orthographic model, the proposed PPA
model accurately describes the relationship between polarization phase angle
and surface normal under perspective projection. In addition, the PPA model
makes it possible to estimate surface normals from only one single-view phase
angle map and does not suffer from the so-called {\pi}-ambiguity problem.
Experiments on real data show that the PPA model is more accurate for surface
normal estimation with a perspective camera than the orthographic model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperNet: Self-Supervised Hyperspectral Spatial-Spectral Feature Understanding Network for Hyperspectral Change Detection. (arXiv:2207.09634v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09634">
<div class="article-summary-box-inner">
<span><p>The fast development of self-supervised learning lowers the bar learning
feature representation from massive unlabeled data and has triggered a series
of research on change detection of remote sensing images. Challenges in
adapting self-supervised learning from natural images classification to remote
sensing images change detection arise from difference between the two tasks.
The learned patch-level feature representations are not satisfying for the
pixel-level precise change detection. In this paper, we proposed a novel
pixel-level self-supervised hyperspectral spatial-spectral understanding
network (HyperNet) to accomplish pixel-wise feature representation for
effective hyperspectral change detection. Concretely, not patches but the whole
images are fed into the network and the multi-temporal spatial-spectral
features are compared pixel by pixel. Instead of processing the two-dimensional
imaging space and spectral response dimension in hybrid style, a powerful
spatial-spectral attention module is put forward to explore the spatial
correlation and discriminative spectral features of multi-temporal
hyperspectral images (HSIs), separately. Only the positive samples at the same
location of bi-temporal HSIs are created and forced to be aligned, aiming at
learning the spectral difference-invariant features. Moreover, a new similarity
loss function named focal cosine is proposed to solve the problem of imbalanced
easy and hard positive samples comparison, where the weights of those hard
samples are enlarged and highlighted to promote the network training. Six
hyperspectral datasets have been adopted to test the validity and
generalization of proposed HyperNet. The extensive experiments demonstrate the
superiority of HyperNet over the state-of-the-art algorithms on downstream
hyperspectral change detection tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DC-BENCH: Dataset Condensation Benchmark. (arXiv:2207.09639v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09639">
<div class="article-summary-box-inner">
<span><p>Dataset Condensation is a newly emerging technique aiming at learning a tiny
dataset that captures the rich information encoded in the original dataset. As
the size of datasets contemporary machine learning models rely on becomes
increasingly large, condensation methods become a prominent direction for
accelerating network training and reducing data storage. Despite numerous
methods have been proposed in this rapidly growing field, evaluating and
comparing different condensation methods is non-trivial and still remains an
open issue. The quality of condensed dataset are often shadowed by many
critical contributing factors to the end performance, such as data augmentation
and model architectures. The lack of a systematic way to evaluate and compare
condensation methods not only hinders our understanding of existing techniques,
but also discourages practical usage of the synthesized datasets. This work
provides the first large-scale standardized benchmark on Dataset Condensation.
It consists of a suite of evaluations to comprehensively reflect the
generability and effectiveness of condensation methods through the lens of
their generated dataset. Leveraging this benchmark, we conduct a large-scale
study of current condensation methods, and report many insightful findings that
open up new possibilities for future development. The benchmark library,
including evaluators, baseline methods, and generated datasets, is open-sourced
to facilitate future research and application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchically Self-Supervised Transformer for Human Skeleton Representation Learning. (arXiv:2207.09644v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09644">
<div class="article-summary-box-inner">
<span><p>Despite the success of fully-supervised human skeleton sequence modeling,
utilizing self-supervised pre-training for skeleton sequence representation
learning has been an active field because acquiring task-specific skeleton
annotations at large scales is difficult. Recent studies focus on learning
video-level temporal and discriminative information using contrastive learning,
but overlook the hierarchical spatial-temporal nature of human skeletons.
Different from such superficial supervision at the video level, we propose a
self-supervised hierarchical pre-training scheme incorporated into a
hierarchical Transformer-based skeleton sequence encoder (Hi-TRS), to
explicitly capture spatial, short-term, and long-term temporal dependencies at
frame, clip, and video levels, respectively. To evaluate the proposed
self-supervised pre-training scheme with Hi-TRS, we conduct extensive
experiments covering three skeleton-based downstream tasks including action
recognition, action detection, and motion prediction. Under both supervised and
semi-supervised evaluation protocols, our method achieves the state-of-the-art
performance. Additionally, we demonstrate that the prior knowledge learned by
our model in the pre-training stage has strong transfer capability for
different downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aware of the History: Trajectory Forecasting with the Local Behavior Data. (arXiv:2207.09646v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09646">
<div class="article-summary-box-inner">
<span><p>The historical trajectories previously passing through a location may help
infer the future trajectory of an agent currently at this location. Despite
great improvements in trajectory forecasting with the guidance of
high-definition maps, only a few works have explored such local historical
information. In this work, we re-introduce this information as a new type of
input data for trajectory forecasting systems: the local behavior data, which
we conceptualize as a collection of location-specific historical trajectories.
Local behavior data helps the systems emphasize the prediction locality and
better understand the impact of static map objects on moving agents. We propose
a novel local-behavior-aware (LBA) prediction framework that improves
forecasting accuracy by fusing information from observed trajectories, HD maps,
and local behavior data. Also, where such historical data is insufficient or
unavailable, we employ a local-behavior-free (LBF) prediction framework, which
adopts a knowledge-distillation-based architecture to infer the impact of
missing data. Extensive experiments demonstrate that upgrading existing methods
with these two frameworks significantly improves their performances.
Especially, the LBA framework boosts the SOTA methods' performance on the
nuScenes dataset by at least 14% for the K=1 metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenText: Unsupervised Artistic Text Generation via Decoupled Font and Texture Manipulation. (arXiv:2207.09649v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09649">
<div class="article-summary-box-inner">
<span><p>Automatic artistic text generation is an emerging topic which receives
increasing attention due to its wide applications. The artistic text can be
divided into three components, content, font, and texture, respectively.
Existing artistic text generation models usually focus on manipulating one
aspect of the above components, which is a sub-optimal solution for
controllable general artistic text generation. To remedy this issue, we propose
a novel approach, namely GenText, to achieve general artistic text style
transfer by separably migrating the font and texture styles from the different
source images to the target images in an unsupervised manner. Specifically, our
current work incorporates three different stages, stylization, destylization,
and font transfer, respectively, into a unified platform with a single powerful
encoder network and two separate style generator networks, one for font
transfer, the other for stylization and destylization. The destylization stage
first extracts the font style of the font reference image, then the font
transfer stage generates the target content with the desired font style.
Finally, the stylization stage renders the resulted font image with respect to
the texture style in the reference image. Moreover, considering the difficult
data acquisition of paired artistic text images, our model is designed under
the unsupervised setting, where all stages can be effectively optimized from
unpaired data. Qualitative and quantitative results are performed on artistic
text benchmarks, which demonstrate the superior performance of our proposed
model. The code with models will become publicly available in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Topological Interactions for Multi-Class Medical Image Segmentation. (arXiv:2207.09654v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09654">
<div class="article-summary-box-inner">
<span><p>Deep learning methods have achieved impressive performance for multi-class
medical image segmentation. However, they are limited in their ability to
encode topological interactions among different classes (e.g., containment and
exclusion). These constraints naturally arise in biomedical images and can be
crucial in improving segmentation quality. In this paper, we introduce a novel
topological interaction module to encode the topological interactions into a
deep neural network. The implementation is completely convolution-based and
thus can be very efficient. This empowers us to incorporate the constraints
into end-to-end training and enrich the feature representation of neural
networks. The efficacy of the proposed method is validated on different types
of interactions. We also demonstrate the generalizability of the method on both
proprietary and public challenge datasets, in both 2D and 3D settings, as well
as across different modalities such as CT and Ultrasound. Code is available at:
https://github.com/TopoXLab/TopoInteraction
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for One-stage Object Detector using Offsets to Bounding Box. (arXiv:2207.09656v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09656">
<div class="article-summary-box-inner">
<span><p>Most existing domain adaptive object detection methods exploit adversarial
feature alignment to adapt the model to a new domain. Recent advances in
adversarial feature alignment strives to reduce the negative effect of
alignment, or negative transfer, that occurs because the distribution of
features varies depending on the category of objects. However, by analyzing the
features of the anchor-free one-stage detector, in this paper, we find that
negative transfer may occur because the feature distribution varies depending
on the regression value for the offset to the bounding box as well as the
category. To obtain domain invariance by addressing this issue, we align the
feature conditioned on the offset value, considering the modality of the
feature distribution. With a very simple and effective conditioning method, we
propose OADA (Offset-Aware Domain Adaptive object detector) that achieves
state-of-the-art performances in various experimental settings. In addition, by
analyzing through singular value decomposition, we find that our model enhances
both discriminability and transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Depth from Focus in the Wild. (arXiv:2207.09658v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09658">
<div class="article-summary-box-inner">
<span><p>For better photography, most recent commercial cameras including smartphones
have either adopted large-aperture lens to collect more light or used a burst
mode to take multiple images within short times. These interesting features
lead us to examine depth from focus/defocus.
</p>
<p>In this work, we present a convolutional neural network-based depth
estimation from single focal stacks. Our method differs from relevant
state-of-the-art works with three unique features. First, our method allows
depth maps to be inferred in an end-to-end manner even with image alignment.
Second, we propose a sharp region detection module to reduce blur ambiguities
in subtle focus changes and weakly texture-less regions. Third, we design an
effective downsampling module to ease flows of focal information in feature
extractions. In addition, for the generalization of the proposed network, we
develop a simulator to realistically reproduce the features of commercial
cameras, such as changes in field of view, focal length and principal points.
</p>
<p>By effectively incorporating these three unique features, our network
achieves the top rank in the DDFF 12-Scene benchmark on most metrics. We also
demonstrate the effectiveness of the proposed method on various quantitative
evaluations and real-world images taken from various off-the-shelf cameras
compared with state-of-the-art methods. Our source code is publicly available
at https://github.com/wcy199705/DfFintheWild.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hand-Assisted Expression Recognition Method from Synthetic Images at the Fourth ABAW Challenge. (arXiv:2207.09661v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09661">
<div class="article-summary-box-inner">
<span><p>Learning from synthetic images plays an important role in facial expression
recognition task due to the difficulties of labeling the real images, and it is
challenging because of the gap between the synthetic images and real images.
The fourth Affective Behavior Analysis in-the-wild Competition raises the
challenge and provides the synthetic images generated from Aff-Wild2 dataset.
In this paper, we propose a hand-assisted expression recognition method to
reduce the gap between the synthetic data and real data. Our method consists of
two parts: expression recognition module and hand prediction module. Expression
recognition module extracts expression information and hand prediction module
predicts whether the image contains hands. Decision mode is used to combine the
results of two modules, and post-pruning is used to improve the result. F1
score is used to verify the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HTNet: Anchor-free Temporal Action Localization with Hierarchical Transformers. (arXiv:2207.09662v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09662">
<div class="article-summary-box-inner">
<span><p>Temporal action localization (TAL) is a task of identifying a set of actions
in a video, which involves localizing the start and end frames and classifying
each action instance. Existing methods have addressed this task by using
predefined anchor windows or heuristic bottom-up boundary-matching strategies,
which are major bottlenecks in inference time. Additionally, the main challenge
is the inability to capture long-range actions due to a lack of global
contextual information. In this paper, we present a novel anchor-free
framework, referred to as HTNet, which predicts a set of &lt;start time, end time,
class&gt; triplets from a video based on a Transformer architecture. After the
prediction of coarse boundaries, we refine it through a background feature
sampling (BFS) module and hierarchical Transformers, which enables our model to
aggregate global contextual information and effectively exploit the inherent
semantic relationships in a video. We demonstrate how our method localizes
accurate action instances and achieves state-of-the-art performance on two TAL
benchmark datasets: THUMOS14 and ActivityNet 1.3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streamable Neural Fields. (arXiv:2207.09663v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09663">
<div class="article-summary-box-inner">
<span><p>Neural fields have emerged as a new data representation paradigm and have
shown remarkable success in various signal representations. Since they preserve
signals in their network parameters, the data transfer by sending and receiving
the entire model parameters prevents this emerging technology from being used
in many practical scenarios. We propose streamable neural fields, a single
model that consists of executable sub-networks of various widths. The proposed
architectural and training techniques enable a single network to be streamable
over time and reconstruct different qualities and parts of signals. For
example, a smaller sub-network produces smooth and low-frequency signals, while
a larger sub-network can represent fine details. Experimental results have
shown the effectiveness of our method in various domains, such as 2D images,
videos, and 3D signed distance functions. Finally, we demonstrate that our
proposed method improves training stability, by exploiting parameter sharing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-label Guided Cross-video Pixel Contrast for Robotic Surgical Scene Segmentation with Limited Annotations. (arXiv:2207.09664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09664">
<div class="article-summary-box-inner">
<span><p>Surgical scene segmentation is fundamentally crucial for prompting cognitive
assistance in robotic surgery. However, pixel-wise annotating surgical video in
a frame-by-frame manner is expensive and time consuming. To greatly reduce the
labeling burden, in this work, we study semi-supervised scene segmentation from
robotic surgical video, which is practically essential yet rarely explored
before. We consider a clinically suitable annotation situation under the
equidistant sampling. We then propose PGV-CL, a novel pseudo-label guided
cross-video contrast learning method to boost scene segmentation. It
effectively leverages unlabeled data for a trusty and global model
regularization that produces more discriminative feature representation.
Concretely, for trusty representation learning, we propose to incorporate
pseudo labels to instruct the pair selection, obtaining more reliable
representation pairs for pixel contrast. Moreover, we expand the representation
learning space from previous image-level to cross-video, which can capture the
global semantics to benefit the learning process. We extensively evaluate our
method on a public robotic surgery dataset EndoVis18 and a public cataract
dataset CaDIS. Experimental results demonstrate the effectiveness of our
method, consistently outperforming the state-of-the-art semi-supervised methods
under different labeling ratios, and even surpassing fully supervised training
on EndoVis18 with 10.1% labeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features. (arXiv:2207.09666v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09666">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art methods for image captioning employ region-based
features, as they provide object-level information that is essential to
describe the content of images; they are usually extracted by an object
detector such as Faster R-CNN. However, they have several issues, such as lack
of contextual information, the risk of inaccurate detection, and the high
computational cost. The first two could be resolved by additionally using
grid-based features. However, how to extract and fuse these two types of
features is uncharted. This paper proposes a Transformer-only neural
architecture, dubbed GRIT (Grid- and Region-based Image captioning
Transformer), that effectively utilizes the two visual features to generate
better captions. GRIT replaces the CNN-based detector employed in previous
methods with a DETR-based one, making it computationally faster. Moreover, its
monolithic design consisting only of Transformers enables end-to-end training
of the model. This innovative design and the integration of the dual visual
features bring about significant performance improvement. The experimental
results on several image captioning benchmarks show that GRIT outperforms
previous methods in inference accuracy and speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERA: Expert Retrieval and Assembly for Early Action Prediction. (arXiv:2207.09675v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09675">
<div class="article-summary-box-inner">
<span><p>Early action prediction aims to successfully predict the class label of an
action before it is completely performed. This is a challenging task because
the beginning stages of different actions can be very similar, with only minor
subtle differences for discrimination. In this paper, we propose a novel Expert
Retrieval and Assembly (ERA) module that retrieves and assembles a set of
experts most specialized at using discriminative subtle differences, to
distinguish an input sample from other highly similar samples. To encourage our
model to effectively use subtle differences for early action prediction, we
push experts to discriminate exclusively between samples that are highly
similar, forcing these experts to learn to use subtle differences that exist
between those samples. Additionally, we design an effective Expert Learning
Rate Optimization method that balances the experts' optimization and leads to
better performance. We evaluate our ERA module on four public action datasets
and achieve state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Deepfake Detection by Analysing Image Matching. (arXiv:2207.09679v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09679">
<div class="article-summary-box-inner">
<span><p>This paper aims to interpret how deepfake detection models learn artifact
features of images when just supervised by binary labels. To this end, three
hypotheses from the perspective of image matching are proposed as follows. 1.
Deepfake detection models indicate real/fake images based on visual concepts
that are neither source-relevant nor target-relevant, that is, considering such
visual concepts as artifact-relevant. 2. Besides the supervision of binary
labels, deepfake detection models implicitly learn artifact-relevant visual
concepts through the FST-Matching (i.e. the matching fake, source, target
images) in the training set. 3. Implicitly learned artifact visual concepts
through the FST-Matching in the raw training set are vulnerable to video
compression. In experiments, the above hypotheses are verified among various
DNNs. Furthermore, based on this understanding, we propose the FST-Matching
Deepfake Detection Model to boost the performance of forgery detection on
compressed videos. Experiment results show that our method achieves great
performance, especially on highly-compressed (e.g. c40) videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Versatile Uses of Partial Distance Correlation in Deep Learning. (arXiv:2207.09684v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09684">
<div class="article-summary-box-inner">
<span><p>Comparing the functional behavior of neural network models, whether it is a
single network over time or two (or more networks) during or post-training, is
an essential step in understanding what they are learning (and what they are
not), and for identifying strategies for regularization or efficiency
improvements. Despite recent progress, e.g., comparing vision transformers to
CNNs, systematic comparison of function, especially across different networks,
remains difficult and is often carried out layer by layer. Approaches such as
canonical correlation analysis (CCA) are applicable in principle, but have been
sparingly used so far. In this paper, we revisit a (less widely known) from
statistics, called distance correlation (and its partial variant), designed to
evaluate correlation between feature spaces of different dimensions. We
describe the steps necessary to carry out its deployment for large scale models
-- this opens the door to a surprising array of applications ranging from
conditioning one deep model w.r.t. another, learning disentangled
representations as well as optimizing diverse models that would directly be
more robust to adversarial attacks. Our experiments suggest a versatile
regularizer (or constraint) with many advantages, which avoids some of the
common difficulties one faces in such analyses. Code is at
https://github.com/zhenxingjian/Partial_Distance_Correlation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BigColor: Colorization using a Generative Color Prior for Natural Images. (arXiv:2207.09685v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09685">
<div class="article-summary-box-inner">
<span><p>For realistic and vivid colorization, generative priors have recently been
exploited. However, such generative priors often fail for in-the-wild complex
images due to their limited representation space. In this paper, we propose
BigColor, a novel colorization approach that provides vivid colorization for
diverse in-the-wild images with complex structures. While previous generative
priors are trained to synthesize both image structures and colors, we learn a
generative color prior to focus on color synthesis given the spatial structure
of an image. In this way, we reduce the burden of synthesizing image structures
from the generative prior and expand its representation space to cover diverse
images. To this end, we propose a BigGAN-inspired encoder-generator network
that uses a spatial feature map instead of a spatially-flattened BigGAN latent
code, resulting in an enlarged representation space. Our method enables robust
colorization for diverse inputs in a single forward pass, supports arbitrary
input resolutions, and provides multi-modal colorization results. We
demonstrate that BigColor significantly outperforms existing methods especially
on in-the-wild images with complex structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-Compositional Neural Implicit Surfaces. (arXiv:2207.09686v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09686">
<div class="article-summary-box-inner">
<span><p>The neural implicit representation has shown its effectiveness in novel view
synthesis and high-quality 3D reconstruction from multi-view images. However,
most approaches focus on holistic scene representation yet ignore individual
objects inside it, thus limiting potential downstream applications. In order to
learn object-compositional representation, a few works incorporate the 2D
semantic map as a cue in training to grasp the difference between objects. But
they neglect the strong connections between object geometry and instance
semantic information, which leads to inaccurate modeling of individual
instance. This paper proposes a novel framework, ObjectSDF, to build an
object-compositional neural implicit representation with high fidelity in 3D
reconstruction and object representation. Observing the ambiguity of
conventional volume rendering pipelines, we model the scene by combining the
Signed Distance Functions (SDF) of individual object to exert explicit surface
constraint. The key in distinguishing different instances is to revisit the
strong association between an individual object's SDF and semantic label.
Particularly, we convert the semantic information to a function of object SDF
and develop a unified and compact representation for scene and objects.
Experimental results show the superiority of ObjectSDF framework in
representing both the holistic object-compositional scene and the individual
instances. Code can be found at https://qianyiwu.github.io/objectsdf/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Inspired Underwater Image Enhancement. (arXiv:2207.09689v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09689">
<div class="article-summary-box-inner">
<span><p>A main challenge faced in the deep learning-based Underwater Image
Enhancement (UIE) is that the ground truth high-quality image is unavailable.
Most of the existing methods first generate approximate reference maps and then
train an enhancement network with certainty. This kind of method fails to
handle the ambiguity of the reference map. In this paper, we resolve UIE into
distribution estimation and consensus process. We present a novel probabilistic
network to learn the enhancement distribution of degraded underwater images.
Specifically, we combine conditional variational autoencoder with adaptive
instance normalization to construct the enhancement distribution. After that,
we adopt a consensus process to predict a deterministic result based on a set
of samples from the distribution. By learning the enhancement distribution, our
method can cope with the bias introduced in the reference map labeling to some
extent. Additionally, the consensus process is useful to capture a robust and
stable result. We examined the proposed method on two widely used real-world
underwater image enhancement datasets. Experimental results demonstrate that
our approach enables sampling possible enhancement predictions. Meanwhile, the
consensus estimate yields competitive performance compared with
state-of-the-art UIE methods. Code available at
https://github.com/zhenqifu/PUIE-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Meta-Tuning for Content-aware Neural Video Delivery. (arXiv:2207.09691v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09691">
<div class="article-summary-box-inner">
<span><p>Recently, Deep Neural Networks (DNNs) are utilized to reduce the bandwidth
and improve the quality of Internet video delivery. Existing methods train
corresponding content-aware super-resolution (SR) model for each video chunk on
the server, and stream low-resolution (LR) video chunks along with SR models to
the client. Although they achieve promising results, the huge computational
cost of network training limits their practical applications. In this paper, we
present a method named Efficient Meta-Tuning (EMT) to reduce the computational
cost. Instead of training from scratch, EMT adapts a meta-learned model to the
first chunk of the input video. As for the following chunks, it fine-tunes the
partial parameters selected by gradient masking of previous adapted model. In
order to achieve further speedup for EMT, we propose a novel sampling strategy
to extract the most challenging patches from video frames. The proposed
strategy is highly efficient and brings negligible additional cost. Our method
significantly reduces the computational cost and achieves even better
performance, paving the way for applying neural video delivery techniques to
practical applications. We conduct extensive experiments based on various
efficient SR architectures, including ESPCN, SRCNN, FSRCNN and EDSR-1,
demonstrating the generalization ability of our work. The code is released at
\url{https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Object Detection With Inaccurate Bounding Boxes. (arXiv:2207.09697v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09697">
<div class="article-summary-box-inner">
<span><p>Learning accurate object detectors often requires large-scale training data
with precise object bounding boxes. However, labeling such data is expensive
and time-consuming. As the crowd-sourcing labeling process and the ambiguities
of the objects may raise noisy bounding box annotations, the object detectors
will suffer from the degenerated training data. In this work, we aim to address
the challenge of learning robust object detectors with inaccurate bounding
boxes. Inspired by the fact that localization precision suffers significantly
from inaccurate bounding boxes while classification accuracy is less affected,
we propose leveraging classification as a guidance signal for refining
localization results. Specifically, by treating an object as a bag of
instances, we introduce an Object-Aware Multiple Instance Learning approach
(OA-MIL), featured with object-aware instance selection and object-aware
instance extension. The former aims to select accurate instances for training,
instead of directly using inaccurate box annotations. The latter focuses on
generating high-quality instances for selection. Extensive experiments on
synthetic noisy datasets (i.e., noisy PASCAL VOC and MS-COCO) and a real noisy
wheat head dataset demonstrate the effectiveness of our OA-MIL. Code is
available at https://github.com/cxliu0/OA-MIL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resolving Copycat Problems in Visual Imitation Learning via Residual Action Prediction. (arXiv:2207.09705v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09705">
<div class="article-summary-box-inner">
<span><p>Imitation learning is a widely used policy learning method that enables
intelligent agents to acquire complex skills from expert demonstrations. The
input to the imitation learning algorithm is usually composed of both the
current observation and historical observations since the most recent
observation might not contain enough information. This is especially the case
with image observations, where a single image only includes one view of the
scene, and it suffers from a lack of motion information and object occlusions.
In theory, providing multiple observations to the imitation learning agent will
lead to better performance. However, surprisingly people find that sometimes
imitation from observation histories performs worse than imitation from the
most recent observation. In this paper, we explain this phenomenon from the
information flow within the neural network perspective. We also propose a novel
imitation learning neural network architecture that does not suffer from this
issue by design. Furthermore, our method scales to high-dimensional image
observations. Finally, we benchmark our approach on two widely used simulators,
CARLA and MuJoCo, and it successfully alleviates the copycat problem and
surpasses the existing solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Sequence Representations by Non-local Recurrent Neural Memory. (arXiv:2207.09710v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09710">
<div class="article-summary-box-inner">
<span><p>The key challenge of sequence representation learning is to capture the
long-range temporal dependencies. Typical methods for supervised sequence
representation learning are built upon recurrent neural networks to capture
temporal dependencies. One potential limitation of these methods is that they
only model one-order information interactions explicitly between adjacent time
steps in a sequence, hence the high-order interactions between nonadjacent time
steps are not fully exploited. It greatly limits the capability of modeling the
long-range temporal dependencies since the temporal features learned by
one-order interactions cannot be maintained for a long term due to temporal
information dilution and gradient vanishing. To tackle this limitation, we
propose the Non-local Recurrent Neural Memory (NRNM) for supervised sequence
representation learning, which performs non-local operations \MR{by means of
self-attention mechanism} to learn full-order interactions within a sliding
temporal memory block and models global interactions between memory blocks in a
gated recurrent manner. Consequently, our model is able to capture long-range
dependencies. Besides, the latent high-level features contained in high-order
interactions can be distilled by our model. We validate the effectiveness and
generalization of our NRNM on three types of sequence applications across
different modalities, including sequence classification, step-wise sequential
prediction and sequence similarity learning. Our model compares favorably
against other state-of-the-art methods specifically designed for each of these
sequence applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Learning for Emotion Descriptors Estimation at the fourth ABAW Challenge. (arXiv:2207.09716v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09716">
<div class="article-summary-box-inner">
<span><p>Facial valence/arousal, expression and action unit are related tasks in
facial affective analysis. However, the tasks only have limited performance in
the wild due to the various collected conditions. The 4th competition on
affective behavior analysis in the wild (ABAW) provided images with
valence/arousal, expression and action unit labels. In this paper, we introduce
multi-task learning framework to enhance the performance of three related tasks
in the wild. Feature sharing and label fusion are used to utilize their
relations. We conduct experiments on the provided training and validating data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Representation Learning for Unsupervised Cross-domain Image Retrieval. (arXiv:2207.09721v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09721">
<div class="article-summary-box-inner">
<span><p>Current supervised cross-domain image retrieval methods can achieve excellent
performance. However, the cost of data collection and labeling imposes an
intractable barrier to practical deployment in real applications. In this
paper, we investigate the unsupervised cross-domain image retrieval task, where
class labels and pairing annotations are no longer a prerequisite for training.
This is an extremely challenging task because there is no supervision for both
in-domain feature representation learning and cross-domain alignment. We
address both challenges by introducing: 1) a new cluster-wise contrastive
learning mechanism to help extract class semantic-aware features, and 2) a
novel distance-of-distance loss to effectively measure and minimize the domain
discrepancy without any external supervision. Experiments on the Office-Home
and DomainNet datasets consistently show the superior image retrieval
accuracies of our framework over state-of-the-art approaches. Our source code
can be found at https://github.com/conghuihu/UCDIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OTPose: Occlusion-Aware Transformer for Pose Estimation in Sparsely-Labeled Videos. (arXiv:2207.09725v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09725">
<div class="article-summary-box-inner">
<span><p>Although many approaches for multi-human pose estimation in videos have shown
profound results, they require densely annotated data which entails excessive
man labor. Furthermore, there exists occlusion and motion blur that inevitably
lead to poor estimation performance. To address these problems, we propose a
method that leverages an attention mask for occluded joints and encodes
temporal dependency between frames using transformers. First, our framework
composes different combinations of sparsely annotated frames that denote the
track of the overall joint movement. We propose an occlusion attention mask
from these combinations that enable encoding occlusion-aware heatmaps as a
semi-supervised task. Second, the proposed temporal encoder employs transformer
architecture to effectively aggregate the temporal relationship and
keypoint-wise attention from each time step and accurately refines the target
frame's final pose estimation. We achieve state-of-the-art pose estimation
results for PoseTrack2017 and PoseTrack2018 datasets and demonstrate the
robustness of our approach to occlusion and motion blur in sparsely annotated
video data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting data augmentation for subspace clustering. (arXiv:2207.09728v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09728">
<div class="article-summary-box-inner">
<span><p>Subspace clustering is the classical problem of clustering a collection of
data samples that approximately lie around several low-dimensional subspaces.
The current state-of-the-art approaches for this problem are based on the
self-expressive model which represents the samples as linear combination of
other samples. However, these approaches require sufficiently well-spread
samples for accurate representation which might not be necessarily accessible
in many applications. In this paper, we shed light on this commonly neglected
issue and argue that data distribution within each subspace plays a critical
role in the success of self-expressive models. Our proposed solution to tackle
this issue is motivated by the central role of data augmentation in the
generalization power of deep neural networks. We propose two subspace
clustering frameworks for both unsupervised and semi-supervised settings that
use augmented samples as an enlarged dictionary to improve the quality of the
self-expressive representation. We present an automatic augmentation strategy
using a few labeled samples for the semi-supervised problem relying on the fact
that the data samples lie in the union of multiple linear subspaces.
Experimental results confirm the effectiveness of data augmentation, as it
significantly improves the performance of general self-expressive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossHuman: Learning Cross-Guidance from Multi-Frame Images for Human Reconstruction. (arXiv:2207.09735v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09735">
<div class="article-summary-box-inner">
<span><p>We propose CrossHuman, a novel method that learns cross-guidance from
parametric human model and multi-frame RGB images to achieve high-quality 3D
human reconstruction. To recover geometry details and texture even in invisible
regions, we design a reconstruction pipeline combined with tracking-based
methods and tracking-free methods. Given a monocular RGB sequence, we track the
parametric human model in the whole sequence, the points (voxels) corresponding
to the target frame are warped to reference frames by the parametric body
motion. Guided by the geometry priors of the parametric body and spatially
aligned features from RGB sequence, the robust implicit surface is fused.
Moreover, a multi-frame transformer (MFT) and a self-supervised warp refinement
module are integrated to the framework to relax the requirements of parametric
body and help to deal with very loose cloth. Compared with previous works, our
CrossHuman enables high-fidelity geometry details and texture in both visible
and invisible regions and improves the accuracy of the human reconstruction
even under estimated inaccurate parametric human models. The experiments
demonstrate that our method achieves state-of-the-art (SOTA) performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Latent Spaces of Generative Models for Medical Images using Unsupervised Methods. (arXiv:2207.09740v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09740">
<div class="article-summary-box-inner">
<span><p>Generative models such as Generative Adversarial Networks (GANs) and
Variational Autoencoders (VAEs) play an increasingly important role in medical
image analysis. The latent spaces of these models often show semantically
meaningful directions corresponding to human-interpretable image
transformations. However, until now, their exploration for medical images has
been limited due to the requirement of supervised data. Several methods for
unsupervised discovery of interpretable directions in GAN latent spaces have
shown interesting results on natural images. This work explores the potential
of applying these techniques on medical images by training a GAN and a VAE on
thoracic CT scans and using an unsupervised method to discover interpretable
directions in the resulting latent space. We find several directions
corresponding to non-trivial image transformations, such as rotation or breast
size. Furthermore, the directions show that the generative models capture 3D
structure despite being presented only with 2D data. The results show that
unsupervised methods to discover interpretable directions in GANs generalize to
VAEs and can be applied to medical images. This opens a wide array of future
work using these methods in medical image analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial Affect Analysis: Learning from Synthetic Data & Multi-Task Learning Challenges. (arXiv:2207.09748v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09748">
<div class="article-summary-box-inner">
<span><p>Facial affect analysis remains a challenging task with its setting
transitioned from lab-controlled to in-the-wild situations. In this paper, we
present novel frameworks to handle the two challenges in the 4th Affective
Behavior Analysis In-The-Wild (ABAW) competition: i) Multi-Task-Learning (MTL)
Challenge and ii) Learning from Synthetic Data (LSD) Challenge. For MTL
challenge, we adopt the SMM-EmotionNet with a better ensemble strategy of
feature vectors. For LSD challenge, we propose respective methods to combat the
problems of single labels, imbalanced distribution, fine-tuning limitations,
and choice of model architectures. Experimental results on the official
validation sets from the competition demonstrated that our proposed approaches
outperformed baselines by a large margin. The code is available at
https://github.com/sylyoung/ABAW4-HUST-ANT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-adaptive Spatial-Temporal Video Sampler for Few-shot Action Recognition. (arXiv:2207.09759v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09759">
<div class="article-summary-box-inner">
<span><p>A primary challenge faced in few-shot action recognition is inadequate video
data for training. To address this issue, current methods in this field mainly
focus on devising algorithms at the feature level while little attention is
paid to processing input video data. Moreover, existing frame sampling
strategies may omit critical action information in temporal and spatial
dimensions, which further impacts video utilization efficiency. In this paper,
we propose a novel video frame sampler for few-shot action recognition to
address this issue, where task-specific spatial-temporal frame sampling is
achieved via a temporal selector (TS) and a spatial amplifier (SA).
Specifically, our sampler first scans the whole video at a small computational
cost to obtain a global perception of video frames. The TS plays its role in
selecting top-T frames that contribute most significantly and subsequently. The
SA emphasizes the discriminative information of each frame by amplifying
critical regions with the guidance of saliency maps. We further adopt
task-adaptive learning to dynamically adjust the sampling strategy according to
the episode task at hand. Both the implementations of TS and SA are
differentiable for end-to-end optimization, facilitating seamless integration
of our proposed sampler with most few-shot action recognition methods.
Extensive experiments show a significant boost in the performances on various
benchmarks including long-term videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIPSO: Geometrically Informed Propagation for Online Adaptation in 3D LiDAR Segmentation. (arXiv:2207.09763v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09763">
<div class="article-summary-box-inner">
<span><p>3D point cloud semantic segmentation is fundamental for autonomous driving.
Most approaches in the literature neglect an important aspect, i.e., how to
deal with domain shift when handling dynamic scenes. This can significantly
hinder the navigation capabilities of self-driving vehicles. This paper
advances the state of the art in this research field. Our first contribution
consists in analysing a new unexplored scenario in point cloud segmentation,
namely Source-Free Online Unsupervised Domain Adaptation (SF-OUDA). We
experimentally show that state-of-the-art methods have a rather limited ability
to adapt pre-trained deep network models to unseen domains in an online manner.
Our second contribution is an approach that relies on adaptive self-training
and geometric-feature propagation to adapt a pre-trained source model online
without requiring either source data or target labels. Our third contribution
is to study SF-OUDA in a challenging setup where source data is synthetic and
target data is point clouds captured in the real world. We use the recent
SynLiDAR dataset as a synthetic source and introduce two new synthetic (source)
datasets, which can stimulate future synthetic-to-real autonomous driving
research. Our experiments show the effectiveness of our segmentation approach
on thousands of real-world point clouds. Code and synthetic datasets are
available at https://github.com/saltoricristiano/gipso-sfouda.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborating Domain-shared and Target-specific Feature Clustering for Cross-domain 3D Action Recognition. (arXiv:2207.09767v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09767">
<div class="article-summary-box-inner">
<span><p>In this work, we consider the problem of cross-domain 3D action recognition
in the open-set setting, which has been rarely explored before. Specifically,
there is a source domain and a target domain that contain the skeleton
sequences with different styles and categories, and our purpose is to cluster
the target data by utilizing the labeled source data and unlabeled target data.
For such a challenging task, this paper presents a novel approach dubbed CoDT
to collaboratively cluster the domain-shared features and target-specific
features. CoDT consists of two parallel branches. One branch aims to learn
domain-shared features with supervised learning in the source domain, while the
other is to learn target-specific features using contrastive learning in the
target domain. To cluster the features, we propose an online clustering
algorithm that enables simultaneous promotion of robust pseudo label generation
and feature clustering. Furthermore, to leverage the complementarity of
domain-shared features and target-specific features, we propose a novel
collaborative clustering strategy to enforce pair-wise relationship consistency
between the two branches. We conduct extensive experiments on multiple
cross-domain 3D action recognition datasets, and the results demonstrate the
effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Convolutional Neural Network with Meta Feature Learning for Abnormality Detection in Wireless Capsule Endoscopy Images. (arXiv:2207.09769v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09769">
<div class="article-summary-box-inner">
<span><p>Wireless Capsule Endoscopy is one of the most advanced non-invasive methods
for the examination of gastrointestinal tracts. An intelligent computer-aided
diagnostic system for detecting gastrointestinal abnormalities like polyp,
bleeding, inflammation, etc. is highly exigent in wireless capsule endoscopy
image analysis. Abnormalities greatly differ in their shape, size, color, and
texture, and some appear to be visually similar to normal regions. This poses a
challenge in designing a binary classifier due to intra-class variations. In
this study, a hybrid convolutional neural network is proposed for abnormality
detection that extracts a rich pool of meaningful features from wireless
capsule endoscopy images using a variety of convolution operations. It consists
of three parallel convolutional neural networks, each with a distinctive
feature learning capability. The first network utilizes depthwise separable
convolution, while the second employs cosine normalized convolution operation.
A novel meta-feature extraction mechanism is introduced in the third network,
to extract patterns from the statistical information drawn over the features
generated from the first and second networks and its own previous layer. The
network trio effectively handles intra-class variance and efficiently detects
gastrointestinal abnormalities. The proposed hybrid convolutional neural
network model is trained and tested on two widely used publicly available
datasets. The test results demonstrate that the proposed model outperforms six
state-of-the-art methods with 97\% and 98\% classification accuracy on KID and
Kvasir-Capsule datasets respectively. Cross dataset evaluation results also
demonstrate the generalization performance of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localization supervision of chest x-ray classifiers using label-specific eye-tracking annotation. (arXiv:2207.09771v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09771">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have been successfully applied to chest
x-ray (CXR) images. Moreover, annotated bounding boxes have been shown to
improve the interpretability of a CNN in terms of localizing abnormalities.
However, only a few relatively small CXR datasets containing bounding boxes are
available, and collecting them is very costly. Opportunely, eye-tracking (ET)
data can be collected in a non-intrusive way during the clinical workflow of a
radiologist. We use ET data recorded from radiologists while dictating CXR
reports to train CNNs. We extract snippets from the ET data by associating them
with the dictation of keywords and use them to supervise the localization of
abnormalities. We show that this method improves a model's interpretability
without impacting its image-level classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drivable Volumetric Avatars using Texel-Aligned Features. (arXiv:2207.09774v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09774">
<div class="article-summary-box-inner">
<span><p>Photorealistic telepresence requires both high-fidelity body modeling and
faithful driving to enable dynamically synthesized appearance that is
indistinguishable from reality. In this work, we propose an end-to-end
framework that addresses two core challenges in modeling and driving full-body
avatars of real people. One challenge is driving an avatar while staying
faithful to details and dynamics that cannot be captured by a global
low-dimensional parameterization such as body pose. Our approach supports
driving of clothed avatars with wrinkles and motion that a real driving
performer exhibits beyond the training corpus. Unlike existing global state
representations or non-parametric screen-space approaches, we introduce
texel-aligned features -- a localised representation which can leverage both
the structural prior of a skeleton-based parametric model and observed sparse
image signals at the same time. Another challenge is modeling a temporally
coherent clothed avatar, which typically requires precise surface tracking. To
circumvent this, we propose a novel volumetric avatar representation by
extending mixtures of volumetric primitives to articulated objects. By
explicitly incorporating articulation, our approach naturally generalizes to
unseen poses. We also introduce a localized viewpoint conditioning, which leads
to a large improvement in generalization of view-dependent appearance. The
proposed volumetric representation does not require high-quality mesh tracking
as a prerequisite and brings significant quality improvements compared to
mesh-based counterparts. In our experiments, we carefully examine our design
choices and demonstrate the efficacy of our approach, outperforming the
state-of-the-art methods on challenging driving scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Practical Scenario of Open-set Object Detection: Open at Category Level and Closed at Super-category Level. (arXiv:2207.09775v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09775">
<div class="article-summary-box-inner">
<span><p>Open-set object detection (OSOD) has recently attracted considerable
attention. It is to detect unknown objects while correctly
detecting/classifying known objects. We first point out that the scenario of
OSOD considered in recent studies, which considers an unlimited variety of
unknown objects similar to open-set recognition (OSR), has a fundamental issue.
That is, we cannot determine what to detect and what not for such unlimited
unknown objects, which is necessary for detection tasks. This issue leads to
difficulty with the evaluation of methods' performance on unknown object
detection. We then introduce a novel scenario of OSOD, which deals with only
unknown objects that share the super-category with known objects. It has many
real-world applications, e.g., detecting an increasing number of fine-grained
objects. This new setting is free from the above issue and evaluation
difficulty. Moreover, it makes detecting unknown objects more realistic owing
to the visual similarity between known and unknown objects. We show through
experimental results that a simple method based on the uncertainty of class
prediction from standard detectors outperforms the current state-of-the-art
OSOD methods tested in the previous setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AU-Supervised Convolutional Vision Transformers for Synthetic Facial Expression Recognition. (arXiv:2207.09777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09777">
<div class="article-summary-box-inner">
<span><p>The paper describes our proposed methodology for the six basic expression
classification track of Affective Behavior Analysis in-the-wild (ABAW)
Competition 2022. In Learing from Synthetic Data(LSD) task, facial expression
recognition (FER) methods aim to learn the representation of expression from
the artificially generated data and generalise to real data. Because of the
ambiguous of the synthetic data and the objectivity of the facial Action Unit
(AU), we resort to the AU information for performance boosting, and make
contributions as follows. First, to adapt the model to synthetic scenarios, we
use the knowledge from pre-trained large-scale face recognition data. Second,
we propose a conceptually-new framework, termed as AU-Supervised Convolutional
Vision Transformers (AU-CVT), which clearly improves the performance of FER by
jointly training auxiliary datasets with AU or pseudo AU labels. Our AU-CVT
achieved F1 score as $0.6863$, accuracy as $0.7433$ on the validation set. The
source code of our work is publicly available online:
https://github.com/msy1412/ABAW4
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR Segmentation. (arXiv:2207.09778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09778">
<div class="article-summary-box-inner">
<span><p>3D LiDAR semantic segmentation is fundamental for autonomous driving. Several
Unsupervised Domain Adaptation (UDA) methods for point cloud data have been
recently proposed to improve model generalization for different sensors and
environments. Researchers working on UDA problems in the image domain have
shown that sample mixing can mitigate domain shift. We propose a new approach
of sample mixing for point cloud UDA, namely Compositional Semantic Mix
(CoSMix), the first UDA approach for point cloud segmentation based on sample
mixing. CoSMix consists of a two-branch symmetric network that can process
labelled synthetic data (source) and real-world unlabelled point clouds
(target) concurrently. Each branch operates on one domain by mixing selected
pieces of data from the other one, and by using the semantic information
derived from source labels and target pseudo-labels. We evaluate CoSMix on two
large-scale datasets, showing that it outperforms state-of-the-art methods by a
large margin. Our code is available at
https://github.com/saltoricristiano/cosmix-uda.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceFormer: Scale-aware Blind Face Restoration with Transformers. (arXiv:2207.09790v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09790">
<div class="article-summary-box-inner">
<span><p>Blind face restoration usually encounters with diverse scale face inputs,
especially in the real world. However, most of the current works support
specific scale faces, which limits its application ability in real-world
scenarios. In this work, we propose a novel scale-aware blind face restoration
framework, named FaceFormer, which formulates facial feature restoration as
scale-aware transformation. The proposed Facial Feature Up-sampling (FFUP)
module dynamically generates upsampling filters based on the original
scale-factor priors, which facilitate our network to adapt to arbitrary face
scales. Moreover, we further propose the facial feature embedding (FFE) module
which leverages transformer to hierarchically extract diversity and robustness
of facial latent. Thus, our FaceFormer achieves fidelity and robustness
restored faces, which possess realistic and symmetrical details of facial
components. Extensive experiments demonstrate that our proposed method trained
with synthetic dataset generalizes better to a natural low quality images than
current state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Industrial Anomaly Detection via Pattern Generative and Contrastive Networks. (arXiv:2207.09792v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09792">
<div class="article-summary-box-inner">
<span><p>It is hard to collect enough flaw images for training deep learning network
in industrial production. Therefore, existing industrial anomaly detection
methods prefer to use CNN-based unsupervised detection and localization network
to achieve this task. However, these methods always fail when there are
varieties happened in new signals since traditional end-to-end networks suffer
barriers of fitting nonlinear model in high-dimensional space. Moreover, they
have a memory library by clustering the feature of normal images essentially,
which cause it is not robust to texture change. To this end, we propose the
Vision Transformer based (VIT-based) unsupervised anomaly detection network. It
utilizes a hierarchical task learning and human experience to enhance its
interpretability. Our network consists of pattern generation and comparison
networks. Pattern generation network uses two VIT-based encoder modules to
extract the feature of two consecutive image patches, then uses VIT-based
decoder module to learn the human designed style of these features and predict
the third image patch. After this, we use the Siamese-based network to compute
the similarity of the generation image patch and original image patch. Finally,
we refine the anomaly localization by the bi-directional inference strategy.
Comparison experiments on public dataset MVTec dataset show our method achieves
99.8% AUC, which surpasses previous state-of-the-art methods. In addition, we
give a qualitative illustration on our own leather and cloth datasets. The
accurate segment results strongly prove the accuracy of our method in anomaly
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EASNet: Searching Elastic and Accurate Network Architecture for Stereo Matching. (arXiv:2207.09796v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09796">
<div class="article-summary-box-inner">
<span><p>Recent advanced studies have spent considerable human efforts on optimizing
network architectures for stereo matching but hardly achieved both high
accuracy and fast inference speed. To ease the workload in network design,
neural architecture search (NAS) has been applied with great success to various
sparse prediction tasks, such as image classification and object detection.
However, existing NAS studies on the dense prediction task, especially stereo
matching, still cannot be efficiently and effectively deployed on devices of
different computing capabilities. To this end, we propose to train an elastic
and accurate network for stereo matching (EASNet) that supports various 3D
architectural settings on devices with different computing capabilities. Given
the deployment latency constraint on the target device, we can quickly extract
a sub-network from the full EASNet without additional training while the
accuracy of the sub-network can still be maintained. Extensive experiments show
that our EASNet outperforms both state-of-the-art human-designed and NAS-based
architectures on Scene Flow and MPI Sintel datasets in terms of model accuracy
and inference speed. Particularly, deployed on an inference GPU, EASNet
achieves a new SOTA 0.73 EPE on the Scene Flow dataset with 100 ms, which is
4.5$\times$ faster than LEAStereo with a better quality model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Transformer for Automatic 3D Annotation and Object Detection. (arXiv:2207.09805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09805">
<div class="article-summary-box-inner">
<span><p>Despite a growing number of datasets being collected for training 3D object
detection models, significant human effort is still required to annotate 3D
boxes on LiDAR scans. To automate the annotation and facilitate the production
of various customized datasets, we propose an end-to-end multimodal transformer
(MTrans) autolabeler, which leverages both LiDAR scans and images to generate
precise 3D box annotations from weak 2D bounding boxes. To alleviate the
pervasive sparsity problem that hinders existing autolabelers, MTrans densifies
the sparse point clouds by generating new 3D points based on 2D image
information. With a multi-task design, MTrans segments the
foreground/background, densifies LiDAR point clouds, and regresses 3D boxes
simultaneously. Experimental results verify the effectiveness of the MTrans for
improving the quality of the generated labels. By enriching the sparse point
clouds, our method achieves 4.48\% and 4.03\% better 3D AP on KITTI moderate
and hard samples, respectively, versus the state-of-the-art autolabeler. MTrans
can also be extended to improve the accuracy for 3D object detection, resulting
in a remarkable 89.45\% AP on KITTI hard samples. Codes are at
\url{https://github.com/Cliu2/MTrans}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing. (arXiv:2207.09812v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09812">
<div class="article-summary-box-inner">
<span><p>Machine learning is transforming the video editing industry. Recent advances
in computer vision have leveled-up video editing tasks such as intelligent
reframing, rotoscoping, color grading, or applying digital makeups. However,
most of the solutions have focused on video manipulation and VFX. This work
introduces the Anatomy of Video Editing, a dataset, and benchmark, to foster
research in AI-assisted video editing. Our benchmark suite focuses on video
editing tasks, beyond visual effects, such as automatic footage organization
and assisted video assembling. To enable research on these fronts, we annotate
more than 1.5M tags, with relevant concepts to cinematography, from 196176
shots sampled from movie scenes. We establish competitive baseline methods and
detailed analyses for each of the tasks. We hope our work sparks innovative
research towards underexplored areas of AI-assisted video editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis. (arXiv:2207.09814v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09814">
<div class="article-summary-box-inner">
<span><p>In this paper, we present NUWA-Infinity, a generative model for infinite
visual synthesis, which is defined as the task of generating arbitrarily-sized
high-resolution images or long-duration videos. An autoregressive over
autoregressive generation mechanism is proposed to deal with this variable-size
generation task, where a global patch-level autoregressive model considers the
dependencies between patches, and a local token-level autoregressive model
considers dependencies between visual tokens within each patch. A Nearby
Context Pool (NCP) is introduced to cache-related patches already generated as
the context for the current patch being generated, which can significantly save
computation costs without sacrificing patch-level dependency modeling. An
Arbitrary Direction Controller (ADC) is used to decide suitable generation
orders for different visual synthesis tasks and learn order-aware positional
embeddings. Compared to DALL-E, Imagen and Parti, NUWA-Infinity can generate
high-resolution images with arbitrary sizes and support long-duration video
generation additionally. Compared to NUWA, which also covers images and videos,
NUWA-Infinity has superior visual synthesis capabilities in terms of resolution
and variable-size generation. The GitHub link is
https://github.com/microsoft/NUWA. The homepage link is
https://nuwa-infinity.microsoft.com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIF: United Neural Implicit Functions for Clothed Human Reconstruction and Animation. (arXiv:2207.09835v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09835">
<div class="article-summary-box-inner">
<span><p>We propose united implicit functions (UNIF), a part-based method for clothed
human reconstruction and animation with raw scans and skeletons as the input.
Previous part-based methods for human reconstruction rely on ground-truth part
labels from SMPL and thus are limited to minimal-clothed humans. In contrast,
our method learns to separate parts from body motions instead of part
supervision, thus can be extended to clothed humans and other articulated
objects. Our Partition-from-Motion is achieved by a bone-centered
initialization, a bone limit loss, and a section normal loss that ensure stable
part division even when the training poses are limited. We also present a
minimal perimeter loss for SDF to suppress extra surfaces and part overlapping.
Another core of our method is an adjacent part seaming algorithm that produces
non-rigid deformations to maintain the connection between parts which
significantly relieves the part-based artifacts. Under this algorithm, we
further propose "Competing Parts", a method that defines blending weights by
the relative position of a point to bones instead of the absolute position,
avoiding the generalization problem of neural implicit functions with inverse
LBS (linear blend skinning). We demonstrate the effectiveness of our method by
clothed human body reconstruction and animation on the CAPE and the ClothSeq
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EleGANt: Exquisite and Locally Editable GAN for Makeup Transfer. (arXiv:2207.09840v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09840">
<div class="article-summary-box-inner">
<span><p>Most existing methods view makeup transfer as transferring color
distributions of different facial regions and ignore details such as eye
shadows and blushes. Besides, they only achieve controllable transfer within
predefined fixed regions. This paper emphasizes the transfer of makeup details
and steps towards more flexible controls. To this end, we propose Exquisite and
locally editable GAN for makeup transfer (EleGANt). It encodes facial
attributes into pyramidal feature maps to preserves high-frequency information.
It uses attention to extract makeup features from the reference and adapt them
to the source face, and we introduce a novel Sow-Attention Module that applies
attention within shifted overlapped windows to reduce the computational cost.
Moreover, EleGANt is the first to achieve customized local editing within
arbitrary areas by corresponding editing on the feature maps. Extensive
experiments demonstrate that EleGANt generates realistic makeup faces with
exquisite details and achieves state-of-the-art performance. The code is
available at https://github.com/Chenyu-Yang-2000/EleGANt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Word Learning in Children from the Performance of Computer Vision Systems. (arXiv:2207.09847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09847">
<div class="article-summary-box-inner">
<span><p>For human children as well as machine learning systems, a key challenge in
learning a word is linking the word to the visual phenomena it describes. We
explore this aspect of word learning by using the performance of computer
vision systems as a proxy for the difficulty of learning a word from visual
cues. We show that the age at which children acquire different categories of
words is predicted by the performance of visual classification and captioning
systems, over and above the expected effects of word frequency. The performance
of the computer vision systems is related to human judgments of the
concreteness of words, supporting the idea that we are capturing the
relationship between words and visual phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Embedded Monocular Vision Approach for Ground-Aware Objects Detection and Position Estimation. (arXiv:2207.09851v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09851">
<div class="article-summary-box-inner">
<span><p>In the RoboCup Small Size League (SSL), teams are encouraged to propose
solutions for executing basic soccer tasks inside the SSL field using only
embedded sensing information. Thus, this work proposes an embedded monocular
vision approach for detecting objects and estimating relative positions inside
the soccer field. Prior knowledge from the environment is exploited by assuming
objects lay on the ground, and the onboard camera has its position fixed on the
robot. We implemented the proposed method on an NVIDIA Jetson Nano and employed
SSD MobileNet v2 for 2D Object Detection with TensorRT optimization, detecting
balls, robots, and goals with distances up to 3.5 meters. Ball localization
evaluation shows that the proposed solution overcomes the currently used SSL
vision system for positions closer than 1 meter to the onboard camera with a
Root Mean Square Error of 14.37 millimeters. In addition, the proposed method
achieves real-time performance with an average processing speed of 30 frames
per second.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Everything is There in Latent Space: Attribute Editing and Attribute Style Manipulation by StyleGAN Latent Space Exploration. (arXiv:2207.09855v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09855">
<div class="article-summary-box-inner">
<span><p>Unconstrained Image generation with high realism is now possible using recent
Generative Adversarial Networks (GANs). However, it is quite challenging to
generate images with a given set of attributes. Recent methods use style-based
GAN models to perform image editing by leveraging the semantic hierarchy
present in the layers of the generator. We present Few-shot Latent-based
Attribute Manipulation and Editing (FLAME), a simple yet effective framework to
perform highly controlled image editing by latent space manipulation.
Specifically, we estimate linear directions in the latent space (of a
pre-trained StyleGAN) that controls semantic attributes in the generated image.
In contrast to previous methods that either rely on large-scale attribute
labeled datasets or attribute classifiers, FLAME uses minimal supervision of a
few curated image pairs to estimate disentangled edit directions. FLAME can
perform both individual and sequential edits with high precision on a diverse
set of images while preserving identity. Further, we propose a novel task of
Attribute Style Manipulation to generate diverse styles for attributes such as
eyeglass and hair. We first encode a set of synthetic images of the same
identity but having different attribute styles in the latent space to estimate
an attribute style manifold. Sampling a new latent from this manifold will
result in a new attribute style in the generated image. We propose a novel
sampling method to sample latent from the manifold, enabling us to generate a
diverse set of attribute styles beyond the styles present in the training set.
FLAME can generate diverse attribute styles in a disentangled manner. We
illustrate the superior performance of FLAME against previous image editing
methods by extensive qualitative and quantitative comparisons. FLAME also
generalizes well on multiple datasets such as cars and churches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Stability of Deep Image Quality Assessment With Respect to Image Scaling. (arXiv:2207.09856v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09856">
<div class="article-summary-box-inner">
<span><p>Image quality assessment (IQA) is a fundamental metric for image processing
tasks (e.g., compression). With full-reference IQAs, traditional IQAs, such as
PSNR and SSIM, have been used. Recently, IQAs based on deep neural networks
(deep IQAs), such as LPIPS and DISTS, have also been used. It is known that
image scaling is inconsistent among deep IQAs, as some perform down-scaling as
pre-processing, whereas others instead use the original image size. In this
paper, we show that the image scale is an influential factor that affects deep
IQA performance. We comprehensively evaluate four deep IQAs on the same five
datasets, and the experimental results show that image scale significantly
influences IQA performance. We found that the most appropriate image scale is
often neither the default nor the original size, and the choice differs
depending on the methods and datasets used. We visualized the stability and
found that PieAPP is the most stable among the four deep IQAs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discrete-Constrained Regression for Local Counting Models. (arXiv:2207.09865v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09865">
<div class="article-summary-box-inner">
<span><p>Local counts, or the number of objects in a local area, is a continuous value
by nature. Yet recent state-of-the-art methods show that formulating counting
as a classification task performs better than regression. Through a series of
experiments on carefully controlled synthetic data, we show that this
counter-intuitive result is caused by imprecise ground truth local counts.
Factors such as biased dot annotations and incorrectly matched Gaussian kernels
used to generate ground truth counts introduce deviations from the true local
counts. Standard continuous regression is highly sensitive to these errors,
explaining the performance gap between classification and regression. To
mitigate the sensitivity, we loosen the regression formulation from a
continuous scale to a discrete ordering and propose a novel
discrete-constrained (DC) regression. Applied to crowd counting, DC-regression
is more accurate than both classification and standard regression on three
public benchmarks. A similar advantage also holds for the age estimation task,
verifying the overall effectiveness of DC-regression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Mixture of Experts Learning for Generalizable Face Anti-Spoofing. (arXiv:2207.09868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09868">
<div class="article-summary-box-inner">
<span><p>With various face presentation attacks emerging continually, face
anti-spoofing (FAS) approaches based on domain generalization (DG) have drawn
growing attention. Existing DG-based FAS approaches always capture the
domain-invariant features for generalizing on the various unseen domains.
However, they neglect individual source domains' discriminative characteristics
and diverse domain-specific information of the unseen domains, and the trained
model is not sufficient to be adapted to various unseen domains. To address
this issue, we propose an Adaptive Mixture of Experts Learning (AMEL)
framework, which exploits the domain-specific information to adaptively
establish the link among the seen source domains and unseen target domains to
further improve the generalization. Concretely, Domain-Specific Experts (DSE)
are designed to investigate discriminative and unique domain-specific features
as a complement to common domain-invariant features. Moreover, Dynamic Expert
Aggregation (DEA) is proposed to adaptively aggregate the complementary
information of each source expert based on the domain relevance to the unseen
target domain. And combined with meta-learning, these modules work
collaboratively to adaptively aggregate meaningful domain-specific information
for the various unseen target domains. Extensive experiments and visualizations
demonstrate the effectiveness of our method against the state-of-the-art
competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Neural Network Training Method for Autonomous Driving Using Semi-Pseudo-Labels and 3D Data Augmentations. (arXiv:2207.09869v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09869">
<div class="article-summary-box-inner">
<span><p>Training neural networks to perform 3D object detection for autonomous
driving requires a large amount of diverse annotated data. However, obtaining
training data with sufficient quality and quantity is expensive and sometimes
impossible due to human and sensor constraints. Therefore, a novel solution is
needed for extending current training methods to overcome this limitation and
enable accurate 3D object detection. Our solution for the above-mentioned
problem combines semi-pseudo-labeling and novel 3D augmentations. For
demonstrating the applicability of the proposed method, we have designed a
convolutional neural network for 3D object detection which can significantly
increase the detection range in comparison with the training data distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negative Samples are at Large: Leveraging Hard-distance Elastic Loss for Re-identification. (arXiv:2207.09884v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09884">
<div class="article-summary-box-inner">
<span><p>We present a Momentum Re-identification (MoReID) framework that can leverage
a very large number of negative samples in training for general
re-identification task. The design of this framework is inspired by Momentum
Contrast (MoCo), which uses a dictionary to store current and past batches to
build a large set of encoded samples. As we find it less effective to use past
positive samples which may be highly inconsistent to the encoded feature
property formed with the current positive samples, MoReID is designed to use
only a large number of negative samples stored in the dictionary. However, if
we train the model using the widely used Triplet loss that uses only one sample
to represent a set of positive/negative samples, it is hard to effectively
leverage the enlarged set of negative samples acquired by the MoReID framework.
To maximize the advantage of using the scaled-up negative sample set, we newly
introduce Hard-distance Elastic loss (HE loss), which is capable of using more
than one hard sample to represent a large number of samples. Our experiments
demonstrate that a large number of negative samples provided by MoReID
framework can be utilized at full capacity only with the HE loss, achieving the
state-of-the-art accuracy on three re-ID benchmarks, VeRi-776, Market-1501, and
VeRi-Wild.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Labeling instructions matter in biomedical image analysis. (arXiv:2207.09899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09899">
<div class="article-summary-box-inner">
<span><p>Biomedical image analysis algorithm validation depends on high-quality
annotation of reference datasets, for which labeling instructions are key.
Despite their importance, their optimization remains largely unexplored. Here,
we present the first systematic study of labeling instructions and their impact
on annotation quality in the field. Through comprehensive examination of
professional practice and international competitions registered at the MICCAI
Society, we uncovered a discrepancy between annotators' needs for labeling
instructions and their current quality and availability. Based on an analysis
of 14,040 images annotated by 156 annotators from four professional companies
and 708 Amazon Mechanical Turk (MTurk) crowdworkers using instructions with
different information density levels, we further found that including exemplary
images significantly boosts annotation performance compared to text-only
descriptions, while solely extending text descriptions does not. Finally,
professional annotators constantly outperform MTurk crowdworkers. Our study
raises awareness for the need of quality standards in biomedical image analysis
labeling instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A note on the variation of geometric functionals. (arXiv:2207.09915v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09915">
<div class="article-summary-box-inner">
<span><p>Calculus of Variation combined with Differential Geometry as tools of
modelling and solving problems in image processing and computer vision were
introduced in the late 80's and the 90s of the 20th century. The beginning of
an extensive work in these directions was marked by works such as Geodesic
Active Contours (GAC), the Beltrami framework, level set method of Osher and
Sethian the works of Charpiat et al. and the works by Chan and Vese to name
just a few. In many cases the optimization of these functional are done by the
gradient descent method via the calculation of the Euler-Lagrange equations.
Straightforward use of the resulted EL equations in the gradient descent scheme
leads to non-geometric and in some cases non sensical equations. It is
costumary to modify these EL equations or even the functional itself in order
to obtain geometric and/or sensical equations. The aim of this note is to point
to the correct way to derive the EL and the gradient descent equations such
that the resulted gradient descent equation is geometric and makes sense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Framework for Few-shot Skeleton-based Temporal Action Segmentation. (arXiv:2207.09925v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09925">
<div class="article-summary-box-inner">
<span><p>Temporal action segmentation (TAS) aims to classify and locate actions in the
long untrimmed action sequence. With the success of deep learning, many deep
models for action segmentation have emerged. However, few-shot TAS is still a
challenging problem. This study proposes an efficient framework for the
few-shot skeleton-based TAS, including a data augmentation method and an
improved model. The data augmentation approach based on motion interpolation is
presented here to solve the problem of insufficient data, and can increase the
number of samples significantly by synthesizing action sequences. Besides, we
concatenate a Connectionist Temporal Classification (CTC) layer with a network
designed for skeleton-based TAS to obtain an optimized model. Leveraging CTC
can enhance the temporal alignment between prediction and ground truth and
further improve the segment-wise metrics of segmentation results. Extensive
experiments on both public and self-constructed datasets, including two
small-scale datasets and one large-scale dataset, show the effectiveness of two
proposed methods in improving the performance of the few-shot skeleton-based
TAS task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViGAT: Bottom-up event recognition and explanation in video using factorized graph attention network. (arXiv:2207.09927v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09927">
<div class="article-summary-box-inner">
<span><p>In this paper a pure-attention bottom-up approach, called ViGAT, that
utilizes an object detector together with a Vision Transformer (ViT) backbone
network to derive object and frame features, and a head network to process
these features for the task of event recognition and explanation in video, is
proposed. The ViGAT head consists of graph attention network (GAT) blocks
factorized along the spatial and temporal dimensions in order to capture
effectively both local and long-term dependencies between objects or frames.
Moreover, using the weighted in-degrees (WiDs) derived from the adjacency
matrices at the various GAT blocks, we show that the proposed architecture can
identify the most salient objects and frames that explain the decision of the
network. A comprehensive evaluation study is performed, demonstrating that the
proposed approach provides state-of-the-art results on three large, publicly
available video datasets (FCVID, Mini-Kinetics, ActivityNet).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Landmark-based Stent Tracking in X-ray Fluoroscopy. (arXiv:2207.09933v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09933">
<div class="article-summary-box-inner">
<span><p>In clinical procedures of angioplasty (i.e., open clogged coronary arteries),
devices such as balloons and stents need to be placed and expanded in arteries
under the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,
the resulting images are often noisy. To check the correct placement of these
devices, typically multiple motion-compensated frames are averaged to enhance
the view. Therefore, device tracking is a necessary procedure for this purpose.
Even though angioplasty devices are designed to have radiopaque markers for the
ease of tracking, current methods struggle to deliver satisfactory results due
to the small marker size and complex scenes in angioplasty. In this paper, we
propose an end-to-end deep learning framework for single stent tracking, which
consists of three hierarchical modules: U-Net based landmark detection, ResNet
based stent proposal and feature extraction, and graph convolutional neural
network (GCN) based stent tracking that temporally aggregates both spatial
information and appearance features. The experiments show that our method
performs significantly better in detection compared with the state-of-the-art
point-based tracking models. In addition, its fast inference speed satisfies
clinical requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepIPC: Deeply Integrated Perception and Control for Mobile Robot in Real Environments. (arXiv:2207.09934v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09934">
<div class="article-summary-box-inner">
<span><p>We propose DeepIPC, an end-to-end multi-task model that handles both
perception and control tasks in driving a mobile robot autonomously. The model
consists of two main parts, perception and controller modules. The perception
module takes RGB image and depth map to perform semantic segmentation and
bird's eye view (BEV) semantic mapping along with providing their encoded
features. Meanwhile, the controller module processes these features with the
measurement of GNSS locations and angular speed to estimate waypoints that come
with latent features. Then, two different agents are used to translate
waypoints and latent features into a set of navigational controls to drive the
robot. The model is evaluated by predicting driving records and performing
automated driving under various conditions in the real environment. Based on
the experimental results, DeepIPC achieves the best drivability and multi-task
performance even with fewer parameters compared to the other models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Efficient and Scale-Robust Ultra-High-Definition Image Demoireing. (arXiv:2207.09935v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09935">
<div class="article-summary-box-inner">
<span><p>With the rapid development of mobile devices, modern widely-used mobile
phones typically allow users to capture 4K resolution (i.e.,
ultra-high-definition) images. However, for image demoireing, a challenging
task in low-level vision, existing works are generally carried out on
low-resolution or synthetic images. Hence, the effectiveness of these methods
on 4K resolution images is still unknown. In this paper, we explore moire
pattern removal for ultra-high-definition images. To this end, we propose the
first ultra-high-definition demoireing dataset (UHDM), which contains 5,000
real-world 4K resolution image pairs, and conduct a benchmark study on current
state-of-the-art methods. Further, we present an efficient baseline model
ESDNet for tackling 4K moire images, wherein we build a semantic-aligned
scale-aware module to address the scale variation of moire patterns. Extensive
experiments manifest the effectiveness of our approach, which outperforms
state-of-the-art methods by a large margin while being much more lightweight.
Code and dataset are available at https://xinyu-andy.github.io/uhdm-page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probable Domain Generalization via Quantile Risk Minimization. (arXiv:2207.09944v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09944">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) seeks predictors which perform well on unseen test
distributions by leveraging labeled training data from multiple related
distributions or domains. To achieve this, the standard formulation optimizes
for worst-case performance over the set of all possible domains. However, with
worst-case shifts very unlikely in practice, this generally leads to
overly-conservative solutions. In fact, a recent study found that no DG
algorithm outperformed empirical risk minimization in terms of average
performance. In this work, we argue that DG is neither a worst-case problem nor
an average-case problem, but rather a probabilistic one. To this end, we
propose a probabilistic framework for DG, which we call Probable Domain
Generalization, wherein our key idea is that distribution shifts seen during
training should inform us of probable shifts at test time. To realize this, we
explicitly relate training and test domains as draws from the same underlying
meta-distribution, and propose a new optimization problem -- Quantile Risk
Minimization (QRM) -- which requires that predictors generalize with high
probability. We then prove that QRM: (i) produces predictors that generalize to
new domains with a desired probability, given sufficiently many domains and
samples; and (ii) recovers the causal predictor as the desired probability of
generalization approaches one. In our experiments, we introduce a more holistic
quantile-focused evaluation protocol for DG, and show that our algorithms
outperform state-of-the-art baselines on real and synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VirtualPose: Learning Generalizable 3D Human Pose Models from Virtual Data. (arXiv:2207.09949v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09949">
<div class="article-summary-box-inner">
<span><p>While monocular 3D pose estimation seems to have achieved very accurate
results on the public datasets, their generalization ability is largely
overlooked. In this work, we perform a systematic evaluation of the existing
methods and find that they get notably larger errors when tested on different
cameras, human poses and appearance. To address the problem, we introduce
VirtualPose, a two-stage learning framework to exploit the hidden "free lunch"
specific to this task, i.e. generating infinite number of poses and cameras for
training models at no cost. To that end, the first stage transforms images to
abstract geometry representations (AGR), and then the second maps them to 3D
poses. It addresses the generalization issue from two aspects: (1) the first
stage can be trained on diverse 2D datasets to reduce the risk of over-fitting
to limited appearance; (2) the second stage can be trained on diverse AGR
synthesized from a large number of virtual cameras and poses. It outperforms
the SOTA methods without using any paired images and 3D poses from the
benchmarks, which paves the way for practical applications. Code is available
at https://github.com/wkom/VirtualPose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Pedestrian Group Representations for Multi-modal Trajectory Prediction. (arXiv:2207.09953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09953">
<div class="article-summary-box-inner">
<span><p>Modeling the dynamics of people walking is a problem of long-standing
interest in computer vision. Many previous works involving pedestrian
trajectory prediction define a particular set of individual actions to
implicitly model group actions. In this paper, we present a novel architecture
named GP-Graph which has collective group representations for effective
pedestrian trajectory prediction in crowded environments, and is compatible
with all types of existing approaches. A key idea of GP-Graph is to model both
individual-wise and group-wise relations as graph representations. To do this,
GP-Graph first learns to assign each pedestrian into the most likely behavior
group. Using this assignment information, GP-Graph then forms both intra- and
inter-group interactions as graphs, accounting for human-human relations within
a group and group-group relations, respectively. To be specific, for the
intra-group interaction, we mask pedestrian graph edges out of an associated
group. We also propose group pooling&amp;unpooling operations to represent a group
with multiple pedestrians as one graph node. Lastly, GP-Graph infers a
probability map for socially-acceptable future trajectories from the integrated
features of both group interactions. Moreover, we introduce a group-level
latent vector sampling to ensure collective inferences over a set of possible
future trajectories. Extensive experiments are conducted to validate the
effectiveness of our architecture, which demonstrates consistent performance
improvements with publicly available benchmarks. Code is publicly available at
https://github.com/inhwanbae/GPGraph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Telepresence Video Quality Assessment. (arXiv:2207.09956v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09956">
<div class="article-summary-box-inner">
<span><p>Video conferencing, which includes both video and audio content, has
contributed to dramatic increases in Internet traffic, as the COVID-19 pandemic
forced millions of people to work and learn from home. Global Internet traffic
of video conferencing has dramatically increased Because of this, efficient and
accurate video quality tools are needed to monitor and perceptually optimize
telepresence traffic streamed via Zoom, Webex, Meet, etc. However, existing
models are limited in their prediction capabilities on multi-modal, live
streaming telepresence content. Here we address the significant challenges of
Telepresence Video Quality Assessment (TVQA) in several ways. First, we
mitigated the dearth of subjectively labeled data by collecting ~2k
telepresence videos from different countries, on which we crowdsourced ~80k
subjective quality labels. Using this new resource, we created a
first-of-a-kind online video quality prediction framework for live streaming,
using a multi-modal learning framework with separate pathways to compute visual
and audio quality predictions. Our all-in-one model is able to provide accurate
quality predictions at the patch, frame, clip, and audiovisual levels. Our
model achieves state-of-the-art performance on both existing quality databases
and our new TVQA database, at a considerably lower computational expense,
making it an attractive solution for mobile and embedded systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores. (arXiv:2207.09957v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09957">
<div class="article-summary-box-inner">
<span><p>Machine learning models are typically deployed in a test setting that differs
from the training setting, potentially leading to decreased model performance
because of domain shift. If we could estimate the performance that a
pre-trained model would achieve on data from a specific deployment setting, for
example a certain clinic, we could judge whether the model could safely be
deployed or if its performance degrades unacceptably on the specific data.
Existing approaches estimate this based on the confidence of predictions made
on unlabeled test data from the deployment's domain. We find existing methods
struggle with data that present class imbalance, because the methods used to
calibrate confidence do not account for bias induced by class imbalance,
consequently failing to estimate class-wise accuracy. Here, we introduce
class-wise calibration within the framework of performance estimation for
imbalanced datasets. Specifically, we derive class-specific modifications of
state-of-the-art confidence-based model evaluation methods including
temperature scaling (TS), difference of confidences (DoC), and average
thresholded confidence (ATC). We also extend the methods to estimate Dice
similarity coefficient (DSC) in image segmentation. We conduct experiments on
four tasks and find the proposed modifications consistently improve the
estimation accuracy for imbalanced datasets. Our methods improve accuracy
estimation by 18\% in classification under natural domain shifts, and double
the estimation accuracy on segmentation tasks, when compared with prior
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Few-Shot Class-Incremental Learning with Open-Set Hypothesis in Hyperbolic Geometry. (arXiv:2207.09963v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09963">
<div class="article-summary-box-inner">
<span><p>Few-Shot Class-Incremental Learning (FSCIL) aims at incrementally learning
novel classes from a few labeled samples by avoiding the overfitting and
catastrophic forgetting simultaneously. The current protocol of FSCIL is built
by mimicking the general class-incremental learning setting, while it is not
totally appropriate due to the different data configuration, i.e., novel
classes are all in the limited data regime. In this paper, we rethink the
configuration of FSCIL with the open-set hypothesis by reserving the
possibility in the first session for incoming categories. To assign better
performances on both close-set and open-set recognition to the model,
Hyperbolic Reciprocal Point Learning module (Hyper-RPL) is built on Reciprocal
Point Learning (RPL) with hyperbolic neural networks. Besides, for learning
novel categories from limited labeled data, we incorporate a hyperbolic metric
learning (Hyper-Metric) module into the distillation-based framework to
alleviate the overfitting issue and better handle the trade-off issue between
the preservation of old knowledge and the acquisition of new knowledge. The
comprehensive assessments of the proposed configuration and modules on three
benchmark datasets are executed to validate the effectiveness concerning three
evaluation indicators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M2-Net: Multi-stages Specular Highlight Detection and Removal in Multi-scenes. (arXiv:2207.09965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09965">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel uniformity framework for highlight
detection and removal in multi-scenes, including synthetic images, face images,
natural images, and text images. The framework consists of three main
components, highlight feature extractor module, highlight coarse removal
module, and highlight refine removal module. Firstly, the highlight feature
extractor module can directly separate the highlight feature and non-highlight
feature from the original highlight image. Then highlight removal image is
obtained using a coarse highlight removal network. To further improve the
highlight removal effect, the refined highlight removal image is finally
obtained using refine highlight removal module based on contextual highlight
attention mechanisms. Extensive experimental results in multiple scenes
indicate that the proposed framework can obtain excellent visual effects of
highlight removal and achieve state-of-the-art results in several quantitative
evaluation metrics. Our algorithm is applied for the first time in video
highlight removal with promising results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal and cross-modal attention for audio-visual zero-shot learning. (arXiv:2207.09966v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09966">
<div class="article-summary-box-inner">
<span><p>Audio-visual generalised zero-shot learning for video classification requires
understanding the relations between the audio and visual information in order
to be able to recognise samples from novel, previously unseen classes at test
time. The natural semantic and temporal alignment between audio and visual data
in video data can be exploited to learn powerful representations that
generalise to unseen classes at test time. We propose a multi-modal and
Temporal Cross-attention Framework (\modelName) for audio-visual generalised
zero-shot learning. Its inputs are temporally aligned audio and visual features
that are obtained from pre-trained networks. Encouraging the framework to focus
on cross-modal correspondence across time instead of self-attention within the
modalities boosts the performance significantly. We show that our proposed
framework that ingests temporal features yields state-of-the-art performance on
the \ucf, \vgg, and \activity benchmarks for (generalised) zero-shot learning.
Code for reproducing all results is available at
\url{https://github.com/ExplainableML/TCAF-GZSL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralBF: Neural Bilateral Filtering for Top-down Instance Segmentation on Point Clouds. (arXiv:2207.09978v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09978">
<div class="article-summary-box-inner">
<span><p>We introduce a method for instance proposal generation for 3D point clouds.
Existing techniques typically directly regress proposals in a single
feed-forward step, leading to inaccurate estimation. We show that this serves
as a critical bottleneck, and propose a method based on iterative bilateral
filtering with learned kernels. Following the spirit of bilateral filtering, we
consider both the deep feature embeddings of each point, as well as their
locations in the 3D space. We show via synthetic experiments that our method
brings drastic improvements when generating instance proposals for a given
point of interest. We further validate our method on the challenging ScanNet
benchmark, achieving the best instance segmentation performance amongst the
sub-category of top-down methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DecoupleNet: Decoupled Network for Domain Adaptive Semantic Segmentation. (arXiv:2207.09988v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09988">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation in semantic segmentation has been raised to
alleviate the reliance on expensive pixel-wise annotations. It leverages a
labeled source domain dataset as well as unlabeled target domain images to
learn a segmentation network. In this paper, we observe two main issues of the
existing domain-invariant learning framework. (1) Being distracted by the
feature distribution alignment, the network cannot focus on the segmentation
task. (2) Fitting source domain data well would compromise the target domain
performance. To address these issues, we propose DecoupleNet that alleviates
source domain overfitting and enables the final model to focus more on the
segmentation task. Furthermore, we put forward Self-Discrimination (SD) and
introduce an auxiliary classifier to learn more discriminative target domain
features with pseudo labels. Finally, we propose Online Enhanced Self-Training
(OEST) to contextually enhance the quality of pseudo labels in an online
manner. Experiments show our method outperforms existing state-of-the-art
methods, and extensive ablation studies verify the effectiveness of each
component. Code is available at https://github.com/dvlab-research/DecoupleNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overcoming Shortcut Learning in a Target Domain by Generalizing Basic Visual Factors from a Source Domain. (arXiv:2207.10002v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10002">
<div class="article-summary-box-inner">
<span><p>Shortcut learning occurs when a deep neural network overly relies on spurious
correlations in the training dataset in order to solve downstream tasks. Prior
works have shown how this impairs the compositional generalization capability
of deep learning models. To address this problem, we propose a novel approach
to mitigate shortcut learning in uncontrolled target domains. Our approach
extends the training set with an additional dataset (the source domain), which
is specifically designed to facilitate learning independent representations of
basic visual factors. We benchmark our idea on synthetic target domains where
we explicitly control shortcut opportunities as well as real-world target
domains. Furthermore, we analyze the effect of different specifications of the
source domain and the network architecture on compositional generalization. Our
main finding is that leveraging data from a source domain is an effective way
to mitigate shortcut learning. By promoting independence across different
factors of variation in the learned representations, networks can learn to
consider only predictive factors and ignore potential shortcut factors during
inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BYEL : Bootstrap on Your Emotion Latent. (arXiv:2207.10003v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10003">
<div class="article-summary-box-inner">
<span><p>According to the problem of dataset construction cost for training in deep
learning and the development of generative models, more and more researches are
being conducted to train with synthetic data and to inference using real data.
We propose emotion aware Self-Supervised Learning using ABAW's Learning
Synthetic Data (LSD) dataset. We pre-train our method to LSD dataset as a
self-supervised learning and then use the same LSD dataset to do downstream
training on the emotion classification task as a supervised learning. As a
result, a higher result(0.63) than baseline(0.5) was obtained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E-Graph: Minimal Solution for Rigid Rotation with Extensibility Graphs. (arXiv:2207.10008v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10008">
<div class="article-summary-box-inner">
<span><p>Minimal solutions for relative rotation and translation estimation tasks have
been explored in different scenarios, typically relying on the so-called
co-visibility graph. However, how to build direct rotation relationships
between two frames without overlap is still an open topic, which, if solved,
could greatly improve the accuracy of visual odometry.
</p>
<p>In this paper, a new minimal solution is proposed to solve relative rotation
estimation between two images without overlapping areas by exploiting a new
graph structure, which we call Extensibility Graph (E-Graph). Differently from
a co-visibility graph, high-level landmarks, including vanishing directions and
plane normals, are stored in our E-Graph, which are geometrically extensible.
Based on E-Graph, the rotation estimation problem becomes simpler and more
elegant, as it can deal with pure rotational motion and requires fewer
assumptions, e.g. Manhattan/Atlanta World, planar/vertical motion. Finally, we
embed our rotation estimation strategy into a complete camera tracking and
mapping system which obtains 6-DoF camera poses and a dense 3D mesh model.
</p>
<p>Extensive experiments on public benchmarks demonstrate that the proposed
method achieves state-of-the-art tracking performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Domain Adaptation for Face Anti-Spoofing. (arXiv:2207.10015v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10015">
<div class="article-summary-box-inner">
<span><p>Face anti-spoofing (FAS) approaches based on unsupervised domain adaption
(UDA) have drawn growing attention due to promising performances for target
scenarios. Most existing UDA FAS methods typically fit the trained models to
the target domain via aligning the distribution of semantic high-level
features. However, insufficient supervision of unlabeled target domains and
neglect of low-level feature alignment degrade the performances of existing
methods. To address these issues, we propose a novel perspective of UDA FAS
that directly fits the target data to the models, i.e., stylizes the target
data to the source-domain style via image translation, and further feeds the
stylized data into the well-trained source model for classification. The
proposed Generative Domain Adaptation (GDA) framework combines two carefully
designed consistency constraints: 1) Inter-domain neural statistic consistency
guides the generator in narrowing the inter-domain gap. 2) Dual-level semantic
consistency ensures the semantic quality of stylized images. Besides, we
propose intra-domain spectrum mixup to further expand target data distributions
to ensure generalization and reduce the intra-domain gap. Extensive experiments
and visualizations demonstrate the effectiveness of our method against the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Secrets of Event-Based Optical Flow. (arXiv:2207.10022v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10022">
<div class="article-summary-box-inner">
<span><p>Event cameras respond to scene dynamics and offer advantages to estimate
motion. Following recent image-based deep-learning achievements, optical flow
estimation methods for event cameras have rushed to combine those image-based
methods with event data. However, it requires several adaptations (data
conversion, loss function, etc.) as they have very different properties. We
develop a principled method to extend the Contrast Maximization framework to
estimate optical flow from events alone. We investigate key elements: how to
design the objective function to prevent overfitting, how to warp events to
deal better with occlusions, and how to improve convergence with multi-scale
raw events. With these key elements, our method ranks first among unsupervised
methods on the MVSEC benchmark, and is competitive on the DSEC benchmark.
Moreover, our method allows us to expose the issues of the ground truth flow in
those benchmarks, and produces remarkable results when it is transferred to
unsupervised learning settings. Our code is available at
https://github.com/tub-rip/event_based_optical_flow
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tailoring Self-Supervision for Supervised Learning. (arXiv:2207.10023v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10023">
<div class="article-summary-box-inner">
<span><p>Recently, it is shown that deploying a proper self-supervision is a
prospective way to enhance the performance of supervised learning. Yet, the
benefits of self-supervision are not fully exploited as previous pretext tasks
are specialized for unsupervised representation learning. To this end, we begin
by presenting three desirable properties for such auxiliary tasks to assist the
supervised objective. First, the tasks need to guide the model to learn rich
features. Second, the transformations involved in the self-supervision should
not significantly alter the training distribution. Third, the tasks are
preferred to be light and generic for high applicability to prior arts.
Subsequently, to show how existing pretext tasks can fulfill these and be
tailored for supervised learning, we propose a simple auxiliary
self-supervision task, predicting localizable rotation (LoRot). Our exhaustive
experiments validate the merits of LoRot as a pretext task tailored for
supervised learning in terms of robustness and generalization capability. Our
code is available at https://github.com/wjun0830/Localizable-Rotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Difficulty-Aware Simulator for Open Set Recognition. (arXiv:2207.10024v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10024">
<div class="article-summary-box-inner">
<span><p>Open set recognition (OSR) assumes unknown instances appear out of the blue
at the inference time. The main challenge of OSR is that the response of models
for unknowns is totally unpredictable. Furthermore, the diversity of open set
makes it harder since instances have different difficulty levels. Therefore, we
present a novel framework, DIfficulty-Aware Simulator (DIAS), that generates
fakes with diverse difficulty levels to simulate the real world. We first
investigate fakes from generative adversarial network (GAN) in the classifier's
viewpoint and observe that these are not severely challenging. This leads us to
define the criteria for difficulty by regarding samples generated with GANs
having moderate-difficulty. To produce hard-difficulty examples, we introduce
Copycat, imitating the behavior of the classifier. Furthermore, moderate- and
easy-difficulty samples are also yielded by our modified GAN and Copycat,
respectively. As a result, DIAS outperforms state-of-the-art methods with both
metrics of AUROC and F-score. Our code is available at
https://github.com/wjun0830/Difficulty-Aware-Simulator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks. (arXiv:2207.10025v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10025">
<div class="article-summary-box-inner">
<span><p>Facial expression in-the-wild is essential for various interactive computing
domains. Especially, "Learning from Synthetic Data" (LSD) is an important topic
in the facial expression recognition task. In this paper, we propose a
multi-task learning-based facial expression recognition approach which consists
of emotion and appearance learning branches that can share all face
information, and present preliminary results for the LSD challenge introduced
in the 4th affective behavior analysis in-the-wild (ABAW) competition. Our
method achieved the mean F1 score of 0.71.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locality Guidance for Improving Vision Transformers on Tiny Datasets. (arXiv:2207.10026v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10026">
<div class="article-summary-box-inner">
<span><p>While the Vision Transformer (VT) architecture is becoming trendy in computer
vision, pure VT models perform poorly on tiny datasets. To address this issue,
this paper proposes the locality guidance for improving the performance of VTs
on tiny datasets. We first analyze that the local information, which is of
great importance for understanding images, is hard to be learned with limited
data due to the high flexibility and intrinsic globality of the self-attention
mechanism in VTs. To facilitate local information, we realize the locality
guidance for VTs by imitating the features of an already trained convolutional
neural network (CNN), inspired by the built-in local-to-global hierarchy of
CNN. Under our dual-task learning paradigm, the locality guidance provided by a
lightweight CNN trained on low-resolution images is adequate to accelerate the
convergence and improve the performance of VTs to a large extent. Therefore,
our locality guidance approach is very simple and efficient, and can serve as a
basic performance enhancement method for VTs on tiny datasets. Extensive
experiments demonstrate that our method can significantly improve VTs when
training from scratch on tiny datasets and is compatible with different kinds
of VTs and datasets. For example, our proposed method can boost the performance
of various VTs on tiny datasets (e.g., 13.07% for DeiT, 8.98% for T2T and 7.85%
for PVT), and enhance even stronger baseline PVTv2 by 1.86% to 79.30%, showing
the potential of VTs on tiny datasets. The code is available at
https://github.com/lkhl/tiny-transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOTCOM: The Multi-Object Tracking Dataset Complexity Metric. (arXiv:2207.10031v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10031">
<div class="article-summary-box-inner">
<span><p>There exists no comprehensive metric for describing the complexity of
Multi-Object Tracking (MOT) sequences. This lack of metrics decreases
explainability, complicates comparison of datasets, and reduces the
conversation on tracker performance to a matter of leader board position. As a
remedy, we present the novel MOT dataset complexity metric (MOTCOM), which is a
combination of three sub-metrics inspired by key problems in MOT: occlusion,
erratic motion, and visual similarity. The insights of MOTCOM can open nuanced
discussions on tracker performance and may lead to a wider acknowledgement of
novel contributions developed for either less known datasets or those aimed at
solving sub-problems. We evaluate MOTCOM on the comprehensive MOT17, MOT20, and
MOTSynth datasets and show that MOTCOM is far better at describing the
complexity of MOT sequences compared to the conventional density and number of
tracks. Project page at https://vap.aau.dk/motcom
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Sparse 3D Object Detection. (arXiv:2207.10035v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10035">
<div class="article-summary-box-inner">
<span><p>As the perception range of LiDAR increases, LiDAR-based 3D object detection
becomes a dominant task in the long-range perception task of autonomous
driving. The mainstream 3D object detectors usually build dense feature maps in
the network backbone and prediction head. However, the computational and
spatial costs on the dense feature map are quadratic to the perception range,
which makes them hardly scale up to the long-range setting. To enable efficient
long-range LiDAR-based object detection, we build a fully sparse 3D object
detector (FSD). The computational and spatial cost of FSD is roughly linear to
the number of points and independent of the perception range. FSD is built upon
the general sparse voxel encoder and a novel sparse instance recognition (SIR)
module. SIR first groups the points into instances and then applies
instance-wise feature extraction and prediction. In this way, SIR resolves the
issue of center feature missing, which hinders the design of the fully sparse
architecture for all center-based or anchor-based detectors. Moreover, SIR
avoids the time-consuming neighbor queries in previous point-based methods by
grouping points into instances. We conduct extensive experiments on the
large-scale Waymo Open Dataset to reveal the working mechanism of FSD, and
state-of-the-art performance is reported. To demonstrate the superiority of FSD
in long-range detection, we also conduct experiments on Argoverse 2 Dataset,
which has a much larger perception range ($200m$) than Waymo Open Dataset
($75m$). On such a large perception range, FSD achieves state-of-the-art
performance and is 2.4$\times$ faster than the dense counterpart.Codes will be
released at https://github.com/TuSimple/SST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model. (arXiv:2207.10040v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10040">
<div class="article-summary-box-inner">
<span><p>Image restoration algorithms for atmospheric turbulence are known to be much
more challenging to design than traditional ones such as blur or noise because
the distortion caused by the turbulence is an entanglement of spatially varying
blur, geometric distortion, and sensor noise. Existing CNN-based restoration
methods built upon convolutional kernels with static weights are insufficient
to handle the spatially dynamical atmospheric turbulence effect. To address
this problem, in this paper, we propose a physics-inspired transformer model
for imaging through atmospheric turbulence. The proposed network utilizes the
power of transformer blocks to jointly extract a dynamical turbulence
distortion map and restore a turbulence-free image. In addition, recognizing
the lack of a comprehensive dataset, we collect and present two new real-world
turbulence datasets that allow for evaluation with both classical objective
metrics (e.g., PSNR and SSIM) and a new task-driven metric using text
recognition accuracy. Both real testing sets and all related code will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Densely Constrained Depth Estimator for Monocular 3D Object Detection. (arXiv:2207.10047v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10047">
<div class="article-summary-box-inner">
<span><p>Estimating accurate 3D locations of objects from monocular images is a
challenging problem because of lacking depth. Previous work shows that
utilizing the object's keypoint projection constraints to estimate multiple
depth candidates boosts the detection performance. However, the existing
methods can only utilize vertical edges as projection constraints for depth
estimation. So these methods only use a small number of projection constraints
and produce insufficient depth candidates, leading to inaccurate depth
estimation. In this paper, we propose a method that utilizes dense projection
constraints from edges of any direction. In this way, we employ much more
projection constraints and produce considerable depth candidates. Besides, we
present a graph matching weighting module to merge the depth candidates. The
proposed method DCD (Densely Constrained Detector) achieves state-of-the-art
performance on the KITTI and WOD benchmarks. Code is released at
https://github.com/BraveGroup/DCD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretraining a Neural Network before Knowing Its Architecture. (arXiv:2207.10049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10049">
<div class="article-summary-box-inner">
<span><p>Training large neural networks is possible by training a smaller hypernetwork
that predicts parameters for the large ones. A recently released Graph
HyperNetwork (GHN) trained this way on one million smaller ImageNet
architectures is able to predict parameters for large unseen networks such as
ResNet-50. While networks with predicted parameters lose performance on the
source task, the predicted parameters have been found useful for fine-tuning on
other tasks. We study if fine-tuning based on the same GHN is still useful on
novel strong architectures that were published after the GHN had been trained.
We found that for recent architectures such as ConvNeXt, GHN initialization
becomes less useful than for ResNet-50. One potential reason is the increased
distribution shift of novel architectures from those used to train the GHN. We
also found that the predicted parameters lack the diversity necessary to
successfully fine-tune parameters with gradient descent. We alleviate this
limitation by applying simple post-processing techniques to predicted
parameters before fine-tuning them on a target task and improve fine-tuning of
ResNet-50 and ConvNeXt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Clothed Human Reconstruction in the Wild. (arXiv:2207.10053v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10053">
<div class="article-summary-box-inner">
<span><p>Although much progress has been made in 3D clothed human reconstruction, most
of the existing methods fail to produce robust results from in-the-wild images,
which contain diverse human poses and appearances. This is mainly due to the
large domain gap between training datasets and in-the-wild datasets. The
training datasets are usually synthetic ones, which contain rendered images
from GT 3D scans. However, such datasets contain simple human poses and less
natural image appearances compared to those of real in-the-wild datasets, which
makes generalization of it to in-the-wild images extremely challenging. To
resolve this issue, in this work, we propose ClothWild, a 3D clothed human
reconstruction framework that firstly addresses the robustness on in-thewild
images. First, for the robustness to the domain gap, we propose a weakly
supervised pipeline that is trainable with 2D supervision targets of
in-the-wild datasets. Second, we design a DensePose-based loss function to
reduce ambiguities of the weak supervision. Extensive empirical tests on
several public in-the-wild datasets demonstrate that our proposed ClothWild
produces much more accurate and robust results than the state-of-the-art
methods. The codes are available in here:
https://github.com/hygenie1228/ClothWild_RELEASE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular 3D Object Reconstruction with GAN Inversion. (arXiv:2207.10061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10061">
<div class="article-summary-box-inner">
<span><p>Recovering a textured 3D mesh from a monocular image is highly challenging,
particularly for in-the-wild objects that lack 3D ground truths. In this work,
we present MeshInversion, a novel framework to improve the reconstruction by
exploiting the generative prior of a 3D GAN pre-trained for 3D textured mesh
synthesis. Reconstruction is achieved by searching for a latent space in the 3D
GAN that best resembles the target mesh in accordance with the single view
observation. Since the pre-trained GAN encapsulates rich 3D semantics in terms
of mesh geometry and texture, searching within the GAN manifold thus naturally
regularizes the realness and fidelity of the reconstruction. Importantly, such
regularization is directly applied in the 3D space, providing crucial guidance
of mesh parts that are unobserved in the 2D space. Experiments on standard
benchmarks show that our framework obtains faithful 3D reconstructions with
consistent geometry and texture across both observed and unobserved parts.
Moreover, it generalizes well to meshes that are less commonly seen, such as
the extended articulation of deformable objects. Code is released at
https://github.com/junzhezhang/mesh-inversion
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic uncertainty intervals for disentangled latent spaces. (arXiv:2207.10074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10074">
<div class="article-summary-box-inner">
<span><p>Meaningful uncertainty quantification in computer vision requires reasoning
about semantic information -- say, the hair color of the person in a photo or
the location of a car on the street. To this end, recent breakthroughs in
generative modeling allow us to represent semantic information in disentangled
latent spaces, but providing uncertainties on the semantic latent variables has
remained challenging. In this work, we provide principled uncertainty intervals
that are guaranteed to contain the true semantic factors for any underlying
generative model. The method does the following: (1) it uses quantile
regression to output a heuristic uncertainty interval for each element in the
latent space (2) calibrates these uncertainties such that they contain the true
value of the latent for a new, unseen input. The endpoints of these calibrated
intervals can then be propagated through the generator to produce interpretable
uncertainty visualizations for each semantic factor. This technique reliably
communicates semantically meaningful, principled, and instance-adaptive
uncertainty in inverse problems like image super-resolution and image
completion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is an Object-Centric Video Representation Beneficial for Transfer?. (arXiv:2207.10075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10075">
<div class="article-summary-box-inner">
<span><p>The objective of this work is to learn an object-centric video
representation, with the aim of improving transferability to novel tasks, i.e.,
tasks different from the pre-training task of action classification. To this
end, we introduce a new object-centric video recognition model based on a
transformer architecture. The model learns a set of object-centric summary
vectors for the video, and uses these vectors to fuse the visual and
spatio-temporal trajectory `modalities' of the video clip. We also introduce a
novel trajectory contrast loss to further enhance objectness in these summary
vectors. With experiments on four datasets -- SomethingSomething-V2,
SomethingElse, Action Genome and EpicKitchens -- we show that the
object-centric model outperforms prior video representations (both
object-agnostic and object-aware), when: (1) classifying actions on unseen
objects and unseen environments; (2) low-shot learning to novel classes; (3)
linear probe to other downstream tasks; as well as (4) for standard action
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discover and Mitigate Unknown Biases with Debiasing Alternate Networks. (arXiv:2207.10077v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10077">
<div class="article-summary-box-inner">
<span><p>Deep image classifiers have been found to learn biases from datasets. To
mitigate the biases, most previous methods require labels of protected
attributes (e.g., age, skin tone) as full-supervision, which has two
limitations: 1) it is infeasible when the labels are unavailable; 2) they are
incapable of mitigating unknown biases -- biases that humans do not
preconceive. To resolve those problems, we propose Debiasing Alternate Networks
(DebiAN), which comprises two networks -- a Discoverer and a Classifier. By
training in an alternate manner, the discoverer tries to find multiple unknown
biases of the classifier without any annotations of biases, and the classifier
aims at unlearning the biases identified by the discoverer. While previous
works evaluate debiasing results in terms of a single bias, we create
Multi-Color MNIST dataset to better benchmark mitigation of multiple biases in
a multi-bias setting, which not only reveals the problems in previous methods
but also demonstrates the advantage of DebiAN in identifying and mitigating
multiple biases simultaneously. We further conduct extensive experiments on
real-world datasets, showing that the discoverer in DebiAN can identify unknown
biases that may be hard to be found by humans. Regarding debiasing, DebiAN
achieves strong bias mitigation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of the hands in egocentric vision: A survey. (arXiv:1912.10867v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.10867">
<div class="article-summary-box-inner">
<span><p>Egocentric vision (a.k.a. first-person vision - FPV) applications have
thrived over the past few years, thanks to the availability of affordable
wearable cameras and large annotated datasets. The position of the wearable
camera (usually mounted on the head) allows recording exactly what the camera
wearers have in front of them, in particular hands and manipulated objects.
This intrinsic advantage enables the study of the hands from multiple
perspectives: localizing hands and their parts within the images; understanding
what actions and activities the hands are involved in; and developing
human-computer interfaces that rely on hand gestures. In this survey, we review
the literature that focuses on the hands using egocentric vision, categorizing
the existing approaches into: localization (where are the hands or parts of
them?); interpretation (what are the hands doing?); and application (e.g.,
systems that used egocentric hand cues for solving a specific problem).
Moreover, a list of the most prominent datasets with hand-based annotations is
provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Attacks on the DNN Interpretation System. (arXiv:2011.10698v3 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10698">
<div class="article-summary-box-inner">
<span><p>Interpretability is crucial to understand the inner workings of deep neural
networks (DNNs) and many interpretation methods generate saliency maps that
highlight parts of the input image that contribute the most to the prediction
made by the DNN. In this paper we design a backdoor attack that alters the
saliency map produced by the network for an input image only with injected
trigger that is invisible to the naked eye while maintaining the prediction
accuracy. The attack relies on injecting poisoned data with a trigger into the
training data set. The saliency maps are incorporated in the penalty term of
the objective function that is used to train a deep model and its influence on
model training is conditioned upon the presence of a trigger. We design two
types of attacks: targeted attack that enforces a specific modification of the
saliency map and untargeted attack when the importance scores of the top pixels
from the original saliency map are significantly reduced. We perform empirical
evaluation of the proposed backdoor attacks on gradient-based and gradient-free
interpretation methods for a variety of deep learning architectures. We show
that our attacks constitute a serious security threat when deploying deep
learning models developed by untrusty sources. Finally, in the Supplement we
demonstrate that the proposed methodology can be used in an inverted setting,
where the correct saliency map can be obtained only in the presence of a
trigger (key), effectively making the interpretation system available only to
selected users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Accurate Active Camera Localization. (arXiv:2012.04263v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04263">
<div class="article-summary-box-inner">
<span><p>In this work, we tackle the problem of active camera localization, which
controls the camera movements actively to achieve an accurate camera pose. The
past solutions are mostly based on Markov Localization, which reduces the
position-wise camera uncertainty for localization. These approaches localize
the camera in the discrete pose space and are agnostic to the
localization-driven scene property, which restricts the camera pose accuracy in
the coarse scale. We propose to overcome these limitations via a novel active
camera localization algorithm, composed of a passive and an active localization
module. The former optimizes the camera pose in the continuous pose space by
establishing point-wise camera-world correspondences. The latter explicitly
models the scene and camera uncertainty components to plan the right path for
accurate camera pose estimation. We validate our algorithm on the challenging
localization scenarios from both synthetic and scanned real-world indoor
scenes. Experimental results demonstrate that our algorithm outperforms both
the state-of-the-art Markov Localization based approach and other compared
approaches on the fine-scale camera pose accuracy. Code and data are released
at https://github.com/qhFang/AccurateACL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D2C-SR: A Divergence to Convergence Approach for Real-World Image Super-Resolution. (arXiv:2103.14373v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14373">
<div class="article-summary-box-inner">
<span><p>In this paper, we present D2C-SR, a novel framework for the task of
real-world image super-resolution. As an ill-posed problem, the key challenge
in super-resolution related tasks is there can be multiple predictions for a
given low-resolution input. Most classical deep learning based approaches
ignored the fundamental fact and lack explicit modeling of the underlying
high-frequency distribution which leads to blurred results. Recently, some
methods of GAN-based or learning super-resolution space can generate simulated
textures but do not promise the accuracy of the textures which have low
quantitative performance. Rethinking both, we learn the distribution of
underlying high-frequency details in a discrete form and propose a two-stage
pipeline: divergence stage to convergence stage. At divergence stage, we
propose a tree-based structure deep network as our divergence backbone.
Divergence loss is proposed to encourage the generated results from the
tree-based network to diverge into possible high-frequency representations,
which is our way of discretely modeling the underlying high-frequency
distribution. At convergence stage, we assign spatial weights to fuse these
divergent predictions to obtain the final output with more accurate details.
Our approach provides a convenient end-to-end manner to inference. We conduct
evaluations on several real-world benchmarks, including a new proposed
D2CRealSR dataset with x8 scaling factor. Our experiments demonstrate that
D2C-SR achieves better accuracy and visual improvements against
state-of-the-art methods, with a significantly less parameters number and our
D2C structure can also be applied as a generalized structure to some other
methods to obtain improvement. Our codes and dataset are available at
https://github.com/megvii-research/D2C-SR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training strategies and datasets for facial representation learning. (arXiv:2103.16554v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16554">
<div class="article-summary-box-inner">
<span><p>What is the best way to learn a universal face representation? Recent work on
Deep Learning in the area of face analysis has focused on supervised learning
for specific tasks of interest (e.g. face recognition, facial landmark
localization etc.) but has overlooked the overarching question of how to find a
facial representation that can be readily adapted to several facial analysis
tasks and datasets. To this end, we make the following 4 contributions: (a) we
introduce, for the first time, a comprehensive evaluation benchmark for facial
representation learning consisting of 5 important face analysis tasks. (b) We
systematically investigate two ways of large-scale representation learning
applied to faces: supervised and unsupervised pre-training. Importantly, we
focus our evaluations on the case of few-shot facial learning. (c) We
investigate important properties of the training datasets including their size
and quality (labelled, unlabelled or even uncurated). (d) To draw our
conclusions, we conducted a very large number of experiments. Our main two
findings are: (1) Unsupervised pre-training on completely in-the-wild,
uncurated data provides consistent and, in some cases, significant accuracy
improvements for all facial tasks considered. (2) Many existing facial video
datasets seem to have a large amount of redundancy. We will release code, and
pre-trained models to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TREND: Truncated Generalized Normal Density Estimation of Inception Embeddings for GAN Evaluation. (arXiv:2104.14767v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14767">
<div class="article-summary-box-inner">
<span><p>Evaluating image generation models such as generative adversarial networks
(GANs) is a challenging problem. A common approach is to compare the
distributions of the set of ground truth images and the set of generated test
images. The Frech\'et Inception distance is one of the most widely used metrics
for evaluation of GANs, which assumes that the features from a trained
Inception model for a set of images follow a normal distribution. In this
paper, we argue that this is an over-simplified assumption, which may lead to
unreliable evaluation results, and more accurate density estimation can be
achieved using a truncated generalized normal distribution. Based on this, we
propose a novel metric for accurate evaluation of GANs, named TREND (TRuncated
gEneralized Normal Density estimation of inception embeddings). We demonstrate
that our approach significantly reduces errors of density estimation, which
consequently eliminates the risk of faulty evaluation results. Furthermore, we
show that the proposed metric significantly improves robustness of evaluation
results against variation of the number of image samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaCLR: Motion-aware Contrastive Learning of Representations for Videos. (arXiv:2106.09703v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09703">
<div class="article-summary-box-inner">
<span><p>We present MaCLR, a novel method to explicitly perform cross-modal
self-supervised video representations learning from visual and motion
modalities. Compared to previous video representation learning methods that
mostly focus on learning motion cues implicitly from RGB inputs, MaCLR enriches
standard contrastive learning objectives for RGB video clips with a cross-modal
learning objective between a Motion pathway and a Visual pathway. We show that
the representation learned with our MaCLR method focuses more on foreground
motion regions and thus generalizes better to downstream tasks. To demonstrate
this, we evaluate MaCLR on five datasets for both action recognition and action
detection, and demonstrate state-of-the-art self-supervised performance on all
datasets. Furthermore, we show that MaCLR representation can be as effective as
representations learned with full supervision on UCF101 and HMDB51 action
recognition, and even outperform the supervised representation for action
recognition on VidSitu and SSv2, and action detection on AVA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Unbiased Visual Emotion Recognition via Causal Intervention. (arXiv:2107.12096v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12096">
<div class="article-summary-box-inner">
<span><p>Although much progress has been made in visual emotion recognition,
researchers have realized that modern deep networks tend to exploit dataset
characteristics to learn spurious statistical associations between the input
and the target. Such dataset characteristics are usually treated as dataset
bias, which damages the robustness and generalization performance of these
recognition systems. In this work, we scrutinize this problem from the
perspective of causal inference, where such dataset characteristic is termed as
a confounder which misleads the system to learn the spurious correlation. To
alleviate the negative effects brought by the dataset bias, we propose a novel
Interventional Emotion Recognition Network (IERN) to achieve the backdoor
adjustment, which is one fundamental deconfounding technique in causal
inference. Specifically, IERN starts by disentangling the dataset-related
context feature from the actual emotion feature, where the former forms the
confounder. The emotion feature will then be forced to see each confounder
stratum equally before being fed into the classifier. A series of designed
tests validate the efficacy of IERN, and experiments on three emotion
benchmarks demonstrate that IERN outperforms state-of-the-art approaches for
unbiased visual emotion recognition. Code is available at
https://github.com/donydchen/causal_emotion
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised learning methods and applications in medical imaging analysis: A survey. (arXiv:2109.08685v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08685">
<div class="article-summary-box-inner">
<span><p>The scarcity of high-quality annotated medical imaging datasets is a major
problem that collides with machine learning applications in the field of
medical imaging analysis and impedes its advancement. Self-supervised learning
is a recent training paradigm that enables learning robust representations
without the need for human annotation which can be considered an effective
solution for the scarcity of annotated medical data. This article reviews the
state-of-the-art research directions in self-supervised learning approaches for
image data with a concentration on their applications in the field of medical
imaging analysis. The article covers a set of the most recent self-supervised
learning methods from the computer vision field as they are applicable to the
medical imaging analysis and categorize them as predictive, generative, and
contrastive approaches. Moreover, the article covers 40 of the most recent
research papers in the field of self-supervised learning in medical imaging
analysis aiming at shedding the light on the recent innovation in the field.
Finally, the article concludes with possible future research directions in the
field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04079">
<div class="article-summary-box-inner">
<span><p>Accurate and reliable lane detection is vital for the safe performance of
lane-keeping assistance and lane departure warning systems. However, under
certain challenging circumstances, it is difficult to get satisfactory
performance in accurately detecting the lanes from one single image as mostly
done in current literature. Since lane markings are continuous lines, the lanes
that are difficult to be accurately detected in the current single image can
potentially be better deduced if information from previous frames is
incorporated. This study proposes a novel hybrid spatial-temporal (ST)
sequence-to-one deep learning architecture. This architecture makes full use of
the ST information in multiple continuous image frames to detect the lane
markings in the very last frame. Specifically, the hybrid model integrates the
following aspects: (a) the single image feature extraction module equipped with
the spatial convolutional neural network; (b) the ST feature integration module
constructed by ST recurrent neural network; (c) the encoder-decoder structure,
which makes this image segmentation problem work in an end-to-end supervised
learning format. Extensive experiments reveal that the proposed model
architecture can effectively handle challenging driving scenes and outperforms
available state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PointMixer: MLP-Mixer for Point Cloud Understanding. (arXiv:2111.11187v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11187">
<div class="article-summary-box-inner">
<span><p>MLP-Mixer has newly appeared as a new challenger against the realm of CNNs
and transformer. Despite its simplicity compared to transformer, the concept of
channel-mixing MLPs and token-mixing MLPs achieves noticeable performance in
visual recognition tasks. Unlike images, point clouds are inherently sparse,
unordered and irregular, which limits the direct use of MLP-Mixer for point
cloud understanding. In this paper, we propose PointMixer, a universal point
set operator that facilitates information sharing among unstructured 3D points.
By simply replacing token-mixing MLPs with a softmax function, PointMixer can
"mix" features within/between point sets. By doing so, PointMixer can be
broadly used in the network as inter-set mixing, intra-set mixing, and pyramid
mixing. Extensive experiments show the competitive or superior performance of
PointMixer in semantic segmentation, classification, and point reconstruction
against transformer-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Space-Partitioning RANSAC. (arXiv:2111.12385v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12385">
<div class="article-summary-box-inner">
<span><p>A new algorithm is proposed to accelerate RANSAC model quality calculations.
The method is based on partitioning the joint correspondence space, e.g., 2D-2D
point correspondences, into a pair of regular grids. The grid cells are mapped
by minimal sample models, estimated within RANSAC, to reject correspondences
that are inconsistent with the model parameters early. The proposed technique
is general. It works with arbitrary transformations even if a point is mapped
to a point set, e.g., as a fundamental matrix maps to epipolar lines. The
method is tested on thousands of image pairs from publicly available datasets
on fundamental and essential matrix, homography and radially distorted
homography estimation. On average, it reduces the RANSAC run-time by 41% with
provably no deterioration in the accuracy. It can be straightforwardly plugged
into state-of-the-art RANSAC frameworks, e.g. VSAC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Algebraic Representation for Systematic Generalization in Abstract Reasoning. (arXiv:2111.12990v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12990">
<div class="article-summary-box-inner">
<span><p>Is intelligence realized by connectionist or classicist? While connectionist
approaches have achieved superhuman performance, there has been growing
evidence that such task-specific superiority is particularly fragile in
systematic generalization. This observation lies in the central debate between
connectionist and classicist, wherein the latter continually advocates an
algebraic treatment in cognitive architectures. In this work, we follow the
classicist's call and propose a hybrid approach to improve systematic
generalization in reasoning. Specifically, we showcase a prototype with
algebraic representation for the abstract spatial-temporal reasoning task of
Raven's Progressive Matrices (RPM) and present the ALgebra-Aware
Neuro-Semi-Symbolic (ALANS) learner. The ALANS learner is motivated by abstract
algebra and the representation theory. It consists of a neural visual
perception frontend and an algebraic abstract reasoning backend: the frontend
summarizes the visual information from object-based representation, while the
backend transforms it into an algebraic structure and induces the hidden
operator on the fly. The induced operator is later executed to predict the
answer's representation, and the choice most similar to the prediction is
selected as the solution. Extensive experiments show that by incorporating an
algebraic treatment, the ALANS learner outperforms various pure connectionist
models in domains requiring systematic generalization. We further show the
generative nature of the learned algebraic representation; it can be decoded by
isomorphism to generate an answer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ManiFest: Manifold Deformation for Few-shot Image Translation. (arXiv:2111.13681v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13681">
<div class="article-summary-box-inner">
<span><p>Most image-to-image translation methods require a large number of training
images, which restricts their applicability. We instead propose ManiFest: a
framework for few-shot image translation that learns a context-aware
representation of a target domain from a few images only. To enforce feature
consistency, our framework learns a style manifold between source and proxy
anchor domains (assumed to be composed of large numbers of images). The learned
manifold is interpolated and deformed towards the few-shot target domain via
patch-based adversarial and feature statistics alignment losses. All of these
components are trained simultaneously during a single end-to-end loop. In
addition to the general few-shot translation task, our approach can
alternatively be conditioned on a single exemplar image to reproduce its
specific style. Extensive experiments demonstrate the efficacy of ManiFest on
multiple tasks, outperforming the state-of-the-art on all metrics and in both
the general- and exemplar-based scenarios. Our code is available at
https://github.com/cv-rits/Manifest .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Image Transformations for Transfer-based Adversarial Attack. (arXiv:2111.13844v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13844">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks provide a good way to study the robustness of deep
learning models. One category of methods in transfer-based black-box attack
utilizes several image transformation operations to improve the transferability
of adversarial examples, which is effective, but fails to take the specific
characteristic of the input image into consideration. In this work, we propose
a novel architecture, called Adaptive Image Transformation Learner (AITL),
which incorporates different image transformation operations into a unified
framework to further improve the transferability of adversarial examples.
Unlike the fixed combinational transformations used in existing works, our
elaborately designed transformation learner adaptively selects the most
effective combination of image transformations specific to the input image.
Extensive experiments on ImageNet demonstrate that our method significantly
improves the attack success rates on both normally trained models and defense
models under various settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Discriminative Shrinkage Deep Networks for Image Deconvolution. (arXiv:2111.13876v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13876">
<div class="article-summary-box-inner">
<span><p>Most existing methods usually formulate the non-blind deconvolution problem
into a maximum-a-posteriori framework and address it by manually designing
kinds of regularization terms and data terms of the latent clear images.
However, explicitly designing these two terms is quite challenging and usually
leads to complex optimization problems which are difficult to solve. In this
paper, we propose an effective non-blind deconvolution approach by learning
discriminative shrinkage functions to implicitly model these terms. In contrast
to most existing methods that use deep convolutional neural networks (CNNs) or
radial basis functions to simply learn the regularization term, we formulate
both the data term and regularization term and split the deconvolution model
into data-related and regularization-related sub-problems according to the
alternating direction method of multipliers. We explore the properties of the
Maxout function and develop a deep CNN model with a Maxout layer to learn
discriminative shrinkage functions to directly approximate the solutions of
these two sub-problems. Moreover, given the fast-Fourier-transform-based image
restoration usually leads to ringing artifacts while conjugate-gradient-based
approach is time-consuming, we develop the Conjugate Gradient Network to
restore the latent clear images effectively and efficiently. Experimental
results show that the proposed method performs favorably against the
state-of-the-art ones in terms of efficiency and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">META: Mimicking Embedding via oThers' Aggregation for Generalizable Person Re-identification. (arXiv:2112.08684v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08684">
<div class="article-summary-box-inner">
<span><p>Domain generalizable (DG) person re-identification (ReID) aims to test across
unseen domains without access to the target domain data at training time, which
is a realistic but challenging problem. In contrast to methods assuming an
identical model for different domains, Mixture of Experts (MoE) exploits
multiple domain-specific networks for leveraging complementary information
between domains, obtaining impressive results. However, prior MoE-based DG ReID
methods suffer from a large model size with the increase of the number of
source domains, and most of them overlook the exploitation of domain-invariant
characteristics. To handle the two issues above, this paper presents a new
approach called Mimicking Embedding via oThers' Aggregation (META) for DG ReID.
To avoid the large model size, experts in META do not add a branch network for
each source domain but share all the parameters except for the batch
normalization layers. Besides multiple experts, META leverages Instance
Normalization (IN) and introduces it into a global branch to pursue invariant
features across domains. Meanwhile, META considers the relevance of an unseen
target sample and source domains via normalization statistics and develops an
aggregation network to adaptively integrate multiple experts for mimicking
unseen target domain. Benefiting from a proposed consistency loss and an
episodic training algorithm, we can expect META to mimic embedding for a truly
unseen target domain. Extensive experiments verify that META surpasses
state-of-the-art DG ReID methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Uncertain Single-View Depths in Colonoscopies. (arXiv:2112.08906v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08906">
<div class="article-summary-box-inner">
<span><p>Estimating depth information from endoscopic images is a prerequisite for a
wide set of AI-assisted technologies, such as accurate localization and
measurement of tumors, or identification of non-inspected areas. As the domain
specificity of colonoscopies -- deformable low-texture environments with
fluids, poor lighting conditions and abrupt sensor motions -- pose challenges
to multi-view 3D reconstructions, single-view depth learning stands out as a
promising line of research. Depth learning can be extended in a Bayesian
setting, which enables continual learning, improves decision making and can be
used to compute confidence intervals or quantify uncertainty for in-body
measurements. In this paper, we explore for the first time Bayesian deep
networks for single-view depth estimation in colonoscopies. Our specific
contribution is two-fold: 1) an exhaustive analysis of scalable Bayesian
networks for depth learning in different datasets, highlighting challenges and
conclusions regarding synthetic-to-real domain changes and supervised vs.
self-supervised methods; and 2) a novel teacher-student approach to deep depth
learning that takes into account the teacher uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Responsive Listening Head Generation: A Benchmark Dataset and Baseline. (arXiv:2112.13548v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13548">
<div class="article-summary-box-inner">
<span><p>We present a new listening head generation benchmark, for synthesizing
responsive feedbacks of a listener (e.g., nod, smile) during a face-to-face
conversation. As the indispensable complement to talking heads generation,
listening head generation has seldomly been studied in literature.
Automatically synthesizing listening behavior that actively responds to a
talking head, is critical to applications such as digital human, virtual agents
and social robots. In this work, we propose a novel dataset "ViCo",
highlighting the listening head generation during a face-to-face conversation.
A total number of 92 identities (67 speakers and 76 listeners) are involved in
ViCo, featuring 483 clips in a paired "speaking-listening" pattern, where
listeners show three listening styles based on their attitudes: positive,
neutral, negative. Different from traditional speech-to-gesture or talking-head
generation, listening head generation takes as input both the audio and visual
signals from the speaker, and gives non-verbal feedbacks (e.g., head motions,
facial expressions) in a real-time manner. Our dataset supports a wide range of
applications such as human-to-human interaction, video-to-video translation,
cross-modal understanding and generation. To encourage further research, we
also release a listening head generation baseline, conditioning on different
listening attitudes. Code &amp; ViCo dataset: https://project.mhzhou.com/vico.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Visual-Auditory Human-eye Fixation Prediction with Multigranularity Perception. (arXiv:2112.13697v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13697">
<div class="article-summary-box-inner">
<span><p>Thanks to the rapid advances in deep learning techniques and the wide
availability of large-scale training sets, the performance of video saliency
detection models has been improving steadily and significantly. However, deep
learning-based visualaudio fixation prediction is still in its infancy. At
present, only a few visual-audio sequences have been furnished, with real
fixations being recorded in real visual-audio environments. Hence, it would be
neither efficient nor necessary to recollect real fixations under the same
visual-audio circumstances. To address this problem, this paper promotes a
novel approach in a weakly supervised manner to alleviate the demand of
large-scale training sets for visual-audio model training. By using only the
video category tags, we propose the selective class activation mapping (SCAM)
and its upgrade (SCAM+). In the spatial-temporal-audio circumstance, the former
follows a coarse-to-fine strategy to select the most discriminative regions,
and these regions are usually capable of exhibiting high consistency with the
real human-eye fixations. The latter equips the SCAM with an additional
multi-granularity perception mechanism, making the whole process more
consistent with that of the real human visual system. Moreover, we distill
knowledge from these regions to obtain complete new spatial-temporal-audio
(STA) fixation prediction (FP) networks, enabling broad applications in cases
where video tags are not available. Without resorting to any real human-eye
fixation, the performances of these STA FP networks are comparable to those of
fully supervised networks. The code and results are publicly available at
https://github.com/guotaowang/STANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Poseur: Direct Human Pose Regression with Transformers. (arXiv:2201.07412v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07412">
<div class="article-summary-box-inner">
<span><p>We propose a direct, regression-based approach to 2D human pose estimation
from single images. We formulate the problem as a sequence prediction task,
which we solve using a Transformer network. This network directly learns a
regression mapping from images to the keypoint coordinates, without resorting
to intermediate representations such as heatmaps. This approach avoids much of
the complexity associated with heatmap-based approaches. To overcome the
feature misalignment issues of previous regression-based methods, we propose an
attention mechanism that adaptively attends to the features that are most
relevant to the target keypoints, considerably improving the accuracy.
Importantly, our framework is end-to-end differentiable, and naturally learns
to exploit the dependencies between keypoints. Experiments on MS-COCO and MPII,
two predominant pose-estimation datasets, demonstrate that our method
significantly improves upon the state-of-the-art in regression-based pose
estimation. More notably, ours is the first regression-based approach to
perform favorably compared to the best heatmap-based pose estimation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Increasing the Cost of Model Extraction with Calibrated Proof of Work. (arXiv:2201.09243v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09243">
<div class="article-summary-box-inner">
<span><p>In model extraction attacks, adversaries can steal a machine learning model
exposed via a public API by repeatedly querying it and adjusting their own
model based on obtained predictions. To prevent model stealing, existing
defenses focus on detecting malicious queries, truncating, or distorting
outputs, thus necessarily introducing a tradeoff between robustness and model
utility for legitimate users. Instead, we propose to impede model extraction by
requiring users to complete a proof-of-work before they can read the model's
predictions. This deters attackers by greatly increasing (even up to 100x) the
computational effort needed to leverage query access for model extraction.
Since we calibrate the effort required to complete the proof-of-work to each
query, this only introduces a slight overhead for regular users (up to 2x). To
achieve this, our calibration applies tools from differential privacy to
measure the information revealed by a query. Our method requires no
modification of the victim model and can be applied by machine learning
practitioners to guard their publicly exposed models against being easily
stolen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FORML: Learning to Reweight Data for Fairness. (arXiv:2202.01719v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01719">
<div class="article-summary-box-inner">
<span><p>Machine learning models are trained to minimize the mean loss for a single
metric, and thus typically do not consider fairness and robustness. Neglecting
such metrics in training can make these models prone to fairness violations
when training data are imbalanced or test distributions differ. This work
introduces Fairness Optimized Reweighting via Meta-Learning (FORML), a training
algorithm that balances fairness and robustness with accuracy by jointly
learning training sample weights and neural network parameters. The approach
increases model fairness by learning to balance the contributions from both
over- and under-represented sub-groups through dynamic reweighting of the data
learned from a user-specified held-out set representative of the distribution
under which fairness is desired. FORML improves equality of opportunity
fairness criteria on image classification tasks, reduces bias of corrupted
labels, and facilitates building more fair datasets via data condensation.
These improvements are achieved without pre-processing data or post-processing
model outputs, without learning an additional weighting function, without
changing model architecture, and while maintaining accuracy on the original
predictive metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Devil in Graph Spectral Domain for 3D Point Cloud Attacks. (arXiv:2202.07261v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07261">
<div class="article-summary-box-inner">
<span><p>3D dynamic point clouds provide a discrete representation of real-world
objects or scenes in motion, which have been widely applied in immersive
telepresence, autonomous driving, surveillance, \textit{etc}. However, point
clouds acquired from sensors are usually perturbed by noise, which affects
downstream tasks such as surface reconstruction and analysis. Although many
efforts have been made for static point cloud denoising, few works address
dynamic point cloud denoising. In this paper, we propose a novel gradient-based
dynamic point cloud denoising method, exploiting the temporal correspondence
for the estimation of gradient fields -- also a fundamental problem in dynamic
point cloud processing and analysis. The gradient field is the gradient of the
log-probability function of the noisy point cloud, based on which we perform
gradient ascent so as to converge each point to the underlying clean surface.
We estimate the gradient of each surface patch by exploiting the temporal
correspondence, where the temporally corresponding patches are searched
leveraging on rigid motion in classical mechanics. In particular, we treat each
patch as a rigid object, which moves in the gradient field of an adjacent frame
via force until reaching a balanced state, i.e., when the sum of gradients over
the patch reaches 0. Since the gradient would be smaller when the point is
closer to the underlying surface, the balanced patch would fit the underlying
surface well, thus leading to the temporal correspondence. Finally, the
position of each point in the patch is updated along the direction of the
gradient averaged from corresponding patches in adjacent frames. Experimental
results demonstrate that the proposed model outperforms state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Provable Stochastic Optimization for Global Contrastive Learning: Small Batch Does Not Harm Performance. (arXiv:2202.12387v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12387">
<div class="article-summary-box-inner">
<span><p>In this paper, we study contrastive learning from an optimization
perspective, aiming to analyze and address a fundamental issue of existing
contrastive learning methods that either rely on a large batch size or a large
dictionary of feature vectors. We consider a global objective for contrastive
learning, which contrasts each positive pair with all negative pairs for an
anchor point. From the optimization perspective, we explain why existing
methods such as SimCLR require a large batch size in order to achieve a
satisfactory result. In order to remove such requirement, we propose a
memory-efficient Stochastic Optimization algorithm for solving the Global
objective of Contrastive Learning of Representations, named SogCLR. We show
that its optimization error is negligible under a reasonable condition after a
sufficient number of iterations or is diminishing for a slightly different
global contrastive objective. Empirically, we demonstrate that SogCLR with
small batch size (e.g., 256) can achieve similar performance as SimCLR with
large batch size (e.g., 8192) on self-supervised learning task on ImageNet-1K.
We also attempt to show that the proposed optimization technique is generic and
can be applied to solving other contrastive losses, e.g., two-way contrastive
losses for bimodal contrastive learning. The proposed method is implemented in
our open-sourced library LibAUC (www.libauc.org).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning for Real-World Super-Resolution from Dual Zoomed Observations. (arXiv:2203.01325v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01325">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider two challenging issues in reference-based
super-resolution (RefSR), (i) how to choose a proper reference image, and (ii)
how to learn real-world RefSR in a self-supervised manner. Particularly, we
present a novel self-supervised learning approach for real-world image SR from
observations at dual camera zooms (SelfDZSR). Considering the popularity of
multiple cameras in modern smartphones, the more zoomed (telephoto) image can
be naturally leveraged as the reference to guide the SR of the lesser zoomed
(short-focus) image. Furthermore, SelfDZSR learns a deep network to obtain the
SR result of short-focus image to have the same resolution as the telephoto
image. For this purpose, we take the telephoto image instead of an additional
high-resolution image as the supervision information and select a center patch
from it as the reference to super-resolve the corresponding short-focus image
patch. To mitigate the effect of the misalignment between short-focus
low-resolution (LR) image and telephoto ground-truth (GT) image, we design an
auxiliary-LR generator and map the GT to an auxiliary-LR while keeping the
spatial position unchanged. Then the auxiliary-LR can be utilized to deform the
LR features by the proposed adaptive spatial transformer networks (AdaSTN), and
match the Ref features to GT. During testing, SelfDZSR can be directly deployed
to super-solve the whole short-focus image with the reference of telephoto
image. Experiments show that our method achieves better quantitative and
qualitative performance against state-of-the-arts. Codes are available at
https://github.com/cszhilu1998/SelfDZSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextformer: A Transformer with Spatio-Channel Attention for Context Modeling in Learned Image Compression. (arXiv:2203.02452v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02452">
<div class="article-summary-box-inner">
<span><p>Entropy modeling is a key component for high-performance image compression
algorithms. Recent developments in autoregressive context modeling helped
learning-based methods to surpass their classical counterparts. However, the
performance of those models can be further improved due to the underexploited
spatio-channel dependencies in latent space, and the suboptimal implementation
of context adaptivity. Inspired by the adaptive characteristics of the
transformers, we propose a transformer-based context model, named
Contextformer, which generalizes the de facto standard attention mechanism to
spatio-channel attention. We replace the context model of a modern compression
framework with the Contextformer and test it on the widely used Kodak,
CLIC2020, and Tecnick image datasets. Our experimental results show that the
proposed model provides up to 11% rate savings compared to the standard
Versatile Video Coding (VVC) Test Model (VTM) 16.2, and outperforms various
learning-based models in terms of PSNR and MS-SSIM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PASS: Part-Aware Self-Supervised Pre-Training for Person Re-Identification. (arXiv:2203.03931v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03931">
<div class="article-summary-box-inner">
<span><p>In person re-identification (ReID), very recent researches have validated
pre-training the models on unlabelled person images is much better than on
ImageNet. However, these researches directly apply the existing self-supervised
learning (SSL) methods designed for image classification to ReID without any
adaption in the framework. These SSL methods match the outputs of local views
(e.g., red T-shirt, blue shorts) to those of the global views at the same time,
losing lots of details. In this paper, we propose a ReID-specific pre-training
method, Part-Aware Self-Supervised pre-training (PASS), which can generate
part-level features to offer fine-grained information and is more suitable for
ReID. PASS divides the images into several local areas, and the local views
randomly cropped from each area are assigned with a specific learnable [PART]
token. On the other hand, the [PART]s of all local areas are also appended to
the global views. PASS learns to match the output of the local views and global
views on the same [PART]. That is, the learned [PART] of the local views from a
local area is only matched with the corresponding [PART] learned from the
global views. As a result, each [PART] can focus on a specific local area of
the image and extracts fine-grained information of this area. Experiments show
PASS sets the new state-of-the-art performances on Market1501 and MSMT17 on
various ReID tasks, e.g., vanilla ViT-S/16 pre-trained by PASS achieves
92.2\%/90.2\%/88.5\% mAP accuracy on Market1501 for supervised/UDA/USL ReID.
Our codes are available at https://github.com/CASIA-IVA-Lab/PASS-reID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RankSeg: Adaptive Pixel Classification with Image Category Ranking for Segmentation. (arXiv:2203.04187v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04187">
<div class="article-summary-box-inner">
<span><p>The segmentation task has traditionally been formulated as a complete-label
pixel classification task to predict a class for each pixel from a fixed number
of predefined semantic categories shared by all images or videos. Yet,
following this formulation, standard architectures will inevitably encounter
various challenges under more realistic settings where the scope of categories
scales up (e.g., beyond the level of 1k). On the other hand, in a typical image
or video, only a few categories, i.e., a small subset of the complete label are
present. Motivated by this intuition, in this paper, we propose to decompose
segmentation into two sub-problems: (i) image-level or video-level multi-label
classification and (ii) pixel-level rank-adaptive selected-label
classification. Given an input image or video, our framework first conducts
multi-label classification over the complete label, then sorts the complete
label and selects a small subset according to their class confidence scores. We
then use a rank-adaptive pixel classifier to perform the pixel-wise
classification over only the selected labels, which uses a set of rank-oriented
learnable temperature parameters to adjust the pixel classifications scores.
Our approach is conceptually general and can be used to improve various
existing segmentation frameworks by simply using a lightweight multi-label
classification head and rank-adaptive pixel classifier. We demonstrate the
effectiveness of our framework with competitive experimental results across
four tasks, including image semantic segmentation, image panoptic segmentation,
video instance segmentation, and video semantic segmentation. Especially, with
our RankSeg, Mask2Former gains +0.8%/+0.7%/+0.7% on ADE20K panoptic
segmentation/YouTubeVIS 2019 video instance segmentation/VSPW video semantic
segmentation benchmarks respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant. (arXiv:2203.04203v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04203">
<div class="article-summary-box-inner">
<span><p>A long-standing goal of intelligent assistants such as AR glasses/robots has
been to assist users in affordance-centric real-world scenarios, such as "how
can I run the microwave for 1 minute?". However, there is still no clear task
definition and suitable benchmarks. In this paper, we define a new task called
Affordance-centric Question-driven Task Completion, where the AI assistant
should learn from instructional videos to provide step-by-step help in the
user's view. To support the task, we constructed AssistQ, a new dataset
comprising 531 question-answer samples from 100 newly filmed instructional
videos. We also developed a novel Question-to-Actions (Q2A) model to address
the AQTC task and validate it on the AssistQ dataset. The results show that our
model significantly outperforms several VQA-related baselines while still
having large room for improvement. We expect our task and dataset to advance
Egocentric AI Assistant's development. Our project page is available at:
https://showlab.github.io/assistq/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Align-Deform-Subtract: An Interventional Framework for Explaining Object Differences. (arXiv:2203.04694v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04694">
<div class="article-summary-box-inner">
<span><p>Given two object images, how can we explain their differences in terms of the
underlying object properties? To address this question, we propose
Align-Deform-Subtract (ADS) -- an interventional framework for explaining
object differences. By leveraging semantic alignments in image-space as
counterfactual interventions on the underlying object properties, ADS
iteratively quantifies and removes differences in object properties. The result
is a set of "disentangled" error measures which explain object differences in
terms of the underlying properties. Experiments on real and synthetic data
illustrate the efficacy of the framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes. (arXiv:2203.05203v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05203">
<div class="article-summary-box-inner">
<span><p>3D dense captioning is a recently-proposed novel task, where point clouds
contain more geometric information than the 2D counterpart. However, it is also
more challenging due to the higher complexity and wider variety of inter-object
relations contained in point clouds. Existing methods only treat such relations
as by-products of object feature learning in graphs without specifically
encoding them, which leads to sub-optimal results. In this paper, aiming at
improving 3D dense captioning via capturing and utilizing the complex relations
in the 3D scene, we propose MORE, a Multi-Order RElation mining model, to
support generating more descriptive and comprehensive captions. Technically,
our MORE encodes object relations in a progressive manner since complex
relations can be deduced from a limited number of basic ones. We first devise a
novel Spatial Layout Graph Convolution (SLGC), which semantically encodes
several first-order relations as edges of a graph constructed over 3D object
proposals. Next, from the resulting graph, we further extract multiple triplets
which encapsulate basic first-order relations as the basic unit, and construct
several Object-centric Triplet Attention Graphs (OTAG) to infer multi-order
relations for every target object. The updated node features from OTAG are
aggregated and fed into the caption decoder to provide abundant relational
cues, so that captions including diverse relations with context objects can be
generated. Extensive experiments on the Scan2Cap dataset prove the
effectiveness of our proposed MORE and its components, and we also outperform
the current state-of-the-art method. Our code is available at
https://github.com/SxJyJay/MORE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuromorphic Data Augmentation for Training Spiking Neural Networks. (arXiv:2203.06145v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06145">
<div class="article-summary-box-inner">
<span><p>Developing neuromorphic intelligence on event-based datasets with Spiking
Neural Networks (SNNs) has recently attracted much research attention. However,
the limited size of event-based datasets makes SNNs prone to overfitting and
unstable convergence. This issue remains unexplored by previous academic works.
In an effort to minimize this generalization gap, we propose Neuromorphic Data
Augmentation (NDA), a family of geometric augmentations specifically designed
for event-based datasets with the goal of significantly stabilizing the SNN
training and reducing the generalization gap between training and test
performance. The proposed method is simple and compatible with existing SNN
training pipelines. Using the proposed augmentation, for the first time, we
demonstrate the feasibility of unsupervised contrastive learning for SNNs. We
conduct comprehensive experiments on prevailing neuromorphic vision benchmarks
and show that NDA yields substantial improvements over previous
state-of-the-art results. For example, the NDA-based SNN achieves accuracy gain
on CIFAR10-DVS and N-Caltech 101 by 10.1% and 13.7%, respectively. Code is
available on GitHub https://github.com/Intelligent-Computing-Lab-Yale/NDA_SNN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bringing Rolling Shutter Images Alive with Dual Reversed Distortion. (arXiv:2203.06451v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06451">
<div class="article-summary-box-inner">
<span><p>Rolling shutter (RS) distortion can be interpreted as the result of picking a
row of pixels from instant global shutter (GS) frames over time during the
exposure of the RS camera. This means that the information of each instant GS
frame is partially, yet sequentially, embedded into the row-dependent
distortion. Inspired by this fact, we address the challenging task of reversing
this process, i.e., extracting undistorted GS frames from images suffering from
RS distortion. However, since RS distortion is coupled with other factors such
as readout settings and the relative velocity of scene elements to the camera,
models that only exploit the geometric correlation between temporally adjacent
images suffer from poor generality in processing data with different readout
settings and dynamic scenes with both camera motion and object motion. In this
paper, instead of two consecutive frames, we propose to exploit a pair of
images captured by dual RS cameras with reversed RS directions for this highly
challenging task. Grounded on the symmetric and complementary nature of dual
reversed distortion, we develop a novel end-to-end model, IFED, to generate
dual optical flow sequence through iterative learning of the velocity field
during the RS time. Extensive experimental results demonstrate that IFED is
superior to naive cascade schemes, as well as the state-of-the-art which
utilizes adjacent RS images. Most importantly, although it is trained on a
synthetic dataset, IFED is shown to be effective at retrieving GS frame
sequences from real-world RS distorted images of dynamic scenes. Code is
available at https://github.com/zzh-tech/Dual-Reversed-RS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes. (arXiv:2203.09440v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09440">
<div class="article-summary-box-inner">
<span><p>Many basic indoor activities such as eating or writing are always conducted
upon different tabletops (e.g., coffee tables, writing desks). It is
indispensable to understanding tabletop scenes in 3D indoor scene parsing
applications. Unfortunately, it is hard to meet this demand by directly
deploying data-driven algorithms, since 3D tabletop scenes are rarely available
in current datasets. To remedy this defect, we introduce TO-Scene, a
large-scale dataset focusing on tabletop scenes, which contains 20,740 scenes
with three variants. To acquire the data, we design an efficient and scalable
framework, where a crowdsourcing UI is developed to transfer CAD objects from
ModelNet and ShapeNet onto tables from ScanNet, then the output tabletop scenes
are simulated into real scans and annotated automatically.
</p>
<p>Further, a tabletop-aware learning strategy is proposed for better perceiving
the small-sized tabletop instances. Notably, we also provide a real scanned
test set TO-Real to verify the practical value of TO-Scene. Experiments show
that the algorithms trained on TO-Scene indeed work on the realistic test data,
and our proposed tabletop-aware learning strategy greatly improves the
state-of-the-art results on both 3D semantic segmentation and object detection
tasks. Dataset and code are available at
https://github.com/GAP-LAB-CUHK-SZ/TO-Scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoGS: Controllable Generation and Search from Sketch and Style. (arXiv:2203.09554v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09554">
<div class="article-summary-box-inner">
<span><p>We present CoGS, a novel method for the style-conditioned, sketch-driven
synthesis of images. CoGS enables exploration of diverse appearance
possibilities for a given sketched object, enabling decoupled control over the
structure and the appearance of the output. Coarse-grained control over object
structure and appearance are enabled via an input sketch and an exemplar
"style" conditioning image to a transformer-based sketch and style encoder to
generate a discrete codebook representation. We map the codebook representation
into a metric space, enabling fine-grained control over selection and
interpolation between multiple synthesis options before generating the image
via a vector quantized GAN (VQGAN) decoder. Our framework thereby unifies
search and synthesis tasks, in that a sketch and style pair may be used to run
an initial synthesis which may be refined via combination with similar results
in a search corpus to produce an image more closely matching the user's intent.
We show that our model, trained on the 125 object classes of our newly created
Pseudosketches dataset, is capable of producing a diverse gamut of semantic
content and appearance styles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Visual Tracking by Segmentation. (arXiv:2203.11191v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11191">
<div class="article-summary-box-inner">
<span><p>Estimating the target extent poses a fundamental challenge in visual object
tracking. Typically, trackers are box-centric and fully rely on a bounding box
to define the target in the scene. In practice, objects often have complex
shapes and are not aligned with the image axis. In these cases, bounding boxes
do not provide an accurate description of the target and often contain a
majority of background pixels. We propose a segmentation-centric tracking
pipeline that not only produces a highly accurate segmentation mask, but also
internally works with segmentation masks instead of bounding boxes. Thus, our
tracker is able to better learn a target representation that clearly
differentiates the target in the scene from background content. In order to
achieve the necessary robustness for the challenging tracking scenario, we
propose a separate instance localization component that is used to condition
the segmentation decoder when producing the output mask. We infer a bounding
box from the segmentation mask, validate our tracker on challenging tracking
datasets and achieve the new state of the art on LaSOT with a success AUC score
of 69.7%. Since most tracking datasets do not contain mask annotations, we
cannot use them to evaluate predicted segmentation masks. Instead, we validate
our segmentation quality on two popular video object segmentation datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Scene Graph Generation with Data Transfer. (arXiv:2203.11654v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11654">
<div class="article-summary-box-inner">
<span><p>Scene graph generation (SGG) is designed to extract (subject, predicate,
object) triplets in images. Recent works have made a steady progress on SGG,
and provide useful tools for high-level vision and language understanding.
However, due to the data distribution problems including long-tail distribution
and semantic ambiguity, the predictions of current SGG models tend to collapse
to several frequent but uninformative predicates (e.g., on, at), which limits
practical application of these models in downstream tasks. To deal with the
problems above, we propose a novel Internal and External Data Transfer
(IETrans) method, which can be applied in a plug-and-play fashion and expanded
to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the
data distribution problem by automatically creating an enhanced dataset that
provides more sufficient and coherent annotations for all predicates. By
training on the enhanced dataset, a Neural Motif model doubles the macro
performance while maintaining competitive micro performance. The code and data
are publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Prompt Tuning. (arXiv:2203.12119v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12119">
<div class="article-summary-box-inner">
<span><p>The current modus operandi in adapting pre-trained models involves updating
all the backbone parameters, ie, full fine-tuning. This paper introduces Visual
Prompt Tuning (VPT) as an efficient and effective alternative to full
fine-tuning for large-scale Transformer models in vision. Taking inspiration
from recent advances in efficiently tuning large language models, VPT
introduces only a small amount (less than 1% of model parameters) of trainable
parameters in the input space while keeping the model backbone frozen. Via
extensive experiments on a wide variety of downstream recognition tasks, we
show that VPT achieves significant performance gains compared to other
parameter efficient tuning protocols. Most importantly, VPT even outperforms
full fine-tuning in many cases across model capacities and training data
scales, while reducing per-task storage cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale and Cross-scale Contrastive Learning for Semantic Segmentation. (arXiv:2203.13409v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13409">
<div class="article-summary-box-inner">
<span><p>This work considers supervised contrastive learning for semantic
segmentation. We apply contrastive learning to enhance the discriminative power
of the multi-scale features extracted by semantic segmentation networks. Our
key methodological insight is to leverage samples from the feature spaces
emanating from multiple stages of a model's encoder itself requiring neither
data augmentation nor online memory banks to obtain a diverse set of samples.
To allow for such an extension we introduce an efficient and effective sampling
process, that enables applying contrastive losses over the encoder's features
at multiple scales. Furthermore, by first mapping the encoder's multi-scale
representations to a common feature space, we instantiate a novel form of
supervised local-global constraint by introducing cross-scale contrastive
learning linking high-resolution local features to low-resolution global
features. Combined, our multi-scale and cross-scale contrastive losses boost
performance of various models (DeepLabV3, HRNet, OCRNet, UPerNet) with both CNN
and Transformer backbones, when evaluated on 4 diverse datasets from natural
(Cityscapes, PascalContext, ADE20K) but also surgical (CaDIS) domains. Our code
is available at https://github.com/RViMLab/MS_CS_ContrSeg. datasets from
natural (Cityscapes, PascalContext, ADE20K) but also surgical (CaDIS) domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Learning Neural Representations from Shadows. (arXiv:2203.15946v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15946">
<div class="article-summary-box-inner">
<span><p>We present a method that learns neural shadow fields which are neural scene
representations that are only learnt from the shadows present in the scene.
While traditional shape-from-shadow (SfS) algorithms reconstruct geometry from
shadows, they assume a fixed scanning setup and fail to generalize to complex
scenes. Neural rendering algorithms, on the other hand, rely on photometric
consistency between RGB images, but largely ignore physical cues such as
shadows, which have been shown to provide valuable information about the scene.
We observe that shadows are a powerful cue that can constrain neural scene
representations to learn SfS, and even outperform NeRF to reconstruct otherwise
hidden geometry. We propose a graphics-inspired differentiable approach to
render accurate shadows with volumetric rendering, predicting a shadow map that
can be compared to the ground truth shadow. Even with just binary shadow maps,
we show that neural rendering can localize the object and estimate coarse
geometry. Our approach reveals that sparse cues in images can be used to
estimate geometry using differentiable volumetric rendering. Moreover, our
framework is highly generalizable and can work alongside existing 3D
reconstruction techniques that otherwise only use photometric consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection. (arXiv:2203.16317v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16317">
<div class="article-summary-box-inner">
<span><p>In this paper, we delve into two key techniques in Semi-Supervised Object
Detection (SSOD), namely pseudo labeling and consistency training. We observe
that these two techniques currently neglect some important properties of object
detection, hindering efficient learning on unlabeled data. Specifically, for
pseudo labeling, existing works only focus on the classification score yet fail
to guarantee the localization precision of pseudo boxes; For consistency
training, the widely adopted random-resize training only considers the
label-level consistency but misses the feature-level one, which also plays an
important role in ensuring the scale invariance. To address the problems
incurred by noisy pseudo boxes, we design Noisy Pseudo box Learning (NPL) that
includes Prediction-guided Label Assignment (PLA) and Positive-proposal
Consistency Voting (PCV). PLA relies on model predictions to assign labels and
makes it robust to even coarse pseudo boxes; while PCV leverages the regression
consistency of positive proposals to reflect the localization quality of pseudo
boxes. Furthermore, in consistency training, we propose Multi-view
Scale-invariant Learning (MSL) that includes mechanisms of both label- and
feature-level consistency, where feature consistency is achieved by aligning
shifted feature pyramids between two images with identical content but varied
scales. On COCO benchmark, our method, termed PSEudo labeling and COnsistency
training (PseCo), outperforms the SOTA (Soft Teacher) by 2.0, 1.8, 2.0 points
under 1%, 5%, and 10% labelling ratios, respectively. It also significantly
improves the learning efficiency for SSOD, e.g., PseCo halves the training time
of the SOTA approach but achieves even better performance. Code is available at
https://github.com/ligang-cs/PseCo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RFNet-4D: Joint Object Reconstruction and Flow Estimation from 4D Point Clouds. (arXiv:2203.16482v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16482">
<div class="article-summary-box-inner">
<span><p>Object reconstruction from 3D point clouds has achieved impressive progress
in the computer vision and computer graphics research field. However,
reconstruction from time-varying point clouds (a.k.a. 4D point clouds) is
generally overlooked. In this paper, we propose a new network architecture,
namely RFNet-4D, that jointly reconstruct objects and their motion flows from
4D point clouds. The key insight is that simultaneously performing both tasks
via learning spatial and temporal features from a sequence of point clouds can
leverage individual tasks, leading to improved overall performance. To prove
this ability, we design a temporal vector field learning module using
unsupervised learning approach for flow estimation, leveraged by supervised
learning of spatial structures for object reconstruction. Extensive experiments
and analyses on benchmark dataset validated the effectiveness and efficiency of
our method. As shown in experimental results, our method achieves
state-of-the-art performance on both flow estimation and object reconstruction
while performing much faster than existing methods in both training and
inference. Our code and data are available at
https://github.com/hkust-vgd/RFNet-4D
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring hand use in the home after cervical spinal cord injury using egocentric video. (arXiv:2203.16996v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16996">
<div class="article-summary-box-inner">
<span><p>Background: Egocentric video has recently emerged as a potential solution for
monitoring hand function in individuals living with tetraplegia in the
community, especially for its ability to detect functional use in the home
environment. Objective: To develop and validate a wearable vision-based system
for measuring hand use in the home among individuals living with tetraplegia.
Methods: Several deep learning algorithms for detecting functional hand-object
interactions were developed and compared. The most accurate algorithm was used
to extract measures of hand function from 65 hours of unscripted video recorded
at home by 20 participants with tetraplegia. These measures were: the
percentage of interaction time over total recording time (Perc); the average
duration of individual interactions (Dur); the number of interactions per hour
(Num). To demonstrate the clinical validity of the technology, egocentric
measures were correlated with validated clinical assessments of hand function
and independence (Graded Redefined Assessment of Strength, Sensibility and
Prehension - GRASSP, Upper Extremity Motor Score - UEMS, and Spinal Cord
Independent Measure - SCIM). Results: Hand-object interactions were
automatically detected with a median F1-score of 0.80 (0.67-0.87). Our results
demonstrated that higher UEMS and better prehension were related to greater
time spent interacting, whereas higher SCIM and better hand sensation resulted
in a higher number of interactions performed during the egocentric video
recordings. Conclusions: For the first time, measures of hand function
automatically estimated in an unconstrained environment in individuals with
tetraplegia have been validated against internationally accepted measures of
hand function. Future work will necessitate a formal evaluation of the
reliability and responsiveness of the egocentric-based performance measures for
hand use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unitail: Detecting, Reading, and Matching in Retail Scene. (arXiv:2204.00298v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00298">
<div class="article-summary-box-inner">
<span><p>To make full use of computer vision technology in stores, it is required to
consider the actual needs that fit the characteristics of the retail scene.
Pursuing this goal, we introduce the United Retail Datasets (Unitail), a
large-scale benchmark of basic visual tasks on products that challenges
algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped
instances annotated, the Unitail offers a detection dataset to align product
appearance better. Furthermore, it provides a gallery-style OCR dataset
containing 1454 product categories, 30k text regions, and 21k transcriptions to
enable robust reading on products and motivate enhanced product matching.
Besides benchmarking the datasets using various state-of-the-arts, we customize
a new detector for product detection and provide a simple OCR-based matching
solution that verifies its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and Retrieval. (arXiv:2204.00486v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00486">
<div class="article-summary-box-inner">
<span><p>Cognitive science has shown that humans perceive videos in terms of events
separated by the state changes of dominant subjects. State changes trigger new
events and are one of the most useful among the large amount of redundant
information perceived. However, previous research focuses on the overall
understanding of segments without evaluating the fine-grained status changes
inside. In this paper, we introduce a new dataset called Kinetic-GEB+. The
dataset consists of over 170k boundaries associated with captions describing
status changes in the generic events in 12K videos. Upon this new dataset, we
propose three tasks supporting the development of a more fine-grained, robust,
and human-like understanding of videos through status changes. We evaluate many
representative baselines in our dataset, where we also design a new TPD
(Temporal-based Pairwise Difference) Modeling method for visual difference and
achieve significant performance improvements. Besides, the results show there
are still formidable challenges for current methods in the utilization of
different granularities, representation of visual difference, and the accurate
localization of status changes. Further analysis shows that our dataset can
drive developing more powerful methods to understand status changes and thus
improve video level comprehension. The dataset is available at
https://github.com/Yuxuan-W/GEB-Plus
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DFNet: Enhance Absolute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00559">
<div class="article-summary-box-inner">
<span><p>We introduce a camera relocalization pipeline that combines absolute pose
regression (APR) and direct feature matching. By incorporating
exposure-adaptive novel view synthesis, our method successfully addresses
photometric distortions in outdoor environments that existing photometric-based
methods fail to handle. With domain-invariant feature matching, our solution
improves pose regression accuracy using semi-supervised learning on unlabeled
data. In particular, the pipeline consists of two components: Novel View
Synthesizer and DFNet. The former synthesizes novel views compensating for
changes in exposure and the latter regresses camera poses and extracts robust
features that close the domain gap between real images and synthetic ones.
Furthermore, we introduce an online synthetic data generation scheme. We show
that these approaches effectively enhance camera pose estimation both in indoor
and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by
outperforming existing single-image APR methods by as much as 56%, comparable
to 3D structure-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation. (arXiv:2204.00570v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00570">
<div class="article-summary-box-inner">
<span><p>We consider unsupervised domain adaptation (UDA), where labeled data from a
source domain (e.g., photographs) and unlabeled data from a target domain
(e.g., sketches) are used to learn a classifier for the target domain.
Conventional UDA methods (e.g., domain adversarial training) learn
domain-invariant features to improve generalization to the target domain. In
this paper, we show that contrastive pre-training, which learns features on
unlabeled source and target data and then fine-tunes on labeled source data, is
competitive with strong UDA methods. However, we find that contrastive
pre-training does not learn domain-invariant features, diverging from
conventional UDA intuitions. We show theoretically that contrastive
pre-training can learn features that vary subtantially across domains but still
generalize to the target domain, by disentangling domain and class information.
Our results suggest that domain invariance is not necessary for UDA. We
empirically validate our theory on benchmark vision datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An application of Pixel Interval Down-sampling (PID) for dense tiny microorganism counting on environmental microorganism images. (arXiv:2204.01341v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01341">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel pixel interval down-sampling network (PID-Net)
for dense tiny object (yeast cells) counting tasks with higher accuracy. The
PID-Net is an end-to-end convolutional neural network (CNN) model with an
encoder--decoder architecture. The pixel interval down-sampling operations are
concatenated with max-pooling operations to combine the sparse and dense
features. This addresses the limitation of contour conglutination of dense
objects while counting. The evaluation was conducted using classical
segmentation metrics (the Dice, Jaccard and Hausdorff distance) as well as
counting metrics. The experimental results show that the proposed PID-Net had
the best performance and potential for dense tiny object counting tasks, which
achieved 96.97\% counting accuracy on the dataset with 2448 yeast cell images.
By comparing with the state-of-the-art approaches, such as Attention U-Net,
Swin U-Net and Trans U-Net, the proposed PID-Net can segment dense tiny objects
with clearer boundaries and fewer incorrect debris, which shows the great
potential of PID-Net in the task of accurate counting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FOSTER: Feature Boosting and Compression for Class-Incremental Learning. (arXiv:2204.04662v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04662">
<div class="article-summary-box-inner">
<span><p>The ability to learn new concepts continually is necessary in this
ever-changing world. However, deep neural networks suffer from catastrophic
forgetting when learning new categories. Many works have been proposed to
alleviate this phenomenon, whereas most of them either fall into the
stability-plasticity dilemma or take too much computation or storage overhead.
Inspired by the gradient boosting algorithm to gradually fit the residuals
between the target model and the previous ensemble model, we propose a novel
two-stage learning paradigm FOSTER, empowering the model to learn new
categories adaptively. Specifically, we first dynamically expand new modules to
fit the residuals between the target and the output of the original model.
Next, we remove redundant parameters and feature dimensions through an
effective distillation strategy to maintain the single backbone model. We
validate our method FOSTER on CIFAR-100 and ImageNet-100/1000 under different
settings. Experimental results show that our method achieves state-of-the-art
performance. Code is available at: https://github.com/G-U-N/ECCV22-FOSTER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Baselines for Image Restoration. (arXiv:2204.04676v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04676">
<div class="article-summary-box-inner">
<span><p>Although there have been significant advances in the field of image
restoration recently, the system complexity of the state-of-the-art (SOTA)
methods is increasing as well, which may hinder the convenient analysis and
comparison of methods. In this paper, we propose a simple baseline that exceeds
the SOTA methods and is computationally efficient. To further simplify the
baseline, we reveal that the nonlinear activation functions, e.g. Sigmoid,
ReLU, GELU, Softmax, etc. are not necessary: they could be replaced by
multiplication or removed. Thus, we derive a Nonlinear Activation Free Network,
namely NAFNet, from the baseline. SOTA results are achieved on various
challenging benchmarks, e.g. 33.69 dB PSNR on GoPro (for image deblurring),
exceeding the previous SOTA 0.38 dB with only 8.4% of its computational costs;
40.30 dB PSNR on SIDD (for image denoising), exceeding the previous SOTA 0.28
dB with less than half of its computational costs. The code and the pre-trained
models are released at https://github.com/megvii-research/NAFNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v10 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06718">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. The image convolution operation
helps CNNs to get good performance on image-related tasks. However, the image
convolution has high computation complexity and hard to be implemented. This
paper proposes the CEMNet, which can be trained in the frequency domain. The
most important motivation of this research is that we can use the
straightforward element-wise multiplication operation to replace the image
convolution in the frequency domain based on the Cross-Correlation Theorem,
which obviously reduces the computation complexity. We further introduce a
Weight Fixation mechanism to alleviate the problem of over-fitting, and analyze
the working behavior of Batch Normalization, Leaky ReLU, and Dropout in the
frequency domain to design their counterparts for CEMNet. Also, to deal with
complex inputs brought by Discrete Fourier Transform, we design a two-branches
network structure for CEMNet. Experimental results imply that CEMNet achieves
good performance on MNIST and CIFAR-10 databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. (arXiv:2204.13170v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13170">
<div class="article-summary-box-inner">
<span><p>In Federated Learning (FL), a number of clients or devices collaborate to
train a model without sharing their data. Models are optimized locally at each
client and further communicated to a central hub for aggregation. While FL is
an appealing decentralized training paradigm, heterogeneity among data from
different clients can cause the local optimization to drift away from the
global objective. In order to estimate and therefore remove this drift,
variance reduction techniques have been incorporated into FL optimization
recently. However, these approaches inaccurately estimate the clients' drift
and ultimately fail to remove it properly. In this work, we propose an adaptive
algorithm that accurately estimates drift across clients. In comparison to
previous works, our approach necessitates less storage and communication
bandwidth, as well as lower compute costs. Additionally, our proposed
methodology induces stability by constraining the norm of estimates for client
drift, making it more practical for large scale FL. Experimental findings
demonstrate that the proposed algorithm converges significantly faster and
achieves higher accuracy than the baselines across various FL benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2205.06230v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06230">
<div class="article-summary-box-inner">
<span><p>Combining simple architectures with large-scale pre-training has led to
massive improvements in image classification. For object detection,
pre-training and scaling approaches are less well established, especially in
the long-tailed and open-vocabulary setting, where training data is relatively
scarce. In this paper, we propose a strong recipe for transferring image-text
models to open-vocabulary object detection. We use a standard Vision
Transformer architecture with minimal modifications, contrastive image-text
pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling
properties of this setup shows that increasing image-level pre-training and
model size yield consistent improvements on the downstream detection task. We
provide the adaptation strategies and regularizations needed to attain very
strong performance on zero-shot text-conditioned and one-shot image-conditioned
object detection. Code and models are available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Vertex Descent: A New Direction for 3D Human Model Fitting. (arXiv:2205.06254v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06254">
<div class="article-summary-box-inner">
<span><p>We propose a novel optimization-based paradigm for 3D human model fitting on
images and scans. In contrast to existing approaches that directly regress the
parameters of a low-dimensional statistical body model (e.g. SMPL) from input
images, we train an ensemble of per-vertex neural fields network. The network
predicts, in a distributed manner, the vertex descent direction towards the
ground truth, based on neural features extracted at the current vertex
projection. At inference, we employ this network, dubbed LVD, within a
gradient-descent optimization pipeline until its convergence, which typically
occurs in a fraction of a second even when initializing all vertices into a
single point. An exhaustive evaluation demonstrates that our approach is able
to capture the underlying body of clothed people with very different body
shapes, achieving a significant improvement compared to state-of-the-art. LVD
is also applicable to 3D model fitting of humans and hands, for which we show a
significant improvement to the SOTA with a much simpler and faster method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-rigid Point Cloud Registration with Neural Deformation Pyramid. (arXiv:2205.12796v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12796">
<div class="article-summary-box-inner">
<span><p>Non-rigid point cloud registration is a key component in many computer vision
and computer graphics applications. The high complexity of the unknown
non-rigid motion make this task a challenging problem. In this paper, we break
down this problem via hierarchical motion decomposition. Our method called
Neural Deformation Pyramid (NDP) represents non-rigid motion using a pyramid
architecture. Each pyramid level, denoted by a Multi-Layer Perception (MLP),
takes as input a sinusoidally encoded 3D point and outputs its motion
increments from the previous level. The sinusoidal function starts with a low
input frequency and gradually increases when the pyramid level goes down. This
allows a multi-level rigid to nonrigid motion decomposition and also speeds up
the solving by 50 times compared to the existing MLP-based approach. Our method
achieves advanced partialto-partial non-rigid point cloud registration results
on the 4DMatch/4DLoMatch benchmark under both no-learned and supervised
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Co-Training for Unsupervised Domain Adaptation of Semantic Segmentation Models. (arXiv:2205.15781v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15781">
<div class="article-summary-box-inner">
<span><p>Semantic image segmentation is a central and challenging task in autonomous
driving, addressed by training deep models. Since this training draws to a
curse of human-based image labeling, using synthetic images with automatically
generated labels together with unlabeled real-world images is a promising
alternative. This implies to address an unsupervised domain adaptation (UDA)
problem. In this paper, we propose a new co-training procedure for
synth-to-real UDA of semantic segmentation models. It consists of a
self-training stage, which provides two domain-adapted models, and a model
collaboration loop for the mutual improvement of these two models. These models
are then used to provide the final semantic segmentation labels (pseudo-labels)
for the real-world images. The overall procedure treats the deep models as
black boxes and drives their collaboration at the level of pseudo-labeled
target images, i.e., neither modifying loss functions is required, nor explicit
feature alignment. We test our proposal on standard synthetic and real-world
datasets for on-board semantic segmentation. Our procedure shows improvements
ranging from ~13 to ~26 mIoU points over baselines, so establishing new
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection. (arXiv:2206.07458v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07458">
<div class="article-summary-box-inner">
<span><p>The goal of this work is to reconstruct speech from a silent talking face
video. Recent studies have shown impressive performance on synthesizing speech
from silent talking face videos. However, they have not explicitly considered
on varying identity characteristics of different speakers, which place a
challenge in the video-to-speech synthesis, and this becomes more critical in
unseen-speaker settings. Our approach is to separate the speech content and the
visage-style from a given silent talking face video. By guiding the model to
independently focus on modeling the two representations, we can obtain the
speech of high intelligibility from the model even when the input video of an
unseen subject is given. To this end, we introduce speech-visage selection that
separates the speech content and the speaker identity from the visual features
of the input video. The disentangled representations are jointly incorporated
to synthesize speech through visage-style based synthesizer which generates
speech by coating the visage-styles while maintaining the speech content. Thus,
the proposed framework brings the advantage of synthesizing the speech
containing the right content even with the silent talking face video of an
unseen subject. We validate the effectiveness of the proposed framework on the
GRID, TCD-TIMIT volunteer, and LRW datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diversified Adversarial Attacks based on Conjugate Gradient Method. (arXiv:2206.09628v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09628">
<div class="article-summary-box-inner">
<span><p>Deep learning models are vulnerable to adversarial examples, and adversarial
attacks used to generate such examples have attracted considerable research
interest. Although existing methods based on the steepest descent have achieved
high attack success rates, ill-conditioned problems occasionally reduce their
performance. To address this limitation, we utilize the conjugate gradient (CG)
method, which is effective for this type of problem, and propose a novel attack
algorithm inspired by the CG method, named the Auto Conjugate Gradient (ACG)
attack. The results of large-scale evaluation experiments conducted on the
latest robust models show that, for most models, ACG was able to find more
adversarial examples with fewer iterations than the existing SOTA algorithm
Auto-PGD (APGD). We investigated the difference in search performance between
ACG and APGD in terms of diversification and intensification, and define a
measure called Diversity Index (DI) to quantify the degree of diversity. From
the analysis of the diversity using this index, we show that the more diverse
search of the proposed method remarkably improves its attack success rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions. (arXiv:2206.14180v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14180">
<div class="article-summary-box-inner">
<span><p>Image-based virtual try-on aims to synthesize an image of a person wearing a
given clothing item. To solve the task, the existing methods warp the clothing
item to fit the person's body and generate the segmentation map of the person
wearing the item before fusing the item with the person. However, when the
warping and the segmentation generation stages operate individually without
information exchange, the misalignment between the warped clothes and the
segmentation map occurs, which leads to the artifacts in the final image. The
information disconnection also causes excessive warping near the clothing
regions occluded by the body parts, so-called pixel-squeezing artifacts. To
settle the issues, we propose a novel try-on condition generator as a unified
module of the two stages (i.e., warping and segmentation generation stages). A
newly proposed feature fusion block in the condition generator implements the
information exchange, and the condition generator does not create any
misalignment or pixel-squeezing artifacts. We also introduce discriminator
rejection that filters out the incorrect segmentation map predictions and
assures the performance of virtual try-on frameworks. Experiments on a
high-resolution dataset demonstrate that our model successfully handles the
misalignment and occlusion, and significantly outperforms the baselines. Code
is available at https://github.com/sangyun884/HR-VITON.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NARRATE: A Normal Assisted Free-View Portrait Stylizer. (arXiv:2207.00974v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00974">
<div class="article-summary-box-inner">
<span><p>In this work, we propose NARRATE, a novel pipeline that enables
simultaneously editing portrait lighting and perspective in a photorealistic
manner. As a hybrid neural-physical face model, NARRATE leverages complementary
benefits of geometry-aware generative approaches and normal-assisted physical
face models. In a nutshell, NARRATE first inverts the input portrait to a
coarse geometry and employs neural rendering to generate images resembling the
input, as well as producing convincing pose changes. However, inversion step
introduces mismatch, bringing low-quality images with less facial details. As
such, we further estimate portrait normal to enhance the coarse geometry,
creating a high-fidelity physical face model. In particular, we fuse the neural
and physical renderings to compensate for the imperfect inversion, resulting in
both realistic and view-consistent novel perspective images. In relighting
stage, previous works focus on single view portrait relighting but ignoring
consistency between different perspectives as well, leading unstable and
inconsistent lighting effects for view changes. We extend Total Relighting to
fix this problem by unifying its multi-view input normal maps with the physical
face model. NARRATE conducts relighting with consistent normal maps, imposing
cross-view constraints and exhibiting stable and coherent illumination effects.
We experimentally demonstrate that NARRATE achieves more photorealistic,
reliable results over prior works. We further bridge NARRATE with animation and
style transfer tools, supporting pose change, light change, facial animation,
and style transfer, either separately or in combination, all at a photographic
quality. We showcase vivid free-view facial animations as well as 3D-aware
relightable stylization, which help facilitate various AR/VR applications like
virtual cinematography, 3D video conferencing, and post-production.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harmonizer: Learning to Perform White-Box Image and Video Harmonization. (arXiv:2207.01322v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01322">
<div class="article-summary-box-inner">
<span><p>Recent works on image harmonization solve the problem as a pixel-wise image
translation task via large autoencoders. They have unsatisfactory performances
and slow inference speeds when dealing with high-resolution images. In this
work, we observe that adjusting the input arguments of basic image filters,
e.g., brightness and contrast, is sufficient for humans to produce realistic
images from the composite ones. Hence, we frame image harmonization as an
image-level regression problem to learn the arguments of the filters that
humans use for the task. We present a Harmonizer framework for image
harmonization. Unlike prior methods that are based on black-box autoencoders,
Harmonizer contains a neural network for filter argument prediction and several
white-box filters (based on the predicted arguments) for image harmonization.
We also introduce a cascade regressor and a dynamic loss strategy for
Harmonizer to learn filter arguments more stably and precisely. Since our
network only outputs image-level arguments and the filters we used are
efficient, Harmonizer is much lighter and faster than existing methods.
Comprehensive experiments demonstrate that Harmonizer surpasses existing
methods notably, especially with high-resolution inputs. Finally, we apply
Harmonizer to video harmonization, which achieves consistent results across
frames and 56 fps at 1080P resolution. Code and models are available at:
https://github.com/ZHKKKe/Harmonizer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphVid: It Only Takes a Few Nodes to Understand a Video. (arXiv:2207.01375v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01375">
<div class="article-summary-box-inner">
<span><p>We propose a concise representation of videos that encode perceptually
meaningful features into graphs. With this representation, we aim to leverage
the large amount of redundancies in videos and save computations. First, we
construct superpixel-based graph representations of videos by considering
superpixels as graph nodes and create spatial and temporal connections between
adjacent superpixels. Then, we leverage Graph Convolutional Networks to process
this representation and predict the desired output. As a result, we are able to
train models with much fewer parameters, which translates into short training
periods and a reduction in computation resource requirements. A comprehensive
experimental study on the publicly available datasets Kinetics-400 and Charades
shows that the proposed method is highly cost-effective and uses limited
commodity hardware during training and inference. It reduces the computational
requirements 10-fold while achieving results that are comparable to
state-of-the-art methods. We believe that the proposed approach is a promising
direction that could open the door to solving video understanding more
efficiently and enable more resource limited users to thrive in this research
field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Self-supervised Learning for Video Understanding. (arXiv:2207.01975v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01975">
<div class="article-summary-box-inner">
<span><p>The ubiquity of camera-enabled mobile devices has lead to large amounts of
unlabelled video data being produced at the edge. Although various
self-supervised learning (SSL) methods have been proposed to harvest their
latent spatio-temporal representations for task-specific training, practical
challenges including privacy concerns and communication costs prevent SSL from
being deployed at large scales. To mitigate these issues, we propose the use of
Federated Learning (FL) to the task of video SSL. In this work, we evaluate the
performance of current state-of-the-art (SOTA) video-SSL techniques and
identify their shortcomings when integrated into the large-scale FL setting
simulated with kinetics-400 dataset. We follow by proposing a novel federated
SSL framework for video, dubbed FedVSSL, that integrates different aggregation
strategies and partial weight updating. Extensive experiments demonstrate the
effectiveness and significance of FedVSSL as it outperforms the centralized
SOTA for the downstream retrieval task by 6.66% on UCF-101 and 5.13% on
HMDB-51.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DG-STFM: 3D Geometric Guided Student-Teacher Feature Matching. (arXiv:2207.02375v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02375">
<div class="article-summary-box-inner">
<span><p>We tackle the essential task of finding dense visual correspondences between
a pair of images. This is a challenging problem due to various factors such as
poor texture, repetitive patterns, illumination variation, and motion blur in
practical scenarios. In contrast to methods that use dense correspondence
ground-truths as direct supervision for local feature matching training, we
train 3DG-STFM: a multi-modal matching model (Teacher) to enforce the depth
consistency under 3D dense correspondence supervision and transfer the
knowledge to 2D unimodal matching model (Student). Both teacher and student
models consist of two transformer-based matching modules that obtain dense
correspondences in a coarse-to-fine manner. The teacher model guides the
student model to learn RGB-induced depth information for the matching purpose
on both coarse and fine branches. We also evaluate 3DG-STFM on a model
compression task. To the best of our knowledge, 3DG-STFM is the first
student-teacher learning method for the local feature matching task. The
experiments show that our method outperforms state-of-the-art methods on indoor
and outdoor camera pose estimations, and homography estimation problems. Code
is available at: https://github.com/Ryan-prime/3DG-STFM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation. (arXiv:2207.02466v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02466">
<div class="article-summary-box-inner">
<span><p>The inherent ambiguity in ground-truth annotations of 3D bounding boxes
caused by occlusions, signal missing, or manual annotation errors can confuse
deep 3D object detectors during training, thus deteriorating the detection
accuracy. However, existing methods overlook such issues to some extent and
treat the labels as deterministic. In this paper, we formulate the label
uncertainty problem as the diversity of potentially plausible bounding boxes of
objects, then propose GLENet, a generative framework adapted from conditional
variational autoencoders, to model the one-to-many relationship between a
typical 3D object and its potential ground-truth bounding boxes with latent
variables. The label uncertainty generated by GLENet is a plug-and-play module
and can be conveniently integrated into existing deep 3D detectors to build
probabilistic detectors and supervise the learning of the localization
uncertainty. Besides, we propose an uncertainty-aware quality estimator
architecture in probabilistic detectors to guide the training of IoU-branch
with predicted localization uncertainty. We incorporate the proposed methods
into various popular base 3D detectors and demonstrate significant and
consistent performance gains on both KITTI and Waymo benchmark datasets.
Especially, the proposed GLENet-VR outperforms all published LiDAR-based
approaches by a large margin and ranks $1^{st}$ among single-modal methods on
the challenging KITTI test set. We will make the source code and pre-trained
models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Scale-Aware, Robust, and Generalizable Unsupervised Monocular Depth Estimation by Integrating IMU Motion Dynamics. (arXiv:2207.04680v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04680">
<div class="article-summary-box-inner">
<span><p>Unsupervised monocular depth and ego-motion estimation has drawn extensive
research attention in recent years. Although current methods have reached a
high up-to-scale accuracy, they usually fail to learn the true scale metric due
to the inherent scale ambiguity from training with monocular sequences. In this
work, we tackle this problem and propose DynaDepth, a novel scale-aware
framework that integrates information from vision and IMU motion dynamics.
Specifically, we first propose an IMU photometric loss and a cross-sensor
photometric consistency loss to provide dense supervision and absolute scales.
To fully exploit the complementary information from both sensors, we further
drive a differentiable camera-centric extended Kalman filter (EKF) to update
the IMU preintegrated motions when observing visual measurements. In addition,
the EKF formulation enables learning an ego-motion uncertainty measure, which
is non-trivial for unsupervised methods. By leveraging IMU during training,
DynaDepth not only learns an absolute scale, but also provides a better
generalization ability and robustness against vision degradation such as
illumination change and moving objects. We validate the effectiveness of
DynaDepth by conducting extensive experiments and simulations on the KITTI and
Make3D datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DCCF: Deep Comprehensible Color Filter Learning Framework for High-Resolution Image Harmonization. (arXiv:2207.04788v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04788">
<div class="article-summary-box-inner">
<span><p>Image color harmonization algorithm aims to automatically match the color
distribution of foreground and background images captured in different
conditions. Previous deep learning based models neglect two issues that are
critical for practical applications, namely high resolution (HR) image
processing and model comprehensibility. In this paper, we propose a novel Deep
Comprehensible Color Filter (DCCF) learning framework for high-resolution image
harmonization. Specifically, DCCF first downsamples the original input image to
its low-resolution (LR) counter-part, then learns four human comprehensible
neural filters (i.e. hue, saturation, value and attentive rendering filters) in
an end-to-end manner, finally applies these filters to the original input image
to get the harmonized result. Benefiting from the comprehensible neural
filters, we could provide a simple yet efficient handler for users to cooperate
with deep model to get the desired results with very little effort when
necessary. Extensive experiments demonstrate the effectiveness of DCCF learning
framework and it outperforms state-of-the-art post-processing method on
iHarmony4 dataset on images' full-resolutions by achieving 7.63% and 1.69%
relative improvements on MSE and PSNR respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Category-Level 6D Object Pose and Size Estimation using Self-Supervised Deep Prior Deformation Networks. (arXiv:2207.05444v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05444">
<div class="article-summary-box-inner">
<span><p>It is difficult to precisely annotate object instances and their semantics in
3D space, and as such, synthetic data are extensively used for these tasks,
e.g., category-level 6D object pose and size estimation. However, the easy
annotations in synthetic domains bring the downside effect of synthetic-to-real
(Sim2Real) domain gap. In this work, we aim to address this issue in the task
setting of Sim2Real, unsupervised domain adaptation for category-level 6D
object pose and size estimation. We propose a method that is built upon a novel
Deep Prior Deformation Network, shortened as DPDN. DPDN learns to deform
features of categorical shape priors to match those of object observations, and
is thus able to establish deep correspondence in the feature space for direct
regression of object poses and sizes. To reduce the Sim2Real domain gap, we
formulate a novel self-supervised objective upon DPDN via consistency learning;
more specifically, we apply two rigid transformations to each object
observation in parallel, and feed them into DPDN respectively to yield dual
sets of predictions; on top of the parallel learning, an inter-consistency term
is employed to keep cross consistency between dual predictions for improving
the sensitivity of DPDN to pose changes, while individual intra-consistency
ones are used to enforce self-adaptation within each learning itself. We train
DPDN on both training sets of the synthetic CAMERA25 and real-world REAL275
datasets; our results outperform the existing methods on REAL275 test set under
both the unsupervised and supervised settings. Ablation studies also verify the
efficacy of our designs. Our code is released publicly at
https://github.com/JiehongLin/Self-DPDN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliminating Gradient Conflict in Reference-based Line-Art Colorization. (arXiv:2207.06095v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06095">
<div class="article-summary-box-inner">
<span><p>Reference-based line-art colorization is a challenging task in computer
vision. The color, texture, and shading are rendered based on an abstract
sketch, which heavily relies on the precise long-range dependency modeling
between the sketch and reference. Popular techniques to bridge the cross-modal
information and model the long-range dependency employ the attention mechanism.
However, in the context of reference-based line-art colorization, several
techniques would intensify the existing training difficulty of attention, for
instance, self-supervised training protocol and GAN-based losses. To understand
the instability in training, we detect the gradient flow of attention and
observe gradient conflict among attention branches. This phenomenon motivates
us to alleviate the gradient issue by preserving the dominant gradient branch
while removing the conflict ones. We propose a novel attention mechanism using
this training strategy, Stop-Gradient Attention (SGA), outperforming the
attention baseline by a large margin with better training stability. Compared
with state-of-the-art modules in line-art colorization, our approach
demonstrates significant improvements in Fr\'echet Inception Distance (FID, up
to 27.21%) and structural similarity index measure (SSIM, up to 25.67%) on
several benchmarks. The code of SGA is available at
https://github.com/kunkun0w0/SGA .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransGrasp: Grasp Pose Estimation of a Category of Objects by Transferring Grasps from Only One Labeled Instance. (arXiv:2207.07861v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07861">
<div class="article-summary-box-inner">
<span><p>Grasp pose estimation is an important issue for robots to interact with the
real world. However, most of existing methods require exact 3D object models
available beforehand or a large amount of grasp annotations for training. To
avoid these problems, we propose TransGrasp, a category-level grasp pose
estimation method that predicts grasp poses of a category of objects by
labeling only one object instance. Specifically, we perform grasp pose transfer
across a category of objects based on their shape correspondences and propose a
grasp pose refinement module to further fine-tune grasp pose of grippers so as
to ensure successful grasps. Experiments demonstrate the effectiveness of our
method on achieving high-quality grasps with the transferred grasp poses. Our
code is available at https://github.com/yanjh97/TransGrasp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras. (arXiv:2207.08000v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08000">
<div class="article-summary-box-inner">
<span><p>We propose DiffuStereo, a novel system using only sparse cameras (8 in this
work) for high-quality 3D human reconstruction. At its core is a novel
diffusion-based stereo module, which introduces diffusion models, a type of
powerful generative models, into the iterative stereo matching network. To this
end, we design a new diffusion kernel and additional stereo constraints to
facilitate stereo matching and depth estimation in the network. We further
present a multi-level stereo network architecture to handle high-resolution (up
to 4k) inputs without requiring unaffordable memory footprint. Given a set of
sparse-view color images of a human, the proposed multi-level diffusion-based
stereo network can produce highly accurate depth maps, which are then converted
into a high-quality 3D human model through an efficient multi-view fusion
strategy. Overall, our method enables automatic reconstruction of human models
with quality on par to high-end dense-view camera rigs, and this is achieved
using a much more light-weight hardware setup. Experiments show that our method
outperforms state-of-the-art methods by a large margin both qualitatively and
quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExAgt: Expert-guided Augmentation for Representation Learning of Traffic Scenarios. (arXiv:2207.08609v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08609">
<div class="article-summary-box-inner">
<span><p>Representation learning in recent years has been addressed with
self-supervised learning methods. The input data is augmented into two
distorted views and an encoder learns the representations that are invariant to
distortions -- cross-view prediction. Augmentation is one of the key components
in cross-view self-supervised learning frameworks to learn visual
representations. This paper presents ExAgt, a novel method to include expert
knowledge for augmenting traffic scenarios, to improve the learnt
representations without any human annotation. The expert-guided augmentations
are generated in an automated fashion based on the infrastructure, the
interactions between the EGO and the traffic participants and an ideal sensor
model. The ExAgt method is applied in two state-of-the-art cross-view
prediction methods and the representations learnt are tested in downstream
tasks like classification and clustering. Results show that the ExAgt method
improves representation learning compared to using only standard augmentations
and it provides a better representation space stability. The code is available
at https://github.com/lab176344/ExAgt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance-Aware Observer Network for Out-of-Distribution Object Segmentation. (arXiv:2207.08782v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08782">
<div class="article-summary-box-inner">
<span><p>Recent work on Observer Network has shown promising results on
Out-Of-Distribution (OOD) detection for semantic segmentation. These methods
have difficulty in precisely locating the point of interest in the image, i.e,
the anomaly. This limitation is due to the difficulty of fine-grained
prediction at the pixel level. To address this issue, we provide instance
knowledge to the observer. We extend the approach of ObsNet by harnessing an
instance-wise mask prediction. We use an additional, class agnostic, object
detector to filter and aggregate observer predictions. Finally, we predict an
unique anomaly score for each instance in the image. We show that our proposed
method accurately disentangle in-distribution objects from Out-Of-Distribution
objects on three datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">eCDT: Event Clustering for Simultaneous Feature Detection and Tracking-. (arXiv:2207.09108v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09108">
<div class="article-summary-box-inner">
<span><p>Contrary to other standard cameras, event cameras interpret the world in an
entirely different manner; as a collection of asynchronous events. Despite
event camera's unique data output, many event feature detection and tracking
algorithms have shown significant progress by making detours to frame-based
data representations. This paper questions the need to do so and proposes a
novel event data-friendly method that achieve simultaneous feature detection
and tracking, called event Clustering-based Detection and Tracking (eCDT). Our
method employs a novel clustering method, named as k-NN Classifier-based
Spatial Clustering and Applications with Noise (KCSCAN), to cluster adjacent
polarity events to retrieve event trajectories.With the aid of a Head and Tail
Descriptor Matching process, event clusters that reappear in a different
polarity are continually tracked, elongating the feature tracks. Thanks to our
clustering approach in spatio-temporal space, our method automatically solves
feature detection and feature tracking simultaneously. Also, eCDT can extract
feature tracks at any frequency with an adjustable time window, which does not
corrupt the high temporal resolution of the original event data. Our method
achieves 30% better feature tracking ages compared with the state-of-the-art
approach while also having a low error approximately equal to it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expert-LaSTS: Expert-Knowledge Guided Latent Space for Traffic Scenarios. (arXiv:2207.09120v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09120">
<div class="article-summary-box-inner">
<span><p>Clustering traffic scenarios and detecting novel scenario types are required
for scenario-based testing of autonomous vehicles. These tasks benefit from
either good similarity measures or good representations for the traffic
scenarios. In this work, an expert-knowledge aided representation learning for
traffic scenarios is presented. The latent space so formed is used for
successful clustering and novel scenario type detection. Expert-knowledge is
used to define objectives that the latent representations of traffic scenarios
shall fulfill. It is presented, how the network architecture and loss is
designed from these objectives, thereby incorporating expert-knowledge. An
automatic mining strategy for traffic scenarios is presented, such that no
manual labeling is required. Results show the performance advantage compared to
baseline methods. Additionally, extensive analysis of the latent space is
performed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Stop Learning: Towards Continual Learning for the CLIP Model. (arXiv:2207.09248v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09248">
<div class="article-summary-box-inner">
<span><p>The Contrastive Language-Image Pre-training (CLIP) Model is a recently
proposed large-scale pre-train model which attracts increasing attention in the
computer vision community. Benefiting from its gigantic image-text training
set, the CLIP model has learned outstanding capabilities in zero-shot learning
and image-text matching. To boost the recognition performance of CLIP on some
target visual concepts, it is often desirable to further update the CLIP model
by fine-tuning some classes-of-interest on extra training data. This operation,
however, raises an important concern: will the update hurt the zero-shot
learning or image-text matching capability of the CLIP, i.e., the catastrophic
forgetting issue? If yes, could existing continual learning algorithms be
adapted to alleviate the risk of catastrophic forgetting? To answer these
questions, this work conducts a systemic study on the continual learning issue
of the CLIP model. We construct evaluation protocols to measure the impact of
fine-tuning updates and explore different ways to upgrade existing continual
learning methods to mitigate the forgetting issue of the CLIP model. Our study
reveals the particular challenges of CLIP continual learning problem and lays a
foundation for further researches. Moreover, we propose a new algorithm, dubbed
Learning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact
effectiveness for alleviating the forgetting issue of the CLIP model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking IoU-based Optimization for Single-stage 3D Object Detection. (arXiv:2207.09332v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09332">
<div class="article-summary-box-inner">
<span><p>Since Intersection-over-Union (IoU) based optimization maintains the
consistency of the final IoU prediction metric and losses, it has been widely
used in both regression and classification branches of single-stage 2D object
detectors. Recently, several 3D object detection methods adopt IoU-based
optimization and directly replace the 2D IoU with 3D IoU. However, such a
direct computation in 3D is very costly due to the complex implementation and
inefficient backward operations. Moreover, 3D IoU-based optimization is
sub-optimal as it is sensitive to rotation and thus can cause training
instability and detection performance deterioration. In this paper, we propose
a novel Rotation-Decoupled IoU (RDIoU) method that can mitigate the
rotation-sensitivity issue, and produce more efficient optimization objectives
compared with 3D IoU during the training stage. Specifically, our RDIoU
simplifies the complex interactions of regression parameters by decoupling the
rotation variable as an independent term, yet preserving the geometry of 3D
IoU. By incorporating RDIoU into both the regression and classification
branches, the network is encouraged to learn more precise bounding boxes and
concurrently overcome the misalignment issue between classification and
regression. Extensive experiments on the benchmark KITTI and Waymo Open Dataset
validate that our RDIoU method can bring substantial improvement for the
single-stage 3D object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space. (arXiv:2206.11895v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11895">
<div class="article-summary-box-inner">
<span><p>Humans are remarkably flexible in understanding viewpoint changes due to
visual cortex supporting the perception of 3D structure. In contrast, most of
the computer vision models that learn visual representation from a pool of 2D
images often fail to generalize over novel camera viewpoints. Recently, the
vision architectures have shifted towards convolution-free architectures,
visual Transformers, which operate on tokens derived from image patches.
However, neither these Transformers nor 2D convolutional networks perform
explicit operations to learn viewpoint-agnostic representation for visual
understanding. To this end, we propose a 3D Token Representation Layer (3DTRL)
that estimates the 3D positional information of the visual tokens and leverages
it for learning viewpoint-agnostic representations. The key elements of 3DTRL
include a pseudo-depth estimator and a learned camera matrix to impose
geometric transformations on the tokens. These enable 3DTRL to recover the 3D
positional information of the tokens from 2D patches. In practice, 3DTRL is
easily plugged-in into a Transformer. Our experiments demonstrate the
effectiveness of 3DTRL in many vision tasks including image classification,
multi-view video alignment, and action recognition. The models with 3DTRL
outperform their backbone Transformers in all the tasks with minimal added
computation. Our project page is at
https://www3.cs.stonybrook.edu/~jishang/3dtrl/3dtrl.html
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-21 23:09:27.347743631 UTC">2022-07-21 23:09:27 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>