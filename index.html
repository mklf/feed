<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-24T01:30:00Z">02-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Generation of Perspective API: Efficient Multilingual Character-level Transformers. (arXiv:2202.11176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11176">
<div class="article-summary-box-inner">
<span><p>On the world wide web, toxic content detectors are a crucial line of defense
against potentially hateful and offensive messages. As such, building highly
effective classifiers that enable a safer internet is an important research
area. Moreover, the web is a highly multilingual, cross-cultural community that
develops its own lingo over time. As such, it is crucial to develop models that
are effective across a diverse range of languages, usages, and styles. In this
paper, we present the fundamentals behind the next version of the Perspective
API from Google Jigsaw. At the heart of the approach is a single multilingual
token-free Charformer model that is applicable across a range of languages,
domains, and tasks. We demonstrate that by forgoing static vocabularies, we
gain flexibility across a variety of settings. We additionally outline the
techniques employed to make such a byte-level model efficient and feasible for
productionization. Through extensive experiments on multilingual toxic comment
classification benchmarks derived from real API traffic and evaluation on an
array of code-switching, covert toxicity, emoji-based hate, human-readable
obfuscation, distribution shift, and bias evaluation settings, we show that our
proposed approach outperforms strong baselines. Finally, we present our
findings from deploying this system in production.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Augmented BERT Mutual Network in Multi-turn Spoken Dialogues. (arXiv:2202.11299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11299">
<div class="article-summary-box-inner">
<span><p>Modern spoken language understanding (SLU) systems rely on sophisticated
semantic notions revealed in single utterances to detect intents and slots.
However, they lack the capability of modeling multi-turn dynamics within a
dialogue particularly in long-term slot contexts. Without external knowledge,
depending on limited linguistic legitimacy within a word sequence may overlook
deep semantic information across dialogue turns. In this paper, we propose to
equip a BERT-based joint model with a knowledge attention module to mutually
leverage dialogue contexts between two SLU tasks. A gating mechanism is further
utilized to filter out irrelevant knowledge triples and to circumvent
distracting comprehension. Experimental results in two complicated multi-turn
dialogue datasets have demonstrate by mutually modeling two SLU tasks with
filtered knowledge and dialogue contexts, our approach has considerable
improvements compared with several competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Commonsense Reasoning for Identifying and Understanding the Implicit Need of Help and Synthesizing Assistive Actions. (arXiv:2202.11337v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11337">
<div class="article-summary-box-inner">
<span><p>Human-Robot Interaction (HRI) is an emerging subfield of service robotics.
While most existing approaches rely on explicit signals (i.e. voice, gesture)
to engage, current literature is lacking solutions to address implicit user
needs. In this paper, we present an architecture to (a) detect user implicit
need of help and (b) generate a set of assistive actions without prior
learning. Task (a) will be performed using state-of-the-art solutions for Scene
Graph Generation coupled to the use of commonsense knowledge; whereas, task (b)
will be performed using additional commonsense knowledge as well as a sentiment
analysis on graph structure. Finally, we propose an evaluation of our solution
using established benchmarks (e.g. ActionGenome dataset) along with human
experiments. The main motivation of our approach is the embedding of the
perception-decision-action loop in a single architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Learning for Short Text Classification. (arXiv:2202.11345v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11345">
<div class="article-summary-box-inner">
<span><p>In the short text, the extreme short length, feature sparsity and high
ambiguity pose huge challenge to classification tasks. Recently, as an
effective method for tuning Pre-trained Language Models for specific downstream
tasks, prompt-learning has attracted vast amount of attention and research. The
main intuition behind the prompt-learning is to insert template into the input
and convert the text classification tasks into equivalent cloze-style tasks.
However, most prompt-learning methods expand label words manually or only
consider the class name for knowledge incorporating in cloze-style prediction,
which will inevitably incurred omissions and bias in classification tasks. In
this paper, we propose a simple short text classification approach that makes
use of prompt-learning based on knowledgeable expansion, which can consider
both the short text itself and class name during expanding label words space.
Specifically, the top $N$ concepts related to the entity in short text are
retrieved from the open Knowledge Graph like Probase, and we further refine the
expanded label words by the distance calculation between selected concepts and
class label. Experimental results show that our approach obtains obvious
improvement compared with other fine-tuning, prompt-learning and knowledgeable
prompt-tuning methods, outperforming the state-of-the-art by up to 6 Accuracy
points on three well-known datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling arbitrary translation objectives with Adaptive Tree Search. (arXiv:2202.11444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11444">
<div class="article-summary-box-inner">
<span><p>We introduce an adaptive tree search algorithm, that can find high-scoring
outputs under translation models that make no assumptions about the form or
structure of the search objective. This algorithm -- a deterministic variant of
Monte Carlo tree search -- enables the exploration of new kinds of models that
are unencumbered by constraints imposed to make decoding tractable, such as
autoregressivity or conditional independence assumptions. When applied to
autoregressive models, our algorithm has different biases than beam search has,
which enables a new analysis of the role of decoding bias in autoregressive
models. Empirically, we show that our adaptive tree search algorithm finds
outputs with substantially better model scores compared to beam search in
autoregressive models, and compared to reranking techniques in models whose
scores do not decompose additively with respect to the words in the output. We
also characterise the correlation of several translation model objectives with
respect to BLEU. We find that while some standard models are poorly calibrated
and benefit from the beam search bias, other often more robust models
(autoregressive models tuned to maximize expected automatic metric scores, the
noisy channel model and a newly proposed objective) benefit from increasing
amounts of search using our proposed decoder, whereas the beam search bias
limits the improvements obtained from such objectives. Thus, we argue that as
models improve, the improvements may be masked by over-reliance on beam search
or reranking based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt. (arXiv:2202.11451v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11451">
<div class="article-summary-box-inner">
<span><p>Prompt-based tuning has been proven effective for pretrained language models
(PLMs). While most of the existing work focuses on the monolingual prompts, we
study the multilingual prompts for multilingual PLMs, especially in the
zero-shot cross-lingual setting. To alleviate the effort of designing different
prompts for multiple languages, we propose a novel model that uses a unified
prompt for all languages, called UniPrompt. Different from the discrete prompts
and soft prompts, the unified prompt is model-based and language-agnostic.
Specifically, the unified prompt is initialized by a multilingual PLM to
produce language-independent representation, after which is fused with the text
input. During inference, the prompts can be pre-computed so that no extra
computation cost is needed. To collocate with the unified prompt, we propose a
new initialization method for the target label word to further improve the
model's transferability across languages. Extensive experiments show that our
proposed methods can significantly outperform the strong baselines across
different languages. We will release data and code to facilitate future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Short-answer scoring with ensembles of pretrained language models. (arXiv:2202.11558v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11558">
<div class="article-summary-box-inner">
<span><p>We investigate the effectiveness of ensembles of pretrained transformer-based
language models on short answer questions using the Kaggle Automated Short
Answer Scoring dataset. We fine-tune a collection of popular small, base, and
large pretrained transformer-based language models, and train one feature-base
model on the dataset with the aim of testing ensembles of these models. We used
an early stopping mechanism and hyperparameter optimization in training. We
observe that generally that the larger models perform slightly better, however,
they still fall short of state-of-the-art results one their own. Once we
consider ensembles of models, there are ensembles of a number of large networks
that do produce state-of-the-art results, however, these ensembles are too
large to realistically be put in a production environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refining the state-of-the-art in Machine Translation, optimizing NMT for the JA <-> EN language pair by leveraging personal domain expertise. (arXiv:2202.11669v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11669">
<div class="article-summary-box-inner">
<span><p>Documenting the construction of an NMT (Neural Machine Translation) system
for En/Ja based on the Transformer architecture leveraging the OpenNMT
framework. A systematic exploration of corpora pre-processing, hyperparameter
tuning and model architecture is carried out to obtain optimal performance. The
system is evaluated using standard auto-evaluation metrics such as BLEU, and my
subjective opinion as a Japanese linguist.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation Social Network Dataset. (arXiv:2202.11684v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11684">
<div class="article-summary-box-inner">
<span><p>Misinformation is becoming increasingly prevalent on social media and in news
articles. It has become so widespread that we require algorithmic assistance
utilising machine learning to detect such content. Training these machine
learning models require datasets of sufficient scale, diversity and quality.
However, datasets in the field of automatic misinformation detection are
predominantly monolingual, include a limited amount of modalities and are not
of sufficient scale and quality. Addressing this, we develop a data collection
and linking system (MuMiN-trawl), to build a public misinformation graph
dataset (MuMiN), containing rich social media data (tweets, replies, users,
images, articles, hashtags) spanning 21 million tweets belonging to 26 thousand
Twitter threads, each of which have been semantically linked to 13 thousand
fact-checked claims across dozens of topics, events and domains, in 41
different languages, spanning more than a decade. The dataset is made available
as a heterogeneous graph via a Python package (mumin). We provide baseline
results for two node classification tasks related to the veracity of a claim
involving social media, and demonstrate that these are challenging tasks, with
the highest macro-average F1-score being 62.55% and 61.45% for the two tasks,
respectively. The MuMiN ecosystem is available at
https://mumin-dataset.github.io/, including the data, documentation, tutorials
and leaderboards.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics. (arXiv:2202.11705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11705">
<div class="article-summary-box-inner">
<span><p>Many applications of text generation require incorporating different
constraints to control the semantics or style of generated text. These
constraints can be hard (e.g., ensuring certain keywords are included in the
output) and soft (e.g., contextualizing the output with the left- or right-hand
context). In this paper, we present Energy-based Constrained Decoding with
Langevin Dynamics (COLD), a decoding framework which unifies constrained
generation as specifying constraints through an energy function, then
performing efficient differentiable reasoning over the constraints through
gradient-based sampling. COLD decoding is a flexible framework that can be
applied directly to off-the-shelf left-to-right language models without the
need for any task-specific fine-tuning, as demonstrated through three
challenging text generation applications: lexically-constrained generation,
abductive reasoning, and counterfactual reasoning. Our experiments on these
constrained generation tasks point to the effectiveness of our approach, both
in terms of automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Moments in Video Collections Using Natural Language. (arXiv:1907.12763v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.12763">
<div class="article-summary-box-inner">
<span><p>We introduce the task of retrieving relevant video moments from a large
corpus of untrimmed, unsegmented videos given a natural language query. Our
task poses unique challenges as a system must efficiently identify both the
relevant videos and localize the relevant moments in the videos. To address
these challenges, we propose SpatioTemporal Alignment with Language (STAL), a
model that represents a video moment as a set of regions within a series of
short video clips and aligns a natural language query to the moment's regions.
Our alignment cost compares variable-length language and video features using
symmetric squared Chamfer distance, which allows for efficient indexing and
retrieval of the video moments. Moreover, aligning language features to regions
within a video moment allows for finer alignment compared to methods that
extract only an aggregate feature from the entire video moment. We evaluate our
approach on two recently proposed datasets for temporal localization of moments
in video with natural language (DiDeMo and Charades-STA) extended to our video
corpus moment retrieval setting. We show that our STAL re-ranking model
outperforms the recently proposed Moment Context Network on all criteria across
all datasets on our proposed task, obtaining relative gains of 37% - 118% for
average recall and up to 30% for median rank. Moreover, our approach achieves
more than 130x faster retrieval and 8x smaller index size with a 1M video
corpus in an approximate setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-based Latent Personal Analysis and its use for impersonation detection in social media. (arXiv:2004.02346v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.02346">
<div class="article-summary-box-inner">
<span><p>Zipf's law defines an inverse proportion between a word's ranking in a given
corpus and its frequency in it, roughly dividing the vocabulary into frequent
words and infrequent ones. Here, we stipulate that within a domain an author's
signature can be derived from, in loose terms, the author's missing popular
words and frequently used infrequent-words. We devise a method, termed Latent
Personal Analysis (LPA), for finding domain-based attributes for entities in a
domain: their distance from the domain and their signature, which determines
how they most differ from a domain. We identify the most suitable distance
metric for the method among several and construct the distances and personal
signatures for authors, the domain's entities. The signature consists of both
over-used terms (compared to the average), and missing popular terms. We
validate the correctness and power of the signatures in identifying users and
set existence conditions. We then show uses for the method in explainable
authorship attribution: we define algorithms that utilize LPA to identify two
types of impersonation in social media: (1) authors with sockpuppets (multiple)
accounts; (2) front users accounts, operated by several authors. We validate
the algorithms and employ them over a large scale dataset obtained from a
social media site with over 4000 users. We corroborate these results using
temporal rate analysis. LPA can further be used to devise personal attributes
in a wide range of scientific domains in which the constituents have a
long-tail distribution of elements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing various Semantic Models for Amharic: Experimentation and Evaluation with multiple Tasks and Datasets. (arXiv:2011.01154v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.01154">
<div class="article-summary-box-inner">
<span><p>The availability of different pre-trained semantic models enabled the quick
development of machine learning components for downstream applications. Despite
the availability of abundant text data for low resource languages, only a few
semantic models are publicly available. Publicly available pre-trained models
are usually built as a multilingual version of semantic models that can not fit
well for each language due to context variations. In this work, we introduce
different semantic models for Amharic. After we experiment with the existing
pre-trained semantic models, we trained and fine-tuned nine new different
models using a monolingual text corpus. The models are build using word2Vec
embeddings, distributional thesaurus (DT), contextual embeddings, and DT
embeddings obtained via network embedding algorithms. Moreover, we employ these
models for different NLP tasks and investigate their impact. We find that newly
trained models perform better than pre-trained multilingual models.
Furthermore, models based on contextual embeddings from RoBERTA perform better
than the word2Vec models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A2P-MANN: Adaptive Attention Inference Hops Pruned Memory-Augmented Neural Networks. (arXiv:2101.09693v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09693">
<div class="article-summary-box-inner">
<span><p>In this work, to limit the number of required attention inference hops in
memory-augmented neural networks, we propose an online adaptive approach called
A2P-MANN. By exploiting a small neural network classifier, an adequate number
of attention inference hops for the input query is determined. The technique
results in elimination of a large number of unnecessary computations in
extracting the correct answer. In addition, to further lower computations in
A2P-MANN, we suggest pruning weights of the final FC (fully-connected) layers.
To this end, two pruning approaches, one with negligible accuracy loss and the
other with controllable loss on the final accuracy, are developed. The efficacy
of the technique is assessed by using the twenty question-answering (QA) tasks
of bAbI dataset. The analytical assessment reveals, on average, more than 42%
fewer computations compared to the baseline MANN at the cost of less than 1%
accuracy loss. In addition, when used along with the previously published
zero-skipping technique, a computation count reduction of up to 68% is
achieved. Finally, when the proposed approach (without zero-skipping) is
implemented on the CPU and GPU platforms, up to 43% runtime reduction is
achieved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12672">
<div class="article-summary-box-inner">
<span><p>State-of-the-art models in natural language processing rely on separate rigid
subword tokenization algorithms, which limit their generalization ability and
adaptation to new settings. In this paper, we propose a new model inductive
bias that learns a subword tokenization end-to-end as part of the model. To
this end, we introduce a soft gradient-based subword tokenization module (GBST)
that automatically learns latent subword representations from characters in a
data-driven fashion. Concretely, GBST enumerates candidate subword blocks and
learns to score them in a position-wise fashion using a block scoring network.
We additionally introduce Charformer, a deep Transformer model that integrates
GBST and operates on the byte level. Via extensive experiments on English GLUE,
multilingual, and noisy text datasets, we show that Charformer outperforms a
series of competitive byte-level baselines while generally performing on par
and sometimes outperforming subword-based models. Additionally, Charformer is
fast, improving the speed of both vanilla byte-level and subword-level
Transformers by 28%-100% while maintaining competitive quality. We believe this
work paves the way for highly performant token-free models that are trained
completely end-to-end.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10314">
<div class="article-summary-box-inner">
<span><p>We present small-text, a simple and modular active learning library, which
offers pool-based active learning for single- and multi-label text
classification in Python. It comes with various pre-implemented
state-of-the-art query strategies, including some that can leverage the GPU.
Clearly defined interfaces allow the combination of a multitude of classifiers,
query strategies, and stopping criteria, thereby facilitating a quick mix and
match, and enabling a rapid development of both active learning experiments and
applications. To make various classifiers accessible in a consistent way, it
integrates several well-known existing machine learning libraries, namely,
scikit-learn, PyTorch, and huggingface transformers, where the latter
integrations are available as optionally installable extensions, making the
availability of a GPU competely optional. The library is available under the
MIT License at https://github.com/webis-de/small-text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's not Rocket Science : Interpreting Figurative Language in Narratives. (arXiv:2109.00087v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00087">
<div class="article-summary-box-inner">
<span><p>Figurative language is ubiquitous in English. Yet, the vast majority of NLP
research focuses on literal language. Existing text representations by design
rely on compositionality, while figurative language is often non-compositional.
In this paper, we study the interpretation of two non-compositional figurative
languages (idioms and similes). We collected datasets of fictional narratives
containing a figurative expression along with crowd-sourced plausible and
implausible continuations relying on the correct interpretation of the
expression. We then trained models to choose or generate the plausible
continuation. Our experiments show that models based solely on pre-trained
language models perform substantially worse than humans on these tasks. We
additionally propose knowledge-enhanced models, adopting human strategies for
interpreting figurative language types : inferring meaning from the context and
relying on the constituent words' literal meanings. The knowledge-enhanced
models improve the performance on both the discriminative and generative tasks,
further bridging the gap from human performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Models for Text Coherence Assessment. (arXiv:2109.02176v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02176">
<div class="article-summary-box-inner">
<span><p>Coherence is an important aspect of text quality and is crucial for ensuring
its readability. It is essential desirable for outputs from text generation
systems like summarization, question answering, machine translation, question
generation, table-to-text, etc. An automated coherence scoring model is also
helpful in essay scoring or providing writing feedback. A large body of
previous work has leveraged entity-based methods, syntactic patterns, discourse
relations, and more recently traditional deep learning architectures for text
coherence assessment. Previous work suffers from drawbacks like the inability
to handle long-range dependencies, out-of-vocabulary words, or model sequence
information. We hypothesize that coherence assessment is a cognitively complex
task that requires deeper models and can benefit from other related tasks.
Accordingly, in this paper, we propose four different Transformer-based
architectures for the task: vanilla Transformer, hierarchical Transformer,
multi-task learning-based model, and a model with fact-based input
representation. Our experiments with popular benchmark datasets across multiple
domains on four different coherence assessment tasks demonstrate that our
models achieve state-of-the-art results outperforming existing models by a good
margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v5 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03370">
<div class="article-summary-box-inner">
<span><p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus
consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly
labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in
total. We collect the data from YouTube and Podcast, which covers a variety of
speaking styles, scenarios, domains, topics, and noisy conditions. An optical
character recognition (OCR) based method is introduced to generate the
audio/text segmentation candidates for the YouTube data on its corresponding
video captions, while a high-quality ASR transcription system is used to
generate audio/text pair candidates for the Podcast data. Then we propose a
novel end-to-end label error detection approach to further validate and filter
the candidates. We also provide three manually labelled high-quality test sets
along with WenetSpeech for evaluation -- Dev for cross-validation purpose in
training, Test_Net, collected from Internet for matched test, and
Test\_Meeting, recorded from real meetings for more challenging mismatched
test. Baseline systems trained with WenetSpeech are provided for three popular
speech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition
results on the three test sets are also provided as benchmarks. To the best of
our knowledge, WenetSpeech is the current largest open-sourced Mandarin speech
corpus with transcriptions, which benefits research on production-level speech
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Semantic Parsing via Retrieval Augmentation. (arXiv:2110.08458v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08458">
<div class="article-summary-box-inner">
<span><p>In practical applications of semantic parsing, we often want to rapidly
change the behavior of the parser, such as enabling it to handle queries in a
new domain, or changing its predictions on certain targeted queries. While we
can introduce new training examples exhibiting the target behavior, a mechanism
for enacting such behavior changes without expensive model re-training would be
preferable. To this end, we propose ControllAble Semantic Parser via Exemplar
Retrieval (CASPER). Given an input query, the parser retrieves related
exemplars from a retrieval index, augments them to the query, and then applies
a generative seq2seq model to produce an output parse. The exemplars act as a
control mechanism over the generic generative model: by manipulating the
retrieval index or how the augmented query is constructed, we can manipulate
the behavior of the parser. On the MTOP dataset, in addition to achieving
state-of-the-art on the standard setup, we show that CASPER can parse queries
in a new domain, adapt the prediction toward the specified patterns, or adapt
to new semantic schemas without having to further re-train the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Trained Language Models for Interactive Decision-Making. (arXiv:2202.01771v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01771">
<div class="article-summary-box-inner">
<span><p>Language model (LM) pre-training has proven useful for a wide variety of
language processing tasks, but can such pre-training be leveraged for more
general machine learning problems? We investigate the effectiveness of language
modeling to scaffold learning and generalization in autonomous decision-making.
We describe a framework for imitation learning in which goals and observations
are represented as a sequence of embeddings, and translated into actions using
a policy network initialized with a pre-trained transformer LM. We demonstrate
that this framework enables effective combinatorial generalization across
different environments, such as VirtualHome and BabyAI. In particular, for test
tasks involving novel goals or novel scenes, initializing policies with
language models improves task completion rates by 43.6% in VirtualHome. We
hypothesize and investigate three possible factors underlying the effectiveness
of LM-based policy initialization. We find that sequential representations (vs.
fixed-dimensional feature vectors) and the LM objective (not just the
transformer architecture) are both important for generalization. Surprisingly,
however, the format of the policy inputs encoding (e.g. as a natural language
string vs. an arbitrary sequential encoding) has little influence. Together,
these results suggest that language modeling induces representations that are
useful for modeling not just language, but also goals and plans; these
representations can aid learning and generalization even outside of language
processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09662">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models are able to generate fluent text and be
efficiently adapted across various natural language generation tasks. However,
language models that are pretrained on large unlabeled web text corpora have
been shown to suffer from degenerating toxic content and social bias behaviors,
consequently hindering their safe deployment. Various detoxification methods
were proposed to mitigate the language model's toxicity; however, these methods
struggled to detoxify language models when conditioned on prompts that contain
specific social identities related to gender, race, or religion. In this study,
we propose Reinforce-Detoxify; A reinforcement learning-based method for
mitigating toxicity in language models. We address the challenge of safety in
language models and propose a new reward model that is able to detect toxic
content and mitigate unintended bias towards social identities in toxicity
prediction. The experiments demonstrate that the Reinforce-Detoxify method for
language model detoxification outperforms existing detoxification approaches in
automatic evaluation metrics, indicating the ability of our approach in
language model detoxification and less prone to unintended bias toward social
identities in generated content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleBERT: Chinese pretraining by font style information. (arXiv:2202.09955v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09955">
<div class="article-summary-box-inner">
<span><p>With the success of down streaming task using English pre-trained language
model, the pre-trained Chinese language model is also necessary to get a better
performance of Chinese NLP task. Unlike the English language, Chinese has its
special characters such as glyph information. So in this article, we propose
the Chinese pre-trained language model StyleBERT which incorporate the
following embedding information to enhance the savvy of language model, such as
word, pinyin, five stroke and chaizi. The experiments show that the model
achieves well performances on a wide range of Chinese NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Cluster Patterns for Abstractive Summarization. (arXiv:2202.10967v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10967">
<div class="article-summary-box-inner">
<span><p>Nowadays, pre-trained sequence-to-sequence models such as BERTSUM and BART
have shown state-of-the-art results in abstractive summarization. In these
models, during fine-tuning, the encoder transforms sentences to context vectors
in the latent space and the decoder learns the summary generation task based on
the context vectors. In our approach, we consider two clusters of salient and
non-salient context vectors, using which the decoder can attend more to salient
context vectors for summary generation. For this, we propose a novel clustering
transformer layer between the encoder and the decoder, which first generates
two clusters of salient and non-salient vectors, and then normalizes and
shrinks the clusters to make them apart in the latent space. Our experimental
result shows that the proposed model outperforms the existing BART model by
learning these distinct cluster patterns, improving up to 4% in ROUGE and 0.3%
in BERTScore on average in CNN/DailyMail and XSUM data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Sentence Embedding with Generalized Pooling. (arXiv:1806.09828v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1806.09828">
<div class="article-summary-box-inner">
<span><p>Pooling is an essential component of a wide variety of sentence
representation and embedding models. This paper explores generalized pooling
methods to enhance sentence embedding. We propose vector-based multi-head
attention that includes the widely used max pooling, mean pooling, and scalar
self-attention as special cases. The model benefits from properly designed
penalization terms to reduce redundancy in multi-head attention. We evaluate
the proposed model on three different tasks: natural language inference (NLI),
author profiling, and sentiment classification. The experiments show that the
proposed model achieves significant improvement over strong
sentence-encoding-based methods, resulting in state-of-the-art performances on
four datasets. The proposed approach can be easily implemented for more
problems than we discuss in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Attack Synthesis by Extracting Finite State Machines from Protocol Specification Documents. (arXiv:2202.09470v1 [cs.CR] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09470">
<div class="article-summary-box-inner">
<span><p>Automated attack discovery techniques, such as attacker synthesis or
model-based fuzzing, provide powerful ways to ensure network protocols operate
correctly and securely. Such techniques, in general, require a formal
representation of the protocol, often in the form of a finite state machine
(FSM). Unfortunately, many protocols are only described in English prose, and
implementing even a simple network protocol as an FSM is time-consuming and
prone to subtle logical errors. Automatically extracting protocol FSMs from
documentation can significantly contribute to increased use of these techniques
and result in more robust and secure protocol implementations.
</p>
<p>In this work we focus on attacker synthesis as a representative technique for
protocol security, and on RFCs as a representative format for protocol prose
description. Unlike other works that rely on rule-based approaches or use
off-the-shelf NLP tools directly, we suggest a data-driven approach for
extracting FSMs from RFC documents. Specifically, we use a hybrid approach
consisting of three key steps: (1) large-scale word-representation learning for
technical language, (2) focused zero-shot learning for mapping protocol text to
a protocol-independent information language, and (3) rule-based mapping from
protocol-independent information to a specific protocol FSM. We show the
generalizability of our FSM extraction by using the RFCs for six different
protocols: BGPv4, DCCP, LTP, PPTP, SCTP and TCP. We demonstrate how automated
extraction of an FSM from an RFC can be applied to the synthesis of attacks,
with TCP and DCCP as case-studies. Our approach shows that it is possible to
automate attacker synthesis against protocols by using textual specifications
such as RFCs.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Roto-Translation Equivariant Super-Resolution of Two-Dimensional Flows Using Convolutional Neural Networks. (arXiv:2202.11099v1 [physics.flu-dyn])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11099">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) often process vectors as quantities
having no direction like colors in images. This study investigates the effect
of treating vectors as geometrical objects in terms of super-resolution of
velocity on two-dimensional fluids. Vector is distinguished from scalar by the
transformation law associated with a change in basis, which can be incorporated
as the prior knowledge using the equivariant deep learning. We convert existing
CNNs into equivariant ones by making each layer equivariant with respect to
rotation and translation. The training data in the low- and high-resolution are
generated with the downsampling or the spectral nudging. When the data inherit
the rotational symmetry, the equivariant CNNs show comparable accuracy with the
non-equivariant ones. Since the number of parameters is smaller in the
equivariant CNNs, these models are trainable with a smaller size of the data.
In this case, the transformation law of vector should be incorporated as the
prior knowledge, where vector is explicitly treated as a quantity having
direction. Two examples demonstrate that the symmetry of the data can be
broken. In the first case, a downsampling method makes the correspondence
between low- and high-resolution patterns dependent on the orientation. In the
second case, the input data are insufficient to recognize the rotation of
coordinates in the experiment with the spectral nudging. In both cases, the
accuracy of the CNNs deteriorates if the equivariance is forced to be imposed,
and the usage of conventional CNNs may be justified even though vector is
processed as a quantity having no direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Free Object Segments for Long-Tailed Instance Segmentation. (arXiv:2202.11124v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11124">
<div class="article-summary-box-inner">
<span><p>One fundamental challenge in building an instance segmentation model for a
large number of classes in complex scenes is the lack of training examples,
especially for rare objects. In this paper, we explore the possibility to
increase the training examples without laborious data collection and
annotation. We find that an abundance of instance segments can potentially be
obtained freely from object-centric im-ages, according to two insights: (i) an
object-centric image usually contains one salient object in a simple
background; (ii) objects from the same class often share similar appearances or
similar contrasts to the background. Motivated by these insights, we propose a
simple and scalable framework FreeSeg for extracting and leveraging these
"free" object foreground segments to facilitate model training in long-tailed
instance segmentation. Concretely, we employ off-the-shelf object foreground
extraction techniques (e.g., image co-segmentation) to generate instance mask
candidates, followed by segments refinement and ranking. The resulting
high-quality object segments can be used to augment the existing long-tailed
dataset, e.g., by copying and pasting the segments onto the original training
images. On the LVIS benchmark, we show that FreeSeg yields substantial
improvements on top of strong baselines and achieves state-of-the-art accuracy
for segmenting rare object categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning. (arXiv:2202.11202v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11202">
<div class="article-summary-box-inner">
<span><p>Indiscriminate data poisoning attacks are quite effective against supervised
learning. However, not much is known about their impact on unsupervised
contrastive learning (CL). This paper is the first to consider indiscriminate
data poisoning attacks on contrastive learning, demonstrating the feasibility
of such attacks, and their differences from indiscriminate poisoning of
supervised learning. We also highlight differences between contrastive learning
algorithms, and show that some algorithms (e.g., SimCLR) are more vulnerable
than others (e.g., MoCo). We differentiate between two types of data poisoning
attacks: sample-wise attacks, which add specific noise to each image, cause the
largest drop in accuracy, but do not transfer well across SimCLR, MoCo, and
BYOL. In contrast, attacks that use class-wise noise, though cause a smaller
drop in accuracy, transfer well across different CL algorithms. Finally, we
show that a new data augmentation based on matrix completion can be highly
effective in countering data poisoning attacks on unsupervised contrastive
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-Smoothed Backdoor Attack. (arXiv:2202.11203v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11203">
<div class="article-summary-box-inner">
<span><p>By injecting a small number of poisoned samples into the training set,
backdoor attacks aim to make the victim model produce designed outputs on any
input injected with pre-designed backdoors. In order to achieve a high attack
success rate using as few poisoned training samples as possible, most existing
attack methods change the labels of the poisoned samples to the target class.
This practice often results in severe over-fitting of the victim model over the
backdoors, making the attack quite effective in output control but easier to be
identified by human inspection or automatic defense algorithms.
</p>
<p>In this work, we proposed a label-smoothing strategy to overcome the
over-fitting problem of these attack methods, obtaining a
\textit{Label-Smoothed Backdoor Attack} (LSBA). In the LSBA, the label of the
poisoned sample $\bm{x}$ will be changed to the target class with a probability
of $p_n(\bm{x})$ instead of 100\%, and the value of $p_n(\bm{x})$ is
specifically designed to make the prediction probability the target class be
only slightly greater than those of the other classes. Empirical studies on
several existing backdoor attacks show that our strategy can considerably
improve the stealthiness of these attacks and, at the same time, achieve a high
attack success rate. In addition, our strategy makes it able to manually
control the prediction probability of the design output through manipulating
the applied and activated number of LSBAs\footnote{Source code will be
published at \url{https://github.com/v-mipeng/LabelSmoothedAttack.git}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arbitrary Shape Text Detection using Transformers. (arXiv:2202.11221v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11221">
<div class="article-summary-box-inner">
<span><p>Recent text detection frameworks require several handcrafted components such
as anchor generation, non-maximum suppression (NMS), or multiple processing
stages (e.g. label generation) to detect arbitrarily shaped text images. In
contrast, we propose an end-to-end trainable architecture based on Detection
using Transformers (DETR), that outperforms previous state-of-the-art methods
in arbitrary-shaped text detection. At its core, our proposed method leverages
a bounding box loss function that accurately measures the arbitrary detected
text regions' changes in scale and aspect ratio. This is possible due to a
hybrid shape representation made from Bezier curves, that are further split
into piece-wise polygons. The proposed loss function is then a combination of a
generalized-split-intersection-over-union loss defined over the piece-wise
polygons and regularized by a Smooth-$\ln$ regression over the Bezier curve's
control points. We evaluate our proposed model using Total-Text and CTW-1500
datasets for curved text, and MSRA-TD500 and ICDAR15 datasets for
multi-oriented text, and show that the proposed method outperforms the previous
state-of-the-art methods in arbitrary-shape text detection tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Efficient Deep Convolutional Neural Network-based Sensor Fusion for Autonomous Driving. (arXiv:2202.11231v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11231">
<div class="article-summary-box-inner">
<span><p>Autonomous driving demands accurate perception and safe decision-making. To
achieve this, automated vehicles are now equipped with multiple sensors (e.g.,
camera, Lidar, etc.), enabling them to exploit complementary environmental
context by fusing data from different sensing modalities. With the success of
Deep Convolutional Neural Network(DCNN), the fusion between DCNNs has been
proved as a promising strategy to achieve satisfactory perception accuracy.
However, mainstream existing DCNN fusion schemes conduct fusion by directly
element-wisely adding feature maps extracted from different modalities together
at various stages, failing to consider whether the features being fused are
matched or not. Therefore, we first propose a feature disparity metric to
quantitatively measure the degree of feature disparity between the feature maps
being fused. We then propose Fusion-filter as a feature-matching techniques to
tackle the feature-mismatching issue. We also propose a Layer-sharing technique
in the deep layer that can achieve better accuracy with less computational
overhead. Together with the help of the feature disparity to be an additional
loss, our proposed technologies enable DCNN to learn corresponding feature maps
with similar characteristics and complementary visual context from different
modalities to achieve better accuracy. Experimental results demonstrate that
our proposed fusion technique can achieve better accuracy on KITTI dataset with
less computational resources demand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Augmented Classification for Long-Tail Visual Recognition. (arXiv:2202.11233v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11233">
<div class="article-summary-box-inner">
<span><p>We introduce Retrieval Augmented Classification (RAC), a generic approach to
augmenting standard image classification pipelines with an explicit retrieval
module. RAC consists of a standard base image encoder fused with a parallel
retrieval branch that queries a non-parametric external memory of pre-encoded
images and associated text snippets. We apply RAC to the problem of long-tail
classification and demonstrate a significant improvement over previous
state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7%
respectively), despite using only the training datasets themselves as the
external information source. We demonstrate that RAC's retrieval module,
without prompting, learns a high level of accuracy on tail classes. This, in
turn, frees the base encoder to focus on common classes, and improve its
performance thereon. RAC represents an alternative approach to utilizing large,
pretrained models without requiring fine-tuning, as well as a first step
towards more effectively making use of external memory within common computer
vision architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FUNQUE: Fusion of Unified Quality Evaluators. (arXiv:2202.11241v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11241">
<div class="article-summary-box-inner">
<span><p>Fusion-based quality assessment has emerged as a powerful method for
developing high-performance quality models from quality models that
individually achieve lower performances. A prominent example of such an
algorithm is VMAF, which has been widely adopted as an industry standard for
video quality prediction along with SSIM. In addition to advancing the
state-of-the-art, it is imperative to alleviate the computational burden
presented by the use of a heterogeneous set of quality models. In this paper,
we unify "atom" quality models by computing them on a common transform domain
that accounts for the Human Visual System, and we propose FUNQUE, a quality
model that fuses unified quality evaluators. We demonstrate that in comparison
to the state-of-the-art, FUNQUE offers significant improvements in both
correlation against subjective scores and efficiency, due to computation
sharing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An End-to-End Cascaded Image Deraining and Object Detection Neural Network. (arXiv:2202.11279v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11279">
<div class="article-summary-box-inner">
<span><p>While the deep learning-based image deraining methods have made great
progress in recent years, there are two major shortcomings in their application
in real-world situations. Firstly, the gap between the low-level vision task
represented by rain removal and the high-level vision task represented by
object detection is significant, and the low-level vision task can hardly
contribute to the high-level vision task. Secondly, the quality of the
deraining dataset needs to be improved. In fact, the rain lines in many
baselines have a large gap with the real rain lines, and the resolution of the
deraining dataset images is generally not ideally. Meanwhile, there are few
common datasets for both the low-level vision task and the high-level vision
task. In this paper, we explore the combination of the low-level vision task
with the high-level vision task. Specifically, we propose an end-to-end object
detection network for reducing the impact of rainfall, which consists of two
cascaded networks, an improved image deraining network and an object detection
network, respectively. We also design the components of the loss function to
accommodate the characteristics of the different sub-networks. We then propose
a dataset based on the KITTI dataset for rainfall removal and object detection,
on which our network surpasses the state-of-the-art with a significant
improvement in metrics. Besides, our proposed network is measured on driving
videos collected by self-driving vehicles and shows positive results for rain
removal and object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LPF-Defense: 3D Adversarial Defense based on Frequency Analysis. (arXiv:2202.11287v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11287">
<div class="article-summary-box-inner">
<span><p>Although 3D point cloud classification has recently been widely deployed in
different application scenarios, it is still very vulnerable to adversarial
attacks. This increases the importance of robust training of 3D models in the
face of adversarial attacks. Based on our analysis on the performance of
existing adversarial attacks, more adversarial perturbations are found in the
mid and high-frequency components of input data. Therefore, by suppressing the
high-frequency content in the training phase, the models robustness against
adversarial examples is improved. Experiments showed that the proposed defense
method decreases the success rate of six attacks on PointNet, PointNet++ ,, and
DGCNN models. In particular, improvements are achieved with an average increase
of classification accuracy by 3.8 % on drop100 attack and 4.26 % on drop200
attack compared to the state-of-the-art methods. The method also improves
models accuracy on the original dataset compared to other available methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Inlier Evaluation for Unsupervised Point Cloud Registration. (arXiv:2202.11292v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11292">
<div class="article-summary-box-inner">
<span><p>Unsupervised point cloud registration algorithm usually suffers from the
unsatisfied registration precision in the partially overlapping problem due to
the lack of effective inlier evaluation. In this paper, we propose a
neighborhood consensus based reliable inlier evaluation method for robust
unsupervised point cloud registration. It is expected to capture the
discriminative geometric difference between the source neighborhood and the
corresponding pseudo target neighborhood for effective inlier distinction.
Specifically, our model consists of a matching map refinement module and an
inlier evaluation module. In our matching map refinement module, we improve the
point-wise matching map estimation by integrating the matching scores of
neighbors into it. The aggregated neighborhood information potentially
facilitates the discriminative map construction so that high-quality
correspondences can be provided for generating the pseudo target point cloud.
Based on the observation that the outlier has the significant structure-wise
difference between its source neighborhood and corresponding pseudo target
neighborhood while this difference for inlier is small, the inlier evaluation
module exploits this difference to score the inlier confidence for each
estimated correspondence. In particular, we construct an effective graph
representation for capturing this geometric difference between the
neighborhoods. Finally, with the learned correspondences and the corresponding
inlier confidence, we use the weighted SVD algorithm for transformation
estimation. Under the unsupervised setting, we exploit the Huber function based
global alignment loss, the local neighborhood consensus loss, and spatial
consistency loss for model optimization. The experimental results on extensive
datasets demonstrate that our unsupervised point cloud registration method can
yield comparable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are We Ready for Robust and Resilient SLAM? A Framework For Quantitative Characterization of SLAM Datasets. (arXiv:2202.11312v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11312">
<div class="article-summary-box-inner">
<span><p>Reliability of SLAM systems is considered one of the critical requirements in
many modern autonomous systems. This directed the efforts to developing many
state-of-the-art systems, creating challenging datasets, and introducing
rigorous metrics to measure SLAM system performance. However, the link between
datasets and performance in the robustness/resilience context has rarely been
explored. In order to fill this void, characterization the operating conditions
of SLAM systems is essential in order to provide an environment for
quantitative measurement of robustness and resilience. In this paper, we argue
that for proper evaluation of SLAM performance, the characterization of SLAM
datasets serves as a critical first step. The study starts by reviewing
previous efforts for quantitative characterization of SLAM datasets. Then, the
problem of perturbations characterization is discussed and the linkage to SLAM
robustness/resilience is established. After that, we propose a novel, generic
and extendable framework for quantitative analysis and comparison of SLAM
datasets. Additionally, a description of different characterization parameters
is provided. Finally, we demonstrate the application of our framework by
presenting the characterization results of three SLAM datasets: KITTI,
EuroC-MAV, and TUM-VI highlighting the level of insights achieved by the
proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Absolute Zero-Shot Learning. (arXiv:2202.11319v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11319">
<div class="article-summary-box-inner">
<span><p>Considering the increasing concerns about data copyright and privacy issues,
we present a novel Absolute Zero-Shot Learning (AZSL) paradigm, i.e., training
a classifier with zero real data. The key innovation is to involve a teacher
model as the data safeguard to guide the AZSL model training without data
leaking. The AZSL model consists of a generator and student network, which can
achieve date-free knowledge transfer while maintaining the performance of the
teacher network. We investigate `black-box' and `white-box' scenarios in AZSL
task as different levels of model security. Besides, we also provide discussion
of teacher model in both inductive and transductive settings. Despite
embarrassingly simple implementations and data-missing disadvantages, our AZSL
framework can retain state-of-the-art ZSL and GZSL performance under the
`white-box' scenario. Extensive qualitative and quantitative analysis also
demonstrates promising results when deploying the model under `black-box'
scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EcoFusion: Energy-Aware Adaptive Sensor Fusion for Efficient Autonomous Vehicle Perception. (arXiv:2202.11330v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11330">
<div class="article-summary-box-inner">
<span><p>Autonomous vehicles use multiple sensors, large deep-learning models, and
powerful hardware platforms to perceive the environment and navigate safely. In
many contexts, some sensing modalities negatively impact perception while
increasing energy consumption. We propose EcoFusion: an energy-aware sensor
fusion approach that uses context to adapt the fusion method and reduce energy
consumption without affecting perception performance. EcoFusion performs up to
9.5% better at object detection than existing fusion methods with approximately
60% less energy and 58% lower latency on the industry-standard Nvidia Drive PX2
hardware platform. We also propose several context-identification strategies,
implement a joint optimization between energy and performance, and present
scenario-specific results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Commonsense Reasoning for Identifying and Understanding the Implicit Need of Help and Synthesizing Assistive Actions. (arXiv:2202.11337v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11337">
<div class="article-summary-box-inner">
<span><p>Human-Robot Interaction (HRI) is an emerging subfield of service robotics.
While most existing approaches rely on explicit signals (i.e. voice, gesture)
to engage, current literature is lacking solutions to address implicit user
needs. In this paper, we present an architecture to (a) detect user implicit
need of help and (b) generate a set of assistive actions without prior
learning. Task (a) will be performed using state-of-the-art solutions for Scene
Graph Generation coupled to the use of commonsense knowledge; whereas, task (b)
will be performed using additional commonsense knowledge as well as a sentiment
analysis on graph structure. Finally, we propose an evaluation of our solution
using established benchmarks (e.g. ActionGenome dataset) along with human
experiments. The main motivation of our approach is the embedding of the
perception-decision-action loop in a single architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deepfake Detection for Facial Images with Facemasks. (arXiv:2202.11359v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11359">
<div class="article-summary-box-inner">
<span><p>Hyper-realistic face image generation and manipulation have givenrise to
numerous unethical social issues, e.g., invasion of privacy,threat of security,
and malicious political maneuvering, which re-sulted in the development of
recent deepfake detection methodswith the rising demands of deepfake forensics.
Proposed deepfakedetection methods to date have shown remarkable detection
perfor-mance and robustness. However, none of the suggested deepfakedetection
methods assessed the performance of deepfakes withthe facemask during the
pandemic crisis after the outbreak of theCovid-19. In this paper, we thoroughly
evaluate the performance ofstate-of-the-art deepfake detection models on the
deepfakes withthe facemask. Also, we propose two approaches to enhance
themasked deepfakes detection:face-patchandface-crop. The experi-mental
evaluations on both methods are assessed through the base-line deepfake
detection models on the various deepfake datasets.Our extensive experiments
show that, among the two methods,face-cropperforms better than theface-patch,
and could be a trainmethod for deepfake detection models to detect fake faces
withfacemask in real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localizing Small Apples in Complex Apple Orchard Environments. (arXiv:2202.11372v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11372">
<div class="article-summary-box-inner">
<span><p>The localization of fruits is an essential first step in automated
agricultural pipelines for yield estimation or fruit picking. One example of
this is the localization of apples in images of entire apple trees. Since the
apples are very small objects in such scenarios, we tackle this problem by
adapting the object proposal generation system AttentionMask that focuses on
small objects. We adapt AttentionMask by either adding a new module for very
small apples or integrating it into a tiling framework. Both approaches clearly
outperform standard object proposal generation systems on the MinneApple
dataset covering complex apple orchard environments. Our evaluation further
analyses the improvement w.r.t. the apple sizes and shows the different
characteristics of our two approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion Network for Action Recognition. (arXiv:2202.11374v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11374">
<div class="article-summary-box-inner">
<span><p>Action recognition has been a heated topic in computer vision for its wide
application in vision systems. Previous approaches achieve improvement by
fusing the modalities of the skeleton sequence and RGB video. However, such
methods have a dilemma between the accuracy and efficiency for the high
complexity of the RGB video network. To solve the problem, we propose a
multi-modality feature fusion network to combine the modalities of the skeleton
sequence and RGB frame instead of the RGB video, as the key information
contained by the combination of skeleton sequence and RGB frame is close to
that of the skeleton sequence and RGB video. In this way, the complementary
information is retained while the complexity is reduced by a large margin. To
better explore the correspondence of the two modalities, a two-stage fusion
framework is introduced in the network. In the early fusion stage, we introduce
a skeleton attention module that projects the skeleton sequence on the single
RGB frame to help the RGB frame focus on the limb movement regions. In the late
fusion stage, we propose a cross-attention module to fuse the skeleton feature
and the RGB feature by exploiting the correlation. Experiments on two
benchmarks NTU RGB+D and SYSU show that the proposed model achieves competitive
performance compared with the state-of-the-art methods while reduces the
complexity of the network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale Sparse Representation-Based Shadow Inpainting for Retinal OCT Images. (arXiv:2202.11377v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11377">
<div class="article-summary-box-inner">
<span><p>Inpainting shadowed regions cast by superficial blood vessels in retinal
optical coherence tomography (OCT) images is critical for accurate and robust
machine analysis and clinical diagnosis. Traditional sequence-based approaches
such as propagating neighboring information to gradually fill in the missing
regions are cost-effective. But they generate less satisfactory outcomes when
dealing with larger missing regions and texture-rich structures. Emerging deep
learning-based methods such as encoder-decoder networks have shown promising
results in natural image inpainting tasks. However, they typically need a long
computational time for network training in addition to the high demand on the
size of datasets, which makes it difficult to be applied on often small medical
datasets. To address these challenges, we propose a novel multi-scale shadow
inpainting framework for OCT images by synergically applying sparse
representation and deep learning: sparse representation is used to extract
features from a small amount of training images for further inpainting and to
regularize the image after the multi-scale image fusion, while convolutional
neural network (CNN) is employed to enhance the image quality. During the image
inpainting, we divide preprocessed input images into different branches based
on the shadow width to harvest complementary information from different scales.
Finally, a sparse representation-based regularizing module is designed to
refine the generated contents after multi-scale feature aggregation.
Experiments are conducted to compare our proposal versus both traditional and
deep learning-based techniques on synthetic and real-world shadows. Results
demonstrate that our proposed method achieves favorable image inpainting in
terms of visual quality and quantitative metrics, especially when wide shadows
are presented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Teacher Knowledge Distillation for Incremental Implicitly-Refined Classification. (arXiv:2202.11384v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11384">
<div class="article-summary-box-inner">
<span><p>Incremental learning methods can learn new classes continually by distilling
knowledge from the last model (as a teacher model) to the current model (as a
student model) in the sequentially learning process. However, these methods
cannot work for Incremental Implicitly-Refined Classification (IIRC), an
incremental learning extension where the incoming classes could have two
granularity levels, a superclass label and a subclass label. This is because
the previously learned superclass knowledge may be occupied by the subclass
knowledge learned sequentially. To solve this problem, we propose a novel
Multi-Teacher Knowledge Distillation (MTKD) strategy. To preserve the subclass
knowledge, we use the last model as a general teacher to distill the previous
knowledge for the student model. To preserve the superclass knowledge, we use
the initial model as a superclass teacher to distill the superclass knowledge
as the initial model contains abundant superclass knowledge. However,
distilling knowledge from two teacher models could result in the student model
making some redundant predictions. We further propose a post-processing
mechanism, called as Top-k prediction restriction to reduce the redundant
predictions. Our experimental results on IIRC-ImageNet120 and IIRC-CIFAR100
show that the proposed method can achieve better classification accuracy
compared with existing state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Metric Learning-Based Semi-Supervised Regression With Alternate Learning. (arXiv:2202.11388v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11388">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel deep metric learning-based semi-supervised
regression (DML-S2R) method for parameter estimation problems. The proposed
DML-S2R method aims to mitigate the problems of insufficient amount of labeled
samples without collecting any additional samples with target values. To this
end, the proposed DML-S2R method is made up of two main steps: i) pairwise
similarity modeling with scarce labeled data; and ii) triplet-based metric
learning with abundant unlabeled data. The first step aims to model pairwise
sample similarities by using a small number of labeled samples. This is
achieved by estimating the target value differences of labeled samples with a
Siamese neural network (SNN). The second step aims to learn a triplet-based
metric space (in which similar samples are close to each other and dissimilar
samples are far apart from each other) when the number of labeled samples is
insufficient. This is achieved by employing the SNN of the first step for
triplet-based deep metric learning that exploits not only labeled samples but
also unlabeled samples. For the end-to-end training of DML-S2R, we investigate
an alternate learning strategy for the two steps. Due to this strategy, the
encoded information in each step becomes a guidance for learning the other
step. The experimental results confirm the success of DML-S2R compared to the
state-of-the-art semi-supervised regression methods. The code of the proposed
method is publicly available at https://git.tu-berlin.de/rsim/DML-S2R.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed-Block Neural Architecture Search for Medical Image Segmentation. (arXiv:2202.11401v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11401">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) have the potential for making various clinical
procedures more time-efficient by automating medical image segmentation. Due to
their strong, in some cases human-level, performance, they have become the
standard approach in this field. The design of the best possible medical image
segmentation DNNs, however, is task-specific. Neural Architecture Search (NAS),
i.e., the automation of neural network design, has been shown to have the
capability to outperform manually designed networks for various tasks. However,
the existing NAS methods for medical image segmentation have explored a quite
limited range of types of DNN architectures that can be discovered. In this
work, we propose a novel NAS search space for medical image segmentation
networks. This search space combines the strength of a generalised
encoder-decoder structure, well known from U-Net, with network blocks that have
proven to have a strong performance in image classification tasks. The search
is performed by looking for the best topology of multiple cells simultaneously
with the configuration of each cell within, allowing for interactions between
topology and cell-level attributes. From experiments on two publicly available
datasets, we find that the networks discovered by our proposed NAS method have
better performance than well-known handcrafted segmentation networks, and
outperform networks found with other NAS approaches that perform only topology
search, and topology-level search followed by cell-level search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProFormer: Learning Data-efficient Representations of Body Movement with Prototype-based Feature Augmentation and Visual Transformers. (arXiv:2202.11423v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11423">
<div class="article-summary-box-inner">
<span><p>Automatically understanding human behaviour allows household robots to
identify the most critical needs and plan how to assist the human according to
the current situation. However, the majority of such methods are developed
under the assumption that a large amount of labelled training examples is
available for all concepts-of-interest. Robots, on the other hand, operate in
constantly changing unstructured environments, and need to adapt to novel
action categories from very few samples. Methods for data-efficient recognition
from body poses increasingly leverage skeleton sequences structured as
image-like arrays and then used as input to convolutional neural networks. We
look at this paradigm from the perspective of transformer networks, for the
first time exploring visual transformers as data-efficient encoders of skeleton
movement. In our pipeline, body pose sequences cast as image-like
representations are converted into patch embeddings and then passed to a visual
transformer backbone optimized with deep metric learning. Inspired by recent
success of feature enhancement methods in semi-supervised learning, we further
introduce ProFormer -- an improved training strategy which uses soft-attention
applied on iteratively estimated action category prototypes used to augment the
embeddings and compute an auxiliary consistency loss. Extensive experiments
consistently demonstrate the effectiveness of our approach for one-shot
recognition from body poses, achieving state-of-the-art results on multiple
datasets and surpassing the best published approach on the challenging NTU-120
one-shot benchmark by 1.84%. Our code will be made publicly available at
https://github.com/KPeng9510/ProFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Self-Supervised Cross-Modal Image Retrieval Method In Remote Sensing. (arXiv:2202.11429v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11429">
<div class="article-summary-box-inner">
<span><p>Due to the availability of multi-modal remote sensing (RS) image archives,
one of the most important research topics is the development of cross-modal RS
image retrieval (CM-RSIR) methods that search semantically similar images
across different modalities. Existing CM-RSIR methods require annotated
training images (which is time-consuming, costly and not feasible to gather in
large-scale applications) and do not concurrently address intra- and
inter-modal similarity preservation and inter-modal discrepancy elimination. In
this paper, we introduce a novel self-supervised cross-modal image retrieval
method that aims to: i) model mutual-information between different modalities
in a self-supervised manner; ii) retain the distributions of modal-specific
feature spaces similar; and iii) define most similar images within each
modality without requiring any annotated training images. To this end, we
propose a novel objective including three loss functions that simultaneously:
i) maximize mutual information of different modalities for inter-modal
similarity preservation; ii) minimize the angular distance of multi-modal image
tuples for the elimination of inter-modal discrepancies; and iii) increase
cosine similarity of most similar images within each modality for the
characterization of intra-modal similarities. Experimental results show the
effectiveness of the proposed method compared to state-of-the-art methods. The
code of the proposed method is publicly available at
https://git.tu-berlin.de/rsim/SS-CM-RSIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On PAC-Bayesian reconstruction guarantees for VAEs. (arXiv:2202.11455v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11455">
<div class="article-summary-box-inner">
<span><p>Despite its wide use and empirical successes, the theoretical understanding
and study of the behaviour and performance of the variational autoencoder (VAE)
have only emerged in the past few years. We contribute to this recent line of
work by analysing the VAE's reconstruction ability for unseen test data,
leveraging arguments from the PAC-Bayes theory. We provide generalisation
bounds on the theoretical reconstruction error, and provide insights on the
regularisation effect of VAE objectives. We illustrate our theoretical results
with supporting experiments on classical benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLOGAN: Handwriting Style Synthesis for Arbitrary-Length and Out-of-Vocabulary Text. (arXiv:2202.11456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11456">
<div class="article-summary-box-inner">
<span><p>Large amounts of labeled data are urgently required for the training of
robust text recognizers. However, collecting handwriting data of diverse
styles, along with an immense lexicon, is considerably expensive. Although data
synthesis is a promising way to relieve data hunger, two key issues of
handwriting synthesis, namely, style representation and content embedding,
remain unsolved. To this end, we propose a novel method that can synthesize
parameterized and controllable handwriting Styles for arbitrary-Length and
Out-of-vocabulary text based on a Generative Adversarial Network (GAN), termed
SLOGAN. Specifically, we propose a style bank to parameterize the specific
handwriting styles as latent vectors, which are input to a generator as style
priors to achieve the corresponding handwritten styles. The training of the
style bank requires only the writer identification of the source images, rather
than attribute annotations. Moreover, we embed the text content by providing an
easily obtainable printed style image, so that the diversity of the content can
be flexibly achieved by changing the input printed image. Finally, the
generator is guided by dual discriminators to handle both the handwriting
characteristics that appear as separated characters and in a series of cursive
joins. Our method can synthesize words that are not included in the training
vocabulary and with various new styles. Extensive experiments have shown that
high-quality text images with great style diversity and rich vocabulary can be
synthesized using our method, thereby enhancing the robustness of the
recognizer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thermal hand image segmentation for biometric recognition. (arXiv:2202.11462v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11462">
<div class="article-summary-box-inner">
<span><p>In this paper we present a method to identify people by means of thermal (TH)
and visible (VIS) hand images acquired simultaneously with a TESTO 882-3
camera. In addition, we also present a new database specially acquired for this
work. The real challenge when dealing with TH images is the cold finger areas,
which can be confused with the acquisition surface. This problem is solved by
taking advantage of the VIS information. We have performed different tests to
show how TH and VIS images work in identification problems. Experimental
results reveal that TH hand image is as suitable for biometric recognition
systems as VIS hand images, and better results are obtained when combining this
information. A Biometric Dispersion Matcher has been used as a feature vector
dimensionality reduction technique as well as a classification task. Its
selection criteria helps to reduce the length of the vectors used to perform
identification up to a hundred measurements. Identification rates reach a
maximum value of 98.3% under these conditions, when using a database of 104
people.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstruction Task Finds Universal Winning Tickets. (arXiv:2202.11484v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11484">
<div class="article-summary-box-inner">
<span><p>Pruning well-trained neural networks is effective to achieve a promising
accuracy-efficiency trade-off in computer vision regimes. However, most of
existing pruning algorithms only focus on the classification task defined on
the source domain. Different from the strong transferability of the original
model, a pruned network is hard to transfer to complicated downstream tasks
such as object detection arXiv:arch-ive/2012.04643. In this paper, we show that
the image-level pretrain task is not capable of pruning models for diverse
downstream tasks. To mitigate this problem, we introduce image reconstruction,
a pixel-level task, into the traditional pruning framework. Concretely, an
autoencoder is trained based on the original model, and then the pruning
process is optimized with both autoencoder and classification losses. The
empirical study on benchmark downstream tasks shows that the proposed method
can outperform state-of-the-art results explicitly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmentation based unsupervised domain adaptation. (arXiv:2202.11486v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11486">
<div class="article-summary-box-inner">
<span><p>The insertion of deep learning in medical image analysis had lead to the
development of state-of-the art strategies in several applications such a
disease classification, as well as abnormality detection and segmentation.
However, even the most advanced methods require a huge and diverse amount of
data to generalize. Because in realistic clinical scenarios, data acquisition
and annotation is expensive, deep learning models trained on small and
unrepresentative data tend to outperform when deployed in data that differs
from the one used for training (e.g data from different scanners). In this
work, we proposed a domain adaptation methodology to alleviate this problem in
segmentation models. Our approach takes advantage of the properties of
adversarial domain adaptation and consistency training to achieve more robust
adaptation. Using two datasets with white matter hyperintensities (WMH)
annotations, we demonstrated that the proposed method improves model
generalization even in corner cases where individual strategies tend to fail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MITI: SLAM Benchmark for Laparoscopic Surgery. (arXiv:2202.11496v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11496">
<div class="article-summary-box-inner">
<span><p>We propose a new benchmark for evaluating stereoscopic visual-inertial
computer vision algorithms (SLAM/ SfM/ 3D Reconstruction/ Visual-Inertial
Odometry) for minimally invasive surgical (MIS) interventions in the abdomen.
Our MITI Dataset available at [https://mediatum.ub.tum.<a href="/abs/de/1621941">de/1621941</a>] provides all
the necessary data by a complete recording of a handheld surgical intervention
at Research Hospital Rechts der Isar of TUM. It contains multimodal sensor
information from IMU, stereoscopic video, and infrared (IR) tracking as ground
truth for evaluation. Furthermore, calibration for the stereoscope,
accelerometer, magnetometer, the rigid transformations in the sensor setup, and
time-offsets are available. We wisely chose a suitable intervention that
contains very few cutting and tissue deformation and shows a full scan of the
abdomen with a handheld camera such that it is ideal for testing SLAM
algorithms. Intending to promote the progress of visual-inertial algorithms
designed for MIS application, we hope that our clinical training dataset helps
and enables researchers to enhance algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual-tactile sensing for Real-time liquid Volume Estimation in Grasping. (arXiv:2202.11503v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11503">
<div class="article-summary-box-inner">
<span><p>We propose a deep visuo-tactile model for realtime estimation of the liquid
inside a deformable container in a proprioceptive way.We fuse two sensory
modalities, i.e., the raw visual inputs from the RGB camera and the tactile
cues from our specific tactile sensor without any extra sensor calibrations.The
robotic system is well controlled and adjusted based on the estimation model in
real time. The main contributions and novelties of our work are listed as
follows: 1) Explore a proprioceptive way for liquid volume estimation by
developing an end-to-end predictive model with multi-modal convolutional
networks, which achieve a high precision with an error of around 2 ml in the
experimental validation. 2) Propose a multi-task learning architecture which
comprehensively considers the losses from both classification and regression
tasks, and comparatively evaluate the performance of each variant on the
collected data and actual robotic platform. 3) Utilize the proprioceptive
robotic system to accurately serve and control the requested volume of liquid,
which is continuously flowing into a deformable container in real time. 4)
Adaptively adjust the grasping plan to achieve more stable grasping and
manipulation according to the real-time liquid volume prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-supervised learning for image-based classification of primary melanomas into genomic immune subgroups. (arXiv:2202.11524v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11524">
<div class="article-summary-box-inner">
<span><p>Determining early-stage prognostic markers and stratifying patients for
effective treatment are two key challenges for improving outcomes for melanoma
patients. Previous studies have used tumour transcriptome data to stratify
patients into immune subgroups, which were associated with differential
melanoma specific survival and potential treatment strategies. However,
acquiring transcriptome data is a time-consuming and costly process. Moreover,
it is not routinely used in the current clinical workflow. Here we attempt to
overcome this by developing deep learning models to classify gigapixel H&amp;E
stained pathology slides, which are well established in clinical workflows,
into these immune subgroups. Previous subtyping approaches have employed
supervised learning which requires fully annotated data, or have only examined
single genetic mutations in melanoma patients. We leverage a multiple-instance
learning approach, which only requires slide-level labels and uses an attention
mechanism to highlight regions of high importance to the classification.
Moreover, we show that pathology-specific self-supervised models generate
better representations compared to pathology-agnostic models for improving our
model performance, achieving a mean AUC of 0.76 for classifying histopathology
images as high or low immune subgroups. We anticipate that this method may
allow us to find new biomarkers of high importance and could act as a tool for
clinicians to infer the immune landscape of tumours and stratify patients,
without needing to carry out additional expensive genetic tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffractive optical system design by cascaded propagation. (arXiv:2202.11535v1 [physics.optics])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11535">
<div class="article-summary-box-inner">
<span><p>Modern design of complex optical systems relies heavily on computational
tools. These typically utilize geometrical optics as well as Fourier optics,
which enables the use of diffractive elements to manipulate light with features
on the scale of a wavelength. Fourier optics is typically used for designing
thin elements, placed in the system's aperture, generating a shift-invariant
Point Spread Function (PSF). A major bottleneck in applying Fourier Optics in
many cases of interest, e.g. when dealing with multiple, or out-of-aperture
elements, comes from numerical complexity. In this work, we propose and
implement an efficient and differentiable propagation model based on the
Collins integral, which enables the optimization of diffraction optical systems
with unprecedented design freedom using backpropagation. We demonstrate the
applicability of our method, numerically and experimentally, by engineering
shift-variant PSFs via thin plate elements placed in arbitrary planes inside
complex imaging systems, performing cascaded optimization of multiple planes,
and designing optimal machine-vision systems by deep learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut. (arXiv:2202.11539v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11539">
<div class="article-summary-box-inner">
<span><p>Transformers trained with self-supervised learning using self-distillation
loss (DINO) have been shown to produce attention maps that highlight salient
foreground objects. In this paper, we demonstrate a graph-based approach that
uses the self-supervised transformer features to discover an object from an
image. Visual tokens are viewed as nodes in a weighted graph with edges
representing a connectivity score based on the similarity of tokens. Foreground
objects can then be segmented using a normalized graph-cut to group
self-similar regions. We solve the graph-cut problem using spectral clustering
with generalized eigen-decomposition and show that the second smallest
eigenvector provides a cutting solution since its absolute value indicates the
likelihood that a token belongs to a foreground object. Despite its simplicity,
this approach significantly boosts the performance of unsupervised object
discovery: we improve over the recent state of the art LOST by a margin of
6.9%, 8.1%, and 8.1% respectively on the VOC07, VOC12, and COCO20K. The
performance can be further improved by adding a second stage class-agnostic
detector (CAD). Our proposed method can be easily extended to unsupervised
saliency detection and weakly supervised object detection. For unsupervised
saliency detection, we improve IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS,
DUT-OMRON respectively compared to previous state of the art. For weakly
supervised object detection, we achieve competitive performance on CUB and
ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Amodal Panoptic Segmentation. (arXiv:2202.11542v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11542">
<div class="article-summary-box-inner">
<span><p>Humans have the remarkable ability to perceive objects as a whole, even when
parts of them are occluded. This ability of amodal perception forms the basis
of our perceptual and cognitive understanding of our world. To enable robots to
reason with this capability, we formulate and propose a novel task that we name
amodal panoptic segmentation. The goal of this task is to simultaneously
predict the pixel-wise semantic segmentation labels of the visible regions of
stuff classes and the instance segmentation labels of both the visible and
occluded regions of thing classes. To facilitate research on this new task, we
extend two established benchmark datasets with pixel-level amodal panoptic
segmentation labels that we make publicly available as KITTI-360-APS and
BDD100K-APS. We present several strong baselines, along with the amodal
panoptic quality (APQ) and amodal parsing coverage (APC) metrics to quantify
the performance in an interpretable manner. Furthermore, we propose the novel
amodal panoptic segmentation network (APSNet), as a first step towards
addressing this task by explicitly modeling the complex relationships between
the occluders and occludes. Extensive experimental evaluations demonstrate that
APSNet achieves state-of-the-art performance on both benchmarks and more
importantly exemplifies the utility of amodal recognition. The benchmarks are
available at <a href="http://amodal-panoptic.cs.uni-freiburg.de.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Bayesian ICP Covariance Estimation. (arXiv:2202.11607v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11607">
<div class="article-summary-box-inner">
<span><p>Covariance estimation for the Iterative Closest Point (ICP) point cloud
registration algorithm is essential for state estimation and sensor fusion
purposes. We argue that a major source of error for ICP is in the input data
itself, from the sensor noise to the scene geometry. Benefiting from recent
developments in deep learning for point clouds, we propose a data-driven
approach to learn an error model for ICP. We estimate covariances modeling
data-dependent heteroscedastic aleatoric uncertainty, and epistemic uncertainty
using a variational Bayesian approach. The system evaluation is performed on
LiDAR odometry on different datasets, highlighting good results in comparison
to the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Classification on Small Datasets via Masked Feature Mixing. (arXiv:2202.11616v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11616">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks require large amounts of labeled data
samples. For many real-world applications, this is a major limitation which is
commonly treated by augmentation methods. In this work, we address the problem
of learning deep neural networks on small datasets. Our proposed architecture
called ChimeraMix learns a data augmentation by generating compositions of
instances. The generative model encodes images in pairs, combines the features
guided by a mask, and creates new samples. For evaluation, all methods are
trained from scratch without any additional data. Several experiments on
benchmark datasets, e.g. ciFAIR-10, STL-10, and ciFAIR-100, demonstrate the
superior performance of ChimeraMix compared to current state-of-the-art methods
for classification on small datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection in 3D Point Clouds using Deep Geometric Descriptors. (arXiv:2202.11660v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11660">
<div class="article-summary-box-inner">
<span><p>We present a new method for the unsupervised detection of geometric anomalies
in high-resolution 3D point clouds. In particular, we propose an adaptation of
the established student-teacher anomaly detection framework to three
dimensions. A student network is trained to match the output of a pretrained
teacher network on anomaly-free point clouds. When applied to test data,
regression errors between the teacher and the student allow reliable
localization of anomalous structures. To construct an expressive teacher
network that extracts dense local geometric descriptors, we introduce a novel
self-supervised pretraining strategy. The teacher is trained by reconstructing
local receptive fields and does not require annotations. Extensive experiments
on the comprehensive MVTec 3D Anomaly Detection dataset highlight the
effectiveness of our approach, which outperforms the next-best method by a
large margin. Ablation studies show that our approach meets the requirements of
practical applications regarding performance, runtime, and memory consumption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Motion Detection Using Sharpened Dimensionality Reduction and Clustering. (arXiv:2202.11667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11667">
<div class="article-summary-box-inner">
<span><p>Sharpened dimensionality reduction (SDR), which belongs to the class of
multidimensional projection techniques, has recently been introduced to tackle
the challenges in the exploratory and visual analysis of high-dimensional data.
SDR has been applied to various real-world datasets, such as human activity
sensory data and astronomical datasets. However, manually labeling the samples
from the generated projection are expensive. To address this problem, we
propose here to use clustering methods such as k-means, Hierarchical
Clustering, Density-Based Spatial Clustering of Applications with Noise
(DBSCAN), and Spectral Clustering to easily label the 2D projections of
high-dimensional data. We test our pipeline of SDR and the clustering methods
on a range of synthetic and real-world datasets, including two different public
human activity datasets extracted from smartphone accelerometer or gyroscope
recordings of various movements. We apply clustering to assess the visual
cluster separation of SDR, both qualitatively and quantitatively. We conclude
that clustering SDR results yields better labeling results than clustering
plain DR, and that k-means is the recommended clustering method for SDR in
terms of clustering accuracy, ease-of-use, and computational scalability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paying U-Attention to Textures: Multi-Stage Hourglass Vision Transformer for Universal Texture Synthesis. (arXiv:2202.11703v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11703">
<div class="article-summary-box-inner">
<span><p>We present a novel U-Attention vision Transformer for universal texture
synthesis. We exploit the natural long-range dependencies enabled by the
attention mechanism to allow our approach to synthesize diverse textures while
preserving their structures in a single inference. We propose a multi-stage
hourglass backbone that attends to the global structure and performs patch
mapping at varying scales in a coarse-to-fine-to-coarse stream. Further
completed by skip connection and convolution designs that propagate and fuse
information at different scales, our U-Attention architecture unifies attention
to microstructures, mesostructures and macrostructures, and progressively
refines synthesis results at successive stages. We show that our method
achieves stronger 2$\times$ synthesis than previous work on both stochastic and
structured textures while generalizing to unseen textures without fine-tuning.
Ablation studies demonstrate the effectiveness of each component of our
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Moments in Video Collections Using Natural Language. (arXiv:1907.12763v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.12763">
<div class="article-summary-box-inner">
<span><p>We introduce the task of retrieving relevant video moments from a large
corpus of untrimmed, unsegmented videos given a natural language query. Our
task poses unique challenges as a system must efficiently identify both the
relevant videos and localize the relevant moments in the videos. To address
these challenges, we propose SpatioTemporal Alignment with Language (STAL), a
model that represents a video moment as a set of regions within a series of
short video clips and aligns a natural language query to the moment's regions.
Our alignment cost compares variable-length language and video features using
symmetric squared Chamfer distance, which allows for efficient indexing and
retrieval of the video moments. Moreover, aligning language features to regions
within a video moment allows for finer alignment compared to methods that
extract only an aggregate feature from the entire video moment. We evaluate our
approach on two recently proposed datasets for temporal localization of moments
in video with natural language (DiDeMo and Charades-STA) extended to our video
corpus moment retrieval setting. We show that our STAL re-ranking model
outperforms the recently proposed Moment Context Network on all criteria across
all datasets on our proposed task, obtaining relative gains of 37% - 118% for
average recall and up to 30% for median rank. Moreover, our approach achieves
more than 130x faster retrieval and 8x smaller index size with a 1M video
corpus in an approximate setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation of structural parts of rosebush plants with 3D point-based deep learning methods. (arXiv:2012.11489v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11489">
<div class="article-summary-box-inner">
<span><p>Segmentation of structural parts of 3D models of plants is an important step
for plant phenotyping, especially for monitoring architectural and
morphological traits. Current state-of-the art approaches rely on hand-crafted
3D local features for modeling geometric variations in plant structures. While
recent advancements in deep learning on point clouds have the potential of
extracting relevant local and global characteristics, the scarcity of labeled
3D plant data impedes the exploration of this potential. We adapted six recent
point-based deep learning architectures (PointNet, PointNet++, DGCNN, PointCNN,
ShellNet, RIConv) for segmentation of structural parts of rosebush models. We
generated 3D synthetic rosebush models to provide adequate amount of labeled
data for modification and pre-training of these architectures. To evaluate
their performance on real rosebush plants, we used the ROSE-X data set of fully
annotated point cloud models. We provided experiments with and without the
incorporation of synthetic data to demonstrate the potential of point-based
deep learning techniques even with limited labeled data of real plants. The
experimental results show that PointNet++ produces the highest segmentation
accuracy among the six point-based deep learning methods. The advantage of
PointNet++ is that it provides a flexibility in the scales of the hierarchical
organization of the point cloud data. Pre-training with synthetic 3D models
boosted the performance of all architectures, except for PointNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Vision Transformer. (arXiv:2012.12556v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.12556">
<div class="article-summary-box-inner">
<span><p>Transformer, first applied to the field of natural language processing, is a
type of deep neural network mainly based on the self-attention mechanism.
Thanks to its strong representation capabilities, researchers are looking at
ways to apply transformer to computer vision tasks. In a variety of visual
benchmarks, transformer-based models perform similar to or better than other
types of networks such as convolutional and recurrent neural networks. Given
its high performance and less need for vision-specific inductive bias,
transformer is receiving more and more attention from the computer vision
community. In this paper, we review these vision transformer models by
categorizing them in different tasks and analyzing their advantages and
disadvantages. The main categories we explore include the backbone network,
high/mid-level vision, low-level vision, and video processing. We also include
efficient transformer methods for pushing transformer into real device-based
applications. Furthermore, we also take a brief look at the self-attention
mechanism in computer vision, as it is the base component in transformer.
Toward the end of this paper, we discuss the challenges and provide several
further research directions for vision transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Background Subtraction based on Arithmetic Distribution Neural Network. (arXiv:2104.08390v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08390">
<div class="article-summary-box-inner">
<span><p>We propose a universal background subtraction framework based on the
Arithmetic Distribution Neural Network (ADNN) for learning the distributions of
temporal pixels. In our ADNN model, the arithmetic distribution operations are
utilized to introduce the arithmetic distribution layers, including the product
distribution layer and the sum distribution layer. Furthermore, in order to
improve the accuracy of the proposed approach, an improved Bayesian refinement
model based on neighboring information, with a GPU implementation, is
incorporated. In the forward pass and backpropagation of the proposed
arithmetic distribution layers, histograms are considered as probability
density functions rather than matrices. Thus, the proposed approach is able to
utilize the probability information of the histogram and achieve promising
results with a very simple architecture compared to traditional convolutional
neural networks. Evaluations using standard benchmarks demonstrate the
superiority of the proposed approach compared to state-of-the-art traditional
and deep learning methods. To the best of our knowledge, this is the first
method to propose network layers based on arithmetic distribution operations
for learning distributions during background subtraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Superpixel-based Knowledge Infusion in Deep Neural Networks for Image Classification. (arXiv:2105.09448v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09448">
<div class="article-summary-box-inner">
<span><p>Superpixels are higher-order perceptual groups of pixels in an image, often
carrying much more information than the raw pixels. There is an inherent
relational structure to the relationship among different superpixels of an
image such as adjacent superpixels are neighbours of each other. Our interest
here is to treat these relative positions of various superpixels as relational
information of an image. This relational information can convey higher-order
spatial information about the image, such as the relationship between
superpixels representing two eyes in an image of a cat. That is, two eyes are
placed adjacent to each other in a straight line or the mouth is below the
nose. Our motive in this paper is to assist computer vision models,
specifically those based on Deep Neural Networks (DNNs), by incorporating this
higher-order information from superpixels. We construct a hybrid model that
leverages (a) Convolutional Neural Network (CNN) to deal with spatial
information in an image and (b) Graph Neural Network (GNN) to deal with
relational superpixel information in the image. The proposed model is learned
using a generic hybrid loss function. Our experiments are extensive, and we
evaluate the predictive performance of our proposed hybrid vision model on
seven different image classification datasets from a variety of domains such as
digit and object recognition, biometrics, medical imaging. The results
demonstrate that the relational superpixel information processed by a GNN can
improve the performance of a standard CNN-based vision system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Coreset Selection for Rehearsal-based Continual Learning. (arXiv:2106.01085v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01085">
<div class="article-summary-box-inner">
<span><p>A dataset is a shred of crucial evidence to describe a task. However, each
data point in the dataset does not have the same potential, as some of the data
points can be more representative or informative than others. This unequal
importance among the data points may have a large impact in rehearsal-based
continual learning, where we store a subset of the training examples (coreset)
to be replayed later to alleviate catastrophic forgetting. In continual
learning, the quality of the samples stored in the coreset directly affects the
model's effectiveness and efficiency. The coreset selection problem becomes
even more important under realistic settings, such as imbalanced continual
learning or noisy data scenarios. To tackle this problem, we propose Online
Coreset Selection (OCS), a simple yet effective method that selects the most
representative and informative coreset at each iteration and trains them in an
online manner. Our proposed method maximizes the model's adaptation to a
current dataset while selecting high-affinity samples to past tasks, which
directly inhibits catastrophic forgetting. We validate the effectiveness of our
coreset selection mechanism over various standard, imbalanced, and noisy
datasets against strong continual learning baselines, demonstrating that it
improves task adaptation and prevents catastrophic forgetting in a
sample-efficient manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting consistency for semi-supervised semantic segmentation. (arXiv:2106.07075v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07075">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning is especially interesting in the dense prediction
context due to high cost of pixel-level ground truth. Unfortunately, most such
approaches are evaluated on outdated architectures which hamper research due to
very slow training and high requirements on GPU RAM. We address this concern by
presenting a simple and effective baseline which works very well both on
standard and efficient architectures. Our baseline is based on one-way
consistency and non-linear geometric and photometric perturbations. We show
advantage of perturbing only the student branch and present a plausible
explanation of such behaviour. Experiments on Cityscapes and CIFAR-10
demonstrate competitive performance with respect to prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid mmWave and Camera System for Long-Range Depth Imaging. (arXiv:2106.07856v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07856">
<div class="article-summary-box-inner">
<span><p>mmWave radars offer excellent depth resolution owing to their high bandwidth
at mmWave radio frequencies. Yet, they suffer intrinsically from poor angular
resolution, that is an order-of-magnitude worse than camera systems, and are
therefore not a capable 3-D imaging solution in isolation. We propose
Metamoran, a system that combines the complimentary strengths of radar and
camera systems to obtain depth images at high azimuthal resolutions at
distances of several tens of meters with high accuracy, all from a single fixed
vantage point. Metamoran enables rich long-range depth imaging outdoors with
applications to roadside safety infrastructure, surveillance and wide-area
mapping. Our key insight is to use the high azimuth resolution from cameras
using computer vision techniques, including image segmentation and monocular
depth estimation, to obtain object shapes and use these as priors for our novel
specular beamforming algorithm. We also design this algorithm to work in
cluttered environments with weak reflections and in partially occluded
scenarios. We perform a detailed evaluation of Metamoran's depth imaging and
sensing capabilities in 200 diverse scenes at a major U.S. city. Our evaluation
shows that Metamoran estimates the depth of an object up to 60~m away with a
median error of 28~cm, an improvement of 13$\times$ compared to a naive
radar+camera baseline and 23$\times$ compared to monocular depth estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07436">
<div class="article-summary-box-inner">
<span><p>Shapley values are widely used to explain black-box models, but they are
costly to calculate because they require many model evaluations. We introduce
FastSHAP, a method for estimating Shapley values in a single forward pass using
a learned explainer model. FastSHAP amortizes the cost of explaining many
inputs via a learning approach inspired by the Shapley value's weighted least
squares characterization, and it can be trained using standard stochastic
gradient optimization. We compare FastSHAP to existing estimation approaches,
revealing that it generates high-quality explanations with orders of magnitude
speedup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CycleMLP: A MLP-like Architecture for Dense Prediction. (arXiv:2107.10224v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10224">
<div class="article-summary-box-inner">
<span><p>This paper presents a simple MLP-like architecture, CycleMLP, which is a
versatile backbone for visual recognition and dense predictions. As compared to
modern MLP architectures, e.g., MLP-Mixer, ResMLP, and gMLP, whose
architectures are correlated to image size and thus are infeasible in object
detection and segmentation, CycleMLP has two advantages compared to modern
approaches. (1) It can cope with various image sizes. (2) It achieves linear
computational complexity to image size by using local windows. In contrast,
previous MLPs have $O(N^2)$ computations due to fully spatial connections. We
build a family of models which surpass existing MLPs and even state-of-the-art
Transformer-based models, e.g., Swin Transformer, while using fewer parameters
and FLOPs. We expand the MLP-like models' applicability, making them a
versatile backbone for dense prediction tasks. CycleMLP achieves competitive
results on object detection, instance segmentation, and semantic segmentation.
In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K
dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot
robustness on ImageNet-C dataset. Code is available at
https://github.com/ShoufaChen/CycleMLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RGB Image Classification with Quantum Convolutional Ansaetze. (arXiv:2107.11099v2 [quant-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11099">
<div class="article-summary-box-inner">
<span><p>With the rapid growth of qubit numbers and coherence times in quantum
hardware technology, implementing shallow neural networks on the so-called
Noisy Intermediate-Scale Quantum (NISQ) devices has attracted a lot of
interest. Many quantum (convolutional) circuit ansaetze are proposed for
grayscale images classification tasks with promising empirical results.
However, when applying these ansaetze on RGB images, the intra-channel
information that is useful for vision tasks is not extracted effectively. In
this paper, we propose two types of quantum circuit ansaetze to simulate
convolution operations on RGB images, which differ in the way how inter-channel
and intra-channel information are extracted. To the best of our knowledge, this
is the first work of a quantum convolutional circuit to deal with RGB images
effectively, with a higher test accuracy compared to the purely classical CNNs.
We also investigate the relationship between the size of quantum circuit ansatz
and the learnability of the hybrid quantum-classical convolutional neural
network. Through experiments based on CIFAR-10 and MNIST datasets, we
demonstrate that a larger size of the quantum circuit ansatz improves
predictive performance in multiclass classification tasks, providing useful
insights for near term quantum algorithm developments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dynamic 3D Spontaneous Micro-expression Database: Establishment and Evaluation. (arXiv:2108.00166v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00166">
<div class="article-summary-box-inner">
<span><p>Micro-expressions are spontaneous, unconscious facial movements that show
people's true inner emotions and have great potential in related fields of
psychological testing. Since the face is a 3D deformation object, the
occurrence of an expression can arouse spatial deformation of the face, but
limited by the available databases are 2D videos, lacking the description of 3D
spatial information of micro-expressions. Therefore, we proposed a new
micro-expression database containing 2D video sequences and 3D point clouds
sequences. The database includes 373 micro-expressions sequences, and these
samples were classified using the objective method based on facial action
coding system, as well as the non-objective method that combines video contents
and participants' self-reports. We extracted 2D and 3D features using the local
binary patterns on three orthogonal planes (LBP-TOP) and curvature algorithms,
respectively, and evaluated the classification accuracies of these two features
and their fusion results with leave-one-subject-out (LOSO) and 10-fold
cross-validation. Further, we performed various neural network algorithms for
database classification, the results show that classification accuracies are
improved by fusing 3D features than using only 2D features. The database offers
original and cropped micro-expression samples, which will facilitate the
exploration and research on 3D Spatio-temporal features of micro-expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ultrafast Focus Detection for Automated Microscopy. (arXiv:2108.12050v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12050">
<div class="article-summary-box-inner">
<span><p>Technological advancements in modern scientific instruments, such as scanning
electron microscopes (SEMs), have significantly increased data acquisition
rates and image resolutions enabling new questions to be explored; however, the
resulting data volumes and velocities, combined with automated experiments, are
quickly overwhelming scientists as there remain crucial steps that require
human intervention, for example reviewing image focus. We present a fast
out-of-focus detection algorithm for electron microscopy images collected
serially and demonstrate that it can be used to provide near-real-time quality
control for neuroscience workflows. Our technique, \textit{Multi-scale
Histologic Feature Detection}, adapts classical computer vision techniques and
is based on detecting various fine-grained histologic features. We exploit the
inherent parallelism in the technique to employ GPU primitives in order to
accelerate characterization. We show that our method can detect of out-of-focus
conditions within just 20ms. To make these capabilities generally available, we
deploy our feature detector as an on-demand service and show that it can be
used to determine the degree of focus in approximately 230ms, enabling
near-real-time use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-based Medical Image Segmentation using Matrix Product State Tensor Networks. (arXiv:2109.07138v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07138">
<div class="article-summary-box-inner">
<span><p>Tensor networks are efficient factorisations of high-dimensional tensors into
a network of lower-order tensors. They have been most commonly used to model
entanglement in quantum many-body systems and more recently are witnessing
increased applications in supervised machine learning. In this work, we
formulate image segmentation in a supervised setting with tensor networks. The
key idea is to first lift the pixels in image patches to exponentially
high-dimensional feature spaces and using a linear decision hyper-plane to
classify the input pixels into foreground and background classes. The
high-dimensional linear model itself is approximated using the matrix product
state (MPS) tensor network. The MPS is weight-shared between the
non-overlapping image patches resulting in our strided tensor network model.
The performance of the proposed model is evaluated on three 2D- and one 3D-
biomedical imaging datasets. The performance of the proposed tensor network
segmentation model is compared with relevant baseline methods. In the 2D
experiments, the tensor network model yields competitive performance compared
to the baseline methods while being more resource efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image Translation. (arXiv:2109.12492v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12492">
<div class="article-summary-box-inner">
<span><p>Recently, there has been an increasing interest in image editing methods that
employ pre-trained unconditional image generators (e.g., StyleGAN). However,
applying these methods to translate images to multiple visual domains remains
challenging. Existing works do not often preserve the domain-invariant part of
the image (e.g., the identity in human face translations), they do not usually
handle multiple domains, or do not allow for multi-modal translations. This
work proposes an implicit style function (ISF) to straightforwardly achieve
multi-modal and multi-domain image-to-image translation from pre-trained
unconditional generators. The ISF manipulates the semantics of an input latent
code to make the image generated from it lying in the desired visual domain.
Our results in human face and animal manipulations show significantly improved
results over the baselines. Our model enables cost-effective multi-modal
unsupervised image-to-image translations at high resolution using pre-trained
unconditional GANs. The code and data are available at:
\url{https://github.com/yhlleo/stylegan-mmuit}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Cluster Separation Using High-Dimensional Sharpened Dimensionality Reduction. (arXiv:2110.00317v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00317">
<div class="article-summary-box-inner">
<span><p>Applying dimensionality reduction (DR) to large, high-dimensional data sets
can be challenging when distinguishing the underlying high-dimensional data
clusters in a 2D projection for exploratory analysis. We address this problem
by first sharpening the clusters in the original high-dimensional data prior to
the DR step using Local Gradient Clustering (LGC). We then project the
sharpened data from the high-dimensional space to 2D by a user-selected DR
method. The sharpening step aids this method to preserve cluster separation in
the resulting 2D projection. With our method, end-users can label each distinct
cluster to further analyze an otherwise unlabeled data set. Our
`High-Dimensional Sharpened DR' (HD-SDR) method, tested on both synthetic and
real-world data sets, is favorable to DR methods with poor cluster separation
and yields a better visual cluster separation than these DR methods with no
sharpening. Our method achieves good quality (measured by quality metrics) and
scales computationally well with large high-dimensional data. To illustrate its
concrete applications, we further apply HD-SDR on a recent astronomical
catalog.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Regress Bodies from Images using Differentiable Semantic Rendering. (arXiv:2110.03480v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03480">
<div class="article-summary-box-inner">
<span><p>Learning to regress 3D human body shape and pose (e.g.~SMPL parameters) from
monocular images typically exploits losses on 2D keypoints, silhouettes, and/or
part-segmentation when 3D training data is not available. Such losses, however,
are limited because 2D keypoints do not supervise body shape and segmentations
of people in clothing do not match projected minimally-clothed SMPL shapes. To
exploit richer image information about clothed people, we introduce
higher-level semantic information about clothing to penalize clothed and
non-clothed regions of the image differently. To do so, we train a body
regressor using a novel Differentiable Semantic Rendering - DSR loss. For
Minimally-Clothed regions, we define the DSR-MC loss, which encourages a tight
match between a rendered SMPL body and the minimally-clothed regions of the
image. For clothed regions, we define the DSR-C loss to encourage the rendered
SMPL body to be inside the clothing mask. To ensure end-to-end differentiable
training, we learn a semantic clothing prior for SMPL vertices from thousands
of clothed human scans. We perform extensive qualitative and quantitative
experiments to evaluate the role of clothing semantics on the accuracy of 3D
human pose and shape estimation. We outperform all previous state-of-the-art
methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code
and trained models are available for research at https://dsr.is.tue.mpg.de/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handwritten Mathematical Expression Recognition via Attention Aggregation based Bi-directional Mutual Learning. (arXiv:2112.03603v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03603">
<div class="article-summary-box-inner">
<span><p>Handwritten mathematical expression recognition aims to automatically
generate LaTeX sequences from given images. Currently, attention-based
encoder-decoder models are widely used in this task. They typically generate
target sequences in a left-to-right (L2R) manner, leaving the right-to-left
(R2L) contexts unexploited. In this paper, we propose an Attention aggregation
based Bi-directional Mutual learning Network (ABM) which consists of one shared
encoder and two parallel inverse decoders (L2R and R2L). The two decoders are
enhanced via mutual distillation, which involves one-to-one knowledge transfer
at each training step, making full use of the complementary information from
two inverse directions. Moreover, in order to deal with mathematical symbols in
diverse scales, an Attention Aggregation Module (AAM) is proposed to
effectively integrate multi-scale coverage attentions. Notably, in the
inference phase, given that the model already learns knowledge from two inverse
directions, we only use the L2R branch for inference, keeping the original
parameter size and inference speed. Extensive experiments demonstrate that our
proposed approach achieves the recognition accuracy of 56.85 % on CROHME 2014,
52.92 % on CROHME 2016, and 53.96 % on CROHME 2019 without data augmentation
and model ensembling, substantially outperforming the state-of-the-art methods.
The source code is available in https://github.com/XH-B/ABM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Exploring Pose Estimation as an Auxiliary Learning Task for Visible-Infrared Person Re-identification. (arXiv:2201.03859v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03859">
<div class="article-summary-box-inner">
<span><p>Visible-infrared person re-identification (VI-ReID) has been challenging due
to the existence of large discrepancies between visible and infrared
modalities. Most pioneering approaches reduce intra-class variations and
inter-modality discrepancies by learning modality-shared and ID-related
features. However, an explicit modality-shared cue, i.e., body keypoints, has
not been fully exploited in VI-ReID. Additionally, existing feature learning
paradigms imposed constraints on either global features or partitioned feature
stripes, which neglect the prediction consistency of global and part features.
To address the above problems, we exploit Pose Estimation as an auxiliary
learning task to assist the VI-ReID task in an end-to-end framework. By jointly
training these two tasks in a mutually beneficial manner, our model learns
higher quality modality-shared and ID-related features. On top of it, the
learnings of global features and local features are seamlessly synchronized by
Hierarchical Feature Constraint (HFC), where the former supervises the latter
using the knowledge distillation strategy. Experimental results on two
benchmark VI-ReID datasets show that the proposed method consistently improves
state-of-the-art methods by significant margins. Specifically, our method
achieves nearly 20$\%$ mAP improvements against the state-of-the-art method on
the RegDB dataset. Our intriguing findings highlight the usage of auxiliary
task learning in VI-ReID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GradMax: Growing Neural Networks using Gradient Information. (arXiv:2201.05125v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05125">
<div class="article-summary-box-inner">
<span><p>The architecture and the parameters of neural networks are often optimized
independently, which requires costly retraining of the parameters whenever the
architecture is modified. In this work we instead focus on growing the
architecture without requiring costly retraining. We present a method that adds
new neurons during training without impacting what is already learned, while
improving the training dynamics. We achieve the latter by maximizing the
gradients of the new weights and find the optimal initialization efficiently by
means of the singular value decomposition (SVD). We call this technique
Gradient Maximizing Growth (GradMax) and demonstrate its effectiveness in
variety of vision tasks and architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Inexact Unlearning Requires Revisiting Forgetting. (arXiv:2201.06640v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06640">
<div class="article-summary-box-inner">
<span><p>Existing methods in inexact unlearning are evaluated by measuring
indistinguishability from models retrained after removing the deletion set. We
argue that achieving indistinguishability is unnecessary and its practical
relaxations are insufficient. We formulate the goal of unlearning as forgetting
all information specific to the deletion set while maintaining high utility and
resource efficiency.
</p>
<p>We introduce a novel test for forgetting called Interclass Confusion (IC).
Despite being a black-box test, IC can investigate whether information from the
deletion set was erased until the early layers of the network. We analyze two
aspects of forgetting: (i) memorization and (ii) property generalization. We
empirically show that two simple unlearning methods, exact-unlearning and
catastrophic-forgetting the final k layers of a network, outperforms prior
unlearning methods when scaled to large deletion sets. Overall, we believe our
formulation and the IC test will guide the design of better unlearning
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLA-NeRF: Category-Level Articulated Neural Radiance Field. (arXiv:2202.00181v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00181">
<div class="article-summary-box-inner">
<span><p>We propose CLA-NeRF -- a Category-Level Articulated Neural Radiance Field
that can perform view synthesis, part segmentation, and articulated pose
estimation. CLA-NeRF is trained at the object category level using no CAD
models and no depth, but a set of RGB images with ground truth camera poses and
part segments. During inference, it only takes a few RGB views (i.e., few-shot)
of an unseen 3D object instance within the known category to infer the object
part segmentation and the neural radiance field. Given an articulated pose as
input, CLA-NeRF can perform articulation-aware volume rendering to generate the
corresponding RGB image at any camera pose. Moreover, the articulated pose of
an object can be estimated via inverse rendering. In our experiments, we
evaluate the framework across five categories on both synthetic and real-world
data. In all cases, our method shows realistic deformation results and accurate
articulated pose estimation. We believe that both few-shot articulated object
rendering and articulated pose estimation open doors for robots to perceive and
interact with unseen articulated objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guide Local Feature Matching by Overlap Estimation. (arXiv:2202.09050v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09050">
<div class="article-summary-box-inner">
<span><p>Local image feature matching under large appearance, viewpoint, and distance
changes is challenging yet important. Conventional methods detect and match
tentative local features across the whole images, with heuristic consistency
checks to guarantee reliable matches. In this paper, we introduce a novel
Overlap Estimation method conditioned on image pairs with TRansformer, named
OETR, to constrain local feature matching in the commonly visible region. OETR
performs overlap estimation in a two-step process of feature correlation and
then overlap regression. As a preprocessing module, OETR can be plugged into
any existing local feature detection and matching pipeline, to mitigate
potential view angle or scale variance. Intensive experiments show that OETR
can boost state-of-the-art local feature matching performance substantially,
especially for image pairs with small shared regions. The code will be publicly
available at https://github.com/AbyssGaze/OETR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RDP-Net: Region Detail Preserving Network for Change Detection. (arXiv:2202.09745v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09745">
<div class="article-summary-box-inner">
<span><p>Change detection (CD) is an essential earth observation technique. It
captures the dynamic information of land objects. With the rise of deep
learning, neural networks (NN) have shown great potential in CD. However,
current NN models introduce backbone architectures that lose the detail
information during learning. Moreover, current NN models are heavy in
parameters, which prevents their deployment on edge devices such as drones. In
this work, we tackle this issue by proposing RDP-Net: a region detail
preserving network for CD. We propose an efficient training strategy that
quantifies the importance of individual samples during the warmup period of NN
training. Then, we perform non-uniform sampling based on the importance score
so that the NN could learn detail information from easy to hard. Next, we
propose an effective edge loss that improves the network's attention on details
such as boundaries and small regions. As a result, we provide a NN model that
achieves the state-of-the-art empirical performance in CD with only 1.70M
parameters. We hope our RDP-Net would benefit the practical CD applications on
compact devices and could inspire more people to bring change detection to a
new level with the efficient training strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Feature based Cross-slide Registration. (arXiv:2202.09971v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09971">
<div class="article-summary-box-inner">
<span><p>Cross-slide image analysis provides additional information by analysing the
expression of different biomarkers as compared to a single slide analysis.
Slides stained with different biomarkers are analysed side by side which may
reveal unknown relations between the different biomarkers. During the slide
preparation, a tissue section may be placed at an arbitrary orientation as
compared to other sections of the same tissue block. The problem is compounded
by the fact that tissue contents are likely to change from one section to the
next and there may be unique artefacts on some of the slides. This makes
registration of each section to a reference section of the same tissue block an
important pre-requisite task before any cross-slide analysis. We propose a deep
feature based registration (DFBR) method which utilises data-driven features to
estimate the rigid transformation. We adopted a multi-stage strategy for
improving the quality of registration. We also developed a visualisation tool
to view registered pairs of WSIs at different magnifications. With the help of
this tool, one can apply a transformation on the fly without the need to
generate transformed source WSI in a pyramidal form. We compared the
performance of data-driven features with that of hand-crafted features on the
COMET dataset. Our approach can align the images with low registration errors.
Generally, the success of non-rigid registration is dependent on the quality of
rigid registration. To evaluate the efficacy of the DFBR method, the first two
steps of the ANHIR winner's framework are replaced with our DFBR to register
challenge provided image pairs. The modified framework produce comparable
results to that of challenge winning team.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Winning Solution to the iFLYTEK Challenge 2021 Cultivated Land Extraction from High-Resolution Remote Sensing Image. (arXiv:2202.10974v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10974">
<div class="article-summary-box-inner">
<span><p>Extracting cultivated land accurately from high-resolution remote images is a
basic task for precision agriculture. This report introduces our solution to
the iFLYTEK challenge 2021 cultivated land extraction from high-resolution
remote sensing image. The challenge requires segmenting cultivated land objects
in very high-resolution multispectral remote sensing images. We established a
highly effective and efficient pipeline to solve this problem. We first divided
the original images into small tiles and separately performed instance
segmentation on each tile. We explored several instance segmentation algorithms
that work well on natural images and developed a set of effective methods that
are applicable to remote sensing images. Then we merged the prediction results
of all small tiles into seamless, continuous segmentation results through our
proposed overlap-tile fusion strategy. We achieved the first place among 486
teams in the challenge.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-24 23:08:10.858801498 UTC">2022-02-24 23:08:10 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>