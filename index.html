<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-11T01:30:00Z">07-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation. (arXiv:2207.03509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03509">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models (PLMs) are often domain- or task-adapted via
fine-tuning or prompting. Finetuning requires modifying all of the parameters
and having enough data to avoid overfitting while prompting requires no
training and few examples but limits performance. Instead, we prepare PLMs for
data- and parameter-efficient adaptation by learning to learn the difference
between general and adapted PLMs. This difference is expressed in terms of
model weights and sublayer structure through our proposed dynamic low-rank
reparameterization and learned architecture controller. Experiments on few-shot
dialogue completion, low-resource abstractive summarization, and multi-domain
language modeling show improvements in adaptation time and performance over
direct finetuning or preparation via domain-adaptive pretraining. Ablations
show our task-adaptive reparameterization (TARP) and model search (TAMS)
components individually improve on other parameter-efficient transfer like
adapters and structure-learning methods like learned sparsification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus. (arXiv:2207.03546v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03546">
<div class="article-summary-box-inner">
<span><p>BibleTTS is a large, high-quality, open speech dataset for ten languages
spoken in Sub-Saharan Africa. The corpus contains up to 86 hours of aligned,
studio quality 48kHz single speaker recordings per language, enabling the
development of high-quality text-to-speech models. The ten languages
represented are: Akuapem Twi, Asante Twi, Chichewa, Ewe, Hausa, Kikuyu,
Lingala, Luganda, Luo, and Yoruba. This corpus is a derivative work of Bible
recordings made and released by the Open.Bible project from Biblica. We have
aligned, cleaned, and filtered the original recordings, and additionally
hand-checked a subset of the alignments for each language. We present results
for text-to-speech models with Coqui TTS. The data is released under a
commercial-friendly CC-BY-SA license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Translation with Compiler Representations. (arXiv:2207.03578v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03578">
<div class="article-summary-box-inner">
<span><p>In this paper, we leverage low-level compiler intermediate representations
(IR) to improve code translation. Traditional transpilers rely on syntactic
information and handcrafted rules, which limits their applicability and
produces unnatural-looking code. Applying neural machine translation (NMT)
approaches to code has successfully broadened the set of programs on which one
can get a natural-looking translation. However, they treat the code as
sequences of text tokens, and still do not differentiate well enough between
similar pieces of code which have different semantics in different languages.
The consequence is low quality translation, reducing the practicality of NMT,
and stressing the need for an approach significantly increasing its accuracy.
Here we propose to augment code translation with IRs, specifically LLVM IR,
with results on the C++, Java, Rust, and Go languages. Our method improves upon
the state of the art for unsupervised code translation, increasing the number
of correct translations by 11% on average, and up to 79% for the Java - Rust
pair. We extend previous test sets for code translation, by adding hundreds of
Go and Rust functions. Additionally, we train models with high performance on
the problem of IR decompilation, generating programming source code from IR,
and study using IRs as intermediary pivot for translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quote Erat Demonstrandum: A Web Interface for Exploring the Quotebank Corpus. (arXiv:2207.03592v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03592">
<div class="article-summary-box-inner">
<span><p>The use of attributed quotes is the most direct and least filtered pathway of
information propagation in news. Consequently, quotes play a central role in
the conception, reception, and analysis of news stories. Since quotes provide a
more direct window into a speaker's mind than regular reporting, they are a
valuable resource for journalists and researchers alike. While substantial
research efforts have been devoted to methods for the automated extraction of
quotes from news and their attribution to speakers, few comprehensive corpora
of attributed quotes from contemporary sources are available to the public.
Here, we present an adaptive web interface for searching Quotebank, a massive
collection of quotes from the news, which we make available at
https://quotebank.dlab.tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. (arXiv:2207.03637v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03637">
<div class="article-summary-box-inner">
<span><p>The information in tables can be an important complement to text, making
table-based question answering (QA) systems of great value. The intrinsic
complexity of handling tables often adds an extra burden to both model design
and data annotation. In this paper, we aim to develop a simple table-based QA
model with minimal annotation effort. Motivated by the fact that table-based QA
requires both alignment between questions and tables and the ability to perform
complicated reasoning over multiple table elements, we propose an omnivorous
pretraining approach that consumes both natural and synthetic data to endow
models with these respective abilities. Specifically, given freely available
tables, we leverage retrieval to pair them with relevant natural sentences for
mask-based pretraining, and synthesize NL questions by converting SQL sampled
from tables for pretraining with a QA loss. We perform extensive experiments in
both few-shot and full settings, and the results clearly demonstrate the
superiority of our model OmniTab, with the best multitasking approach achieving
an absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively,
also establishing a new state-of-the-art on WikiTableQuestions. Detailed
ablations and analyses reveal different characteristics of natural and
synthetic data, shedding light on future directions in omnivorous pretraining.
Code, pretraining data, and pretrained models are available at
https://github.com/jzbjyb/OmniTab.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SETSum: Summarization and Visualization of Student Evaluations of Teaching. (arXiv:2207.03640v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03640">
<div class="article-summary-box-inner">
<span><p>Student Evaluations of Teaching (SETs) are widely used in colleges and
universities. Typically SET results are summarized for instructors in a static
PDF report. The report often includes summary statistics for quantitative
ratings and an unsorted list of open-ended student comments. The lack of
organization and summarization of the raw comments hinders those interpreting
the reports from fully utilizing informative feedback, making accurate
inferences, and designing appropriate instructional improvements. In this work,
we introduce a novel system, SETSum, that leverages sentiment analysis, aspect
extraction, summarization, and visualization techniques to provide organized
illustrations of SET findings to instructors and other reviewers. Ten
university professors from diverse departments serve as evaluators of the
system and all agree that SETSum helps them interpret SET results more
efficiently; and 6 out of 10 instructors prefer our system over the standard
static PDF report (while the remaining 4 would like to have both). This
demonstrates that our work holds the potential to reform the SET reporting
conventions in the future. Our code is available at
https://github.com/evahuyn/SETSum
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Getting BART to Ride the Idiomatic Train: Learning to Represent Idiomatic Expressions. (arXiv:2207.03679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03679">
<div class="article-summary-box-inner">
<span><p>Idiomatic expressions (IEs), characterized by their non-compositionality, are
an important part of natural language. They have been a classical challenge to
NLP, including pre-trained language models that drive today's state-of-the-art.
Prior work has identified deficiencies in their contextualized representation
stemming from the underlying compositional paradigm of representation. In this
work, we take a first-principles approach to build idiomaticity into BART using
an adapter as a lightweight non-compositional language expert trained on
idiomatic sentences. The improved capability over baselines (e.g., BART) is
seen via intrinsic and extrinsic methods, where idiom embeddings score 0.19
points higher in homogeneity score for embedding clustering, and up to 25%
higher sequence accuracy on the idiom processing tasks of IE sense
disambiguation and span detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crake: Causal-Enhanced Table-Filler for Question Answering over Large Scale Knowledge Base. (arXiv:2207.03680v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03680">
<div class="article-summary-box-inner">
<span><p>Semantic parsing solves knowledge base (KB) question answering (KBQA) by
composing a KB query, which generally involves node extraction (NE) and graph
composition (GC) to detect and connect related nodes in a query. Despite the
strong causal effects between NE and GC, previous works fail to directly model
such causalities in their pipeline, hindering the learning of subtask
correlations. Also, the sequence-generation process for GC in previous works
induces ambiguity and exposure bias, which further harms accuracy. In this
work, we formalize semantic parsing into two stages. In the first stage (graph
structure generation), we propose a causal-enhanced table-filler to overcome
the issues in sequence-modelling and to learn the internal causalities. In the
second stage (relation extraction), an efficient beam-search algorithm is
presented to scale complex queries on large-scale KBs. Experiments on LC-QuAD
1.0 indicate that our method surpasses previous state-of-the-arts by a large
margin (17%) while remaining time and space efficiency. The code and models are
available at https://github.com/AOZMH/Crake.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hidden Schema Networks. (arXiv:2207.03777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03777">
<div class="article-summary-box-inner">
<span><p>Most modern language models infer representations that, albeit powerful, lack
both compositionality and semantic interpretability. Starting from the
assumption that a large proportion of semantic content is necessarily
relational, we introduce a neural language model that discovers networks of
symbols (schemata) from text datasets. Using a variational autoencoder (VAE)
framework, our model encodes sentences into sequences of symbols (composed
representation), which correspond to the nodes visited by biased random walkers
on a global latent graph. Sentences are then generated back, conditioned on the
selected symbol sequences. We first demonstrate that the model is able to
uncover ground-truth graphs from artificially generated datasets of random
token sequences. Next we leverage pretrained BERT and GPT-2 language models as
encoder and decoder, respectively, to train our model on language modelling
tasks. Qualitatively, our results show that the model is able to infer schema
networks encoding different aspects of natural language. Quantitatively, the
model achieves state-of-the-art scores on VAE language modeling benchmarks.
Source code to reproduce our experiments is available at
https://github.com/ramsesjsf/HiddenSchemaNetworks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis. (arXiv:2207.03800v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03800">
<div class="article-summary-box-inner">
<span><p>Unconstrained lip-to-speech synthesis aims to generate corresponding speeches
from silent videos of talking faces with no restriction on head poses or
vocabulary. Current works mainly use sequence-to-sequence models to solve this
problem, either in an autoregressive architecture or a flow-based
non-autoregressive architecture. However, these models suffer from several
drawbacks: 1) Instead of directly generating audios, they use a two-stage
pipeline that first generates mel-spectrograms and then reconstructs audios
from the spectrograms. This causes cumbersome deployment and degradation of
speech quality due to error propagation; 2) The audio reconstruction algorithm
used by these models limits the inference speed and audio quality, while neural
vocoders are not available for these models since their output spectrograms are
not accurate enough; 3) The autoregressive model suffers from high inference
latency, while the flow-based model has high memory occupancy: neither of them
is efficient enough in both time and memory usage. To tackle these problems, we
propose FastLTS, a non-autoregressive end-to-end model which can directly
synthesize high-quality speech audios from unconstrained talking videos with
low latency, and has a relatively small model size. Besides, different from the
widely used 3D-CNN visual frontend for lip movement encoding, we for the first
time propose a transformer-based visual frontend for this task. Experiments
show that our model achieves $19.76\times$ speedup for audio waveform
generation compared with the current autoregressive model on input sequences of
3 seconds, and obtains superior audio quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficiency Study for SPLADE Models. (arXiv:2207.03834v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03834">
<div class="article-summary-box-inner">
<span><p>Latency and efficiency issues are often overlooked when evaluating IR models
based on Pretrained Language Models (PLMs) in reason of multiple hardware and
software testing scenarios. Nevertheless, efficiency is an important part of
such systems and should not be overlooked.
</p>
<p>In this paper, we focus on improving the efficiency of the SPLADE model since
it has achieved state-of-the-art zero-shot performance and competitive results
on TREC collections. SPLADE efficiency can be controlled via a regularization
factor, but solely controlling this regularization has been shown to not be
efficient enough. In order to reduce the latency gap between SPLADE and
traditional retrieval systems, we propose several techniques including L1
regularization for queries, a separation of document/query encoders, a
FLOPS-regularized middle-training, and the use of faster query encoders. Our
benchmark demonstrates that we can drastically improve the efficiency of these
models while increasing the performance metrics on in-domain data. To our
knowledge, {we propose the first neural models that, under the same computing
constraints, \textit{achieve similar latency (less than 4ms difference) as
traditional BM25}, while having \textit{similar performance (less than 10\%
MRR@10 reduction)} as the state-of-the-art single-stage neural rankers on
in-domain data}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSTEA: Dialogue State Tracking with Entity Adaptive Pre-training. (arXiv:2207.03858v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03858">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking (DST) is a core sub-module of a dialogue system,
which aims to extract the appropriate belief state (domain-slot-value) from a
system and user utterances. Most previous studies have attempted to improve
performance by increasing the size of the pre-trained model or using additional
features such as graph relations. In this study, we propose dialogue state
tracking with entity adaptive pre-training (DSTEA), a system in which key
entities in a sentence are more intensively trained by the encoder of the DST
model. DSTEA extracts important entities from input dialogues in four ways, and
then applies selective knowledge masking to train the model effectively.
Although DSTEA conducts only pre-training without directly infusing additional
knowledge to the DST model, it achieved better performance than the best-known
benchmark models on MultiWOZ 2.0, 2.1, and 2.2. The effectiveness of DSTEA was
verified through various comparative experiments with regard to the entity type
and different adaptive settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Medical Information Extraction Workbench to Process German Clinical Text. (arXiv:2207.03885v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03885">
<div class="article-summary-box-inner">
<span><p>Background: In the information extraction and natural language processing
domain, accessible datasets are crucial to reproduce and compare results.
Publicly available implementations and tools can serve as benchmark and
facilitate the development of more complex applications. However, in the
context of clinical text processing the number of accessible datasets is scarce
-- and so is the number of existing tools. One of the main reasons is the
sensitivity of the data. This problem is even more evident for non-English
languages.
</p>
<p>Approach: In order to address this situation, we introduce a workbench: a
collection of German clinical text processing models. The models are trained on
a de-identified corpus of German nephrology reports.
</p>
<p>Result: The presented models provide promising results on in-domain data.
Moreover, we show that our models can be also successfully applied to other
biomedical text in German. Our workbench is made publicly available so it can
be used out of the box, as a benchmark or transferred to related problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Approach to Ensure Fairness in News Articles. (arXiv:2207.03938v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03938">
<div class="article-summary-box-inner">
<span><p>Recommender systems, information retrieval, and other information access
systems present unique challenges for examining and applying concepts of
fairness and bias mitigation in unstructured text. This paper introduces Dbias,
which is a Python package to ensure fairness in news articles. Dbias is a
trained Machine Learning (ML) pipeline that can take a text (e.g., a paragraph
or news story) and detects if the text is biased or not. Then, it detects the
biased words in the text, masks them, and recommends a set of sentences with
new words that are bias-free or at least less biased. We incorporate the
elements of data science best practices to ensure that this pipeline is
reproducible and usable. We show in experiments that this pipeline can be
effective for mitigating biases and outperforms the common neural network
architectures in ensuring fairness in the news articles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination. (arXiv:2207.03961v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03961">
<div class="article-summary-box-inner">
<span><p>As humans, we can modify our assumptions about a scene by imagining
alternative objects or concepts in our minds. For example, we can easily
anticipate the implications of the sun being overcast by rain clouds (e.g., the
street will get wet) and accordingly prepare for that. In this paper, we
introduce a new task/dataset called Commonsense Reasoning for Counterfactual
Scene Imagination (CoSIm) which is designed to evaluate the ability of AI
systems to reason about scene change imagination. In this task/dataset, models
are given an image and an initial question-response pair about the image. Next,
a counterfactual imagined scene change (in textual form) is applied, and the
model has to predict the new response to the initial question based on this
scene change. We collect 3.5K high-quality and challenging data instances, with
each instance consisting of an image, a commonsense question with a response, a
description of a counterfactual change, a new response to the question, and
three distractor responses. Our dataset contains various complex scene change
types (such as object addition/removal/state change, event description,
environment change, etc.) that require models to imagine many different
scenarios and reason about the changed scenes. We present a baseline model
based on a vision-language Transformer (i.e., LXMERT) and ablation studies.
Through human evaluation, we demonstrate a large human-model performance gap,
suggesting room for promising future work on this challenging counterfactual,
scene imagination task. Our code and dataset are publicly available at:
https://github.com/hyounghk/CoSIm
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Time Like the Present: Effects of Language Change on Automated Comment Moderation. (arXiv:2207.04003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04003">
<div class="article-summary-box-inner">
<span><p>The spread of online hate has become a significant problem for newspapers
that host comment sections. As a result, there is growing interest in using
machine learning and natural language processing for (semi-) automated abusive
language detection to avoid manual comment moderation costs or having to shut
down comment sections altogether. However, much of the past work on abusive
language detection assumes that classifiers operate in a static language
environment, despite language and news being in a state of constant flux. In
this paper, we show using a new German newspaper comments dataset that the
classifiers trained with naive ML techniques like a random-test train split
will underperform on future data, and that a time stratified evaluation split
is more appropriate. We also show that classifier performance rapidly degrades
when evaluated on data from a different period than the training data. Our
findings suggest that it is necessary to consider the temporal dynamics of
language when developing an abusive language detection system or risk deploying
a model that will quickly become defunct.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ABB-BERT: A BERT model for disambiguating abbreviations and contractions. (arXiv:2207.04008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04008">
<div class="article-summary-box-inner">
<span><p>Abbreviations and contractions are commonly found in text across different
domains. For example, doctors' notes contain many contractions that can be
personalized based on their choices. Existing spelling correction models are
not suitable to handle expansions because of many reductions of characters in
words. In this work, we propose ABB-BERT, a BERT-based model, which deals with
an ambiguous language containing abbreviations and contractions. ABB-BERT can
rank them from thousands of options and is designed for scale. It is trained on
Wikipedia text, and the algorithm allows it to be fine-tuned with little
compute to get better performance for a domain or person. We are publicly
releasing the training dataset for abbreviations and contractions derived from
Wikipedia.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASL-Homework-RGBD Dataset: An annotated dataset of 45 fluent and non-fluent signers performing American Sign Language homeworks. (arXiv:2207.04021v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04021">
<div class="article-summary-box-inner">
<span><p>We are releasing a dataset containing videos of both fluent and non-fluent
signers using American Sign Language (ASL), which were collected using a Kinect
v2 sensor. This dataset was collected as a part of a project to develop and
evaluate computer vision algorithms to support new technologies for automatic
detection of ASL fluency attributes. A total of 45 fluent and non-fluent
participants were asked to perform signing homework assignments that are
similar to the assignments used in introductory or intermediate level ASL
courses. The data is annotated to identify several aspects of signing including
grammatical features and non-manual markers. Sign language recognition is
currently very data-driven and this dataset can support the design of
recognition technologies, especially technologies that can benefit ASL
learners. This dataset might also be interesting to ASL education researchers
who want to contrast fluent and non-fluent signing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications. (arXiv:2207.04043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04043">
<div class="article-summary-box-inner">
<span><p>Innovation is a major driver of economic and social development, and
information about many kinds of innovation is embedded in semi-structured data
from patents and patent applications. Although the impact and novelty of
innovations expressed in patent data are difficult to measure through
traditional means, ML offers a promising set of techniques for evaluating
novelty, summarizing contributions, and embedding semantics. In this paper, we
introduce the Harvard USPTO Patent Dataset (HUPD), a large-scale,
well-structured, and multi-purpose corpus of English-language patent
applications filed to the United States Patent and Trademark Office (USPTO)
between 2004 and 2018. With more than 4.5 million patent documents, HUPD is two
to three times larger than comparable corpora. Unlike previously proposed
patent datasets in NLP, HUPD contains the inventor-submitted versions of patent
applications--not the final versions of granted patents--thereby allowing us to
study patentability at the time of filing using NLP methods for the first time.
It is also novel in its inclusion of rich structured metadata alongside the
text of patent filings: By providing each application's metadata along with all
of its text fields, the dataset enables researchers to perform new sets of NLP
tasks that leverage variation in structured covariates. As a case study on the
types of research HUPD makes possible, we introduce a new task to the NLP
community--namely, binary classification of patent decisions. We additionally
show the structured metadata provided in the dataset enables us to conduct
explicit studies of concept shifts for this task. Finally, we demonstrate how
HUPD can be used for three additional tasks: multi-class classification of
patent subject areas, language modeling, and summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Sampling of Dependency Structures. (arXiv:2109.06521v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06521">
<div class="article-summary-box-inner">
<span><p>Probabilistic distributions over spanning trees in directed graphs are a
fundamental model of dependency structure in natural language processing,
syntactic dependency trees. In NLP, dependency trees often have an additional
root constraint: only one edge may emanate from the root. However, no sampling
algorithm has been presented in the literature to account for this additional
constraint. In this paper, we adapt two spanning tree sampling algorithms to
faithfully sample dependency trees from a graph subject to the root constraint.
Wilson (1996)'s sampling algorithm has a running time of $\mathcal{O}(H)$ where
$H$ is the mean hitting time of the graph. Colbourn (1996)'s sampling algorithm
has a running time of $\mathcal{O}(N^3)$, which is often greater than the mean
hitting time of a directed graph. Additionally, we build upon Colbourn's
algorithm and present a novel extension that can sample $K$ trees without
replacement in $\mathcal{O}(K N^3 + K^2 N)$ time. To the best of our knowledge,
no algorithm has been given for sampling spanning trees without replacement
from a directed graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">There is no rose without a thorn: Finding weaknesses on BlenderBot 2.0 in terms of Model, Data and User-Centric Approach. (arXiv:2201.03239v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03239">
<div class="article-summary-box-inner">
<span><p>BlenderBot 2.0 is a dialogue model that represents open-domain chatbots by
reflecting real-time information and remembering user information for an
extended period using an internet search module and multi-session. Nonetheless,
the model still has room for improvement. To this end, we examine BlenderBot
2.0 limitations and errors from three perspectives: model, data, and user. From
the data point of view, we highlight the unclear guidelines provided to workers
during the crowdsourcing process, as well as a lack of a process for refining
hate speech in the collected data and verifying the accuracy of internet-based
information. From a user perspective, we identify nine types of limitations of
BlenderBot 2.0, and their causes are thoroughly investigated. Furthermore, for
each point of view, we propose practical improvement methods and discuss
several potential future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning. (arXiv:2205.00731v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00731">
<div class="article-summary-box-inner">
<span><p>Machine reading comprehension has aroused wide concerns, since it explores
the potential of model for text understanding. To further equip the machine
with the reasoning capability, the challenging task of logical reasoning is
proposed. Previous works on logical reasoning have proposed some strategies to
extract the logical units from different aspects. However, there still remains
a challenge to model the long distance dependency among the logical units.
Also, it is demanding to uncover the logical structures of the text and further
fuse the discrete logic to the continuous text embedding. To tackle the above
issues, we propose an end-to-end model Logiformer which utilizes a two-branch
graph transformer network for logical reasoning of text. Firstly, we introduce
different extraction strategies to split the text into two sets of logical
units, and construct the logical graph and the syntax graph respectively. The
logical graph models the causal relations for the logical branch while the
syntax graph captures the co-occurrence relations for the syntax branch.
Secondly, to model the long distance dependency, the node sequence from each
graph is fed into the fully connected graph transformer structures. The two
adjacent matrices are viewed as the attention biases for the graph transformer
layers, which map the discrete logical structures to the continuous text
embedding space. Thirdly, a dynamic gate mechanism and a question-aware
self-attention module are introduced before the answer prediction to update the
features. The reasoning process provides the interpretability by employing the
logical units, which are consistent with human cognition. The experimental
results show the superiority of our model, which outperforms the
state-of-the-art single model on two logical reasoning benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Structured Span Selector. (arXiv:2205.03977v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03977">
<div class="article-summary-box-inner">
<span><p>Many natural language processing tasks, e.g., coreference resolution and
semantic role labeling, require selecting text spans and making decisions about
them.
</p>
<p>A typical approach to such tasks is to score all possible spans and greedily
select spans for task-specific downstream processing.
</p>
<p>This approach, however, does not incorporate any inductive bias about what
sort of spans ought to be selected, e.g., that selected spans tend to be
syntactic constituents.
</p>
<p>In this paper, we propose a novel grammar-based structured span selection
model which learns to make use of the partial span-level annotation provided
for such problems.
</p>
<p>Compared to previous approaches, our approach gets rid of the heuristic
greedy span selection scheme, allowing us to model the downstream task on an
optimal set of spans. We evaluate our model on two popular span prediction
tasks: coreference resolution and semantic role labeling.
</p>
<p>We show empirical improvements on both.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models. (arXiv:2206.11484v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11484">
<div class="article-summary-box-inner">
<span><p>This paper presents exploratory work on whether and to what extent biases
against queer and trans people are encoded in large language models (LLMs) such
as BERT. We also propose a method for reducing these biases in downstream
tasks: finetuning the models on data written by and/or about queer people. To
measure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,
modeled after other bias-detection benchmarks but addressing homophobic and
transphobic biases. We found that BERT shows significant homophobic bias, but
this bias can be mostly mitigated by finetuning BERT on a natural language
corpus written by members of the LGBTQ+ community.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolution Neural Network based Mode Decomposition for Degenerated Modes via Multiple Images from Polarizers. (arXiv:2207.03489v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03489">
<div class="article-summary-box-inner">
<span><p>In this paper, a mode decomposition (MD) method for degenerated modes has
been studied. Convolution neural network (CNN) has been applied for image
training and predicting the mode coefficients. Four-fold degenerated $LP_{11}$
series has been the target to be decomposed. Multiple images are regarded as an
input to decompose the degenerate modes. Total of seven different images,
including the full original near-field image, and images after linear
polarizers of four directions (0$^\circ$, 45$^\circ$, 90$^\circ$, and
135$^\circ$), and images after two circular polarizers (right-handed and
left-handed) has been considered for training, validation, and test. The output
label of the model has been chosen as the real and imaginary components of the
mode coefficient, and the loss function has been selected to be the
root-mean-square (RMS) of the labels. The RMS and mean-absolute-error (MAE) of
the label, intensity, phase, and field correlation between the actual and
predicted values have been selected to be the metrics to evaluate the CNN
model. The CNN model has been trained with 100,000 three-dimensional images
with depths of three, four, and seven. The performance of the trained model was
evaluated via 10,000 test samples with four sets of images - images after three
linear polarizers (0$^\circ$, 45$^\circ$, 90$^\circ$) and image after
right-handed circular polarizer - showed 0.0634 of label RMS, 0.0292 of
intensity RMS, 0.1867 rad of phase MAE, and 0.9978 of average field
correlation. The performance of 4 image sets showed at least 50.68\% of
performance enhancement compared to models considering only images after linear
polarizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">False Negative Reduction in Semantic Segmentation under Domain Shift using Depth Estimation. (arXiv:2207.03513v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03513">
<div class="article-summary-box-inner">
<span><p>State-of-the-art deep neural networks demonstrate outstanding performance in
semantic segmentation. However, their performance is tied to the domain
represented by the training data. Open world scenarios cause inaccurate
predictions which is hazardous in safety relevant applications like automated
driving. In this work, we enhance semantic segmentation predictions using
monocular depth estimation to improve segmentation by reducing the occurrence
of non-detected objects in presence of domain shift. To this end, we infer a
depth heatmap via a modified segmentation network which generates
foreground-background masks, operating in parallel to a given semantic
segmentation network. Both segmentation masks are aggregated with a focus on
foreground classes (here road users) to reduce false negatives. To also reduce
the occurrence of false positives, we apply a pruning based on uncertainty
estimates. Our approach is modular in a sense that it post-processes the output
of any semantic segmentation network. In our experiments, we observe less
non-detected objects of most important classes and an enhanced generalization
to other domains compared to the basic semantic segmentation prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Should All Proposals be Treated Equally in Object Detection?. (arXiv:2207.03520v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03520">
<div class="article-summary-box-inner">
<span><p>The complexity-precision trade-off of an object detector is a critical
problem for resource constrained vision tasks. Previous works have emphasized
detectors implemented with efficient backbones. The impact on this trade-off of
proposal processing by the detection head is investigated in this work. It is
hypothesized that improved detection efficiency requires a paradigm shift,
towards the unequal processing of proposals, assigning more computation to good
proposals than poor ones. This results in better utilization of available
computational budget, enabling higher accuracy for the same FLOPS. We formulate
this as a learning problem where the goal is to assign operators to proposals,
in the detection head, so that the total computational cost is constrained and
the precision is maximized. The key finding is that such matching can be
learned as a function that maps each proposal embedding into a one-hot code
over operators. While this function induces a complex dynamic network routing
mechanism, it can be implemented by a simple MLP and learned end-to-end with
off-the-shelf object detectors. This 'dynamic proposal processing' (DPP) is
shown to outperform state-of-the-art end-to-end object detectors (DETR, Sparse
R-CNN) by a clear margin for a given computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RWT-SLAM: Robust Visual SLAM for Highly Weak-textured Environments. (arXiv:2207.03539v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03539">
<div class="article-summary-box-inner">
<span><p>As a fundamental task for intelligent robots, visual SLAM has made great
progress over the past decades. However, robust SLAM under highly weak-textured
environments still remains very challenging. In this paper, we propose a novel
visual SLAM system named RWT-SLAM to tackle this problem. We modify LoFTR
network which is able to produce dense point matching under low-textured scenes
to generate feature descriptors. To integrate the new features into the popular
ORB-SLAM framework, we develop feature masks to filter out the unreliable
features and employ KNN strategy to strengthen the matching robustness. We also
retrained visual vocabulary upon new descriptors for efficient loop closing.
The resulting RWT-SLAM is tested in various public datasets such as TUM and
OpenLORIS, as well as our own data. The results shows very promising
performance under highly weak-textured environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highlight Specular Reflection Separation based on Tensor Low-rank and Sparse Decomposition Using Polarimetric Cues. (arXiv:2207.03543v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03543">
<div class="article-summary-box-inner">
<span><p>This paper is concerned with specular reflection removal based on tensor
low-rank decomposition framework with the help of polarization information. Our
method is motivated by the observation that the specular highlight of an image
is sparsely distributed while the remaining diffuse reflection can be well
approximated by a linear combination of several distinct colors using a
low-rank and sparse decomposition framework. Unlike current solutions, our
tensor low-rank decomposition keeps the spatial structure of specular and
diffuse information which enables us to recover the diffuse image under strong
specular reflection or in saturated regions. We further define and impose a new
polarization regularization term as constraint on color channels. This
regularization boosts the performance of the method to recover an accurate
diffuse image by handling the color distortion, a common problem of
chromaticity-based methods, especially in case of strong specular reflection.
Through comprehensive experiments on both synthetic and real polarization
images, we demonstrate that our method is able to significantly improve the
accuracy of highlight specular removal, and outperform the competitive methods
to recover the diffuse image, especially in regions of strong specular
reflection or in saturated areas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Embedding-Dynamic Approach to Self-supervised Learning. (arXiv:2207.03552v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03552">
<div class="article-summary-box-inner">
<span><p>A number of recent self-supervised learning methods have shown impressive
performance on image classification and other tasks. A somewhat bewildering
variety of techniques have been used, not always with a clear understanding of
the reasons for their benefits, especially when used in combination. Here we
treat the embeddings of images as point particles and consider model
optimization as a dynamic process on this system of particles. Our dynamic
model combines an attractive force for similar images, a locally dispersive
force to avoid local collapse, and a global dispersive force to achieve a
globally-homogeneous distribution of particles. The dynamic perspective
highlights the advantage of using a delayed-parameter image embedding (a la
BYOL) together with multiple views of the same image. It also uses a
purely-dynamic local dispersive force (Brownian motion) that shows improved
performance over other methods and does not require knowledge of other particle
coordinates. The method is called MSBReg which stands for (i) a Multiview
centroid loss, which applies an attractive force to pull different image view
embeddings toward their centroid, (ii) a Singular value loss, which pushes the
particle system toward spatially homogeneous density, (iii) a Brownian
diffusive loss. We evaluate downstream classification performance of MSBReg on
ImageNet as well as transfer learning tasks including fine-grained
classification, multi-class object classification, object detection, and
instance segmentation. In addition, we also show that applying our
regularization term to other methods further improves their performance and
stabilize the training by preventing a mode collapse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mirror Complementary Transformer Network for RGB-thermal Salient Object Detection. (arXiv:2207.03558v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03558">
<div class="article-summary-box-inner">
<span><p>RGB-thermal salient object detection (RGB-T SOD) aims to locate the common
prominent objects of an aligned visible and thermal infrared image pair and
accurately segment all the pixels belonging to those objects. It is promising
in challenging scenes such as nighttime and complex backgrounds due to the
insensitivity to lighting conditions of thermal images. Thus, the key problem
of RGB-T SOD is to make the features from the two modalities complement and
adjust each other flexibly, since it is inevitable that any modalities of RGB-T
image pairs failure due to challenging scenes such as extreme light conditions
and thermal crossover. In this paper, we propose a novel mirror complementary
Transformer network (MCNet) for RGB-T SOD. Specifically, we introduce a
Transformer-based feature extraction module to effective extract hierarchical
features of RGB and thermal images. Then, through the attention-based feature
interaction and serial multiscale dilated convolution (SDC) based feature
fusion modules, the proposed model achieves the complementary interaction of
low-level features and the semantic fusion of deep features. Finally, based on
the mirror complementary structure, the salient regions of the two modalities
can be accurately extracted even one modality is invalid. To demonstrate the
robustness of the proposed model under challenging scenes in real world, we
build a novel RGB-T SOD dataset VT723 based on a large public semantic
segmentation RGB-T dataset used in the autonomous driving domain. Expensive
experiments on benchmark and VT723 datasets show that the proposed method
outperforms state-of-the-art approaches, including CNN-based and
Transformer-based methods. The code and dataset will be released later at
https://github.com/jxr326/SwinMCNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The use of deep learning enables high diagnostic accuracy in detecting syndesmotic instability on weight-bearing CT scanning. (arXiv:2207.03568v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03568">
<div class="article-summary-box-inner">
<span><p>Delayed diagnosis of syndesmosis instability can lead to significant
morbidity and accelerated arthritic change in the ankle joint. Weight-bearing
computed tomography (WBCT) has shown promising potential for early and reliable
detection of isolated syndesmotic instability using 3D volumetric measurements.
While these measurements have been reported to be highly accurate, they are
also experience-dependent, time-consuming, and need a particular 3D measurement
software tool that leads the clinicians to still show more interest in the
conventional diagnostic methods for syndesmotic instability. The purpose of
this study was to increase accuracy, accelerate analysis time, and reduce
inter-observer bias by automating 3D volume assessment of syndesmosis anatomy
using WBCT scans. We conducted a retrospective study using previously collected
WBCT scans of patients with unilateral syndesmotic instability. 144 bilateral
ankle WBCT scans were evaluated (48 unstable, 96 control). We developed three
deep learning (DL) models for analyzing WBCT scans to recognize syndesmosis
instability. These three models included two state-of-the-art models (Model 1 -
3D convolutional neural network [CNN], and Model 2 - CNN with long short-term
memory [LSTM]), and a new model (Model 3 - differential CNN LSTM) that we
introduced in this study. Model 1 failed to analyze the WBCT scans (F1-score =
0). Model 2 only misclassified two cases (F1-score = 0.80). Model 3
outperformed Model 2 and achieved a nearly perfect performance, misclassifying
only one case (F1-score = 0.91) in the control group as unstable while being
faster than Model 2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demystifying the Adversarial Robustness of Random Transformation Defenses. (arXiv:2207.03574v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03574">
<div class="article-summary-box-inner">
<span><p>Neural networks' lack of robustness against attacks raises concerns in
security-sensitive settings such as autonomous vehicles. While many
countermeasures may look promising, only a few withstand rigorous evaluation.
Defenses using random transformations (RT) have shown impressive results,
particularly BaRT (Raff et al., 2019) on ImageNet. However, this type of
defense has not been rigorously evaluated, leaving its robustness properties
poorly understood. Their stochastic properties make evaluation more challenging
and render many proposed attacks on deterministic models inapplicable. First,
we show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluation
is ineffective and likely overestimates its robustness. We then attempt to
construct the strongest possible RT defense through the informed selection of
transformations and Bayesian optimization for tuning their parameters.
Furthermore, we create the strongest possible attack to evaluate our RT
defense. Our new attack vastly outperforms the baseline, reducing the accuracy
by 83% compared to the 19% reduction by the commonly used EoT attack
($4.3\times$ improvement). Our result indicates that the RT defense on the
Imagenette dataset (a ten-class subset of ImageNet) is not robust against
adversarial examples. Extending the study further, we use our new attack to
adversarially train RT defense (called AdvRT), resulting in a large robustness
gain. Code is available at
https://github.com/wagnergroup/demystify-random-transform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GaitTAKE: Gait Recognition by Temporal Attention \\and Keypoint-guided Embedding. (arXiv:2207.03608v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03608">
<div class="article-summary-box-inner">
<span><p>Gait recognition, which refers to the recognition or identification of a
person based on their body shape and walking styles, derived from video data
captured from a distance, is widely used in crime prevention, forensic
identification, and social security. However, to the best of our knowledge,
most of the existing methods use appearance, posture and temporal feautures
without considering a learned temporal attention mechanism for global and local
information fusion. In this paper, we propose a novel gait recognition
framework, called Temporal Attention and Keypoint-guided Embedding (GaitTAKE),
which effectively fuses temporal-attention-based global and local appearance
feature and temporal aggregated human pose feature. Experimental results show
that our proposed method achieves a new SOTA in gait recognition with rank-1
accuracy of 98.0% (normal), 97.5% (bag) and 92.2% (coat) on the CASIA-B gait
dataset; 90.4% accuracy on the OU-MVLP gait dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoseGU: 3D Human Pose Estimation with Novel Human Pose Generator and Unbiased Learning. (arXiv:2207.03618v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03618">
<div class="article-summary-box-inner">
<span><p>3D pose estimation has recently gained substantial interests in computer
vision domain. Existing 3D pose estimation methods have a strong reliance on
large size well-annotated 3D pose datasets, and they suffer poor model
generalization on unseen poses due to limited diversity of 3D poses in training
sets. In this work, we propose PoseGU, a novel human pose generator that
generates diverse poses with access only to a small size of seed samples, while
equipping the Counterfactual Risk Minimization to pursue an unbiased evaluation
objective. Extensive experiments demonstrate PoseGU outforms almost all the
state-of-the-art 3D human pose methods under consideration over three popular
benchmark datasets. Empirical analysis also proves PoseGU generates 3D poses
with improved data diversity and better generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity. (arXiv:2207.03620v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03620">
<div class="article-summary-box-inner">
<span><p>Transformers have quickly shined in the computer vision world since the
emergence of Vision Transformers (ViTs). The dominant role of convolutional
neural networks (CNNs) seems to be challenged by increasingly effective
transformer-based models. Very recently, a couple of advanced convolutional
models strike back with large kernels motivated by the local but large
attention mechanism, showing appealing performance and efficiency. While one of
them, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31
with improved performance, the performance starts to saturate as the kernel
size continues growing, compared to the scaling trend of advanced ViTs such as
Swin Transformer. In this paper, we explore the possibility of training extreme
convolutions larger than 31x31 and test whether the performance gap can be
eliminated by strategically enlarging convolutions. This study ends up with a
recipe for applying extremely large kernels from the perspective of sparsity,
which can smoothly scale up kernels to 61x61 with better performance. Built on
this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN
architecture equipped with 51x51 kernels that can perform on par with or better
than state-of-the-art hierarchical Transformers and modern ConvNet
architectures like ConvNeXt and RepLKNet, on ImageNet classification as well as
typical downstream tasks. Our code is available here
https://github.com/VITA-Group/SLaK.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Support Vector Model of Pruning Trees Evaluation Based on OTSU Algorithm. (arXiv:2207.03638v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03638">
<div class="article-summary-box-inner">
<span><p>The tree pruning process is the key to promoting fruits' growth and improving
their productions due to effects on the photosynthesis efficiency of fruits and
nutrition transportation in branches. Currently, pruning is still highly
dependent on human labor. The workers' experience will strongly affect the
robustness of the performance of the tree pruning. Thus, it is a challenge for
workers and farmers to evaluate the pruning performance. Intended for a better
solution to the problem, this paper presents a novel pruning classification
strategy model called "OTSU-SVM" to evaluate the pruning performance based on
the shadows of branches and leaves. This model considers not only the available
illuminated area of the tree but also the uniformity of the illuminated area of
the tree. More importantly, our group implements OTSU algorithm into the model,
which highly reinforces robustness of the evaluation of this model. In
addition, the data from the pear trees in the Yuhang District, Hangzhou is also
used in the experiment. In this experiment, we prove that the OTSU-SVM has good
accuracy with 80% and high performance in the evaluation of the pruning for the
pear trees. It can provide more successful pruning if applied into the orchard.
A successful pruning can broaden the illuminated area of individual fruit, and
increase nutrition transportation from the target branch, dramatically
elevating the weights and production of the fruits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pruning Early Exit Networks. (arXiv:2207.03644v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03644">
<div class="article-summary-box-inner">
<span><p>Deep learning models that perform well often have high computational costs.
In this paper, we combine two approaches that try to reduce the computational
cost while keeping the model performance high: pruning and early exit networks.
We evaluate two approaches of pruning early exit networks: (1) pruning the
entire network at once, (2) pruning the base network and additional linear
classifiers in an ordered fashion. Experimental results show that pruning the
entire network at once is a better strategy in general. However, at high
accuracy rates, the two approaches have a similar performance, which implies
that the processes of pruning and early exit can be separated without loss of
optimality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abs-CAM: A Gradient Optimization Interpretable Approach for Explanation of Convolutional Neural Networks. (arXiv:2207.03648v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03648">
<div class="article-summary-box-inner">
<span><p>The black-box nature of Deep Neural Networks (DNNs) severely hinders its
performance improvement and application in specific scenes. In recent years,
class activation mapping-based method has been widely used to interpret the
internal decisions of models in computer vision tasks. However, when this
method uses backpropagation to obtain gradients, it will cause noise in the
saliency map, and even locate features that are irrelevant to decisions. In
this paper, we propose an Absolute value Class Activation Mapping-based
(Abs-CAM) method, which optimizes the gradients derived from the
backpropagation and turns all of them into positive gradients to enhance the
visual features of output neurons' activation, and improve the localization
ability of the saliency map. The framework of Abs-CAM is divided into two
phases: generating initial saliency map and generating final saliency map. The
first phase improves the localization ability of the saliency map by optimizing
the gradient, and the second phase linearly combines the initial saliency map
with the original image to enhance the semantic information of the saliency
map. We conduct qualitative and quantitative evaluation of the proposed method,
including Deletion, Insertion, and Pointing Game. The experimental results show
that the Abs-CAM can obviously eliminate the noise in the saliency map, and can
better locate the features related to decisions, and is superior to the
previous methods in recognition and localization tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Dialog as Conversation about Objects Living in Space-Time. (arXiv:2207.03656v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03656">
<div class="article-summary-box-inner">
<span><p>It would be a technological feat to be able to create a system that can hold
a meaningful conversation with humans about what they watch. A setup toward
that goal is presented as a video dialog task, where the system is asked to
generate natural utterances in response to a question in an ongoing dialog. The
task poses great visual, linguistic, and reasoning challenges that cannot be
easily overcome without an appropriate representation scheme over video and
dialog that supports high-level reasoning. To tackle these challenges we
present a new object-centric framework for video dialog that supports neural
reasoning dubbed COST - which stands for Conversation about Objects in
Space-Time. Here dynamic space-time visual content in videos is first parsed
into object trajectories. Given this video abstraction, COST maintains and
tracks object-associated dialog states, which are updated upon receiving new
questions. Object interactions are dynamically and conditionally inferred for
each question, and these serve as the basis for relational reasoning among
them. COST also maintains a history of previous answers, and this allows
retrieval of relevant object-centric information to enrich the answer forming
process. Language production then proceeds in a step-wise manner, taking into
the context of the current utterance, the existing dialog, the current
question. We evaluate COST on the DSTC7 and DSTC8 benchmarks, demonstrating its
competitiveness against state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deepfake Face Traceability with Disentangling Reversing Network. (arXiv:2207.03666v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03666">
<div class="article-summary-box-inner">
<span><p>Deepfake face not only violates the privacy of personal identity, but also
confuses the public and causes huge social harm. The current deepfake detection
only stays at the level of distinguishing true and false, and cannot trace the
original genuine face corresponding to the fake face, that is, it does not have
the ability to trace the source of evidence. The deepfake countermeasure
technology for judicial forensics urgently calls for deepfake traceability.
This paper pioneers an interesting question about face deepfake, active
forensics that "know it and how it happened". Given that deepfake faces do not
completely discard the features of original faces, especially facial
expressions and poses, we argue that original faces can be approximately
speculated from their deepfake counterparts. Correspondingly, we design a
disentangling reversing network that decouples latent space features of
deepfake faces under the supervision of fake-original face pair samples to
infer original faces in reverse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning High-quality Proposals for Acne Detection. (arXiv:2207.03674v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03674">
<div class="article-summary-box-inner">
<span><p>Acne detection is crucial for interpretative diagnosis and precise treatment
of skin disease. The arbitrary boundary and small size of acne lesions lead to
a significant number of poor-quality proposals in two-stage detection. In this
paper, we propose a novel head structure for Region Proposal Network to improve
the proposals' quality in two ways. At first, a Spatial Aware Double Head(SADH)
structure is proposed to disentangle the representation learning for
classification and localization from two different spatial perspectives. The
proposed SADH ensures a steeper classification confidence gradient and
suppresses the proposals having low intersection-over-union(IoU) with the
matched ground truth. Then, we propose a Normalized Wasserstein Distance
prediction branch to improve the correlation between the proposals'
classification scores and IoUs. In addition, to facilitate further research on
acne detection, we construct a new dataset named AcneSCU, with high-resolution
imageries, precise annotations, and fine-grained lesion categories. Extensive
experiments are conducted on both AcneSCU and the public dataset ACNE04, and
the results demonstrate the proposed method could improve the proposals'
quality, consistently outperforming state-of-the-art approaches. Code and the
collected dataset are available in
https://github.com/pingguokiller/acnedetection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning. (arXiv:2207.03677v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03677">
<div class="article-summary-box-inner">
<span><p>Neural architecture search (NAS) has demonstrated amazing success in
searching for efficient deep neural networks (DNNs) from a given supernet. In
parallel, the lottery ticket hypothesis has shown that DNNs contain small
subnetworks that can be trained from scratch to achieve a comparable or higher
accuracy than original DNNs. As such, it is currently a common practice to
develop efficient DNNs via a pipeline of first search and then prune.
Nevertheless, doing so often requires a search-train-prune-retrain process and
thus prohibitive computational cost. In this paper, we discover for the first
time that both efficient DNNs and their lottery subnetworks (i.e., lottery
tickets) can be directly identified from a supernet, which we term as
SuperTickets, via a two-in-one training scheme with jointly architecture
searching and parameter pruning. Moreover, we develop a progressive and unified
SuperTickets identification strategy that allows the connectivity of
subnetworks to change during supernet training, achieving better accuracy and
efficiency trade-offs than conventional sparse training. Finally, we evaluate
whether such identified SuperTickets drawn from one task can transfer well to
other tasks, validating their potential of handling multiple tasks
simultaneously. Extensive experiments and ablation studies on three tasks and
four benchmark datasets validate that our proposed SuperTickets achieve boosted
accuracy and efficiency trade-offs than both typical NAS and pruning pipelines,
regardless of having retraining or not. Codes and pretrained models are
available at https://github.com/RICE-EIC/SuperTickets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Music-driven Dance Regeneration with Controllable Key Pose Constraints. (arXiv:2207.03682v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03682">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel framework for music-driven dance motion
synthesis with controllable key pose constraint. In contrast to methods that
generate dance motion sequences only based on music without any other
controllable conditions, this work targets on synthesizing high-quality dance
motion driven by music as well as customized poses performed by users. Our
model involves two single-modal transformer encoders for music and motion
representations and a cross-modal transformer decoder for dance motions
generation. The cross-modal transformer decoder achieves the capability of
synthesizing smooth dance motion sequences, which keeps a consistency with key
poses at corresponding positions, by introducing the local neighbor position
embedding. Such mechanism makes the decoder more sensitive to key poses and the
corresponding positions. Our dance synthesis model achieves satisfactory
performance both on quantitative and qualitative evaluations with extensive
experiments, which demonstrates the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptive Fundus Image Segmentation with Category-level Regularization. (arXiv:2207.03684v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03684">
<div class="article-summary-box-inner">
<span><p>Existing unsupervised domain adaptation methods based on adversarial learning
have achieved good performance in several medical imaging tasks. However, these
methods focus only on global distribution adaptation and ignore distribution
constraints at the category level, which would lead to sub-optimal adaptation
performance. This paper presents an unsupervised domain adaptation framework
based on category-level regularization that regularizes the category
distribution from three perspectives. Specifically, for inter-domain category
regularization, an adaptive prototype alignment module is proposed to align
feature prototypes of the same category in the source and target domains. In
addition, for intra-domain category regularization, we tailored a
regularization technique for the source and target domains, respectively. In
the source domain, a prototype-guided discriminative loss is proposed to learn
more discriminative feature representations by enforcing intra-class
compactness and inter-class separability, and as a complement to traditional
supervised loss. In the target domain, an augmented consistency category
regularization loss is proposed to force the model to produce consistent
predictions for augmented/unaugmented target images, which encourages
semantically similar regions to be given the same label. Extensive experiments
on two publicly fundus datasets show that the proposed approach significantly
outperforms other state-of-the-art comparison algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Implicit Dictionary via Mixture-of-Expert Training. (arXiv:2207.03691v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03691">
<div class="article-summary-box-inner">
<span><p>Representing visual signals by coordinate-based deep fully-connected networks
has been shown advantageous in fitting complex details and solving inverse
problems than discrete grid-based representation. However, acquiring such a
continuous Implicit Neural Representation (INR) requires tedious per-scene
training on tons of signal measurements, which limits its practicality. In this
paper, we present a generic INR framework that achieves both data and training
efficiency by learning a Neural Implicit Dictionary (NID) from a data
collection and representing INR as a functional combination of basis sampled
from the dictionary. Our NID assembles a group of coordinate-based subnetworks
which are tuned to span the desired function space. After training, one can
instantly and robustly acquire an unseen scene representation by solving the
coding coefficients. To parallelly optimize a large group of networks, we
borrow the idea from Mixture-of-Expert (MoE) to design and train our network
with a sparse gating mechanism. Our experiments show that, NID can improve
reconstruction of 2D images or 3D scenes by 2 orders of magnitude faster with
up to 98% less input data. We further demonstrate various applications of NID
in image inpainting and occlusion removal, which are considered to be
challenging with vanilla INR. Our codes are available in
https://github.com/VITA-Group/Neural-Implicit-Dict.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining Discriminative Food Regions for Accurate Food Recognition. (arXiv:2207.03692v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03692">
<div class="article-summary-box-inner">
<span><p>Automatic food recognition is the very first step towards passive dietary
monitoring. In this paper, we address the problem of food recognition by mining
discriminative food regions. Taking inspiration from Adversarial Erasing, a
strategy that progressively discovers discriminative object regions for weakly
supervised semantic segmentation, we propose a novel network architecture in
which a primary network maintains the base accuracy of classifying an input
image, an auxiliary network adversarially mines discriminative food regions,
and a region network classifies the resulting mined regions. The global (the
original input image) and the local (the mined regions) representations are
then integrated for the final prediction. The proposed architecture denoted as
PAR-Net is end-to-end trainable, and highlights discriminative regions in an
online fashion. In addition, we introduce a new fine-grained food dataset named
as Sushi-50, which consists of 50 different sushi categories. Extensive
experiments have been conducted to evaluate the proposed approach. On three
food datasets chosen (Food-101, Vireo-172, and Sushi-50), our approach performs
consistently and achieves state-of-the-art results (top-1 testing accuracy of
$90.4\%$, $90.2\%$, $92.0\%$, respectively) compared with other existing
approaches. Dataset and code are available at
https://github.com/Jianing-Qiu/PARNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SST-Calib: Simultaneous Spatial-Temporal Parameter Calibration between LIDAR and Camera. (arXiv:2207.03704v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03704">
<div class="article-summary-box-inner">
<span><p>With information from multiple input modalities, sensor fusion-based
algorithms usually out-perform their single-modality counterparts in robotics.
Camera and LIDAR, with complementary semantic and depth information, are the
typical choices for detection tasks in complicated driving environments. For
most camera-LIDAR fusion algorithms, however, the calibration of the sensor
suite will greatly impact the performance. More specifically, the detection
algorithm usually requires an accurate geometric relationship among multiple
sensors as the input, and it is often assumed that the contents from these
sensors are captured at the same time. Preparing such sensor suites involves
carefully designed calibration rigs and accurate synchronization mechanisms,
and the preparation process is usually done offline. In this work, a
segmentation-based framework is proposed to jointly estimate the geometrical
and temporal parameters in the calibration of a camera-LIDAR suite. A semantic
segmentation mask is first applied to both sensor modalities, and the
calibration parameters are optimized through pixel-wise bidirectional loss. We
specifically incorporated the velocity information from optical flow for
temporal parameters. Since supervision is only performed at the segmentation
level, no calibration label is needed within the framework. The proposed
algorithm is tested on the KITTI dataset, and the result shows an accurate
real-time calibration of both geometric and temporal parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-based Smoky Vehicle Detection with A Coarse-to-Fine Framework. (arXiv:2207.03708v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03708">
<div class="article-summary-box-inner">
<span><p>Automatic smoky vehicle detection in videos is a superior solution to the
traditional expensive remote sensing one with ultraviolet-infrared light
devices for environmental protection agencies. However, it is challenging to
distinguish vehicle smoke from shadow and wet regions coming from rear vehicle
or clutter roads, and could be worse due to limited annotated data. In this
paper, we first introduce a real-world large-scale smoky vehicle dataset with
75,000 annotated smoky vehicle images, facilitating the effective training of
advanced deep learning models. To enable fair algorithm comparison, we also
build a smoky vehicle video dataset including 163 long videos with
segment-level annotations. Moreover, we present a new Coarse-to-fine Deep Smoky
vehicle detection (CoDeS) framework for efficient smoky vehicle detection. The
CoDeS first leverages a light-weight YOLO detector for fast smoke detection
with high recall rate, and then applies a smoke-vehicle matching strategy to
eliminate non-vehicle smoke, and finally uses a elaborately-designed 3D model
to further refine the results in spatial temporal space. Extensive experiments
in four metrics demonstrate that our framework is significantly superior to
those hand-crafted feature based methods and recent advanced methods. The code
and dataset will be released at https://github.com/pengxj/smokyvehicle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Harnessing Prior Structures and Temporal Consistency for Sign Language Video Generation. (arXiv:2207.03714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03714">
<div class="article-summary-box-inner">
<span><p>Sign language is the window for people differently-abled to express their
feelings as well as emotions. However, it remains challenging for people to
learn sign language in a short time. To address this real-world challenge, in
this work, we study the motion transfer system, which can transfer the user
photo to the sign language video of specific words. In particular, the
appearance content of the output video comes from the provided user image,
while the motion of the video is extracted from the specified tutorial video.
We observe two primary limitations in adopting the state-of-the-art motion
transfer methods to sign language generation:(1) Existing motion transfer works
ignore the prior geometrical knowledge of the human body. (2) The previous
image animation methods only take image pairs as input in the training stage,
which could not fully exploit the temporal information within videos. In an
attempt to address the above-mentioned limitations, we propose Structure-aware
Temporal Consistency Network (STCNet) to jointly optimize the prior structure
of human with the temporal consistency for sign language video generation.
There are two main contributions in this paper. (1) We harness a fine-grained
skeleton detector to provide prior knowledge of the body keypoints. In this
way, we ensure the keypoint movement in a valid range and make the model become
more explainable and robust. (2) We introduce two cycle-consistency losses,
i.e., short-term cycle loss and long-term cycle loss, which are conducted to
assure the continuity of the generated video. We optimize the two losses and
keypoint detector network in an end-to-end manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bounding Box Disparity: 3D Metrics for Object Detection With Full Degree of Freedom. (arXiv:2207.03720v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03720">
<div class="article-summary-box-inner">
<span><p>The most popular evaluation metric for object detection in 2D images is
Intersection over Union (IoU). Existing implementations of the IoU metric for
3D object detection usually neglect one or more degrees of freedom. In this
paper, we first derive the analytic solution for three dimensional bounding
boxes. As a second contribution, a closed-form solution of the volume-to-volume
distance is derived. Finally, the Bounding Box Disparity is proposed as a
combined positive continuous metric. We provide open source implementations of
the three metrics as standalone python functions, as well as extensions to the
Open3D library and as ROS nodes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Effectiveness of Video Perceptual Representation in Blind Video Quality Assessment. (arXiv:2207.03723v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03723">
<div class="article-summary-box-inner">
<span><p>With the rapid growth of in-the-wild videos taken by non-specialists, blind
video quality assessment (VQA) has become a challenging and demanding problem.
Although lots of efforts have been made to solve this problem, it remains
unclear how the human visual system (HVS) relates to the temporal quality of
videos. Meanwhile, recent work has found that the frames of natural video
transformed into the perceptual domain of the HVS tend to form a straight
trajectory of the representations. With the obtained insight that distortion
impairs the perceived video quality and results in a curved trajectory of the
perceptual representation, we propose a temporal perceptual quality index
(TPQI) to measure the temporal distortion by describing the graphic morphology
of the representation. Specifically, we first extract the video perceptual
representations from the lateral geniculate nucleus (LGN) and primary visual
area (V1) of the HVS, and then measure the straightness and compactness of
their trajectories to quantify the degradation in naturalness and content
continuity of video. Experiments show that the perceptual representation in the
HVS is an effective way of predicting subjective temporal quality, and thus
TPQI can, for the first time, achieve comparable performance to the spatial
quality metric and be even more effective in assessing videos with large
temporal variations. We further demonstrate that by combining with NIQE, a
spatial quality metric, TPQI can achieve top performance over popular
in-the-wild video datasets. More importantly, TPQI does not require any
additional information beyond the video being evaluated and thus can be applied
to any datasets without parameter tuning. Source code is available at
https://github.com/UoLMM/TPQI-VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TGRMPT: A Head-Shoulder Aided Multi-Person Tracker and a New Large-Scale Dataset for Tour-Guide Robot. (arXiv:2207.03726v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03726">
<div class="article-summary-box-inner">
<span><p>A service robot serving safely and politely needs to track the surrounding
people robustly, especially for Tour-Guide Robot (TGR). However, existing
multi-object tracking (MOT) or multi-person tracking (MPT) methods are not
applicable to TGR for the following reasons: 1. lacking relevant large-scale
datasets; 2. lacking applicable metrics to evaluate trackers. In this work, we
target the visual perceptual tasks for TGR and present the TGRDB dataset, a
novel large-scale multi-person tracking dataset containing roughly 5.6 hours of
annotated videos and over 450 long-term trajectories. Besides, we propose a
more applicable metric to evaluate trackers using our dataset. As part of our
work, we present TGRMPT, a novel MPT system that incorporates information from
head shoulder and whole body, and achieves state-of-the-art performance. We
have released our codes and dataset in https://github.com/wenwenzju/TGRMPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEMS: Scene Expansion using Generative Models of Graphs. (arXiv:2207.03729v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03729">
<div class="article-summary-box-inner">
<span><p>Applications based on image retrieval require editing and associating in
intermediate spaces that are representative of the high-level concepts like
objects and their relationships rather than dense, pixel-level representations
like RGB images or semantic-label maps. We focus on one such representation,
scene graphs, and propose a novel scene expansion task where we enrich an input
seed graph by adding new nodes (objects) and the corresponding relationships.
To this end, we formulate scene graph expansion as a sequential prediction task
involving multiple steps of first predicting a new node and then predicting the
set of relationships between the newly predicted node and previous nodes in the
graph. We propose a sequencing strategy for observed graphs that retains the
clustering patterns amongst nodes. In addition, we leverage external knowledge
to train our graph generation model, enabling greater generalization of node
predictions. Due to the inefficiency of existing maximum mean discrepancy (MMD)
based metrics for graph generation problems in evaluating predicted
relationships between nodes (objects), we design novel metrics that
comprehensively evaluate different aspects of predicted relations. We conduct
extensive experiments on Visual Genome and VRD datasets to evaluate the
expanded scene graphs using the standard MMD-based metrics and our proposed
metrics. We observe that the graphs generated by our method, GEMS, better
represent the real distribution of the scene graphs than the baseline methods
like GraphRNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Deep Learning with Good Old-Fashioned Machine Learning. (arXiv:2207.03757v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03757">
<div class="article-summary-box-inner">
<span><p>We present a comprehensive, stacking-based framework for combining deep
learning with good old-fashioned machine learning, called Deep GOld. Our
framework involves ensemble selection from 51 retrained pretrained deep
networks as first-level models, and 10 machine-learning algorithms as
second-level models. Enabled by today's state-of-the-art software tools and
hardware platforms, Deep GOld delivers consistent improvement when tested on
four image-classification datasets: Fashion MNIST, CIFAR10, CIFAR100, and Tiny
ImageNet. Of 120 experiments, in all but 10 Deep GOld improved the original
networks' performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual Axle Detector based on Analysis of Bridge Acceleration Measurements by Fully Convolutional Network. (arXiv:2207.03758v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03758">
<div class="article-summary-box-inner">
<span><p>In the practical application of the Bridge Weigh-In-Motion (BWIM) methods,
the position of the wheels or axles during the passage of a vehicle is in most
cases a prerequisite. To avoid the use of conventional axle detectors and
bridge type specific methods, we propose a novel method for axle detection
through the placement of accelerometers at any point of a bridge. In order to
develop a model that is as simple and comprehensible as possible, the axle
detection task is implemented as a binary classification problem instead of a
regression problem. The model is implemented as a Fully Convolutional Network
to process signals in the form of Continuous Wavelet Transforms. This allows
passages of any length to be processed in a single step with maximum efficiency
while utilising multiple scales in a single evaluation. This enables our method
to use acceleration signals at any location of the bridge structure serving as
Virtual Axle Detectors (VADs) without being limited to specific structural
types of bridges. To test the proposed method, we analysed 3787 train passages
recorded on a steel trough railway bridge of a long-distance traffic line. Our
results on the measurement data show that our model detects 95% of the axes,
thus, 128,599 of 134,800 previously unseen axles were correctly detected. In
total, 90% of the axles can be detected with a maximum spatial error of 20cm,
with a maximum velocity of $v_{\mathrm{max}}=56,3~\mathrm{m/s}$. The analysis
shows that our developed model can use accelerometers as VADs even under real
operating conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Intrinsic Common Discriminative Features Learning for Face Forgery Detection using Adversarial Learning. (arXiv:2207.03776v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03776">
<div class="article-summary-box-inner">
<span><p>Existing face forgery detection methods usually treat face forgery detection
as a binary classification problem and adopt deep convolution neural networks
to learn discriminative features. The ideal discriminative features should be
only related to the real/fake labels of facial images. However, we observe that
the features learned by vanilla classification networks are correlated to
unnecessary properties, such as forgery methods and facial identities. Such
phenomenon would limit forgery detection performance especially for the
generalization ability. Motivated by this, we propose a novel method which
utilizes adversarial learning to eliminate the negative effect of different
forgery methods and facial identities, which helps classification network to
learn intrinsic common discriminative features for face forgery detection. To
leverage data lacking ground truth label of facial identities, we design a
special identity discriminator based on similarity information derived from
off-the-shelf face recognition model. With the help of adversarial learning,
our face forgery detection model learns to extract common discriminative
features through eliminating the effect of forgery methods and facial
identities. Extensive experiments demonstrate the effectiveness of the proposed
method under both intra-dataset and cross-dataset evaluation settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VidConv: A modernized 2D ConvNet for Efficient Video Recognition. (arXiv:2207.03782v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03782">
<div class="article-summary-box-inner">
<span><p>Since being introduced in 2020, Vision Transformers (ViT) has been steadily
breaking the record for many vision tasks and are often described as
``all-you-need" to replace ConvNet. Despite that, ViTs are generally
computational, memory-consuming, and unfriendly for embedded devices. In
addition, recent research shows that standard ConvNet if redesigned and trained
appropriately can compete favorably with ViT in terms of accuracy and
scalability. In this paper, we adopt the modernized structure of ConvNet to
design a new backbone for action recognition. Particularly, our main target is
to serve for industrial product deployment, such as FPGA boards in which only
standard operations are supported. Therefore, our network simply consists of 2D
convolutions, without using any 3D convolution, long-range attention plugin, or
Transformer blocks. While being trained with much fewer epochs (5x-10x), our
backbone surpasses the methods using (2+1)D and 3D convolution, and achieve
comparable results with ViT on two benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Target-free Extrinsic Calibration of a Multi-Sensor System from a Sequence of Static Viewpoints. (arXiv:2207.03785v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03785">
<div class="article-summary-box-inner">
<span><p>Mobile robotic applications need precise information about the geometric
position of the individual sensors on the platform. This information is given
by the extrinsic calibration parameters which define how the sensor is rotated
and translated with respect to a fixed reference coordinate system. Erroneous
calibration parameters have a negative impact on typical robotic estimation
tasks, e.g. SLAM. In this work we propose a new method for a continuous
estimation of the calibration parameters during operation of the robot. The
parameter estimation is based on the matching of point clouds which are
acquired by the sensors from multiple static viewpoints. Consequently, our
method does not need any special calibration targets and is applicable to any
sensor whose measurements can be converted to point clouds. We demonstrate the
suitability of our method by calibrating a multi-sensor system composed by 2
lidar sensors, 3 cameras, and an imaging radar sensor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complementing Brightness Constancy with Deep Networks for Optical Flow Prediction. (arXiv:2207.03790v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03790">
<div class="article-summary-box-inner">
<span><p>State-of-the-art methods for optical flow estimation rely on deep learning,
which require complex sequential training schemes to reach optimal performances
on real-world data. In this work, we introduce the COMBO deep network that
explicitly exploits the brightness constancy (BC) model used in traditional
methods. Since BC is an approximate physical model violated in several
situations, we propose to train a physically-constrained network complemented
with a data-driven network. We introduce a unique and meaningful flow
decomposition between the physical prior and the data-driven complement,
including an uncertainty quantification of the BC model. We derive a joint
training scheme for learning the different components of the decomposition
ensuring an optimal cooperation, in a supervised but also in a semi-supervised
context. Experiments show that COMBO can improve performances over
state-of-the-art supervised networks, e.g. RAFT, reaching state-of-the-art
results on several benchmarks. We highlight how COMBO can leverage the BC model
and adapt to its limitations. Finally, we show that our semi-supervised method
can significantly simplify the training procedure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis. (arXiv:2207.03800v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03800">
<div class="article-summary-box-inner">
<span><p>Unconstrained lip-to-speech synthesis aims to generate corresponding speeches
from silent videos of talking faces with no restriction on head poses or
vocabulary. Current works mainly use sequence-to-sequence models to solve this
problem, either in an autoregressive architecture or a flow-based
non-autoregressive architecture. However, these models suffer from several
drawbacks: 1) Instead of directly generating audios, they use a two-stage
pipeline that first generates mel-spectrograms and then reconstructs audios
from the spectrograms. This causes cumbersome deployment and degradation of
speech quality due to error propagation; 2) The audio reconstruction algorithm
used by these models limits the inference speed and audio quality, while neural
vocoders are not available for these models since their output spectrograms are
not accurate enough; 3) The autoregressive model suffers from high inference
latency, while the flow-based model has high memory occupancy: neither of them
is efficient enough in both time and memory usage. To tackle these problems, we
propose FastLTS, a non-autoregressive end-to-end model which can directly
synthesize high-quality speech audios from unconstrained talking videos with
low latency, and has a relatively small model size. Besides, different from the
widely used 3D-CNN visual frontend for lip movement encoding, we for the first
time propose a transformer-based visual frontend for this task. Experiments
show that our model achieves $19.76\times$ speedup for audio waveform
generation compared with the current autoregressive model on input sequences of
3 seconds, and obtains superior audio quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Transfer Learning: Co-finetuning for Action Localisation. (arXiv:2207.03807v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03807">
<div class="article-summary-box-inner">
<span><p>Transfer learning is the predominant paradigm for training deep networks on
small target datasets. Models are typically pretrained on large ``upstream''
datasets for classification, as such labels are easy to collect, and then
finetuned on ``downstream'' tasks such as action localisation, which are
smaller due to their finer-grained annotations. In this paper, we question this
approach, and propose co-finetuning -- simultaneously training a single model
on multiple ``upstream'' and ``downstream'' tasks. We demonstrate that
co-finetuning outperforms traditional transfer learning when using the same
total amount of data, and also show how we can easily extend our approach to
multiple ``upstream'' datasets to further improve performance. In particular,
co-finetuning significantly improves the performance on rare classes in our
downstream task, as it has a regularising effect, and enables the network to
learn feature representations that transfer between different datasets.
Finally, we observe how co-finetuning with public, video classification
datasets, we are able to achieve state-of-the-art results for spatio-temporal
action localisation on the challenging AVA and AVA-Kinetics datasets,
outperforming recent works which develop intricate models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Zero-shot Learning via Contrastive Optimization of Attribute Representations. (arXiv:2207.03824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03824">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning (ZSL) aims to recognize classes that do not have samples
in the training set. One representative solution is to directly learn an
embedding function associating visual features with corresponding class
semantics for recognizing new classes. Many methods extend upon this solution,
and recent ones are especially keen on extracting rich features from images,
e.g. attribute features. These attribute features are normally extracted within
each individual image; however, the common traits for features across images
yet belonging to the same attribute are not emphasized. In this paper, we
propose a new framework to boost ZSL by explicitly learning attribute
prototypes beyond images and contrastively optimizing them with attribute-level
features within images. Besides the novel architecture, two elements are
highlighted for attribute representations: a new prototype generation module is
designed to generate attribute prototypes from attribute semantics; a hard
example-based contrastive optimization scheme is introduced to reinforce
attribute-level features in the embedding space. We explore two alternative
backbones, CNN-based and transformer-based, to build our framework and conduct
experiments on three standard benchmarks, CUB, SUN, AwA2. Results on these
benchmarks demonstrate that our method improves the state of the art by a
considerable margin. Our codes will be available at
https://github.com/dyabel/CoAR-ZSL.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Methods : Hamiltonian Domain Translation. (arXiv:2207.03843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03843">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel approach to domain translation. Leveraging
established parallels between generative models and dynamical systems, we
propose a reformulation of the Cycle-GAN architecture. By embedding our model
with a Hamiltonian structure, we obtain a continuous, expressive and most
importantly invertible generative model for domain translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consecutive Pretraining: A Knowledge Transfer Learning Strategy with Relevant Unlabeled Data for Remote Sensing Domain. (arXiv:2207.03860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03860">
<div class="article-summary-box-inner">
<span><p>Currently, under supervised learning, a model pretrained by a large-scale
nature scene dataset and then fine-tuned on a few specific task labeling data
is the paradigm that has dominated the knowledge transfer learning. It has
reached the status of consensus solution for task-aware model training in
remote sensing domain (RSD). Unfortunately, due to different categories of
imaging data and stiff challenges of data annotation, there is not a large
enough and uniform remote sensing dataset to support large-scale pretraining in
RSD. Moreover, pretraining models on large-scale nature scene datasets by
supervised learning and then directly fine-tuning on diverse downstream tasks
seems to be a crude method, which is easily affected by inevitable labeling
noise, severe domain gaps and task-aware discrepancies. Thus, in this paper,
considering the self-supervised pretraining and powerful vision transformer
(ViT) architecture, a concise and effective knowledge transfer learning
strategy called ConSecutive PreTraining (CSPT) is proposed based on the idea of
not stopping pretraining in natural language processing (NLP), which can
gradually bridge the domain gap and transfer knowledge from the nature scene
domain to the RSD. The proposed CSPT also can release the huge potential of
unlabeled data for task-aware model training. Finally, extensive experiments
are carried out on twelve datasets in RSD involving three types of downstream
tasks (e.g., scene classification, object detection and land cover
classification) and two types of imaging data (e.g., optical and SAR). The
results show that by utilizing the proposed CSPT for task-aware model training,
almost all downstream tasks in RSD can outperform the previous method of
supervised pretraining-then-fine-tuning and even surpass the state-of-the-art
(SOTA) performance without any expensive labeling consumption and careful model
design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pixel-level Correspondence for Self-Supervised Learning from Video. (arXiv:2207.03866v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03866">
<div class="article-summary-box-inner">
<span><p>While self-supervised learning has enabled effective representation learning
in the absence of labels, for vision, video remains a relatively untapped
source of supervision. To address this, we propose Pixel-level Correspondence
(PiCo), a method for dense contrastive learning from video. By tracking points
with optical flow, we obtain a correspondence map which can be used to match
local features at different points in time. We validate PiCo on standard
benchmarks, outperforming self-supervised baselines on multiple dense
prediction tasks, without compromising performance on image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Sequential Descriptors for Sequence-based Visual Place Recognition. (arXiv:2207.03868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03868">
<div class="article-summary-box-inner">
<span><p>In robotics, Visual Place Recognition is a continuous process that receives
as input a video stream to produce a hypothesis of the robot's current position
within a map of known places. This task requires robust, scalable, and
efficient techniques for real applications. This work proposes a detailed
taxonomy of techniques using sequential descriptors, highlighting different
mechanism to fuse the information from the individual images. This
categorization is supported by a complete benchmark of experimental results
that provides evidence on the strengths and weaknesses of these different
architectural choices. In comparison to existing sequential descriptors
methods, we further investigate the viability of Transformers instead of CNN
backbones, and we propose a new ad-hoc sequence-level aggregator called
SeqVLAD, which outperforms prior state of the art on different datasets. The
code is available at https://github.com/vandal-vpr/vg-transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BlindSpotNet: Seeing Where We Cannot See. (arXiv:2207.03870v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03870">
<div class="article-summary-box-inner">
<span><p>We introduce 2D blind spot estimation as a critical visual task for road
scene understanding. By automatically detecting road regions that are occluded
from the vehicle's vantage point, we can proactively alert a manual driver or a
self-driving system to potential causes of accidents (e.g., draw attention to a
road region from which a child may spring out). Detecting blind spots in full
3D would be challenging, as 3D reasoning on the fly even if the car is equipped
with LiDAR would be prohibitively expensive and error prone. We instead propose
to learn to estimate blind spots in 2D, just from a monocular camera. We
achieve this in two steps. We first introduce an automatic method for
generating ``ground-truth'' blind spot training data for arbitrary driving
videos by leveraging monocular depth estimation, semantic segmentation, and
SLAM. The key idea is to reason in 3D but from 2D images by defining blind
spots as those road regions that are currently invisible but become visible in
the near future. We construct a large-scale dataset with this automatic offline
blind spot estimation, which we refer to as Road Blind Spot (RBS) dataset.
Next, we introduce BlindSpotNet (BSN), a simple network that fully leverages
this dataset for fully automatic estimation of frame-wise blind spot
probability maps for arbitrary driving videos. Extensive experimental results
demonstrate the validity of our RBS Dataset and the effectiveness of our BSN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Power of Transfer Learning in Agricultural Applications: AgriNet. (arXiv:2207.03881v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03881">
<div class="article-summary-box-inner">
<span><p>Advances in deep learning and transfer learning have paved the way for
various automation classification tasks in agriculture, including plant
diseases, pests, weeds, and plant species detection. However, agriculture
automation still faces various challenges, such as the limited size of datasets
and the absence of plant-domain-specific pretrained models. Domain specific
pretrained models have shown state of art performance in various computer
vision tasks including face recognition and medical imaging diagnosis. In this
paper, we propose AgriNet dataset, a collection of 160k agricultural images
from more than 19 geographical locations, several images captioning devices,
and more than 423 classes of plant species and diseases. We also introduce
AgriNet models, a set of pretrained models on five ImageNet architectures:
VGG16, VGG19, Inception-v3, InceptionResNet-v2, and Xception. AgriNet-VGG19
achieved the highest classification accuracy of 94 % and the highest F1-score
of 92%. Additionally, all proposed models were found to accurately classify the
423 classes of plant species, diseases, pests, and weeds with a minimum
accuracy of 87% for the Inception-v3 model.Finally, experiments to evaluate of
superiority of AgriNet models compared to ImageNet models were conducted on two
external datasets: pest and plant diseases dataset from Bangladesh and a plant
diseases dataset from Kashmir.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Networks and Other Generative Models. (arXiv:2207.03887v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03887">
<div class="article-summary-box-inner">
<span><p>Generative networks are fundamentally different in their aim and methods
compared to CNNs for classification, segmentation, or object detection. They
have initially not been meant to be an image analysis tool, but to produce
naturally looking images. The adversarial training paradigm has been proposed
to stabilize generative methods, and has proven to be highly successful --
though by no means from the first attempt.
</p>
<p>This chapter gives a basic introduction into the motivation for Generative
Adversarial Networks (GANs) and traces the path of their success by abstracting
the basic task and working mechanism, and deriving the difficulty of early
practical approaches. Methods for a more stable training will be shown, and
also typical signs for poor convergence and their reasons.
</p>
<p>Though this chapter focuses on GANs that are meant for image generation and
image analysis, the adversarial training paradigm itself is not specific to
images, and also generalizes to tasks in image analysis. Examples of
architectures for image semantic segmentation and abnormality detection will be
acclaimed, before contrasting GANs with further generative modeling approaches
lately entering the scene. This will allow a contextualized view on the limits
but also benefits of GANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defense Against Multi-target Trojan Attacks. (arXiv:2207.03895v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03895">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks on deep learning-based models pose a significant threat
to the current AI infrastructure. Among them, Trojan attacks are the hardest to
defend against. In this paper, we first introduce a variation of the Badnet
kind of attacks that introduces Trojan backdoors to multiple target classes and
allows triggers to be placed anywhere in the image. The former makes it more
potent and the latter makes it extremely easy to carry out the attack in the
physical space. The state-of-the-art Trojan detection methods fail with this
threat model. To defend against this attack, we first introduce a trigger
reverse-engineering mechanism that uses multiple images to recover a variety of
potential triggers. We then propose a detection mechanism by measuring the
transferability of such recovered triggers. A Trojan trigger will have very
high transferability i.e. they make other images also go to the same class. We
study many practical advantages of our attack method and then demonstrate the
detection performance using a variety of image datasets. The experimental
results show the superior detection performance of our method over the
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Big Learning: A Universal Machine Learning Paradigm?. (arXiv:2207.03899v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03899">
<div class="article-summary-box-inner">
<span><p>Recent breakthroughs based on big/foundation models reveal a vague avenue for
artificial intelligence, that is, bid data, big/foundation models, big
learning, $\cdots$. Following that avenue, here we elaborate on the newly
introduced big learning. Specifically, big learning comprehensively exploits
the available information inherent in large-scale complete/incomplete data, by
simultaneously learning to model many-to-all joint/conditional/marginal data
distributions (thus named big learning) with one universal foundation model. We
reveal that big learning is what existing foundation models are implicitly
doing; accordingly, our big learning provides high-level guidance for flexible
design and improvements of foundation models, accelerating the true
self-learning on the Internet. Besides, big learning ($i$) is equipped with
marvelous flexibility for both training data and training-task customization;
($ii$) potentially delivers all joint/conditional/marginal data capabilities
after training; ($iii$) significantly reduces the training-test gap with
improved model generalization; and ($iv$) unifies conventional machine learning
paradigms e.g. supervised learning, unsupervised learning, generative learning,
etc. and enables their flexible cooperation, manifesting a universal learning
paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reproducing sensory induced hallucinations via neural fields. (arXiv:2207.03901v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03901">
<div class="article-summary-box-inner">
<span><p>Understanding sensory-induced cortical patterns in the primary visual cortex
V1 is an important challenge both for physiological motivations and for
improving our understanding of human perception and visual organisation. In
this work, we focus on pattern formation in the visual cortex when the cortical
activity is driven by a geometric visual hallucination-like stimulus. In
particular, we present a theoretical framework for sensory-induced
hallucinations which allows one to reproduce novel psychophysical results such
as the MacKay effect (Nature, 1957) and the Billock and Tsou experiences (PNAS,
2007).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Mask Attention Interaction and Scale Enhancement Network for SAR Ship Instance Segmentation. (arXiv:2207.03912v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03912">
<div class="article-summary-box-inner">
<span><p>Most of existing synthetic aperture radar (SAR) ship in-stance segmentation
models do not achieve mask interac-tion or offer limited interaction
performance. Besides, their multi-scale ship instance segmentation performance
is moderate especially for small ships. To solve these problems, we propose a
mask attention interaction and scale enhancement network (MAI-SE-Net) for SAR
ship instance segmentation. MAI uses an atrous spatial pyra-mid pooling (ASPP)
to gain multi-resolution feature re-sponses, a non-local block (NLB) to model
long-range spa-tial dependencies, and a concatenation shuffle attention block
(CSAB) to improve interaction benefits. SE uses a content-aware reassembly of
features block (CARAFEB) to generate an extra pyramid bottom-level to boost
small ship performance, a feature balance operation (FBO) to improve scale
feature description, and a global context block (GCB) to refine features.
Experimental results on two public SSDD and HRSID datasets reveal that
MAI-SE-Net outperforms the other nine competitive models, better than the
suboptimal model by 4.7% detec-tion AP and 3.4% segmentation AP on SSDD and by
3.0% detection AP and 2.4% segmentation AP on HRSID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection. (arXiv:2207.03917v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03917">
<div class="article-summary-box-inner">
<span><p>This paper presents a Refinement Pyramid Transformer (RePFormer) for robust
facial landmark detection. Most facial landmark detectors focus on learning
representative image features. However, these CNN-based feature representations
are not robust enough to handle complex real-world scenarios due to ignoring
the internal structure of landmarks, as well as the relations between landmarks
and context. In this work, we formulate the facial landmark detection task as
refining landmark queries along pyramid memories. Specifically, a pyramid
transformer head (PTH) is introduced to build both homologous relations among
landmarks and heterologous relations between landmarks and cross-scale
contexts. Besides, a dynamic landmark refinement (DLR) module is designed to
decompose the landmark regression into an end-to-end refinement procedure,
where the dynamically aggregated queries are transformed to residual
coordinates predictions. Extensive experimental results on four facial landmark
detection benchmarks and their various subsets demonstrate the superior
performance and high robustness of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Furigana Text in Images. (arXiv:2207.03960v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03960">
<div class="article-summary-box-inner">
<span><p>Furigana are pronunciation notes used in Japanese writing. Being able to
detect these can help improve optical character recognition (OCR) performance
or make more accurate digital copies of Japanese written media by correctly
displaying furigana. This project focuses on detecting furigana in Japanese
books and comics. While there has been research into the detection of Japanese
text in general, there are currently no proposed methods for detecting
furigana.
</p>
<p>We construct a new dataset containing Japanese written media and annotations
of furigana. We propose an evaluation metric for such data which is similar to
the evaluation protocols used in object detection except that it allows groups
of objects to be labeled by one annotation. We propose a method for detection
of furigana that is based on mathematical morphology and connected component
analysis. We evaluate the detections of the dataset and compare different
methods for text extraction. We also evaluate different types of images such as
books and comics individually and discuss the challenges of each type of image.
</p>
<p>The proposed method reaches an F1-score of 76\% on the dataset. The method
performs well on regular books, but less so on comics, and books of irregular
format. Finally, we show that the proposed method can improve the performance
of OCR by 5\% on the manga109 dataset.
</p>
<p>Source code is available via
\texttt{\url{https://github.com/nikolajkb/FuriganaDetection}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination. (arXiv:2207.03961v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03961">
<div class="article-summary-box-inner">
<span><p>As humans, we can modify our assumptions about a scene by imagining
alternative objects or concepts in our minds. For example, we can easily
anticipate the implications of the sun being overcast by rain clouds (e.g., the
street will get wet) and accordingly prepare for that. In this paper, we
introduce a new task/dataset called Commonsense Reasoning for Counterfactual
Scene Imagination (CoSIm) which is designed to evaluate the ability of AI
systems to reason about scene change imagination. In this task/dataset, models
are given an image and an initial question-response pair about the image. Next,
a counterfactual imagined scene change (in textual form) is applied, and the
model has to predict the new response to the initial question based on this
scene change. We collect 3.5K high-quality and challenging data instances, with
each instance consisting of an image, a commonsense question with a response, a
description of a counterfactual change, a new response to the question, and
three distractor responses. Our dataset contains various complex scene change
types (such as object addition/removal/state change, event description,
environment change, etc.) that require models to imagine many different
scenarios and reason about the changed scenes. We present a baseline model
based on a vision-language Transformer (i.e., LXMERT) and ablation studies.
Through human evaluation, we demonstrate a large human-model performance gap,
suggesting room for promising future work on this challenging counterfactual,
scene imagination task. Our code and dataset are publicly available at:
https://github.com/hyounghk/CoSIm
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Collapse in Contrast Maximization Frameworks. (arXiv:2207.04007v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04007">
<div class="article-summary-box-inner">
<span><p>Context maximization (CMax) is a framework that provides state-of-the-art
results on several event-based computer vision tasks, such as ego-motion or
optical flow estimation. However, it may suffer from a problem called event
collapse, which is an undesired solution where events are warped into too few
pixels. As prior works have largely ignored the issue or proposed workarounds,
it is imperative to analyze this phenomenon in detail. Our work demonstrates
event collapse in its simplest form and proposes collapse metrics by using
first principles of space-time deformation based on differential geometry and
physics. We experimentally show on publicly available datasets that the
proposed metrics mitigate event collapse and do not harm well-posed warps. To
the best of our knowledge, regularizers based on the proposed metrics are the
only effective solution against event collapse in the experimental settings
considered, compared with other methods. We hope that this work inspires
further research to tackle more complex warp models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCAtt: A Cognitive-Conditioned Driver Attention Dataset (Supplementary Material). (arXiv:2207.04028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04028">
<div class="article-summary-box-inner">
<span><p>The task of driver attention prediction has drawn considerable interest among
researchers in robotics and the autonomous vehicle industry. Driver attention
prediction can play an instrumental role in mitigating and preventing high-risk
events, like collisions and casualties. However, existing driver attention
prediction models neglect the distraction state and intention of the driver,
which can significantly influence how they observe their surroundings. To
address these issues, we present a new driver attention dataset, CoCAtt
(Cognitive-Conditioned Attention). Unlike previous driver attention datasets,
CoCAtt includes per-frame annotations that describe the distraction state and
intention of the driver. In addition, the attention data in our dataset is
captured in both manual and autopilot modes using eye-tracking devices of
different resolutions. Our results demonstrate that incorporating the above two
driver states into attention modeling can improve the performance of driver
attention prediction. To the best of our knowledge, this work is the first to
provide autopilot attention data. Furthermore, CoCAtt is currently the largest
and the most diverse driver attention dataset in terms of autonomy levels, eye
tracker resolutions, and driving scenarios. CoCAtt is available for download at
https://cocatt-dataset.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">k-means Mask Transformer. (arXiv:2207.04044v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04044">
<div class="article-summary-box-inner">
<span><p>The rise of transformers in vision tasks not only advances network backbone
designs, but also starts a brand-new page to achieve end-to-end image
recognition (e.g., object detection and panoptic segmentation). Originated from
Natural Language Processing (NLP), transformer architectures, consisting of
self-attention and cross-attention, effectively learn long-range interactions
between elements in a sequence. However, we observe that most existing
transformer-based vision models simply borrow the idea from NLP, neglecting the
crucial difference between languages and images, particularly the extremely
large sequence length of spatially flattened pixel features. This subsequently
impedes the learning in cross-attention between pixel features and object
queries. In this paper, we rethink the relationship between pixels and object
queries and propose to reformulate the cross-attention learning as a clustering
process. Inspired by the traditional k-means clustering algorithm, we develop a
k-means Mask Xformer (kMaX-DeepLab) for segmentation tasks, which not only
improves the state-of-the-art, but also enjoys a simple and elegant design. As
a result, our kMaX-DeepLab achieves a new state-of-the-art performance on COCO
val set with 58.0% PQ, and Cityscapes val set with 68.4% PQ, 44.0% AP, and
83.5% mIoU without test-time augmentation or external dataset. We hope our work
can shed some light on designing transformers tailored for vision tasks. Code
and models are available at https://github.com/google-research/deeplab2
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evolutionary Multi-objective Architecture Search Framework: Application to COVID-19 3D CT Classification. (arXiv:2101.10667v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10667">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has threatened global health. Many studies have applied
deep convolutional neural networks (CNN) to recognize COVID-19 based on chest
3D computed tomography (CT). Recent works show that no model generalizes well
across CT datasets from different countries, and manually designing models for
specific datasets requires expertise; thus, neural architecture search (NAS)
that aims to search models automatically has become an attractive solution. To
reduce the search cost on large 3D CT datasets, most NAS-based works use the
weight-sharing (WS) strategy to make all models share weights within a
supernet; however, WS inevitably incurs search instability, leading to
inaccurate model estimation. In this work, we propose an efficient Evolutionary
Multi-objective ARchitecture Search (EMARS) framework. We propose a new
objective, namely potential, which can help exploit promising models to
indirectly reduce the number of models involved in weights training, thus
alleviating search instability. We demonstrate that under objectives of
accuracy and potential, EMARS can balance exploitation and exploration, i.e.,
reducing search time and finding better models. Our searched models are small
and perform better than prior works on three public COVID-19 3D CT datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Topography Characterization Using a Simple Optical Device and Artificial Neural Networks. (arXiv:2103.08482v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08482">
<div class="article-summary-box-inner">
<span><p>State-of-the-art methods for quantifying wear in cylinder liners of large
internal combustion engines require disassembly and cutting of the liner. This
is followed by laboratory-based high-resolution microscopic surface depth
measurement that quantitatively evaluates wear based on bearing load curves
(Abbott-Firestone curves). Such methods are destructive, time-consuming and
costly. The goal of the research presented is to develop nondestructive yet
reliable methods for quantifying the surface topography. A novel machine
learning framework is proposed that allows prediction of the bearing load
curves from RGB images of the liner surface that can be collected with a
handheld microscope. A joint deep learning approach involving two neural
network modules optimizes the prediction quality of surface roughness
parameters as well and is trained using a custom-built database containing 422
aligned depth profile and reflection image pairs of liner surfaces. The
observed success suggests its great potential for on-site wear assessment of
engines during service.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein GANs. (arXiv:2103.16938v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16938">
<div class="article-summary-box-inner">
<span><p>Real-time estimation of actual environment depth is an essential module for
various autonomous system tasks such as localization, obstacle detection and
pose estimation. During the last decade of machine learning, extensive
deployment of deep learning methods to computer vision tasks yielded successful
approaches for realistic depth synthesis out of a simple RGB modality. While
most of these models rest on paired depth data or availability of video
sequences and stereo images, there is a lack of methods facing single-image
depth synthesis in an unsupervised manner. Therefore, in this study, latest
advancements in the field of generative neural networks are leveraged to fully
unsupervised single-image depth synthesis. To be more exact, two
cycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are
implemented and simultaneously optimized using the Wasserstein-1 distance. To
ensure plausibility of the proposed method, we apply the models to a self
acquised industrial data set as well as to the renown NYU Depth v2 data set,
which allows comparison with existing approaches. The observed success in this
study suggests high potential for unpaired single-image depth estimation in
real world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Greedy Bayesian Posterior Approximation with Deep Ensembles. (arXiv:2105.14275v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14275">
<div class="article-summary-box-inner">
<span><p>Ensembles of independently trained neural networks are a state-of-the-art
approach to estimate predictive uncertainty in Deep Learning, and can be
interpreted as an approximation of the posterior distribution via a mixture of
delta functions. The training of ensembles relies on non-convexity of the loss
landscape and random initialization of their individual members, making the
resulting posterior approximation uncontrolled. This paper proposes a novel and
principled method to tackle this limitation, minimizing an $f$-divergence
between the true posterior and a kernel density estimator (KDE) in a function
space. We analyze this objective from a combinatorial point of view, and show
that it is submodular with respect to mixture components for any $f$.
Subsequently, we consider the problem of greedy ensemble construction. From the
marginal gain on the negative $f$-divergence, which quantifies an improvement
in posterior approximation yielded by adding a new component into the KDE, we
derive a novel diversity term for ensemble methods. The performance of our
approach is demonstrated on computer vision out-of-distribution detection
benchmarks in a range of architectures trained on multiple datasets. The source
code of our method is made publicly available at
https://github.com/Oulu-IMEDS/greedy_ensembles_training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction. (arXiv:2106.12102v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12102">
<div class="article-summary-box-inner">
<span><p>Most modern deep learning-based multi-view 3D reconstruction techniques use
RNNs or fusion modules to combine information from multiple images after
independently encoding them. These two separate steps have loose connections
and do not allow easy information sharing among views. We propose LegoFormer, a
transformer model for voxel-based 3D reconstruction that uses the attention
layers to share information among views during all computational stages.
Moreover, instead of predicting each voxel independently, we propose to
parametrize the output with a series of low-rank decomposition factors. This
reformulation allows the prediction of an object as a set of independent
regular structures then aggregated to obtain the final reconstruction.
Experiments conducted on ShapeNet demonstrate the competitive performance of
our model with respect to the state of the art while having increased
interpretability thanks to the self-attention layers. We also show promising
generalization results to real data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image scaling by de la Vall\'ee-Poussin filtered interpolation. (arXiv:2109.13897v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13897">
<div class="article-summary-box-inner">
<span><p>We present a new image scaling method both for downscaling and upscaling,
running with any scale factor or desired size. The resized image is achieved by
sampling a bivariate polynomial which globally interpolates the data at the new
scale. The method's particularities lay in both the sampling model and the
interpolation polynomial we use. Rather than classical uniform grids, we
consider an unusual sampling system based on Chebyshev zeros of the first kind.
Such optimal distribution of nodes permits to consider near--best interpolation
polynomials defined by a filter of de la Vall\'ee Poussin type. The action ray
of this filter provides an additional parameter that can be suitably regulated
to improve the approximation. The method has been tested on a significant
number of different image datasets. The results are evaluated in qualitative
and quantitative terms and compared with other available competitive methods.
The perceived quality of the resulting scaled images is such that important
details are preserved, and the appearance of artifacts is low. Competitive
quality measurement values, good visual quality, limited computational effort,
and moderate memory demand make the method suitable for real-world
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. (arXiv:2111.08851v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08851">
<div class="article-summary-box-inner">
<span><p>In recent times, deep neural networks achieved outstanding predictive
performance on various classification and pattern recognition tasks. However,
many real-world prediction problems have ordinal response variables, and this
ordering information is ignored by conventional classification losses such as
the multi-category cross-entropy. Ordinal regression methods for deep neural
networks address this. One such method is the CORAL method, which is based on
an earlier binary label extension framework and achieves rank consistency among
its output layer tasks by imposing a weight-sharing constraint. However, while
earlier experiments showed that CORAL's rank consistency is beneficial for
performance, {it is limited by a weight-sharing constraint in a neural
network's fully connected output layer. We propose a new method for
rank-consistent ordinal regression without this limitation. Our rank-consistent
ordinal regression framework (CORN) achieves rank consistency by a novel
training scheme. This training scheme uses} conditional training sets to obtain
the unconditional rank probabilities through applying the chain rule for
conditional probability distributions. Experiments on various datasets
demonstrate the efficacy of the proposed method to utilize the ordinal target
information, and the absence of the weight-sharing restriction improves the
performance substantially compared to the CORAL reference approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions. (arXiv:2111.10337v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10337">
<div class="article-summary-box-inner">
<span><p>We study joint video and language (VL) pre-training to enable cross-modality
learning and benefit plentiful downstream VL tasks. Existing works either
extract low-quality video features or learn limited text embedding, while
neglecting that high-resolution videos and diversified semantics can
significantly improve cross-modality learning. In this paper, we propose a
novel High-resolution and Diversified VIdeo-LAnguage pre-training model
(HD-VILA) for many visual tasks. In particular, we collect a large dataset with
two distinct properties: 1) the first high-resolution dataset including 371.5k
hours of 720p videos, and 2) the most diversified dataset covering 15 popular
YouTube categories. To enable VL pre-training, we jointly optimize the HD-VILA
model by a hybrid Transformer that learns rich spatiotemporal features, and a
multimodal Transformer that enforces interactions of the learned video features
with diversified texts. Our pre-training model achieves new state-of-the-art
results in 10 VL understanding tasks and 2 more novel text-to-visual generation
tasks. For example, we outperform SOTA models with relative increases of 40.4%
R@1 in zero-shot MSR-VTT text-to-video retrieval task and 55.4% in
high-resolution dataset LSMDC. The learned VL embedding is also effective in
generating visually pleasing and semantically relevant results in
text-to-visual editing and super-resolution tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">diffConv: Analyzing Irregular Point Clouds with an Irregular View. (arXiv:2111.14658v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14658">
<div class="article-summary-box-inner">
<span><p>Standard spatial convolutions assume input data with a regular neighborhood
structure. Existing methods typically generalize convolution to the irregular
point cloud domain by fixing a regular "view" through e.g. a fixed neighborhood
size, where the convolution kernel size remains the same for each point.
However, since point clouds are not as structured as images, the fixed neighbor
number gives an unfortunate inductive bias. We present a novel graph
convolution named Difference Graph Convolution (diffConv), which does not rely
on a regular view. diffConv operates on spatially-varying and density-dilated
neighborhoods, which are further adapted by a learned masked attention
mechanism. Experiments show that our model is very robust to the noise,
obtaining state-of-the-art performance in 3D shape classification and scene
understanding tasks, along with a faster inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CloudWalker: Random walks for 3D point cloud shape analysis. (arXiv:2112.01050v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01050">
<div class="article-summary-box-inner">
<span><p>Point clouds are gaining prominence as a method for representing 3D shapes,
but their irregular structure poses a challenge for deep learning methods. In
this paper we propose CloudWalker, a novel method for learning 3D shapes using
random walks. Previous works attempt to adapt Convolutional Neural Networks
(CNNs) or impose a grid or mesh structure to 3D point clouds. This work
presents a different approach for representing and learning the shape from a
given point set. The key idea is to impose structure on the point set by
multiple random walks through the cloud for exploring different regions of the
3D object. Then we learn a per-point and per-walk representation and aggregate
multiple walk predictions at inference. Our approach achieves state-of-the-art
results for two 3D shape analysis tasks: classification and retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Attentional Network for Semantic Segmentation. (arXiv:2112.04108v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04108">
<div class="article-summary-box-inner">
<span><p>Recent non-local self-attention methods have proven to be effective in
capturing long-range dependencies for semantic segmentation. These methods
usually form a similarity map of RC*C (by compressing spatial dimensions) or
RHW*HW (by compressing channels) to describe the feature relations along either
channel or spatial dimensions, where C is the number of channels, H and W are
the spatial dimensions of the input feature map. However, such practices tend
to condense feature dependencies along the other dimensions,hence causing
attention missing, which might lead to inferior results for small/thin
categories or inconsistent segmentation inside large objects. To address this
problem, we propose anew approach, namely Fully Attentional Network (FLANet),to
encode both spatial and channel attentions in a single similarity map while
maintaining high computational efficiency. Specifically, for each channel map,
our FLANet can harvest feature responses from all other channel maps, and the
associated spatial positions as well, through a novel fully attentional module.
Our new method has achieved state-of-the-art performance on three challenging
semantic segmentation datasets,i.e., 83.6%, 46.99%, and 88.5% on the Cityscapes
test set,the ADE20K validation set, and the PASCAL VOC test set,respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Low-Light Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03145">
<div class="article-summary-box-inner">
<span><p>Images captured in the low-light condition suffer from low visibility and
various imaging artifacts, e.g., real noise. Existing supervised enlightening
algorithms require a large set of pixel-aligned training image pairs, which are
hard to prepare in practice. Though weakly-supervised or unsupervised methods
can alleviate such challenges without using paired training images, some
real-world artifacts inevitably get falsely amplified because of the lack of
corresponded supervision. In this paper, instead of using perfectly aligned
images for training, we creatively employ the misaligned real-world images as
the guidance, which are considerably easier to collect. Specifically, we
propose a Cross-Image Disentanglement Network (CIDN) to separately extract
cross-image brightness and image-specific content features from
low/normal-light images. Based on that, CIDN can simultaneously correct the
brightness and suppress image artifacts in the feature domain, which largely
increases the robustness to the pixel shifts. Furthermore, we collect a new
low-light image enhancement dataset consisting of misaligned training images
with real-world corruptions. Experimental results show that our model achieves
state-of-the-art performances on both the newly proposed dataset and other
popular low-light datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Attention Network. (arXiv:2202.09741v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09741">
<div class="article-summary-box-inner">
<span><p>While originally designed for natural language processing tasks, the
self-attention mechanism has recently taken various computer vision areas by
storm. However, the 2D nature of images brings three challenges for applying
self-attention in computer vision. (1) Treating images as 1D sequences neglects
their 2D structures. (2) The quadratic complexity is too expensive for
high-resolution images. (3) It only captures spatial adaptability but ignores
channel adaptability. In this paper, we propose a novel linear attention named
large kernel attention (LKA) to enable self-adaptive and long-range
correlations in self-attention while avoiding its shortcomings. Furthermore, we
present a neural network based on LKA, namely Visual Attention Network (VAN).
While extremely simple, VAN surpasses similar size vision transformers(ViTs)
and convolutional neural networks(CNNs) in various tasks, including image
classification, object detection, semantic segmentation, panoptic segmentation,
pose estimation, etc. For example, VAN-B6 achieves 87.8% accuracy on ImageNet
benchmark and set new state-of-the-art performance (58.2 PQ) for panoptic
segmentation. Besides, VAN-B2 surpasses Swin-T 4% mIoU (50.1 vs. 46.1) for
semantic segmentation on ADE20K benchmark, 2.6% AP (48.8 vs. 46.2) for object
detection on COCO dataset. It provides a novel method and a simple yet strong
baseline for the community. Code is available at
https://github.com/Visual-Attention-Network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v4 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12154">
<div class="article-summary-box-inner">
<span><p>Trojan attacks on deep neural networks are both dangerous and surreptitious.
Over the past few years, Trojan attacks have advanced from using only a single
input-agnostic trigger and targeting only one class to using multiple,
input-specific triggers and targeting multiple classes. However, Trojan
defenses have not caught up with this development. Most defense methods still
make inadequate assumptions about Trojan triggers and target classes, thus, can
be easily circumvented by modern Trojan attacks. To deal with this problem, we
propose two novel "filtering" defenses called Variational Input Filtering (VIF)
and Adversarial Input Filtering (AIF) which leverage lossy data compression and
adversarial learning respectively to effectively purify potential Trojan
triggers in the input at run time without making assumptions about the number
of triggers/target classes or the input dependence property of triggers. In
addition, we introduce a new defense mechanism called
"Filtering-then-Contrasting" (FtC) which helps avoid the drop in classification
accuracy on clean data caused by "filtering", and combine it with VIF/AIF to
derive new defenses of this kind. Extensive experimental results and ablation
studies show that our proposed defenses significantly outperform well-known
baseline defenses in mitigating five advanced Trojan attacks including two
recent state-of-the-art while being quite robust to small amounts of training
data and large-norm triggers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised classification of medical ultrasound images based on generative adversarial network. (arXiv:2203.06184v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06184">
<div class="article-summary-box-inner">
<span><p>Medical ultrasound (US) is one of the most widely used imaging modalities in
clinical practice. However, its use presents unique challenges such as variable
imaging quality. Deep learning (DL) can be used as an advanced medical US image
analysis tool, while the performance of the DL model is greatly limited by the
scarcity of big datasets. Here, we develop semi-supervised classification
enhancement (SSCE) structures by combining convolutional neural network (CNN)
and generative adversarial network (GAN) to address the data shortage. A breast
cancer dataset with 780 images is used as our base dataset. The results show
that our SSCE structures obtain an accuracy of up to 97.9%, showing a maximum
21.6% improvement compared with utilizing CNN models alone and outperforming
the previous methods using the same dataset by up to 23.9%. We believe our
proposed state-of-the-art method can be regarded as a potential auxiliary tool
for the diagnoses of medical US images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparsity and Heterogeneous Dropout for Continual Learning in the Null Space of Neural Activations. (arXiv:2203.06514v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06514">
<div class="article-summary-box-inner">
<span><p>Continual/lifelong learning from a non-stationary input data stream is a
cornerstone of intelligence. Despite their phenomenal performance in a wide
variety of applications, deep neural networks are prone to forgetting their
previously learned information upon learning new ones. This phenomenon is
called "catastrophic forgetting" and is deeply rooted in the
stability-plasticity dilemma. Overcoming catastrophic forgetting in deep neural
networks has become an active field of research in recent years. In particular,
gradient projection-based methods have recently shown exceptional performance
at overcoming catastrophic forgetting. This paper proposes two
biologically-inspired mechanisms based on sparsity and heterogeneous dropout
that significantly increase a continual learner's performance over a long
sequence of tasks. Our proposed approach builds on the Gradient Projection
Memory (GPM) framework. We leverage k-winner activations in each layer of a
neural network to enforce layer-wise sparse activations for each task, together
with a between-task heterogeneous dropout that encourages the network to use
non-overlapping activation patterns between different tasks. In addition, we
introduce two new benchmarks for continual learning under distributional shift,
namely Continual Swiss Roll and ImageNet SuperDog-40. Lastly, we provide an
in-depth analysis of our proposed method and demonstrate a significant
performance boost on various benchmark continual learning problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unbiased Directed Object Attention Graph for Object Navigation. (arXiv:2204.04421v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04421">
<div class="article-summary-box-inner">
<span><p>Object navigation tasks require agents to locate specific objects in unknown
environments based on visual information. Previously, graph convolutions were
used to implicitly explore the relationships between objects. However, due to
differences in visibility among objects, it is easy to generate biases in
object attention. Thus, in this paper, we propose a directed object attention
(DOA) graph to guide the agent in explicitly learning the attention
relationships between objects, thereby reducing the object attention bias. In
particular, we use the DOA graph to perform unbiased adaptive object attention
(UAOA) on the object features and unbiased adaptive image attention (UAIA) on
the raw images, respectively. To distinguish features in different branches, a
concise adaptive branch energy distribution (ABED) method is proposed. We
assess our methods on the AI2-Thor dataset. Compared with the state-of-the-art
(SOTA) method, our method reports 7.4%, 8.1% and 17.6% increase in success rate
(SR), success weighted by path length (SPL) and success weighted by action
efficiency (SAE), respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Satellite-based high-resolution maps of cocoa for C\^ote d'Ivoire and Ghana. (arXiv:2206.06119v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06119">
<div class="article-summary-box-inner">
<span><p>C\^ote d'Ivoire and Ghana, the world's largest producers of cocoa, account
for two thirds of the global cocoa production. In both countries, cocoa is the
primary perennial crop, providing income to almost two million farmers. Yet
precise maps of cocoa planted area are missing, hindering accurate
quantification of expansion in protected areas, production and yields, and
limiting information available for improved sustainability governance. Here, we
combine cocoa plantation data with publicly available satellite imagery in a
deep learning framework and create high-resolution maps of cocoa plantations
for both countries, validated in situ. Our results suggest that cocoa
cultivation is an underlying driver of over 37% and 13% of forest loss in
protected areas in C\^ote d'Ivoire and Ghana, respectively, and that official
reports substantially underestimate the planted area, up to 40% in Ghana. These
maps serve as a crucial building block to advance understanding of conservation
and economic development in cocoa producing regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13673">
<div class="article-summary-box-inner">
<span><p>Event cameras continue to attract interest due to desirable characteristics
such as high dynamic range, low latency, virtually no motion blur, and high
energy efficiency. One of the potential applications that would benefit from
these characteristics lies in visual place recognition for robot localization,
i.e. matching a query observation to the corresponding reference place in the
database. In this letter, we explore the distinctiveness of event streams from
a small subset of pixels (in the tens or hundreds). We demonstrate that the
absolute difference in the number of events at those pixel locations
accumulated into event frames can be sufficient for the place recognition task,
when pixels that display large variations in the reference set are used. Using
such sparse (over image coordinates) but varying (variance over the number of
events per pixel location) pixels enables frequent and computationally cheap
updates of the location estimates. Furthermore, when event frames contain a
constant number of events, our method takes full advantage of the event-driven
nature of the sensory stream and displays promising robustness to changes in
velocity. We evaluate our proposed approach on the Brisbane-Event-VPR dataset
in an outdoor driving scenario, as well as the newly contributed indoor
QCR-Event-VPR dataset that was captured with a DAVIS346 camera mounted on a
mobile robotic platform. Our results show that our approach achieves
competitive performance when compared to several baseline methods on those
datasets, and is particularly well suited for compute- and energy-constrained
platforms such as interplanetary rovers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis. (arXiv:2206.15328v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15328">
<div class="article-summary-box-inner">
<span><p>The human annotations are imperfect, especially when produced by junior
practitioners. Multi-expert consensus is usually regarded as golden standard,
while this annotation protocol is too expensive to implement in many real-world
projects. In this study, we propose a method to refine human annotation, named
Neural Annotation Refinement (NeAR). It is based on a learnable implicit
function, which decodes a latent vector into represented shape. By integrating
the appearance as an input of implicit functions, the appearance-aware NeAR
fixes the annotation artefacts. Our method is demonstrated on the application
of adrenal gland analysis. We first show that the NeAR can repair distorted
golden standards on a public adrenal gland segmentation dataset. Besides, we
develop a new Adrenal gLand ANalysis (ALAN) dataset with the proposed NeAR,
where each case consists of a 3D shape of adrenal gland and its diagnosis label
(normal vs. abnormal) assigned by experts. We show that models trained on the
shapes repaired by the NeAR can diagnose adrenal glands better than the
original ones. The ALAN dataset will be open-source, with 1,584 shapes for
adrenal gland diagnosis, which serves as a new benchmark for medical shape
analysis. Code and dataset are available at https://github.com/M3DV/NeAR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Detection Using Transfer Learning Approach from Computed Tomography Images. (arXiv:2207.00259v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00259">
<div class="article-summary-box-inner">
<span><p>Our main goal in this study is to propose a transfer learning based method
for COVID-19 detection from Computed Tomography (CT) images. The transfer
learning model used for the task is a pretrained Xception model. Both model
architecture and pre-trained weights on ImageNet were used. The resulting
modified model was trained with 128 batch size and 224x224, 3 channeled input
images, converted from original 512x512, grayscale images. The dataset used is
a the COV19-CT-DB. Labels in the dataset include COVID-19 cases and
Non-COVID-19 cases for COVID-1919 detection. Firstly, a accuracy and loss on
the validation partition of the dataset as well as precision recall and macro
F1 score were used to measure the performance of the proposed method. The
resulting Macro F1 score on the validation set exceeded the baseline model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Cross-Modal Knowledge Sharing Pre-training for Vision-Language Representation Learning and Retrieval. (arXiv:2207.00733v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00733">
<div class="article-summary-box-inner">
<span><p>Recently, the cross-modal pre-training task has been a hotspot because of its
wide application in various down-streaming researches including retrieval,
captioning, question answering and so on. However, exiting methods adopt a
one-stream pre-training model to explore the united vision-language
representation for conducting cross-modal retrieval, which easily suffer from
the calculation explosion. Moreover, although the conventional double-stream
structures are quite efficient, they still lack the vital cross-modal
interactions, resulting in low performances. Motivated by these challenges, we
put forward a Contrastive Cross-Modal Knowledge Sharing Pre-training (COOKIE)
to grasp the joint text-image representations. Structurally, COOKIE adopts the
traditional double-stream structure because of the acceptable time consumption.
To overcome the inherent defects of double-stream structure as mentioned above,
we elaborately design two effective modules. Concretely, the first module is a
weight-sharing transformer that builds on the head of the visual and textual
encoders, aiming to semantically align text and image. This design enables
visual and textual paths focus on the same semantics. The other one is three
specially designed contrastive learning, aiming to share knowledge between
different models. The shared cross-modal knowledge develops the study of
unimodal representation greatly, promoting the single-modal retrieval tasks.
Extensive experimental results on multi-modal matching researches that includes
cross-modal retrieval, text matching, and image retrieval reveal the superiors
in calculation efficiency and statistical indicators of our pre-training model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Game State Learning via Game Scene Augmentation. (arXiv:2207.01289v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01289">
<div class="article-summary-box-inner">
<span><p>Having access to accurate game state information is of utmost importance for
any artificial intelligence task including game-playing, testing, player
modeling, and procedural content generation. Self-Supervised Learning (SSL)
techniques have shown to be capable of inferring accurate game state
information from the high-dimensional pixel input of game footage into
compressed latent representations. Contrastive Learning is a popular SSL
paradigm where the visual understanding of the game's images comes from
contrasting dissimilar and similar game states defined by simple image
augmentation methods. In this study, we introduce a new game scene augmentation
technique -- named GameCLR -- that takes advantage of the game-engine to define
and synthesize specific, highly-controlled renderings of different game states,
thereby, boosting contrastive learning performance. We test our GameCLR
technique on images of the CARLA driving simulator environment and compare it
against the popular SimCLR baseline SSL method. Our results suggest that
GameCLR can infer the game's state information from game footage more
accurately compared to the baseline. Our proposed approach allows us to conduct
game artificial intelligence research by directly utilizing screen pixels as
input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatiotemporal Feature Learning Based on Two-Step LSTM and Transformer for CT Scans. (arXiv:2207.01579v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01579">
<div class="article-summary-box-inner">
<span><p>Computed tomography (CT) imaging could be very practical for diagnosing
various diseases. However, the nature of the CT images is even more diverse
since the resolution and number of the slices of a CT scan are determined by
the machine and its settings. Conventional deep learning models are hard to
tickle such diverse data since the essential requirement of the deep neural
network is the consistent shape of the input data. In this paper, we propose a
novel, effective, two-step-wise approach to tickle this issue for COVID-19
symptom classification thoroughly. First, the semantic feature embedding of
each slice for a CT scan is extracted by conventional backbone networks. Then,
we proposed a long short-term memory (LSTM) and Transformer-based sub-network
to deal with temporal feature learning, leading to spatiotemporal feature
representation learning. In this fashion, the proposed two-step LSTM model
could prevent overfitting, as well as increase performance. Comprehensive
experiments reveal that the proposed two-step method not only shows excellent
performance but also could be compensated for each other. More specifically,
the two-step LSTM model has a lower false-negative rate, while the 2-step Swin
model has a lower false-positive rate. In summary, it is suggested that the
model ensemble could be adopted for more stable and promising performance in
real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot Learning. (arXiv:2207.01798v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01798">
<div class="article-summary-box-inner">
<span><p>Generalized Zero-Shot Learning (GZSL) aims to recognize images from both the
seen and unseen classes by transferring semantic knowledge from seen to unseen
classes. It is a promising solution to take the advantage of generative models
to hallucinate realistic unseen samples based on the knowledge learned from the
seen classes. However, due to the generation shifts, the synthesized samples by
most existing methods may drift from the real distribution of the unseen data.
To address this issue, we propose a novel flow-based generative framework that
consists of multiple conditional affine coupling layers for learning unseen
data generation. Specifically, we discover and address three potential problems
that trigger the generation shifts, i.e., semantic inconsistency, variance
collapse, and structure disorder. First, to enhance the reflection of the
semantic information in the generated samples, we explicitly embed the semantic
information into the transformation in each conditional affine coupling layer.
Second, to recover the intrinsic variance of the real unseen features, we
introduce a boundary sample mining strategy with entropy maximization to
discover more difficult visual variants of semantic prototypes and hereby
adjust the decision boundary of the classifiers. Third, a relative positioning
strategy is proposed to revise the attribute embeddings, guiding them to fully
preserve the inter-class geometric structure and further avoid structure
disorder in the semantic space. Extensive experimental results on four GZSL
benchmark datasets demonstrate that GSMFlow achieves the state-of-the-art
performance on GZSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers. (arXiv:2207.02255v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02255">
<div class="article-summary-box-inner">
<span><p>We present OSFormer, the first one-stage transformer framework for
camouflaged instance segmentation (CIS). OSFormer is based on two key designs.
First, we design a location-sensing transformer (LST) to obtain the location
label and instance-aware parameters by introducing the location-guided queries
and the blend-convolution feedforward network. Second, we develop a
coarse-to-fine fusion (CFF) to merge diverse context information from the LST
encoder and CNN backbone. Coupling these two components enables OSFormer to
efficiently blend local features and long-range context dependencies for
predicting camouflaged instances. Compared with two-stage frameworks, our
OSFormer reaches 41% AP and achieves good convergence efficiency without
requiring enormous training data, i.e., only 3,040 samples under 60 epochs.
Code link: https://github.com/PJLallen/OSFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers. (arXiv:2207.02327v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02327">
<div class="article-summary-box-inner">
<span><p>Diffusion MRI tractography is an advanced imaging technique for quantitative
mapping of the brain's structural connectivity. Whole brain tractography (WBT)
data contains over hundreds of thousands of individual fiber streamlines
(estimated brain connections), and this data is usually parcellated to create
compact representations for data analysis applications such as disease
classification. In this paper, we propose a novel parcellation-free WBT
analysis framework, TractoFormer, that leverages tractography information at
the level of individual fiber streamlines and provides a natural mechanism for
interpretation of results using the attention mechanism of transformers.
TractoFormer includes two main contributions. First, we propose a novel and
simple 2D image representation of WBT, TractoEmbedding, to encode 3D fiber
spatial relationships and any feature of interest that can be computed from
individual fibers (such as FA or MD). Second, we design a network based on
vision transformers (ViTs) that includes: 1) data augmentation to overcome
model overfitting on small datasets, 2) identification of discriminative fibers
for interpretation of results, and 3) ensemble learning to leverage fiber
information from different brain regions. In a synthetic data experiment,
TractoFormer successfully identifies discriminative fibers with simulated group
differences. In a disease classification experiment comparing several methods,
TractoFormer achieves the highest accuracy in classifying schizophrenia vs
control. Discriminative fibers are identified in left hemispheric frontal and
parietal superficial white matter regions, which have previously been shown to
be affected in schizophrenia patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty of Atmospheric Motion Vectors by Sampling Tempered Posterior Distributions. (arXiv:2207.03182v2 [stat.ME] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03182">
<div class="article-summary-box-inner">
<span><p>Atmospheric motion vectors (AMVs) extracted from satellite imagery are the
only wind observations with good global coverage. They are important features
for feeding numerical weather prediction (NWP) models. Several Bayesian models
have been proposed to estimate AMVs. Although critical for correct assimilation
into NWP models, very few methods provide a thorough characterization of the
estimation errors. The difficulty of estimating errors stems from the
specificity of the posterior distribution, which is both very high dimensional,
and highly ill-conditioned due to a singular likelihood, which becomes critical
in particular in the case of missing data (unobserved pixels). This work
studies the evaluation of the expected error of AMVs using gradient-based
Markov Chain Monte Carlo (MCMC) algorithms. Our main contribution is to propose
a tempering strategy, which amounts to sampling a local approximation of the
joint posterior distribution of AMVs and image variables in the neighborhood of
a point estimate. In addition, we provide efficient preconditioning with the
covariance related to the prior family itself (fractional Brownian motion),
with possibly different hyper-parameters. From a theoretical point of view, we
show that under regularity assumptions, the family of tempered posterior
distributions converges in distribution as temperature decreases to an
{optimal} Gaussian approximation at a point estimate given by the Maximum A
Posteriori (MAP) log-density. From an empirical perspective, we evaluate the
proposed approach based on some quantitative Bayesian evaluation criteria. Our
numerical simulations performed on synthetic and real meteorological data
reveal a significant gain in terms of accuracy of the AMV point estimates and
of their associated expected error estimates, but also a substantial
acceleration in the convergence speed of the MCMC algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralGrasps: Learning Implicit Representations for Grasps of Multiple Robotic Hands. (arXiv:2207.02959v1 [cs.RO] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02959">
<div class="article-summary-box-inner">
<span><p>We introduce a neural implicit representation for grasps of objects from
multiple robotic hands. Different grasps across multiple robotic hands are
encoded into a shared latent space. Each latent vector is learned to decode to
the 3D shape of an object and the 3D shape of a robotic hand in a grasping pose
in terms of the signed distance functions of the two 3D shapes. In addition,
the distance metric in the latent space is learned to preserve the similarity
between grasps across different robotic hands, where the similarity of grasps
is defined according to contact regions of the robotic hands. This property
enables our method to transfer grasps between different grippers including a
human hand, and grasp transfer has the potential to share grasping skills
between robots and enable robots to learn grasping skills from humans.
Furthermore, the encoded signed distance functions of objects and grasps in our
implicit representation can be used for 6D object pose estimation with grasping
contact optimization from partial point clouds, which enables robotic grasping
in the real world.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-11 23:08:08.928470462 UTC">2022-07-11 23:08:08 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>