<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-01T01:30:00Z">10-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Policy Question Answering Assistant: A Query-Guided Extractive Summarization Approach. (arXiv:2109.14638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14638">
<div class="article-summary-box-inner">
<span><p>Existing work on making privacy policies accessible has explored new
presentation forms such as color-coding based on the risk factors or
summarization to assist users with conscious agreement. To facilitate a more
personalized interaction with the policies, in this work, we propose an
automated privacy policy question answering assistant that extracts a summary
in response to the input user query. This is a challenging task because users
articulate their privacy-related questions in a very different language than
the legal language of the policy, making it difficult for the system to
understand their inquiry. Moreover, existing annotated data in this domain are
limited. We address these problems by paraphrasing to bring the style and
language of the user's question closer to the language of privacy policies. Our
content scoring module uses the existing in-domain data to find relevant
information in the policy and incorporates it in a summary. Our pipeline is
able to find an answer for 89% of the user queries in the privacyQA dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classifying Tweet Sentiment Using the Hidden State and Attention Matrix of a Fine-tuned BERTweet Model. (arXiv:2109.14692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14692">
<div class="article-summary-box-inner">
<span><p>This paper introduces a study on tweet sentiment classification. Our task is
to classify a tweet as either positive or negative. We approach the problem in
two steps, namely embedding and classifying. Our baseline methods include
several combinations of traditional embedding methods and classification
algorithms. Furthermore, we explore the current state-of-the-art tweet analysis
model, BERTweet, and propose a novel approach in which features are engineered
from the hidden states and attention matrices of the model, inspired by
empirical study of the tweets. Using a multi-layer perceptron trained with a
high dropout rate for classification, our proposed approach achieves a
validation accuracy of 0.9111.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Approach For Sparse Representations Using The Locally Competitive Algorithm For Audio. (arXiv:2109.14705v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14705">
<div class="article-summary-box-inner">
<span><p>Gammachirp filterbank has been used to approximate the cochlea in sparse
coding algorithms. An oriented grid search optimization was applied to adapt
the gammachirp's parameters and improve the Matching Pursuit (MP) algorithm's
sparsity along with the reconstruction quality. However, this combination of a
greedy algorithm with a grid search at each iteration is computationally
demanding and not suitable for real-time applications. This paper presents an
adaptive approach to optimize the gammachirp's parameters but in the context of
the Locally Competitive Algorithm (LCA) that requires much fewer computations
than MP. The proposed method consists of taking advantage of the LCA's neural
architecture to automatically adapt the gammachirp's filterbank using the
backpropagation algorithm. Results demonstrate an improvement in the LCA's
performance with our approach in terms of sparsity, reconstruction quality, and
convergence time. This approach can yield a significant advantage over existing
approaches for real-time applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief. (arXiv:2109.14723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14723">
<div class="article-summary-box-inner">
<span><p>Although pretrained language models (PTLMs) contain significant amounts of
world knowledge, they can still produce inconsistent answers to questions when
probed, even after specialized training. As a result, it can be hard to
identify what the model actually "believes" about the world, making it
susceptible to inconsistent behavior and simple errors. Our goal is to reduce
these problems. Our approach is to embed a PTLM in a broader system that also
includes an evolving, symbolic memory of beliefs -- a BeliefBank -- that
records but then may modify the raw PTLM answers. We describe two mechanisms to
improve belief consistency in the overall system. First, a reasoning component
-- a weighted MaxSAT solver -- revises beliefs that significantly clash with
others. Second, a feedback component issues future queries to the PTLM using
known beliefs as context. We show that, in a controlled experimental setting,
these two mechanisms result in more consistent beliefs in the overall system,
improving both the accuracy and consistency of its answers over time. This is
significant as it is a first step towards PTLM-based architectures with a
systematic notion of belief, enabling them to construct a more coherent picture
of the world, and improve over time without model retraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System. (arXiv:2109.14739v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14739">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have been recently shown to benefit task-oriented
dialogue (TOD) systems. Despite their success, existing methods often formulate
this task as a cascaded generation problem which can lead to error accumulation
across different sub-tasks and greater data annotation overhead. In this study,
we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In
addition, we introduce a new dialogue multi-task pre-training strategy that
allows the model to learn the primary TOD task completion skills from
heterogeneous dialog corpora. We extensively test our model on three benchmark
TOD tasks, including end-to-end dialogue modelling, dialogue state tracking,
and intent classification. Experimental results show that PPTOD achieves new
state of the art on all evaluated tasks in both high-resource and low-resource
scenarios. Furthermore, comparisons against previous SOTA methods show that the
responses generated by PPTOD are more factually correct and semantically
coherent as judged by human annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications. (arXiv:2109.14776v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14776">
<div class="article-summary-box-inner">
<span><p>Certainty and uncertainty are fundamental to science communication. Hedges
have widely been used as proxies for uncertainty. However, certainty is a
complex construct, with authors expressing not only the degree but the type and
aspects of uncertainty in order to give the reader a certain impression of what
is known. Here, we introduce a new study of certainty that models both the
level and the aspects of certainty in scientific findings. Using a new dataset
of 2167 annotated scientific findings, we demonstrate that hedges alone account
for only a partial explanation of certainty. We show that both the overall
certainty and individual aspects can be predicted with pre-trained language
models, providing a more complete picture of the author's intended
communication. Downstream analyses on 431K scientific findings from news and
scientific abstracts demonstrate that modeling sentence-level and aspect-level
certainty is meaningful for areas like science communication. Both the model
and datasets used in this paper are released at
https://blablablab.si.umich.edu/projects/certainty/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tipping the Scales: A Corpus-Based Reconstruction of Adjective Scales in the McGill Pain Questionnaire. (arXiv:2109.14788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14788">
<div class="article-summary-box-inner">
<span><p>Modern medical diagnosis relies on precise pain assessment tools in
translating clinical information from patient to physician. The McGill Pain
Questionnaire (MPQ) is a clinical pain assessment technique that utilizes 78
adjectives of different intensities in 20 different categories to quantity a
patient's pain. The questionnaire's efficacy depends on a predictable pattern
of adjective use by patients experiencing pain. In this study, I recreate the
MPQ's adjective intensity orderings using data gathered from patient forums and
modern NLP techniques. I extract adjective intensity relationships by searching
for key linguistic contexts, and then combine the relationship information to
form robust adjective scales. Of 17 adjective relationships predicted by this
research, 10 show agreement with the MPQ, which is statistically significant at
the .1 alpha level. The results suggest predictable patterns of adjective use
by people experiencing pain, but call into question the MPQ's categories for
grouping adjectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phonetic Word Embeddings. (arXiv:2109.14796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14796">
<div class="article-summary-box-inner">
<span><p>This work presents a novel methodology for calculating the phonetic
similarity between words taking motivation from the human perception of sounds.
This metric is employed to learn a continuous vector embedding space that
groups similar sounding words together and can be used for various downstream
computational phonology tasks. The efficacy of the method is presented for two
different languages (English, Hindi) and performance gains over previous
reported works are discussed on established tests for predicting phonetic
similarity. To address limited benchmarking mechanisms in this field, we also
introduce a heterographic pun dataset based evaluation methodology to compare
the effectiveness of acoustic similarity algorithms. Further, a visualization
of the embedding space is presented with a discussion on the various possible
use-cases of this novel algorithm. An open-source implementation is also shared
to aid reproducibility and enable adoption in related tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Fake News Detection Using Bidirectiona lEncoder Representations from Transformers Based Models. (arXiv:2109.14816v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14816">
<div class="article-summary-box-inner">
<span><p>Nowadays, the development of social media allows people to access the latest
news easily. During the COVID-19 pandemic, it is important for people to access
the news so that they can take corresponding protective measures. However, the
fake news is flooding and is a serious issue especially under the global
pandemic. The misleading fake news can cause significant loss in terms of the
individuals and the society. COVID-19 fake news detection has become a novel
and important task in the NLP field. However, fake news always contain the
correct portion and the incorrect portion. This fact increases the difficulty
of the classification task. In this paper, we fine tune the pre-trained
Bidirectional Encoder Representations from Transformers (BERT) model as our
base model. We add BiLSTM layers and CNN layers on the top of the finetuned
BERT model with frozen parameters or not frozen parameters methods
respectively. The model performance evaluation results showcase that our best
model (BERT finetuned model with frozen parameters plus BiLSTM layers) achieves
state-of-the-art results towards COVID-19 fake news detection task. We also
explore keywords evaluation methods using our best model and evaluate the model
performance after removing keywords.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Process discovery on deviant traces and other stranger things. (arXiv:2109.14883v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14883">
<div class="article-summary-box-inner">
<span><p>As the need to understand and formalise business processes into a model has
grown over the last years, the process discovery research field has gained more
and more importance, developing two different classes of approaches to model
representation: procedural and declarative. Orthogonally to this
classification, the vast majority of works envisage the discovery task as a
one-class supervised learning process guided by the traces that are recorded
into an input log. In this work instead, we focus on declarative processes and
embrace the less-popular view of process discovery as a binary supervised
learning task, where the input log reports both examples of the normal system
execution, and traces representing "stranger" behaviours according to the
domain semantics. We therefore deepen how the valuable information brought by
both these two sets can be extracted and formalised into a model that is
"optimal" according to user-defined goals. Our approach, namely NegDis, is
evaluated w.r.t. other relevant works in this field, and shows promising
results as regards both the performance and the quality of the obtained
solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems. (arXiv:2109.14895v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14895">
<div class="article-summary-box-inner">
<span><p>In translating text where sentiment is the main message, human translators
give particular attention to sentiment-carrying words. The reason is that an
incorrect translation of such words would miss the fundamental aspect of the
source text, i.e. the author's sentiment. In the online world, MT systems are
extensively used to translate User-Generated Content (UGC) such as reviews,
tweets, and social media posts, where the main message is often the author's
positive or negative attitude towards the topic of the text. It is important in
such scenarios to accurately measure how far an MT system can be a reliable
real-life utility in transferring the correct affect message. This paper
tackles an under-recognised problem in the field of machine translation
evaluation which is judging to what extent automatic metrics concur with the
gold standard of human evaluation for a correct translation of sentiment. We
evaluate the efficacy of conventional quality metrics in spotting a
mistranslation of sentiment, especially when it is the sole error in the MT
output. We propose a numerical `sentiment-closeness' measure appropriate for
assessing the accuracy of a translated affect message in UGC text by an MT
system. We will show that incorporating this sentiment-aware measure can
significantly enhance the correlation of some available quality metrics with
the human judgement of an accurate translation of sentiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DICoE@FinSim-3: Financial Hypernym Detection using Augmented Terms and Distance-based Features. (arXiv:2109.14906v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14906">
<div class="article-summary-box-inner">
<span><p>We present the submission of team DICoE for FinSim-3, the 3rd Shared Task on
Learning Semantic Similarities for the Financial Domain. The task provides a
set of terms in the financial domain and requires to classify them into the
most relevant hypernym from a financial ontology. After augmenting the terms
with their Investopedia definitions, our system employs a Logistic Regression
classifier over financial word embeddings and a mix of hand-crafted and
distance-based features. Also, for the first time in this task, we employ
different replacement methods for out-of-vocabulary terms, leading to improved
performance. Finally, we have also experimented with word representations
generated from various financial corpora. Our best-performing submission ranked
4th on the task's leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT got a Date: Introducing Transformers to Temporal Tagging. (arXiv:2109.14927v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14927">
<div class="article-summary-box-inner">
<span><p>Temporal expressions in text play a significant role in language
understanding and correctly identifying them is fundamental to various
retrieval and natural language processing systems. Previous works have slowly
shifted from rule-based to neural architectures, capable of tagging expressions
with higher accuracy. However, neural models can not yet distinguish between
different expression types at the same level as their rule-based counterparts.
n this work, we aim to identify the most suitable transformer architecture for
joint temporal tagging and type classification, as well as, investigating the
effect of semi-supervised training on the performance of these systems. After
studying variants of token classification and encoder-decoder architectures, we
ultimately present a transformer encoder-decoder model using RoBERTa language
model as our best performing system. By supplementing training resources with
weakly labeled data from rule-based systems, our model surpasses previous works
in temporal tagging and type classification, especially on rare classes.
Additionally, we make the code and pre-trained experiment publicly available
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prose2Poem: The blessing of Transformer-based Language Models in translating Prose to Persian Poetry. (arXiv:2109.14934v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14934">
<div class="article-summary-box-inner">
<span><p>Persian Poetry has consistently expressed its philosophy, wisdom, speech, and
rationale on the basis of its couplets, making it an enigmatic language on its
own to both native and non-native speakers. Nevertheless, the notice able gap
between Persian prose and poem has left the two pieces of literature
medium-less. Having curated a parallel corpus of prose and their equivalent
poems, we introduce a novel Neural Machine Translation (NMT) approach to
translate prose to ancient Persian poetry using transformer-based Language
Models in an extremely low-resource setting. More specifically, we trained a
Transformer model from scratch to obtain initial translations and pretrained
different variations of BERT to obtain final translations. To address the
challenge of using masked language modelling under poeticness criteria, we
heuristically joined the two models and generated valid poems in terms of
automatic and human assessments. Final results demonstrate the eligibility and
creativity of our novel heuristically aided approach among Literature
professionals and non-professionals in generating novel Persian poems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntactic Persistence in Language Models: Priming as a Window into Abstract Language Representations. (arXiv:2109.14989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14989">
<div class="article-summary-box-inner">
<span><p>We investigate the extent to which modern, neural language models are
susceptible to syntactic priming, the phenomenon where the syntactic structure
of a sentence makes the same structure more probable in a follow-up sentence.
We explore how priming can be used to study the nature of the syntactic
knowledge acquired by these models. We introduce a novel metric and release
Prime-LM, a large corpus where we control for various linguistic factors which
interact with priming strength. We find that recent large Transformer models
indeed show evidence of syntactic priming, but also that the syntactic
generalisations learned by these models are to some extent modulated by
semantic information. We report surprisingly strong priming effects when
priming with multiple sentences, each with different words and meaning but with
identical syntactic structure. We conclude that the syntactic priming paradigm
is a highly useful, additional tool for gaining insights into the capacities of
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A surprisal--duration trade-off across and within the world's languages. (arXiv:2109.15000v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15000">
<div class="article-summary-box-inner">
<span><p>While there exist scores of natural languages, each with its unique features
and idiosyncrasies, they all share a unifying theme: enabling human
communication. We may thus reasonably predict that human cognition shapes how
these languages evolve and are used. Assuming that the capacity to process
information is roughly constant across human populations, we expect a
surprisal--duration trade-off to arise both across and within languages. We
analyse this trade-off using a corpus of 600 languages and, after controlling
for several potential confounds, we find strong supporting evidence in both
settings. Specifically, we find that, on average, phones are produced faster in
languages where they are less surprising, and vice versa. Further, we confirm
that more surprising phones are longer, on average, in 319 languages out of the
600. We thus conclude that there is strong evidence of a surprisal--duration
trade-off in operation, both across and within the world's languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Neural Compression Via Concurrent Pruning and Self-Distillation. (arXiv:2109.15014v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15014">
<div class="article-summary-box-inner">
<span><p>Pruning aims to reduce the number of parameters while maintaining performance
close to the original network. This work proposes a novel
\emph{self-distillation} based pruning strategy, whereby the representational
similarity between the pruned and unpruned versions of the same network is
maximized. Unlike previous approaches that treat distillation and pruning
separately, we use distillation to inform the pruning criteria, without
requiring a separate student network as in knowledge distillation. We show that
the proposed {\em cross-correlation objective for self-distilled pruning}
implicitly encourages sparse solutions, naturally complementing magnitude-based
pruning criteria. Experiments on the GLUE and XGLUE benchmarks show that
self-distilled pruning increases mono- and cross-lingual language model
performance. Self-distilled pruned models also outperform smaller Transformers
with an equal number of parameters and are competitive against (6 times) larger
distilled networks. We also observe that self-distillation (1) maximizes class
separability, (2) increases the signal-to-noise ratio, and (3) converges faster
after pruning steps, providing further insights into why self-distilled pruning
improves generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Efficient Post-training Quantization of Pre-trained Language Models. (arXiv:2109.15082v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15082">
<div class="article-summary-box-inner">
<span><p>Network quantization has gained increasing attention with the rapid growth of
large pre-trained language models~(PLMs). However, most existing quantization
methods for PLMs follow quantization-aware training~(QAT) that requires
end-to-end training with full access to the entire dataset. Therefore, they
suffer from slow training, large memory overhead, and data security issues. In
this paper, we study post-training quantization~(PTQ) of PLMs, and propose
module-wise quantization error minimization~(MREM), an efficient solution to
mitigate these issues. By partitioning the PLM into multiple modules, we
minimize the reconstruction error incurred by quantization for each module. In
addition, we design a new model parallel training strategy such that each
module can be trained locally on separate computing devices without waiting for
preceding modules, which brings nearly the theoretical training speed-up (e.g.,
$4\times$ on $4$ GPUs). Experiments on GLUE and SQuAD benchmarks show that our
proposed PTQ solution not only performs close to QAT, but also enjoys
significant reductions in training time, memory overhead, and data consumption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Key Point Analysis via Contrastive Learning and Extractive Argument Summarization. (arXiv:2109.15086v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15086">
<div class="article-summary-box-inner">
<span><p>Key point analysis is the task of extracting a set of concise and high-level
statements from a given collection of arguments, representing the gist of these
arguments. This paper presents our proposed approach to the Key Point Analysis
shared task, collocated with the 8th Workshop on Argument Mining. The approach
integrates two complementary components. One component employs contrastive
learning via a siamese neural network for matching arguments to key points; the
other is a graph-based extractive summarization model for generating key
points. In both automatic and manual evaluation, our approach was ranked best
among all submissions to the shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional generalization in semantic parsing with pretrained transformers. (arXiv:2109.15101v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15101">
<div class="article-summary-box-inner">
<span><p>Large-scale pretraining instills large amounts of knowledge in deep neural
networks. This, in turn, improves the generalization behavior of these models
in downstream tasks. What exactly are the limits to the generalization benefits
of large-scale pretraining? Here, we report observations from some simple
experiments aimed at addressing this question in the context of two semantic
parsing tasks involving natural language, SCAN and COGS. We show that language
models pretrained exclusively with non-English corpora, or even with
programming language corpora, significantly improve out-of-distribution
generalization in these benchmarks, compared with models trained from scratch,
even though both benchmarks are English-based. This demonstrates the
surprisingly broad transferability of pretrained representations and knowledge.
Pretraining with a large-scale protein sequence prediction task, on the other
hand, mostly deteriorates the generalization performance in SCAN and COGS,
suggesting that pretrained representations do not transfer universally and that
there are constraints on the similarity between the pretraining and downstream
domains for successful transfer. Finally, we show that larger models are harder
to train from scratch and their generalization accuracy is lower when trained
up to convergence on the relatively small SCAN and COGS datasets, but the
benefits of large-scale pretraining become much clearer with larger models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossAug: A Contrastive Data Augmentation Method for Debiasing Fact Verification Models. (arXiv:2109.15107v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15107">
<div class="article-summary-box-inner">
<span><p>Fact verification datasets are typically constructed using crowdsourcing
techniques due to the lack of text sources with veracity labels. However, the
crowdsourcing process often produces undesired biases in data that cause models
to learn spurious patterns. In this paper, we propose CrossAug, a contrastive
data augmentation method for debiasing fact verification models. Specifically,
we employ a two-stage augmentation pipeline to generate new claims and
evidences from existing samples. The generated samples are then paired
cross-wise with the original pair, forming contrastive samples that facilitate
the model to rely less on spurious patterns and learn more robust
representations. Experimental results show that our method outperforms the
previous state-of-the-art debiasing technique by 3.6% on the debiased extension
of the FEVER dataset, with a total performance boost of 10.13% from the
baseline. Furthermore, we evaluate our approach in data-scarce settings, where
models can be more susceptible to biases due to the lack of training data.
Experimental results demonstrate that our approach is also effective at
debiasing in these low-resource conditions, exceeding the baseline performance
on the Symmetric dataset with just 1% of the original data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of the CLEF-2019 CheckThat!: Automatic Identification and Verification of Claims. (arXiv:2109.15118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15118">
<div class="article-summary-box-inner">
<span><p>We present an overview of the second edition of the CheckThat! Lab at CLEF
2019. The lab featured two tasks in two different languages: English and
Arabic. Task 1 (English) challenged the participating systems to predict which
claims in a political debate or speech should be prioritized for fact-checking.
Task 2 (Arabic) asked to (A) rank a given set of Web pages with respect to a
check-worthy claim based on their usefulness for fact-checking that claim, (B)
classify these same Web pages according to their degree of usefulness for
fact-checking the target claim, (C) identify useful passages from these pages,
and (D) use the useful pages to predict the claim's factuality. CheckThat!
provided a full evaluation framework, consisting of data in English (derived
from fact-checking sources) and Arabic (gathered and annotated from scratch)
and evaluation based on mean average precision (MAP) and normalized discounted
cumulative gain (nDCG) for ranking, and F1 for classification. A total of 47
teams registered to participate in this lab, and fourteen of them actually
submitted runs (compared to nine last year). The evaluation results show that
the most successful approaches to Task 1 used various neural networks and
logistic regression. As for Task 2, learning-to-rank was used by the highest
scoring runs for subtask A, while different classifiers were used in the other
subtasks. We release to the research community all datasets from the lab as
well as the evaluation scripts, which should enable further research in the
important tasks of check-worthiness estimation and automatic claim
verification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved statistical machine translation using monolingual paraphrases. (arXiv:2109.15119v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15119">
<div class="article-summary-box-inner">
<span><p>We propose a novel monolingual sentence paraphrasing method for augmenting
the training data for statistical machine translation systems "for free" -- by
creating it from data that is already available rather than having to create
more aligned data. Starting with a syntactic tree, we recursively generate new
sentence variants where noun compounds are paraphrased using suitable
prepositions, and vice-versa -- preposition-containing noun phrases are turned
into noun compounds. The evaluation shows an improvement equivalent to 33%-50%
of that of doubling the amount of training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUper Team at SemEval-2016 Task 3: Building a feature-rich system for community question answering. (arXiv:2109.15120v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15120">
<div class="article-summary-box-inner">
<span><p>We present the system we built for participating in SemEval-2016 Task 3 on
Community Question Answering. We achieved the best results on subtask C, and
strong results on subtasks A and B, by combining a rich set of various types of
features: semantic, lexical, metadata, and user-related. The most important
group turned out to be the metadata for the question and for the comment,
semantic vectors trained on QatarLiving data and similarities between the
question and the comment for subtasks A and C, and between the original and the
related question for Subtask B.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-Rich Named Entity Recognition for Bulgarian Using Conditional Random Fields. (arXiv:2109.15121v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15121">
<div class="article-summary-box-inner">
<span><p>The paper presents a feature-rich approach to the automatic recognition and
categorization of named entities (persons, organizations, locations, and
miscellaneous) in news text for Bulgarian. We combine well-established features
used for other languages with language-specific lexical, syntactic and
morphological information. In particular, we make use of the rich tagset
annotation of the BulTreeBank (680 morpho-syntactic tags), from which we derive
suitable task-specific tagsets (local and nonlocal). We further add
domain-specific gazetteers and additional unlabeled data, achieving F1=89.4%,
which is comparable to the state-of-the-art results for English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Text Style Transfer using Deep Learning. (arXiv:2109.15144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15144">
<div class="article-summary-box-inner">
<span><p>Style is an integral component of a sentence indicated by the choice of words
a person makes. Different people have different ways of expressing themselves,
however, they adjust their speaking and writing style to a social context, an
audience, an interlocutor or the formality of an occasion. Text style transfer
is defined as a task of adapting and/or changing the stylistic manner in which
a sentence is written, while preserving the meaning of the original sentence.
</p>
<p>A systematic review of text style transfer methodologies using deep learning
is presented in this paper. We point out the technological advances in deep
neural networks that have been the driving force behind current successes in
the fields of natural language understanding and generation. The review is
structured around two key stages in the text style transfer process, namely,
representation learning and sentence generation in a new style. The discussion
highlights the commonalities and differences between proposed solutions as well
as challenges and opportunities that are expected to direct and foster further
research in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Sarcasm Detection Based on Contrastive Attention Mechanism. (arXiv:2109.15153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15153">
<div class="article-summary-box-inner">
<span><p>In the past decade, sarcasm detection has been intensively conducted in a
textual scenario. With the popularization of video communication, the analysis
in multi-modal scenarios has received much attention in recent years.
Therefore, multi-modal sarcasm detection, which aims at detecting sarcasm in
video conversations, becomes increasingly hot in both the natural language
processing community and the multi-modal analysis community. In this paper,
considering that sarcasm is often conveyed through incongruity between
modalities (e.g., text expressing a compliment while acoustic tone indicating a
grumble), we construct a Contras-tive-Attention-based Sarcasm Detection
(ConAttSD) model, which uses an inter-modality contrastive attention mechanism
to extract several contrastive features for an utterance. A contrastive feature
represents the incongruity of information between two modalities. Our
experiments on MUStARD, a benchmark multi-modal sarcasm dataset, demonstrate
the effectiveness of the proposed ConAttSD model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focused Contrastive Training for Test-based Constituency Analysis. (arXiv:2109.15159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15159">
<div class="article-summary-box-inner">
<span><p>We propose a scheme for self-training of grammaticality models for
constituency analysis based on linguistic tests. A pre-trained language model
is fine-tuned by contrastive estimation of grammatical sentences from a corpus,
and ungrammatical sentences that were perturbed by a syntactic test, a
transformation that is motivated by constituency theory. We show that
consistent gains can be achieved if only certain positive instances are chosen
for training, depending on whether they could be the result of a test
transformation. This way, the positives, and negatives exhibit similar
characteristics, which makes the objective more challenging for the language
model, and also allows for additional markup that indicates the position of the
test application within the sentence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual AMR Parsing with Noisy Knowledge Distillation. (arXiv:2109.15196v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15196">
<div class="article-summary-box-inner">
<span><p>We study multilingual AMR parsing from the perspective of knowledge
distillation, where the aim is to learn and improve a multilingual AMR parser
by using an existing English parser as its teacher. We constrain our
exploration in a strict multilingual setting: there is but one model to parse
all different languages including English. We identify that noisy input and
precise output are the key to successful distillation. Together with extensive
pre-training, we obtain an AMR parser whose performances surpass all previously
published results on four different foreign languages, including German,
Spanish, Italian, and Chinese, by large margins (up to 18.8 \textsc{Smatch}
points on Chinese and on average 11.3 \textsc{Smatch} points). Our parser also
achieves comparable performance on English to the latest state-of-the-art
English-only parser.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments. (arXiv:2109.15207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15207">
<div class="article-summary-box-inner">
<span><p>In the Vision-and-Language Navigation (VLN) task an embodied agent navigates
a 3D environment, following natural language instructions. A challenge in this
task is how to handle 'off the path' scenarios where an agent veers from a
reference path. Prior work supervises the agent with actions based on the
shortest path from the agent's location to the goal, but such goal-oriented
supervision is often not in alignment with the instruction. Furthermore, the
evaluation metrics employed by prior work do not measure how much of a language
instruction the agent is able to follow. In this work, we propose a simple and
effective language-aligned supervision scheme, and a new metric that measures
the number of sub-instructions the agent has completed during navigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A formal model for ledger management systems based on contracts and temporal logic. (arXiv:2109.15212v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15212">
<div class="article-summary-box-inner">
<span><p>A key component of blockchain technology is the ledger, viz., a database
that, unlike standard databases, keeps in memory the complete history of past
transactions as in a notarial archive for the benefit of any future test. In
second-generation blockchains such as Ethereum the ledger is coupled with smart
contracts, which enable the automation of transactions associated with
agreements between the parties of a financial or commercial nature. The
coupling of smart contracts and ledgers provides the technological background
for very innovative application areas, such as Decentralized Autonomous
Organizations (DAOs), Initial Coin Offerings (ICOs) and Decentralized Finance
(DeFi), which propelled blockchains beyond cryptocurrencies that were the only
focus of first generation blockchains such as the Bitcoin. However, the
currently used implementation of smart contracts as arbitrary programming
constructs has made them susceptible to dangerous bugs that can be exploited
maliciously and has moved their semantics away from that of legal contracts. We
propose here to recompose the split and recover the reliability of databases by
formalizing a notion of contract modelled as a finite-state automaton with
well-defined computational characteristics derived from an encoding in terms of
allocations of resources to actors, as an alternative to the approach based on
programming. To complete the work, we use temporal logic as the basis for an
abstract query language that is effectively suited to the historical nature of
the information kept in the ledger.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SlovakBERT: Slovak Masked Language Model. (arXiv:2109.15254v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15254">
<div class="article-summary-box-inner">
<span><p>We introduce a new Slovak masked language model called SlovakBERT in this
paper. It is the first Slovak-only transformers-based model trained on a
sizeable corpus. We evaluate the model on several NLP tasks and achieve
state-of-the-art results. We publish the masked language model, as well as the
subsequently fine-tuned models for part-of-speech tagging, sentiment analysis
and semantic textual similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks. (arXiv:2109.15256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15256">
<div class="article-summary-box-inner">
<span><p>Systematic compositionality is an essential mechanism in human language,
allowing the recombination of known parts to create novel expressions. However,
existing neural models have been shown to lack this basic ability in learning
symbolic structures. Motivated by the failure of a Transformer model on the
SCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing
a command into actions, we propose two auxiliary sequence prediction tasks that
track the progress of function and argument semantics, as additional training
supervision. These automatically-generated sequences are more representative of
the underlying compositional symbolic structures of the input data. During
inference, the model jointly predicts the next action and the next tokens in
the auxiliary sequences at each step. Experiments on the SCAN dataset show that
our method encourages the Transformer to understand compositional structures of
the command, improving its accuracy on multiple challenging splits from &lt;= 10%
to 100%. With only 418 (5%) training instances, our approach still achieves
97.8% accuracy on the MCD1 split. Therefore, we argue that compositionality can
be induced in Transformers given minimal but proper guidance. We also show that
a better result is achieved using less contextualized vectors as the
attention's query, providing insights into architecture choices in achieving
systematic compositionality. Finally, we show positive generalization results
on the groundedSCAN task (Ruis et al., 2020). Our code is publicly available
at: https://github.com/jiangycTarheel/compositional-auxseq
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MatSciBERT: A Materials Domain Language Model for Text Mining and Information Extraction. (arXiv:2109.15290v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15290">
<div class="article-summary-box-inner">
<span><p>An overwhelmingly large amount of knowledge in the materials domain is
generated and stored as text published in peer-reviewed scientific literature.
Recent developments in natural language processing, such as bidirectional
encoder representations from transformers (BERT) models, provide promising
tools to extract information from these texts. However, direct application of
these models in the materials domain may yield suboptimal results as the models
themselves may not be trained on notations and jargon that are specific to the
domain. Here, we present a materials-aware language model, namely, MatSciBERT,
which is trained on a large corpus of scientific literature published in the
materials domain. We further evaluate the performance of MatSciBERT on three
downstream tasks, namely, abstract classification, named entity recognition,
and relation extraction, on different materials datasets. We show that
MatSciBERT outperforms SciBERT, a language model trained on science corpus, on
all the tasks. Further, we discuss some of the applications of MatSciBERT in
the materials domain for extracting information, which can, in turn, contribute
to materials discovery or optimization. Finally, to make the work accessible to
the larger materials community, we make the pretrained and finetuned weights
and the models of MatSciBERT freely accessible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-granular Legal Topic Classification on Greek Legislation. (arXiv:2109.15298v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15298">
<div class="article-summary-box-inner">
<span><p>In this work, we study the task of classifying legal texts written in the
Greek language. We introduce and make publicly available a novel dataset based
on Greek legislation, consisting of more than 47 thousand official, categorized
Greek legislation resources. We experiment with this dataset and evaluate a
battery of advanced methods and classifiers, ranging from traditional machine
learning and RNN-based methods to state-of-the-art Transformer-based methods.
We show that recurrent architectures with domain-specific word embeddings offer
improved overall performance while being competitive even to transformer-based
models. Finally, we show that cutting-edge multilingual and monolingual
transformer-based models brawl on the top of the classifiers' ranking, making
us question the necessity of training monolingual transfer learning models as a
rule of thumb. To the best of our knowledge, this is the first time the task of
Greek legal text classification is considered in an open research project,
while also Greek is a language with very limited NLP resources in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Text Classification via Self-Pretraining. (arXiv:2109.15300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15300">
<div class="article-summary-box-inner">
<span><p>We present a neural semi-supervised learning model termed Self-Pretraining.
Our model is inspired by the classic self-training algorithm. However, as
opposed to self-training, Self-Pretraining is threshold-free, it can
potentially update its belief about previously labeled documents, and can cope
with the semantic drift problem. Self-Pretraining is iterative and consists of
two classifiers. In each iteration, one classifier draws a random set of
unlabeled documents and labels them. This set is used to initialize the second
classifier, to be further trained by the set of labeled documents. The
algorithm proceeds to the next iteration and the classifiers' roles are
reversed. To improve the flow of information across the iterations and also to
cope with the semantic drift problem, Self-Pretraining employs an iterative
distillation process, transfers hypotheses across the iterations, utilizes a
two-stage training model, uses an efficient learning rate schedule, and employs
a pseudo-label transformation heuristic. We have evaluated our model in three
publicly available social media datasets. Our experiments show that
Self-Pretraining outperforms the existing state-of-the-art semi-supervised
classifiers across multiple settings. Our code is available at
https://github.com/p-karisani/self_pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Graph Contextualized Knowledge into Pre-trained Language Models. (arXiv:1912.00147v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.00147">
<div class="article-summary-box-inner">
<span><p>Complex node interactions are common in knowledge graphs, and these
interactions also contain rich knowledge information. However, traditional
methods usually treat a triple as a training unit during the knowledge
representation learning (KRL) procedure, neglecting contextualized information
of the nodes in knowledge graphs (KGs). We generalize the modeling object to a
very general form, which theoretically supports any subgraph extracted from the
knowledge graph, and these subgraphs are fed into a novel transformer-based
model to learn the knowledge embeddings. To broaden usage scenarios of
knowledge, pre-trained language models are utilized to build a model that
incorporates the learned knowledge representations. Experimental results
demonstrate that our model achieves the state-of-the-art performance on several
medical NLP tasks, and improvement above TransE indicates that our KRL method
captures the graph contextualized information effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Sequence Learning for Generating Adequate Question-Answer Pairs. (arXiv:2010.01620v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01620">
<div class="article-summary-box-inner">
<span><p>Creating multiple-choice questions to assess reading comprehension of a given
article involves generating question-answer pairs (QAPs) on the main points of
the document. We present a learning scheme to generate adequate QAPs via
meta-sequence representations of sentences. A meta sequence is a sequence of
vectors comprising semantic and syntactic tags. In particular, we devise a
scheme called MetaQA to learn meta sequences from training data to form pairs
of a meta sequence for a declarative sentence (MD) and a corresponding
interrogative sentence (MIs). On a given declarative sentence, a trained MetaQA
model converts it to a meta sequence, finds a matched MD, and uses the
corresponding MIs and the input sentence to generate QAPs. We implement MetaQA
for the English language using semantic-role labeling, part-of-speech tagging,
and named-entity recognition, and show that trained on a small dataset, MetaQA
generates efficiently over the official SAT practice reading tests a large
number of syntactically and semantically correct QAPs with over 97\% accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09697">
<div class="article-summary-box-inner">
<span><p>The capacity of neural networks like the widely adopted transformer is known
to be very high. Evidence is emerging that they learn successfully due to
inductive bias in the training routine, typically a variant of gradient descent
(GD). To better understand this bias, we study the tendency for transformer
parameters to grow in magnitude ($\ell_2$ norm) during training, and its
implications for the emergent representations within self attention layers.
Empirically, we document norm growth in the training of transformer language
models, including T5 during its pretraining. As the parameters grow in
magnitude, we prove that the network approximates a discretized network with
saturated activation functions. Such "saturated" networks are known to have a
reduced capacity compared to the full network family that can be described in
terms of formal languages and automata. Our results suggest saturation is a new
characterization of an inductive bias implicit in GD of particular interest for
NLP. We leverage the emergent discrete structure in a saturated transformer to
analyze the role of different attention heads, finding that some focus locally
on a small number of positions, while other heads compute global averages,
allowing counting. We believe understanding the interplay between these two
capabilities may shed further light on the structure of computation within
large transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does My Representation Capture X? Probe-Ably. (arXiv:2104.05807v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05807">
<div class="article-summary-box-inner">
<span><p>Probing (or diagnostic classification) has become a popular strategy for
investigating whether a given set of intermediate features is present in the
representations of neural models. Probing studies may have misleading results,
but various recent works have suggested more reliable methodologies that
compensate for the possible pitfalls of probing. However, these best practices
are numerous and fast-evolving. To simplify the process of running a set of
probing experiments in line with suggested methodologies, we introduce
Probe-Ably: an extendable probing framework which supports and automates the
application of probing methods to the user's inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media. (arXiv:2104.06999v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06999">
<div class="article-summary-box-inner">
<span><p>Online social media platforms increasingly rely on Natural Language
Processing (NLP) techniques to detect abusive content at scale in order to
mitigate the harms it causes to their users. However, these techniques suffer
from various sampling and association biases present in training data, often
resulting in sub-par performance on content relevant to marginalized groups,
potentially furthering disproportionate harms towards them. Studies on such
biases so far have focused on only a handful of axes of disparities and
subgroups that have annotations/lexicons available. Consequently, biases
concerning non-Western contexts are largely ignored in the literature. In this
paper, we introduce a weakly supervised method to robustly detect lexical
biases in broader geocultural contexts. Through a case study on a publicly
available toxicity detection model, we demonstrate that our method identifies
salient groups of cross-geographic errors, and, in a follow up, demonstrate
that these groupings reflect human judgments of offensive and inoffensive
language in those geographic contexts. We also conduct analysis of a model
trained on a dataset with ground truth labels to better understand these
biases, and present preliminary mitigation experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. (arXiv:2104.08758v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08758">
<div class="article-summary-box-inner">
<span><p>Large language models have led to remarkable progress on many NLP tasks, and
researchers are turning to ever-larger text corpora to train them. Some of the
largest corpora available are made by scraping significant portions of the
internet, and are frequently introduced with only minimal documentation. In
this work we provide some of the first documentation for the Colossal Clean
Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set
of filters to a single snapshot of Common Crawl. We begin by investigating
where the data came from, and find a significant amount of text from unexpected
sources like patents and US military websites. Then we explore the content of
the text itself, and find machine-generated text (e.g., from machine
translation systems) and evaluation examples from other benchmark NLP datasets.
To understand the impact of the filters applied to create this dataset, we
evaluate the text that was removed, and show that blocklist filtering
disproportionately removes text from and about minority individuals. Finally,
we conclude with some recommendations for how to created and document web-scale
datasets from a scrape of the internet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Machine Reading Comprehension for Vietnamese Healthcare Texts. (arXiv:2105.01542v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01542">
<div class="article-summary-box-inner">
<span><p>Machine reading comprehension (MRC) is a sub-field in natural language
processing that aims to assist computers understand unstructured texts and then
answer questions related to them. In practice, the conversation is an essential
way to communicate and transfer information. To help machines understand
conversation texts, we present UIT-ViCoQA, a new corpus for conversational
machine reading comprehension in the Vietnamese language. This corpus consists
of 10,000 questions with answers over 2,000 conversations about health news
articles. Then, we evaluate several baseline approaches for conversational
machine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1
score of 45.27%, which is 30.91 points behind human performance (76.18%),
indicating that there is ample room for improvement. Our dataset is available
at our website: <a href="http://nlp.uit.edu.vn/datasets/">this http URL</a> for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. (arXiv:2105.02605v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02605">
<div class="article-summary-box-inner">
<span><p>The representation learning on textual graph is to generate low-dimensional
embeddings for the nodes based on the individual textual features and the
neighbourhood information. Recent breakthroughs on pretrained language models
and graph neural networks push forward the development of corresponding
techniques. The existing works mainly rely on the cascaded model architecture:
the textual features of nodes are independently encoded by language models at
first; the textual embeddings are aggregated by graph neural networks
afterwards. However, the above architecture is limited due to the independent
modeling of textual features. In this work, we propose GraphFormers, where
layerwise GNN components are nested alongside the transformer blocks of
language models. With the proposed architecture, the text encoding and the
graph aggregation are fused into an iterative workflow, making each node's
semantic accurately comprehended from the global perspective. In addition, a
progressive learning strategy is introduced, where the model is successively
trained on manipulated data and original data to reinforce its capability of
integrating information on graph. Extensive evaluations are conducted on three
large-scale benchmark datasets, where GraphFormers outperform the SOTA
baselines with comparable running efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation. (arXiv:2106.11096v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11096">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a novel and easy-to-apply data augmentation
strategy, namely Bilateral Generation (BiG), with a contrastive training
objective for improving the performance of ranking question answer pairs with
existing labeled data. In specific, we synthesize pseudo-positive QA pairs in
contrast to the original negative QA pairs with two pre-trained generation
models, one for question generation, the other for answer generation, which are
fine-tuned on the limited positive QA pairs from the original dataset. With the
augmented dataset, we design a contrastive training objective for learning to
rank question answer pairs. Experimental results on three benchmark datasets
show that our method significantly improves the performance of ranking models
by making full use of existing labeled data and can be easily applied to
different ranking models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Rule Generation for Time Expression Normalization. (arXiv:2108.13658v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13658">
<div class="article-summary-box-inner">
<span><p>The understanding of time expressions includes two sub-tasks: recognition and
normalization. In recent years, significant progress has been made in the
recognition of time expressions while research on normalization has lagged
behind. Existing SOTA normalization methods highly rely on rules or grammars
designed by experts, which limits their performance on emerging corpora, such
as social media texts. In this paper, we model time expression normalization as
a sequence of operations to construct the normalized temporal value, and we
present a novel method called ARTime, which can automatically generate
normalization rules from training data without expert interventions.
Specifically, ARTime automatically captures possible operation sequences from
annotated data and generates normalization rules on time expressions with
common surface forms. The experimental results show that ARTime can
significantly surpass SOTA methods on the Tweets benchmark, and achieves
competitive results with existing expert-engineered rule methods on the
TempEval-3 benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Bias in NLP -- Application to Hate Speech Classification using transfer learning techniques. (arXiv:2109.09725v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09725">
<div class="article-summary-box-inner">
<span><p>In this paper, a BERT based neural network model is applied to the JIGSAW
data set in order to create a model identifying hateful and toxic comments
(strictly seperated from offensive language) in online social platforms
(English language), in this case Twitter. Three other neural network
architectures and a GPT-2 model are also applied on the provided data set in
order to compare these different models. The trained BERT model is then applied
on two different data sets to evaluate its generalisation power, namely on
another Twitter data set and the data set HASOC 2019 which includes Twitter and
also Facebook comments; we focus on the English HASOC 2019 data. In addition,
it can be shown that by fine-tuning the trained BERT model on these two data
sets by applying different transfer learning scenarios via retraining partial
or all layers the predictive scores improve compared to simply applying the
model pre-trained on the JIGSAW data set. With our results, we get precisions
from 64% to around 90% while still achieving acceptable recall values of at
least lower 60s%, proving that BERT is suitable for real use cases in social
platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?. (arXiv:2109.11321v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11321">
<div class="article-summary-box-inner">
<span><p>Large language models are known to suffer from the hallucination problem in
that they are prone to output statements that are false or inconsistent,
indicating a lack of knowledge. A proposed solution to this is to provide the
model with additional data modalities that complements the knowledge obtained
through text. We investigate the use of visual data to complement the knowledge
of large language models by proposing a method for evaluating visual knowledge
transfer to text for uni- or multimodal language models. The method is based on
two steps, 1) a novel task querying for knowledge of memory colors, i.e.
typical colors of well-known objects, and 2) filtering of model training data
to clearly separate knowledge contributions. Additionally, we introduce a model
architecture that involves a visual imagination step and evaluate it with our
proposed method. We find that our method can successfully be used to measure
visual knowledge transfer capabilities in models and that our novel model
architecture shows promising results for leveraging multimodal knowledge in a
unimodal setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog. (arXiv:2109.12212v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12212">
<div class="article-summary-box-inner">
<span><p>Online conversations include more than just text. Increasingly, image-based
responses such as memes and animated gifs serve as culturally recognized and
often humorous responses in conversation. However, while NLP has broadened to
multimodal models, conversational dialog systems have largely focused only on
generating text replies. Here, we introduce a new dataset of 1.56M text-gif
conversation turns and introduce a new multimodal conversational model Pepe the
King Prawn for selecting gif-based replies. We demonstrate that our model
produces relevant and high-quality gif responses and, in a large randomized
control trial of multiple models replying to real users, we show that our model
replies with gifs that are significantly better received by the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prefix-to-SQL: Text-to-SQL Generation from Incomplete User Questions. (arXiv:2109.13066v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13066">
<div class="article-summary-box-inner">
<span><p>Existing text-to-SQL research only considers complete questions as the input,
but lay-users might strive to formulate a complete question. To build a smarter
natural language interface to database systems (NLIDB) that also processes
incomplete questions, we propose a new task, prefix-to-SQL which takes question
prefix from users as the input and predicts the intended SQL. We construct a
new benchmark called PAGSAS that contains 124K user question prefixes and the
intended SQL for 5 sub-tasks Advising, GeoQuery, Scholar, ATIS, and Spider.
Additionally, we propose a new metric SAVE to measure how much effort can be
saved by users. Experimental results show that PAGSAS is challenging even for
strong baseline models such as T5. As we observe the difficulty of
prefix-to-SQL is related to the number of omitted tokens, we incorporate
curriculum learning of feeding examples with an increasing number of omitted
tokens. This improves scores on various sub-tasks by as much as 9% recall
scores on sub-task GeoQuery in PAGSAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Generation of Word Problems for Academic Education via Natural Language Processing (NLP). (arXiv:2109.13123v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13123">
<div class="article-summary-box-inner">
<span><p>Digital learning platforms enable students to learn on a flexible and
individual schedule as well as providing instant feedback mechanisms. The field
of STEM education requires students to solve numerous training exercises to
grasp underlying concepts. It is apparent that there are restrictions in
current online education in terms of exercise diversity and individuality. Many
exercises show little variance in structure and content, hindering the adoption
of abstraction capabilities by students. This thesis proposes an approach to
generate diverse, context rich word problems. In addition to requiring the
generated language to be grammatically correct, the nature of word problems
implies additional constraints on the validity of contents. The proposed
approach is proven to be effective in generating valid word problems for
mathematical statistics. The experimental results present a tradeoff between
generation time and exercise validity. The system can easily be parametrized to
handle this tradeoff according to the requirements of specific use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPSL: End-to-end perception and reasoning with applications to zero shot learning. (arXiv:2109.13662v2 [eess.SY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13662">
<div class="article-summary-box-inner">
<span><p>We introduce DeepPSL a variant of Probabilistic Soft Logic (PSL) to produce
an end-to-end trainable system that integrates reasoning and perception. PSL
represents first-order logic in terms of a convex graphical model -- Hinge Loss
Markov random fields (HL-MRFs). PSL stands out among probabilistic logic
frameworks due to its tractability having been applied to systems of more than
1 billion ground rules. The key to our approach is to represent predicates in
first-order logic using deep neural networks and then to approximately
back-propagate through the HL-MRF and thus train every aspect of the
first-order system being represented. We believe that this approach represents
an interesting direction for the integration of deep learning and reasoning
techniques with applications to knowledge base learning, multi-task learning,
and explainability. We evaluate DeepPSL on a zero shot learning problem in
image classification. State of the art results demonstrate the utility and
flexibility of our approach.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Self-Supervised Contrastive Learning via Ensemble Similarity Distillation. (arXiv:2109.14611v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14611">
<div class="article-summary-box-inner">
<span><p>This paper investigates the feasibility of learning good representation space
with unlabeled client data in the federated scenario. Existing works trivially
inherit the supervised federated learning methods, which does not apply to the
model heterogeneity and has the potential risk of privacy exposure. To tackle
the problems above, we first identify that self-supervised contrastive local
training is more robust against the non-i.i.d.-ness than the traditional
supervised learning paradigm. Then we propose a novel federated self-supervised
contrastive learning framework FLESD that supports architecture-agnostic local
training and communication-efficient global aggregation. At each round of
communication, the server first gathers a fraction of the clients' inferred
similarity matrices on a public dataset. Then FLESD ensembles the similarity
matrices and trains the global model via similarity distillation. We verify the
effectiveness of our proposed framework by a series of empirical experiments
and show that FLESD has three main advantages over the existing methods: it
handles the model heterogeneity, is less prone to privacy leak, and is more
communication-efficient. We will release the code of this paper in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FathomNet: A global underwater image training set for enabling artificial intelligence in the ocean. (arXiv:2109.14646v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14646">
<div class="article-summary-box-inner">
<span><p>Ocean-going platforms are integrating high-resolution camera feeds for
observation and navigation, producing a deluge of visual data. The volume and
rate of this data collection can rapidly outpace researchers' abilities to
process and analyze them. Recent advances in machine learning enable fast,
sophisticated analysis of visual data, but have had limited success in the
oceanographic world due to lack of dataset standardization, sparse annotation
tools, and insufficient formatting and aggregation of existing, expertly
curated imagery for use by data scientists. To address this need, we have built
FathomNet, a public platform that makes use of existing (and future), expertly
curated data. Initial efforts have leveraged MBARI's Video Annotation and
Reference System and annotated deep sea video database, which has more than 7M
annotations, 1M framegrabs, and 5k terms in the knowledgebase, with additional
contributions by National Geographic Society (NGS) and NOAA's Office of Ocean
Exploration and Research. FathomNet has over 100k localizations of 1k midwater
and benthic classes, and contains iconic and non-iconic views of marine
animals, underwater equipment, debris, etc. We will demonstrate how machine
learning models trained on FathomNet data can be applied across different
institutional video data, (e.g., NGS' Deep Sea Camera System and NOAA's ROV
Deep Discoverer), and enable automated acquisition and tracking of midwater
animals using MBARI's ROV MiniROV. As FathomNet continues to develop and
incorporate more image data from other oceanographic community members, this
effort will enable scientists, explorers, policymakers, storytellers, and the
public to understand and care for our ocean.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-aware Mean Teacher for Source-free Unsupervised Domain Adaptive 3D Object Detection. (arXiv:2109.14651v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14651">
<div class="article-summary-box-inner">
<span><p>Pseudo-label based self training approaches are a popular method for
source-free unsupervised domain adaptation. However, their efficacy depends on
the quality of the labels generated by the source trained model. These labels
may be incorrect with high confidence, rendering thresholding methods
ineffective. In order to avoid reinforcing errors caused by label noise, we
propose an uncertainty-aware mean teacher framework which implicitly filters
incorrect pseudo-labels during training. Leveraging model uncertainty allows
the mean teacher network to perform implicit filtering by down-weighing losses
corresponding uncertain pseudo-labels. Effectively, we perform automatic
soft-sampling of pseudo-labeled data while aligning predictions from the
student and teacher networks. We demonstrate our method on several domain
adaptation scenarios, from cross-dataset to cross-weather conditions, and
achieve state-of-the-art performance in these cases, on the KITTI lidar target
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Egocentric Hand-Object Interactions from Hand Pose Estimation. (arXiv:2109.14657v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14657">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the problem of estimating the hand pose from the
egocentric view when the hand is interacting with objects. Specifically, we
propose a method to label a dataset Ego-Siam which contains the egocentric
images pair-wisely. We also use the collected pairwise data to train our
encoder-decoder style network which has been proven efficient in. This could
bring extra training efficiency and testing accuracy. Our network is
lightweight and can be performed with over 30 FPS with an outdated GPU. We
demonstrate that our method outperforms Mueller et al. which is the state of
the art work dealing with egocentric hand-object interaction problems on the
GANerated dataset. To show the ability to preserve the semantic information of
our method, we also report the performance of grasp type classification on
GUN-71 dataset and outperforms the benchmark by only using the predicted 3-d
hand pose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation of Roads in Satellite Images using specially modified U-Net CNNs. (arXiv:2109.14671v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14671">
<div class="article-summary-box-inner">
<span><p>The image classification problem has been deeply investigated by the research
community, with computer vision algorithms and with the help of Neural
Networks. The aim of this paper is to build an image classifier for satellite
images of urban scenes that identifies the portions of the images in which a
road is located, separating these portions from the rest. Unlike conventional
computer vision algorithms, convolutional neural networks (CNNs) provide
accurate and reliable results on this task. Our novel approach uses a sliding
window to extract patches out of the whole image, data augmentation for
generating more training/testing data and lastly a series of specially modified
U-Net CNNs. This proposed technique outperforms all other baselines tested in
terms of mean F-score metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Estimation of Ulcerative Colitis Severity from Endoscopy Videos using Ordinal Multi-Instance Learning. (arXiv:2109.14685v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14685">
<div class="article-summary-box-inner">
<span><p>Ulcerative colitis (UC) is a chronic inflammatory bowel disease characterized
by relapsing inflammation of the large intestine. The severity of UC is often
represented by the Mayo Endoscopic Subscore (MES) which quantifies mucosal
disease activity from endoscopy videos. In clinical trials, an endoscopy video
is assigned an MES based upon the most severe disease activity observed in the
video. For this reason, severe inflammation spread throughout the colon will
receive the same MES as an otherwise healthy colon with severe inflammation
restricted to a small, localized segment. Therefore, the extent of disease
activity throughout the large intestine, and overall response to treatment, may
not be completely captured by the MES. In this work, we aim to automatically
estimate UC severity for each frame in an endoscopy video to provide a higher
resolution assessment of disease activity throughout the colon. Because
annotating severity at the frame-level is expensive, labor-intensive, and
highly subjective, we propose a novel weakly supervised, ordinal classification
method to estimate frame severity from video MES labels alone. Using clinical
trial data, we first achieved 0.92 and 0.90 AUC for predicting mucosal healing
and remission of UC, respectively. Then, for severity estimation, we
demonstrate that our models achieve substantial Cohen's Kappa agreement with
ground truth MES labels, comparable to the inter-rater agreement of expert
clinicians. These findings indicate that our framework could serve as a
foundation for novel clinical endpoints, based on a more localized scoring
system, to better evaluate UC drug efficacy in clinical trials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Aided Beam Tracking: Explore the Proper Use of Camera Images with Deep Learning. (arXiv:2109.14686v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14686">
<div class="article-summary-box-inner">
<span><p>We investigate the problem of wireless beam tracking on mmWave bands with the
assistance of camera images. In particular, based on the user's beam indices
used and camera images taken in the trajectory, we predict the optimal beam
indices in the next few time spots. To resolve this problem, we first
reformulate the "ViWi" dataset in [1] to get rid of the image repetition
problem. Then we develop a deep learning approach and investigate various model
components to achieve the best performance. Finally, we explore whether, when,
and how to use the image for better beam prediction. To answer this question,
we split the dataset into three clusters -- (LOS, light NLOS, serious
NLOS)-like -- based on the standard deviation of the beam sequence. With
experiments we demonstrate that using the image indeed helps beam tracking
especially when the user is in serious NLOS, and the solution relies on
carefully-designed dataset for training a model. Generally speaking, including
NLOS-like data for training a model does not benefit beam tracking of the user
in LOS, but including light NLOS-like data for training a model benefits beam
tracking of the user in serious NLOS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LR-to-HR Face Hallucination with an Adversarial Progressive Attribute-Induced Network. (arXiv:2109.14690v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14690">
<div class="article-summary-box-inner">
<span><p>Face super-resolution is a challenging and highly ill-posed problem since a
low-resolution (LR) face image may correspond to multiple high-resolution (HR)
ones during the hallucination process and cause a dramatic identity change for
the final super-resolved results. Thus, to address this problem, we propose an
end-to-end progressive learning framework incorporating facial attributes and
enforcing additional supervision from multi-scale discriminators. By
incorporating facial attributes into the learning process and progressively
resolving the facial image, the mapping between LR and HR images is constrained
more, and this significantly helps to reduce the ambiguity and uncertainty in
one-to-many mapping. In addition, we conduct thorough evaluations on the CelebA
dataset following the settings of previous works (i.e. super-resolving by a
factor of 8x from tiny 16x16 face images.), and the results demonstrate that
the proposed approach can yield satisfactory face hallucination images
outperforming other state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Network Compression through Generalized Kronecker Product Decomposition. (arXiv:2109.14710v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14710">
<div class="article-summary-box-inner">
<span><p>Modern Convolutional Neural Network (CNN) architectures, despite their
superiority in solving various problems, are generally too large to be deployed
on resource constrained edge devices. In this paper, we reduce memory usage and
floating-point operations required by convolutional layers in CNNs. We compress
these layers by generalizing the Kronecker Product Decomposition to apply to
multidimensional tensors, leading to the Generalized Kronecker Product
Decomposition(GKPD). Our approach yields a plug-and-play module that can be
used as a drop-in replacement for any convolutional layer. Experimental results
for image classification on CIFAR-10 and ImageNet datasets using ResNet,
MobileNetv2 and SeNet architectures substantiate the effectiveness of our
proposed approach. We find that GKPD outperforms state-of-the-art decomposition
methods including Tensor-Train and Tensor-Ring as well as other relevant
compression methods such as pruning and knowledge distillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">USIS: Unsupervised Semantic Image Synthesis. (arXiv:2109.14715v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14715">
<div class="article-summary-box-inner">
<span><p>Semantic Image Synthesis (SIS) is a subclass of image-to-image translation
where a photorealistic image is synthesized from a segmentation mask. SIS has
mostly been addressed as a supervised problem. However, state-of-the-art
methods depend on a huge amount of labeled data and cannot be applied in an
unpaired setting. On the other hand, generic unpaired image-to-image
translation frameworks underperform in comparison, because they color-code
semantic layouts and feed them to traditional convolutional networks, which
then learn correspondences in appearance instead of semantic content. In this
initial work, we propose a new Unsupervised paradigm for Semantic Image
Synthesis (USIS) as a first step towards closing the performance gap between
paired and unpaired settings. Notably, the framework deploys a SPADE generator
that learns to output images with visually separable semantic classes using a
self-supervised segmentation loss. Furthermore, in order to match the color and
texture distribution of real images without losing high-frequency information,
we propose to use whole image wavelet-based discrimination. We test our
methodology on 3 challenging datasets and demonstrate its ability to generate
multimodal photorealistic images with an improved quality in the unpaired
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Targeted Gradient Descent: A Novel Method for Convolutional Neural Networks Fine-tuning and Online-learning. (arXiv:2109.14729v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14729">
<div class="article-summary-box-inner">
<span><p>A convolutional neural network (ConvNet) is usually trained and then tested
using images drawn from the same distribution. To generalize a ConvNet to
various tasks often requires a complete training dataset that consists of
images drawn from different tasks. In most scenarios, it is nearly impossible
to collect every possible representative dataset as a priori. The new data may
only become available after the ConvNet is deployed in clinical practice.
ConvNet, however, may generate artifacts on out-of-distribution testing
samples. In this study, we present Targeted Gradient Descent (TGD), a novel
fine-tuning method that can extend a pre-trained network to a new task without
revisiting data from the previous task while preserving the knowledge acquired
from previous training. To a further extent, the proposed method also enables
online learning of patient-specific data. The method is built on the idea of
reusing a pre-trained ConvNet's redundant kernels to learn new knowledge. We
compare the performance of TGD to several commonly used training approaches on
the task of Positron emission tomography (PET) image denoising. Results from
clinical images show that TGD generated results on par with
training-from-scratch while significantly reducing data preparation and network
training time. More importantly, it enables online learning on the testing
study to enhance the network's generalization capability in real-world
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Egocentric Hand-object Interaction Detection and Application. (arXiv:2109.14734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14734">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a method to detect the hand-object interaction from
an egocentric perspective. In contrast to massive data-driven discriminator
based method like \cite{Shan20}, we propose a novel workflow that utilises the
cues of hand and object. Specifically, we train networks predicting hand pose,
hand mask and in-hand object mask to jointly predict the hand-object
interaction status. We compare our method with the most recent work from Shan
et al. \cite{Shan20} on selected images from EPIC-KITCHENS
\cite{damen2018scaling} dataset and achieve $89\%$ accuracy on HOI (hand-object
interaction) detection which is comparative to Shan's ($92\%$). However, for
real-time performance, with the same machine, our method can run over
$\textbf{30}$ FPS which is much efficient than Shan's
($\textbf{1}\sim\textbf{2}$ FPS). Furthermore, with our approach, we are able
to segment script-less activities from where we extract the frames with the HOI
status detection. We achieve $\textbf{68.2\%}$ and $\textbf{82.8\%}$ F1 score
on GTEA \cite{fathi2011learning} and the UTGrasp \cite{cai2015scalable} dataset
respectively which are all comparative to the SOTA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlocking the potential of deep learning for marine ecology: overview, applications, and outlook. (arXiv:2109.14737v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14737">
<div class="article-summary-box-inner">
<span><p>The deep learning revolution is touching all scientific disciplines and
corners of our lives as a means of harnessing the power of big data. Marine
ecology is no exception. These new methods provide analysis of data from
sensors, cameras, and acoustic recorders, even in real time, in ways that are
reproducible and rapid. Off-the-shelf algorithms can find, count, and classify
species from digital images or video and detect cryptic patterns in noisy data.
Using these opportunities requires collaboration across ecological and data
science disciplines, which can be challenging to initiate. To facilitate these
collaborations and promote the use of deep learning towards ecosystem-based
management of the sea, this paper aims to bridge the gap between marine
ecologists and computer scientists. We provide insight into popular deep
learning approaches for ecological data analysis in plain language, focusing on
the techniques of supervised learning with deep neural networks, and illustrate
challenges and opportunities through established and emerging applications of
deep learning to marine ecology. We use established and future-looking case
studies on plankton, fishes, marine mammals, pollution, and nutrient cycling
that involve object detection, classification, tracking, and segmentation of
visualized data. We conclude with a broad outlook of the field's opportunities
and challenges, including potential technological advances and issues with
managing complex data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Object at Hand: Automated Editing for Mixed Reality Video Guidance from Hand-Object Interactions. (arXiv:2109.14744v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14744">
<div class="article-summary-box-inner">
<span><p>In this paper, we concern with the problem of how to automatically extract
the steps that compose real-life hand activities. This is a key competence
towards processing, monitoring and providing video guidance in Mixed Reality
systems. We use egocentric vision to observe hand-object interactions in
real-world tasks and automatically decompose a video into its constituent
steps. Our approach combines hand-object interaction (HOI) detection, object
similarity measurement and a finite state machine (FSM) representation to
automatically edit videos into steps. We use a combination of Convolutional
Neural Networks (CNNs) and the FSM to discover, edit cuts and merge segments
while observing real hand activities. We evaluate quantitatively and
qualitatively our algorithm on two datasets: the GTEA\cite{li2015delving}, and
a new dataset we introduce for Chinese Tea making. Results show our method is
able to segment hand-object interaction videos into key step segments with high
levels of precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improvising the Learning of Neural Networks on Hyperspherical Manifold. (arXiv:2109.14746v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14746">
<div class="article-summary-box-inner">
<span><p>The impact of convolution neural networks (CNNs) in the supervised settings
provided tremendous increment in performance. The representations learned from
CNN's operated on hyperspherical manifold led to insightful outcomes in face
recognition, face identification and other supervised tasks. A broad range of
activation functions is developed with hypersphere intuition which performs
superior to softmax in euclidean space. The main motive of this research is to
provide insights. First, the stereographic projection is implied to transform
data from Euclidean space ($\mathbb{R}^{n}$) to hyperspherical manifold
($\mathbb{S}^{n}$) to analyze the performance of angular margin losses.
Secondly, proving both theoretically and practically that decision boundaries
constructed on hypersphere using stereographic projection obliges the learning
of neural networks. Experiments have proved that applying stereographic
projection on existing state-of-the-art angular margin objective functions led
to improve performance for standard image classification data sets
(CIFAR-10,100). The code is publicly available at:
https://github.com/barulalithb/stereo-angular-margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaHistoSeg: A Python Framework for Meta Learning in Histopathology Image Segmentation. (arXiv:2109.14754v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14754">
<div class="article-summary-box-inner">
<span><p>Few-shot learning is a standard practice in most deep learning based
histopathology image segmentation, given the relatively low number of digitized
slides that are generally available. While many models have been developed for
domain specific histopathology image segmentation, cross-domain generalization
remains a key challenge for properly validating models. Here, tooling and
datasets to benchmark model performance across histopathological domains are
lacking. To address this limitation, we introduce MetaHistoSeg - a Python
framework that implements unique scenarios in both meta learning and instance
based transfer learning. Designed for easy extension to customized datasets and
task sampling schemes, the framework empowers researchers with the ability of
rapid model design and experimentation. We also curate a histopathology meta
dataset - a benchmark dataset for training and validating models on
out-of-distribution performance across a range of cancer types. In experiments
we showcase the usage of MetaHistoSeg with the meta dataset and find that both
meta-learning and instance based transfer learning deliver comparable results
on average, but in some cases tasks can greatly benefit from one over the
other.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chest X-Rays Image Classification from beta-Variational Autoencoders Latent Features. (arXiv:2109.14760v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14760">
<div class="article-summary-box-inner">
<span><p>Chest X-Ray (CXR) is one of the most common diagnostic techniques used in
everyday clinical practice all around the world. We hereby present a work which
intends to investigate and analyse the use of Deep Learning (DL) techniques to
extract information from such images and allow to classify them, trying to keep
our methodology as general as possible and possibly also usable in a real world
scenario without much effort, in the future. To move in this direction, we
trained several beta-Variational Autoencoder (beta-VAE) models on the CheXpert
dataset, one of the largest publicly available collection of labeled CXR
images; from these models, latent features have been extracted and used to
train other Machine Learning models, able to classify the original images from
the features extracted by the beta-VAE. Lastly, tree-based models have been
combined together in ensemblings to improve the results without the necessity
of further training or models engineering. Expecting some drop in pure
performance with the respect to state of the art classification specific
models, we obtained encouraging results, which show the viability of our
approach and the usability of the high level features extracted by the
autoencoders for classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Prior Knowledge Based Tumor and Tumoral Subregion Segmentation Tool for Pediatric Brain Tumors. (arXiv:2109.14775v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14775">
<div class="article-summary-box-inner">
<span><p>In the past few years, deep learning (DL) models have drawn great attention
and shown superior performance on brain tumor and subregion segmentation tasks.
However, the success is limited to segmentation of adult gliomas, where
sufficient data have been collected, manually labeled, and published for
training DL models. It is still challenging to segment pediatric tumors,
because the appearances are different from adult gliomas. Hence, directly
applying a pretained DL model on pediatric data usually generates unacceptable
results. Because pediatric data is very limited, both labeled and unlabeled, we
present a brain tumor segmentation model that is based on knowledge rather than
learning from data. We also provide segmentation of more subregions for super
heterogeneous tumor like atypical teratoid rhabdoid tumor (ATRT). Our proposed
approach showed superior performance on both whole tumor and subregion
segmentation tasks to DL based models on our pediatric data when training data
is not available for transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated airway segmentation by learning graphical structure. (arXiv:2109.14792v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14792">
<div class="article-summary-box-inner">
<span><p>In this research project, we put forward an advanced method for airway
segmentation based on the existent convolutional neural network (CNN) and graph
neural network (GNN). The method is originated from the vessel segmentation,
but we ameliorate it and enable the novel model to perform better for datasets
from computed tomography (CT) scans. Current methods for airway segmentation
are considering the regular grid only. No matter what the detailed model is,
including the 3-dimensional CNN or 2-dimensional CNN in three directions, the
overall graph structures are not taken into consideration. In our model, with
the neighbourhoods of airway taken into account, the graph structure is
incorporated and the segmentation of airways are improved compared with the
traditional CNN methods. We perform experiments on the chest CT scans, where
the ground truth segmentation labels are produced manually. The proposed model
shows that compared with the CNN-only method, the combination of CNN and GNN
has a better performance in that the bronchi in the chest CT scans can be
detected in most cases. In addition, the model we propose has a wide extension
since the architecture is also utilitarian in fulfilling similar aims in other
datasets. Hence, the state-of-the-art model is of great significance and highly
applicable in our daily lives.
</p>
<p>Keywords: Airway segmentation, Convolutional neural network, Graph neural
network
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Landmark Detection Based Spatiotemporal Motion Estimation for 4D Dynamic Medical Images. (arXiv:2109.14805v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14805">
<div class="article-summary-box-inner">
<span><p>Motion estimation is a fundamental step in dynamic medical image processing
for the assessment of target organ anatomy and function. However, existing
image-based motion estimation methods, which optimize the motion field by
evaluating the local image similarity, are prone to produce implausible
estimation, especially in the presence of large motion. In this study, we
provide a novel motion estimation framework of Dense-Sparse-Dense (DSD), which
comprises two stages. In the first stage, we process the raw dense image to
extract sparse landmarks to represent the target organ anatomical topology and
discard the redundant information that is unnecessary for motion estimation.
For this purpose, we introduce an unsupervised 3D landmark detection network to
extract spatially sparse but representative landmarks for the target organ
motion estimation. In the second stage, we derive the sparse motion
displacement from the extracted sparse landmarks of two images of different
time points. Then, we present a motion reconstruction network to construct the
motion field by projecting the sparse landmarks displacement back into the
dense image domain. Furthermore, we employ the estimated motion field from our
two-stage DSD framework as initialization and boost the motion estimation
quality in light-weight yet effective iterative optimization. We evaluate our
method on two dynamic medical imaging tasks to model cardiac motion and lung
respiratory motion, respectively. Our method has produced superior motion
estimation accuracy compared to existing comparative methods. Besides, the
extensive experimental results demonstrate that our solution can extract well
representative anatomical landmarks without any requirement of manual
annotation. Our code is publicly available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GT U-Net: A U-Net Like Group Transformer Network for Tooth Root Segmentation. (arXiv:2109.14813v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14813">
<div class="article-summary-box-inner">
<span><p>To achieve an accurate assessment of root canal therapy, a fundamental step
is to perform tooth root segmentation on oral X-ray images, in that the
position of tooth root boundary is significant anatomy information in root
canal therapy evaluation. However, the fuzzy boundary makes the tooth root
segmentation very challenging. In this paper, we propose a novel end-to-end
U-Net like Group Transformer Network (GT U-Net) for the tooth root
segmentation. The proposed network retains the essential structure of U-Net but
each of the encoders and decoders is replaced by a group Transformer, which
significantly reduces the computational cost of traditional Transformer
architectures by using the grouping structure and the bottleneck structure. In
addition, the proposed GT U-Net is composed of a hybrid structure of
convolution and Transformer, which makes it independent of pre-training
weights. For optimization, we also propose a shape-sensitive Fourier Descriptor
(FD) loss function to make use of shape prior knowledge. Experimental results
show that our proposed network achieves the state-of-the-art performance on our
collected tooth root segmentation dataset and the public retina dataset DRIVE.
Code has been released at https://github.com/Kent0n-Li/GT-U-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spark in the Dark: Evaluating Encoder-Decoder Pairs for COVID-19 CT's Semantic Segmentation. (arXiv:2109.14818v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14818">
<div class="article-summary-box-inner">
<span><p>With the COVID-19 global pandemic, computerassisted diagnoses of medical
images have gained a lot of attention, and robust methods of Semantic
Segmentation of Computed Tomography (CT) turned highly desirable. Semantic
Segmentation of CT is one of many research fields of automatic detection of
Covid-19 and was widely explored since the Covid19 outbreak. In the robotic
field, Semantic Segmentation of organs and CTs are widely used in robots
developed for surgery tasks. As new methods and new datasets are proposed
quickly, it becomes apparent the necessity of providing an extensive evaluation
of those methods. To provide a standardized comparison of different
architectures across multiple recently proposed datasets, we propose in this
paper an extensive benchmark of multiple encoders and decoders with a total of
120 architectures evaluated in five datasets, with each dataset being validated
through a five-fold cross-validation strategy, totaling 3.000 experiments. To
the best of our knowledge, this is the largest evaluation in number of
encoders, decoders, and datasets proposed in the field of Covid-19 CT
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Dense Reconstruction with Consistent Scene Segments. (arXiv:2109.14821v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14821">
<div class="article-summary-box-inner">
<span><p>In this paper, a method for dense semantic 3D scene reconstruction from an
RGB-D sequence is proposed to solve high-level scene understanding tasks.
First, each RGB-D pair is consistently segmented into 2D semantic maps based on
a camera tracking backbone that propagates objects' labels with high
probabilities from full scans to corresponding ones of partial views. Then a
dense 3D mesh model of an unknown environment is incrementally generated from
the input RGB-D sequence. Benefiting from 2D consistent semantic segments and
the 3D model, a novel semantic projection block (SP-Block) is proposed to
extract deep feature volumes from 2D segments of different views. Moreover, the
semantic volumes are fused into deep volumes from a point cloud encoder to make
the final semantic segmentation. Extensive experimental evaluations on public
datasets show that our system achieves accurate 3D dense reconstruction and
state-of-the-art semantic prediction performances simultaneously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IntentVizor: Towards Generic Query Guided Interactive Video Summarization Using Slow-Fast Graph Convolutional Networks. (arXiv:2109.14834v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14834">
<div class="article-summary-box-inner">
<span><p>The target of automatic Video summarization is to create a short skim of the
original long video while preserving the major content/events. There is a
growing interest in the integration of user's queries into video summarization,
or query-driven video summarization. This video summarization method predicts a
concise synopsis of the original video based on the user query, which is
commonly represented by the input text. However, two inherent problems exist in
this query-driven way. First, the query text might not be enough to describe
the exact and diverse needs of the user. Second, the user cannot edit once the
summaries are produced, limiting this summarization technique's practical
value. We assume the needs of the user should be subtle and need to be adjusted
interactively. To solve these two problems, we propose a novel IntentVizor
framework, which is an interactive video summarization framework guided by
genric multi-modality queries. The input query that describes the user's needs
is not limited to text but also the video snippets. We further conclude these
multi-modality finer-grained queries as user `intent', which is a newly
proposed concept in this paper. This intent is interpretable, interactable, and
better quantifies/describes the user's needs. To be more specific, We use a set
of intents to represent the inputs of users to design our new interactive
visual analytic interface. Users can interactively control and adjust these
mixed-initiative intents to obtain a more satisfying summary of this newly
proposed interface. Also, as algorithms help users achieve their summarization
goal via video understanding, we propose two novel intent/scoring networks
based on the slow-fast feature for our algorithm part. We conduct our
experiments on two benchmark datasets. The comparison with the state-of-the-art
methods verifies the effectiveness of the proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Image Compression with Probabilistic Decoding. (arXiv:2109.14837v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14837">
<div class="article-summary-box-inner">
<span><p>Lossy image compression is a many-to-one process, thus one bitstream
corresponds to multiple possible original images, especially at low bit rates.
However, this nature was seldom considered in previous studies on image
compression, which usually chose one possible image as reconstruction, e.g. the
one with the maximal a posteriori probability. We propose a learned image
compression framework to natively support probabilistic decoding. The
compressed bitstream is decoded into a series of parameters that instantiate a
pre-chosen distribution; then the distribution is used by the decoder to sample
and reconstruct images. The decoder may adopt different sampling strategies and
produce diverse reconstructions, among which some have higher signal fidelity
and some others have better visual quality. The proposed framework is dependent
on a revertible neural network-based transform to convert pixels into
coefficients that obey the pre-chosen distribution as much as possible. Our
code and models will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AffectGAN: Affect-Based Generative Art Driven by Semantics. (arXiv:2109.14845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14845">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel method for generating artistic images that
express particular affective states. Leveraging state-of-the-art deep learning
methods for visual generation (through generative adversarial networks),
semantic models from OpenAI, and the annotated dataset of the visual art
encyclopedia WikiArt, our AffectGAN model is able to generate images based on
specific or broad semantic prompts and intended affective outcomes. A small
dataset of 32 images generated by AffectGAN is annotated by 50 participants in
terms of the particular emotion they elicit, as well as their quality and
novelty. Results show that for most instances the intended emotion used as a
prompt for image generation matches the participants' responses. This
small-scale study brings forth a new vision towards blending affective
computing with computational creativity, enabling generative systems with
intentionality in terms of the emotions they wish their output to elicit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HLIC: Harmonizing Optimization Metrics in Learned Image Compression by Reinforcement Learning. (arXiv:2109.14863v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14863">
<div class="article-summary-box-inner">
<span><p>Learned image compression is making good progress in recent years. Peak
signal-to-noise ratio (PSNR) and multi-scale structural similarity (MS-SSIM)
are the two most popular evaluation metrics. As different metrics only reflect
certain aspects of human perception, works in this field normally optimize two
models using PSNR and MS-SSIM as loss function separately, which is suboptimal
and makes it difficult to select the model with best visual quality or overall
performance. Towards solving this problem, we propose to Harmonize optimization
metrics in Learned Image Compression (HLIC) using online loss function
adaptation by reinforcement learning. By doing so, we are able to leverage the
advantages of both PSNR and MS-SSIM, achieving better visual quality and higher
VMAF score. To our knowledge, our work is the first to explore automatic loss
function adaptation for harmonizing optimization metrics in low level vision
tasks like learned image compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Segmentation Models using an Uncertainty Slice Sampling Based Annotation Workflow. (arXiv:2109.14879v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14879">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation neural networks require pixel-level annotations in
large quantities to achieve a good performance. In the medical domain, such
annotations are expensive, because they are time-consuming and require expert
knowledge. Active learning optimizes the annotation effort by devising
strategies to select cases for labeling that are most informative to the model.
In this work, we propose an uncertainty slice sampling (USS) strategy for
semantic segmentation of 3D medical volumes that selects 2D image slices for
annotation and compare it with various other strategies. We demonstrate the
efficiency of USS on a CT liver segmentation task using multi-site data. After
five iterations, the training data resulting from USS consisted of 2410 slices
(4% of all slices in the data pool) compared to 8121 (13%), 8641 (14%), and
3730 (6%) for uncertainty volume (UVS), random volume (RVS), and random slice
(RSS) sampling, respectively. Despite being trained on the smallest amount of
data, the model based on the USS strategy evaluated on 234 test volumes
significantly outperformed models trained according to other strategies and
achieved a mean Dice index of 0.964, a relative volume error of 4.2%, a mean
surface distance of 1.35 mm, and a Hausdorff distance of 23.4 mm. This was only
slightly inferior to 0.967, 3.8%, 1.18 mm, and 22.9 mm achieved by a model
trained on all available data, but the robustness analysis using the 5th
percentile of Dice and the 95th percentile of the remaining metrics
demonstrated that USS resulted not only in the most robust model compared to
other sampling schemes, but also outperformed the model trained on all data
according to Dice (0.946 vs. 0.945) and mean surface distance (1.92 mm vs. 2.03
mm).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossCLR: Cross-modal Contrastive Learning For Multi-modal Video Representations. (arXiv:2109.14910v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14910">
<div class="article-summary-box-inner">
<span><p>Contrastive learning allows us to flexibly define powerful losses by
contrasting positive pairs from sets of negative samples. Recently, the
principle has also been used to learn cross-modal embeddings for video and
text, yet without exploiting its full potential. In particular, previous losses
do not take the intra-modality similarities into account, which leads to
inefficient embeddings, as the same content is mapped to multiple points in the
embedding space. With CrossCLR, we present a contrastive loss that fixes this
issue. Moreover, we define sets of highly related samples in terms of their
input embeddings and exclude them from the negative samples to avoid issues
with false negatives. We show that these principles consistently improve the
quality of the learned embeddings. The joint embeddings learned with CrossCLR
extend the state of the art in video-text retrieval on Youcook2 and LSMDC
datasets and in video captioning on Youcook2 dataset by a large margin. We also
demonstrate the generality of the concept by learning improved joint embeddings
for other pairs of modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forming a sparse representation for visual place recognition using a neurorobotic approach. (arXiv:2109.14916v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14916">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel unsupervised neural network model for visual
information encoding which aims to address the problem of large-scale visual
localization. Inspired by the structure of the visual cortex, the model (namely
HSD) alternates layers of topologic sparse coding and pooling to build a more
compact code of visual information. Intended for visual place recognition (VPR)
systems that use local descriptors, the impact of its integration in a
bio-inpired model for self-localization (LPMP) is evaluated. Our experimental
results on the KITTI dataset show that HSD improves the runtime speed of LPMP
by a factor of at least 2 and its localization accuracy by 10%. A comparison
with CoHog, a state-of-the-art VPR approach, showed that our method achieves
slightly better results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Localization Method for Measuring Abdominal Muscle Dimensions in Ultrasound Images. (arXiv:2109.14919v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14919">
<div class="article-summary-box-inner">
<span><p>Health professionals extensively use Two- Dimensional (2D) Ultrasound (US)
videos and images to visualize and measure internal organs for various purposes
including evaluation of muscle architectural changes. US images can be used to
measure abdominal muscles dimensions for the diagnosis and creation of
customized treatment plans for patients with Low Back Pain (LBP), however, they
are difficult to interpret. Due to high variability, skilled professionals with
specialized training are required to take measurements to avoid low
intra-observer reliability. This variability stems from the challenging nature
of accurately finding the correct spatial location of measurement endpoints in
abdominal US images. In this paper, we use a Deep Learning (DL) approach to
automate the measurement of the abdominal muscle thickness in 2D US images. By
treating the problem as a localization task, we develop a modified Fully
Convolutional Network (FCN) architecture to generate blobs of coordinate
locations of measurement endpoints, similar to what a human operator does. We
demonstrate that using the TrA400 US image dataset, our network achieves a Mean
Absolute Error (MAE) of 0.3125 on the test set, which almost matches the
performance of skilled ultrasound technicians. Our approach can facilitate next
steps for automating the process of measurements in 2D US images, while
reducing inter-observer as well as intra-observer variability for more
effective clinical outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Validation of Machine Learning Algorithms for Surgical Workflow and Skill Analysis with the HeiChole Benchmark. (arXiv:2109.14956v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14956">
<div class="article-summary-box-inner">
<span><p>PURPOSE: Surgical workflow and skill analysis are key technologies for the
next generation of cognitive surgical assistance systems. These systems could
increase the safety of the operation through context-sensitive warnings and
semi-autonomous robotic assistance or improve training of surgeons via
data-driven feedback. In surgical workflow analysis up to 91% average precision
has been reported for phase recognition on an open data single-center dataset.
In this work we investigated the generalizability of phase recognition
algorithms in a multi-center setting including more difficult recognition tasks
such as surgical action and surgical skill. METHODS: To achieve this goal, a
dataset with 33 laparoscopic cholecystectomy videos from three surgical centers
with a total operation time of 22 hours was created. Labels included annotation
of seven surgical phases with 250 phase transitions, 5514 occurences of four
surgical actions, 6980 occurences of 21 surgical instruments from seven
instrument categories and 495 skill classifications in five skill dimensions.
The dataset was used in the 2019 Endoscopic Vision challenge, sub-challenge for
surgical workflow and skill analysis. Here, 12 teams submitted their machine
learning algorithms for recognition of phase, action, instrument and/or skill
assessment. RESULTS: F1-scores were achieved for phase recognition between
23.9% and 67.7% (n=9 teams), for instrument presence detection between 38.5%
and 63.8% (n=8 teams), but for action recognition only between 21.8% and 23.3%
(n=5 teams). The average absolute error for skill assessment was 0.78 (n=1
team). CONCLUSION: Surgical workflow and skill analysis are promising
technologies to support the surgical team, but are not solved yet, as shown by
our comparison of algorithms. This novel benchmark can be used for comparable
evaluation and validation of future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmented reality navigation system for visual prosthesis. (arXiv:2109.14957v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14957">
<div class="article-summary-box-inner">
<span><p>The visual functions of visual prostheses such as field of view, resolution
and dynamic range, seriously restrict the person's ability to navigate in
unknown environments. Implanted patients still require constant assistance for
navigating from one location to another. Hence, there is a need for a system
that is able to assist them safely during their journey. In this work, we
propose an augmented reality navigation system for visual prosthesis that
incorporates a software of reactive navigation and path planning which guides
the subject through convenient, obstacle-free route. It consists on four steps:
locating the subject on a map, planning the subject trajectory, showing it to
the subject and re-planning without obstacles. We have also designed a
simulated prosthetic vision environment which allows us to systematically study
navigation performance. Twelve subjects participated in the experiment.
Subjects were guided by the augmented reality navigation system and their
instruction was to navigate through different environments until they reached
two goals, cross the door and find an object (bin), as fast and accurately as
possible. Results show how our augmented navigation system help navigation
performance by reducing the time and distance to reach the goals, even
significantly reducing the number of obstacles collisions, compared to other
baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moving Object Detection for Event-based vision using Graph Spectral Clustering. (arXiv:2109.14979v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14979">
<div class="article-summary-box-inner">
<span><p>Moving object detection has been a central topic of discussion in computer
vision for its wide range of applications like in self-driving cars, video
surveillance, security, and enforcement. Neuromorphic Vision Sensors (NVS) are
bio-inspired sensors that mimic the working of the human eye. Unlike
conventional frame-based cameras, these sensors capture a stream of
asynchronous 'events' that pose multiple advantages over the former, like high
dynamic range, low latency, low power consumption, and reduced motion blur.
However, these advantages come at a high cost, as the event camera data
typically contains more noise and has low resolution. Moreover, as event-based
cameras can only capture the relative changes in brightness of a scene, event
data do not contain usual visual information (like texture and color) as
available in video data from normal cameras. So, moving object detection in
event-based cameras becomes an extremely challenging task. In this paper, we
present an unsupervised Graph Spectral Clustering technique for Moving Object
Detection in Event-based data (GSCEventMOD). We additionally show how the
optimum number of moving objects can be automatically determined. Experimental
comparisons on publicly available datasets show that the proposed GSCEventMOD
algorithm outperforms a number of state-of-the-art techniques by a maximum
margin of 30%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Point Cloud Simplification: A Learnable Feature Preserving Approach. (arXiv:2109.14982v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14982">
<div class="article-summary-box-inner">
<span><p>The recent advances in 3D sensing technology have made possible the capture
of point clouds in significantly high resolution. However, increased detail
usually comes at the expense of high storage, as well as computational costs in
terms of processing and visualization operations. Mesh and Point Cloud
simplification methods aim to reduce the complexity of 3D models while
retaining visual quality and relevant salient features. Traditional
simplification techniques usually rely on solving a time-consuming optimization
problem, hence they are impractical for large-scale datasets. In an attempt to
alleviate this computational burden, we propose a fast point cloud
simplification method by learning to sample salient points. The proposed method
relies on a graph neural network architecture trained to select an arbitrary,
user-defined, number of points from the input space and to re-arrange their
positions so as to minimize the visual perception error. The approach is
extensively evaluated on various datasets using several perceptual metrics.
Importantly, our method is able to generalize to out-of-distribution shapes,
hence demonstrating zero-shot capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Semantic Contour for Object Detection. (arXiv:2109.15009v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15009">
<div class="article-summary-box-inner">
<span><p>Modern object detectors are vulnerable to adversarial examples, which brings
potential risks to numerous applications, e.g., self-driving car. Among attacks
regularized by $\ell_p$ norm, $\ell_0$-attack aims to modify as few pixels as
possible. Nevertheless, the problem is nontrivial since it generally requires
to optimize the shape along with the texture simultaneously, which is an
NP-hard problem. To address this issue, we propose a novel method of
Adversarial Semantic Contour (ASC) guided by object contour as prior. With this
prior, we reduce the searching space to accelerate the $\ell_0$ optimization,
and also introduce more semantic information which should affect the detectors
more. Based on the contour, we optimize the selection of modified pixels via
sampling and their colors with gradient descent alternately. Extensive
experiments demonstrate that our proposed ASC outperforms the most commonly
manually designed patterns (e.g., square patches and grids) on task of
disappearing. By modifying no more than 5\% and 3.5\% of the object area
respectively, our proposed ASC can successfully mislead the mainstream object
detectors including the SSD512, Yolov4, Mask RCNN, Faster RCNN, etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Pose Transfer with Correspondence Learning and Mesh Refinement. (arXiv:2109.15025v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15025">
<div class="article-summary-box-inner">
<span><p>3D pose transfer is one of the most challenging 3D generation tasks. It aims
to transfer the pose of a source mesh to a target mesh and keep the identity
(e.g., body shape) of the target mesh. Some previous works require key point
annotations to build reliable correspondence between the source and target
meshes, while other methods do not consider any shape correspondence between
sources and targets, which leads to limited generation quality. In this work,
we propose a correspondence-refinement network to help the 3D pose transfer for
both human and animal meshes. The correspondence between source and target
meshes is first established by solving an optimal transport problem. Then, we
warp the source mesh according to the dense correspondence and obtain a coarse
warped mesh. The warped mesh will be better refined with our proposed
\textit{Elastic Instance Normalization}, which is a conditional normalization
layer and can help to generate high-quality meshes. Extensive experimental
results show that the proposed architecture can effectively transfer the poses
from source to target meshes and produce better results with satisfied visual
performance than state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Riedones3D: a celtic coin dataset for registration and fine-grained clustering. (arXiv:2109.15033v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15033">
<div class="article-summary-box-inner">
<span><p>Clustering coins with respect to their die is an important component of
numismatic research and crucial for understanding the economic history of
tribes (especially when literary production does not exist, in celtic culture).
It is a very hard task that requires a lot of times and expertise. To cluster
thousands of coins, automatic methods are becoming necessary. Nevertheless,
public datasets for coin die clustering evaluation are too rare, though they
are very important for the development of new methods. Therefore, we propose a
new 3D dataset of 2 070 scans of coins. With this dataset, we propose two
benchmarks, one for point cloud registration, essential for coin die
recognition, and a benchmark of coin die clustering. We show how we
automatically cluster coins to help experts, and perform a preliminary
evaluation for these two tasks. The code of the baseline and the dataset will
be publicly available at https://www.npm3d.fr/coins-riedones3d and
https://www.chronocarto.eu/spip.php?article84&amp;lang=fr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPATE-GAN: Improved Generative Modeling of Dynamic Spatio-Temporal Patterns with an Autoregressive Embedding Loss. (arXiv:2109.15044v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15044">
<div class="article-summary-box-inner">
<span><p>From ecology to atmospheric sciences, many academic disciplines deal with
data characterized by intricate spatio-temporal complexities, the modeling of
which often requires specialized approaches. Generative models of these data
are of particular interest, as they enable a range of impactful downstream
applications like simulation or creating synthetic training data. Recent work
has highlighted the potential of generative adversarial nets (GANs) for
generating spatio-temporal data. A new GAN algorithm COT-GAN, inspired by the
theory of causal optimal transport (COT), was proposed in an attempt to better
tackle this challenge. However, the task of learning more complex
spatio-temporal patterns requires additional knowledge of their specific data
structures. In this study, we propose a novel loss objective combined with
COT-GAN based on an autoregressive embedding to reinforce the learning of
spatio-temporal dynamics. We devise SPATE (spatio-temporal association), a new
metric measuring spatio-temporal autocorrelation by using the deviance of
observations from their expected values. We compute SPATE for real and
synthetic data samples and use it to compute an embedding loss that considers
space-time interactions, nudging the GAN to learn outputs that are faithful to
the observed dynamics. We test this new objective on a diverse set of complex
spatio-temporal patterns: turbulent flows, log-Gaussian Cox processes and
global weather data. We show that our novel embedding loss improves performance
without any changes to the architecture of the COT-GAN backbone, highlighting
our model's increased capacity for capturing autoregressive structures. We also
contextualize our work with respect to recent advances in physics-informed deep
learning and interdisciplinary work connecting neural networks with geographic
and geophysical sciences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Contextual Video Compression. (arXiv:2109.15047v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15047">
<div class="article-summary-box-inner">
<span><p>Most of the existing neural video compression methods adopt the predictive
coding framework, which first generates the predicted frame and then encodes
its residue with the current frame. However, as for compression ratio,
predictive coding is only a sub-optimal solution as it uses simple subtraction
operation to remove the redundancy across frames. In this paper, we propose a
deep contextual video compression framework to enable a paradigm shift from
predictive coding to conditional coding. In particular, we try to answer the
following questions: how to define, use, and learn condition under a deep video
compression framework. To tap the potential of conditional coding, we propose
using feature domain context as condition. This enables us to leverage the high
dimension context to carry rich information to both the encoder and the
decoder, which helps reconstruct the high-frequency contents for higher video
quality. Our framework is also extensible, in which the condition can be
flexibly designed. Experiments show that our method can significantly
outperform the previous state-of-the-art (SOTA) deep video compression methods.
When compared with x265 using veryslow preset, we can achieve 26.0% bitrate
saving for 1080P standard test videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Workflow Augmentation of Video Data for Event Recognition with Time-Sensitive Neural Networks. (arXiv:2109.15063v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15063">
<div class="article-summary-box-inner">
<span><p>Supervised training of neural networks requires large, diverse and well
annotated data sets. In the medical field, this is often difficult to achieve
due to constraints in time, expert knowledge and prevalence of an event.
Artificial data augmentation can help to prevent overfitting and improve the
detection of rare events as well as overall performance. However, most
augmentation techniques use purely spatial transformations, which are not
sufficient for video data with temporal correlations. In this paper, we present
a novel methodology for workflow augmentation and demonstrate its benefit for
event recognition in cataract surgery. The proposed approach increases the
frequency of event alternation by creating artificial videos. The original
video is split into event segments and a workflow graph is extracted from the
original annotations. Finally, the segments are assembled into new videos based
on the workflow graph. Compared to the original videos, the frequency of event
alternation in the augmented cataract surgery videos increased by 26%. Further,
a 3% higher classification accuracy and a 7.8% higher precision was achieved
compared to a state-of-the-art approach. Our approach is particularly helpful
to increase the occurrence of rare but important events and can be applied to a
large variety of use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iShape: A First Step Towards Irregular Shape Instance Segmentation. (arXiv:2109.15068v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15068">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a brand new dataset to promote the study of
instance segmentation for objects with irregular shapes. Our key observation is
that though irregularly shaped objects widely exist in daily life and
industrial scenarios, they received little attention in the instance
segmentation field due to the lack of corresponding datasets. To fill this gap,
we propose iShape, an irregular shape dataset for instance segmentation. iShape
contains six sub-datasets with one real and five synthetics, each represents a
scene of a typical irregular shape. Unlike most existing instance segmentation
datasets of regular objects, iShape has many characteristics that challenge
existing instance segmentation algorithms, such as large overlaps between
bounding boxes of instances, extreme aspect ratios, and large numbers of
connected components per instance. We benchmark popular instance segmentation
methods on iShape and find their performance drop dramatically. Hence, we
propose an affinity-based instance segmentation algorithm, called ASIS, as a
stronger baseline. ASIS explicitly combines perception and reasoning to solve
Arbitrary Shape Instance Segmentation including irregular objects. Experimental
results show that ASIS outperforms the state-of-the-art on iShape. Dataset and
code are available at https://ishape.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Multi-Domain Mitosis Detection. (arXiv:2109.15092v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15092">
<div class="article-summary-box-inner">
<span><p>Domain variability is a common bottle neck in developing generalisable
algorithms for various medical applications. Motivated by the observation that
the domain variability of the medical images is to some extent compact, we
propose to learn a target representative feature space through unpaired image
to image translation (CycleGAN). We comprehensively evaluate the performanceand
usefulness by utilising the transformation to mitosis detection with candidate
proposal and classification. This work presents a simple yet effective
multi-step mitotic figure detection algorithm developed as a baseline for the
MIDOG challenge. On the preliminary test set, the algorithm scoresan F1 score
of 0.52.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Homography Estimation in Dynamic Surgical Scenes for Laparoscopic Camera Motion Extraction. (arXiv:2109.15098v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15098">
<div class="article-summary-box-inner">
<span><p>Current laparoscopic camera motion automation relies on rule-based approaches
or only focuses on surgical tools. Imitation Learning (IL) methods could
alleviate these shortcomings, but have so far been applied to oversimplified
setups. Instead of extracting actions from oversimplified setups, in this work
we introduce a method that allows to extract a laparoscope holder's actions
from videos of laparoscopic interventions. We synthetically add camera motion
to a newly acquired dataset of camera motion free da Vinci surgery image
sequences through the introduction of a novel homography generation algorithm.
The synthetic camera motion serves as a supervisory signal for camera motion
estimation that is invariant to object and tool motion. We perform an extensive
evaluation of state-of-the-art (SOTA) Deep Neural Networks (DNNs) across
multiple compute regimes, finding our method transfers from our camera motion
free da Vinci surgery dataset to videos of laparoscopic interventions,
outperforming classical homography estimation approaches in both, precision by
41%, and runtime on a CPU by 43%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PP-LCNet: A Lightweight CPU Convolutional Neural Network. (arXiv:2109.15099v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15099">
<div class="article-summary-box-inner">
<span><p>We propose a lightweight CPU network based on the MKLDNN acceleration
strategy, named PP-LCNet, which improves the performance of lightweight models
on multiple tasks. This paper lists technologies which can improve network
accuracy while the latency is almost constant. With these improvements, the
accuracy of PP-LCNet can greatly surpass the previous network structure with
the same inference time for classification. As shown in Figure 1, it
outperforms the most state-of-the-art models. And for downstream tasks of
computer vision, it also performs very well, such as object detection, semantic
segmentation, etc. All our experiments are implemented based on PaddlePaddle.
Code and pretrained models are available at PaddleClas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fake It Till You Make It: Face analysis in the wild using synthetic data alone. (arXiv:2109.15102v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15102">
<div class="article-summary-box-inner">
<span><p>We demonstrate that it is possible to perform face-related computer vision in
the wild using synthetic data alone. The community has long enjoyed the
benefits of synthesizing training data with graphics, but the domain gap
between real and synthetic data has remained a problem, especially for human
faces. Researchers have tried to bridge this gap with data mixing, domain
adaptation, and domain-adversarial training, but we show that it is possible to
synthesize data with minimal domain gap, so that models trained on synthetic
data generalize to real in-the-wild datasets. We describe how to combine a
procedurally-generated parametric 3D face model with a comprehensive library of
hand-crafted assets to render training images with unprecedented realism and
diversity. We train machine learning systems for face-related tasks such as
landmark localization and face parsing, showing that synthetic data can both
match real data in accuracy as well as open up new approaches where manual
labelling would be impossible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion-aware Self-supervised Video Representation Learning via Foreground-background Merging. (arXiv:2109.15130v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15130">
<div class="article-summary-box-inner">
<span><p>In light of the success of contrastive learning in the image domain, current
self-supervised video representation learning methods usually employ
contrastive loss to facilitate video representation learning. When naively
pulling two augmented views of a video closer, the model however tends to learn
the common static background as a shortcut but fails to capture the motion
information, a phenomenon dubbed as background bias. This bias makes the model
suffer from weak generalization ability, leading to worse performance on
downstream tasks such as action recognition. To alleviate such bias, we propose
Foreground-background Merging (FAME) to deliberately compose the foreground
region of the selected video onto the background of others. Specifically,
without any off-the-shelf detector, we extract the foreground and background
regions via the frame difference and color statistics, and shuffle the
background regions among the videos. By leveraging the semantic consistency
between the original clips and the fused ones, the model focuses more on the
foreground motion pattern and is thus more robust to the background context.
Extensive experiments demonstrate that FAME can significantly boost the
performance in different downstream tasks with various backbones. When
integrated with MoCo, FAME reaches 84.8% and 53.5% accuracy on UCF101 and
HMDB51, respectively, achieving the state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HSVA: Hierarchical Semantic-Visual Adaptation for Zero-Shot Learning. (arXiv:2109.15163v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15163">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning (ZSL) tackles the unseen class recognition problem,
transferring semantic knowledge from seen classes to unseen ones. Typically, to
guarantee desirable knowledge transfer, a common (latent) space is adopted for
associating the visual and semantic domains in ZSL. However, existing common
space learning methods align the semantic and visual domains by merely
mitigating distribution disagreement through one-step adaptation. This strategy
is usually ineffective due to the heterogeneous nature of the feature
representations in the two domains, which intrinsically contain both
distribution and structure variations. To address this and advance ZSL, we
propose a novel hierarchical semantic-visual adaptation (HSVA) framework.
Specifically, HSVA aligns the semantic and visual domains by adopting a
hierarchical two-step adaptation, i.e., structure adaptation and distribution
adaptation. In the structure adaptation step, we take two task-specific
encoders to encode the source data (visual domain) and the target data
(semantic domain) into a structure-aligned common space. To this end, a
supervised adversarial discrepancy (SAD) module is proposed to adversarially
minimize the discrepancy between the predictions of two task-specific
classifiers, thus making the visual and semantic feature manifolds more closely
aligned. In the distribution adaptation step, we directly minimize the
Wasserstein distance between the latent multivariate Gaussian distributions to
align the visual and semantic distributions using a common encoder. Finally,
the structure and distribution adaptation are derived in a unified framework
under two partially-aligned variational autoencoders. Extensive experiments on
four benchmark datasets demonstrate that HSVA achieves superior performance on
both conventional and generalized ZSL. The code is available at
\url{https://github.com/shiming-chen/HSVA} .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Technical Report for ICCV 2021 VIPriors Re-identification Challenge. (arXiv:2109.15164v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15164">
<div class="article-summary-box-inner">
<span><p>Person re-identification has always been a hot and challenging task. This
paper introduces our solution for the re-identification track in VIPriors
Challenge 2021. In this challenge, the difficulty is how to train the model
from scratch without any pretrained weight. In our method, we show use
state-of-the-art data processing strategies, model designs, and post-processing
ensemble methods, it is possible to overcome the difficulty of data shortage
and obtain competitive results. (1) Both image augmentation strategy and novel
pre-processing method for occluded images can help the model learn more
discriminative features. (2) Several strong backbones and multiple loss
functions are used to learn more representative features. (3) Post-processing
techniques including re-ranking, automatic query expansion, ensemble learning,
etc., significantly improve the final performance. The final score of our team
(ALONG) is 96.5154% mAP, ranking first in the leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoSeg: Cognitively Inspired Unsupervised Generic Event Segmentation. (arXiv:2109.15170v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15170">
<div class="article-summary-box-inner">
<span><p>Some cognitive research has discovered that humans accomplish event
segmentation as a side effect of event anticipation. Inspired by this
discovery, we propose a simple yet effective end-to-end self-supervised
learning framework for event segmentation/boundary detection. Unlike the
mainstream clustering-based methods, our framework exploits a transformer-based
feature reconstruction scheme to detect event boundary by reconstruction
errors. This is consistent with the fact that humans spot new events by
leveraging the deviation between their prediction and what is actually
perceived. Thanks to their heterogeneity in semantics, the frames at boundaries
are difficult to be reconstructed (generally with large reconstruction errors),
which is favorable for event boundary detection. Additionally, since the
reconstruction occurs on the semantic feature level instead of pixel level, we
develop a temporal contrastive feature embedding module to learn the semantic
visual representation for frame feature reconstruction. This procedure is like
humans building up experiences with "long-term memory". The goal of our work is
to segment generic events rather than localize some specific ones. We focus on
achieving accurate event boundaries. As a result, we adopt F1 score
(Precision/Recall) as our primary evaluation metric for a fair comparison with
previous approaches. Meanwhile, we also calculate the conventional frame-based
MoF and IoU metric. We thoroughly benchmark our work on four publicly available
datasets and demonstrate much better results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Cannot Easily Catch Me: A Low-Detectable Adversarial Patch for Object Detectors. (arXiv:2109.15177v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15177">
<div class="article-summary-box-inner">
<span><p>Blind spots or outright deceit can bedevil and deceive machine learning
models. Unidentified objects such as digital "stickers," also known as
adversarial patches, can fool facial recognition systems, surveillance systems
and self-driving cars. Fortunately, most existing adversarial patches can be
outwitted, disabled and rejected by a simple classification network called an
adversarial patch detector, which distinguishes adversarial patches from
original images. An object detector classifies and predicts the types of
objects within an image, such as by distinguishing a motorcyclist from the
motorcycle, while also localizing each object's placement within the image by
"drawing" so-called bounding boxes around each object, once again separating
the motorcyclist from the motorcycle. To train detectors even better, however,
we need to keep subjecting them to confusing or deceitful adversarial patches
as we probe for the models' blind spots. For such probes, we came up with a
novel approach, a Low-Detectable Adversarial Patch, which attacks an object
detector with small and texture-consistent adversarial patches, making these
adversaries less likely to be recognized. Concretely, we use several geometric
primitives to model the shapes and positions of the patches. To enhance our
attack performance, we also assign different weights to the bounding boxes in
terms of loss function. Our experiments on the common detection dataset COCO as
well as the driving-video dataset D2-City show that LDAP is an effective attack
method, and can resist the adversarial patch detector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments. (arXiv:2109.15207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15207">
<div class="article-summary-box-inner">
<span><p>In the Vision-and-Language Navigation (VLN) task an embodied agent navigates
a 3D environment, following natural language instructions. A challenge in this
task is how to handle 'off the path' scenarios where an agent veers from a
reference path. Prior work supervises the agent with actions based on the
shortest path from the agent's location to the goal, but such goal-oriented
supervision is often not in alignment with the instruction. Furthermore, the
evaluation metrics employed by prior work do not measure how much of a language
instruction the agent is able to follow. In this work, we propose a simple and
effective language-aligned supervision scheme, and a new metric that measures
the number of sub-instructions the agent has completed during navigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Out-of-Distribution Detection and Localization with Natural Synthetic Anomalies (NSA). (arXiv:2109.15222v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15222">
<div class="article-summary-box-inner">
<span><p>We introduce a new self-supervised task, NSA, for training an end-to-end
model for anomaly detection and localization using only normal data. NSA uses
Poisson image editing to seamlessly blend scaled patches of various sizes from
separate images. This creates a wide range of synthetic anomalies which are
more similar to natural sub-image irregularities than previous
data-augmentation strategies for self-supervised anomaly detection. We evaluate
the proposed method using natural and medical images. Our experiments with the
MVTec AD dataset show that a model trained to localize NSA anomalies
generalizes well to detecting real-world a priori unknown types of
manufacturing defects. Our method achieves an overall detection AUROC of 97.2
outperforming all previous methods that learn from scratch without pre-training
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Tactile Grasp Force Sensing Using Fingernail Imaging via Deep Neural Networks. (arXiv:2109.15231v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15231">
<div class="article-summary-box-inner">
<span><p>This paper has introduced a novel approach for the real-time estimation of 3D
tactile forces exerted by human fingertips via vision only. The introduced
approach is entirely monocular vision-based and does not require any physical
force sensor. Therefore, it is scalable, non-intrusive, and easily fused with
other perception systems such as body pose estimation, making it ideal for HRI
applications where force sensing is necessary. The introduced approach consists
of three main modules: finger tracking for detection and tracking of each
individual finger, image alignment for preserving the spatial information in
the images, and the force model for estimating the 3D forces from coloration
patterns in the images. The model has been implemented experimentally, and the
results have shown a maximum RMS error of 8.4% (for the entire range of force
levels) along all three directions. The estimation accuracy is comparable to
the offline models in the literature, such as EigneNail, while, this model is
capable of performing force estimation at 30 frames per second.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferability Estimation for Semantic Segmentation Task. (arXiv:2109.15242v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15242">
<div class="article-summary-box-inner">
<span><p>Transferability estimation is a fundamental problem in transfer learning to
predict how good the performance is when transferring a source model (or source
task) to a target task. With the guidance of transferability score, we can
efficiently select the highly transferable source models without performing the
real transfer in practice. Recent analytical transferability metrics are mainly
designed for image classification, and currently there is no specific
investigation for the transferability estimation of semantic segmentation task,
which is an essential problem in autonomous driving, medical image analysis,
etc. Consequently, we further extend the recent analytical transferability
metric OTCE (Optimal Transport based Conditional Entropy) score to the semantic
segmentation task. The challenge in applying the OTCE score is the high
dimensional segmentation output, which is difficult to find the optimal
coupling between so many pixels under an acceptable computation cost. Thus we
propose to randomly sample N pixels for computing OTCE score and take the
expectation over K repetitions as the final transferability score. Experimental
evaluation on Cityscapes, BDD100K and GTA5 datasets demonstrates that the OTCE
score highly correlates with the transfer performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T\"oRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis. (arXiv:2109.15271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15271">
<div class="article-summary-box-inner">
<span><p>Neural networks can represent and accurately reconstruct radiance fields for
static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes
captured with monocular video, with promising performance. However, the
monocular setting is known to be an under-constrained problem, and so methods
rely on data-driven priors for reconstructing dynamic content. We replace these
priors with measurements from a time-of-flight (ToF) camera, and introduce a
neural representation based on an image formation model for continuous-wave ToF
cameras. Instead of working with processed depth maps, we model the raw ToF
sensor measurements to improve reconstruction quality and avoid issues with low
reflectance regions, multi-path interference, and a sensor's limited
unambiguous depth range. We show that this approach improves robustness of
dynamic scene reconstruction to erroneous calibration and large motions, and
discuss the benefits and limitations of integrating RGB+ToF sensors that are
now available on modern smartphones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAAS: Differentiable Architecture and Augmentation Policy Search. (arXiv:2109.15273v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15273">
<div class="article-summary-box-inner">
<span><p>Neural architecture search (NAS) has been an active direction of automatic
machine learning (Auto-ML), aiming to explore efficient network structures. The
searched architecture is evaluated by training on datasets with fixed data
augmentation policies. However, recent works on auto-augmentation show that the
suited augmentation policies can vary over different structures. Therefore,
this work considers the possible coupling between neural architectures and data
augmentation and proposes an effective algorithm jointly searching for them.
Specifically, 1) for the NAS task, we adopt a single-path based differentiable
method with Gumbel-softmax reparameterization strategy due to its memory
efficiency; 2) for the auto-augmentation task, we introduce a novel search
method based on policy gradient algorithm, which can significantly reduce the
computation complexity. Our approach achieves 97.91% accuracy on CIFAR-10 and
76.6% Top-1 accuracy on ImageNet dataset, showing the outstanding performance
of our search algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bend-Net: Bending Loss Regularized Multitask Learning Network for Nuclei Segmentation in Histopathology Images. (arXiv:2109.15283v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15283">
<div class="article-summary-box-inner">
<span><p>Separating overlapped nuclei is a major challenge in histopathology image
analysis. Recently published approaches have achieved promising overall
performance on nuclei segmentation; however, their performance on separating
overlapped nuclei is quite limited. To address the issue, we propose a novel
multitask learning network with a bending loss regularizer to separate
overlapped nuclei accurately. The newly proposed multitask learning
architecture enhances the generalization by learning shared representation from
three tasks: instance segmentation, nuclei distance map prediction, and
overlapped nuclei distance map prediction. The proposed bending loss defines
high penalties to concave contour points with large curvatures, and applies
small penalties to convex contour points with small curvatures. Minimizing the
bending loss avoids generating contours that encompass multiple nuclei. In
addition, two new quantitative metrics, Aggregated Jaccard Index of overlapped
nuclei (AJIO) and Accuracy of overlapped nuclei (ACCO), are designed for the
evaluation of overlapped nuclei segmentation. We validate the proposed approach
on the CoNSeP and MoNuSegv1 datasets using seven quantitative metrics:
Aggregate Jaccard Index, Dice, Segmentation Quality, Recognition Quality,
Panoptic Quality, AJIO, and ACCO. Extensive experiments demonstrate that the
proposed Bend-Net outperforms eight state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for LiDAR Panoptic Segmentation. (arXiv:2109.15286v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15286">
<div class="article-summary-box-inner">
<span><p>Scene understanding is a pivotal task for autonomous vehicles to safely
navigate in the environment. Recent advances in deep learning enable accurate
semantic reconstruction of the surroundings from LiDAR data. However, these
models encounter a large domain gap while deploying them on vehicles equipped
with different LiDAR setups which drastically decreases their performance.
Fine-tuning the model for every new setup is infeasible due to the expensive
and cumbersome process of recording and manually labeling new data.
Unsupervised Domain Adaptation (UDA) techniques are thus essential to fill this
domain gap and retain the performance of models on new sensor setups without
the need for additional data labeling. In this paper, we propose AdaptLPS, a
novel UDA approach for LiDAR panoptic segmentation that leverages task-specific
knowledge and accounts for variation in the number of scan lines, mounting
position, intensity distribution, and environmental conditions. We tackle the
UDA task by employing two complementary domain adaptation strategies,
data-based and model-based. While data-based adaptations reduce the domain gap
by processing the raw LiDAR scans to resemble the scans in the target domain,
model-based techniques guide the network in extracting features that are
representative for both domains. Extensive evaluations on three pairs of
real-world autonomous driving datasets demonstrate that AdaptLPS outperforms
existing UDA approaches by up to 6.41 pp in terms of the PQ score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identity-Disentangled Neural Deformation Model for Dynamic Meshes. (arXiv:2109.15299v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15299">
<div class="article-summary-box-inner">
<span><p>Neural shape models can represent complex 3D shapes with a compact latent
space. When applied to dynamically deforming shapes such as the human hands,
however, they would need to preserve temporal coherence of the deformation as
well as the intrinsic identity of the subject. These properties are difficult
to regularize with manually designed loss functions. In this paper, we learn a
neural deformation model that disentangles the identity-induced shape
variations from pose-dependent deformations using implicit neural functions. We
perform template-free unsupervised learning on 3D scans without explicit mesh
correspondence or semantic correspondences of shapes across subjects. We can
then apply the learned model to reconstruct partial dynamic 4D scans of novel
subjects performing unseen actions. We propose two methods to integrate global
pose alignment with our neural deformation model. Experiments demonstrate the
efficacy of our method in the disentanglement of identities and pose. Our
method also outperforms traditional skeleton-driven models in reconstructing
surface details such as palm prints or tendons without limitations from a fixed
template.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Few-Shot Action Recognition via Action-Appearance Aligned Meta-Adaptation. (arXiv:2109.15317v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15317">
<div class="article-summary-box-inner">
<span><p>We present MetaUVFS as the first Unsupervised Meta-learning algorithm for
Video Few-Shot action recognition. MetaUVFS leverages over 550K unlabeled
videos to train a two-stream 2D and 3D CNN architecture via contrastive
learning to capture the appearance-specific spatial and action-specific
spatio-temporal video features respectively. MetaUVFS comprises a novel
Action-Appearance Aligned Meta-adaptation (A3M) module that learns to focus on
the action-oriented video features in relation to the appearance features via
explicit few-shot episodic meta-learning over unsupervised hard-mined episodes.
Our action-appearance alignment and explicit few-shot learner conditions the
unsupervised training to mimic the downstream few-shot task, enabling MetaUVFS
to significantly outperform all unsupervised methods on few-shot benchmarks.
Moreover, unlike previous few-shot action recognition methods that are
supervised, MetaUVFS needs neither base-class labels nor a supervised
pretrained backbone. Thus, we need to train MetaUVFS just once to perform
competitively or sometimes even outperform state-of-the-art supervised methods
on popular HMDB51, UCF101, and Kinetics100 few-shot datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensor-Guided Optical Flow. (arXiv:2109.15321v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15321">
<div class="article-summary-box-inner">
<span><p>This paper proposes a framework to guide an optical flow network with
external cues to achieve superior accuracy either on known or unseen domains.
Given the availability of sparse yet accurate optical flow hints from an
external source, these are injected to modulate the correlation scores computed
by a state-of-the-art optical flow network and guide it towards more accurate
predictions. Although no real sensor can provide sparse flow hints, we show how
these can be obtained by combining depth measurements from active sensors with
geometry and hand-crafted optical flow algorithms, leading to accurate enough
hints for our purpose. Experimental results with a state-of-the-art flow
network on standard benchmarks support the effectiveness of our framework, both
in simulated and real conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to synthesise the ageing brain without longitudinal data. (arXiv:1912.02620v6 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.02620">
<div class="article-summary-box-inner">
<span><p>How will my face look when I get older? Or, for a more challenging question:
How will my brain look when I get older? To answer this question one must
devise (and learn from data) a multivariate auto-regressive function which
given an image and a desired target age generates an output image. While
collecting data for faces may be easier, collecting longitudinal brain data is
not trivial. We propose a deep learning-based method that learns to simulate
subject-specific brain ageing trajectories without relying on longitudinal
data. Our method synthesises images conditioned on two factors: age (a
continuous variable), and status of Alzheimer's Disease (AD, an ordinal
variable). With an adversarial formulation we learn the joint distribution of
brain appearance, age and AD status, and define reconstruction losses to
address the challenging problem of preserving subject identity. We compare with
several benchmarks using two widely used datasets. We evaluate the quality and
realism of synthesised images using ground-truth longitudinal data and a
pre-trained age predictor. We show that, despite the use of cross-sectional
data, our model learns patterns of gray matter atrophy in the middle temporal
gyrus in patients with AD. To demonstrate generalisation ability, we train on
one dataset and evaluate predictions on the other. In conclusion, our model
shows an ability to separate age, disease influence and anatomy using only 2D
cross-sectional data that should be useful in large studies into
neurodegenerative disease, that aim to combine several data sources. To
facilitate such future studies by the community at large our code is made
available at https://github.com/xiat0616/BrainAgeing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Deformable Image Registration from Optimization: Perspective, Modules, Bilevel Training and Beyond. (arXiv:2004.14557v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14557">
<div class="article-summary-box-inner">
<span><p>Conventional deformable registration methods aim at solving an optimization
model carefully designed on image pairs and their computational costs are
exceptionally high. In contrast, recent deep learning based approaches can
provide fast deformation estimation. These heuristic network architectures are
fully data-driven and thus lack explicit geometric constraints, e.g.,
topology-preserving, which are indispensable to generate plausible
deformations. We design a new deep learning based framework to optimize a
diffeomorphic model via multi-scale propagation in order to integrate
advantages and avoid limitations of these two categories of approaches.
Specifically, we introduce a generic optimization model to formulate
diffeomorphic registration and develop a series of learnable architectures to
obtain propagative updating in the coarse-to-fine feature space. Moreover, we
propose a novel bilevel self-tuned training strategy, allowing efficient search
of task-specific hyper-parameters. This training strategy increases the
flexibility to various types of data while reduces computational and human
burdens. We conduct two groups of image registration experiments on 3D volume
datasets including image-to-atlas registration on brain MRI data and
image-to-image registration on liver CT data. Extensive results demonstrate the
state-of-the-art performance of the proposed method with diffeomorphic
guarantee and extreme efficiency. We also apply our framework to challenging
multi-modal image registration, and investigate how our registration to support
the down-streaming tasks for medical image analysis including multi-modal
fusion and image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutTransformer: Layout Generation and Completion with Self-attention. (arXiv:2006.14615v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14615">
<div class="article-summary-box-inner">
<span><p>We address the problem of scene layout generation for diverse domains such as
images, mobile applications, documents, and 3D objects. Most complex scenes,
natural or human-designed, can be expressed as a meaningful arrangement of
simpler compositional graphical primitives. Generating a new layout or
extending an existing layout requires understanding the relationships between
these primitives. To do this, we propose LayoutTransformer, a novel framework
that leverages self-attention to learn contextual relationships between layout
elements and generate novel layouts in a given domain. Our framework allows us
to generate a new layout either from an empty set or from an initial seed set
of primitives, and can easily scale to support an arbitrary of primitives per
layout. Furthermore, our analyses show that the model is able to automatically
capture the semantic properties of the primitives. We propose simple
improvements in both representation of layout primitives, as well as training
methods to demonstrate competitive performance in very diverse data domains
such as object bounding boxes in natural images(COCO bounding box), documents
(PubLayNet), mobile applications (RICO dataset) as well as 3D shapes
(Part-Net). Code and other materials will be made available at
https://kampta.github.io/layout.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient-based Hyperparameter Optimization Over Long Horizons. (arXiv:2007.07869v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.07869">
<div class="article-summary-box-inner">
<span><p>Gradient-based hyperparameter optimization has earned a widespread popularity
in the context of few-shot meta-learning, but remains broadly impractical for
tasks with long horizons (many gradient steps), due to memory scaling and
gradient degradation issues. A common workaround is to learn hyperparameters
online, but this introduces greediness which comes with a significant
performance drop. We propose forward-mode differentiation with sharing (FDS), a
simple and efficient algorithm which tackles memory scaling issues with
forward-mode differentiation, and gradient degradation issues by sharing
hyperparameters that are contiguous in time. We provide theoretical guarantees
about the noise reduction properties of our algorithm, and demonstrate its
efficiency empirically by differentiating through $\sim 10^4$ gradient steps of
unrolled optimization. We consider large hyperparameter search ranges on
CIFAR-10 where we significantly outperform greedy gradient-based alternatives,
while achieving $\times 20$ speedups compared to the state-of-the-art black-box
methods. Code is available at: \url{https://github.com/polo5/FDS}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Suppression of Independent and Correlated Noise with Similarity-based Unsupervised Deep Learning. (arXiv:2011.03384v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.03384">
<div class="article-summary-box-inner">
<span><p>Denoising is one of the most important data processing tasks and is generally
a prerequisite for downstream image analysis in many fields. Despite their
superior denoising performance, supervised deep denoising methods require
paired noise-clean or noise-noise samples often unavailable in practice. On the
other hand, unsupervised deep denoising methods such as Noise2Void and its
variants predict masked pixels from their neighboring pixels in single noisy
images. However, these unsupervised algorithms only work under the independent
noise assumption while real noise distributions are usually correlated with
complex structural patterns. Here we propose the first-of-its-kind feature
similarity-based unsupervised denoising approach that works in a nonlocal and
nonlinear fashion to suppress not only independent but also correlated noise.
Our approach is referred to as Noise2Sim since different noisy sub-images with
similar signals are extracted to form as many as possible training pairs so
that the parameters of a deep denoising network can be optimized in a
self-learning fashion. Theoretically, the theorem is established that Noise2Sim
is equivalent to the supervised learning methods under mild conditions.
Experimentally, Noise2Sim achieves excellent results on natural, microscopic,
low-dose CT and photon-counting micro-CT images, removing image noise
independent or not and being superior to the competitive denoising methods.
Potentially, Noise2Sim would open a new direction of research and lead to the
development of adaptive denoising tools in diverse applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Retrieval and Synthesis (X-MRS): Closing the Modality Gap in Shared Representation Learning. (arXiv:2012.01345v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01345">
<div class="article-summary-box-inner">
<span><p>Computational food analysis (CFA) naturally requires multi-modal evidence of
a particular food, e.g., images, recipe text, etc. A key to making CFA possible
is multi-modal shared representation learning, which aims to create a joint
representation of the multiple views (text and image) of the data. In this work
we propose a method for food domain cross-modal shared representation learning
that preserves the vast semantic richness present in the food data. Our
proposed method employs an effective transformer-based multilingual recipe
encoder coupled with a traditional image embedding architecture. Here, we
propose the use of imperfect multilingual translations to effectively
regularize the model while at the same time adding functionality across
multiple languages and alphabets. Experimental analysis on the public Recipe1M
dataset shows that the representation learned via the proposed method
significantly outperforms the current state-of-the-arts (SOTA) on retrieval
tasks. Furthermore, the representational power of the learned representation is
demonstrated through a generative food image synthesis model conditioned on
recipe embeddings. Synthesized images can effectively reproduce the visual
appearance of paired samples, indicating that the learned representation
captures the joint semantics of both the textual recipe and its visual content,
thus narrowing the modality gap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-to-real for high-resolution optical tactile sensing: From images to 3D contact force distributions. (arXiv:2012.11295v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11295">
<div class="article-summary-box-inner">
<span><p>The images captured by vision-based tactile sensors carry information about
high-resolution tactile fields, such as the distribution of the contact forces
applied to their soft sensing surface. However, extracting the information
encoded in the images is challenging and often addressed with learning-based
approaches, which generally require a large amount of training data. This
article proposes a strategy to generate tactile images in simulation for a
vision-based tactile sensor based on an internal camera that tracks the motion
of spherical particles within a soft material. The deformation of the material
is simulated in a finite element environment under a diverse set of contact
conditions, and spherical particles are projected to a simulated image.
Features extracted from the images are mapped to the 3D contact force
distribution, with the ground truth also obtained via finite-element
simulations, with an artificial neural network that is therefore entirely
trained on synthetic data avoiding the need for real-world data collection. The
resulting model exhibits high accuracy when evaluated on real-world tactile
images, is transferable across multiple tactile sensors without further
training, and is suitable for efficient real-time inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlocking Pixels for Reinforcement Learning via Implicit Attention. (arXiv:2102.04353v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04353">
<div class="article-summary-box-inner">
<span><p>There has recently been significant interest in training reinforcement
learning (RL) agents in vision-based environments. This poses many challenges,
such as high dimensionality and the potential for observational overfitting
through spurious correlations. A promising approach to solve both of these
problems is an attention bottleneck, which provides a simple and effective
framework for learning high performing policies, even in the presence of
distractions. However, due to poor scalability of attention architectures,
these methods cannot be applied beyond low resolution visual inputs, using
large patches (thus small attention matrices). In this paper we make use of new
efficient attention algorithms, recently shown to be highly effective for
Transformers, and demonstrate that these techniques can be successfully adopted
for the RL setting. This allows our attention-based controllers to scale to
larger visual inputs, and facilitate the use of smaller patches, even
individual pixels, improving generalization. We show this on a range of tasks
from the Distracting Control Suite to vision-based quadruped robots locomotion.
We provide rigorous theoretical analysis of the proposed algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation. (arXiv:2102.12867v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12867">
<div class="article-summary-box-inner">
<span><p>Recent methods for long-tailed instance segmentation still struggle on rare
object classes with few training data. We propose a simple yet effective
method, Feature Augmentation and Sampling Adaptation (FASA), that addresses the
data scarcity issue by augmenting the feature space especially for rare
classes. Both the Feature Augmentation (FA) and feature sampling components are
adaptive to the actual training status -- FA is informed by the feature mean
and variance of observed real samples from past iterations, and we sample the
generated virtual features in a loss-adapted manner to avoid over-fitting. FASA
does not require any elaborate loss design, and removes the need for
inter-class transfer learning that often involves large cost and
manually-defined head/tail class groups. We show FASA is a fast, generic method
that can be easily plugged into standard or long-tailed segmentation
frameworks, with consistent performance gains and little added cost. FASA is
also applicable to other tasks like long-tailed classification with
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Event-based Visuomotor Policies. (arXiv:2103.00806v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00806">
<div class="article-summary-box-inner">
<span><p>Event-based cameras are dynamic vision sensors that provide asynchronous
measurements of changes in per-pixel brightness at a microsecond level. This
makes them significantly faster than conventional frame-based cameras, and an
appealing choice for high-speed navigation. While an interesting sensor
modality, this asynchronously streamed event data poses a challenge for machine
learning techniques that are more suited for frame-based data. In this paper,
we present an event variational autoencoder and show that it is feasible to
learn compact representations directly from asynchronous spatiotemporal event
data. Furthermore, we show that such pretrained representations can be used for
event-based reinforcement learning instead of end-to-end reward driven
perception. We validate this framework of learning event-based visuomotor
policies by applying it to an obstacle avoidance scenario in simulation.
Compared to techniques that treat event data as images, we show that
representations learnt from event streams result in faster policy training,
adapt to different control capacities, and demonstrate a higher degree of
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Representation Learning via Maximization of Local Mutual Information. (arXiv:2103.04537v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04537">
<div class="article-summary-box-inner">
<span><p>We propose and demonstrate a representation learning approach by maximizing
the mutual information between local features of images and text. The goal of
this approach is to learn useful image representations by taking advantage of
the rich information contained in the free text that describes the findings in
the image. Our method trains image and text encoders by encouraging the
resulting representations to exhibit high local mutual information. We make use
of recent advances in mutual information estimation with neural network
discriminators. We argue that the sum of local mutual information is typically
a lower bound on the global mutual information. Our experimental results in the
downstream image classification tasks demonstrate the advantages of using local
features for image-text representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScanMix: Learning from Severe Label Noise via Semantic Clustering and Semi-Supervised Learning. (arXiv:2103.11395v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11395">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the problem of training deep neural networks in the
presence of severe label noise. Our proposed training algorithm ScanMix,
combines semantic clustering with semi-supervised learning (SSL) to improve the
feature representations and enable an accurate identification of noisy samples,
even in severe label noise scenarios. To be specific, ScanMix is designed based
on the expectation maximisation (EM) framework, where the E-step estimates the
value of a latent variable to cluster the training images based on their
appearance representations and classification results, and the M-step optimises
the SSL classification and learns effective feature representations via
semantic clustering. In our evaluations, we show state-of-the-art results on
standard benchmarks for symmetric, asymmetric and semantic label noise on
CIFAR-10 and CIFAR-100, as well as large scale real label noise on WebVision.
Most notably, for the benchmarks contaminated with large noise rates (80% and
above), our results are up to 27% better than the related work. The code is
available at https://github.com/ragavsachdeva/ScanMix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Instance Segmentation with a Propose-Reduce Paradigm. (arXiv:2103.13746v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13746">
<div class="article-summary-box-inner">
<span><p>Video instance segmentation (VIS) aims to segment and associate all instances
of predefined classes for each frame in videos. Prior methods usually obtain
segmentation for a frame or clip first, and merge the incomplete results by
tracking or matching. These methods may cause error accumulation in the merging
step. Contrarily, we propose a new paradigm -- Propose-Reduce, to generate
complete sequences for input videos by a single step. We further build a
sequence propagation head on the existing image-level instance segmentation
network for long-term propagation. To ensure robustness and high recall of our
proposed framework, multiple sequences are proposed where redundant sequences
of the same instance are reduced. We achieve state-of-the-art performance on
two representative benchmark datasets -- we obtain 47.6% in terms of AP on
YouTube-VIS validation set and 70.4% for J&amp;F on DAVIS-UVOS validation set. Code
is available at https://github.com/dvlab-research/ProposeReduce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Designing a Practical Degradation Model for Deep Blind Image Super-Resolution. (arXiv:2103.14006v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14006">
<div class="article-summary-box-inner">
<span><p>It is widely acknowledged that single image super-resolution (SISR) methods
would not perform well if the assumed degradation model deviates from those in
real images. Although several degradation models take additional factors into
consideration, such as blur, they are still not effective enough to cover the
diverse degradations of real images. To address this issue, this paper proposes
to design a more complex but practical degradation model that consists of
randomly shuffled blur, downsampling and noise degradations. Specifically, the
blur is approximated by two convolutions with isotropic and anisotropic
Gaussian kernels; the downsampling is randomly chosen from nearest, bilinear
and bicubic interpolations; the noise is synthesized by adding Gaussian noise
with different noise levels, adopting JPEG compression with different quality
factors, and generating processed camera sensor noise via reverse-forward
camera image signal processing (ISP) pipeline model and RAW image noise model.
To verify the effectiveness of the new degradation model, we have trained a
deep blind ESRGAN super-resolver and then applied it to super-resolve both
synthetic and real images with diverse degradations. The experimental results
demonstrate that the new degradation model can help to significantly improve
the practicability of deep super-resolvers, thus providing a powerful
alternative solution for real SISR applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sketch2Mesh: Reconstructing and Editing 3D Shapes from Sketches. (arXiv:2104.00482v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00482">
<div class="article-summary-box-inner">
<span><p>Reconstructing 3D shape from 2D sketches has long been an open problem
because the sketches only provide very sparse and ambiguous information. In
this paper, we use an encoder/decoder architecture for the sketch to mesh
translation. When integrated into a user interface that provides camera
parameters for the sketches, this enables us to leverage its latent
parametrization to represent and refine a 3D mesh so that its projections match
the external contours outlined in the sketch. We will show that this approach
is easy to deploy, robust to style changes, and effective. Furthermore, it can
be used for shape refinement given only single pen strokes. We compare our
approach to state-of-the-art methods on sketches -- both hand-drawn and
synthesized -- and demonstrate that we outperform them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twins: Revisiting the Design of Spatial Attention in Vision Transformers. (arXiv:2104.13840v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13840">
<div class="article-summary-box-inner">
<span><p>Very recently, a variety of vision transformer architectures for dense
prediction tasks have been proposed and they show that the design of spatial
attention is critical to their success in these tasks. In this work, we revisit
the design of the spatial attention and demonstrate that a carefully-devised
yet simple spatial attention mechanism performs favourably against the
state-of-the-art schemes. As a result, we propose two vision transformer
architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures
are highly-efficient and easy to implement, only involving matrix
multiplications that are highly optimized in modern deep learning frameworks.
More importantly, the proposed architectures achieve excellent performance on a
wide range of visual tasks, including image level classification as well as
dense detection and segmentation. The simplicity and strong performance suggest
that our proposed architectures may serve as stronger backbones for many vision
tasks. Our code is released at https://github.com/Meituan-AutoML/Twins .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOLQ: Segmenting Objects by Learning Queries. (arXiv:2106.02351v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02351">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an end-to-end framework for instance segmentation.
Based on the recently introduced DETR [1], our method, termed SOLQ, segments
objects by learning unified queries. In SOLQ, each query represents one object
and has multiple representations: class, location and mask. The object queries
learned perform classification, box regression and mask encoding simultaneously
in an unified vector form. During training phase, the mask vectors encoded are
supervised by the compression coding of raw spatial masks. In inference time,
mask vectors produced can be directly transformed to spatial masks by the
inverse process of compression coding. Experimental results show that SOLQ can
achieve state-of-the-art performance, surpassing most of existing approaches.
Moreover, the joint learning of unified query representation can greatly
improve the detection performance of DETR. We hope our SOLQ can serve as a
strong baseline for the Transformer-based instance segmentation. Code is
available at https://github.com/megvii-research/SOLQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos. (arXiv:2107.11629v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11629">
<div class="article-summary-box-inner">
<span><p>Exploring to what humans pay attention in dynamic panoramic scenes is useful
for many fundamental applications, including augmented reality (AR) in retail,
AR-powered recruitment, and visual language navigation. With this goal in mind,
we propose PV-SOD, a new task that aims to segment salient objects from
panoramic videos. In contrast to existing fixation-/object-level saliency
detection tasks, we focus on audio-induced salient object detection (SOD),
where the salient objects are labeled with the guidance of audio-induced eye
movements. To support this task, we collect the first large-scale dataset,
named ASOD60K, which contains 4K-resolution video frames annotated with a
six-level hierarchy, thus distinguishing itself with richness, diversity and
quality. Specifically, each sequence is marked with both its super-/sub-class,
with objects of each sub-class being further annotated with human eye
fixations, bounding boxes, object-/instance-level masks, and associated
attributes (e.g., geometrical distortion). These coarse-to-fine annotations
enable detailed analysis for PV-SOD modelling, e.g., determining the major
challenges for existing SOD models, and predicting scanpaths to study the
long-term eye fixation behaviors of humans. We systematically benchmark 11
representative approaches on ASOD60K and derive several interesting findings.
We hope this study could serve as a good starting point for advancing SOD
research towards panoramic videos. The dataset and benchmark will be made
publicly available at https://github.com/PanoAsh/ASOD60K.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation and CNN Classification For Automatic COVID-19 Diagnosis From CT-Scan Images On Small Dataset. (arXiv:2108.07148v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07148">
<div class="article-summary-box-inner">
<span><p>We present an automatic COVID1-19 diagnosis framework from lung CT images.
The focus is on signal processing and classification on small datasets with
efforts putting into exploring data preparation and augmentation to improve the
generalization capability of the 2D CNN classification models. We propose a
unique and effective data augmentation method using multiple Hounsfield Unit
(HU) normalization windows. In addition, the original slice image is cropped to
exclude background, and a filter is applied to filter out closed-lung images.
For the classification network, we choose to use 2D Densenet and Xception with
the feature pyramid network (FPN). To further improve the classification
accuracy, an ensemble of multiple CNN models and HU windows is used. On the
training/validation dataset, we achieve a patient classification accuracy of
93.39%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Robust Mitotic Figure Detection with Style Transfer. (arXiv:2109.01124v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01124">
<div class="article-summary-box-inner">
<span><p>We propose a new training scheme for domain generalization in mitotic figure
detection. Mitotic figures show different characteristics for each scanner. We
consider each scanner as a 'domain' and the image distribution specified for
each domain as 'style'. The goal is to train our network to be robust on
scanner types by using various 'style' images. To expand the style variance, we
transfer a style of the training image into arbitrary styles, by defining a
module based on StarGAN. Our model with the proposed training scheme shows
positive performance on MIDOG Preliminary Test-Set containing scanners never
seen before.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">METEOR: A Massive Dense & Heterogeneous Behavior Dataset for Autonomous Driving. (arXiv:2109.07648v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07648">
<div class="article-summary-box-inner">
<span><p>We present a new and complex traffic dataset, METEOR, which captures traffic
patterns in unstructured scenarios in India. METEOR consists of more than 1000
one-minute video clips, over 2 million annotated frames with ego-vehicle
trajectories, and more than 13 million bounding boxes for surrounding vehicles
or traffic agents. METEOR is a unique dataset in terms of capturing the
heterogeneity of microscopic and macroscopic traffic characteristics.
Furthermore, we provide annotations for rare and interesting driving behaviors
such as cut-ins, yielding, overtaking, overspeeding, zigzagging, sudden lane
changing, running traffic signals, driving in the wrong lanes, taking wrong
turns, lack of right-of-way rules at intersections, etc. We also present
diverse traffic scenarios corresponding to rainy weather, nighttime driving,
driving in rural areas with unmarked roads, and high-density traffic scenarios.
We use our novel dataset to evaluate the performance of object detection and
behavior prediction algorithms. We show that state-of-the-art object detectors
fail in these challenging conditions and also propose a new benchmark test:
action-behavior prediction with a baseline mAP score of 70.74.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Gesture Recognition. (arXiv:2109.09396v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09396">
<div class="article-summary-box-inner">
<span><p>The Human-Machine Interaction (HMI) research field is an important topic in
machine learning that has been deeply investigated thanks to the rise of
computing power in the last years. The first time, it is possible to use
machine learning to classify images and/or videos instead of the traditional
computer vision algorithms. The aim of this paper is to build a symbiosis
between a convolutional neural network (CNN) and a recurrent neural network
(RNN) to recognize cultural/anthropological Italian sign language gestures from
videos. The CNN extracts important features that later are used by the RNN.
With RNNs we are able to store temporal information inside the model to provide
contextual information from previous frames to enhance the prediction accuracy.
Our novel approach uses different data augmentation techniques and
regularization methods from only RGB frames to avoid overfitting and provide a
small generalization error.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robotic Vision for Space Mining. (arXiv:2109.12109v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12109">
<div class="article-summary-box-inner">
<span><p>Future Moon bases will likely be constructed using resources mined from the
surface of the Moon. The difficulty of maintaining a human workforce on the
Moon and communications lag with Earth means that mining will need to be
conducted using collaborative robots with a high degree of autonomy. In this
paper, we explore the utility of robotic vision towards addressing several
major challenges in autonomous mining in the lunar environment: lack of
satellite positioning systems, navigation in hazardous terrain, and delicate
robot interactions. Specifically, we describe and report the results of robotic
vision algorithms that we developed for Phase 2 of the NASA Space Robotics
Challenge, which was framed in the context of autonomous collaborative robots
for mining on the Moon. The competition provided a simulated lunar environment
that exhibits the complexities alluded to above. We show how machine
learning-enabled vision could help alleviate the challenges posed by the lunar
environment. A robust multi-robot coordinator was also developed to achieve
long-term operation and effective collaboration between robots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog. (arXiv:2109.12212v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12212">
<div class="article-summary-box-inner">
<span><p>Online conversations include more than just text. Increasingly, image-based
responses such as memes and animated gifs serve as culturally recognized and
often humorous responses in conversation. However, while NLP has broadened to
multimodal models, conversational dialog systems have largely focused only on
generating text replies. Here, we introduce a new dataset of 1.56M text-gif
conversation turns and introduce a new multimodal conversational model Pepe the
King Prawn for selecting gif-based replies. We demonstrate that our model
produces relevant and high-quality gif responses and, in a large randomized
control trial of multiple models replying to real users, we show that our model
replies with gifs that are significantly better received by the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TreeNet: A lightweight One-Shot Aggregation Convolutional Network. (arXiv:2109.12342v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12342">
<div class="article-summary-box-inner">
<span><p>The architecture of deep convolutional networks (CNNs) has evolved for years,
becoming more accurate and faster. However, it is still challenging to design
reasonable network structures that aim at obtaining the best accuracy under a
limited computational budget. In this paper, we propose a Tree block, named
after its appearance, which extends the One-Shot Aggregation (OSA) module while
being more lightweight and flexible. Specifically, the Tree block replaces each
of the $3\times3$ Conv layers in OSA into a stack of shallow residual block
(SRB) and $1\times1$ Conv layer. The $1\times1$ Conv layer is responsible for
dimension increasing and the SRB is fed into the next step. By doing this, when
aggregating the same number of subsequent feature maps, the Tree block has a
deeper network structure while having less model complexity. In addition,
residual connection and efficient channel attention(ECA) is added to the Tree
block to further improve the performance of the network. Based on the Tree
block, we build efficient backbone models calling TreeNets. TreeNet has a
similar network architecture to ResNet, making it flexible to replace ResNet in
various computer vision frameworks. We comprehensively evaluate TreeNet on
common-used benchmarks, including ImageNet-1k for classification, MS COCO for
object detection, and instance segmentation. Experimental results demonstrate
that TreeNet is more efficient and performs favorably against the current
state-of-the-art backbone methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detailed Region-Adaptive Normalization for Heavy Makeup Transfer. (arXiv:2109.14525v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14525">
<div class="article-summary-box-inner">
<span><p>In recent years, facial makeup transfer has attracted growing attention due
to its efficiency and flexibility in transferring makeup styles between
different faces. Although recent works have achieved realistic results, most of
them fail to handle heavy makeup styles with multiple colors and subtle
details. Hence we propose a novel GAN model to handle heavy makeup transfer,
while maintaining the robustness to different poses and expressions. Firstly, a
Makeup Multi-Extraction Network is introduced to learn region-wise makeup
features from multiple layers. Then, a key transferring module called Detailed
Region-Adaptive Normalization is proposed to fuse different levels of makeup
styles in an adaptive way, making great improvement to the quality of heavy
makeup transfer. With the outputs from the two components, Makeup Transfer
Network is used to perform makeup transfer. To evaluate the efficacy of our
proposed method, we collected a new makeup dataset containing a wide range of
heavy styles. Experiments show that our method achieves state-of-the-art
results both on light and heavy makeup styles, and is robust to different poses
and expressions.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-01 23:02:11.218653649 UTC">2021-10-01 23:02:11 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>