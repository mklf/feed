<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-15T01:30:00Z">03-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">When classifying grammatical role, BERT doesn't care about word order... except when it matters. (arXiv:2203.06204v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06204">
<div class="article-summary-box-inner">
<span><p>Because meaning can often be inferred from lexical semantics alone, word
order is often a redundant cue in natural language. For example, the words
chopped, chef, and onion are more likely used to convey "The chef chopped the
onion," not "The onion chopped the chef." Recent work has shown large language
models to be surprisingly word order invariant, but crucially has largely
considered natural prototypical inputs, where compositional meaning mostly
matches lexical expectations. To overcome this confound, we probe grammatical
role representation in English BERT and GPT-2, on instances where lexical
expectations are not sufficient, and word order knowledge is necessary for
correct classification. Such non-prototypical instances are naturally occurring
English sentences with inanimate subjects or animate objects, or sentences
where we systematically swap the arguments to make sentences like "The onion
chopped the chef". We find that, while early layer embeddings are largely
lexical, word order is in fact crucial in defining the later-layer
representations of words in semantically non-prototypical positions. Our
experiments isolate the effect of word order on the contextualization process,
and highlight how models use context in the uncommon, but critical, instances
where it matters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Staged Training for Transformer Language Models. (arXiv:2203.06211v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06211">
<div class="article-summary-box-inner">
<span><p>The current standard approach to scaling transformer language models trains
each model size from a different random initialization. As an alternative, we
consider a staged training setup that begins with a small model and
incrementally increases the amount of compute used for training by applying a
"growth operator" to increase the model depth and width. By initializing each
stage with the output of the previous one, the training process effectively
re-uses the compute from prior stages and becomes more efficient. Our growth
operators each take as input the entire training state (including model
parameters, optimizer state, learning rate schedule, etc.) and output a new
training state from which training continues. We identify two important
properties of these growth operators, namely that they preserve both the loss
and the "training dynamics" after applying the operator. While the
loss-preserving property has been discussed previously, to the best of our
knowledge this work is the first to identify the importance of preserving the
training dynamics (the rate of decrease of the loss during training). To find
the optimal schedule for stages, we use the scaling laws from (Kaplan et al.,
2020) to find a precise schedule that gives the most compute saving by starting
a new stage when training efficiency starts decreasing. We empirically validate
our growth operators and staged training for autoregressive language models,
showing up to 22% compute savings compared to a strong baseline trained from
scratch. Our code is available at https://github.com/allenai/staged-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment. (arXiv:2203.06228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06228">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) have achieved superhuman performance on
many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context
Definition Alignment), a challenging benchmark that measures natural language
understanding (NLU) capabilities of PLMs: Given a definition and a context each
for k words, but not the words themselves, the task is to align the k
definitions with the k contexts. CoDA21 requires a deep understanding of
contexts and definitions, including complex inference and world knowledge. We
find that there is a large gap between human and PLM performance, suggesting
that CoDA21 measures an aspect of NLU that is not sufficiently covered in
existing benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI agents for facilitating social interactions and wellbeing. (arXiv:2203.06244v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06244">
<div class="article-summary-box-inner">
<span><p>Wellbeing AI has been becoming a new trend in individuals' mental health,
organizational health, and flourishing our societies. Various applications of
wellbeing AI have been introduced to our daily lives. While social
relationships within groups are a critical factor for wellbeing, the
development of wellbeing AI for social interactions remains relatively scarce.
In this paper, we provide an overview of the mediative role of AI-augmented
agents for social interactions. First, we discuss the two-dimensional framework
for classifying wellbeing AI: individual/group and analysis/intervention.
Furthermore, wellbeing AI touches on intervening social relationships between
human-human interactions since positive social relationships are key to human
wellbeing. This intervention may raise technical and ethical challenges. We
discuss opportunities and challenges of the relational approach with wellbeing
AI to promote wellbeing in our societies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Inference with A Chinese Entailment Graph. (arXiv:2203.06264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06264">
<div class="article-summary-box-inner">
<span><p>Predicate entailment detection is a crucial task for question-answering from
text, where previous work has explored unsupervised learning of entailment
graphs from typed open relation triples. In this paper, we present the first
pipeline for building Chinese entailment graphs, which involves a novel
high-recall open relation extraction (ORE) method and the first Chinese
fine-grained entity typing dataset under the FIGER type ontology. Through
experiments on the Levy-Holt dataset, we verify the strength of our Chinese
entailment graph, and reveal the cross-lingual complementarity: on the parallel
Levy-Holt dataset, an ensemble of Chinese and English entailment graphs
outperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC
points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Topic Modeling with Deep Mutual Information Estimation. (arXiv:2203.06298v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06298">
<div class="article-summary-box-inner">
<span><p>The emerging neural topic models make topic modeling more easily adaptable
and extendable in unsupervised text mining. However, the existing neural topic
models is difficult to retain representative information of the documents
within the learnt topic representation. In this paper, we propose a neural
topic model which incorporates deep mutual information estimation, i.e., Neural
Topic Modeling with Deep Mutual Information Estimation(NTM-DMIE). NTM-DMIE is a
neural network method for topic learning which maximizes the mutual information
between the input documents and their latent topic representation. To learn
robust topic representation, we incorporate the discriminator to discriminate
negative examples and positive examples via adversarial learning. Moreover, we
use both global and local mutual information to preserve the rich information
of the input documents in the topic representation. We evaluate NTM-DMIE on
several metrics, including accuracy of text clustering, with topic
representation, topic uniqueness and topic coherence. Compared to the existing
methods, the experimental results show that NTM-DMIE can outperform in all the
metrics on the four datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble Semi-supervised Entity Alignment via Cycle-teaching. (arXiv:2203.06308v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06308">
<div class="article-summary-box-inner">
<span><p>Entity alignment is to find identical entities in different knowledge graphs.
Although embedding-based entity alignment has recently achieved remarkable
progress, training data insufficiency remains a critical challenge.
Conventional semi-supervised methods also suffer from the incorrect entity
alignment in newly proposed training data. To resolve these issues, we design
an iterative cycle-teaching framework for semi-supervised entity alignment. The
key idea is to train multiple entity alignment models (called aligners)
simultaneously and let each aligner iteratively teach its successor the
proposed new entity alignment. We propose a diversity-aware alignment selection
method to choose reliable entity alignment for each aligner. We also design a
conflict resolution mechanism to resolve the alignment conflict when combining
the new alignment of an aligner and that from its teacher. Besides, considering
the influence of cycle-teaching order, we elaborately design a strategy to
arrange the optimal order that can maximize the overall performance of multiple
aligners. The cycle-teaching process can break the limitations of each model's
learning capability and reduce the noise in new training data, leading to
improved performance. Extensive experiments on benchmark datasets demonstrate
the effectiveness of the proposed cycle-teaching framework, which significantly
outperforms the state-of-the-art models when the training data is insufficient
and the new entity alignment has much noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELLE: Efficient Lifelong Pre-training for Emerging Data. (arXiv:2203.06311v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06311">
<div class="article-summary-box-inner">
<span><p>Current pre-trained language models (PLM) are typically trained with static
data, ignoring that in real-world scenarios, streaming data of various sources
may continuously grow. This requires PLMs to integrate the information from all
the sources in a lifelong manner. Although this goal could be achieved by
exhaustive pre-training on all the existing data, such a process is known to be
computationally expensive. To this end, we propose ELLE, aiming at efficient
lifelong pre-training for emerging data. Specifically, ELLE consists of (1)
function preserved model expansion, which flexibly expands an existing PLM's
width and depth to improve the efficiency of knowledge acquisition; and (2)
pre-trained domain prompts, which disentangle the versatile knowledge learned
during pre-training and stimulate the proper knowledge for downstream tasks. We
experiment ELLE with streaming data from 5 domains on BERT and GPT. The results
show the superiority of ELLE over various lifelong learning baselines in both
pre-training efficiency and downstream performances. The codes are publicly
available at https://github.com/thunlp/ELLE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Equal Opportunity Fairness through Adversarial Learning. (arXiv:2203.06317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06317">
<div class="article-summary-box-inner">
<span><p>Adversarial training is a common approach for bias mitigation in natural
language processing. Although most work on debiasing is motivated by equal
opportunity, it is not explicitly captured in standard adversarial training. In
this paper, we propose an augmented discriminator for adversarial training,
which takes the target class as input to create richer features and more
explicitly model equal opportunity. Experimental results over two datasets show
that our method substantially improves over standard adversarial debiasing
methods, in terms of the performance--fairness trade-off.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes Reading Comprehension Questions Difficult?. (arXiv:2203.06342v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06342">
<div class="article-summary-box-inner">
<span><p>For a natural language understanding benchmark to be useful in research, it
has to consist of examples that are diverse and difficult enough to
discriminate among current and near-future state-of-the-art systems. However,
we do not yet know how best to select text sources to collect a variety of
challenging examples. In this study, we crowdsource multiple-choice reading
comprehension questions for passages taken from seven qualitatively distinct
sources, analyzing what attributes of passages contribute to the difficulty and
question types of the collected examples. To our surprise, we find that passage
source, length, and readability measures do not significantly affect question
difficulty. Through our manual annotation of seven reasoning types, we observe
several trends between passage sources and reasoning types, e.g., logical
reasoning is more often required in questions written for technical passages.
These results suggest that when creating a new benchmark dataset, selecting a
diverse set of passages can help ensure a diverse range of question types, but
that passage difficulty need not be a priority.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MarkBERT: Marking Word Boundaries Improves Chinese BERT. (arXiv:2203.06378v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06378">
<div class="article-summary-box-inner">
<span><p>We present a Chinese BERT model dubbed MarkBERT that uses word information.
Existing word-based BERT models regard words as basic units, however, due to
the vocabulary limit of BERT, they only cover high-frequency words and fall
back to character level when encountering out-of-vocabulary (OOV) words.
Different from existing works, MarkBERT keeps the vocabulary being Chinese
characters and inserts boundary markers between contiguous words. Such design
enables the model to handle any words in the same way, no matter they are OOV
words or not. Besides, our model has two additional benefits: first, it is
convenient to add word-level learning objectives over markers, which is
complementary to traditional character and sentence-level pre-training tasks;
second, it can easily incorporate richer semantics such as POS tags of words by
replacing generic markers with POS tag-specific markers. MarkBERT pushes the
state-of-the-art of Chinese named entity recognition from 95.4\% to 96.5\% on
the MSRA dataset and from 82.8\% to 84.2\% on the OntoNotes dataset,
respectively. Compared to previous word-based BERT models, MarkBERT achieves
better accuracy on text classification, keyword recognition, and semantic
similarity tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation. (arXiv:2203.06386v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06386">
<div class="article-summary-box-inner">
<span><p>The recent large-scale vision-language pre-training (VLP) of dual-stream
architectures (e.g., CLIP) with a tremendous amount of image-text pair data,
has shown its superiority on various multimodal alignment tasks. Despite its
success, the resulting models are not capable of multimodal generative tasks
due to the weak text encoder. To tackle this problem, we propose to augment the
dual-stream VLP model with a textual pre-trained language model (PLM) via
vision-language knowledge distillation (VLKD), enabling the capability for
multimodal generation. VLKD is pretty data- and computation-efficient compared
to the pre-training from scratch. Experimental results show that the resulting
model has strong zero-shot performance on multimodal generation tasks, such as
open-ended visual question answering and image captioning. For example, it
achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous
state-of-the-art zero-shot model with $7\times$ fewer parameters. Furthermore,
the original textual language understanding and generation ability of the PLM
is maintained after VLKD, which makes our model versatile for both multimodal
and unimodal tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiBERT: Accurate Fully Binarized BERT. (arXiv:2203.06390v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06390">
<div class="article-summary-box-inner">
<span><p>The large pre-trained BERT has achieved remarkable performance on Natural
Language Processing (NLP) tasks but is also computation and memory expensive.
As one of the powerful compression approaches, binarization extremely reduces
the computation and memory consumption by utilizing 1-bit parameters and
bitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit
weight, embedding, and activation) usually suffer a significant performance
drop, and there is rare study addressing this problem. In this paper, with the
theoretical justification and empirical analysis, we identify that the severe
performance drop can be mainly attributed to the information degradation and
optimization direction mismatch respectively in the forward and backward
propagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate
the performance bottlenecks. Specifically, BiBERT introduces an efficient
Bi-Attention structure for maximizing representation information statistically
and a Direction-Matching Distillation (DMD) scheme to optimize the full
binarized BERT accurately. Extensive experiments show that BiBERT outperforms
both the straightforward baseline and existing state-of-the-art quantized BERTs
with ultra-low bit activations by convincing margins on the NLP benchmark. As
the first fully binarized BERT, our method yields impressive 56.3 times and
31.2 times saving on FLOPs and model size, demonstrating the vast advantages
and potential of the fully binarized BERT model in real-world
resource-constrained scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A combined approach to the analysis of speech conversations in a contact center domain. (arXiv:2203.06396v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06396">
<div class="article-summary-box-inner">
<span><p>The ever more accurate search for deep analysis in customer data is a really
strong technological trend nowadays, quite appealing to both private and public
companies. This is particularly true in the contact center domain, where speech
analytics is an extremely powerful methodology for gaining insights from
unstructured data, coming from customer and human agent conversations. In this
work, we describe an experimentation with a speech analytics process for an
Italian contact center, that deals with call recordings extracted from inbound
or outbound flows. First, we illustrate in detail the development of an
in-house speech-to-text solution, based on Kaldi framework, and evaluate its
performance (and compare it to Google Cloud Speech API). Then, we evaluate and
compare different approaches to the semantic tagging of call transcripts,
ranging from classic regular expressions to machine learning models based on
ngrams and logistic regression, and propose a combination of them, which is
shown to provide a consistent benefit. Finally, a decision tree inducer, called
J48S, is applied to the problem of tagging. Such an algorithm is natively
capable of exploiting sequential data, such as texts, for classification
purposes. The solution is compared with the other approaches and is shown to
provide competitive classification performances, while generating highly
interpretable models and reducing the complexity of the data preparation phase.
The potential operational impact of the whole process is thoroughly examined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Proposal to Study "Is High Quality Data All We Need?". (arXiv:2203.06404v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06404">
<div class="article-summary-box-inner">
<span><p>Even though deep neural models have achieved superhuman performance on many
popular benchmarks, they have failed to generalize to OOD or adversarial
datasets. Conventional approaches aimed at increasing robustness include
developing increasingly large models and augmentation with large scale
datasets. However, orthogonal to these trends, we hypothesize that a smaller,
high quality dataset is what we need. Our hypothesis is based on the fact that
deep neural networks are data driven models, and data is what leads/misleads
models. In this work, we propose an empirical study that examines how to select
a subset of and/or create high quality benchmark data, for a model to learn
effectively. We seek to answer if big datasets are truly needed to learn a
task, and whether a smaller subset of high quality data can replace big
datasets. We plan to investigate both data pruning and data creation paradigms
to generate high quality datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey in Adversarial Defences and Robustness in NLP. (arXiv:2203.06414v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06414">
<div class="article-summary-box-inner">
<span><p>In recent years, it has been seen that deep neural networks are lacking
robustness and are likely to break in case of adversarial perturbations in
input data. Strong adversarial attacks are proposed by various authors for
computer vision and Natural Language Processing (NLP). As a counter-effort,
several defense mechanisms are also proposed to save these networks from
failing. In contrast with image data, generating adversarial attacks and
defending these models is not easy in NLP because of the discrete nature of the
text data. However, numerous methods for adversarial defense are proposed of
late, for different NLP tasks such as text classification, named entity
recognition, natural language inferencing, etc. These methods are not just used
for defending neural networks from adversarial attacks, but also used as a
regularization mechanism during training, saving the model from overfitting.
The proposed survey is an attempt to review different methods proposed for
adversarial defenses in NLP in the recent past by proposing a novel taxonomy.
This survey also highlights the fragility of the advanced deep neural networks
in NLP and the challenges in defending them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues. (arXiv:2203.06419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06419">
<div class="article-summary-box-inner">
<span><p>Indirect speech such as sarcasm achieves a constellation of discourse goals
in human communication. While the indirectness of figurative language warrants
speakers to achieve certain pragmatic goals, it is challenging for AI agents to
comprehend such idiosyncrasies of human communication. Though sarcasm
identification has been a well-explored topic in dialogue analysis, for
conversational systems to truly grasp a conversation's innate meaning and
generate appropriate responses, simply detecting sarcasm is not enough; it is
vital to explain its underlying sarcastic connotation to capture its true
essence. In this work, we study the discourse structure of sarcastic
conversations and propose a novel task - Sarcasm Explanation in Dialogue (SED).
Set in a multimodal and code-mixed setting, the task aims to generate natural
language explanations of satirical conversations. To this end, we curate WITS,
a new dataset to support our task. We propose MAF (Modality Aware Fusion), a
multimodal context-aware attention and global information fusion module to
capture multimodality and use it to benchmark WITS. The proposed attention
module surpasses the traditional multimodal fusion baselines and reports the
best performance on almost all metrics. Lastly, we carry out detailed analyses
both quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice. (arXiv:2203.06462v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06462">
<div class="article-summary-box-inner">
<span><p>Classifiers in natural language processing (NLP) often have a large number of
output classes. For example, neural language models (LMs) and machine
translation (MT) models both predict tokens from a vocabulary of thousands. The
Softmax output layer of these models typically receives as input a dense
feature representation, which has much lower dimensionality than the output. In
theory, the result is some words may be impossible to be predicted via argmax,
irrespective of input features, and empirically, there is evidence this happens
in small language models. In this paper we ask whether it can happen in
practical large language models and translation models. To do so, we develop
algorithms to detect such \emph{unargmaxable} tokens in public models. We find
that 13 out of 150 models do indeed have such tokens; however, they are very
infrequent and unlikely to impact model quality. We release our algorithms and
code to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FiNER: Financial Numeric Entity Recognition for XBRL Tagging. (arXiv:2203.06482v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06482">
<div class="article-summary-box-inner">
<span><p>Publicly traded companies are required to submit periodic reports with
eXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging
the reports is tedious and costly. We, therefore, introduce XBRL tagging as a
new entity extraction task for the financial domain and release FiNER-139, a
dataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction
datasets, FiNER-139 uses a much larger label set of 139 entity types. Most
annotated tokens are numeric, with the correct tag per token depending mostly
on context, rather than the token itself. We show that subword fragmentation of
numeric expressions harms BERT's performance, allowing word-level BILSTMs to
perform better. To improve BERT's performance, we propose two simple and
effective solutions that replace numeric expressions with pseudo-tokens
reflecting original token shapes and numeric magnitudes. We also experiment
with FIN-BERT, an existing BERT model for the financial domain, and release our
own BERT (SEC-BERT), pre-trained on financial filings, which performs best.
Through data and error analysis, we finally identify possible limitations to
inspire future work on XBRL tagging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chart-to-Text: A Large-Scale Benchmark for Chart Summarization. (arXiv:2203.06486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06486">
<div class="article-summary-box-inner">
<span><p>Charts are commonly used for exploring data and communicating insights.
Generating natural language summaries from charts can be very helpful for
people in inferring key insights that would otherwise require a lot of
cognitive and perceptual efforts. We present Chart-to-text, a large-scale
benchmark with two datasets and a total of 44,096 charts covering a wide range
of topics and chart types. We explain the dataset construction process and
analyze the datasets. We also introduce a number of state-of-the-art neural
models as baselines that utilize image captioning and data-to-text generation
techniques to tackle two problem variations: one assumes the underlying data
table of the chart is available while the other needs to extract data from
chart images. Our analysis with automatic and human evaluation shows that while
our best models usually generate fluent summaries and yield reasonable BLEU
scores, they also suffer from hallucinations and factual errors as well as
difficulties in correctly explaining complex patterns and trends in charts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Information Hiding in Natural Language Systems. (arXiv:2203.06512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06512">
<div class="article-summary-box-inner">
<span><p>With data privacy becoming more of a necessity than a luxury in today's
digital world, research on more robust models of privacy preservation and
information security is on the rise. In this paper, we take a look at Natural
Language Steganography (NLS) methods, which perform information hiding in
natural language systems, as a means to achieve data security as well as
confidentiality. We summarize primary challenges regarding the secrecy and
imperceptibility requirements of these systems and propose potential directions
of improvement, specifically targeting steganographic text quality. We believe
that this study will act as an appropriate framework to build more resilient
models of Natural Language Steganography, working towards instilling security
within natural language-based neural models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization. (arXiv:2203.06569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06569">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence neural networks have recently achieved great success in
abstractive summarization, especially through fine-tuning large pre-trained
language models on the downstream dataset. These models are typically decoded
with beam search to generate a unique summary. However, the search space is
very large, and with the exposure bias, such decoding is not optimal. In this
paper, we show that it is possible to directly train a second-stage model
performing re-ranking on a set of summary candidates. Our mixture-of-experts
SummaReranker learns to select a better candidate and consistently improves the
performance of the base model. With a base PEGASUS, we push ROUGE scores by
5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34%
on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and
checkpoints will be available at https://github.com/ntunlp/SummaReranker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Impact of COVID-19 on Education by Social Network Mining. (arXiv:2203.06584v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06584">
<div class="article-summary-box-inner">
<span><p>The Covid-19 virus has been one of the most discussed topics on social
networks in 2020 and 2021 and has affected the classic educational paradigm,
worldwide. In this research, many tweets related to the Covid-19 virus and
education are considered and geo-tagged with the help of the GeoNames
geographic database, which contains a large number of place names. To detect
the feeling of users, sentiment analysis is performed using the RoBERTa
language-based model. Finally, we obtain the trends of frequency of total,
positive, and negative tweets for countries with a high number of Covid-19
confirmed cases. Investigating the results reveals a correlation between the
trends of tweet frequency and the official statistic of confirmed cases for
several countries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Informative Causality Extraction from Medical Literature via Dependency-tree based Patterns. (arXiv:2203.06592v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06592">
<div class="article-summary-box-inner">
<span><p>Extracting cause-effect entities from medical literature is an important task
in medical information retrieval. A solution for solving this task can be used
for compilation of various causality relations, such as, causality between
disease and symptoms, between medications and side effects, between genes and
diseases, etc. Existing solutions for extracting cause-effect entities work
well for sentences where the cause and the effect phrases are name entities,
single-word nouns, or noun phrases consisting of two to three words.
Unfortunately, in medical literature, cause and effect phrases in a sentence
are not simply nouns or noun phrases, rather they are complex phrases
consisting of several words, and existing methods fail to correctly extract the
cause and effect entities in such sentences. Partial extraction of cause and
effect entities conveys poor quality, non informative, and often, contradictory
facts, comparing to the one intended in the given sentence. In this work, we
solve this problem by designing an unsupervised method for cause and effect
phrase extraction, PatternCausality, which is specifically suitable for the
medical literature. Our proposed approach first uses a collection of
cause-effect dependency patterns as template to extract head words of cause and
effect phrases and then it uses a novel phrase extraction method to obtain
complete and meaningful cause and effect phrases from a sentence. Experiments
on a cause-effect dataset built from sentences from PubMed articles show that
for extracting cause and effect entities, PatternCausality is substantially
better than the existing methods with an order of magnitude improvement in the
F-score metric over the best of the existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Study and Analysis of Bengali Folklore with Natural Language Processing Systems. (arXiv:2203.06607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06607">
<div class="article-summary-box-inner">
<span><p>Folklore, a solid branch of folk literature, is the hallmark of any nation or
any society. Such as oral tradition; as proverbs or jokes, it also includes
material culture as well as traditional folk beliefs, and various customs.
Bengali folklore is as rich in-depth as it is amazing. Nevertheless, in the
womb of time, it is determined to sustain its existence. Therefore, our aim in
this study is to make our rich folklore more comprehensible to everyone in a
more sophisticated computational way. Some studies concluded various aspects of
the Bengali language with NLP. Our proposed model is to be specific for Bengali
folklore. Technically, it will be the first step towards Bengali natural
language processing for studying and analyzing the folklore of Bengal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Prompt Tuning for Dialog State Tracking. (arXiv:2203.06654v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06654">
<div class="article-summary-box-inner">
<span><p>A desirable dialog system should be able to continually learn new skills
without forgetting old ones, and thereby adapt to new domains or tasks in its
life cycle. However, continually training a model often leads to a well-known
catastrophic forgetting issue. In this paper, we present Continual Prompt
Tuning, a parameter-efficient framework that not only avoids forgetting but
also enables knowledge transfer between tasks. To avoid forgetting, we only
learn and store a few prompt tokens' embeddings for each task while freezing
the backbone pre-trained model. To achieve bi-directional knowledge transfer
among tasks, we propose several techniques (continual prompt initialization,
query fusion, and memory replay) to transfer knowledge from preceding tasks and
a memory-guided technique to transfer knowledge from subsequent tasks.
Extensive experiments demonstrate the effectiveness and efficiency of our
proposed method on continual learning for dialog state tracking, compared with
state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the answering frame timeline. Extensive experiments on the medical
instructional dataset, namely MedVidQA, show the proposed VPTSL outperforms
other state-of-the-art methods, which demonstrates the effectiveness of visual
prompt and the text span predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Personalized Intelligence at Scale. (arXiv:2203.06668v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06668">
<div class="article-summary-box-inner">
<span><p>Personalized Intelligence (PI) is the problem of providing customized AI
experiences tailored to each individual user. In many applications, PI is
preferred or even required. Existing personalization approaches involve
fine-tuning pre-trained models to create new customized models. However, these
approaches require a significant amount of computation to train, scaling with
model size and the number of users, inhibiting PI to be realized widely. In
this work, we introduce a novel model architecture and training/inference
framework to enable Personalized Intelligence at scale. We achieve this by
attaching a Personalization Head (PH) to pre-trained language models (LM).
During training, the base LMs are frozen and only the parameters in PH are
updated and are unique per user. This results in significantly smaller overall
model sizes and training cost than traditional fine-tuning approaches when
scaled across many users. We evaluate PHs on academia and industry-focused
datasets and show that the PHs outperform zeroshot baseline in F1 score and are
significantly more scalable than traditional fine-tuning approaches. We
identify key factors required for effective PH design and training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarizing a virtual robot's past actions in natural language. (arXiv:2203.06671v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06671">
<div class="article-summary-box-inner">
<span><p>We propose and demonstrate the task of giving natural language summaries of
the actions of a robotic agent in a virtual environment. We explain why such a
task is important, what makes it difficult, and discuss how it might be
addressed. To encourage others to work on this, we show how a popular existing
dataset that matches robot actions with natural language descriptions designed
for an instruction following task can be repurposed to serve as a training
ground for robot action summarization work. We propose and test several methods
of learning to generate such summaries, starting from either egocentric video
frames of the robot taking actions or intermediate text representations of the
actions used by an automatic planner. We provide quantitative and qualitative
evaluations of our results, which can serve as a baseline for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciNLI: A Corpus for Natural Language Inference on Scientific Text. (arXiv:2203.06728v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06728">
<div class="article-summary-box-inner">
<span><p>Existing Natural Language Inference (NLI) datasets, while being instrumental
in the advancement of Natural Language Understanding (NLU) research, are not
related to scientific text. In this paper, we introduce SciNLI, a large dataset
for NLI that captures the formality in scientific text and contains 107,412
sentence pairs extracted from scholarly papers on NLP and computational
linguistics. Given that the text used in scientific literature differs vastly
from the text used in everyday language both in terms of vocabulary and
sentence structure, our dataset is well suited to serve as a benchmark for the
evaluation of scientific NLU models. Our experiments show that SciNLI is harder
to classify than the existing NLI datasets. Our best performing model with
XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23 showing
that there is substantial room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtagonistTagger -- a Tool for Entity Linkage of Persons in Texts from Various Languages and Domains. (arXiv:2203.06746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06746">
<div class="article-summary-box-inner">
<span><p>Named entities recognition (NER) and disambiguation (NED) can add semantic
context to the recognized named entities in texts. Named entity linkage in
texts, regardless of a domain, provides links between the entities mentioned in
unstructured texts and individual instances of real-world objects. In this
poster, we present a tool - protagonistTagger - for person NER and NED in
texts. The tool was tested on texts extracted from classic English novels and
Polish Internet news. The tool's performance (both precision and recall)
fluctuates between 78% and even 88%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pruned Graph Neural Network for Short Story Ordering. (arXiv:2203.06778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06778">
<div class="article-summary-box-inner">
<span><p>Text coherence is a fundamental problem in natural language generation and
understanding. Organizing sentences into an order that maximizes coherence is
known as sentence ordering. This paper is proposing a new approach based on the
graph neural network approach to encode a set of sentences and learn orderings
of short stories. We propose a new method for constructing sentence-entity
graphs of short stories to create the edges between sentences and reduce noise
in our graph by replacing the pronouns with their referring entities. We
improve the sentence ordering by introducing an aggregation method based on
majority voting of state-of-the-art methods and our proposed one. Our approach
employs a BERT-based model to learn semantic representations of the sentences.
The results demonstrate that the proposed method significantly outperforms
existing baselines on a corpus of short stories with a new state-of-the-art
performance in terms of Perfect Match Ratio (PMR) and Kendall's Tau (Tau)
metrics. More precisely, our method increases PMR and Tau criteria by more than
5% and 4.3%, respectively. These outcomes highlight the benefit of forming the
edges between sentences based on their cosine similarity. We also observe that
replacing pronouns with their referring entities effectively encodes sentences
in sentence-entity graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can pre-trained Transformers be used in detecting complex sensitive sentences? -- A Monsanto case study. (arXiv:2203.06793v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06793">
<div class="article-summary-box-inner">
<span><p>Each and every organisation releases information in a variety of forms
ranging from annual reports to legal proceedings. Such documents may contain
sensitive information and releasing them openly may lead to the leakage of
confidential information. Detection of sentences that contain sensitive
information in documents can help organisations prevent the leakage of valuable
confidential information. This is especially challenging when such sentences
contain a substantial amount of information or are paraphrased versions of
known sensitive content. Current approaches to sensitive information detection
in such complex settings are based on keyword-based approaches or standard
machine learning models. In this paper, we wish to explore whether pre-trained
transformer models are well suited to detect complex sensitive information.
Pre-trained transformers are typically trained on an enormous amount of text
and therefore readily learn grammar, structure and other linguistic features,
making them particularly attractive for this task. Through our experiments on
the Monsanto trial data set, we observe that the fine-tuned Bidirectional
Encoder Representations from Transformers (BERT) transformer model performs
better than traditional models. We experimented with four different categories
of documents in the Monsanto dataset and observed that BERT achieves better F2
scores by 24.13\% to 65.79\% for GHOST, 30.14\% to 54.88\% for TOXIC, 39.22\%
for CHEMI, 53.57\% for REGUL compared to existing sensitive information
detection models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Commands for Autonomous Vehicles via Layer Fusion with Region-specific Dynamic Layer Attention. (arXiv:2203.06822v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06822">
<div class="article-summary-box-inner">
<span><p>Grounding a command to the visual environment is an essential ingredient for
interactions between autonomous vehicles and humans. In this work, we study the
problem of language grounding for autonomous vehicles, which aims to localize a
region in a visual scene according to a natural language command from a
passenger. Prior work only employs the top layer representations of a
vision-and-language pre-trained model to predict the region referred to by the
command. However, such a method omits the useful features encoded in other
layers, and thus results in inadequate understanding of the input scene and
command. To tackle this limitation, we present the first layer fusion approach
for this task. Since different visual regions may require distinct types of
features to disambiguate them from each other, we further propose the
region-specific dynamic (RSD) layer attention to adaptively fuse the multimodal
information across layers for each region. Extensive experiments on the
Talk2Car benchmark demonstrate that our approach helps predict more accurate
regions and outperforms state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KenMeSH: Knowledge-enhanced End-to-end Biomedical Text Labelling. (arXiv:2203.06835v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06835">
<div class="article-summary-box-inner">
<span><p>Currently, Medical Subject Headings (MeSH) are manually assigned to every
biomedical article published and subsequently recorded in the PubMed database
to facilitate retrieving relevant information. With the rapid growth of the
PubMed database, large-scale biomedical document indexing becomes increasingly
important. MeSH indexing is a challenging task for machine learning, as it
needs to assign multiple labels to each article from an extremely large
hierachically organized collection. To address this challenge, we propose
KenMeSH, an end-to-end model that combines new text features and a dynamic
\textbf{K}nowledge-\textbf{en}hanced mask attention that integrates document
features with MeSH label hierarchy and journal correlation features to index
MeSH terms. Experimental results show the proposed method achieves
state-of-the-art performance on a number of measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities. (arXiv:2203.06849v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06849">
<div class="article-summary-box-inner">
<span><p>Transfer learning has proven to be crucial in advancing the state of speech
and natural language processing research in recent years. In speech, a model
pre-trained by self-supervised learning transfers remarkably well on multiple
tasks. However, the lack of a consistent evaluation methodology is limiting
towards a holistic understanding of the efficacy of such models. SUPERB was a
step towards introducing a common benchmark to evaluate pre-trained models
across various speech tasks. In this paper, we introduce SUPERB-SG, a new
benchmark focused on evaluating the semantic and generative capabilities of
pre-trained models by increasing task diversity and difficulty over SUPERB. We
use a lightweight methodology to test the robustness of representations learned
by pre-trained models under shifts in data domain and quality across different
types of tasks. It entails freezing pre-trained model parameters, only using
simple task-specific trainable heads. The goal is to be inclusive of all
researchers, and encourage efficient use of computational resources. We also
show that the task diversity of SUPERB-SG coupled with limited task supervision
is an effective recipe for evaluating the generalizability of model
representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary and Distance between Sets of Texts based on Topological Data Analysis. (arXiv:1912.09253v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.09253">
<div class="article-summary-box-inner">
<span><p>In this paper, we use topological data analysis (TDA) tools such as
persistent homology, persistent entropy and bottleneck distance, to provide a
{\it TDA-based summary} of any given set of texts and a general method for
computing a distance between any two literary styles, authors or periods. To
this aim, deep-learning word-embedding techniques are combined with these tools
in order to study the topological properties of texts embedded in a metric
space. As a case of study, we use the written texts of three poets of the
Spanish Golden Age: Francisco de Quevedo, Luis de G\'ongora and Lope de Vega.
As far as we know, this is the first time that word embedding, bottleneck
distance, persistent homology and persistent entropy are used together to
characterize texts and to compare different literary styles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks. (arXiv:2004.06015v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06015">
<div class="article-summary-box-inner">
<span><p>Knowledge graph (KG) question generation (QG) aims to generate natural
language questions from KGs and target answers. Previous works mostly focus on
a simple setting which is to generate questions from a single KG triple. In
this work, we focus on a more realistic setting where we aim to generate
questions from a KG subgraph and target answers. In addition, most of previous
works built on either RNN-based or Transformer-based models to encode a
linearized KG sugraph, which totally discards the explicit structure
information of a KG subgraph. To address this issue, we propose to apply a
bidirectional Graph2Seq model to encode the KG subgraph. Furthermore, we
enhance our RNN decoder with node-level copying mechanism to allow directly
copying node attributes from the KG subgraph to the output question. Both
automatic and human evaluation results demonstrate that our model achieves new
state-of-the-art scores, outperforming existing methods by a significant margin
on two QG benchmarks. Experimental results also show that our QG model can
consistently benefit the Question Answering (QA) task as a mean of data
augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Inter-Aspect Dependencies with a Non-temporal Mechanism for Aspect-Based Sentiment Analysis. (arXiv:2008.05179v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.05179">
<div class="article-summary-box-inner">
<span><p>For multiple aspects scenario of aspect-based sentiment analysis (ABSA),
existing approaches typically ignore inter-aspect relations or rely on temporal
dependencies to process aspect-aware representations of all aspects in a
sentence. Although multiple aspects of a sentence appear in a non-adjacent
sequential order, they are not in a strict temporal relationship as natural
language sequence, thus the aspect-aware sentence representations should not be
treated as temporal dependency processing. In this paper, we propose a novel
non-temporal mechanism to enhance the ABSA task through modeling inter-aspect
dependencies. Furthermore, we focus on the well-known class imbalance issue on
the ABSA task and address it by down-weighting the loss assigned to
well-classified instances. Experiments on two distinct domains of SemEval 2014
task 4 demonstrate the effectiveness of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Transformers: A Survey. (arXiv:2009.06732v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06732">
<div class="article-summary-box-inner">
<span><p>Transformer model architectures have garnered immense interest lately due to
their effectiveness across a range of domains like language, vision and
reinforcement learning. In the field of natural language processing for
example, Transformers have become an indispensable staple in the modern deep
learning stack. Recently, a dizzying number of "X-former" models have been
proposed - Reformer, Linformer, Performer, Longformer, to name a few - which
improve upon the original Transformer architecture, many of which make
improvements around computational and memory efficiency. With the aim of
helping the avid researcher navigate this flurry, this paper characterizes a
large and thoughtful selection of recent efficiency-flavored "X-former" models,
providing an organized and comprehensive overview of existing work and models
across multiple domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Document-level Neural Machine Translation. (arXiv:2010.08961v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08961">
<div class="article-summary-box-inner">
<span><p>This paper does not aim at introducing a novel model for document-level
neural machine translation. Instead, we head back to the original Transformer
model and hope to answer the following question: Is the capacity of current
models strong enough for document-level translation? Interestingly, we observe
that the original Transformer with appropriate training techniques can achieve
strong results for document translation, even with a length of 2000 words. We
evaluate this model and several recent approaches on nine document-level
datasets and two sentence-level datasets across six languages. Experiments show
that document-level Transformer models outperforms sentence-level ones and many
previous methods in a comprehensive set of metrics, including BLEU, four
lexical indices, three newly proposed assistant linguistic indicators, and
human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning. (arXiv:2012.15409v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15409">
<div class="article-summary-box-inner">
<span><p>Existed pre-training methods either focus on single-modal tasks or
multi-modal tasks, and cannot effectively adapt to each other. They can only
utilize single-modal data (i.e. text or image) or limited multi-modal data
(i.e. image-text pairs). In this work, we propose a unified-modal pre-training
architecture, namely UNIMO, which can effectively adapt to both single-modal
and multi-modal understanding and generation tasks. Large scale of free text
corpus and image collections can be utilized to improve the capability of
visual and textual understanding, and cross-modal contrastive learning (CMCL)
is leveraged to align the textual and visual information into a unified
semantic space over a corpus of image-text pairs. As the non-paired
single-modal data is very rich, our model can utilize much larger scale of data
to learn more generalizable representations. Moreover, the textual knowledge
and visual knowledge can enhance each other in the unified semantic space. The
experimental results show that UNIMO significantly improves the performance of
several single-modal and multi-modal downstream tasks. Our code and pre-trained
models are public at the UNIMO project page https://unimo-ptm.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Do Your Biomedical Named Entity Recognition Models Generalize to Novel Entities?. (arXiv:2101.00160v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00160">
<div class="article-summary-box-inner">
<span><p>The number of biomedical literature on new biomedical concepts is rapidly
increasing, which necessitates a reliable biomedical named entity recognition
(BioNER) model for identifying new and unseen entity mentions. However, it is
questionable whether existing models can effectively handle them. In this work,
we systematically analyze the three types of recognition abilities of BioNER
models: memorization, synonym generalization, and concept generalization. We
find that although current best models achieve state-of-the-art performance on
benchmarks based on overall performance, they have limitations in identifying
synonyms and new biomedical concepts, indicating they are overestimated in
terms of their generalization abilities. We also investigate failure cases of
models and identify several difficulties in recognizing unseen mentions in
biomedical literature as follows: (1) models tend to exploit dataset biases,
which hinders the models' abilities to generalize, and (2) several biomedical
names have novel morphological patterns with weak name regularity, and models
fail to recognize them. We apply a statistics-based debiasing method to our
problem as a simple remedy and show the improvement in generalization to unseen
mentions. We hope that our analyses and findings would be able to facilitate
further research into the generalization capabilities of NER models in a domain
where their reliability is of utmost importance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inferring COVID-19 Biological Pathways from Clinical Phenotypes via Topological Analysis. (arXiv:2101.07417v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07417">
<div class="article-summary-box-inner">
<span><p>COVID-19 has caused thousands of deaths around the world and also resulted in
a large international economic disruption. Identifying the pathways associated
with this illness can help medical researchers to better understand the
properties of the condition. This process can be carried out by analyzing the
medical records. It is crucial to develop tools and models that can aid
researchers with this process in a timely manner. However, medical records are
often unstructured clinical notes, and this poses significant challenges to
developing the automated systems. In this article, we propose a pipeline to aid
practitioners in analyzing clinical notes and revealing the pathways associated
with this disease. Our pipeline relies on topological properties and consists
of three steps: 1) pre-processing the clinical notes to extract the salient
concepts, 2) constructing a feature space of the patients to characterize the
extracted concepts, and finally, 3) leveraging the topological properties to
distill the available knowledge and visualize the result. Our experiments on a
publicly available dataset of COVID-19 clinical notes testify that our pipeline
can indeed extract meaningful pathways.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Transport-based Adaptation in Dysarthric Speech Tasks. (arXiv:2104.02535v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02535">
<div class="article-summary-box-inner">
<span><p>In many real-world applications, the mismatch between distributions of
training data (source) and test data (target) significantly degrades the
performance of machine learning algorithms. In speech data, causes of this
mismatch include different acoustic environments or speaker characteristics. In
this paper, we address this issue in the challenging context of dysarthric
speech, by multi-source domain/speaker adaptation (MSDA/MSSA). Specifically, we
propose the use of an optimal-transport based approach, called MSDA via
Weighted Joint Optimal Transport (MSDA-WDJOT). We confront the mismatch problem
in dysarthria detection for which the proposed approach outperforms both the
Baseline and the state-of-the-art MSDA models, improving the detection accuracy
of 0.9% over the best competitor method. We then employ MSDA-WJDOT for
dysarthric speaker adaptation in command speech recognition. This provides a
Command Error Rate relative reduction of 16% and 7% over the baseline and the
best competitor model, respectively. Interestingly, MSDA-WJDOT provides a
similarity score between the source and the target, i.e. between speakers in
this case. We leverage this similarity measure to define a Dysarthric and
Healthy score of the target speaker and diagnose the dysarthria with an
accuracy of 95%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on reinforcement learning for language processing. (arXiv:2104.05565v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05565">
<div class="article-summary-box-inner">
<span><p>In recent years some researchers have explored the use of reinforcement
learning (RL) algorithms as key components in the solution of various natural
language processing tasks. For instance, some of these algorithms leveraging
deep neural learning have found their way into conversational systems. This
paper reviews the state of the art of RL methods for their possible use for
different problems of natural language processing, focusing primarily on
conversational systems, mainly due to their growing relevance. We provide
detailed descriptions of the problems as well as discussions of why RL is
well-suited to solve them. Also, we analyze the advantages and limitations of
these methods. Finally, we elaborate on promising research directions in
natural language processing that might benefit from reinforcement learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08773">
<div class="article-summary-box-inner">
<span><p>Humans (e.g., crowdworkers) have a remarkable ability in solving different
tasks, by simply reading textual instructions that define them and looking at a
few examples. Despite the success of the conventional supervised learning on
individual datasets, such models often struggle with generalization across
tasks (e.g., a question-answering system cannot solve classification tasks). A
long-standing challenge in AI is to build a model that learns a new task by
understanding the human-readable instructions that define it. To study this, we
introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their
human-authored instructions, and 193k task instances (input-output pairs). The
instructions are obtained from crowdsourcing instructions used to create
existing NLP datasets and mapped to a unified schema. Using this meta-dataset,
we measure cross-task generalization by training models on seen tasks and
measuring generalization to the remaining unseen ones. We adopt generative
pre-trained language models to encode task-specific instructions along with
input and generate task output. Our results indicate that models benefit from
instructions when evaluated in terms of generalization to unseen tasks (19%
better for models utilizing instructions). These models, however, are far
behind an estimated performance upperbound indicating significant room for more
progress in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAPEX: Table Pre-training via Learning a Neural SQL Executor. (arXiv:2107.07653v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07653">
<div class="article-summary-box-inner">
<span><p>Recent progress in language model pre-training has achieved a great success
via leveraging large-scale unstructured textual data. However, it is still a
challenge to apply pre-training on structured tabular data due to the absence
of large-scale high-quality tabular data. In this paper, we propose TAPEX to
show that table pre-training can be achieved by learning a neural SQL executor
over a synthetic corpus, which is obtained by automatically synthesizing
executable SQL queries and their execution outputs. TAPEX addresses the data
scarcity challenge via guiding the language model to mimic a SQL executor on
the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX
on four benchmark datasets. Experimental results demonstrate that TAPEX
outperforms previous table pre-training approaches by a large margin and
achieves new state-of-the-art results on all of them. This includes the
improvements on the weakly-supervised WikiSQL denotation accuracy to 89.5%
(+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA
denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2%
(+3.2%). To our knowledge, this is the first work to exploit table pre-training
via synthetic executable programs and to achieve new state-of-the-art results
on various downstream tasks. Our code can be found at
https://github.com/microsoft/Table-Pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Natural Language Representation with Large-Scale Out-of-Domain Commonsense. (arXiv:2109.02572v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02572">
<div class="article-summary-box-inner">
<span><p>We study how to enhance text representation via textual commonsense. We point
out that commonsense has the nature of domain discrepancy. Namely, commonsense
has different data formats and is domain-independent from the downstream task.
This nature brings challenges to introducing commonsense in general text
understanding tasks. A typical method of introducing textual knowledge is
continuing pre-training over the commonsense corpus. However, it will cause
catastrophic forgetting to the downstream task due to the domain discrepancy.
In addition, previous methods of directly using textual descriptions as extra
input information cannot apply to large-scale commonsense.
</p>
<p>In this paper, we propose to use large-scale out-of-domain commonsense to
enhance text representation. In order to effectively incorporate the
commonsense, we proposed OK-Transformer (\underline{O}ut-of-domain
\underline{K}nowledge enhanced \underline{Transformer}). OK-Transformer
effectively integrates commonsense descriptions and enhances them to the target
text representation. In addition, OK-Transformer can adapt to the
Transformer-based language models (e.g. BERT, RoBERTa) for free, without
pre-training on large-scale unsupervised corpora. We have verified the
effectiveness of OK-Transformer in multiple applications such as commonsense
reasoning, general text classification, and low-resource commonsense settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PPT: Pre-trained Prompt Tuning for Few-shot Learning. (arXiv:2109.04332v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04332">
<div class="article-summary-box-inner">
<span><p>Prompts for pre-trained language models (PLMs) have shown remarkable
performance by bridging the gap between pre-training tasks and various
downstream tasks. Among these methods, prompt tuning, which freezes PLMs and
only tunes soft prompts, provides an efficient and effective solution for
adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to
be fully explored. In our pilot experiments, we find that prompt tuning
performs comparably with conventional full-model fine-tuning when downstream
data are sufficient, whereas it performs much worse under few-shot learning
settings, which may hinder the application of prompt tuning in practice. We
attribute this low performance to the manner of initializing soft prompts.
Therefore, in this work, we propose to pre-train prompts by adding soft prompts
into the pre-training stage to obtain a better initialization. We name this
Pre-trained Prompt Tuning framework "PPT". To ensure the generalization of PPT,
we formulate similar classification tasks into a unified task form and
pre-train soft prompts for this unified task. Extensive experiments show that
tuning pre-trained prompts for downstream tasks can reach or even outperform
full-model fine-tuning under both full-data and few-shot settings. Our approach
is effective and efficient for using large-scale PLMs in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations. (arXiv:2109.13059v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13059">
<div class="article-summary-box-inner">
<span><p>In NLP, a large volume of tasks involve pairwise comparison between two
sequences (e.g. sentence similarity and paraphrase identification).
Predominantly, two formulations are used for sentence-pair tasks: bi-encoders
and cross-encoders. Bi-encoders produce fixed-dimensional sentence
representations and are computationally efficient, however, they usually
underperform cross-encoders. Cross-encoders can leverage their attention heads
to exploit inter-sentence interactions for better performance but they require
task fine-tuning and are computationally more expensive. In this paper, we
present a completely unsupervised sentence representation model termed as
Trans-Encoder that combines the two learning paradigms into an iterative joint
framework to simultaneously learn enhanced bi- and cross-encoders.
Specifically, on top of a pre-trained Language Model (PLM), we start with
converting it to an unsupervised bi-encoder, and then alternate between the bi-
and cross-encoder task formulations. In each alternation, one task formulation
will produce pseudo-labels which are used as learning signals for the other
task formulation. We then propose an extension to conduct such
self-distillation approach on multiple PLMs in parallel and use the average of
their pseudo-labels for mutual-distillation. Trans-Encoder creates, to the best
of our knowledge, the first completely unsupervised cross-encoder and also a
state-of-the-art unsupervised bi-encoder for sentence similarity. Both the
bi-encoder and cross-encoder formulations of Trans-Encoder outperform recently
proposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT
and SimCSE by up to 5% on the sentence similarity benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LexGLUE: A Benchmark Dataset for Legal Language Understanding in English. (arXiv:2110.00976v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00976">
<div class="article-summary-box-inner">
<span><p>Laws and their interpretations, legal arguments and agreements\ are typically
expressed in writing, leading to the production of vast corpora of legal text.
Their analysis, which is at the center of legal practice, becomes increasingly
elaborate as these collections grow in size. Natural language understanding
(NLU) technologies can be a valuable tool to support legal practitioners in
these endeavors. Their usefulness, however, largely depends on whether current
state-of-the-art models can generalize across various tasks in the legal
domain. To answer this currently open question, we introduce the Legal General
Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets
for evaluating model performance across a diverse set of legal NLU tasks in a
standardized way. We also provide an evaluation and analysis of several generic
and legal-oriented models demonstrating that the latter consistently offer
performance improvements across multiple tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeTS: A Benchmark for Translation Suggestion. (arXiv:2110.05151v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05151">
<div class="article-summary-box-inner">
<span><p>Translation Suggestion (TS), which provides alternatives for specific words
or phrases given the entire documents translated by machine translation (MT)
\cite{lee2021intellicat}, has been proven to play a significant role in post
editing (PE). However, there is still no publicly available data set to support
in-depth research for this problem, and no reproducible experimental results
can be followed by researchers in this community. To break this limitation, we
create a benchmark data set for TS, called \emph{WeTS}, which contains golden
corpus annotated by expert translators on four translation directions. Apart
from the human-annotated golden corpus, we also propose several novel methods
to generate synthetic corpus which can substantially improve the performance of
TS. With the corpus we construct, we introduce the Transformer-based model for
TS, and experimental results show that our model achieves State-Of-The-Art
(SOTA) results on all four translation directions, including English-to-German,
German-to-English, Chinese-to-English and English-to-Chinese. Codes and corpus
can be found at \url{https://github.com/ZhenYangIACAS/WeTS.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?. (arXiv:2110.06918v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06918">
<div class="article-summary-box-inner">
<span><p>Despite their recent popularity and well-known advantages, dense retrievers
still lag behind sparse methods such as BM25 in their ability to reliably match
salient phrases and rare entities in the query and to generalize to
out-of-domain data. It has been argued that this is an inherent limitation of
dense models. We rebut this claim by introducing the Salient Phrase Aware
Retriever (SPAR), a dense retriever with the lexical matching capacity of a
sparse model. We show that a dense Lexical Model {\Lambda} can be trained to
imitate a sparse one, and SPAR is built by augmenting a standard dense
retriever with {\Lambda}. Empirically, SPAR shows superior performance on a
range of tasks including five question answering datasets, MS MARCO passage
retrieval, as well as the EntityQuestions and BEIR benchmarks for out-of-domain
evaluation, exceeding the performance of state-of-the-art dense and sparse
retrievers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIMICause: Representation and automatic extraction of causal relation types from clinical notes. (arXiv:2110.07090v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07090">
<div class="article-summary-box-inner">
<span><p>Understanding causal narratives communicated in clinical notes can help make
strides towards personalized healthcare. Extracted causal information from
clinical notes can be combined with structured EHR data such as patients'
demographics, diagnoses, and medications. This will enhance healthcare
providers' ability to identify aspects of a patient's story communicated in the
clinical notes and help make more informed decisions.
</p>
<p>In this work, we propose annotation guidelines, develop an annotated corpus
and provide baseline scores to identify types and direction of causal relations
between a pair of biomedical concepts in clinical notes; communicated
implicitly or explicitly, identified either in a single sentence or across
multiple sentences.
</p>
<p>We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2
shared task dataset and train four different language model based
architectures. Annotation based on our guidelines achieved a high
inter-annotator agreement i.e. Fleiss' kappa ($\kappa$) score of 0.72, and our
model for identification of causal relations achieved a macro F1 score of 0.56
on the test data. The high inter-annotator agreement for clinical text shows
the quality of our annotation guidelines while the provided baseline F1 score
sets the direction for future research towards understanding narratives in
clinical texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings. (arXiv:2110.07385v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07385">
<div class="article-summary-box-inner">
<span><p>Style transfer is the task of rewriting a sentence into a target style while
approximately preserving content. While most prior literature assumes access to
a large style-labelled corpus, recent work (Riley et al. 2021) has attempted
"few-shot" style transfer using only 3-10 sentences at inference for style
extraction. In this work we study a relevant low-resource setting: style
transfer for languages where no style-labelled corpora are available. We notice
that existing few-shot methods perform this task poorly, often copying inputs
verbatim. We push the state-of-the-art for few-shot style transfer with a new
method modeling the stylistic difference between paraphrases. When compared to
prior work, our model achieves 2-3x better performance in formality transfer
and code-mixing addition across seven languages. Moreover, our method is better
at controlling the style transfer magnitude using an input scalar knob. We
report promising qualitative results for several attribute transfer tasks
(sentiment transfer, simplification, gender neutralization, text anonymization)
all without retraining the model. Finally, we find model evaluation to be
difficult due to the lack of datasets and metrics for many languages. To
facilitate future research we crowdsource formality annotations for 4000
sentence pairs in four Indic languages, and use this data to design our
automatic evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER. (arXiv:2110.08454v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08454">
<div class="article-summary-box-inner">
<span><p>Recent advances in prompt-based learning have shown strong results on
few-shot text classification by using cloze-style templates. Similar attempts
have been made on named entity recognition (NER) which manually design
templates to predict entity types for every text span in a sentence. However,
such methods may suffer from error propagation induced by entity span
detection, high cost due to enumeration of all possible text spans, and
omission of inter-dependencies among token labels in a sentence. Here we
present a simple demonstration-based learning method for NER, which lets the
input be prefaced by task demonstrations for in-context learning. We perform a
systematic study on demonstration strategy regarding what to include (entity
examples, with or without surrounding context), how to select the examples, and
what templates to use. Results on in-domain learning and domain adaptation show
that the model's performance in low-resource settings can be largely improved
with a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train
instances). We also find that good demonstration can save many labeled examples
and consistency in demonstration contributes to better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v8 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for text-to-image and image-to-text
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation tasks without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial results of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena. (arXiv:2112.07566v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07566">
<div class="article-summary-box-inner">
<span><p>We propose VALSE (Vision And Language Structured Evaluation), a novel
benchmark designed for testing general-purpose pretrained vision and language
(V&amp;L) models for their visio-linguistic grounding capabilities on specific
linguistic phenomena. VALSE offers a suite of six tests covering various
linguistic constructs. Solving these requires models to ground linguistic
phenomena in the visual modality, allowing more fine-grained evaluations than
hitherto possible. We build VALSE using methods that support the construction
of valid foils, and report results from evaluating five widely-used V&amp;L models.
Our experiments suggest that current models have considerable difficulty
addressing most phenomena. Hence, we expect VALSE to serve as an important
benchmark to measure future progress of pretrained V&amp;L models from a linguistic
perspective, complementing the canonical task-centred V&amp;L evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebGPT: Browser-assisted question-answering with human feedback. (arXiv:2112.09332v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09332">
<div class="article-summary-box-inner">
<span><p>We fine-tune GPT-3 to answer long-form questions using a text-based
web-browsing environment, which allows the model to search and navigate the
web. By setting up the task so that it can be performed by humans, we are able
to train models on the task using imitation learning, and then optimize answer
quality with human feedback. To make human evaluation of factual accuracy
easier, models must collect references while browsing in support of their
answers. We train and evaluate our models on ELI5, a dataset of questions asked
by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior
cloning, and then performing rejection sampling against a reward model trained
to predict human preferences. This model's answers are preferred by humans 56%
of the time to those of our human demonstrators, and 69% of the time to the
highest-voted answer from Reddit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02639">
<div class="article-summary-box-inner">
<span><p>As humans, we navigate a multimodal world, building a holistic understanding
from all our senses. We introduce MERLOT Reserve, a model that represents
videos jointly over time -- through a new training objective that learns from
audio, subtitles, and video frames. Given a video, we replace snippets of text
and audio with a MASK token; the model learns by choosing the correct
masked-out snippet. Our objective learns faster than alternatives, and performs
well at scale: we pretrain on 20 million YouTube videos.
</p>
<p>Empirical results show that MERLOT Reserve learns strong multimodal
representations. When finetuned, it sets state-of-the-art on Visual Commonsense
Reasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%,
and 1.5% respectively. Ablations show that these tasks benefit from audio
pretraining -- even VCR, a QA task centered around images (without sound).
Moreover, our objective enables out-of-the-box prediction, revealing strong
multimodal commonsense understanding. In a fully zero-shot setting, our model
obtains competitive results on four video tasks, even outperforming supervised
approaches on the recently proposed Situated Reasoning (STAR) benchmark.
</p>
<p>We analyze why audio enables better vision-language representations,
suggesting significant opportunities for future research. We conclude by
discussing ethical and societal implications of multimodal pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command Recognition. (arXiv:2201.03804v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03804">
<div class="article-summary-box-inner">
<span><p>With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
https://github.com/HLTCHKUST/CI-AVSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two heads are better than one: Enhancing medical representations by pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10113">
<div class="article-summary-box-inner">
<span><p>The massive amount of electronic health records (EHRs) has created enormous
potentials for improving healthcare, among which clinical codes (structured
data) and clinical narratives (unstructured data) are two important textual
modalities. Most existing EHR-oriented studies, however, either only focus on a
particular modality or integrate data from different modalities in a shallow
manner, which ignores the intrinsic interactions between them. To address these
issues, we proposed a Medical Multimodal Pre-trained Language Model, named
MedM-PLM, to learn enhanced EHR representations over structured and
unstructured data. In MedM-PLM, two Transformer-based neural networks
components are firstly adopted to learn representative characteristics from
each modality. A cross-modal module is then introduced to model their
interactions. We pre-trained MedM-PLM on the MIMIC-III dataset and verified the
effectiveness of the model on three downstream clinical tasks, i.e., medication
recommendation, 30-day readmission, and ICD coding. Extensive experiments
demonstrate the power of MedM-PLM compared with state-of-the-art methods.
Further analyses and visualizations show the robustness of our model which
could potentially provide more comprehensive interpretations for clinical
decision-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-biased image classification: evaluation based on semantic representations. (arXiv:2201.11014v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11014">
<div class="article-summary-box-inner">
<span><p>Humans show language-biased image recognition for a word-embedded image,
known as picture-word interference. Such interference depends on hierarchical
semantic categories and reflects that human language processing highly
interacts with visual processing. Similar to humans, recent artificial models
jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased
image classification. Exploring whether the bias leads to interference similar
to those observed in humans can contribute to understanding how much the model
acquires hierarchical semantic representations from joint learning of language
and vision. The present study introduces methodological tools from the
cognitive science literature to assess the biases of artificial models.
Specifically, we introduce a benchmark task to test whether words superimposed
on images can distort the image classification across different category levels
and, if it can, whether the perturbation is due to the shared semantic
representation between language and vision. Our dataset is a set of
word-embedded images and consists of a mixture of natural image datasets and
hierarchical word labels with superordinate/basic category levels. Using this
benchmark test, we evaluate the CLIP model. We show that presenting words
distorts the image classification by the model across different category
levels, but the effect does not depend on the semantic relationship between
images and embedded words. This suggests that the semantic word representation
in the CLIP visual processing is not shared with the image representation,
although the word representation strongly dominates for word-embedded images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Wikipedia Help Offline Reinforcement Learning?. (arXiv:2201.12122v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12122">
<div class="article-summary-box-inner">
<span><p>Fine-tuning reinforcement learning (RL) models has been challenging because
of a lack of large scale off-the-shelf datasets as well as high variance in
transferability among different environments. Recent work has looked at
tackling offline RL from the perspective of sequence modeling with improved
results as result of the introduction of the Transformer architecture. However,
when the model is trained from scratch, it suffers from slow convergence
speeds. In this paper, we look to take advantage of this formulation of
reinforcement learning as sequence modeling and investigate the transferability
of pre-trained sequence models on other domains (vision, language) when
finetuned on offline RL tasks (control, games). To this end, we also propose
techniques to improve transfer between these domains. Results show consistent
performance gains in terms of both convergence speed and reward on a variety of
environments, accelerating training by 3-6x and achieving state-of-the-art
performance in a variety of tasks using Wikipedia-pretrained and GPT2 language
models. We hope that this work not only brings light to the potentials of
leveraging generic sequence modeling techniques and pre-trained models for RL,
but also inspires future work on sharing knowledge between generative modeling
tasks of completely different domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning. (arXiv:2202.00535v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00535">
<div class="article-summary-box-inner">
<span><p>Paraphrase generation is a fundamental and long-standing task in natural
language processing. In this paper, we concentrate on two contributions to the
task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a
parameter-efficient method to adapt large pre-trained language models for
paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a
simple model-agnostic method of using specialized prompt tokens for controlled
paraphrase generation with varying levels of lexical novelty. By conducting
extensive experiments on four datasets, we demonstrate the effectiveness of the
proposed approaches for retaining the semantic content of the original text
while inducing lexical novelty in the generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L3Cube-MahaCorpus and MahaBERT: Marathi Monolingual Corpus, Marathi BERT Language Models, and Resources. (arXiv:2202.01159v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01159">
<div class="article-summary-box-inner">
<span><p>We present L3Cube-MahaCorpus a Marathi monolingual data set scraped from
different internet sources. We expand the existing Marathi monolingual corpus
with 24.8M sentences and 289M tokens. We further present, MahaBERT, MahaAlBERT,
and MahaRoBerta all BERT-based masked language models, and MahaFT, the fast
text word embeddings both trained on full Marathi corpus with 752M tokens. We
show the effectiveness of these resources on downstream Marathi sentiment
analysis, text classification, and named entity recognition (NER) tasks. We
also release MahaGPT, a generative Marathi GPT model trained on Marathi corpus.
Marathi is a popular language in India but still lacks these resources. This
work is a step forward in building open resources for the Marathi language. The
data and models are available at https://github.com/l3cube-pune/MarathiNLP .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASSIST: Towards Label Noise-Robust Dialogue State Tracking. (arXiv:2202.13024v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13024">
<div class="article-summary-box-inner">
<span><p>The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state
tracking (DST). However, substantial noise has been discovered in its state
annotations. Such noise brings about huge challenges for training DST models
robustly. Although several refined versions, including MultiWOZ 2.1-2.4, have
been published recently, there are still lots of noisy labels, especially in
the training set. Besides, it is costly to rectify all the problematic
annotations. In this paper, instead of improving the annotation quality
further, we propose a general framework, named ASSIST (lAbel noiSe-robuSt
dIalogue State Tracking), to train DST models robustly from noisy labels.
ASSIST first generates pseudo labels for each sample in the training set by
using an auxiliary model trained on a small clean dataset, then puts the
generated pseudo labels and vanilla noisy labels together to train the primary
model. We show the validity of ASSIST theoretically. Experimental results also
demonstrate that ASSIST improves the joint goal accuracy of DST by up to
$28.16\%$ on MultiWOZ 2.0 and $8.41\%$ on MultiWOZ 2.4, compared to using only
the vanilla noisy labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QuoteR: A Benchmark of Quote Recommendation for Writing. (arXiv:2202.13145v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13145">
<div class="article-summary-box-inner">
<span><p>It is very common to use quotations (quotes) to make our writings more
elegant or convincing. To help people find appropriate quotes efficiently, the
task of quote recommendation is presented, aiming to recommend quotes that fit
the current context of writing. There have been various quote recommendation
approaches, but they are evaluated on different unpublished datasets. To
facilitate the research on this task, we build a large and fully open quote
recommendation dataset called QuoteR, which comprises three parts including
English, standard Chinese and classical Chinese. Any part of it is larger than
previous unpublished counterparts. We conduct an extensive evaluation of
existing quote recommendation methods on QuoteR. Furthermore, we propose a new
quote recommendation model that significantly outperforms previous methods on
all three parts of QuoteR. All the code and data of this paper are available at
https://github.com/thunlp/QuoteR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OCR Improves Machine Translation for Low-Resource Languages. (arXiv:2202.13274v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13274">
<div class="article-summary-box-inner">
<span><p>We aim to investigate the performance of current OCR systems on low resource
languages and low resource scripts. We introduce and make publicly available a
novel benchmark, OCR4MT, consisting of real and synthetic data, enriched with
noise, for 60 low-resource languages in low resource scripts. We evaluate
state-of-the-art OCR systems on our benchmark and analyse most common errors.
We show that OCR monolingual data is a valuable resource that can increase
performance of Machine Translation models, when used in backtranslation. We
then perform an ablation study to investigate how OCR errors impact Machine
Translation performance and determine what is the minimum level of OCR quality
needed for the monolingual data to be useful for Machine Translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation. (arXiv:2202.13663v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13663">
<div class="article-summary-box-inner">
<span><p>Most dominant neural machine translation (NMT) models are restricted to make
predictions only according to the local context of preceding words in a
left-to-right manner. Although many previous studies try to incorporate global
information into NMT models, there still exist limitations on how to
effectively exploit bidirectional global context. In this paper, we propose a
Confidence Based Bidirectional Global Context Aware (CBBGCA) training framework
for NMT, where the NMT model is jointly trained with an auxiliary conditional
masked language model (CMLM). The training consists of two stages: (1)
multi-task joint training; (2) confidence based knowledge distillation. At the
first stage, by sharing encoder parameters, the NMT model is additionally
supervised by the signal from the CMLM decoder that contains bidirectional
global contexts. Moreover, at the second stage, using the CMLM as teacher, we
further pertinently incorporate bidirectional global context to the NMT model
on its unconfidently-predicted target words via knowledge distillation.
Experimental results show that our proposed CBBGCA training framework
significantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on
three large-scale translation datasets, namely WMT'14 English-to-German, WMT'19
Chinese-to-English and WMT'14 English-to-French, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">py-irt: A Scalable Item Response Theory Library for Python. (arXiv:2203.01282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01282">
<div class="article-summary-box-inner">
<span><p>py-irt is a Python library for fitting Bayesian Item Response Theory (IRT)
models. py-irt estimates latent traits of subjects and items, making it
appropriate for use in IRT tasks as well as ideal-point models. py-irt is built
on top of the Pyro and PyTorch frameworks and uses GPU-accelerated training to
scale to large data sets. Code, documentation, and examples can be found at
https://github.com/nd-ball/py-irt. py-irt can be installed from the GitHub page
or the Python Package Index (PyPI).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doctor Recommendation in Online Health Forums via Expertise Learning. (arXiv:2203.02932v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02932">
<div class="article-summary-box-inner">
<span><p>Huge volumes of patient queries are daily generated on online health forums,
rendering manual doctor allocation a labor-intensive task. To better help
patients, this paper studies a novel task of doctor recommendation to enable
automatic pairing of a patient to a doctor with relevant expertise. While most
prior work in recommendation focuses on modeling target users from their past
behavior, we can only rely on the limited words in a query to infer a patient's
needs for privacy reasons. For doctor modeling, we study the joint effects of
their profiles and previous dialogues with other patients and explore their
interactions via self-learning. The learned doctor embeddings are further
employed to estimate their capabilities of handling a patient query with a
multi-head attention mechanism. For experiments, a large-scale dataset is
collected from Chunyu Yisheng, a Chinese online health forum, where our model
exhibits the state-of-the-art results, outperforming baselines only consider
profiles and past dialogues to characterize a doctor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records. (arXiv:2203.03540v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03540">
<div class="article-summary-box-inner">
<span><p>Objective: To develop a large pretrained clinical language model from scratch
using transformer architecture; systematically examine how transformer models
of different sizes could help 5 clinical natural language processing (NLP)
tasks at different linguistic levels. Methods: We created a large corpus with
&gt;90 billion words from clinical narratives (&gt;82 billion words), scientific
literature (6 billion words), and general English text (2.5 billion words). We
developed GatorTron models from scratch using the BERT architecture of
different sizes including 345 million, 3.9 billion, and 8.9 billion parameters,
compared GatorTron with three existing transformer models in the clinical and
biomedical domain on 5 different clinical NLP tasks including clinical concept
extraction, relation extraction, semantic textual similarity, natural language
inference, and medical question answering, to examine how large transformer
models could help clinical NLP at different linguistic levels. Results and
Conclusion: GatorTron scaled up transformer-based clinical language models to a
size of 8.9 billion parameters and achieved state-of-the-art performance on 5
clinical NLP tasks of different linguistic levels targeting various healthcare
information documented in unstructured electronic health records (EHRs). The
proposed GatorTron models performed remarkably better in much complex clinical
NLP tasks such as natural language inference (9.6% and 7.5% improvements) and
question answering (9.5% and 7.77% improvements) compared with existing smaller
clinical transformer models (i.e., BioBERT and ClinicalBERT), demonstrating the
potential of large transformer-based clinical models for advanced medical
artificial intelligent (AI) applications such as question answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessment of contextualised representations in detecting outcome phrases in clinical trials. (arXiv:2203.03547v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03547">
<div class="article-summary-box-inner">
<span><p>Automating the recognition of outcomes reported in clinical trials using
machine learning has a huge potential of speeding up access to evidence
necessary in healthcare decision-making. Prior research has however
acknowledged inadequate training corpora as a challenge for the Outcome
detection (OD) task. Additionally, several contextualized representations like
BERT and ELMO have achieved unparalleled success in detecting various diseases,
genes, proteins, and chemicals, however, the same cannot be emphatically stated
for outcomes, because these models have been relatively under-tested and
studied for the OD task. We introduce "EBM-COMET", a dataset in which 300
PubMed abstracts are expertly annotated for clinical outcomes. Unlike prior
related datasets that use arbitrary outcome classifications, we use labels from
a taxonomy recently published to standardize outcome classifications. To
extract outcomes, we fine-tune a variety of pre-trained contextualized
representations, additionally, we use frozen contextualized and
context-independent representations in our custom neural model augmented with
clinically informed Part-Of-Speech embeddings and a cost-sensitive loss
function. We adopt strict evaluation for the trained models by rewarding them
for correctly identifying full outcome phrases rather than words within the
entities i.e. given an outcome "systolic blood pressure", the models are
rewarded a classification score only when they predict all 3 words in sequence,
otherwise, they are not rewarded. We observe our best model (BioBERT) achieve
81.5\% F1, 81.3\% sensitivity and 98.0\% specificity. We reach a consensus on
which contextualized representations are best suited for detecting outcomes
from clinical-trial abstracts. Furthermore, our best model outperforms scores
published on the original EBM-NLP dataset leader-board scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Mixing of Contextual Information in the Transformer. (arXiv:2203.04212v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04212">
<div class="article-summary-box-inner">
<span><p>The Transformer architecture aggregates input information through the
self-attention mechanism, but there is no clear understanding of how this
information is mixed across the entire model. Additionally, recent works have
demonstrated that attention weights alone are not enough to describe the flow
of information. In this paper, we consider the whole attention block --
multi-head attention, residual connection, and layer normalization -- and
define a metric to measure token-to-token interactions within each layer,
considering the characteristics of the representation space. Then, we aggregate
layer-wise interpretations to provide input attribution scores for model
predictions. Experimentally, we show that our method, ALTI (Aggregation of
Layer-wise Token-to-token Interactions), provides faithful explanations and
outperforms similar aggregation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Onception: Active Learning with Expert Advice for Real World Machine Translation. (arXiv:2203.04507v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04507">
<div class="article-summary-box-inner">
<span><p>Active learning can play an important role in low-resource settings (i.e.,
where annotated data is scarce), by selecting which instances may be more
worthy to annotate. Most active learning approaches for Machine Translation
assume the existence of a pool of sentences in a source language, and rely on
human annotators to provide translations or post-edits, which can still be
costly. In this article, we assume a real world human-in-the-loop scenario in
which: (i) the source sentences may not be readily available, but instead
arrive in a stream; (ii) the automatic translations receive feedback in the
form of a rating, instead of a correct/edited translation, since the
human-in-the-loop might be a user looking for a translation, but not be able to
provide one. To tackle the challenge of deciding whether each incoming pair
source-translations is worthy to query for human feedback, we resort to a
number of stream-based active learning query strategies. Moreover, since we not
know in advance which query strategy will be the most adequate for a certain
language pair and set of Machine Translation models, we propose to dynamically
combine multiple strategies using prediction with expert advice. Our
experiments show that using active learning allows to converge to the best
Machine Translation systems with fewer human interactions. Furthermore,
combining multiple strategies using prediction with expert advice often
outperforms several individual active learning strategies with even fewer
interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A new approach to calculating BERTScore for automatic assessment of translation quality. (arXiv:2203.05598v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05598">
<div class="article-summary-box-inner">
<span><p>The study of the applicability of the BERTScore metric was conducted to
translation quality assessment at the sentence level for English -&gt; Russian
direction. Experiments were performed with a pre-trained Multilingual BERT as
well as with a pair of Monolingual BERT models. To align monolingual
embeddings, an orthogonal transformation based on anchor tokens was used. It
was demonstrated that such transformation helps to prevent mismatching issue
and shown that this approach gives better results than using embeddings of the
Multilingual model. To improve the token matching process it is proposed to
combine all incomplete WorkPiece tokens into meaningful words and use simple
averaging of corresponding vectors and to calculate BERTScore based on anchor
tokens only. Such modifications allowed us to achieve a better correlation of
the model predictions with human judgments. In addition to evaluating machine
translation, several versions of human translation were evaluated as well, the
problems of this approach were listed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. (arXiv:2203.05797v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05797">
<div class="article-summary-box-inner">
<span><p>Most of the open-domain dialogue models tend to perform poorly in the setting
of long-term human-bot conversations. The possible reason is that they lack the
capability of understanding and memorizing long-term dialogue history
information. To address this issue, we present a novel task of Long-term Memory
Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a
dialogue generation framework with Long-Term Memory (LTM) mechanism (called
PLATO-LTM). This LTM mechanism enables our system to accurately extract and
continuously update long-term persona memory without requiring multiple-session
dialogue datasets for model training. To our knowledge, this is the first
attempt to conduct real-time dynamic management of persona information of both
parties, including the user and the bot. Results on DuLeMon indicate that
PLATO-LTM can significantly outperform baselines in terms of long-term dialogue
consistency, leading to better dialogue engagingness.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised classification of medical ultrasound images based on generative adversarial network. (arXiv:2203.06184v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06184">
<div class="article-summary-box-inner">
<span><p>Medical ultrasound (US) is one of the most widely used imaging modalities in
clinical practice. However, its use presents unique challenges such as variable
imaging quality. Deep learning (DL) can be used as an advanced medical US
images analysis tool, while the performance of the DL model is greatly limited
by the scarcity of big datasets. Here, we develop semi-supervised
classification enhancement (SSCE) structures by constructing seven
convolutional neural network (CNN) models and one of the most state-of-the-art
generative adversarial network (GAN) models, StyleGAN2-ADA, to address this
problem. A breast cancer dataset with 780 images is used as our base dataset.
The results show that our SSCE structures obtain an accuracy of up to 97.9%,
showing a maximum 21.6% improvement compared with utilizing CNN models alone
and outperforming the previous methods using the same dataset by up to 23.9%.
We believe our proposed state-of-the-art method can be regarded as a potential
auxiliary tool for on-the-fly diagnoses of medical US images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging universality of jet taggers through transfer learning. (arXiv:2203.06210v1 [hep-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06210">
<div class="article-summary-box-inner">
<span><p>A significant challenge in the tagging of boosted objects via
machine-learning technology is the prohibitive computational cost associated
with training sophisticated models. Nevertheless, the universality of QCD
suggests that a large amount of the information learnt in the training is
common to different physical signals and experimental setups. In this article,
we explore the use of transfer learning techniques to develop fast and
data-efficient jet taggers that leverage such universality. We consider the
graph neural networks LundNet and ParticleNet, and introduce two prescriptions
to transfer an existing tagger into a new signal based either on fine-tuning
all the weights of a model or alternatively on freezing a fraction of them. In
the case of $W$-boson and top-quark tagging, we find that one can obtain
reliable taggers using an order of magnitude less data with a corresponding
speed-up of the training process. Moreover, while keeping the size of the
training data set fixed, we observe a speed-up of the training by up to a
factor of three. This offers a promising avenue to facilitate the use of such
tools in collider physics experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can I see an Example? Active Learning the Long Tail of Attributes and Relations. (arXiv:2203.06215v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06215">
<div class="article-summary-box-inner">
<span><p>There has been significant progress in creating machine learning models that
identify objects in scenes along with their associated attributes and
relationships; however, there is a large gap between the best models and human
capabilities. One of the major reasons for this gap is the difficulty in
collecting sufficient amounts of annotated relations and attributes for
training these systems. While some attributes and relations are abundant, the
distribution in the natural world and existing datasets is long tailed. In this
paper, we address this problem by introducing a novel incremental active
learning framework that asks for attributes and relations in visual scenes.
While conventional active learning methods ask for labels of specific examples,
we flip this framing to allow agents to ask for examples from specific
categories. Using this framing, we introduce an active sampling method that
asks for examples from the tail of the data distribution and show that it
outperforms classical active learning methods on Visual Genome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Image Segmentation on MRI Images with Missing Modalities: A Review. (arXiv:2203.06217v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06217">
<div class="article-summary-box-inner">
<span><p>Dealing with missing modalities in Magnetic Resonance Imaging (MRI) and
overcoming their negative repercussions is considered a hurdle in biomedical
imaging. The combination of a specified set of modalities, which is selected
depending on the scenario and anatomical part being scanned, will provide
medical practitioners with full information about the region of interest in the
human body, hence the missing MRI sequences should be reimbursed. The
compensation of the adverse impact of losing useful information owing to the
lack of one or more modalities is a well-known challenge in the field of
computer vision, particularly for medical image processing tasks including
tumour segmentation, tissue classification, and image generation. Various
approaches have been developed over time to mitigate this problem's negative
implications and this literature review goes through a significant number of
the networks that seek to do so. The approaches reviewed in this work are
reviewed in detail, including earlier techniques such as synthesis methods as
well as later approaches that deploy deep learning, such as common latent space
models, knowledge distillation networks, mutual information maximization, and
generative adversarial networks (GANs). This work discusses the most important
approaches that have been offered at the time of this writing, examining the
novelty, strength, and weakness of each one. Furthermore, the most commonly
used MRI datasets are highlighted and described. The main goal of this research
is to offer a performance evaluation of missing modality compensating networks,
as well as to outline future strategies for dealing with this issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pressure Ulcer Categorisation using Deep Learning: A Clinical Trial to Evaluate Model Performance. (arXiv:2203.06248v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06248">
<div class="article-summary-box-inner">
<span><p>Pressure ulcers are a challenge for patients and healthcare professionals. In
the UK, 700,000 people are affected by pressure ulcers each year. Treating them
costs the National Health Service {\pounds}3.8 million every day. Their
etiology is complex and multifactorial. However, evidence has shown a strong
link between old age, disease-related sedentary lifestyles and unhealthy eating
habits. Pressure ulcers are caused by direct skin contact with a bed or chair
without frequent position changes. Urinary and faecal incontinence, diabetes,
and injuries that restrict body position and nutrition are also known risk
factors. Guidelines and treatments exist but their implementation and success
vary across different healthcare settings. This is primarily because healthcare
practitioners have a) minimal experience in dealing with pressure ulcers, and
b) a general lack of understanding of pressure ulcer treatments. Poorly
managed, pressure ulcers lead to severe pain, poor quality of life, and
significant healthcare costs. In this paper, we report the findings of a
clinical trial conducted by Mersey Care NHS Foundation Trust that evaluated the
performance of a faster region-based convolutional neural network and mobile
platform that categorised and documented pressure ulcers. The neural network
classifies category I, II, III, and IV pressure ulcers, deep tissue injuries,
and unstageable pressure ulcers. Photographs of pressure ulcers taken by
district nurses are transmitted over 4/5G communications to an inferencing
server for classification. Classified images are stored and reviewed to assess
the model's predictions and relevance as a tool for clinical decision making
and standardised reporting. The results from the study generated a mean average
Precision=0.6796, Recall=0.6997, F1-Score=0.6786 with 45 false positives using
an @.75 confidence score threshold.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perception Over Time: Temporal Dynamics for Robust Image Understanding. (arXiv:2203.06254v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06254">
<div class="article-summary-box-inner">
<span><p>While deep learning surpasses human-level performance in narrow and specific
vision tasks, it is fragile and over-confident in classification. For example,
minor transformations in perspective, illumination, or object deformation in
the image space can result in drastically different labeling, which is
especially transparent via adversarial perturbations. On the other hand, human
visual perception is orders of magnitude more robust to changes in the input
stimulus. But unfortunately, we are far from fully understanding and
integrating the underlying mechanisms that result in such robust perception. In
this work, we introduce a novel method of incorporating temporal dynamics into
static image understanding. We describe a neuro-inspired method that decomposes
a single image into a series of coarse-to-fine images that simulates how
biological vision integrates information over time. Next, we demonstrate how
our novel visual perception framework can utilize this information "over time"
using a biologically plausible algorithm with recurrent units, and as a result,
significantly improving its accuracy and robustness over standard CNNs. We also
compare our proposed approach with state-of-the-art models and explicitly
quantify our adversarial robustness properties through multiple ablation
studies. Our quantitative and qualitative results convincingly demonstrate
exciting and transformative improvements over the standard computer vision and
deep learning architectures used today.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preliminary experiments on automatic gender recognition based on online capital letters. (arXiv:2203.06265v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06265">
<div class="article-summary-box-inner">
<span><p>In this paper we present some experiments to automatically classify online
handwritten text based on capital letters. Although handwritten text is not as
discriminative as face or voice, we still found some chance for gender
classification based on handwritten text. Accuracies are up to 74%, even in the
most challenging case of capital letters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MISF: Multi-level Interactive Siamese Filtering for High-Fidelity Image Inpainting. (arXiv:2203.06304v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06304">
<div class="article-summary-box-inner">
<span><p>Although achieving significant progress, existing deep generative inpainting
methods are far from real-world applications due to the low generalization
across different scenes. As a result, the generated images usually contain
artifacts or the filled pixels differ greatly from the ground truth.
Image-level predictive filtering is a widely used image restoration technique,
predicting suitable kernels adaptively according to different input scenes.
Inspired by this inherent advantage, we explore the possibility of addressing
image inpainting as a filtering task. To this end, we first study the
advantages and challenges of image-level predictive filtering for image
inpainting: the method can preserve local structures and avoid artifacts but
fails to fill large missing areas. Then, we propose semantic filtering by
conducting filtering on the deep feature level, which fills the missing
semantic information but fails to recover the details. To address the issues
while adopting the respective advantages, we propose a novel filtering
technique, i.e., Multilevel Interactive Siamese Filtering (MISF), which
contains two branches: kernel prediction branch (KPB) and semantic &amp; image
filtering branch (SIFB). These two branches are interactively linked: SIFB
provides multi-level features for KPB while KPB predicts dynamic kernels for
SIFB. As a result, the final method takes the advantage of effective semantic &amp;
image-level filling for high-fidelity inpainting. We validate our method on
three challenging datasets, i.e., Dunhuang, Places2, and CelebA. Our method
outperforms state-of-the-art baselines on four metrics, i.e., L1, PSNR, SSIM,
and LPIPS. Please try the released code and model at
https://github.com/tsingqguo/misf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensor Radiomics: Paradigm for Systematic Incorporation of Multi-Flavoured Radiomics Features. (arXiv:2203.06314v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06314">
<div class="article-summary-box-inner">
<span><p>Radiomics features extract quantitative information from medical images,
towards the derivation of biomarkers for clinical tasks, such as diagnosis,
prognosis, or treatment response assessment. Different image discretization
parameters (e.g. bin number or size), convolutional filters, segmentation
perturbation, or multi-modality fusion levels can be used to generate radiomics
features and ultimately signatures. Commonly, only one set of parameters is
used; resulting in only one value or flavour for a given RF. We propose tensor
radiomics (TR) where tensors of features calculated with multiple combinations
of parameters (i.e. flavours) are utilized to optimize the construction of
radiomics signatures. We present examples of TR as applied to PET/CT, MRI, and
CT imaging invoking machine learning or deep learning solutions, and
reproducibility analyses: (1) TR via varying bin sizes on CT images of lung
cancer and PET-CT images of head &amp; neck cancer (HNC) for overall survival
prediction. A hybrid deep neural network, referred to as TR-Net, along with two
ML-based flavour fusion methods showed improved accuracy compared to regular
rediomics features. (2) TR built from different segmentation perturbations and
different bin sizes for classification of late-stage lung cancer response to
first-line immunotherapy using CT images. TR improved predicted patient
responses. (3) TR via multi-flavour generated radiomics features in MR imaging
showed improved reproducibility when compared to many single-flavour features.
(4) TR via multiple PET/CT fusions in HNC. Flavours were built from different
fusions using methods, such as Laplacian pyramids and wavelet transforms. TR
improved overall survival prediction. Our results suggest that the proposed TR
paradigm has the potential to improve performance capabilities in different
medical imaging tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformable VisTR: Spatio temporal deformable attention for video instance segmentation. (arXiv:2203.06318v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06318">
<div class="article-summary-box-inner">
<span><p>Video instance segmentation (VIS) task requires classifying, segmenting, and
tracking object instances over all frames in a video clip. Recently, VisTR has
been proposed as end-to-end transformer-based VIS framework, while
demonstrating state-of-the-art performance. However, VisTR is slow to converge
during training, requiring around 1000 GPU hours due to the high computational
cost of its transformer attention module. To improve the training efficiency,
we propose Deformable VisTR, leveraging spatio-temporal deformable attention
module that only attends to a small fixed set of key spatio-temporal sampling
points around a reference point. This enables Deformable VisTR to achieve
linear computation in the size of spatio-temporal feature maps. Moreover, it
can achieve on par performance as the original VisTR with 10$\times$ less GPU
training hours. We validate the effectiveness of our method on the Youtube-VIS
benchmark. Code is available at https://github.com/skrya/DefVIS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PillarGrid: Deep Learning-based Cooperative Perception for 3D Object Detection from Onboard-Roadside LiDAR. (arXiv:2203.06319v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06319">
<div class="article-summary-box-inner">
<span><p>3D object detection plays a fundamental role in enabling autonomous driving,
which is regarded as the significant key to unlocking the bottleneck of
contemporary transportation systems from the perspectives of safety, mobility,
and sustainability. Most of the state-of-the-art (SOTA) object detection
methods from point clouds are developed based on a single onboard LiDAR, whose
performance will be inevitably limited by the range and occlusion, especially
in dense traffic scenarios. In this paper, we propose \textit{PillarGrid}, a
novel cooperative perception method fusing information from multiple 3D LiDARs
(both on-board and roadside), to enhance the situation awareness for connected
and automated vehicles (CAVs). PillarGrid consists of four main phases: 1)
cooperative preprocessing of point clouds, 2) pillar-wise voxelization and
feature extraction, 3) grid-wise deep fusion of features from multiple sensors,
and 4) convolutional neural network (CNN)-based augmented 3D object detection.
A novel cooperative perception platform is developed for model training and
testing. Extensive experimentation shows that PillarGrid outperforms the SOTA
single-LiDAR-based 3D object detection methods with respect to both accuracy
and range by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wavelet Knowledge Distillation: Towards Efficient Image-to-Image Translation. (arXiv:2203.06321v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06321">
<div class="article-summary-box-inner">
<span><p>Remarkable achievements have been attained with Generative Adversarial
Networks (GANs) in image-to-image translation. However, due to a tremendous
amount of parameters, state-of-the-art GANs usually suffer from low efficiency
and bulky memory usage. To tackle this challenge, firstly, this paper
investigates GANs performance from a frequency perspective. The results show
that GANs, especially small GANs lack the ability to generate high-quality high
frequency information. To address this problem, we propose a novel knowledge
distillation method referred to as wavelet knowledge distillation. Instead of
directly distilling the generated images of teachers, wavelet knowledge
distillation first decomposes the images into different frequency bands with
discrete wavelet transformation and then only distills the high frequency
bands. As a result, the student GAN can pay more attention to its learning on
high frequency bands. Experiments demonstrate that our method leads to 7.08
times compression and 6.80 times acceleration on CycleGAN with almost no
performance drop. Additionally, we have studied the relation between
discriminators and generators which shows that the compression of
discriminators can promote the performance of compressed generators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Style Transfer: from Artistic to Photorealistic. (arXiv:2203.06328v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06328">
<div class="article-summary-box-inner">
<span><p>The rapid advancement of deep learning has significantly boomed the
development of photorealistic style transfer. In this review, we reviewed the
development of photorealistic style transfer starting from artistic style
transfer and the contribution of traditional image processing techniques on
photorealistic style transfer, including some work that had been completed in
the Multimedia lab at the University of Alberta. Many techniques were discussed
in this review. However, our focus is on VGG-based techniques, whitening and
coloring transform (WCTs) based techniques, the combination of deep learning
with traditional image processing techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-FedRL: Federated Hyperparameter Optimization for Multi-institutional Medical Image Segmentation. (arXiv:2203.06338v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06338">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) is a distributed machine learning technique that
enables collaborative model training while avoiding explicit data sharing. The
inherent privacy-preserving property of FL algorithms makes them especially
attractive to the medical field. However, in case of heterogeneous client data
distributions, standard FL methods are unstable and require intensive
hyperparameter tuning to achieve optimal performance. Conventional
hyperparameter optimization algorithms are impractical in real-world FL
applications as they involve numerous training trials, which are often not
affordable with limited compute budgets. In this work, we propose an efficient
reinforcement learning~(RL)-based federated hyperparameter optimization
algorithm, termed Auto-FedRL, in which an online RL agent can dynamically
adjust hyperparameters of each client based on the current training progress.
Extensive experiments are conducted to investigate different search strategies
and RL agents. The effectiveness of the proposed method is validated on a
heterogeneous data split of the CIFAR-10 dataset as well as two real-world
medical image segmentation datasets for COVID-19 lesion segmentation in chest
CT and pancreas segmentation in abdominal CT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy. (arXiv:2203.06345v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06345">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have gained increasing popularity as they are
commonly believed to own higher modeling capacity and representation
flexibility, than traditional convolutional networks. However, it is
questionable whether such potential has been fully unleashed in practice, as
the learned ViTs often suffer from over-smoothening, yielding likely redundant
models. Recent works made preliminary attempts to identify and alleviate such
redundancy, e.g., via regularizing embedding similarity or re-injecting
convolution-like structures. However, a "head-to-toe assessment" regarding the
extent of redundancy in ViTs, and how much we could gain by thoroughly
mitigating such, has been absent for this field. This paper, for the first
time, systematically studies the ubiquitous existence of redundancy at all
three levels: patch embedding, attention map, and weight space. In view of
them, we advocate a principle of diversity for training ViTs, by presenting
corresponding regularizers that encourage the representation diversity and
coverage at each of those levels, that enabling capturing more discriminative
information. Extensive experiments on ImageNet with a number of ViT backbones
validate the effectiveness of our proposals, largely eliminating the observed
ViT redundancy and significantly boosting the model generalization. For
example, our diversified DeiT obtains 0.70%~1.76% accuracy boosts on ImageNet
with highly reduced similarity. Our codes are fully available in
https://github.com/VITA-Group/Diverse-ViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LesionPaste: One-Shot Anomaly Detection for Medical Images. (arXiv:2203.06354v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06354">
<div class="article-summary-box-inner">
<span><p>Due to the high cost of manually annotating medical images, especially for
large-scale datasets, anomaly detection has been explored through training
models with only normal data. Lacking prior knowledge of true anomalies is the
main reason for the limited application of previous anomaly detection methods,
especially in the medical image analysis realm. In this work, we propose a
one-shot anomaly detection framework, namely LesionPaste, that utilizes true
anomalies from a single annotated sample and synthesizes artificial anomalous
samples for anomaly detection. First, a lesion bank is constructed by applying
augmentation to randomly selected lesion patches. Then, MixUp is adopted to
paste patches from the lesion bank at random positions in normal images to
synthesize anomalous samples for training. Finally, a classification network is
trained using the synthetic abnormal samples and the true normal data.
Extensive experiments are conducted on two publicly-available medical image
datasets with different types of abnormalities. On both datasets, our proposed
LesionPaste largely outperforms several state-of-the-art unsupervised and
semi-supervised anomaly detection methods, and is on a par with the
fully-supervised counterpart. To note, LesionPaste is even better than the
fully-supervised method in detecting early-stage diabetic retinopathy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EventFormer: AU Event Transformer for Facial Action Unit Event Detection. (arXiv:2203.06355v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06355">
<div class="article-summary-box-inner">
<span><p>Facial action units (AUs) play an indispensable role in human emotion
analysis. We observe that although AU-based high-level emotion analysis is
urgently needed by real-world applications, frame-level AU results provided by
previous works cannot be directly used for such analysis. Moreover, as AUs are
dynamic processes, the utilization of global temporal information is important
but has been gravely ignored in the literature. To this end, we propose
EventFormer for AU event detection, which is the first work directly detecting
AU events from a video sequence by viewing AU event detection as a multiple
class-specific sets prediction problem. Extensive experiments conducted on a
commonly used AU benchmark dataset, BP4D, show the superiority of EventFormer
under suitable metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taking an Emotional Look at Video Paragraph Captioning. (arXiv:2203.06356v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06356">
<div class="article-summary-box-inner">
<span><p>Translating visual data into natural language is essential for machines to
understand the world and interact with humans. In this work, a comprehensive
study is conducted on video paragraph captioning, with the goal to generate
paragraph-level descriptions for a given video. However, current researches
mainly focus on detecting objective facts, ignoring the needs to establish the
logical associations between sentences and to discover more accurate emotions
related to video contents. Such a problem impairs fluent and abundant
expressions of predicted captions, which are far below human language tandards.
To solve this problem, we propose to construct a large-scale emotion and logic
driven multilingual dataset for this task. This dataset is named EMVPC
(standing for "Emotional Video Paragraph Captioning") and contains 53
widely-used emotions in daily life, 376 common scenes corresponding to these
emotions, 10,291 high-quality videos and 20,582 elaborated paragraph captions
with English and Chinese versions. Relevant emotion categories, scene labels,
emotion word labels and logic word labels are also provided in this new
dataset. The proposed EMVPC dataset intends to provide full-fledged video
paragraph captioning in terms of rich emotions, coherent logic and elaborate
expressions, which can also benefit other tasks in vision-language fields.
Furthermore, a comprehensive study is conducted through experiments on existing
benchmark video paragraph captioning datasets and the proposed EMVPC. The
stateof-the-art schemes from different visual captioning tasks are compared in
terms of 15 popular metrics, and their detailed objective as well as subjective
results are summarized. Finally, remaining problems and future directions of
video paragraph captioning are also discussed. The unique perspective of this
work is expected to boost further development in video paragraph captioning
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning. (arXiv:2203.06359v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06359">
<div class="article-summary-box-inner">
<span><p>Non-exemplar class-incremental learning is to recognize both the old and new
classes when old class samples cannot be saved. It is a challenging task since
representation optimization and feature retention can only be achieved under
supervision from new classes. To address this problem, we propose a novel
self-sustaining representation expansion scheme. Our scheme consists of a
structure reorganization strategy that fuses main-branch expansion and
side-branch updating to maintain the old features, and a main-branch
distillation scheme to transfer the invariant knowledge. Furthermore, a
prototype selection mechanism is proposed to enhance the discrimination between
the old and new classes by selectively incorporating new samples into the
distillation process. Extensive experiments on three benchmarks demonstrate
significant incremental performance, outperforming the state-of-the-art methods
by a margin of 3%, 3% and 6%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDT-Net: Multi-domain Transfer by Perceptual Supervision for Unpaired Images in OCT Scan. (arXiv:2203.06363v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06363">
<div class="article-summary-box-inner">
<span><p>Deep learning models tend to underperform in the presence of domain shifts.
Domain transfer has recently emerged as a promising approach wherein images
exhibiting a domain shift are transformed into other domains for augmentation
or adaptation. However, with the absence of paired and annotated images, most
domain transfer methods mainly rely on adversarial networks and weak cycle
consistency, which could result in incomplete domain transfer or poor adherence
to the original image content. In this paper, we introduce MDT-Net to address
the limitations above through a multi-domain transfer model based on perceptual
supervision. Specifically, our model consists of an encoder-decoder network,
which aims to preserve anatomical structures, and multiple domain-specific
transfer modules, which guide the domain transition through feature
transformation. During the inference, MDT-Net can directly transfer images from
the source domain to multiple target domains at one time without any reference
image. To demonstrate the performance of MDT-Net, we evaluate it on RETOUCH
dataset, comprising OCT scans from three different scanner devices (domains),
for multi-domain transfer. We also take the transformed results as additional
training images for fluid segmentation in OCT scans in the tasks of domain
adaptation and data augmentation. Experimental results show that MDT-Net can
outperform other domain transfer models qualitatively and quantitatively.
Furthermore, the significant improvement in dice scores over multiple
segmentation models also demonstrates the effectiveness and efficiency of our
proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiated Relevances Embedding for Group-based Referring Expression Comprehension. (arXiv:2203.06382v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06382">
<div class="article-summary-box-inner">
<span><p>Referring expression comprehension (REC) aims to locate a certain object in
an image referred by a natural language expression. For joint understanding of
regions and expressions, existing REC works typically target on modeling the
cross-modal relevance in each region-expression pair within each single image.
In this paper, we explore a new but general REC-related problem, named
Group-based REC, where the regions and expressions can come from different
subject-related images (images in the same group), e.g., sets of photo albums
or video frames. Different from REC, Group-based REC involves differentiated
cross-modal relevances within each group and across different groups, which,
however, are neglected in the existing one-line paradigm. To this end, we
propose a novel relevance-guided multi-group self-paced learning schema (termed
RMSL), where the within-group region-expression pairs are adaptively assigned
with different priorities according to their cross-modal relevances, and the
bias of the group priority is balanced via an across-group relevance constraint
simultaneously. In particular, based on the visual and textual semantic
features, RMSL conducts an adaptive learning cycle upon triplet ranking, where
(1) the target-negative region-expression pairs with low within-group
relevances are used preferentially in model training to distinguish the primary
semantics of the target objects, and (2) an across-group relevance
regularization is integrated into model training to balance the bias of group
priority. The relevances, the pairs, and the model parameters are alternatively
updated upon a unified self-paced hinge loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation. (arXiv:2203.06386v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06386">
<div class="article-summary-box-inner">
<span><p>The recent large-scale vision-language pre-training (VLP) of dual-stream
architectures (e.g., CLIP) with a tremendous amount of image-text pair data,
has shown its superiority on various multimodal alignment tasks. Despite its
success, the resulting models are not capable of multimodal generative tasks
due to the weak text encoder. To tackle this problem, we propose to augment the
dual-stream VLP model with a textual pre-trained language model (PLM) via
vision-language knowledge distillation (VLKD), enabling the capability for
multimodal generation. VLKD is pretty data- and computation-efficient compared
to the pre-training from scratch. Experimental results show that the resulting
model has strong zero-shot performance on multimodal generation tasks, such as
open-ended visual question answering and image captioning. For example, it
achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous
state-of-the-art zero-shot model with $7\times$ fewer parameters. Furthermore,
the original textual language understanding and generation ability of the PLM
is maintained after VLKD, which makes our model versatile for both multimodal
and unimodal tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint CNN and Transformer Network via weakly supervised Learning for efficient crowd counting. (arXiv:2203.06388v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06388">
<div class="article-summary-box-inner">
<span><p>Currently, for crowd counting, the fully supervised methods via density map
estimation are the mainstream research directions. However, such methods need
location-level annotation of persons in an image, which is time-consuming and
laborious. Therefore, the weakly supervised method just relying upon the
count-level annotation is urgently needed. Since CNN is not suitable for
modeling the global context and the interactions between image patches, crowd
counting with weakly supervised learning via CNN generally can not show good
performance. The weakly supervised model via Transformer was sequentially
proposed to model the global context and learn contrast features. However, the
transformer directly partitions the crowd images into a series of tokens, which
may not be a good choice due to each pedestrian being an independent
individual, and the parameter number of the network is very large. Hence, we
propose a Joint CNN and Transformer Network (JCTNet) via weakly supervised
learning for crowd counting in this paper. JCTNet consists of three parts: CNN
feature extraction module (CFM), Transformer feature extraction module (TFM),
and counting regression module (CRM). In particular, the CFM extracts crowd
semantic information features, then sends their patch partitions to TRM for
modeling global context, and CRM is used to predict the number of people.
Extensive experiments and visualizations demonstrate that JCTNet can
effectively focus on the crowd regions and obtain superior weakly supervised
counting performance on five mainstream datasets. The number of parameters of
the model can be reduced by about 67%~73% compared with the pure Transformer
works. We also tried to explain the phenomenon that a model constrained only by
count-level annotations can still focus on the crowd regions. We believe our
work can promote further research in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection. (arXiv:2203.06398v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06398">
<div class="article-summary-box-inner">
<span><p>Domain Adaptive Object Detection (DAOD) leverages a labeled domain to learn
an object detector generalizing to a novel domain free of annotations. Recent
advances align class-conditional distributions by narrowing down cross-domain
prototypes (class centers). Though great success,they ignore the significant
within-class variance and the domain-mismatched semantics within the training
batch, leading to a sub-optimal adaptation. To overcome these challenges, we
propose a novel SemantIc-complete Graph MAtching (SIGMA) framework for DAOD,
which completes mismatched semantics and reformulates the adaptation with graph
matching. Specifically, we design a Graph-embedded Semantic Completion module
(GSC) that completes mismatched semantics through generating hallucination
graph nodes in missing categories. Then, we establish cross-image graphs to
model class-conditional distributions and learn a graph-guided memory bank for
better semantic completion in turn. After representing the source and target
data as graphs, we reformulate the adaptation as a graph matching problem,
i.e., finding well-matched node pairs across graphs to reduce the domain gap,
which is solved with a novel Bipartite Graph Matching adaptor (BGM). In a
nutshell, we utilize graph nodes to establish semantic-aware node affinity and
leverage graph edges as quadratic constraints in a structure-aware matching
loss, achieving fine-grained adaptation with a node-to-node graph matching.
Extensive experiments verify that SIGMA outperforms existing works
significantly. Our codes are available at
https://github.com/CityU-AIM-Group/SIGMA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kernel Proposal Network for Arbitrary Shape Text Detection. (arXiv:2203.06410v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06410">
<div class="article-summary-box-inner">
<span><p>Segmentation-based methods have achieved great success for arbitrary shape
text detection. However, separating neighboring text instances is still one of
the most challenging problems due to the complexity of texts in scene images.
In this paper, we propose an innovative Kernel Proposal Network (dubbed KPN)
for arbitrary shape text detection. The proposed KPN can separate neighboring
text instances by classifying different texts into instance-independent feature
maps, meanwhile avoiding the complex aggregation process existing in
segmentation-based arbitrary shape text detection methods. To be concrete, our
KPN will predict a Gaussian center map for each text image, which will be used
to extract a series of candidate kernel proposals (i.e., dynamic convolution
kernel) from the embedding feature maps according to their corresponding
keypoint positions. To enforce the independence between kernel proposals, we
propose a novel orthogonal learning loss (OLL) via orthogonal constraints.
Specifically, our kernel proposals contain important self-information learned
by network and location information by position embedding. Finally, kernel
proposals will individually convolve all embedding feature maps for generating
individual embedded maps of text instances. In this way, our KPN can
effectively separate neighboring text instances and improve the robustness
against unclear boundaries. To our knowledge, our work is the first to
introduce the dynamic convolution kernel strategy to efficiently and
effectively tackle the adhesion problem of neighboring text instances in text
detection. Experimental results on challenging datasets verify the impressive
performance and efficiency of our method. The code and model are available at
https://github.com/GXYM/KPN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrence-in-Recurrence Networks for Video Deblurring. (arXiv:2203.06418v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06418">
<div class="article-summary-box-inner">
<span><p>State-of-the-art video deblurring methods often adopt recurrent neural
networks to model the temporal dependency between the frames. While the hidden
states play key role in delivering information to the next frame, abrupt motion
blur tend to weaken the relevance in the neighbor frames. In this paper, we
propose recurrence-in-recurrence network architecture to cope with the
limitations of short-ranged memory. We employ additional recurrent units inside
the RNN cell. First, we employ inner-recurrence module (IRM) to manage the
long-ranged dependency in a sequence. IRM learns to keep track of the cell
memory and provides complementary information to find the deblurred frames.
Second, we adopt an attention-based temporal blending strategy to extract the
necessary part of the information in the local neighborhood. The adpative
temporal blending (ATB) can either attenuate or amplify the features by the
spatial attention. Our extensive experimental results and analysis validate the
effectiveness of IRM and ATB on various RNN architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-stage Video Instance Segmentation: From Frame-in Frame-out to Clip-in Clip-out. (arXiv:2203.06421v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06421">
<div class="article-summary-box-inner">
<span><p>Many video instance segmentation (VIS) methods partition a video sequence
into individual frames to detect and segment objects frame by frame. However,
such a frame-in frame-out (FiFo) pipeline is ineffective to exploit the
temporal information. Based on the fact that adjacent frames in a short clip
are highly coherent in content, we propose to extend the one-stage FiFo
framework to a clip-in clip-out (CiCo) one, which performs VIS clip by clip.
Specifically, we stack FPN features of all frames in a short video clip to
build a spatio-temporal feature cube, and replace the 2D conv layers in the
prediction heads and the mask branch with 3D conv layers, forming clip-level
prediction heads (CPH) and clip-level mask heads (CMH). Then the clip-level
masks of an instance can be generated by feeding its box-level predictions from
CPH and clip-level features from CMH into a small fully convolutional network.
A clip-level segmentation loss is proposed to ensure that the generated
instance masks are temporally coherent in the clip. The proposed CiCo strategy
is free of inter-frame alignment, and can be easily embedded into existing FiFo
based VIS approaches. To validate the generality and effectiveness of our CiCo
strategy, we apply it to two representative FiFo methods, Yolact
\cite{bolya2019yolact} and CondInst \cite{tian2020conditional}, resulting in
two new one-stage VIS models, namely CiCo-Yolact and CiCo-CondInst, which
achieve 37.1/37.3\%, 35.2/35.4\% and 17.2/18.0\% mask AP using the ResNet50
backbone, and 41.8/41.4\%, 38.0/38.9\% and 18.0/18.2\% mask AP using the Swin
Transformer tiny backbone on YouTube-VIS 2019, 2021 and OVIS valid sets,
respectively, recording new state-of-the-arts. Code and video demos of CiCo can
be found at \url{https://github.com/MinghanLi/CiCo}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VariabilityTrack:Multi-Object Tracking with Variable Speed Object Movement. (arXiv:2203.06424v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06424">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking (MOT) aims at estimating bounding boxes and identities
of objects in videos. Most methods can be roughly classified as
tracking-by-detection and joint-detection-association paradigms. Although the
latter has elicited more attention and demonstrates comparable performance
relative than the former, we claim that the tracking-by-detection paradigm is
still the optimal solution in terms of tracking accuracy,such as
ByteTrack,which achieves 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of
MOT17 with 30 FPS running speed on a single V100 GPU.However, under complex
perspectives such as vehicle and UAV acceleration, the performance of such a
tracker using uniform Kalman filter will be greatly affected, resulting in
tracking loss.In this paper, we propose a variable speed Kalman filter
algorithm based on environmental feedback and improve the matching process,
which can greatly improve the tracking effect in complex variable speed scenes
while maintaining high tracking accuracy in relatively static scenes.
Eventually, higher MOTA and IDF1 results can be achieved on MOT17 test set than
ByteTrack
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VAFO-Loss: VAscular Feature Optimised Loss Function for Retinal Artery/Vein Segmentation. (arXiv:2203.06425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06425">
<div class="article-summary-box-inner">
<span><p>Estimating clinically-relevant vascular features following vessel
segmentation is a standard pipeline for retinal vessel analysis, which provides
potential ocular biomarkers for both ophthalmic disease and systemic disease.
In this work, we integrate these clinical features into a novel vascular
feature optimised loss function (VAFO-Loss), in order to regularise networks to
produce segmentation maps, with which more accurate vascular features can be
derived. Two common vascular features, vessel density and fractal dimension,
are identified to be sensitive to intra-segment misclassification, which is a
well-recognised problem in multi-class artery/vein segmentation particularly
hindering the estimation of these vascular features. Thus we encode these two
features into VAFO-Loss. We first show that incorporating our end-to-end
VAFO-Loss in standard segmentation networks indeed improves vascular feature
estimation, yielding quantitative improvement in stroke incidence prediction, a
clinical downstream task. We also report a technically interesting finding that
the trained segmentation network, albeit biased by the feature optimised loss
VAFO-Loss, shows statistically significant improvement in segmentation metrics,
compared to those trained with other state-of-the-art segmentation losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DFTR: Depth-supervised Hierarchical Feature Fusion Transformer for Salient Object Detection. (arXiv:2203.06429v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06429">
<div class="article-summary-box-inner">
<span><p>Automated salient object detection (SOD) plays an increasingly crucial role
in many computer vision applications. Although existing frameworks achieve
impressive SOD performances especially with the development of deep learning
techniques, their performances still have room for improvement. In this work,
we propose a novel pure Transformer-based SOD framework, namely
Depth-supervised hierarchical feature Fusion TRansformer (DFTR), to further
improve the accuracy of both RGB and RGB-D SOD. The proposed DFTR involves
three primary improvements: 1) The backbone of feature encoder is switched from
a convolutional neural network to a Swin Transformer for more effective feature
extraction; 2) We propose a multi-scale feature aggregation (MFA) module to
fully exploit the multi-scale features encoded by the Swin Transformer in a
coarse-to-fine manner; 3) Following recent studies, we formulate an auxiliary
task of depth map prediction and use the ground-truth depth maps as extra
supervision signals for network learning. To enable bidirectional information
flow between saliency and depth branches, a novel multi-task feature fusion
(MFF) module is integrated into our DFTR. We extensively evaluate the proposed
DFTR on ten benchmarking datasets. Experimental results show that our DFTR
consistently outperforms the existing state-of-the-art methods for both RGB and
RGB-D SOD tasks. The code and model will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning-based conditional inpainting for restoration of artifact-affected 4D CT images. (arXiv:2203.06431v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06431">
<div class="article-summary-box-inner">
<span><p>4D CT imaging is an essential component of radiotherapy of thoracic/abdominal
tumors. 4D CT images are, however, often affected by artifacts that compromise
treatment planning quality. In this work, deep learning (DL)-based conditional
inpainting is proposed to restore anatomically correct image information of
artifact-affected areas. The restoration approach consists of a two-stage
process: DL-based detection of common interpolation (INT) and double structure
(DS) artifacts, followed by conditional inpainting applied to the artifact
areas. In this context, conditional refers to a guidance of the inpainting
process by patient-specific image data to ensure anatomically reliable results.
Evaluation is based on 65 in-house 4D CT data sets of lung cancer patients (48
with only slight artifacts, 17 with pronounced artifacts) and the publicly
available DIRLab 4D CT data (independent external test set). Automated artifact
detection revealed a ROC-AUC of 0.99 for INT and 0.97 for DS artifacts
(in-house data). The proposed inpainting method decreased the average root mean
squared error (RMSE) by 60% (DS) and 42% (INT) for the in-house evaluation data
(simulated artifacts for the slight artifact data; original data were
considered as ground truth for RMSE computation). For the external DIR-Lab
data, the RMSE decreased by 65% and 36%, respectively. Applied to the
pronounced artifact data group, on average 68% of the detectable artifacts were
removed. The results highlight the potential of DL-based inpainting for the
restoration of artifact-affected 4D CT data. Improved performance of
conditional inpainting (compared to standard inpainting) illustrates the
benefits of exploiting patient-specific prior knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DATR: Domain-adaptive transformer for multi-domain landmark detection. (arXiv:2203.06433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06433">
<div class="article-summary-box-inner">
<span><p>Accurate anatomical landmark detection plays an increasingly vital role in
medical image analysis. Although existing methods achieve satisfying
performance, they are mostly based on CNN and specialized for a single domain
say associated with a particular anatomical region. In this work, we propose a
universal model for multi-domain landmark detection by taking advantage of
transformer for modeling long dependencies and develop a domain-adaptive
transformer model, named as DATR, which is trained on multiple mixed datasets
from different anatomies and capable of detecting landmarks of any image from
those anatomies. The proposed DATR exhibits three primary features: (i) It is
the first universal model which introduces transformer as an encoder for
multi-anatomy landmark detection; (ii) We design a domain-adaptive transformer
for anatomy-aware landmark detection, which can be effectively extended to any
other transformer network; (iii) Following previous studies, we employ a
light-weighted guidance network, which encourages the transformer network to
detect more accurate landmarks. We carry out experiments on three widely used
X-ray datasets for landmark detection, which have 1,588 images and 62 landmarks
in total, including three different anatomies (head, hand, and chest).
Experimental results demonstrate that our proposed DATR achieves
state-of-the-art performances by most metrics and behaves much better than any
previous convolution-based models. The code will be released publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bringing Rolling Shutter Images Alive with Dual Reversed Distortion. (arXiv:2203.06451v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06451">
<div class="article-summary-box-inner">
<span><p>Rolling shutter (RS) distortion can be interpreted as the result of picking a
row of pixels from instant global shutter (GS) frames over time during the
exposure of the RS camera. This means that the information of each instant GS
frame is partially, yet sequentially, embedded into the row-dependent
distortion. Inspired by this fact, we address the challenging task of reversing
this process, i.e., extracting undistorted GS frames from images suffering from
RS distortion. However, since RS distortion is coupled with other factors such
as readout settings and the relative velocity of scene elements to the camera,
models that only exploit the geometric correlation between temporally adjacent
images suffer from poor generality in processing data with different readout
settings and dynamic scenes with both camera motion and object motion. In this
paper, instead of two consecutive frames, we propose to exploit a pair of
images captured by dual RS cameras with reversed RS directions for this highly
challenging task. Grounded on the symmetric and complementary nature of dual
reversed distortion, we develop a novel end-to-end model, IFED, to generate
dual optical flow sequence through iterative learning of the velocity field
during the RS time. Extensive experimental results demonstrate that IFED is
superior to naive cascade schemes, as well as the state-of-the-art which
utilizes adjacent RS images. Most importantly, although it is trained on a
synthetic dataset, IFED is shown to be effective at retrieving GS frame
sequences from real-world RS distorted images of dynamic scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-GIF: 3D-Controllable Object Generation via Implicit Factorized Representations. (arXiv:2203.06457v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06457">
<div class="article-summary-box-inner">
<span><p>While NeRF-based 3D-aware image generation methods enable viewpoint control,
limitations still remain to be adopted to various 3D applications. Due to their
view-dependent and light-entangled volume representation, the 3D geometry
presents unrealistic quality and the color should be re-rendered for every
desired viewpoint. To broaden the 3D applicability from 3D-aware image
generation to 3D-controllable object generation, we propose the factorized
representations which are view-independent and light-disentangled, and training
schemes with randomly sampled light conditions. We demonstrate the superiority
of our method by visualizing factorized representations, re-lighted images, and
albedo-textured meshes. In addition, we show that our approach improves the
quality of the generated geometry via visualization and quantitative
comparison. To the best of our knowledge, this is the first work that extracts
albedo-textured meshes with unposed 2D images without any additional labels or
assumptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factored Attention and Embedding for Unstructured-view Topic-related Ultrasound Report Generation. (arXiv:2203.06458v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06458">
<div class="article-summary-box-inner">
<span><p>Echocardiography is widely used to clinical practice for diagnosis and
treatment, e.g., on the common congenital heart defects. The traditional manual
manipulation is error-prone due to the staff shortage, excess workload, and
less experience, leading to the urgent requirement of an automated
computer-aided reporting system to lighten the workload of ultrasonologists
considerably and assist them in decision making. Despite some recent successful
attempts in automatical medical report generation, they are trapped in the
ultrasound report generation, which involves unstructured-view images and
topic-related descriptions. To this end, we investigate the task of the
unstructured-view topic-related ultrasound report generation, and propose a
novel factored attention and embedding model (termed FAE-Gen). The proposed
FAE-Gen mainly consists of two modules, i.e., view-guided factored attention
and topic-oriented factored embedding, which 1) capture the homogeneous and
heterogeneous morphological characteristic across different views, and 2)
generate the descriptions with different syntactic patterns and different
emphatic contents for different topics. Experimental evaluations are conducted
on a to-be-released large-scale clinical cardiovascular ultrasound dataset
(CardUltData). Both quantitative comparisons and qualitative analysis
demonstrate the effectiveness and the superiority of FAE-Gen over seven
commonly-used metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Review on Computer Vision-Based Parking Lot Management Applied on Public Datasets. (arXiv:2203.06463v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06463">
<div class="article-summary-box-inner">
<span><p>Computer vision-based parking lot management methods have been extensively
researched upon owing to their flexibility and cost-effectiveness. To evaluate
such methods authors often employ publicly available parking lot image
datasets. In this study, we surveyed and compared robust publicly available
image datasets specifically crafted to test computer vision-based methods for
parking lot management approaches and consequently present a systematic and
comprehensive review of existing works that employ such datasets. The
literature review identified relevant gaps that require further research, such
as the requirement of dataset-independent approaches and methods suitable for
autonomous detection of position of parking spaces. In addition, we have
noticed that several important factors such as the presence of the same cars
across consecutive images, have been neglected in most studies, thereby
rendering unrealistic assessment protocols. Furthermore, the analysis of the
datasets also revealed that certain features that should be present when
developing new benchmarks, such as the availability of video sequences and
images taken in more diverse conditions, including nighttime and snow, have not
been incorporated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Lifelong Person Re-identification via Contrastive Rehearsal. (arXiv:2203.06468v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06468">
<div class="article-summary-box-inner">
<span><p>Existing unsupervised person re-identification (ReID) methods focus on
adapting a model trained on a source domain to a fixed target domain. However,
an adapted ReID model usually only works well on a certain target domain, but
can hardly memorize the source domain knowledge and generalize to upcoming
unseen data. In this paper, we propose unsupervised lifelong person ReID, which
focuses on continuously conducting unsupervised domain adaptation on new
domains without forgetting the knowledge learnt from old domains. To tackle
unsupervised lifelong ReID, we conduct a contrastive rehearsal on a small
number of stored old samples while sequentially adapting to new domains. We
further set an image-to-image similarity constraint between old and new models
to regularize the model updates in a way that suits old knowledge. We
sequentially train our model on several large-scale datasets in an unsupervised
manner and test it on all seen domains as well as several unseen domains to
validate the generalizability of our method. Our proposed unsupervised lifelong
method achieves strong generalizability, which significantly outperforms
previous lifelong methods on both seen and unseen domains. Code will be made
available at https://github.com/chenhao2345/UCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?. (arXiv:2203.06487v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06487">
<div class="article-summary-box-inner">
<span><p>Being able to explain the prediction to clinical end-users is a necessity to
leverage the power of artificial intelligence (AI) models for clinical decision
support. For medical images, a feature attribution map, or heatmap, is the most
common form of explanation that highlights important features for AI models'
prediction. However, it is unknown how well heatmaps perform on explaining
decisions on multi-modal medical images, where each image modality or channel
visualizes distinct clinical information of the same underlying biomedical
phenomenon. Understanding such modality-dependent features is essential for
clinical users' interpretation of AI decisions. To tackle this clinically
important but technically ignored problem, we propose the modality-specific
feature importance (MSFI) metric. It encodes clinical image and explanation
interpretation patterns of modality prioritization and modality-specific
feature localization. We conduct a clinical requirement-grounded, systematic
evaluation using computational methods and a clinician user study. Results show
that the examined 16 heatmap algorithms failed to fulfill clinical requirements
to correctly indicate AI model decision process or decision quality. The
evaluation and MSFI metric can guide the design and selection of XAI algorithms
to meet clinical requirements on multi-modal explanation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEN: Twin Embedding Networks for the Jigsaw Puzzle Problem with Eroded Boundaries. (arXiv:2203.06488v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06488">
<div class="article-summary-box-inner">
<span><p>The jigsaw puzzle problem (JPP) is a well-known research problem, which has
been studied for many years. Solving this problem typically involves a
two-stage scheme, consisting of the computation of a pairwise piece
compatibility measure (CM), coupled with a subsequent puzzle reconstruction
algorithm. Many effective CMs, which apply a simple distance measure, based
merely on the information along the piece edges, have been proposed. However,
the practicality of these classical methods is rather doubtful for problem
instances harder than pure synthetic images. Specifically, these methods tend
to break down in more realistic scenarios involving, e.g., monochromatic
puzzles, eroded boundaries due to piece degradation over long time periods,
missing pieces, etc. To overcome this significant deficiency, a few deep
convolutional neural network (CNN)-based CMs have been recently introduced.
Despite their promising accuracy, these models are very computationally
intensive. Twin Embedding Networks (TEN), to represent a piece with respect to
its boundary in a latent embedding space. Combining this latent representation
with a simple distance measure, we then demonstrate a superior performance, in
terms of accuracy, of our newly proposed pairwise CM, compared to that of
various classical methods, for the problem domain of eroded tile boundaries, a
testbed for a number of real-world JPP variants. Furthermore, we also
demonstrate that TEN is faster by a few orders of magnitude, on average, than
the recent NN models, i.e., it is as fast as the classical methods. In this
regard, the paper makes a significant first attempt at bridging the gap between
the relatively low accuracy (of classical methods) and the intensive
computational complexity (of NN models), for practical, real-world puzzle-like
problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Information Bottleneck Guided Joint Source-Channel Coding. (arXiv:2203.06492v1 [cs.IT])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06492">
<div class="article-summary-box-inner">
<span><p>Joint source channel coding (JSCC) has attracted increasing attentions due to
its robustness and high efficiency. However, the existing research on JSCC
mainly focuses on minimizing the distortion between the transmitted and
received information, while limiting the required data rate. Therefore, even
though the transmitted information is well recovered, the transmitted bits may
be far more than the minimal threshold according to the rate-distortion (RD)
theory. In this paper, we propose an adaptive Information Bottleneck (IB)
guided JSCC (AIB-JSCC), which aims at achieving the theoretically maximal
compression ratio for a given reconstruction quality. In particular, we first
derive a mathematically tractable form of loss function for AIB-JSCC. To keep a
better tradeoff between compression and reconstruction quality, we further
propose an adaptive algorithm that adjusts hyperparameter beta of the proposed
loss function dynamically according to the distortion during training.
Experiment results show that AIB-JSCC can significantly reduce the required
amount of the transmitted data and improve the reconstruction quality and
downstream artificial-intelligent task performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Mixed Quantization Network for Computationally Efficient Mobile Inverse Tone Mapping. (arXiv:2203.06504v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06504">
<div class="article-summary-box-inner">
<span><p>Recovering a high dynamic range (HDR) image from a single low dynamic range
(LDR) image, namely inverse tone mapping (ITM), is challenging due to the lack
of information in over- and under-exposed regions. Current methods focus
exclusively on training high-performing but computationally inefficient ITM
models, which in turn hinder deployment of the ITM models in
resource-constrained environments with limited computing power such as edge and
mobile device applications.
</p>
<p>To this end, we propose combining efficient operations of deep neural
networks with a novel mixed quantization scheme to construct a well-performing
but computationally efficient mixed quantization network (MQN) which can
perform single image ITM on mobile platforms. In the ablation studies, we
explore the effect of using different attention mechanisms, quantization
schemes, and loss functions on the performance of MQN in ITM tasks. In the
comparative analyses, ITM models trained using MQN perform on par with the
state-of-the-art methods on benchmark datasets. MQN models provide up to 10
times improvement on latency and 25 times improvement on memory consumption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparsity and Heterogeneous Dropout for Continual Learning in the Null Space of Neural Activations. (arXiv:2203.06514v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06514">
<div class="article-summary-box-inner">
<span><p>Continual/lifelong learning from a non-stationary input data stream is a
cornerstone of intelligence. Despite their phenomenal performance in a wide
variety of applications, deep neural networks are prone to forgetting their
previously learned information upon learning new ones. This phenomenon is
called "catastrophic forgetting" and is deeply rooted in the
stability-plasticity dilemma. Overcoming catastrophic forgetting in deep neural
networks has become an active field of research in recent years. In particular,
gradient projection-based methods have recently shown exceptional performance
at overcoming catastrophic forgetting. This paper proposes two
biologically-inspired mechanisms based on sparsity and heterogeneous dropout
that significantly increase a continual learner's performance over a long
sequence of tasks. Our proposed approach builds on the Gradient Projection
Memory (GPM) framework. We leverage K-winner activations in each layer of a
neural network to enforce layer-wise sparse activations for each task, together
with a between-task heterogeneous dropout that encourages the network to use
non-overlapping activation patterns between different tasks. In addition, we
introduce Continual Swiss Roll as a lightweight and interpretable -- yet
challenging -- synthetic benchmark for continual learning. Lastly, we provide
an in-depth analysis of our proposed method and demonstrate a significant
performance boost on various benchmark continual learning problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning. (arXiv:2203.06541v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06541">
<div class="article-summary-box-inner">
<span><p>Heatmap regression methods have dominated face alignment area in recent years
while they ignore the inherent relation between different landmarks. In this
paper, we propose a Sparse Local Patch Transformer (SLPT) for learning the
inherent relation. The SLPT generates the representation of each single
landmark from a local patch and aggregates them by an adaptive inherent
relation based on the attention mechanism. The subpixel coordinate of each
landmark is predicted independently based on the aggregated feature. Moreover,
a coarse-to-fine framework is further introduced to incorporate with the SLPT,
which enables the initial landmarks to gradually converge to the target facial
landmarks using fine-grained features from dynamically resized local patches.
Extensive experiments carried out on three popular benchmarks, including WFLW,
300W and COFW, demonstrate that the proposed method works at the
state-of-the-art level with much less computational complexity by learning the
inherent relation between facial landmarks. The code is available at the
project website.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Change Detection from Synthetic Aperture Radar Images via Dual Path Denoising Network. (arXiv:2203.06543v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06543">
<div class="article-summary-box-inner">
<span><p>Benefited from the rapid and sustainable development of synthetic aperture
radar (SAR) sensors, change detection from SAR images has received increasing
attentions over the past few years. Existing unsupervised deep learning-based
methods have made great efforts to exploit robust feature representations, but
they consume much time to optimize parameters. Besides, these methods use
clustering to obtain pseudo-labels for training, and the pseudo-labeled samples
often involve errors, which can be considered as "label noise". To address
these issues, we propose a Dual Path Denoising Network (DPDNet) for SAR image
change detection. In particular, we introduce the random label propagation to
clean the label noise involved in preclassification. We also propose the
distinctive patch convolution for feature representation learning to reduce the
time consumption. Specifically, the attention mechanism is used to select
distinctive pixels in the feature maps, and patches around these pixels are
selected as convolution kernels. Consequently, the DPDNet does not require a
great number of training samples for parameter optimization, and its
computational efficiency is greatly enhanced. Extensive experiments have been
conducted on five SAR datasets to verify the proposed DPDNet. The experimental
results demonstrate that our method outperforms several state-of-the-art
methods in change detection results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CEKD:Cross Ensemble Knowledge Distillation for Augmented Fine-grained Data. (arXiv:2203.06551v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06551">
<div class="article-summary-box-inner">
<span><p>Data augmentation has been proved effective in training deep models. Existing
data augmentation methods tackle the fine-grained problem by blending image
pairs and fusing corresponding labels according to the statistics of mixed
pixels, which produces additional noise harmful to the performance of networks.
Motivated by this, we present a simple yet effective cross ensemble knowledge
distillation (CEKD) model for fine-grained feature learning. We innovatively
propose a cross distillation module to provide additional supervision to
alleviate the noise problem, and propose a collaborative ensemble module to
overcome the target conflict problem. The proposed model can be trained in an
end-to-end manner, and only requires image-level label supervision. Extensive
experiments on widely used fine-grained benchmarks demonstrate the
effectiveness of our proposed model. Specifically, with the backbone of
ResNet-101, CEKD obtains the accuracy of 89.59%, 95.96% and 94.56% in three
datasets respectively, outperforming state-of-the-art API-Net by 0.99%, 1.06%
and 1.16%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning for Automotive mmWave Radar Detection Points Based Instance Segmentation. (arXiv:2203.06553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06553">
<div class="article-summary-box-inner">
<span><p>The automotive mmWave radar plays a key role in advanced driver assistance
systems (ADAS) and autonomous driving. Deep learning-based instance
segmentation enables real-time object identification from the radar detection
points. In the conventional training process, accurate annotation is the key.
However, high-quality annotations of radar detection points are challenging to
achieve due to their ambiguity and sparsity. To address this issue, we propose
a contrastive learning approach for implementing radar detection points-based
instance segmentation. We define the positive and negative samples according to
the ground-truth label, apply the contrastive loss to train the model first,
and then perform training for the following downstream task. In addition, these
two steps can be merged into one, and pseudo labels can be generated for the
unlabeled data to improve the performance further. Thus, there are four
different training settings for our method. Experiments show that when the
ground-truth information is only available for 5% of the training data, our
method still achieves a comparable performance to the approach trained in a
supervised manner with 100% ground-truth information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation. (arXiv:2203.06558v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06558">
<div class="article-summary-box-inner">
<span><p>Training a generalizable 3D part segmentation network is quite challenging
but of great importance in real-world applications. To tackle this problem,
some works design task-specific solutions by translating human understanding of
the task to machine's learning process, which faces the risk of missing the
optimal strategy since machines do not necessarily understand in the exact
human way. Others try to use conventional task-agnostic approaches designed for
domain generalization problems with no task prior knowledge considered. To
solve the above issues, we propose AutoGPart, a generic method enabling
training generalizable 3D part segmentation networks with the task prior
considered. AutoGPart builds a supervision space with geometric prior knowledge
encoded, and lets the machine to search for the optimal supervisions from the
space for a specific segmentation task automatically. Extensive experiments on
three generalizable 3D part segmentation tasks are conducted to demonstrate the
effectiveness and versatility of AutoGPart. We demonstrate that the performance
of segmentation networks using simple backbones can be significantly improved
when trained with supervisions searched by our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Efficient Black-box Adversarial Attacks Guided by a Transfer-based Prior. (arXiv:2203.06560v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06560">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks have been extensively studied in recent years since they
can identify the vulnerability of deep learning models before deployed. In this
paper, we consider the black-box adversarial setting, where the adversary needs
to craft adversarial examples without access to the gradients of a target
model. Previous methods attempted to approximate the true gradient either by
using the transfer gradient of a surrogate white-box model or based on the
feedback of model queries. However, the existing methods inevitably suffer from
low attack success rates or poor query efficiency since it is difficult to
estimate the gradient in a high-dimensional input space with limited
information. To address these problems and improve black-box attacks, we
propose two prior-guided random gradient-free (PRGF) algorithms based on biased
sampling and gradient averaging, respectively. Our methods can take the
advantage of a transfer-based prior given by the gradient of a surrogate model
and the query information simultaneously. Through theoretical analyses, the
transfer-based prior is appropriately integrated with model queries by an
optimal coefficient in each method. Extensive experiments demonstrate that, in
comparison with the alternative state-of-the-arts, both of our methods require
much fewer queries to attack black-box models with higher success rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Worst Case Matters for Few-Shot Recognition. (arXiv:2203.06574v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06574">
<div class="article-summary-box-inner">
<span><p>Few-shot recognition learns a recognition model with very few (e.g., 1 or 5)
images per category, and current few-shot learning methods focus on improving
the average accuracy over many episodes. We argue that in real-world
applications we may often only try one episode instead of many, and hence
maximizing the worst-case accuracy is more important than maximizing the
average accuracy. We empirically show that a high average accuracy not
necessarily means a high worst-case accuracy. Since this objective is not
accessible, we propose to reduce the standard deviation and increase the
average accuracy simultaneously. In turn, we devise two strategies from the
bias-variance tradeoff perspective to implicitly reach this goal: a simple yet
effective stability regularization (SR) loss together with model ensemble to
reduce variance during fine-tuning, and an adaptability calibration mechanism
to reduce the bias. Extensive experiments on benchmark datasets demonstrate the
effectiveness of the proposed strategies, which outperforms current
state-of-the-art methods with a significant margin in terms of not only
average, but also worst-case accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CVFNet: Real-time 3D Object Detection by Learning Cross View Features. (arXiv:2203.06585v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06585">
<div class="article-summary-box-inner">
<span><p>In recent years 3D object detection from LiDAR point clouds has made great
progress thanks to the development of deep learning technologies. Although
voxel or point based methods are popular in 3D object detection, they usually
involve time-consuming operations such as 3D convolutions on voxels or ball
query among points, making the resulting network inappropriate for time
critical applications. On the other hand, 2D view-based methods feature high
computing efficiency while usually obtaining inferior performance than the
voxel or point based methods. In this work, we present a real-time view-based
single stage 3D object detector, namely CVFNet to fulfill this task. To
strengthen the cross-view feature learning under the condition of demanding
efficiency, our framework extracts the features of different views and fuses
them in an efficient progressive way. We first propose a novel Point-Range
feature fusion module that deeply integrates point and range view features in
multiple stages. Then, a special Slice Pillar is designed to well maintain the
3D geometry when transforming the obtained deep point-view features into bird's
eye view. To better balance the ratio of samples, a sparse pillar detection
head is presented to focus the detection on the nonempty grids. We conduct
experiments on the popular KITTI and NuScenes benchmark, and state-of-the-art
performances are achieved in terms of both accuracy and speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AugShuffleNet: Improve ShuffleNetV2 via More Information Communication. (arXiv:2203.06589v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06589">
<div class="article-summary-box-inner">
<span><p>Based on ShuffleNetV2, we build a more powerful and efficient model family,
termed as AugShuffleNets, by introducing higher frequency of cross-layer
information communication for better model performance. Evaluated on the
CIFAR-10 and CIFAR-100 datasets, AugShuffleNet consistently outperforms
ShuffleNetV2 in terms of accuracy, with less computational cost, fewer
parameter count.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Autoencoders for Point Cloud Self-supervised Learning. (arXiv:2203.06604v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06604">
<div class="article-summary-box-inner">
<span><p>As a promising scheme of self-supervised learning, masked autoencoding has
significantly advanced natural language processing and computer vision.
Inspired by this, we propose a neat scheme of masked autoencoders for point
cloud self-supervised learning, addressing the challenges posed by point
cloud's properties, including leakage of location information and uneven
information density. Concretely, we divide the input point cloud into irregular
point patches and randomly mask them at a high ratio. Then, a standard
Transformer based autoencoder, with an asymmetric design and a shifting mask
tokens operation, learns high-level latent features from unmasked point
patches, aiming to reconstruct the masked point patches. Extensive experiments
show that our approach is efficient during pre-training and generalizes well on
various downstream tasks. Specifically, our pre-trained models achieve 84.52\%
accuracy on ScanObjectNN and 94.04% accuracy on ModelNet40, outperforming all
the other self-supervised learning methods. We show with our scheme, a simple
architecture entirely based on standard Transformers can surpass dedicated
Transformer models from supervised learning. Our approach also advances
state-of-the-art accuracies by 1.5%-2.3% in the few-shot object classification.
Furthermore, our work inspires the feasibility of applying unified
architectures from languages and images to the point cloud.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-Aware Generative Adversarial Network for Talking Head Video Generation. (arXiv:2203.06605v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06605">
<div class="article-summary-box-inner">
<span><p>Talking head video generation aims to produce a synthetic human face video
that contains the identity and pose information respectively from a given
source image and a driving video.Existing works for this task heavily rely on
2D representations (e.g. appearance and motion) learned from the input images.
However, dense 3D facial geometry (e.g. pixel-wise depth) is extremely
important for this task as it is particularly beneficial for us to essentially
generate accurate 3D face structures and distinguish noisy information from the
possibly cluttered background. Nevertheless, dense 3D geometry annotations are
prohibitively costly for videos and are typically not available for this video
generation task. In this paper, we first introduce a self-supervised geometry
learning method to automatically recover the dense 3D geometry (i.e.depth) from
the face videos without the requirement of any expensive 3D annotation data.
Based on the learned dense depth maps, we further propose to leverage them to
estimate sparse facial keypoints that capture the critical movement of the
human head. In a more dense way, the depth is also utilized to learn 3D-aware
cross-modal (i.e. appearance and depth) attention to guide the generation of
motion fields for warping source image representations. All these contributions
compose a novel depth-aware generative adversarial network (DaGAN) for talking
head generation. Extensive experiments conducted demonstrate that our proposed
method can generate highly realistic faces, and achieve significant results on
the unseen human faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-LSTM: a robust classifier for video detection on UCF101. (arXiv:2203.06610v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06610">
<div class="article-summary-box-inner">
<span><p>Video detection and human action recognition may be computationally
expensive, and need a long time to train models. In this paper, we were
intended to reduce the training time and the GPU memory usage of video
detection, and achieved a competitive detection accuracy. Other research works
such as Two-stream, C3D, TSN have shown excellent performance on UCF101. Here,
we used a LSTM structure simply for video detection. We used a simple structure
to perform a competitive top-1 accuracy on the entire validation dataset of
UCF101. The LSTM structure is named Context-LSTM, since it may process the deep
temporal features. The Context-LSTM may simulate the human recognition system.
We cascaded the LSTM blocks in PyTorch and connected the cell state flow and
hidden output flow. At the connection of the blocks, we used ReLU, Batch
Normalization, and MaxPooling functions. The Context-LSTM could reduce the
training time and the GPU memory usage, while keeping a state-of-the-art top-1
accuracy on UCF101 entire validation dataset, show a robust performance on
video action detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Single Correspondence Is Enough: Robust Global Registration to Avoid Degeneracy in Urban Environments. (arXiv:2203.06612v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06612">
<div class="article-summary-box-inner">
<span><p>Global registration using 3D point clouds is a crucial technology for mobile
platforms to achieve localization or manage loop-closing situations. In recent
years, numerous researchers have proposed global registration methods to
address a large number of outlier correspondences. Unfortunately, the
degeneracy problem, which represents the phenomenon in which the number of
estimated inliers becomes lower than three, is still potentially inevitable. To
tackle the problem, a degeneracy-robust decoupling-based global registration
method is proposed, called Quatro. In particular, our method employs
quasi-SO(3) estimation by leveraging the Atlanta world assumption in urban
environments to avoid degeneracy in rotation estimation. Thus, the minimum
degree of freedom (DoF) of our method is reduced from three to one. As verified
in indoor and outdoor 3D LiDAR datasets, our proposed method yields robust
global registration performance compared with other global registration
methods, even for distant point cloud pairs. Furthermore, the experimental
results confirm the applicability of our method as a coarse alignment. Our code
is available: https://github.com/url-kaist/quatro.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAS-AT: Adversarial Training with Learnable Attack Strategy. (arXiv:2203.06616v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06616">
<div class="article-summary-box-inner">
<span><p>Adversarial training (AT) is always formulated as a minimax problem, of which
the performance depends on the inner optimization that involves the generation
of adversarial examples (AEs). Most previous methods adopt Projected Gradient
Decent (PGD) with manually specifying attack parameters for AE generation. A
combination of the attack parameters can be referred to as an attack strategy.
Several works have revealed that using a fixed attack strategy to generate AEs
during the whole training phase limits the model robustness and propose to
exploit different attack strategies at different training stages to improve
robustness. But those multi-stage hand-crafted attack strategies need much
domain expertise, and the robustness improvement is limited. In this paper, we
propose a novel framework for adversarial training by introducing the concept
of "learnable attack strategy", dubbed LAS-AT, which learns to automatically
produce attack strategies to improve the model robustness. Our framework is
composed of a target network that uses AEs for training to improve robustness
and a strategy network that produces attack strategies to control the AE
generation. Experimental evaluations on three benchmark databases demonstrate
the superiority of the proposed method. The code is released at
https://github.com/jiaxiaojunQAQ/LAS-AT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Bracket High Dynamic Range Imaging with Event Cameras. (arXiv:2203.06622v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06622">
<div class="article-summary-box-inner">
<span><p>Modern high dynamic range (HDR) imaging pipelines align and fuse multiple low
dynamic range (LDR) images captured at different exposure times. While these
methods work well in static scenes, dynamic scenes remain a challenge since the
LDR images still suffer from saturation and noise. In such scenarios, event
cameras would be a valid complement, thanks to their higher temporal resolution
and dynamic range. In this paper, we propose the first multi-bracket HDR
pipeline combining a standard camera with an event camera. Our results show
better overall robustness when using events, with improvements in PSNR by up to
5dB on synthetic data and up to 0.7dB on real-world data. We also introduce a
new dataset containing bracketed LDR images with aligned events and HDR ground
truth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Deep Semi-supervised Learning: An Empirical Distribution Alignment Framework and Its Generalization Bound. (arXiv:2203.06639v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06639">
<div class="article-summary-box-inner">
<span><p>In this work, we revisit the semi-supervised learning (SSL) problem from a
new perspective of explicitly reducing empirical distribution mismatch between
labeled and unlabeled samples. Benefited from this new perspective, we first
propose a new deep semi-supervised learning framework called Semi-supervised
Learning by Empirical Distribution Alignment (SLEDA), in which existing
technologies from the domain adaptation community can be readily used to
address the semi-supervised learning problem through reducing the empirical
distribution distance between labeled and unlabeled data. Based on this
framework, we also develop a new theoretical generalization bound for the
research community to better understand the semi-supervised learning problem,
in which we show the generalization error of semi-supervised learning can be
effectively bounded by minimizing the training error on labeled data and the
empirical distribution distance between labeled and unlabeled data. Building
upon our new framework and the theoretical bound, we develop a simple and
effective deep semi-supervised learning method called Augmented Distribution
Alignment Network (ADA-Net) by simultaneously adopting the well-established
adversarial training strategy from the domain adaptation community and a simple
sample interpolation strategy for data augmentation. Additionally, we
incorporate both strategies in our ADA-Net into two exiting SSL methods to
further improve their generalization capability, which indicates that our new
framework provides a complementary solution for solving the SSL problem. Our
comprehensive experimental results on two benchmark datasets SVHN and CIFAR-10
for the semi-supervised image recognition task and another two benchmark
datasets ModelNet40 and ShapeNet55 for the semi-supervised point cloud
recognition task demonstrate the effectiveness of our proposed framework for
SSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4. (arXiv:2203.06649v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06649">
<div class="article-summary-box-inner">
<span><p>Modern high-scoring models of vision in the brain score competition do not
stem from Vision Transformers. However, in this short paper, we provide
evidence against the unexpected trend of Vision Transformers (ViT) being not
perceptually aligned with human visual representations by showing how a
dual-stream Transformer, a CrossViT$~\textit{a la}$ Chen et al. (2021), under a
joint rotationally-invariant and adversarial optimization procedure yields 2nd
place in the aggregate Brain-Score 2022 competition averaged across all visual
categories, and currently (March 1st, 2022) holds the 1st place for the highest
explainable variance of area V4. In addition, our current Transformer-based
model also achieves greater explainable variance for areas V4, IT and Behaviour
than a biologically-inspired CNN (ResNet50) that integrates a frontal V1-like
computation module(Dapello et al.,2020). Our team was also the only entry in
the top-5 that shows a positive rank correlation between explained variance per
area and depth in the visual hierarchy.
</p>
<p>Against our initial expectations, these results provide tentative support for
an $\textit{"All roads lead to Rome"}$ argument enforced via a joint
optimization rule even for non biologically-motivated models of vision such as
Vision Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global2Local: A Joint-Hierarchical Attention for Video Captioning. (arXiv:2203.06663v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06663">
<div class="article-summary-box-inner">
<span><p>Recently, automatic video captioning has attracted increasing attention,
where the core challenge lies in capturing the key semantic items, like objects
and actions as well as their spatial-temporal correlations from the redundant
frames and semantic content. To this end, existing works select either the key
video clips in a global level~(across multi frames), or key regions within each
frame, which, however, neglect the hierarchical order, i.e., key frames first
and key regions latter. In this paper, we propose a novel joint-hierarchical
attention model for video captioning, which embeds the key clips, the key
frames and the key regions jointly into the captioning model in a hierarchical
manner. Such a joint-hierarchical attention model first conducts a global
selection to identify key frames, followed by a Gumbel sampling operation to
identify further key regions based on the key frames, achieving an accurate
global-to-local feature representation to guide the captioning. Extensive
quantitative evaluations on two public benchmark datasets MSVD and MSR-VTT
demonstrates the superiority of the proposed method over the state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the answering frame timeline. Extensive experiments on the medical
instructional dataset, namely MedVidQA, show the proposed VPTSL outperforms
other state-of-the-art methods, which demonstrates the effectiveness of visual
prompt and the text span predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PNM: Pixel Null Model for General Image Segmentation. (arXiv:2203.06677v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06677">
<div class="article-summary-box-inner">
<span><p>A major challenge in image segmentation is classifying object boundaries.
Recent efforts propose to refine the segmentation result with boundary masks.
However, models are still prone to misclassifying boundary pixels even when
they correctly capture the object contours. In such cases, even a perfect
boundary map is unhelpful for segmentation refinement. In this paper, we argue
that assigning proper prior weights to error-prone pixels such as object
boundaries can significantly improve the segmentation quality. Specifically, we
present the \textit{pixel null model} (PNM), a prior model that weights each
pixel according to its probability of being correctly classified by a random
segmenter. Empirical analysis shows that PNM captures the misclassification
distribution of different state-of-the-art (SOTA) segmenters. Extensive
experiments on semantic, instance, and panoptic segmentation tasks over three
datasets (Cityscapes, ADE20K, MS COCO) confirm that PNM consistently improves
the segmentation quality of most SOTA methods (including the vision
transformers) and outperforms boundary-based methods by a large margin. We also
observe that the widely-used mean IoU (mIoU) metric is insensitive to
boundaries of different sharpness. As a byproduct, we propose a new metric,
\textit{PNM IoU}, which perceives the boundary sharpness and better reflects
the model segmentation performance in error-prone regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors. (arXiv:2203.06691v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06691">
<div class="article-summary-box-inner">
<span><p>The main question this work aims at answering is: can morphing attack
detection (MAD) solutions be successfully developed based on synthetic data?.
Towards that, this work introduces the first synthetic-based MAD development
dataset, namely the Synthetic Morphing Attack Detection Development dataset
(SMDD). This dataset is utilized successfully to train three MAD backbones
where it proved to lead to high MAD performance, even on completely unknown
attack types. Additionally, an essential aspect of this work is the detailed
legal analyses of the challenges of using and sharing real biometric data,
rendering our proposed SMDD dataset extremely essential. The SMDD dataset,
consisting of 30,000 attack and 50,000 bona fide samples, is made publicly
available for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Protocol Matters: Towards Accurate Scene Text Recognition via Training Protocol Searching. (arXiv:2203.06696v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06696">
<div class="article-summary-box-inner">
<span><p>The development of scene text recognition (STR) in the era of deep learning
has been mainly focused on novel architectures of STR models. However, training
protocol (i.e., settings of the hyper-parameters involved in the training of
STR models), which plays an equally important role in successfully training a
good STR model, is under-explored for scene text recognition. In this work, we
attempt to improve the accuracy of existing STR models by searching for optimal
training protocol. Specifically, we develop a training protocol search
algorithm, based on a newly designed search space and an efficient search
algorithm using evolutionary optimization and proxy tasks. Experimental results
show that our searched training protocol can improve the recognition accuracy
of mainstream STR models by 2.7%~3.9%. In particular, with the searched
training protocol, TRBA-Net achieves 2.1% higher accuracy than the
state-of-the-art STR model (i.e., EFIFSTR), while the inference speed is 2.3x
and 3.7x faster on CPU and GPU respectively. Extensive experiments are
conducted to demonstrate the effectiveness of the proposed method and the
generalization ability of the training protocol found by our search method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Long-Range Attention Network for Image Super-resolution. (arXiv:2203.06697v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06697">
<div class="article-summary-box-inner">
<span><p>Recently, transformer-based methods have demonstrated impressive results in
various vision tasks, including image super-resolution (SR), by exploiting the
self-attention (SA) for feature extraction. However, the computation of SA in
most existing transformer based models is very expensive, while some employed
operations may be redundant for the SR task. This limits the range of SA
computation and consequently the SR performance. In this work, we propose an
efficient long-range attention network (ELAN) for image SR. Specifically, we
first employ shift convolution (shift-conv) to effectively extract the image
local structural information while maintaining the same level of complexity as
1x1 convolution, then propose a group-wise multi-scale self-attention (GMSA)
module, which calculates SA on non-overlapped groups of features using
different window sizes to exploit the long-range image dependency. A highly
efficient long-range attention block (ELAB) is then built by simply cascading
two shift-conv with a GMSA module, which is further accelerated by using a
shared attention mechanism. Without bells and whistles, our ELAN follows a
fairly simple design by sequentially cascading the ELABs. Extensive experiments
demonstrate that ELAN obtains even better results against the transformer-based
SR models but with significantly less complexity. The source code can be found
at https://github.com/xindongzhang/ELAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06717">
<div class="article-summary-box-inner">
<span><p>In this paper we revisit large kernel design in modern convolutional neural
networks (CNNs), which is often neglected in the past few years. Inspired by
recent advances of vision transformers (ViTs), we point out that using a few
large kernels instead of a stack of small convolutions could be a more powerful
paradigm. We therefore summarize 5 guidelines, e.g., applying re-parameterized
large depth-wise convolutions, to design efficient high-performance
large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN
architecture whose kernel size is as large as 31x31. RepLKNet greatly bridges
the performance gap between CNNs and ViTs, e.g., achieving comparable or better
results than Swin Transformer on ImageNet and downstream tasks, while the
latency of RepLKNet is much lower. Moreover, RepLKNet also shows feasible
scalability to big data and large models, obtaining 87.8% top-1 accuracy on
ImageNet and 56.0%} mIoU on ADE20K. At last, our study further suggests
large-kernel CNNs share several nice properties with ViTs, e.g., much larger
effective receptive fields than conventional CNNs, and higher shape bias rather
than texture bias. Code &amp; models at
https://github.com/megvii-research/RepLKNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Food Recipe Recommendation Based on Ingredients Detection Using Deep Learning. (arXiv:2203.06721v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06721">
<div class="article-summary-box-inner">
<span><p>Food is essential for human survival, and people always try to taste
different types of delicious recipes. Frequently, people choose food
ingredients without even knowing their names or pick up some food ingredients
that are not obvious to them from a grocery store. Knowing which ingredients
can be mixed to make a delicious food recipe is essential. Selecting the right
recipe by choosing a list of ingredients is very difficult for a beginner cook.
However, it can be a problem even for experts. One such example is recognising
objects through image processing. Although this process is complex due to
different food ingredients, traditional approaches will lead to an inaccuracy
rate. These problems can be solved by machine learning and deep learning
approaches. In this paper, we implemented a model for food ingredients
recognition and designed an algorithm for recommending recipes based on
recognised ingredients. We made a custom dataset consisting of 9856 images
belonging to 32 different food ingredients classes. Convolution Neural Network
(CNN) model was used to identify food ingredients, and for recipe
recommendations, we have used machine learning. We achieved an accuracy of 94
percent, which is quite impressive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature space reduction as data preprocessing for the anomaly detection. (arXiv:2203.06747v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06747">
<div class="article-summary-box-inner">
<span><p>In this paper, we present two pipelines in order to reduce the feature space
for anomaly detection using the One Class SVM. As a first stage of both
pipelines, we compare the performance of three convolutional autoencoders. We
use the PCA method together with t-SNE as the first pipeline and the
reconstruction errors based method as the second. Both methods have potential
for the anomaly detection, but the reconstruction error metrics prove to be
more robust for this task. We show that the convolutional autoencoder
architecture doesn't have a significant effect for this task and we prove the
potential of our approach on the real world dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decontextualized I3D ConvNet for ultra-distance runners performance analysis at a glance. (arXiv:2203.06749v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06749">
<div class="article-summary-box-inner">
<span><p>In May 2021, the site runnersworld.com published that participation in
ultra-distance races has increased by 1,676% in the last 23 years. Moreover,
nearly 41% of those runners participate in more than one race per year. The
development of wearable devices has undoubtedly contributed to motivating
participants by providing performance measures in real-time. However, we
believe there is room for improvement, particularly from the organizers point
of view. This work aims to determine how the runners performance can be
quantified and predicted by considering a non-invasive technique focusing on
the ultra-running scenario. In this sense, participants are captured when they
pass through a set of locations placed along the race track. Each footage is
considered an input to an I3D ConvNet to extract the participant's running gait
in our work. Furthermore, weather and illumination capture conditions or
occlusions may affect these footages due to the race staff and other runners.
To address this challenging task, we have tracked and codified the
participant's running gait at some RPs and removed the context intending to
ensure a runner-of-interest proper evaluation. The evaluation suggests that the
features extracted by an I3D ConvNet provide enough information to estimate the
participant's performance along the different race tracks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TurbuGAN: An Adversarial Learning Approach to Spatially-Varying Multiframe Blind Deconvolution with Applications to Imaging Through Turbulence. (arXiv:2203.06764v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06764">
<div class="article-summary-box-inner">
<span><p>We present a self-supervised and self-calibrating multi-shot approach to
imaging through atmospheric turbulence, called TurbuGAN. Our approach requires
no paired training data, adapts itself to the distribution of the turbulence,
leverages domain-specific data priors, outperforms existing approaches, and can
generalize from tens to tens of thousands of measurements. We achieve such
functionality through an adversarial sensing framework adapted from CryoGAN,
which uses a discriminator network to match the distributions of captured and
simulated measurements. Our framework builds on CryoGAN by (1) generalizing the
forward measurement model to incorporate physically accurate and
computationally efficient models for light propagation through anisoplanatic
turbulence, (2) enabling adaptation to slightly misspecified forward models,
and (3) leveraging domain-specific prior knowledge using pretrained generative
networks, when available. We validate TurbuGAN in simulation using realistic
models for atmospheric turbulence-induced distortion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Similarity Equivariant Linear Transformation of Joint Orientation-Scale Space Representations. (arXiv:2203.06786v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06786">
<div class="article-summary-box-inner">
<span><p>Convolution is conventionally defined as a linear operation on functions of
one or more variables which commutes with shifts. Group convolution generalizes
the concept to linear operations on functions of group elements representing
more general geometric transformations and which commute with those
transformations. Since similarity transformation is the most general geometric
transformation on images that preserves shape, the group convolution that is
equivariant to similarity transformation is the most general shape preserving
linear operator. Because similarity transformations have four free parameters,
group convolutions are defined on four-dimensional, joint orientation-scale
spaces. Although prior work on equivariant linear operators has been limited to
discrete groups, the similarity group is continuous. In this paper, we describe
linear operators on discrete representations that are equivariant to continuous
similarity transformation. This is achieved by using a basis of functions that
is it joint shiftable-twistable-scalable. These it pinwheel functions use
Fourier series in the orientation dimension and Laplace transform in the
log-scale dimension to form a basis of spatially localized functions that can
be continuously interpolated in position, orientation and scale. Although this
result is potentially significant with respect to visual computation generally,
we present an initial demonstration of its utility by using it to compute a
shape equivariant distribution of closed contours traced by particles
undergoing Brownian motion in velocity. The contours are constrained by sets of
points and line endings representing well known bistable illusory contour
inducing patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Euclidean Invariant Recognition of 2D Shapes Using Histograms of Magnitudes of Local Fourier-Mellin Descriptors. (arXiv:2203.06787v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06787">
<div class="article-summary-box-inner">
<span><p>Because the magnitude of inner products with its basis functions are
invariant to rotation and scale change, the Fourier-Mellin transform has long
been used as a component in Euclidean invariant 2D shape recognition systems.
Yet Fourier-Mellin transform magnitudes are only invariant to rotation and
scale changes about a known center point, and full Euclidean invariant shape
recognition is not possible except when this center point can be consistently
and accurately identified. In this paper, we describe a system where a
Fourier-Mellin transform is computed at every point in the image. The spatial
support of the Fourier-Mellin basis functions is made local by multiplying them
with a polynomial envelope. Significantly, the magnitudes of convolutions with
these complex filters at isolated points are not (by themselves) used as
features for Euclidean invariant shape recognition because reliable
discrimination would require filters with spatial support large enough to fully
encompass the shapes. Instead, we rely on the fact that normalized histograms
of magnitudes are fully Euclidean invariant. We demonstrate a system based on
the VLAD machine learning method that performs Euclidean invariant recognition
of 2D shapes and requires an order of magnitude less training data than
comparable methods based on convolutional neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Learning for Deformable Medical Image Registration by Jointly Optimizing Network Architectures and Objective Functions. (arXiv:2203.06810v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06810">
<div class="article-summary-box-inner">
<span><p>Deformable image registration plays a critical role in various tasks of
medical image analysis. A successful registration algorithm, either derived
from conventional energy optimization or deep networks requires tremendous
efforts from computer experts to well design registration energy or to
carefully tune network architectures for the specific type of medical data. To
tackle the aforementioned problems, this paper proposes an automated learning
registration algorithm (AutoReg) that cooperatively optimizes both
architectures and their corresponding training objectives, enabling
non-computer experts, e.g., medical/clinical users, to conveniently find
off-the-shelf registration algorithms for diverse scenarios. Specifically, we
establish a triple-level framework to deduce registration network architectures
and objectives with an auto-searching mechanism and cooperating optimization.
We conduct image registration experiments on multi-site volume datasets and
various registration tasks. Extensive results demonstrate that our AutoReg may
automatically learn an optimal deep registration network for given volumes and
achieve state-of-the-art performance, also significantly improving computation
efficiency than the mainstream UNet architectures (from 0.558 to 0.270 seconds
for a 3D image pair on the same configuration).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADAS: A Direct Adaptation Strategy for Multi-Target Domain Adaptive Semantic Segmentation. (arXiv:2203.06811v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06811">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a direct adaptation strategy (ADAS), which aims to
directly adapt a single model to multiple target domains in a semantic
segmentation task without pretrained domain-specific models. To do so, we
design a multi-target domain transfer network (MTDT-Net) that aligns visual
attributes across domains by transferring the domain distinctive features
through a new target adaptive denormalization (TAD) module. Moreover, we
propose a bi-directional adaptive region selection (BARS) that reduces the
attribute ambiguity among the class labels by adaptively selecting the regions
with consistent feature statistics. We show that our single MTDT-Net can
synthesize visually pleasing domain transferred images with complex driving
datasets, and BARS effectively filters out the unnecessary region of training
images for each target domain. With the collaboration of MTDT-Net and BARS, our
ADAS achieves state-of-the-art performance for multi-target domain adaptation
(MTDA). To the best of our knowledge, our method is the first MTDA method that
directly adapts to multiple domains in semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Commands for Autonomous Vehicles via Layer Fusion with Region-specific Dynamic Layer Attention. (arXiv:2203.06822v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06822">
<div class="article-summary-box-inner">
<span><p>Grounding a command to the visual environment is an essential ingredient for
interactions between autonomous vehicles and humans. In this work, we study the
problem of language grounding for autonomous vehicles, which aims to localize a
region in a visual scene according to a natural language command from a
passenger. Prior work only employs the top layer representations of a
vision-and-language pre-trained model to predict the region referred to by the
command. However, such a method omits the useful features encoded in other
layers, and thus results in inadequate understanding of the input scene and
command. To tackle this limitation, we present the first layer fusion approach
for this task. Since different visual regions may require distinct types of
features to disambiguate them from each other, we further propose the
region-specific dynamic (RSD) layer attention to adaptively fuse the multimodal
information across layers for each region. Extensive experiments on the
Talk2Car benchmark demonstrate that our approach helps predict more accurate
regions and outperforms state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SKM-TEA: A Dataset for Accelerated MRI Reconstruction with Dense Image Labels for Quantitative Clinical Evaluation. (arXiv:2203.06823v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06823">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging (MRI) is a cornerstone of modern medical imaging.
However, long image acquisition times, the need for qualitative expert
analysis, and the lack of (and difficulty extracting) quantitative indicators
that are sensitive to tissue health have curtailed widespread clinical and
research studies. While recent machine learning methods for MRI reconstruction
and analysis have shown promise for reducing this burden, these techniques are
primarily validated with imperfect image quality metrics, which are discordant
with clinically-relevant measures that ultimately hamper clinical deployment
and clinician trust. To mitigate this challenge, we present the Stanford Knee
MRI with Multi-Task Evaluation (SKM-TEA) dataset, a collection of quantitative
knee MRI (qMRI) scans that enables end-to-end, clinically-relevant evaluation
of MRI reconstruction and analysis tools. This 1.6TB dataset consists of
raw-data measurements of ~25,000 slices (155 patients) of anonymized patient
MRI scans, the corresponding scanner-generated DICOM images, manual
segmentations of four tissues, and bounding box annotations for sixteen
clinically relevant pathologies. We provide a framework for using qMRI
parameter maps, along with image reconstructions and dense image labels, for
measuring the quality of qMRI biomarker estimates extracted from MRI
reconstruction, segmentation, and detection techniques. Finally, we use this
framework to benchmark state-of-the-art baselines on this dataset. We hope our
SKM-TEA dataset and code can enable a broad spectrum of research for modular
image reconstruction and image analysis in a clinically informed manner.
Dataset access, code, and benchmarks are available at
https://github.com/StanfordMIMI/skm-tea.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness Evaluation in Deepfake Detection Models using Metamorphic Testing. (arXiv:2203.06825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06825">
<div class="article-summary-box-inner">
<span><p>Fairness of deepfake detectors in the presence of anomalies are not well
investigated, especially if those anomalies are more prominent in either male
or female subjects. The primary motivation for this work is to evaluate how
deepfake detection model behaves under such anomalies. However, due to the
black-box nature of deep learning (DL) and artificial intelligence (AI)
systems, it is hard to predict the performance of a model when the input data
is modified. Crucially, if this defect is not addressed properly, it will
adversely affect the fairness of the model and result in discrimination of
certain sub-population unintentionally. Therefore, the objective of this work
is to adopt metamorphic testing to examine the reliability of the selected
deepfake detection model, and how the transformation of input variation places
influence on the output. We have chosen MesoInception-4, a state-of-the-art
deepfake detection model, as the target model and makeup as the anomalies.
Makeups are applied through utilizing the Dlib library to obtain the 68 facial
landmarks prior to filling in the RGB values. Metamorphic relations are derived
based on the notion that realistic perturbations of the input images, such as
makeup, involving eyeliners, eyeshadows, blushes, and lipsticks (which are
common cosmetic appearance) applied to male and female images, should not alter
the output of the model by a huge margin. Furthermore, we narrow down the scope
to focus on revealing potential gender biases in DL and AI systems.
Specifically, we are interested to examine whether MesoInception-4 model
produces unfair decisions, which should be considered as a consequence of
robustness issues. The findings from our work have the potential to pave the
way for new research directions in the quality assurance and fairness in DL and
AI systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bures Joint Distribution Alignment with Dynamic Margin for Unsupervised Domain Adaptation. (arXiv:2203.06836v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06836">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) is one of the prominent tasks of
transfer learning, and it provides an effective approach to mitigate the
distribution shift between the labeled source domain and the unlabeled target
domain. Prior works mainly focus on aligning the marginal distributions or the
estimated class-conditional distributions. However, the joint dependency among
the feature and the label is crucial for the adaptation task and is not fully
exploited. To address this problem, we propose the Bures Joint Distribution
Alignment (BJDA) algorithm which directly models the joint distribution shift
based on the optimal transport theory in the infinite-dimensional kernel
spaces. Specifically, we propose a novel alignment loss term that minimizes the
kernel Bures-Wasserstein distance between the joint distributions. Technically,
BJDA can effectively capture the nonlinear structures underlying the data. In
addition, we introduce a dynamic margin in contrastive learning phase to
flexibly characterize the class separability and improve the discriminative
ability of representations. It also avoids the cross-validation procedure to
determine the margin parameter in traditional triplet loss based methods.
Extensive experiments show that BJDA is very effective for the UDA tasks, as it
outperforms state-of-the-art algorithms in most experimental settings. In
particular, BJDA improves the average accuracy of UDA tasks by 2.8% on
Adaptiope, 1.4% on Office-Caltech10, and 1.1% on ImageCLEF-DA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STDAN: Deformable Attention Network for Space-Time Video Super-Resolution. (arXiv:2203.06841v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06841">
<div class="article-summary-box-inner">
<span><p>The target of space-time video super-resolution (STVSR) is to increase the
spatial-temporal resolution of low-resolution (LR) and low frame rate (LFR)
videos. Recent approaches based on deep learning have made significant
improvements, but most of them only use two adjacent frames, that is,
short-term features, to synthesize the missing frame embedding, which suffers
from fully exploring the information flow of consecutive input LR frames. In
addition, existing STVSR models hardly exploit the temporal contexts explicitly
to assist high-resolution (HR) frame reconstruction. To address these issues,
in this paper, we propose a deformable attention network called STDAN for
STVSR. First, we devise a long-short term feature interpolation (LSTFI) module,
which is capable of excavating abundant content from more neighboring input
frames for the interpolation process through a bidirectional RNN structure.
Second, we put forward a spatial-temporal deformable feature aggregation
(STDFA) module, in which spatial and temporal contexts in dynamic video frames
are adaptively captured and aggregated to enhance SR reconstruction.
Experimental results on several datasets demonstrate that our approach
outperforms state-of-the-art STVSR methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RecursiveMix: Mixed Learning with History. (arXiv:2203.06844v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06844">
<div class="article-summary-box-inner">
<span><p>Mix-based augmentation has been proven fundamental to the generalization of
deep vision models. However, current augmentations only mix samples at the
current data batch during training, which ignores the possible knowledge
accumulated in the learning history. In this paper, we propose a recursive
mixed-sample learning paradigm, termed "RecursiveMix" (RM), by exploring a
novel training strategy that leverages the historical input-prediction-label
triplets. More specifically, we iteratively resize the input image batch from
the previous iteration and paste it into the current batch while their labels
are fused proportionally to the area of the operated patches. Further, a
consistency loss is introduced to align the identical image semantics across
the iterations, which helps the learning of scale-invariant feature
representations. Based on ResNet-50, RM largely improves classification
accuracy by $\sim$3.2\% on CIFAR100 and $\sim$2.8\% on ImageNet with negligible
extra computation/storage costs. In the downstream object detection task, the
RM pretrained model outperforms the baseline by 2.1 AP points and surpasses
CutMix by 1.4 AP points under the ATSS detector on COCO. In semantic
segmentation, RM also surpasses the baseline and CutMix by 1.9 and 1.1 mIoU
points under UperNet on ADE20K, respectively. Codes and pretrained models are
available at \url{https://github.com/megvii-research/RecursiveMix}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Scale Open-Set Deep Logo Detection. (arXiv:1911.07440v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.07440">
<div class="article-summary-box-inner">
<span><p>We present an open-set logo detection (OSLD) system, which can detect
(localize and recognize) any number of unseen logo classes without re-training;
it only requires a small set of canonical logo images for each logo class. We
achieve this using a two-stage approach: (1) Generic logo detection to detect
candidate logo regions in an image. (2) Logo matching for matching the detected
logo regions to a set of canonical logo images to recognize them.
</p>
<p>We constructed an open-set logo detection dataset with 12.1k logo classes and
released it for research purposes.We demonstrate the effectiveness of OSLD on
our dataset and on the standard Flickr-32 logo dataset, outperforming the
state-of-the-art open-set and closed-set logo detection methods by a large
margin. OSLD is scalable to millions of logo classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Split and Expand: An inference-time improvement for Weakly Supervised Cell Instance Segmentation. (arXiv:2007.10817v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.10817">
<div class="article-summary-box-inner">
<span><p>We consider the problem of segmenting cell nuclei instances from Hematoxylin
and Eosin (H&amp;E) stains with weak supervision. While most recent works focus on
improving the segmentation quality, this is usually insufficient for instance
segmentation of cell instances clumped together or with a small size. In this
work, we propose a two-step post-processing procedure, Split and Expand, that
directly improves the conversion of segmentation maps to instances. In the
Split step, we split clumps of cells from the segmentation map into individual
cell instances with the guidance of cell-center predictions through Gaussian
Mixture Model clustering. In the Expand step, we find missing small cells using
the cell-center predictions (which tend to capture small cells more
consistently as they are trained using reliable point annotations), and utilize
Layer-wise Relevance Propagation (LRP) explanation results to expand those
cell-center predictions into cell instances. Our Split and Expand
post-processing procedure is training-free and is executed at inference-time
only. To further improve the performance of our method, a feature re-weighting
loss based on LRP is proposed. We test our procedure on the MoNuSeg and TNBC
datasets and show that our proposed method provides statistically significant
improvements on object-level metrics. Our code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Transformers: A Survey. (arXiv:2009.06732v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06732">
<div class="article-summary-box-inner">
<span><p>Transformer model architectures have garnered immense interest lately due to
their effectiveness across a range of domains like language, vision and
reinforcement learning. In the field of natural language processing for
example, Transformers have become an indispensable staple in the modern deep
learning stack. Recently, a dizzying number of "X-former" models have been
proposed - Reformer, Linformer, Performer, Longformer, to name a few - which
improve upon the original Transformer architecture, many of which make
improvements around computational and memory efficiency. With the aim of
helping the avid researcher navigate this flurry, this paper characterizes a
large and thoughtful selection of recent efficiency-flavored "X-former" models,
providing an organized and comprehensive overview of existing work and models
across multiple domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Guided Learning: Towards Open Domain Egocentric Action Recognition with Zero Supervision. (arXiv:2009.07470v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.07470">
<div class="article-summary-box-inner">
<span><p>Advances in deep learning have enabled the development of models that have
exhibited a remarkable tendency to recognize and even localize actions in
videos. However, they tend to experience errors when faced with scenes or
examples beyond their initial training environment. Hence, they fail to adapt
to new domains without significant retraining with large amounts of annotated
data. In this paper, we propose to overcome these limitations by moving to an
open-world setting by decoupling the ideas of recognition and reasoning.
Building upon the compositional representation offered by Grenander's Pattern
Theory formalism, we show that attention and commonsense knowledge can be used
to enable the self-supervised discovery of novel actions in egocentric videos
in an open-world setting, where data from the observed environment (the target
domain) is open i.e., the vocabulary is partially known and training examples
(both labeled and unlabeled) are not available. We show that our approach can
infer and learn novel classes for open vocabulary classification in egocentric
videos and novel object detection with zero supervision. Extensive experiments
show its competitive performance on two publicly available egocentric action
recognition datasets (GTEA Gaze and GTEA Gaze+) under open-world conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SWIPENET: Object detection in noisy underwater images. (arXiv:2010.10006v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.10006">
<div class="article-summary-box-inner">
<span><p>In recent years, deep learning based object detection methods have achieved
promising performance in controlled environments. However, these methods lack
sufficient capabilities to handle underwater object detection due to these
challenges: (1) images in the underwater datasets and real applications are
blurry whilst accompanying severe noise that confuses the detectors and (2)
objects in real applications are usually small. In this paper, we propose a
novel Sample-WeIghted hyPEr Network (SWIPENET), and a robust training paradigm
named Curriculum Multi-Class Adaboost (CMA), to address these two problems at
the same time. Firstly, the backbone of SWIPENET produces multiple high
resolution and semantic-rich Hyper Feature Maps, which significantly improve
small object detection. Secondly, a novel sample-weighted detection loss
function is designed for SWIPENET, which focuses on learning high weight
samples and ignore learning low weight samples. Moreover, inspired by the human
education process that drives the learning from easy to hard concepts, we here
propose the CMA training paradigm that first trains a clean detector which is
free from the influence of noisy data. Then, based on the clean detector,
multiple detectors focusing on learning diverse noisy data are trained and
incorporated into a unified deep ensemble of strong noise immunity. Experiments
on two underwater robot picking contest datasets (URPC2017 and URPC2018) show
that the proposed SWIPENET+CMA framework achieves better accuracy in object
detection against several state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressively Volumetrized Deep Generative Models for Data-Efficient Contextual Learning of MR Image Recovery. (arXiv:2011.13913v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13913">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging (MRI) offers the flexibility to image a given
anatomic volume under a multitude of tissue contrasts. Yet, scan time
considerations put stringent limits on the quality and diversity of MRI data.
The gold-standard approach to alleviate this limitation is to recover
high-quality images from data undersampled across various dimensions, most
commonly the Fourier domain or contrast sets. A primary distinction among
recovery methods is whether the anatomy is processed per volume or per
cross-section. Volumetric models offer enhanced capture of global contextual
information, but they can suffer from suboptimal learning due to elevated model
complexity. Cross-sectional models with lower complexity offer improved
learning behavior, yet they ignore contextual information across the
longitudinal dimension of the volume. Here, we introduce a novel progressive
volumetrization strategy for generative models (ProvoGAN) that serially
decomposes complex volumetric image recovery tasks into successive
cross-sectional mappings task-optimally ordered across individual rectilinear
dimensions. ProvoGAN effectively captures global context and recovers
fine-structural details across all dimensions, while maintaining low model
complexity and improved learning behaviour. Comprehensive demonstrations on
mainstream MRI reconstruction and synthesis tasks show that ProvoGAN yields
superior performance to state-of-the-art volumetric and cross-sectional models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Graph Attention Network with Conditional Kernels for Pixel-Wise Prediction. (arXiv:2101.02843v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02843">
<div class="article-summary-box-inner">
<span><p>Multi-scale representations deeply learned via convolutional neural networks
have shown tremendous importance for various pixel-level prediction problems.
In this paper we present a novel approach that advances the state of the art on
pixel-level prediction in a fundamental aspect, i.e. structured multi-scale
features learning and fusion. In contrast to previous works directly
considering multi-scale feature maps obtained from the inner layers of a
primary CNN architecture, and simply fusing the features with weighted
averaging or concatenation, we propose a probabilistic graph attention network
structure based on a novel Attention-Gated Conditional Random Fields (AG-CRFs)
model for learning and fusing multi-scale representations in a principled
manner. In order to further improve the learning capacity of the network
structure, we propose to exploit feature dependant conditional kernels within
the deep probabilistic framework. Extensive experiments are conducted on four
publicly available datasets (i.e. BSDS500, NYUD-V2, KITTI, and Pascal-Context)
and on three challenging pixel-wise prediction problems involving both discrete
and continuous labels (i.e. monocular depth estimation, object contour
prediction, and semantic segmentation). Quantitative and qualitative results
demonstrate the effectiveness of the proposed latent AG-CRF model and the
overall probabilistic graph attention network with feature conditional kernels
for structured feature learning and pixel-wise prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Underwater Image Enhancement via Learning Water Type Desensitized Representations. (arXiv:2102.00676v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00676">
<div class="article-summary-box-inner">
<span><p>We present a novel underwater image enhancement method termed SCNet to
improve the image quality meanwhile cope with the degradation diversity caused
by the water. SCNet is based on normalization schemes across both spatial and
channel dimensions with the key idea of learning water type desensitized
features. Specifically, we apply whitening to de-correlate activations across
spatial dimensions for each instance in a mini-batch. We also eliminate
channel-wise correlation by standardizing and re-injecting the first two
moments of the activations across channels. The normalization schemes of
spatial and channel dimensions are performed at each scale of the U-Net to
obtain multi-scale representations. With such water type irrelevant encodings,
the decoder can easily reconstruct the clean signal and be unaffected by the
distortion types. Experimental results on two real-world underwater image
datasets show that our approach can successfully enhance images with diverse
water types, and achieves competitive performance in visual quality
improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Learn Once: Universal Anatomical Landmark Detection. (arXiv:2103.04657v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04657">
<div class="article-summary-box-inner">
<span><p>Detecting anatomical landmarks in medical images plays an essential role in
understanding the anatomy and planning automated processing. In recent years, a
variety of deep neural network methods have been developed to detect landmarks
automatically. However, all of those methods are unary in the sense that a
highly specialized network is trained for a single task say associated with a
particular anatomical region. In this work, for the first time, we investigate
the idea of "You Only Learn Once (YOLO)" and develop a universal anatomical
landmark detection model to realize multiple landmark detection tasks with
end-to-end training based on mixed datasets. The model consists of a local
network and a global network: The local network is built upon the idea of
universal U-Net to learn multi-domain local features and the global network is
a parallelly-duplicated sequential of dilated convolutions that extract global
features to further disambiguate the landmark locations. It is worth mentioning
that the new model design requires much fewer parameters than models with
standard convolutions to train. We evaluate our YOLO model on three X-ray
datasets of 1,588 images on the head, hand, and chest, collectively
contributing 62 landmarks. The experimental results show that our proposed
universal model behaves largely better than any previous models trained on
multiple datasets. It even beats the performance of the model that is trained
separately for every single dataset. The code is available at
https://github.com/MIRACLE-Center/YOLO_Universal_Anatomical_Landmark_Detection
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R-PointHop: A Green, Accurate, and Unsupervised Point Cloud Registration Method. (arXiv:2103.08129v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08129">
<div class="article-summary-box-inner">
<span><p>Inspired by the recent PointHop classification method, an unsupervised 3D
point cloud registration method, called R-PointHop, is proposed in this work.
R-PointHop first determines a local reference frame (LRF) for every point using
its nearest neighbors and finds local attributes. Next, R-PointHop obtains
local-to-global hierarchical features by point downsampling, neighborhood
expansion, attribute construction and dimensionality reduction steps. Thus,
point correspondences are built in hierarchical feature space using the nearest
neighbor rule. Afterwards, a subset of salient points with good correspondence
is selected to estimate the 3D transformation. The use of the LRF allows for
invariance of the hierarchical features of points with respect to rotation and
translation, thus making R-PointHop more robust at building point
correspondence, even when the rotation angles are large. Experiments are
conducted on the 3DMatch, ModelNet40, and Stanford Bunny datasets, which
demonstrate the effectiveness of R-PointHop for 3D point cloud registration.
R-PointHop's model size and training time are an order of magnitude smaller
than those of deep learning methods, and its registration errors are smaller,
making it a green and accurate solution. Our codes are available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08773">
<div class="article-summary-box-inner">
<span><p>Humans (e.g., crowdworkers) have a remarkable ability in solving different
tasks, by simply reading textual instructions that define them and looking at a
few examples. Despite the success of the conventional supervised learning on
individual datasets, such models often struggle with generalization across
tasks (e.g., a question-answering system cannot solve classification tasks). A
long-standing challenge in AI is to build a model that learns a new task by
understanding the human-readable instructions that define it. To study this, we
introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their
human-authored instructions, and 193k task instances (input-output pairs). The
instructions are obtained from crowdsourcing instructions used to create
existing NLP datasets and mapped to a unified schema. Using this meta-dataset,
we measure cross-task generalization by training models on seen tasks and
measuring generalization to the remaining unseen ones. We adopt generative
pre-trained language models to encode task-specific instructions along with
input and generate task output. Our results indicate that models benefit from
instructions when evaluated in terms of generalization to unseen tasks (19%
better for models utilizing instructions). These models, however, are far
behind an estimated performance upperbound indicating significant room for more
progress in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Explanations for Model Inversion Attacks. (arXiv:2104.12669v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12669">
<div class="article-summary-box-inner">
<span><p>The successful deployment of artificial intelligence (AI) in many domains
from healthcare to hiring requires their responsible use, particularly in model
explanations and privacy. Explainable artificial intelligence (XAI) provides
more information to help users to understand model decisions, yet this
additional knowledge exposes additional risks for privacy attacks. Hence,
providing explanation harms privacy. We study this risk for image-based model
inversion attacks and identified several attack architectures with increasing
performance to reconstruct private image data from model explanations. We have
developed several multi-modal transposed CNN architectures that achieve
significantly higher inversion performance than using the target model
prediction only. These XAI-aware inversion models were designed to exploit the
spatial knowledge in image explanations. To understand which explanations have
higher privacy risk, we analyzed how various explanation types and factors
influence inversion performance. In spite of some models not providing
explanations, we further demonstrate increased inversion performance even for
non-explainable target models by exploiting explanations of surrogate models
through attention transfer. This method first inverts an explanation from the
target prediction, then reconstructs the target image. These threats highlight
the urgent and significant privacy risks of explanations and calls attention
for new privacy preservation techniques that balance the dual-requirement for
AI explainability and privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Grounding with Transformers. (arXiv:2105.04281v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04281">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a transformer based approach for visual grounding.
Unlike previous proposal-and-rank frameworks that rely heavily on pretrained
object detectors or proposal-free frameworks that upgrade an off-the-shelf
one-stage detector by fusing textual embeddings, our approach is built on top
of a transformer encoder-decoder and is independent of any pretrained detectors
or word embedding models. Termed VGTR -- Visual Grounding with TRansformers,
our approach is designed to learn semantic-discriminative visual features under
the guidance of the textual description without harming their location ability.
This information flow enables our VGTR to have a strong capability in capturing
context-level semantics of both vision and language modalities, rendering us to
aggregate accurate visual clues implied by the description to locate the
interested object instance. Experiments show that our method outperforms
state-of-the-art proposal-free approaches by a considerable margin on five
benchmarks while maintaining fast inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Coreset Selection for Rehearsal-based Continual Learning. (arXiv:2106.01085v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01085">
<div class="article-summary-box-inner">
<span><p>A dataset is a shred of crucial evidence to describe a task. However, each
data point in the dataset does not have the same potential, as some of the data
points can be more representative or informative than others. This unequal
importance among the data points may have a large impact in rehearsal-based
continual learning, where we store a subset of the training examples (coreset)
to be replayed later to alleviate catastrophic forgetting. In continual
learning, the quality of the samples stored in the coreset directly affects the
model's effectiveness and efficiency. The coreset selection problem becomes
even more important under realistic settings, such as imbalanced continual
learning or noisy data scenarios. To tackle this problem, we propose Online
Coreset Selection (OCS), a simple yet effective method that selects the most
representative and informative coreset at each iteration and trains them in an
online manner. Our proposed method maximizes the model's adaptation to a
current dataset while selecting high-affinity samples to past tasks, which
directly inhibits catastrophic forgetting. We validate the effectiveness of our
coreset selection mechanism over various standard, imbalanced, and noisy
datasets against strong continual learning baselines, demonstrating that it
improves task adaptation and prevents catastrophic forgetting in a
sample-efficient manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations. (arXiv:2106.01548v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01548">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViTs) and MLPs signal further efforts on replacing
hand-wired features or inductive biases with general-purpose neural
architectures. Existing works empower the models by massive data, such as
large-scale pre-training and/or repeated strong data augmentations, and still
report optimization-related problems (e.g., sensitivity to initialization and
learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the
lens of loss geometry, intending to improve the models' data efficiency at
training and generalization at inference. Visualization and Hessian reveal
extremely sharp local minima of converged models. By promoting smoothness with
a recently proposed sharpness-aware optimizer, we substantially improve the
accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning
supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and
+11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively,
with the simple Inception-style preprocessing). We show that the improved
smoothness attributes to sparser active neurons in the first few layers. The
resultant ViTs outperform ResNets of similar size and throughput when trained
from scratch on ImageNet without large-scale pre-training or strong data
augmentations. Model checkpoints are available at
\url{https://github.com/google-research/vision_transformer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Memorization in Adversarial Training. (arXiv:2106.01606v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01606">
<div class="article-summary-box-inner">
<span><p>Deep learning models have a propensity for fitting the entire training set
even with random labels, which requires memorization of every training sample.
In this paper, we explore the memorization effect in adversarial training (AT)
for promoting a deeper understanding of model capacity, convergence,
generalization, and especially robust overfitting of the adversarially trained
models. We first demonstrate that deep networks have sufficient capacity to
memorize adversarial examples of training data with completely random labels,
but not all AT algorithms can converge under the extreme circumstance. Our
study of AT with random labels motivates further analyses on the convergence
and generalization of AT. We find that some AT approaches suffer from a
gradient instability issue and most recently suggested complexity measures
cannot explain robust generalization by considering models trained on random
labels. Furthermore, we identify a significant drawback of memorization in AT
that it could result in robust overfitting. We then propose a new mitigation
algorithm motivated by detailed memorization analyses. Extensive experiments on
various datasets validate the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental False Negative Detection for Contrastive Learning. (arXiv:2106.03719v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03719">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has recently shown great potential in vision tasks
through contrastive learning, which aims to discriminate each image, or
instance, in the dataset. However, such instance-level learning ignores the
semantic relationship among instances and sometimes undesirably repels the
anchor from the semantically similar samples, termed as "false negatives". In
this work, we show that the unfavorable effect from false negatives is more
significant for the large-scale datasets with more semantic concepts. To
address the issue, we propose a novel self-supervised contrastive learning
framework that incrementally detects and explicitly removes the false negative
samples. Specifically, following the training process, our method dynamically
detects increasing high-quality false negatives considering that the encoder
gradually improves and the embedding space becomes more semantically
structural. Next, we discuss two strategies to explicitly remove the detected
false negatives during contrastive learning. Extensive experiments show that
our framework outperforms other self-supervised contrastive learning methods on
multiple benchmarks in a limited resource setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResIST: Layer-Wise Decomposition of ResNets for Distributed Training. (arXiv:2107.00961v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00961">
<div class="article-summary-box-inner">
<span><p>We propose ResIST, a novel distributed training protocol for Residual
Networks (ResNets). ResIST randomly decomposes a global ResNet into several
shallow sub-ResNets that are trained independently in a distributed manner for
several local iterations, before having their updates synchronized and
aggregated into the global model. In the next round, new sub-ResNets are
randomly generated and the process repeats until convergence. By construction,
per iteration, ResIST communicates only a small portion of network parameters
to each machine and never uses the full model during training. Thus, ResIST
reduces the per-iteration communication, memory, and time requirements of
ResNet training to only a fraction of the requirements of full-model training.
In comparison to common protocols, like data-parallel training and
data-parallel training with local SGD, ResIST yields a decrease in
communication and compute requirements, while being competitive with respect to
model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Score-Based Point Cloud Denoising (Learning Implicit Gradient Fields for Point Cloud Denoising). (arXiv:2107.10981v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10981">
<div class="article-summary-box-inner">
<span><p>Point clouds acquired from scanning devices are often perturbed by noise,
which affects downstream tasks such as surface reconstruction and analysis. The
distribution of a noisy point cloud can be viewed as the distribution of a set
of noise-free samples $p(x)$ convolved with some noise model $n$, leading to
$(p * n)(x)$ whose mode is the underlying clean surface. To denoise a noisy
point cloud, we propose to increase the log-likelihood of each point from $p *
n$ via gradient ascent -- iteratively updating each point's position. Since $p
* n$ is unknown at test-time, and we only need the score (i.e., the gradient of
the log-probability function) to perform gradient ascent, we propose a neural
network architecture to estimate the score of $p * n$ given only noisy point
clouds as input. We derive objective functions for training the network and
develop a denoising algorithm leveraging on the estimated scores. Experiments
demonstrate that the proposed model outperforms state-of-the-art methods under
a variety of noise models, and shows the potential to be applied in other tasks
such as point cloud upsampling. The code is available at
\url{https://github.com/luost26/score-denoise}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13093">
<div class="article-summary-box-inner">
<span><p>Classifying and analyzing human cells is a lengthy procedure, often involving
a trained professional. In an attempt to expedite this process, an active area
of research involves automating cell classification through use of deep
learning-based techniques. In practice, a large amount of data is required to
accurately train these deep learning models. However, due to the sparse human
cell datasets currently available, the performance of these models is typically
low. This study investigates the feasibility of using few-shot learning-based
techniques to mitigate the data requirements for accurate training. The study
is comprised of three parts: First, current state-of-the-art few-shot learning
techniques are evaluated on human cell classification. The selected techniques
are trained on a non-medical dataset and then tested on two out-of-domain,
human cell datasets. The results indicate that, overall, the test accuracy of
state-of-the-art techniques decreased by at least 30% when transitioning from a
non-medical dataset to a medical dataset. Second, this study evaluates the
potential benefits, if any, to varying the backbone architecture and training
schemes in current state-of-the-art few-shot learning techniques when used in
human cell classification. Even with these variations, the overall test
accuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the
medical datasets. Third, this study presents future directions for using
few-shot learning in human cell classification. In general, few-shot learning
in its current state performs poorly on human cell classification. The study
proves that attempts to modify existing network architectures are not effective
and concludes that future research effort should be focused on improving
robustness towards out-of-domain testing using optimization-based or
self-supervised few-shot learning techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WSDesc: Weakly Supervised 3D Local Descriptor Learning for Point Cloud Registration. (arXiv:2108.02740v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02740">
<div class="article-summary-box-inner">
<span><p>In this work, we present a novel method called WSDesc to learn 3D local
descriptors in a weakly supervised manner for robust point cloud registration.
Our work builds upon recent 3D CNN-based descriptor extractors, which leverage
a voxel-based representation to parameterize local geometry of 3D points.
Instead of using a predefined fixed-size local support in voxelization, we
propose to learn the optimal support in a data-driven manner. To this end, we
design a novel differentiable voxelization layer that can back-propagate the
gradient to the support size optimization. To train the extracted descriptors,
we propose a novel registration loss based on the deviation from rigidity of 3D
transformations, and the loss is weakly supervised by the prior knowledge that
the input point clouds have partial overlap, without requiring ground-truth
alignment information. Through extensive experiments, we show that our learned
descriptors yield superior performance on existing geometric registration
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Full-resolution quality assessment for pansharpening. (arXiv:2108.06144v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06144">
<div class="article-summary-box-inner">
<span><p>A reliable quality assessment procedure for pansharpening methods is of
critical importance for the development of the related solutions.
Unfortunately, the lack of ground-truths to be used as guidance for an
objective evaluation has pushed the community to resort to reference-based
reduced-resolution indexes to assess synthesis ability and to no-reference
subjective quality indexes to be applied on full-resolution datasets to assess
spectral and spatial consistency. Both reference and no-reference indexes
present critical shortcomings which motivate the community to explore new
solutions. In this work, we propose an alternative no-reference full-resolution
assessment framework. On one side we introduce a protocol, namely the
reprojection protocol, to take care of the spectral consistency issue. On the
other side, a new index of the spatial consistency between the pansharpened
image and the panchromatic band at full resolution is also proposed.
Experimental results carried out on different datasets/sensors demonstrate the
effectiveness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards unconstrained joint hand-object reconstruction from RGB videos. (arXiv:2108.07044v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07044">
<div class="article-summary-box-inner">
<span><p>Our work aims to obtain 3D reconstruction of hands and manipulated objects
from monocular videos. Reconstructing hand-object manipulations holds a great
potential for robotics and learning from human demonstrations. The supervised
learning approach to this problem, however, requires 3D supervision and remains
limited to constrained laboratory settings and simulators for which 3D ground
truth is available. In this paper we first propose a learning-free fitting
approach for hand-object reconstruction which can seamlessly handle two-hand
object interactions. Our method relies on cues obtained with common methods for
object detection, hand pose estimation and instance segmentation. We
quantitatively evaluate our approach and show that it can be applied to
datasets with varying levels of difficulty for which training data is
unavailable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12056">
<div class="article-summary-box-inner">
<span><p>Existing machines are functionally specific tools that were made for easy
prediction and control. Tomorrow's machines may be closer to biological systems
in their mutability, resilience, and autonomy. But first they must be capable
of learning, and retaining, new information without repeated exposure to it.
Past efforts to engineer such systems have sought to build or regulate
artificial neural networks using task-specific modules with constrained
circumstances of application. This has not yet enabled continual learning over
long sequences of previously unseen data without corrupting existing knowledge:
a problem known as catastrophic forgetting. In this paper, we introduce a
system that can learn sequentially over previously unseen datasets (ImageNet,
CIFAR-100) with little forgetting over time. This is accomplished by regulating
the activity of weights in a convolutional neural network on the basis of
inputs using top-down modulation generated by a second feed-forward neural
network. We find that our method learns continually under domain transfer with
sparse bursts of activity in weights that are recycled across tasks, rather
than by maintaining task-specific modules. Sparse synaptic bursting is found to
balance enhanced and diminished activity in a way that facilitates adaptation
to new inputs without corrupting previously acquired functions. This behavior
emerges during a prior meta-learning phase in which regulated synapses are
selectively disinhibited, or grown, from an initial state of uniform
suppression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unpaired Deep Image Deraining Using Dual Contrastive Learning. (arXiv:2109.02973v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02973">
<div class="article-summary-box-inner">
<span><p>Learning single image deraining (SID) networks from an unpaired set of clean
and rainy images is practical and valuable as acquiring paired real-world data
is almost infeasible. However, without the paired data as the supervision,
learning a SID network is challenging. Moreover, simply using existing unpaired
learning methods (e.g., unpaired adversarial learning and cycle-consistency
constraints) in the SID task is insufficient to learn the underlying
relationship from rainy inputs to clean outputs as there exists significant
domain gap between the rainy and clean images. In this paper, we develop an
effective unpaired SID adversarial framework which explores mutual properties
of the unpaired exemplars by a dual contrastive learning manner in a deep
feature space, named as DCD-GAN. The proposed method mainly consists of two
cooperative branches: Bidirectional Translation Branch (BTB) and Contrastive
Guidance Branch (CGB). Specifically, BTB exploits full advantage of the
circulatory architecture of adversarial consistency to generate abundant
exemplar pairs and excavates latent feature distributions between two domains
by equipping it with bidirectional mapping. Simultaneously, CGB implicitly
constrains the embeddings of different exemplars in the deep feature space by
encouraging the similar feature distributions closer while pushing the
dissimilar further away, in order to better facilitate rain removal and help
image restoration. Extensive experiments demonstrate that our method performs
favorably against existing unpaired deraining approaches on both synthetic and
real-world datasets, and generates comparable results against several
fully-supervised or semi-supervised models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. (arXiv:2109.04275v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04275">
<div class="article-summary-box-inner">
<span><p>Despite the potential of multi-modal pre-training to learn highly
discriminative feature representations from complementary data modalities,
current progress is being slowed by the lack of large-scale modality-diverse
datasets. By leveraging the natural suitability of E-commerce, where different
modalities capture complementary semantic information, we contribute a
large-scale multi-modal pre-training dataset M5Product. The dataset comprises 5
modalities (image, text, table, video, and audio), covers over 6,000 categories
and 5,000 attributes, and is 500 larger than the largest publicly available
dataset with a similar number of modalities. Furthermore, M5Product contains
incomplete modality pairs and noise while also having a long-tailed
distribution, resembling most real-world problems. We further propose
Self-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework
that integrates the different modalities into a unified model through an
adaptive feature fusion mechanism, where the importance of each modality is
learned directly from the modality embeddings and impacts the inter-modality
contrastive learning and masked tasks within a multi-modal transformer model.
We evaluate the current multi-modal pre-training state-of-the-art approaches
and benchmark their ability to learn from unlabeled data when faced with the
large number of modalities in the M5Product dataset. We conduct extensive
experiments on four downstream tasks and demonstrate the superiority of our
SCALE model, providing insights into the importance of dataset scale and
diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrated Construction of Multimodal Atlases with Structural Connectomes in the Space of Riemannian Metrics. (arXiv:2109.09808v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09808">
<div class="article-summary-box-inner">
<span><p>The structural network of the brain, or structural connectome, can be
represented by fiber bundles generated by a variety of tractography methods.
While such methods give qualitative insights into brain structure, there is
controversy over whether they can provide quantitative information, especially
at the population level. In order to enable population-level statistical
analysis of the structural connectome, we propose representing a connectome as
a Riemannian metric, which is a point on an infinite-dimensional manifold. We
equip this manifold with the Ebin metric, a natural metric structure for this
space, to get a Riemannian manifold along with its associated geometric
properties. We then use this Riemannian framework to apply object-oriented
statistical analysis to define an atlas as the Fr\'echet mean of a population
of Riemannian metrics. This formulation ties into the existing framework for
diffeomorphic construction of image atlases, allowing us to construct a
multimodal atlas by simultaneously integrating complementary white matter
structure details from DWMRI and cortical details from T1-weighted MRI. We
illustrate our framework with 2D data examples of connectome registration and
atlas formation. Finally, we build an example 3D multimodal atlas using T1
images and connectomes derived from diffusion tensors estimated from a subset
of subjects from the Human Connectome Project.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Dense Video Grounding via Parallel Regression. (arXiv:2109.11265v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11265">
<div class="article-summary-box-inner">
<span><p>Video grounding aims to localize the corresponding video moment in an
untrimmed video given a language query. Existing methods often address this
task in an indirect way, by casting it as a proposal-and-match or
fusion-and-detection problem. Solving these surrogate problems often requires
sophisticated label assignment during training and hand-crafted removal of
near-duplicate results. Meanwhile, existing works typically focus on sparse
video grounding with a single sentence as input, which could result in
ambiguous localization due to its unclear description. In this paper, we tackle
a new problem of dense video grounding, by simultaneously localizing multiple
moments with a paragraph as input. From a perspective on video grounding as
language conditioned regression, we present an end-to-end parallel decoding
paradigm by re-purposing a Transformer-alike architecture (PRVG). The key
design in our PRVG is to use languages as queries, and directly regress the
moment boundaries based on language-modulated visual representations. Thanks to
its simplicity in design, our PRVG framework can be applied in different
testing schemes (sparse or dense grounding) and allows for efficient inference
without any post-processing technique. In addition, we devise a robust
proposal-level attention loss to guide the training of PRVG, which is invariant
to moment duration and contributes to model convergence. We perform experiments
on two video grounding benchmarks of ActivityNet Captions and TACoS,
demonstrating that our PRVG can significantly outperform previous methods. We
also perform in-depth studies to investigate the effectiveness of parallel
regression paradigm on video grounding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bringing Generalization to Deep Multi-View Pedestrian Detection. (arXiv:2109.12227v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12227">
<div class="article-summary-box-inner">
<span><p>Multi-view Detection (MVD) is highly effective for occlusion reasoning in a
crowded environment. While recent works using deep learning have made
significant advances in the field, they have overlooked the generalization
aspect, which makes them impractical for real-world deployment. The key novelty
of our work is to formalize three critical forms of generalization and propose
experiments to evaluate them: generalization with i) a varying number of
cameras, ii) varying camera positions, and finally, iii) to new scenes. We find
that existing state-of-the-art models show poor generalization by overfitting
to a single scene and camera configuration. To address the concerns: (a) we
propose a novel Generalized MVD (GMVD) dataset, assimilating diverse scenes
with changing daytime, camera configurations, varying number of cameras, and
(b) we discuss the properties essential to bring generalization to MVD and
propose a barebones model to incorporate them. We perform a comprehensive set
of experiments on the WildTrack, MultiViewX, and the GMVD datasets to motivate
the necessity to evaluate the generalization abilities of MVD methods and to
demonstrate the efficacy of the proposed approach. The code and the proposed
dataset can be found at https://github.com/jeetv/GMVD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion-aware Contrastive Video Representation Learning via Foreground-background Merging. (arXiv:2109.15130v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15130">
<div class="article-summary-box-inner">
<span><p>In light of the success of contrastive learning in the image domain, current
self-supervised video representation learning methods usually employ
contrastive loss to facilitate video representation learning. When naively
pulling two augmented views of a video closer, the model however tends to learn
the common static background as a shortcut but fails to capture the motion
information, a phenomenon dubbed as background bias. Such bias makes the model
suffer from weak generalization ability, leading to worse performance on
downstream tasks such as action recognition. To alleviate such bias, we propose
\textbf{F}oreground-b\textbf{a}ckground \textbf{Me}rging (FAME) to deliberately
compose the moving foreground region of the selected video onto the static
background of others. Specifically, without any off-the-shelf detector, we
extract the moving foreground out of background regions via the frame
difference and color statistics, and shuffle the background regions among the
videos. By leveraging the semantic consistency between the original clips and
the fused ones, the model focuses more on the motion patterns and is debiased
from the background shortcut. Extensive experiments demonstrate that FAME can
effectively resist background cheating and thus achieve the state-of-the-art
performance on downstream tasks across UCF101, HMDB51, and Diving48 datasets.
The code and configurations are released at https://github.com/Mark12Ding/FAME.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Sparse Masks for Diffusion-based Image Inpainting. (arXiv:2110.02636v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02636">
<div class="article-summary-box-inner">
<span><p>Diffusion-based inpainting is a powerful tool for the reconstruction of
images from sparse data. Its quality strongly depends on the choice of known
data. Optimising their spatial location -- the inpainting mask -- is
challenging. A commonly used tool for this task are stochastic optimisation
strategies. However, they are slow as they compute multiple inpainting results.
We provide a remedy in terms of a learned mask generation model. By emulating
the complete inpainting pipeline with two networks for mask generation and
neural surrogate inpainting, we obtain a model for highly efficient adaptive
mask generation. Experiments indicate that our model can achieve competitive
quality with an acceleration by as much as four orders of magnitude. Our
findings serve as a basis for making diffusion-based inpainting more attractive
for applications such as image compression, where fast encoding is highly
desirable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPSN: Motion-aware Pseudo Siamese Network for Indoor Video Head Detection in Buildings. (arXiv:2110.03302v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03302">
<div class="article-summary-box-inner">
<span><p>Head detection in the indoor video is an essential component of building
occupancy detection. While deep models have achieved remarkable progress in
general object detection, they are not satisfying enough in complex indoor
scenes. The indoor surveillance video often includes cluttered background
objects, among which heads have small scales and diverse poses. In this paper,
we propose Motion-aware Pseudo Siamese Network (MPSN), an end-to-end approach
that leverages head motion information to guide the deep model to extract
effective head features in indoor scenarios. By taking the pixel-wise
difference of adjacent frames as the auxiliary input, MPSN effectively enhances
human head motion information and removes the irrelevant objects in the
background. Compared with prior methods, it achieves superior performance on
the two indoor video datasets. Our experiments show that MPSN successfully
suppresses static background objects and highlights the moving instances,
especially human heads in indoor videos. We also compare different methods to
capture head motion, which demonstrates the simplicity and flexibility of MPSN.
Finally, to validate the robustness of MPSN, we conduct adversarial experiments
with a mathematical solution of small perturbations for robust model selection.
Code is available at https://github.com/pl-share/MPSN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm. (arXiv:2110.05208v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05208">
<div class="article-summary-box-inner">
<span><p>Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has
attracted unprecedented attention for its impressive zero-shot recognition
ability and excellent transferability to downstream tasks. However, CLIP is
quite data-hungry and requires 400M image-text pairs for pre-training, thereby
restricting its adoption. This work proposes a novel training paradigm, Data
efficient CLIP (DeCLIP), to alleviate this limitation. We demonstrate that by
carefully utilizing the widespread supervision among the image-text pairs, our
De-CLIP can learn generic visual features more efficiently. Instead of using
the single image-text contrastive supervision, we fully exploit data potential
through the use of (1) self-supervision within each modality; (2) multi-view
supervision across modalities; (3) nearest-neighbor supervision from other
similar pairs. Benefiting from intrinsic supervision, our DeCLIP-ResNet50 can
achieve 60.4% zero-shot top1 accuracy on ImageNet, which is 0.8% above the
CLIP-ResNet50 while using 7.1 x fewer data. Our DeCLIP-ResNet50 outperforms its
counterpart in 8 out of 11 visual datasets when transferred to downstream
tasks. Moreover, Scaling up the model and computing also works well in our
framework.Our code, dataset and models are released at:
https://github.com/Sense-GVT/DeCLIP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Supervised Pre-training for Better Downstream Transferring. (arXiv:2110.06014v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06014">
<div class="article-summary-box-inner">
<span><p>The pretrain-finetune paradigm has shown outstanding performance on many
applications of deep learning, where a model is pre-trained on a upstream large
dataset (e.g. ImageNet), and is then fine-tuned to different downstream tasks.
Though for most cases, the pre-training stage is conducted based on supervised
methods, recent works on self-supervised pre-training have shown powerful
transferability and even outperform supervised pre-training on multiple
downstream tasks. It thus remains an open question how to better generalize
supervised pre-training model to downstream tasks. In this paper, we argue that
the worse transferability of existing supervised pre-training methods arise
from the negligence of valuable intra-class semantic difference. This is
because these methods tend to push images from the same class close to each
other despite of the large diversity in their visual contents, a problem to
which referred as "overfit of upstream tasks". To alleviate this problem, we
propose a new supervised pre-training method based on Leave-One-Out
K-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting
upstream tasks by only requiring each image to share its class label with most
of its k nearest neighbors, thus allowing each class to exhibit a multi-mode
distribution and consequentially preserving part of intra-class difference for
better transferring to downstream tasks. We developed efficient implementation
of the proposed method that scales well to large datasets. Experimental studies
on multiple downstream tasks show that LOOK outperforms other state-of-the-art
methods for supervised and self-supervised pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ego4D: Around the World in 3,000 Hours of Egocentric Video. (arXiv:2110.07058v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07058">
<div class="article-summary-box-inner">
<span><p>We introduce Ego4D, a massive-scale egocentric video dataset and benchmark
suite. It offers 3,670 hours of daily-life activity video spanning hundreds of
scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique
camera wearers from 74 worldwide locations and 9 different countries. The
approach to collection is designed to uphold rigorous privacy and ethics
standards with consenting participants and robust de-identification procedures
where relevant. Ego4D dramatically expands the volume of diverse egocentric
video footage publicly available to the research community. Portions of the
video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo,
and/or synchronized videos from multiple egocentric cameras at the same event.
Furthermore, we present a host of new benchmark challenges centered around
understanding the first-person visual experience in the past (querying an
episodic memory), present (analyzing hand-object manipulation, audio-visual
conversation, and social interactions), and future (forecasting activities). By
publicly sharing this massive annotated dataset and benchmark suite, we aim to
push the frontier of first-person perception. Project page:
https://ego4d-data.org/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Semantic Segmentation by Pixel-to-Prototype Contrast. (arXiv:2110.07110v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07110">
<div class="article-summary-box-inner">
<span><p>Though image-level weakly supervised semantic segmentation (WSSS) has
achieved great progress with Class Activation Maps (CAMs) as the cornerstone,
the large supervision gap between classification and segmentation still hampers
the model to generate more complete and precise pseudo masks for segmentation.
In this study, we propose weakly-supervised pixel-to-prototype contrast that
can provide pixel-level supervisory signals to narrow the gap. Guided by two
intuitive priors, our method is executed across different views and within per
single view of an image, aiming to impose cross-view feature semantic
consistency regularization and facilitate intra(inter)-class
compactness(dispersion) of the feature space. Our method can be seamlessly
incorporated into existing WSSS models without any changes to the base networks
and does not incur any extra inference burden. Extensive experiments manifest
that our method consistently improves two strong baselines by large margins,
demonstrating the effectiveness. Specifically, built on top of SEAM, we improve
the initial seed mIoU on PASCAL VOC 2012 from 55.4% to 61.5%. Moreover, armed
with our method, we increase the segmentation mIoU of EPS from 70.8% to 73.6%,
achieving new state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Translation using Texture Co-occurrence and Spatial Self-Similarity for Texture Debiasing. (arXiv:2110.07920v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07920">
<div class="article-summary-box-inner">
<span><p>Classification models trained on datasets with texture bias usually perform
poorly on out-of-distribution samples since biased representations are embedded
into the model. Recently, various debiasing methods have attempted to
disentangle biased representations, but discarding texture biased features
without altering other relevant information is still a challenging task. In
this paper, we propose a novel texture debiasing approach to generate
additional training images using the content of a source image and the texture
of a target image with a different semantic label to explicitly mitigate
texture biases when training a classifier. Our model ensures texture similarity
between the target and generated images via a texture co-occurrence loss while
preserving the content details from the source image with a spatial
self-similarity loss. Both the generated and original training images are
combined to train an improved classifier that is robust against inconsistent
texture bias representations. We employ five datasets with known texture biases
to demonstrate the ability of our method to mitigate texture bias. In all
cases, our method outperformed existing state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal-Boost: Multimodal Medical Image Super-Resolution using Multi-Attention Network with Wavelet Transform. (arXiv:2110.11684v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11684">
<div class="article-summary-box-inner">
<span><p>Deep learning based single image super resolution (SISR) algorithms has
revolutionized the overall diagnosis framework by continually improving the
architectural components and training strategies associated with convolutional
neural networks (CNN) on low-resolution images. However, existing work lacks in
two ways: i) the SR output produced exhibits poor texture details, and often
produce blurred edges, ii) most of the models have been developed for a single
modality, hence, require modification to adapt to a new one. This work
addresses (i) by proposing generative adversarial network (GAN) with deep
multi-attention modules to learn high-frequency information from low-frequency
data. Existing approaches based on the GAN have yielded good SR results;
however, the texture details of their SR output have been experimentally
confirmed to be deficient for medical images particularly. The integration of
wavelet transform (WT) and GANs in our proposed SR model addresses the
aforementioned limitation concerning textons. While the WT divides the LR image
into multiple frequency bands, the transferred GAN uses multi-attention and
upsample blocks to predict high-frequency components. Additionally, we present
a learning method for training domain-specific classifiers as perceptual loss
functions. Using a combination of multi-attention GAN loss and a perceptual
loss function results in an efficient and reliable performance. Applying the
same model for medical images from diverse modalities is challenging, our work
addresses (ii) by training and performing on several modalities via transfer
learning. Using two medical datasets, we validate our proposed SR network
against existing state-of-the-art approaches and achieve promising results in
terms of SSIM and PSNR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Longitudinal Analysis of Mask and No-Mask on Child Face Recognition. (arXiv:2111.00121v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00121">
<div class="article-summary-box-inner">
<span><p>Face is one of the most widely employed traits for person recognition, even
in many large-scale applications. Despite technological advancements in face
recognition systems, they still face obstacles caused by pose, expression,
occlusion, and aging variations. Owing to the COVID-19 pandemic, contactless
identity verification has become exceedingly vital. Recently, few studies have
been conducted on the effect of face mask on adult face recognition systems
(FRS). However, the impact of aging with face mask on child subject recognition
has not been adequately explored. Thus, the main objective of this study is
analyzing the child longitudinal impact together with face mask and other
covariates on FRS. Specifically, we performed a comparative investigation of
three top performing publicly available face matchers and a post-COVID-19
commercial-off-the-shelf (COTS) system under child cross-age verification and
identification settings using our generated synthetic mask and no-mask samples.
Furthermore, we investigated the longitudinal consequence of eyeglasses with
mask and no-mask. The study exploited no-mask longitudinal child face dataset
(i.e., extended Indian Child Longitudinal Face Dataset) that contains 26,258
face images of 7,473 subjects in the age group of [2, 18] over an average time
span of 3.35 years. Due to the combined effects of face mask and face aging,
the FaceNet, PFE, ArcFace, and COTS face verification system accuracies
decrease approximately 25%, 22%, 18%, 12%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation. (arXiv:2111.08557v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08557">
<div class="article-summary-box-inner">
<span><p>In keypoint estimation tasks such as human pose estimation, heatmap-based
regression is the dominant approach despite possessing notable drawbacks:
heatmaps intrinsically suffer from quantization error and require excessive
computation to generate and post-process. Motivated to find a more efficient
solution, we propose to model individual keypoints and sets of spatially
related keypoints (i.e., poses) as objects within a dense single-stage
anchor-based detection framework. Hence, we call our method KAPAO (pronounced
"Ka-Pow"), for Keypoints And Poses As Objects. KAPAO is applied to the problem
of single-stage multi-person human pose estimation by simultaneously detecting
human pose and keypoint objects and fusing the detections to exploit the
strengths of both object representations. In experiments, we observe that KAPAO
is faster and more accurate than previous methods, which suffer greatly from
heatmap post-processing. The accuracy-speed trade-off is especially favourable
in the practical setting when not using test-time augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection. (arXiv:2111.09099v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09099">
<div class="article-summary-box-inner">
<span><p>Anomaly detection is commonly pursued as a one-class classification problem,
where models can only learn from normal training samples, while being evaluated
on both normal and abnormal test samples. Among the successful approaches for
anomaly detection, a distinguished category of methods relies on predicting
masked information (e.g. patches, future frames, etc.) and leveraging the
reconstruction error with respect to the masked information as an abnormality
score. Different from related methods, we propose to integrate the
reconstruction-based functionality into a novel self-supervised predictive
architectural building block. The proposed self-supervised block is generic and
can easily be incorporated into various state-of-the-art anomaly detection
methods. Our block starts with a convolutional layer with dilated filters,
where the center area of the receptive field is masked. The resulting
activation maps are passed through a channel attention module. Our block is
equipped with a loss that minimizes the reconstruction error with respect to
the masked area in the receptive field. We demonstrate the generality of our
block by integrating it into several state-of-the-art frameworks for anomaly
detection on image and video, providing empirical evidence that shows
considerable performance improvements on MVTec AD, Avenue, and ShanghaiTech. We
release our code as open source at https://github.com/ristea/sspcab.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v8 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for text-to-image and image-to-text
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation tasks without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial results of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition. (arXiv:2111.12994v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12994">
<div class="article-summary-box-inner">
<span><p>Recently, Vision Transformers (ViT), with the self-attention (SA) as the de
facto ingredients, have demonstrated great potential in the computer vision
community. For the sake of trade-off between efficiency and performance, a
group of works merely perform SA operation within local patches, whereas the
global contextual information is abandoned, which would be indispensable for
visual recognition tasks. To solve the issue, the subsequent global-local ViTs
take a stab at marrying local SA with global one in parallel or alternative way
in the model. Nevertheless, the exhaustively combined local and global context
may exist redundancy for various visual data, and the receptive field within
each layer is fixed. Alternatively, a more graceful way is that global and
local context can adaptively contribute per se to accommodate different visual
data. To achieve this goal, we in this paper propose a novel ViT architecture,
termed NomMer, which can dynamically Nominate the synergistic global-local
context in vision transforMer. By investigating the working pattern of our
proposed NomMer, we further explore what context information is focused.
Beneficial from this "dynamic nomination" mechanism, without bells and
whistles, the NomMer can not only achieve 84.5% Top-1 classification accuracy
on ImageNet with only 73M parameters, but also show promising performance on
dense prediction tasks, i.e., object detection and semantic segmentation. The
code and models will be made publicly available at
https://github.com/TencentYoutuResearch/VisualRecognition-NomMer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Real Image Super-resolution via Distortion-Relation Guided Transfer Learning. (arXiv:2111.13078v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13078">
<div class="article-summary-box-inner">
<span><p>Collecting large clean-distorted training image pairs in real world is
non-trivial, which seriously limits the practical applications of these
supervised learning based image super-resolution (SR) methods. Previous works
attempt to address this problem by leveraging unsupervised learning
technologies to alleviate the dependency for paired training samples. However,
these methods typically suffer from unsatisfactory textures synthesis due to
the lack of clean image supervision. Compared with purely unsupervised
solution, the under-explored scheme with Few-Shot clean images (FS-RSR) is more
feasible to tackle this challenging Real Image Super-Resolution task. In this
paper, we are the first to investigate the few-shot real image super-resolution
and propose a Distortion-Relation guided Transfer Learning (termed as DRTL)
framework. DRTL assigns a knowledge graph to capture the distortion relation
between auxiliary tasks (i.e., synthetic distortions) and target tasks (i.e.,
real distortions with few images), and then adopt a gradient weighting strategy
to guide the knowledge transfer from auxiliary task to target task. In this
way, DRTL could quickly learn the most relevant knowledge from the prior
distortions for target distortion. We instantiate DRTL integrated with
pre-training and meta-learning pipelines as an embodiment to realize a
distortion-relation aware FS-RSR. Extensive experiments on multiple benchmarks
demonstrate the effectiveness of DRTL on few-shot real image super-resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Integration of Self-Attention and Convolution. (arXiv:2111.14556v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14556">
<div class="article-summary-box-inner">
<span><p>Convolution and self-attention are two powerful techniques for representation
learning, and they are usually considered as two peer approaches that are
distinct from each other. In this paper, we show that there exists a strong
underlying relation between them, in the sense that the bulk of computations of
these two paradigms are in fact done with the same operation. Specifically, we
first show that a traditional convolution with kernel size k x k can be
decomposed into k^2 individual 1x1 convolutions, followed by shift and
summation operations. Then, we interpret the projections of queries, keys, and
values in self-attention module as multiple 1x1 convolutions, followed by the
computation of attention weights and aggregation of the values. Therefore, the
first stage of both two modules comprises the similar operation. More
importantly, the first stage contributes a dominant computation complexity
(square of the channel size) comparing to the second stage. This observation
naturally leads to an elegant integration of these two seemingly distinct
paradigms, i.e., a mixed model that enjoys the benefit of both self-Attention
and Convolution (ACmix), while having minimum computational overhead compared
to the pure convolution or self-attention counterpart. Extensive experiments
show that our model achieves consistently improved results over competitive
baselines on image recognition and downstream tasks. Code and pre-trained
models will be released at https://github.com/LeapLabTHU/ACmix and
https://gitee.com/mindspore/models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AssistSR: Task-oriented Question-driven Video Segment Retrieval. (arXiv:2111.15050v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15050">
<div class="article-summary-box-inner">
<span><p>It is still a pipe dream that AI assistants on phone and AR glasses can
assist our daily life in addressing our questions like "how to adjust the date
for this watch?" and "how to set its heating duration? (while pointing at an
oven)". The queries used in conventional tasks (i.e. Video Question Answering,
Video Retrieval, Moment Localization) are often factoid and based on pure text.
In contrast, we present a new task called Task-oriented Question-driven Video
Segment Retrieval (TQVSR). Each of our questions is an image-box-text query
that focuses on affordance of items in our daily life and expects relevant
answer segments to be retrieved from a corpus of instructional video-transcript
segments. To support the study of this TQVSR task, we construct a new dataset
called AssistSR. We design novel guidelines to create high-quality samples.
This dataset contains 1.4k multimodal questions on 1k video segments from
instructional videos on diverse daily-used items. To address TQVSR, we develop
a straightforward yet effective model called Dual Multimodal Encoders (DME)
that significantly outperforms several baseline methods while still having
large room for improvement in the future. Moreover, we present detailed
ablation analyses. Our codes and data are available
at~\url{https://github.com/StanLei52/AQVSR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AirObject: A Temporally Evolving Graph Embedding for Object Identification. (arXiv:2111.15150v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15150">
<div class="article-summary-box-inner">
<span><p>Object encoding and identification are vital for robotic tasks such as
autonomous exploration, semantic scene understanding, and re-localization.
Previous approaches have attempted to either track objects or generate
descriptors for object identification. However, such systems are limited to a
"fixed" partial object representation from a single viewpoint. In a robot
exploration setup, there is a requirement for a temporally "evolving" global
object representation built as the robot observes the object from multiple
viewpoints. Furthermore, given the vast distribution of unknown novel objects
in the real world, the object identification process must be class-agnostic. In
this context, we propose a novel temporal 3D object encoding approach, dubbed
AirObject, to obtain global keypoint graph-based embeddings of objects.
Specifically, the global 3D object embeddings are generated using a temporal
convolutional network across structural information of multiple frames obtained
from a graph attention-based encoding method. We demonstrate that AirObject
achieves the state-of-the-art performance for video object identification and
is robust to severe occlusion, perceptual aliasing, viewpoint shift,
deformation, and scale transform, outperforming the state-of-the-art
single-frame and sequential descriptors. To the best of our knowledge,
AirObject is one of the first temporal object encoding methods. Source code is
available at https://github.com/Nik-V9/AirObject.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConDA: Unsupervised Domain Adaptation for LiDAR Segmentation via Regularized Domain Concatenation. (arXiv:2111.15242v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15242">
<div class="article-summary-box-inner">
<span><p>Transferring knowledge learned from the labeled source domain to the raw
target domain for unsupervised domain adaptation (UDA) is essential to the
scalable deployment of an autonomous driving system. State-of-the-art methods
in UDA often employ a key idea: utilizing joint supervision signals from both
of the source domain (with ground-truth) and target domain (with pseudo-labels)
for self-training. In this work, we improve and extend on this aspect. We
present ConDA, a concatenation-based domain adaptation framework for LiDAR
segmentation that: 1) constructs an intermediate domain consisting of
fine-grained interchange signals from both source and target domains without
destabilizing the semantic coherency of objects and background around the
ego-vehicle; and 2) utilizes the intermediate domain for self-training. To
improve both the network training on the source domain and self-training on the
intermediate domain, we propose an anti-aliasing regularizer and an entropy
aggregator to reduce the negative effect caused by the aliasing artifacts and
noisy pseudo labels. Through extensive experiments, we demonstrate that ConDA
significantly outperforms prior arts in mitigating the domain gap not only in
UDA, but also in other DAs with minimum supervisions, such as
semi-/weakly-supervised DAs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Transferability of Supervised Pretraining: an MLP Perspective. (arXiv:2112.00496v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00496">
<div class="article-summary-box-inner">
<span><p>The pretrain-finetune paradigm is a classical pipeline in visual learning.
Recent progress on unsupervised pretraining methods shows superior transfer
performance to their supervised counterparts. This paper revisits this
phenomenon and sheds new light on understanding the transferability gap between
unsupervised and supervised pretraining from a multilayer perceptron (MLP)
perspective. While previous works focus on the effectiveness of MLP on
unsupervised image classification where pretraining and evaluation are
conducted on the same dataset, we reveal that the MLP projector is also the key
factor to better transferability of unsupervised pretraining methods than
supervised pretraining methods. Based on this observation, we attempt to close
the transferability gap between supervised and unsupervised pretraining by
adding an MLP projector before the classifier in supervised pretraining. Our
analysis indicates that the MLP projector can help retain intra-class variation
of visual features, decrease the feature distribution distance between
pretraining and evaluation datasets, and reduce feature redundancy. Extensive
experiments on public benchmarks demonstrate that the added MLP projector
significantly boosts the transferability of supervised pretraining, e.g. +7.2\%
top-1 accuracy on the concept generalization task, +5.8\% top-1 accuracy for
linear evaluation on 12-domain classification tasks, and +0.8\% AP on COCO
object detection task, making supervised pretraining comparable or even better
than unsupervised pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extrapolating from a Single Image to a Thousand Classes using Distillation. (arXiv:2112.00725v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00725">
<div class="article-summary-box-inner">
<span><p>What can neural networks learn about the visual world from a single image?
While it obviously cannot contain the multitudes of possible objects, scenes
and lighting conditions that exist - within the space of all possible
256^(3x224x224) 224-sized square images, it might still provide a strong prior
for natural images. To analyze this hypothesis, we develop a framework for
training neural networks from scratch using a single image by means of
knowledge distillation from a supervisedly pretrained teacher. With this, we
find that the answer to the above question is: 'surprisingly, a lot'. In
quantitative terms, we find top-1 accuracies of 94%/74% on CIFAR-10/100, 66% on
ImageNet, and by extending this method to video and audio, 77% on UCF-101 and
84% on SpeechCommands. In extensive analyses we disentangle the effect of
augmentations, choice of source image and network architectures and also
discover "panda neurons" in networks that have never seen a panda. This work
shows that one image can be used to extrapolate to thousands of object classes
and motivates a renewed research agenda on the fundamental interplay of
augmentations and image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Neural Representations for Direct Volume Rendering. (arXiv:2112.01579v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01579">
<div class="article-summary-box-inner">
<span><p>Despite the potential of neural scene representations to effectively compress
3D scalar fields at high reconstruction quality, the computational complexity
of the training and data reconstruction step using scene representation
networks limits their use in practical applications. In this paper, we analyze
whether scene representation networks can be modified to reduce these
limitations and whether such architectures can also be used for temporal
reconstruction tasks. We propose a novel design of scene representation
networks using GPU tensor cores to integrate the reconstruction seamlessly into
on-chip raytracing kernels, and compare the quality and performance of this
network to alternative network- and non-network-based compression schemes. The
results indicate competitive quality of our design at high compression rates,
and significantly faster decoding times and lower memory consumption during
data reconstruction. We investigate how density gradients can be computed using
the network and show an extension where density, gradient and curvature are
predicted jointly. As an alternative to spatial super-resolution approaches for
time-varying fields, we propose a solution that builds upon latent-space
interpolation to enable random access reconstruction at arbitrary granularity.
We summarize our findings in the form of an assessment of the strengths and
limitations of scene representation networks \changed{for compression domain
volume rendering, and outline future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Data Augmentation Using Feature Interpolation for Low-Shot Image Generation. (arXiv:2112.02450v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02450">
<div class="article-summary-box-inner">
<span><p>Training of generative models especially Generative Adversarial Networks can
easily diverge in low-data setting. To mitigate this issue, we propose a novel
implicit data augmentation approach which facilitates stable training and
synthesize high-quality samples without need of label information.
Specifically, we view the discriminator as a metric embedding of the real data
manifold, which offers proper distances between real data points. We then
utilize information in the feature space to develop a data-driven augmentation
method. Experiments on few-shot generation tasks show the proposed method
significantly improve results from strong baselines, and allows generating
high-quality images with around 100 training samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification-Then-Grounding: Reformulating Video Scene Graphs as Temporal Bipartite Graphs. (arXiv:2112.04222v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04222">
<div class="article-summary-box-inner">
<span><p>Today's VidSGG models are all proposal-based methods, i.e., they first
generate numerous paired subject-object snippets as proposals, and then conduct
predicate classification for each proposal. In this paper, we argue that this
prevalent proposal-based framework has three inherent drawbacks: 1) The
ground-truth predicate labels for proposals are partially correct. 2) They
break the high-order relations among different predicate instances of a same
subject-object pair. 3) VidSGG performance is upper-bounded by the quality of
the proposals. To this end, we propose a new classification-then-grounding
framework for VidSGG, which can avoid all the three overlooked drawbacks.
Meanwhile, under this framework, we reformulate the video scene graphs as
temporal bipartite graphs, where the entities and predicates are two types of
nodes with time slots, and the edges denote different semantic roles between
these nodes. This formulation takes full advantage of our new framework.
Accordingly, we further propose a novel BIpartite Graph based SGG model: BIG.
It consists of a classification stage and a grounding stage, where the former
aims to classify the categories of all the nodes and the edges, and the latter
tries to localize the temporal location of each relation instance. Extensive
ablations on two VidSGG datasets have attested to the effectiveness of our
framework and BIG. Code is available at https://github.com/Dawn-LX/VidSGG-BIG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceFormer: Speech-Driven 3D Facial Animation with Transformers. (arXiv:2112.05329v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05329">
<div class="article-summary-box-inner">
<span><p>Speech-driven 3D facial animation is challenging due to the complex geometry
of human faces and the limited availability of 3D audio-visual data. Prior
works typically focus on learning phoneme-level features of short audio windows
with limited context, occasionally resulting in inaccurate lip movements. To
tackle this limitation, we propose a Transformer-based autoregressive model,
FaceFormer, which encodes the long-term audio context and autoregressively
predicts a sequence of animated 3D face meshes. To cope with the data scarcity
issue, we integrate the self-supervised pre-trained speech representations.
Also, we devise two biased attention mechanisms well suited to this specific
task, including the biased cross-modal multi-head (MH) attention and the biased
causal MH self-attention with a periodic positional encoding strategy. The
former effectively aligns the audio-motion modalities, whereas the latter
offers abilities to generalize to longer audio sequences. Extensive experiments
and a perceptual user study show that our approach outperforms the existing
state-of-the-arts. The code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Overlooked Classifier in Human-Object Interaction Recognition. (arXiv:2112.06392v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06392">
<div class="article-summary-box-inner">
<span><p>Human-Object Interaction (HOI) recognition is challenging due to two factors:
(1) significant imbalance across classes and (2) requiring multiple labels per
image. This paper shows that these two challenges can be effectively addressed
by improving the classifier with the backbone architecture untouched. Firstly,
we encode the semantic correlation among classes into the classification head
by initializing the weights with language embeddings of HOIs. As a result, the
performance is boosted significantly, especially for the few-shot subset.
Secondly, we propose a new loss named LSE-Sign to enhance multi-label learning
on a long-tailed dataset. Our simple yet effective method enables
detection-free HOI classification, outperforming the state-of-the-arts that
require object detection and human pose by a clear margin. Moreover, we
transfer the classification model to instance-level HOI detection by connecting
it with an off-the-shelf object detector. We achieve state-of-the-art without
additional fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena. (arXiv:2112.07566v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07566">
<div class="article-summary-box-inner">
<span><p>We propose VALSE (Vision And Language Structured Evaluation), a novel
benchmark designed for testing general-purpose pretrained vision and language
(V&amp;L) models for their visio-linguistic grounding capabilities on specific
linguistic phenomena. VALSE offers a suite of six tests covering various
linguistic constructs. Solving these requires models to ground linguistic
phenomena in the visual modality, allowing more fine-grained evaluations than
hitherto possible. We build VALSE using methods that support the construction
of valid foils, and report results from evaluating five widely-used V&amp;L models.
Our experiments suggest that current models have considerable difficulty
addressing most phenomena. Hence, we expect VALSE to serve as an important
benchmark to measure future progress of pretrained V&amp;L models from a linguistic
perspective, complementing the canonical task-centred V&amp;L evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of class imbalance on chest x-ray classifiers: towards better evaluation practices for discrimination and calibration performance. (arXiv:2112.12843v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12843">
<div class="article-summary-box-inner">
<span><p>This work aims to analyze standard evaluation practices adopted by the
research community when assessing chest x-ray classifiers, particularly
focusing on the impact of class imbalance in such appraisals. Our analysis
considers a comprehensive definition of model performance, covering not only
discriminative performance but also model calibration, a topic of research that
has received increasing attention during the last years within the machine
learning community. Firstly, we conducted a literature study to analyze common
scientific practices and confirmed that: (1) even when dealing with highly
imbalanced datasets, the community tends to use metrics that are dominated by
the majority class; and (2) it is still uncommon to include calibration studies
for chest x-ray classifiers, albeit its importance in the context of
healthcare. Secondly, we perform a systematic experiment on two major chest
x-ray datasets to explore the behavior of several performance metrics under
different class ratios and show that widely adopted metrics can conceal the
performance in the minority class. Finally, we recommend the inclusion of
complementary metrics to better reflect the system's performance in such
scenarios. Our study indicates that current evaluation practices adopted by the
research community for chest x-ray computer-aided diagnosis systems may not
reflect their performance in real clinical scenarios, and suggest alternatives
to improve this situation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PRIME: A few primitives can boost robustness to common corruptions. (arXiv:2112.13547v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13547">
<div class="article-summary-box-inner">
<span><p>Despite their impressive performance on image classification tasks, deep
networks have a hard time generalizing to unforeseen corruptions of their data.
To fix this vulnerability, prior works have built complex data augmentation
strategies, combining multiple methods to enrich the training data. However,
introducing intricate design choices or heuristics makes it hard to understand
which elements of these methods are indeed crucial for improving robustness. In
this work, we take a step back and follow a principled approach to achieve
robustness to common corruptions. We propose PRIME, a general data augmentation
scheme that relies on simple yet rich families of max-entropy image
transformations. PRIME outperforms the prior art in terms of corruption
robustness, while its simplicity and plug-and-play nature enable combination
with other methods to further boost their robustness. We analyze PRIME to shed
light on the importance of the mixing strategy on synthesizing corrupted
images, and to reveal the robustness-accuracy trade-offs arising in the context
of common corruptions. Finally, we show that the computational efficiency of
our method allows it to be easily used in both on-line and off-line data
augmentation schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Fine-grained Class Clustering via Generative Adversarial Networks. (arXiv:2112.14971v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14971">
<div class="article-summary-box-inner">
<span><p>Unsupervised fine-grained class clustering is a practical yet challenging
task due to the difficulty of feature representations learning of subtle object
details. We introduce C3-GAN, a method that leverages the categorical inference
power of InfoGAN with contrastive learning. We aim to learn feature
representations that encourage a dataset to form distinct cluster boundaries in
the embedding space, while also maximizing the mutual information between the
latent code and its image observation. Our approach is to train a
discriminator, which is also used for inferring clusters, to optimize the
contrastive loss, where image-latent pairs that maximize the mutual information
are considered as positive pairs and the rest as negative pairs. Specifically,
we map the input of a generator, which was sampled from the categorical
distribution, to the embedding space of the discriminator and let them act as a
cluster centroid. In this way, C3-GAN succeeded in learning a
clustering-friendly embedding space where each cluster is distinctively
separable. Experimental results show that C3-GAN achieved the state-of-the-art
clustering performance on four fine-grained image datasets, while also
alleviating the mode collapse phenomenon. Code is available at
https://github.com/naver-ai/c3-gan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language as Queries for Referring Video Object Segmentation. (arXiv:2201.00487v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00487">
<div class="article-summary-box-inner">
<span><p>Referring video object segmentation (R-VOS) is an emerging cross-modal task
that aims to segment the target object referred by a language expression in all
video frames. In this work, we propose a simple and unified framework built
upon Transformer, termed ReferFormer. It views the language as queries and
directly attends to the most relevant regions in the video frames. Concretely,
we introduce a small set of object queries conditioned on the language as the
input to the Transformer. In this manner, all the queries are obligated to find
the referred objects only. They are eventually transformed into dynamic kernels
which capture the crucial object-level information, and play the role of
convolution filters to generate the segmentation masks from feature maps. The
object tracking is achieved naturally by linking the corresponding queries
across frames. This mechanism greatly simplifies the pipeline and the
end-to-end framework is significantly different from the previous methods.
Extensive experiments on Ref-Youtube-VOS, Ref-DAVIS17, A2D-Sentences and
JHMDB-Sentences show the effectiveness of ReferFormer. On Ref-Youtube-VOS,
Refer-Former achieves 55.6J&amp;F with a ResNet-50 backbone without bells and
whistles, which exceeds the previous state-of-the-art performance by 8.4
points. In addition, with the strong Swin-Large backbone, ReferFormer achieves
the best J&amp;F of 64.2 among all existing methods. Moreover, we show the
impressive results of 55.0 mAP and 43.7 mAP on A2D-Sentences andJHMDB-Sentences
respectively, which significantly outperforms the previous methods by a large
margin. Code is publicly available at https://github.com/wjn922/ReferFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Verification of Deep Neural Networks in Perception Tasks Using Fuzzy Logic and Concept Embeddings. (arXiv:2201.00572v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00572">
<div class="article-summary-box-inner">
<span><p>One major drawback of deep convolutional neural networks (CNNs) for use in
safety critical applications is their black-box nature. This makes it hard to
verify or monitor complex, symbolic requirements on already trained computer
vision CNNs. In this work, we present a simple, yet effective, approach to
verify that a CNN complies with symbolic predicate logic rules which relate
visual concepts. It is the first that (1) does not modify the CNN, (2) may use
visual concepts that are no CNN in- or output feature, and (3) can leverage
continuous CNN confidence outputs. To achieve this, we newly combine methods
from explainable artificial intelligence and logic: First, using supervised
concept embedding analysis, the output of a CNN is post-hoc enriched by concept
outputs. Second, rules from prior knowledge are modelled as truth functions
that accept the CNN outputs, and can be evaluated with little computational
overhead. We here investigate the use of fuzzy logic, i.e., continuous truth
values, and of proper output calibration, which both theoretically and
practically show slight benefits. Applicability is demonstrated on
state-of-the-art object detectors for three verification use-cases, where
monitoring of rule breaches can reveal detection errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Autoencoder for Point Cloud Self-supervised Representation Learning. (arXiv:2201.00785v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00785">
<div class="article-summary-box-inner">
<span><p>Many 3D representations (e.g., point clouds) are discrete samples of the
underlying continuous 3D surface. This process inevitably introduces sampling
variations on the underlying 3D shapes. In learning 3D representation, the
variations should be disregarded while transferable knowledge of the underlying
3D shape should be captured. This poses a grand challenge to existing
representation learning paradigms. For example, the standard autoencoding
paradigm forces the encoder to capture such sampling variations as the decoder
has to reconstruct the original point cloud. We introduce Implicit
Autoencoder(IAE), a simple yet effective method that addresses this challenge
by replacing the point cloud decoder with an implicit decoder. The implicit
decoder outputs a continuous representation that is shared among different
point cloud samplings of the same model. Reconstructing under the implicit
representation can prioritize that the encoder discards sampling variations,
introducing more space to learn useful features. We theoretically justify this
claim under a simple linear autoencoder. Moreover, our implicit decoder offers
a rich space to design suitable implicit representations for different tasks.
We demonstrate the usefulness of IAE across various self-supervised learning
tasks for both 3D objects and 3D scenes. Experimental results show that IAE
consistently outperforms the state-of-the-art in each task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction. (arXiv:2201.02184v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02184">
<div class="article-summary-box-inner">
<span><p>Video recordings of speech contain correlated audio and visual information,
providing a strong signal for speech representation learning from the speaker's
lip movements and the produced sound. We introduce Audio-Visual Hidden Unit
BERT (AV-HuBERT), a self-supervised representation learning framework for
audio-visual speech, which masks multi-stream video input and predicts
automatically discovered and iteratively refined multimodal hidden units.
AV-HuBERT learns powerful audio-visual speech representation benefiting both
lip-reading and automatic speech recognition. On the largest public lip-reading
benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of
labeled data, outperforming the former state-of-the-art approach (33.6%)
trained with a thousand times more transcribed video data (31K hours). The
lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled
data from LRS3 and combined with self-training. Using our audio-visual
representation on the same benchmark for audio-only speech recognition leads to
a 40% relative WER reduction over the state-of-the-art performance (1.3% vs
2.3%). Our code and models are available at
https://github.com/facebookresearch/av_hubert
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02639">
<div class="article-summary-box-inner">
<span><p>As humans, we navigate a multimodal world, building a holistic understanding
from all our senses. We introduce MERLOT Reserve, a model that represents
videos jointly over time -- through a new training objective that learns from
audio, subtitles, and video frames. Given a video, we replace snippets of text
and audio with a MASK token; the model learns by choosing the correct
masked-out snippet. Our objective learns faster than alternatives, and performs
well at scale: we pretrain on 20 million YouTube videos.
</p>
<p>Empirical results show that MERLOT Reserve learns strong multimodal
representations. When finetuned, it sets state-of-the-art on Visual Commonsense
Reasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%,
and 1.5% respectively. Ablations show that these tasks benefit from audio
pretraining -- even VCR, a QA task centered around images (without sound).
Moreover, our objective enables out-of-the-box prediction, revealing strong
multimodal commonsense understanding. In a fully zero-shot setting, our model
obtains competitive results on four video tasks, even outperforming supervised
approaches on the recently proposed Situated Reasoning (STAR) benchmark.
</p>
<p>We analyze why audio enables better vision-language representations,
suggesting significant opportunities for future research. We conclude by
discussing ethical and societal implications of multimodal pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-biased image classification: evaluation based on semantic representations. (arXiv:2201.11014v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11014">
<div class="article-summary-box-inner">
<span><p>Humans show language-biased image recognition for a word-embedded image,
known as picture-word interference. Such interference depends on hierarchical
semantic categories and reflects that human language processing highly
interacts with visual processing. Similar to humans, recent artificial models
jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased
image classification. Exploring whether the bias leads to interference similar
to those observed in humans can contribute to understanding how much the model
acquires hierarchical semantic representations from joint learning of language
and vision. The present study introduces methodological tools from the
cognitive science literature to assess the biases of artificial models.
Specifically, we introduce a benchmark task to test whether words superimposed
on images can distort the image classification across different category levels
and, if it can, whether the perturbation is due to the shared semantic
representation between language and vision. Our dataset is a set of
word-embedded images and consists of a mixture of natural image datasets and
hierarchical word labels with superordinate/basic category levels. Using this
benchmark test, we evaluate the CLIP model. We show that presenting words
distorts the image classification by the model across different category
levels, but the effect does not depend on the semantic relationship between
images and embedded words. This suggests that the semantic word representation
in the CLIP visual processing is not shared with the image representation,
although the word representation strongly dominates for word-embedded images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains. (arXiv:2201.11528v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11528">
<div class="article-summary-box-inner">
<span><p>Adversarial examples have posed a severe threat to deep neural networks due
to their transferable nature. Currently, various works have paid great efforts
to enhance the cross-model transferability, which mostly assume the substitute
model is trained in the same domain as the target model. However, in reality,
the relevant information of the deployed model is unlikely to leak. Hence, it
is vital to build a more practical black-box threat model to overcome this
limitation and evaluate the vulnerability of deployed models. In this paper,
with only the knowledge of the ImageNet domain, we propose a Beyond ImageNet
Attack (BIA) to investigate the transferability towards black-box domains
(unknown classification tasks). Specifically, we leverage a generative model to
learn the adversarial function for disrupting low-level features of input
images. Based on this framework, we further propose two variants to narrow the
gap between the source and target domains from the data and model perspectives,
respectively. Extensive experiments on coarse-grained and fine-grained domains
demonstrate the effectiveness of our proposed methods. Notably, our methods
outperform state-of-the-art approaches by up to 7.71\% (towards coarse-grained
domains) and 25.91\% (towards fine-grained domains) on average. Our code is
available at \url{https://github.com/qilong-zhang/Beyond-ImageNet-Attack}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAP: An Attention-Based Module for Faithful Interpretation and Knowledge Injection in Convolutional Neural Networks. (arXiv:2201.11808v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11808">
<div class="article-summary-box-inner">
<span><p>Despite the state-of-the-art performance of deep convolutional neural
networks, they are susceptible to bias and malfunction in unseen situations.
The complex computation behind their reasoning is not sufficiently
human-understandable to develop trust. External explainer methods have tried to
interpret the network decisions in a human-understandable way, but they are
accused of fallacies due to their assumptions and simplifications. On the other
side, the inherent self-interpretability of models, while being more robust to
the mentioned fallacies, cannot be applied to the already trained models. In
this work, we propose a new attention-based pooling layer, called Local
Attention Pooling (LAP), that accomplishes self-interpretability and the
possibility for knowledge injection while improving the model's performance.
Moreover, several weakly-supervised knowledge injection methodologies are
provided to enhance the process of training. We verified our claims by
evaluating several LAP-extended models on three different datasets, including
Imagenet. The proposed framework offers more valid human-understandable and
more faithful-to-the-model interpretations than the commonly used white-box
explainer methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Servoing for Pose Control of Soft Continuum Arm in a Structured Environment. (arXiv:2202.05200v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05200">
<div class="article-summary-box-inner">
<span><p>For soft continuum arms, visual servoing is a popular control strategy that
relies on visual feedback to close the control loop. However, robust visual
servoing is challenging as it requires reliable feature extraction from the
image, accurate control models and sensors to perceive the shape of the arm,
both of which can be hard to implement in a soft robot. This letter circumvents
these challenges by presenting a deep neural network-based method to perform
smooth and robust 3D positioning tasks on a soft arm by visual servoing using a
camera mounted at the distal end of the arm. A convolutional neural network is
trained to predict the actuations required to achieve the desired pose in a
structured environment. Integrated and modular approaches for estimating the
actuations from the image are proposed and are experimentally compared. A
proportional control law is implemented to reduce the error between the desired
and current image as seen by the camera. The model together with the
proportional feedback control makes the described approach robust to several
variations such as new targets, lighting, loads, and diminution of the soft
arm. Furthermore, the model lends itself to be transferred to a new environment
with minimal effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entroformer: A Transformer-based Entropy Model for Learned Image Compression. (arXiv:2202.05492v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05492">
<div class="article-summary-box-inner">
<span><p>One critical component in lossy deep image compression is the entropy model,
which predicts the probability distribution of the quantized latent
representation in the encoding and decoding modules. Previous works build
entropy models upon convolutional neural networks which are inefficient in
capturing global dependencies. In this work, we propose a novel
transformer-based entropy model, termed Entroformer, to capture long-range
dependencies in probability distribution estimation effectively and
efficiently. Different from vision transformers in image classification, the
Entroformer is highly optimized for image compression, including a top-k
self-attention and a diamond relative position encoding. Meanwhile, we further
expand this architecture with a parallel bidirectional context model to speed
up the decoding process. The experiments show that the Entroformer achieves
state-of-the-art performance on image compression while being time-efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Transformer for Fast and Robust Point Cloud Registration. (arXiv:2202.06688v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06688">
<div class="article-summary-box-inner">
<span><p>We study the problem of extracting accurate correspondences for point cloud
registration. Recent keypoint-free methods bypass the detection of repeatable
keypoints which is difficult in low-overlap scenarios, showing great potential
in registration. They seek correspondences over downsampled superpoints, which
are then propagated to dense points. Superpoints are matched based on whether
their neighboring patches overlap. Such sparse and loose matching requires
contextual features capturing the geometric structure of the point clouds. We
propose Geometric Transformer to learn geometric feature for robust superpoint
matching. It encodes pair-wise distances and triplet-wise angles, making it
robust in low-overlap cases and invariant to rigid transformation. The
simplistic design attains surprisingly high matching accuracy such that no
RANSAC is required in the estimation of alignment transformation, leading to
$100$ times acceleration. Our method improves the inlier ratio by $17{\sim}30$
percentage points and the registration recall by over $7$ points on the
challenging 3DLoMatch benchmark. Our code and models are available at
\url{https://github.com/qinzheng93/GeoTransformer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification. (arXiv:2202.07570v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07570">
<div class="article-summary-box-inner">
<span><p>Progress in digital pathology is hindered by high-resolution images and the
prohibitive cost of exhaustive localized annotations. The commonly used
paradigm to categorize pathology images is patch-based processing, which often
incorporates multiple instance learning (MIL) to aggregate local patch-level
representations yielding image-level prediction. Nonetheless, diagnostically
relevant regions may only take a small fraction of the whole tissue, and
current MIL-based approaches often process images uniformly, discarding the
inter-patches interactions. To alleviate these issues, we propose ScoreNet, a
new efficient transformer that exploits a differentiable recommendation stage
to extract discriminative image regions and dedicate computational resources
accordingly. The proposed transformer leverages the local and global attention
of a few dynamically recommended high-resolution regions at an efficient
computational cost. We further introduce a novel Mixup-based data-augmentation,
namely ScoreMix, by leveraging the image's semantic distribution to guide the
data mixing and produce coherent sample-label pairs. ScoreMix is embarrassingly
simple and mitigates the pitfalls of previous augmentations, which assume a
uniform semantic distribution and risk mislabeling the samples. Thorough
experiments and ablation studies on three breast cancer histology datasets of
Haematoxylin &amp; Eosin (H&amp;E) have validated the superiority of our approach over
prior arts, including transformer-based models on tumour regions-of-interest
(TRoIs) classification. ScoreNet equipped with proposed ScoreMix augmentation
demonstrates better generalization capabilities and achieves new
state-of-the-art (SOTA) results with only 50% of the data compared to other
Mixup augmentation variants. Finally, ScoreNet yields high efficacy and
outperforms SOTA efficient transformers, namely TransPath and SwinTransformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Movies2Scenes: Learning Scene Representations Using Movie Similarities. (arXiv:2202.10650v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10650">
<div class="article-summary-box-inner">
<span><p>Labeling movie-scenes is a time-consuming process which makes applying
end-to-end supervised methods for scene-understanding a challenging problem.
Moreover, directly using image-based visual representations for
scene-understanding tasks does not prove to be effective given the large gap
between the two domains. To address these challenges, we propose a novel
contrastive learning approach that uses commonly available movie-level
information (e.g., co-watch, genre, synopsis) to learn a general-purpose
scene-level representation. Our learned representation comfortably outperforms
existing state-of-the-art approaches on eleven downstream tasks evaluated using
multiple benchmark datasets. To further demonstrate generalizability of our
learned representation, we present its comparative results on a set of
video-moderation tasks evaluated using a newly collected large-scale internal
movie dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deepfake Network Architecture Attribution. (arXiv:2202.13843v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13843">
<div class="article-summary-box-inner">
<span><p>With the rapid progress of generation technology, it has become necessary to
attribute the origin of fake images. Existing works on fake image attribution
perform multi-class classification on several Generative Adversarial Network
(GAN) models and obtain high accuracies. While encouraging, these works are
restricted to model-level attribution, only capable of handling images
generated by seen models with a specific seed, loss and dataset, which is
limited in real-world scenarios when fake images may be generated by privately
trained models. This motivates us to ask whether it is possible to attribute
fake images to the source models' architectures even if they are finetuned or
retrained under different configurations. In this work, we present the first
study on Deepfake Network Architecture Attribution to attribute fake images on
architecture-level. Based on an observation that GAN architecture is likely to
leave globally consistent fingerprints while traces left by model weights vary
in different regions, we provide a simple yet effective solution named DNA-Det
for this problem. Extensive experiments on multiple cross-test setups and a
large-scale dataset demonstrate the effectiveness of DNA-Det.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effectiveness of Delivered Information Trade Study. (arXiv:2203.00116v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00116">
<div class="article-summary-box-inner">
<span><p>The sensor to shooter timeline is affected by two main variables: satellite
positioning and asset positioning. Speeding up satellite positioning by adding
more sensors or by decreasing processing time is important only if there is a
prepared shooter, otherwise the main source of time is getting the shooter into
position. However, the intelligence community should work towards the
exploitation of sensors to the highest speed and effectiveness possible.
Achieving a high effectiveness while keeping speed high is a tradeoff that must
be considered in the sensor to shooter timeline. In this paper we investigate
two main ideas, increasing the effectiveness of satellite imagery through image
manipulation and how on-board image manipulation would affect the sensor to
shooter timeline. We cover these ideas in four scenarios: Discrete Event
Simulation of onboard processing versus ground station processing, quality of
information with cloud cover removal, information improvement with super
resolution, and data reduction with image to caption. This paper will show how
image manipulation techniques such as Super Resolution, Cloud Removal, and
Image to Caption will improve the quality of delivered information in addition
to showing how those processes effect the sensor to shooter timeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CycleMix: A Holistic Strategy for Medical Image Segmentation from Scribble Supervision. (arXiv:2203.01475v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01475">
<div class="article-summary-box-inner">
<span><p>Curating a large set of fully annotated training data can be costly,
especially for the tasks of medical image segmentation. Scribble, a weaker form
of annotation, is more obtainable in practice, but training segmentation models
from limited supervision of scribbles is still challenging. To address the
difficulties, we propose a new framework for scribble learning-based medical
image segmentation, which is composed of mix augmentation and cycle consistency
and thus is referred to as CycleMix. For augmentation of supervision, CycleMix
adopts the mixup strategy with a dedicated design of random occlusion, to
perform increments and decrements of scribbles. For regularization of
supervision, CycleMix intensifies the training objective with consistency
losses to penalize inconsistent segmentation, which results in significant
improvement of segmentation performance. Results on two open datasets, i.e.,
ACDC and MSCMRseg, showed that the proposed method achieved exhilarating
performance, demonstrating comparable or even better accuracy than the
fully-supervised methods. The code and expert-made scribble annotations for
MSCMRseg are publicly available at https://github.com/BWGZK/CycleMix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViTransPAD: Video Transformer using convolution and self-attention for Face Presentation Attack Detection. (arXiv:2203.01562v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01562">
<div class="article-summary-box-inner">
<span><p>Face Presentation Attack Detection (PAD) is an important measure to prevent
spoof attacks for face biometric systems. Many works based on Convolution
Neural Networks (CNNs) for face PAD formulate the problem as an image-level
binary classification task without considering the context. Alternatively,
Vision Transformers (ViT) using self-attention to attend the context of an
image become the mainstreams in face PAD. Inspired by ViT, we propose a
Video-based Transformer for face PAD (ViTransPAD) with short/long-range
spatio-temporal attention which can not only focus on local details with short
attention within a frame but also capture long-range dependencies over frames.
Instead of using coarse image patches with single-scale as in ViT, we propose
the Multi-scale Multi-Head Self-Attention (MsMHSA) architecture to accommodate
multi-scale patch partitions of Q, K, V feature maps to the heads of
transformer in a coarse-to-fine manner, which enables to learn a fine-grained
representation to perform pixel-level discrimination for face PAD. Due to lack
inductive biases of convolutions in pure transformers, we also introduce
convolutions to the proposed ViTransPAD to integrate the desirable properties
of CNNs by using convolution patch embedding and convolution projection. The
extensive experiments show the effectiveness of our proposed ViTransPAD with a
preferable accuracy-computation balance, which can serve as a new backbone for
face PAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Object Localization as Domain Adaption. (arXiv:2203.01714v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01714">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object localization (WSOL) focuses on localizing objects
only with the supervision of image-level classification masks. Most previous
WSOL methods follow the classification activation map (CAM) that localizes
objects based on the classification structure with the multi-instance learning
(MIL) mechanism. However, the MIL mechanism makes CAM only activate
discriminative object parts rather than the whole object, weakening its
performance for localizing objects. To avoid this problem, this work provides a
novel perspective that models WSOL as a domain adaption (DA) task, where the
score estimator trained on the source/image domain is tested on the
target/pixel domain to locate objects. Under this perspective, a DA-WSOL
pipeline is designed to better engage DA approaches into WSOL to enhance
localization performance. It utilizes a proposed target sampling strategy to
select different types of target samples. Based on these types of target
samples, domain adaption localization (DAL) loss is elaborated. It aligns the
feature distribution between the two domains by DA and makes the estimator
perceive target domain cues by Universum regularization. Experiments show that
our pipeline outperforms SOTA methods on multi benchmarks. Code are released at
\url{https://github.com/zh460045050/DA-WSOL_CVPR2022}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation. (arXiv:2203.02925v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02925">
<div class="article-summary-box-inner">
<span><p>Weakly supervised temporal action localization aims to localize temporal
boundaries of actions and simultaneously identify their categories with only
video-level category labels. Many existing methods seek to generate pseudo
labels for bridging the discrepancy between classification and localization,
but usually only make use of limited contextual information for pseudo label
generation. To alleviate this problem, we propose a representative snippet
summarization and propagation framework. Our method seeks to mine the
representative snippets in each video for propagating information between video
snippets to generate better pseudo labels. For each video, its own
representative snippets and the representative snippets from a memory bank are
propagated to update the input features in an intra- and inter-video manner.
The pseudo labels are generated from the temporal class activation maps of the
updated features to rectify the predictions of the main branch. Our method
obtains superior performance in comparison to the existing methods on two
benchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in
terms of average mAP on THUMOS14.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signature and Log-signature for the Study of Empirical Distributions Generated with GANs. (arXiv:2203.03226v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03226">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop a new and systematic method to explore and analyze
samples taken by NASA Perseverance on the surface of the planet Mars. A novel
in this context PCA adaptive t-SNE is proposed, as well as the introduction of
statistical measures to study the goodness of fit of the sample distribution.
We go beyond visualization by generating synthetic imagery using Stylegan2-ADA
that resemble the original terrain distribution. We also conduct synthetic
image generation using the recently introduced Scored-based Generative
Modeling. We bring forward the use of the recently developed Signature
Transform as a way to measure the similarity between image distributions and
provide detailed acquaintance and extensive evaluations. We are the first to
pioneer RMSE and MAE Signature and log-signature as an alternative to measure
GAN convergence. Insights on state-of-the-art instance segmentation of the
samples by the use of a model DeepLabv3 are also given.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels. (arXiv:2203.03884v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03884">
<div class="article-summary-box-inner">
<span><p>The crux of semi-supervised semantic segmentation is to assign adequate
pseudo-labels to the pixels of unlabeled images. A common practice is to select
the highly confident predictions as the pseudo ground-truth, but it leads to a
problem that most pixels may be left unused due to their unreliability. We
argue that every pixel matters to the model training, even its prediction is
ambiguous. Intuitively, an unreliable prediction may get confused among the top
classes (i.e., those with the highest probabilities), however, it should be
confident about the pixel not belonging to the remaining classes. Hence, such a
pixel can be convincingly treated as a negative sample to those most unlikely
categories. Based on this insight, we develop an effective pipeline to make
sufficient use of unlabeled data. Concretely, we separate reliable and
unreliable pixels via the entropy of predictions, push each unreliable pixel to
a category-wise queue that consists of negative samples, and manage to train
the model with all candidate pixels. Considering the training evolution, where
the prediction becomes more and more accurate, we adaptively adjust the
threshold for the reliable-unreliable partition. Experimental results on
various benchmarks and training settings demonstrate the superiority of our
approach over the state-of-the-art alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary Camera Rigs. (arXiv:2203.04050v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04050">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation in bird's eye view (BEV) is an important task for
autonomous driving. Though this task has attracted a large amount of research
efforts, it is still challenging to flexibly cope with arbitrary (single or
multiple) camera sensors equipped on the autonomous vehicle. In this paper, we
present BEVSegFormer, an effective transformer-based method for BEV semantic
segmentation from arbitrary camera rigs. Specifically, our method first encodes
image features from arbitrary cameras with a shared backbone. These image
features are then enhanced by a deformable transformer-based encoder. Moreover,
we introduce a BEV transformer decoder module to parse BEV semantic
segmentation results. An efficient multi-camera deformable attention unit is
designed to carry out the BEV-to-image view transformation. Finally, the
queries are reshaped according the layout of grids in the BEV, and upsampled to
produce the semantic segmentation result in a supervised manner. We evaluate
the proposed algorithm on the public nuScenes dataset and a self-collected
dataset. Experimental results show that our method achieves promising
performance on BEV semantic segmentation from arbitrary camera rigs. We also
demonstrate the effectiveness of each component via ablation study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment. (arXiv:2203.04121v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04121">
<div class="article-summary-box-inner">
<span><p>Training a generative adversarial network (GAN) with limited data has been a
challenging task. A feasible solution is to start with a GAN well-trained on a
large scale source domain and adapt it to the target domain with a few samples,
termed as few shot generative model adaption. However, existing methods are
prone to model overfitting and collapse in extremely few shot setting (less
than 10). To solve this problem, we propose a relaxed spatial structural
alignment method to calibrate the target generative models during the adaption.
We design a cross-domain spatial structural consistency loss comprising the
self-correlation and disturbance correlation consistency loss. It helps align
the spatial structural information between the synthesis image pairs of the
source and target domains. To relax the cross-domain alignment, we compress the
original latent space of generative models to a subspace. Image pairs generated
from the subspace are pulled closer. Qualitative and quantitative experiments
show that our method consistently surpasses the state-of-the-art methods in few
shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomous Mosquito Habitat Detection Using Satellite Imagery and Convolutional Neural Networks for Disease Risk Mapping. (arXiv:2203.04463v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04463">
<div class="article-summary-box-inner">
<span><p>Mosquitoes are known vectors for disease transmission that cause over one
million deaths globally each year. The majority of natural mosquito habitats
are areas containing standing water that are challenging to detect using
conventional ground-based technology on a macro scale. Contemporary approaches,
such as drones, UAVs, and other aerial imaging technology are costly when
implemented and are only most accurate on a finer spatial scale whereas the
proposed convolutional neural network(CNN) approach can be applied for disease
risk mapping and further guide preventative efforts on a more global scale. By
assessing the performance of autonomous mosquito habitat detection technology,
the transmission of mosquito-borne diseases can be prevented in a
cost-effective manner. This approach aims to identify the spatiotemporal
distribution of mosquito habitats in extensive areas that are difficult to
survey using ground-based technology by employing computer vision on satellite
imagery for proof of concept. The research presents an evaluation and the
results of 3 different CNN models to determine their accuracy of predicting
large-scale mosquito habitats. For this approach, a dataset was constructed
containing a variety of geographical features. Larger land cover variables such
as ponds/lakes, inlets, and rivers were utilized to classify mosquito habitats
while minute sites were omitted for higher accuracy on a larger scale. Using
the dataset, multiple CNN networks were trained and evaluated for accuracy of
habitat prediction. Utilizing a CNN-based approach on readily available
satellite imagery is cost-effective and scalable, unlike most aerial imaging
technology. Testing revealed that YOLOv4 obtained greater accuracy in mosquito
habitat detection for identifying large-scale mosquito habitats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks. (arXiv:2203.04466v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04466">
<div class="article-summary-box-inner">
<span><p>Neural networks tend to achieve better accuracy with training if they are
larger -- even if the resulting models are overparameterized. Nevertheless,
carefully removing such excess parameters before, during, or after training may
also produce models with similar or even improved accuracy. In many cases, that
can be curiously achieved by heuristics as simple as removing a percentage of
the weights with the smallest absolute value -- even though magnitude is not a
perfect proxy for weight relevance. With the premise that obtaining
significantly better performance from pruning depends on accounting for the
combined effect of removing multiple weights, we revisit one of the classic
approaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose
a tractable heuristic for solving the combinatorial extension of OBS, in which
we select weights for simultaneous removal, as well as a systematic update of
the remaining weights. Our selection method outperforms other methods under
high sparsity, and the weight update is advantageous even when combined with
the other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All You Need is LUV: Unsupervised Collection of Labeled Images using Invisible UV Fluorescent Indicators. (arXiv:2203.04566v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04566">
<div class="article-summary-box-inner">
<span><p>Large-scale semantic image annotation is a significant challenge for
learning-based perception systems in robotics. Current approaches often rely on
human labelers, which can be expensive, or simulation data, which can visually
or physically differ from real data. This paper proposes Labels from
UltraViolet (LUV), a novel framework that enables rapid, labeled data
collection in real manipulation environments without human labeling. LUV uses
transparent, ultraviolet-fluorescent paint with programmable ultraviolet LEDs
to collect paired images of a scene in standard lighting and UV lighting to
autonomously extract segmentation masks and keypoints via color segmentation.
We apply LUV to a suite of diverse robot perception tasks to evaluate its
labeling quality, flexibility, and data collection rate. Results suggest that
LUV is 180-2500 times faster than a human labeler across the tasks. We show
that LUV provides labels consistent with human annotations on unpainted test
images. The networks trained on these labels are used to smooth and fold
crumpled towels with 83% success rate and achieve 1.7mm position error with
respect to human labels on a surgical needle pose estimation task. The low cost
of LUV makes it ideal as a lightweight replacement for human labeling systems,
with the one-time setup costs at $300 equivalent to the cost of collecting
around 200 semantic segmentation labels on Amazon Mechanical Turk. Code,
datasets, visualizations, and supplementary material can be found at
https://sites.google.com/berkeley.edu/luv
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation. (arXiv:2203.04568v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04568">
<div class="article-summary-box-inner">
<span><p>The success of Transformer in computer vision has attracted increasing
attention in the medical imaging community. Especially for medical image
segmentation, many excellent hybrid architectures based on convolutional neural
networks (CNNs) and Transformer have been presented and achieve impressive
performance. However, most of these methods, which embed modular Transformer
into CNNs, struggle to reach their full potential. In this paper, we propose a
novel hybrid architecture for medical image segmentation called PHTrans, which
parallelly hybridizes Transformer and CNN in main building blocks to produce
hierarchical representations from global and local features and adaptively
aggregate them, aiming to fully exploit their strengths to obtain better
segmentation performance. Specifically, PHTrans follows the U-shaped
encoder-decoder design and introduces the parallel hybird module in deep
stages, where convolution blocks and the modified 3D Swin Transformer learn
local features and global dependencies separately, then a sequence-to-volume
operation unifies the dimensions of the outputs to achieve feature aggregation.
Extensive experimental results on both Multi-Atlas Labeling Beyond the Cranial
Vault and Automated Cardiac Diagnosis Challeng datasets corroborate its
effectiveness, consistently outperforming state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification. (arXiv:2203.04614v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04614">
<div class="article-summary-box-inner">
<span><p>A large-scale labeled dataset is a key factor for the success of supervised
deep learning in computer vision. However, a limited number of annotated data
is very common, especially in ophthalmic image analysis, since manual
annotation is time-consuming and labor-intensive. Self-supervised learning
(SSL) methods bring huge opportunities for better utilizing unlabeled data, as
they do not need massive annotations. With an attempt to use as many as
possible unlabeled ophthalmic images, it is necessary to break the dimension
barrier, simultaneously making use of both 2D and 3D images. In this paper, we
propose a universal self-supervised Transformer framework, named Uni4Eye, to
discover the inherent image property and capture domain-specific feature
embedding in ophthalmic images. Uni4Eye can serve as a global feature
extractor, which builds its basis on a Masked Image Modeling task with a Vision
Transformer (ViT) architecture. We employ a Unified Patch Embedding module to
replace the origin patch embedding module in ViT for jointly processing both 2D
and 3D input images. Besides, we design a dual-branch multitask decoder module
to simultaneously perform two reconstruction tasks on the input image and its
gradient map, delivering discriminative representations for better convergence.
We evaluate the performance of our pre-trained Uni4Eye encoder by fine-tuning
it on six downstream ophthalmic image classification tasks. The superiority of
Uni4Eye is successfully established through comparisons to other
state-of-the-art SSL pre-training methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CEU-Net: Ensemble Semantic Segmentation of Hyperspectral Images Using Clustering. (arXiv:2203.04873v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04873">
<div class="article-summary-box-inner">
<span><p>Most semantic segmentation approaches of Hyperspectral images (HSIs) use and
require preprocessing steps in the form of patching to accurately classify
diversified land cover in remotely sensed images. These approaches use patching
to incorporate the rich neighborhood information in images and exploit the
simplicity and segmentability of the most common HSI datasets. In contrast,
most landmasses in the world consist of overlapping and diffused classes,
making neighborhood information weaker than what is seen in common HSI
datasets. To combat this issue and generalize the segmentation models to more
complex and diverse HSI datasets, in this work, we propose our novel flagship
model: Clustering Ensemble U-Net (CEU-Net). CEU-Net uses the ensemble method to
combine spectral information extracted from convolutional neural network (CNN)
training on a cluster of landscape pixels. Our CEU-Net model outperforms
existing state-of-the-art HSI semantic segmentation methods and gets
competitive performance with and without patching when compared to baseline
models. We highlight CEU-Net's high performance across Botswana, KSC, and
Salinas datasets compared to HybridSN and AeroRIT methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Map Learning for Vision and Language Navigation. (arXiv:2203.05137v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05137">
<div class="article-summary-box-inner">
<span><p>We consider the problem of Vision-and-Language Navigation (VLN). The majority
of current methods for VLN are trained end-to-end using either unstructured
memory such as LSTM, or using cross-modal attention over the egocentric
observations of the agent. In contrast to other works, our key insight is that
the association between language and vision is stronger when it occurs in
explicit spatial representations. In this work, we propose a cross-modal map
learning model for vision-and-language navigation that first learns to predict
the top-down semantics on an egocentric map for both observed and unobserved
regions, and then predicts a path towards the goal as a set of waypoints. In
both cases, the prediction is informed by the language through cross-modal
attention mechanisms. We experimentally test the basic hypothesis that
language-driven navigation can be solved given a map, and then show competitive
results on the full VLN-CE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Background Matting Using Background Matching. (arXiv:2203.05193v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05193">
<div class="article-summary-box-inner">
<span><p>Due to the difficulty of solving the matting problem, lots of methods use
some kinds of assistance to acquire high quality alpha matte. Green screen
matting methods rely on physical equipment. Trimap-based methods take manual
interactions as external input. Background-based methods require a
pre-captured, static background. The methods are not flexible and convenient
enough to use widely. Trimap-free methods are flexible but not stable in
complicated video applications. To be stable and flexible in real applications,
we propose an adaptive background matting method. The user first captures their
videos freely, moving the cameras. Then the user captures the background video
afterwards, roughly covering the previous captured regions. We use dynamic
background video instead of static background for accurate matting. The
proposed method is convenient to use in any scenes as the static camera and
background is no more the limitation. To achieve this goal, we use background
matching network to find the best-matched background frame by frame from
dynamic backgrounds. Then, robust semantic estimation network is used to
estimate the coarse alpha matte. Finally, we crop and zoom the target region
according to the coarse alpha matte, and estimate the final accurate alpha
matte. In experiments, the proposed method is able to perform comparably
against the state-of-the-art matting methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-generative Generalized Zero-shot Learning via Task-correlated Disentanglement and Controllable Samples Synthesis. (arXiv:2203.05335v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05335">
<div class="article-summary-box-inner">
<span><p>Synthesizing pseudo samples is currently the most effective way to solve the
Generalized Zero Shot Learning (GZSL) problem. Most models achieve competitive
performance but still suffer from two problems: (1) Feature confounding, the
overall representations confound task-correlated and task-independent features,
and existing models disentangle them in a generative way, but they are
unreasonable to synthesize reliable pseudo samples with limited samples; (2)
Distribution uncertainty, that massive data is needed when existing models
synthesize samples from the uncertain distribution, which causes poor
performance in limited samples of seen classes. In this paper, we propose a
non-generative model to address these problems correspondingly in two modules:
(1) Task-correlated feature disentanglement, to exclude the task-correlated
features from task-independent ones by adversarial learning of domain adaption
towards reasonable synthesis; (2) Controllable pseudo sample synthesis, to
synthesize edge-pseudo and center-pseudo samples with certain characteristics
towards more diversity generated and intuitive transfer. In addation, to
describe the new scene that is the limit seen class samples in the training
process, we further formulate a new ZSL task named the 'Few-shot Seen class and
Zero-shot Unseen class learning' (FSZU). Extensive experiments on four
benchmarks verify that the proposed method is competitive in the GZSL and the
FSZU tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Commonsense Graph for Object Localisation in Partial Scenes. (arXiv:2203.05380v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05380">
<div class="article-summary-box-inner">
<span><p>We solve object localisation in partial scenes, a new problem of estimating
the unknown position of an object (e.g. where is the bag?) given a partial 3D
scan of a scene. The proposed solution is based on a novel scene graph model,
the Spatial Commonsense Graph (SCG), where objects are the nodes and edges
define pairwise distances between them, enriched by concept nodes and
relationships from a commonsense knowledge base. This allows SCG to better
generalise its spatial inference over unknown 3D scenes. The SCG is used to
estimate the unknown position of the target object in two steps: first, we feed
the SCG into a novel Proximity Prediction Network, a graph neural network that
uses attention to perform distance prediction between the node representing the
target object and the nodes representing the observed objects in the SCG;
second, we propose a Localisation Module based on circular intersection to
estimate the object position using all the predicted pairwise distances in
order to be independent of any reference system. We create a new dataset of
partially reconstructed scenes to benchmark our method and baselines for object
localisation in partial scenes, where our proposed method achieves the best
localisation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Investigation of 3D Anomaly Detection and Segmentation. (arXiv:2203.05550v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05550">
<div class="article-summary-box-inner">
<span><p>Anomaly detection and segmentation in images has made tremendous progress in
recent years while 3D information has often been ignored. The objective of this
paper is to further understand the benefit and role of 3D as opposed to color
in image anomaly detection. Our study begins by presenting a surprising
finding: standard color-only anomaly segmentation methods, when applied to 3D
datasets, significantly outperform all current methods. On the other hand, we
observe that color-only methods are insufficient for images containing
geometric anomalies where shape cannot be unambiguously inferred from 2D. This
suggests that better 3D methods are needed. We investigate different
representations for 3D anomaly detection and discover that handcrafted
orientation-invariant representations are unreasonably effective on this task.
We uncover a simple 3D-only method that outperforms all recent approaches while
not using deep learning, external pretraining datasets, or color information.
As the 3D-only method cannot detect color and texture anomalies, we combine it
with 2D color features, granting us the best current results by a large margin
(Pixel-wise ROCAUC: 99.2%, PRO: 95.9% on MVTec 3D-AD). We conclude by
discussing future challenges for 3D anomaly detection and segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Domain Reconstruction Networks with V-Net and K-Net for fast MRI. (arXiv:2203.05725v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05725">
<div class="article-summary-box-inner">
<span><p>Purpose: To introduce a dual-domain reconstruction network with V-Net and
K-Net for accurate MR image reconstruction from undersampled k-space data.
Methods: Most state-of-the-art reconstruction methods apply U-Net or cascaded
U-Nets in image domain and/or k-space domain. Nevertheless, these methods have
following problems: (1) Directly applying U-Net in k-space domain is not
optimal for extracting features in k-space domain; (2) Classical image-domain
oriented U-Net is heavy-weight and hence is inefficient to be cascaded many
times for yielding good reconstruction accuracy; (3) Classical image-domain
oriented U-Net does not fully make use information of encoder network for
extracting features in decoder network; and (4) Existing methods are
ineffective in simultaneously extracting and fusing features in image domain
and its dual k-space domain. To tackle these problems, we propose in this paper
(1) an image-domain encoder-decoder sub-network called V-Net which is more
light-weight for cascading and effective in fully utilizing features in the
encoder for decoding, (2) a k-space domain sub-network called K-Net which is
more suitable for extracting hierarchical features in k-space domain, and (3) a
dual-domain reconstruction network where V-Nets and K-Nets are parallelly and
effectively combined and cascaded. Results: Extensive experimental results on
the challenging fastMRI dataset demonstrate that the proposed KV-Net can
reconstruct high-quality images and outperform current state-of-the-art
approaches with fewer parameters. Conclusions: To reconstruct images
effectively and efficiently from incomplete k-space data, we have presented a
parallel dual-domain KV-Net to combine K-Nets and V-Nets. The KV-Net is more
lightweight than state-of-the-art methods but achieves better reconstruction
performance.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-15 23:08:14.127688850 UTC">2022-03-15 23:08:14 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>