<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-15T01:30:00Z">04-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Distant Supervision Corpus for Extracting Biomedical Relationships Between Chemicals, Diseases and Genes. (arXiv:2204.06584v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06584">
<div class="article-summary-box-inner">
<span><p>We introduce ChemDisGene, a new dataset for training and evaluating
multi-class multi-label document-level biomedical relation extraction models.
Our dataset contains 80k biomedical research abstracts labeled with mentions of
chemicals, diseases, and genes, portions of which human experts labeled with 18
types of biomedical relationships between these entities (intended for
evaluation), and the remainder of which (intended for training) has been
distantly labeled via the CTD database with approximately 78\% accuracy. In
comparison to similar preexisting datasets, ours is both substantially larger
and cleaner; it also includes annotations linking mentions to their entities.
We also provide three baseline deep neural network relation extraction models
trained and evaluated on our new dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EHRKit: A Python Natural Language Processing Toolkit for Electronic Health Record Texts. (arXiv:2204.06604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06604">
<div class="article-summary-box-inner">
<span><p>The Electronic Health Record (EHR) is an essential part of the modern medical
system and impacts healthcare delivery, operations, and research. Unstructured
text is attracting much attention despite structured information in the EHRs
and has become an exciting research field. The success of the recent neural
Natural Language Processing (NLP) method has led to a new direction for
processing unstructured clinical notes. In this work, we create a python
library for clinical texts, EHRKit. This library contains two main parts:
MIMIC-III-specific functions and tasks specific functions. The first part
introduces a list of interfaces for accessing MIMIC-III NOTEEVENTS data,
including basic search, information retrieval, and information extraction. The
second part integrates many third-party libraries for up to 12 off-shelf NLP
tasks such as named entity recognition, summarization, machine translation,
etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity. (arXiv:2204.06618v1 [cs.CC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06618">
<div class="article-summary-box-inner">
<span><p>This paper analyzes three formal models of Transformer encoders that differ
in the form of their self-attention mechanism: unique hard attention (UHAT);
generalized unique hard attention (GUHAT), which generalizes UHAT; and
averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers,
viewed as string acceptors, can only recognize formal languages in the
complexity class AC$^0$, the class of languages recognizable by families of
Boolean circuits of constant depth and polynomial size. This upper bound
subsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages
or the PARITY language, since those languages are outside AC$^0$ (Furst et al.,
1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are
recognizable by AHAT networks, implying that AHAT can recognize languages that
UHAT and GUHAT cannot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing. (arXiv:2204.06625v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06625">
<div class="article-summary-box-inner">
<span><p>Model ensemble is a popular approach to produce a low-variance and
well-generalized model. However, it induces large memory and inference costs,
which are often not affordable for real-world deployment. Existing work has
resorted to sharing weights among models. However, when increasing the
proportion of the shared weights, the resulting models tend to be similar, and
the benefits of using model ensemble diminish. To retain ensemble benefits
while maintaining a low memory cost, we propose a consistency-regularized
ensemble learning approach based on perturbed models, named CAMERO.
Specifically, we share the weights of bottom layers across all models and apply
different perturbations to the hidden representations for different models,
which can effectively promote the model diversity. Meanwhile, we apply a
prediction consistency regularizer across the perturbed models to control the
variance due to the model diversity. Our experiments using large language
models demonstrate that CAMERO significantly improves the generalization
performance of the ensemble model. Specifically, CAMERO outperforms the
standard ensemble of 8 BERT-base models on the GLUE benchmark by 0.7 with a
significantly smaller model size (114.2M vs. 880.6M).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals. (arXiv:2204.06644v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06644">
<div class="article-summary-box-inner">
<span><p>We present an efficient method of pretraining large-scale autoencoding
language models using training signals generated by an auxiliary model.
Originated in ELECTRA, this training strategy has demonstrated
sample-efficiency to pretrain models at the scale of hundreds of millions of
parameters. In this work, we conduct a comprehensive empirical study, and
propose a recipe, namely "Model generated dEnoising TRaining Objective"
(METRO), which incorporates some of the best modeling techniques developed
recently to speed up, stabilize, and enhance pretrained language models without
compromising model effectiveness. The resultant models, METRO-LM, consisting of
up to 5.4 billion parameters, achieve new state-of-the-art on the GLUE,
SuperGLUE, and SQuAD benchmarks. More importantly, METRO-LM are efficient in
that they often outperform previous large models with significantly smaller
model sizes and lower pretraining cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06674">
<div class="article-summary-box-inner">
<span><p>Recent improvements in KG-to-text generation are due to additional auxiliary
pre-trained tasks designed to give the fine-tune task a boost in performance.
These tasks require extensive computational resources while only suggesting
marginal improvements. Here, we demonstrate that by fusing graph-aware elements
into existing pre-trained language models, we are able to outperform
state-of-the-art models and close the gap imposed by additional pre-train
tasks. We do so by proposing a mask structure to capture neighborhood
information and a novel type encoder that adds a bias to the graph-attention
weights depending on the connection type. Experiments on two KG-to-text
benchmark datasets show these models to be superior in quality while involving
fewer parameters and no additional pre-trained tasks. By formulating the
problem as a framework, we can interchange the various proposed components and
begin interpreting KG-to-text generative models based on the topological and
type information found in a graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Schema Graph Fusion Network for Multi-Domain Dialogue State Tracking. (arXiv:2204.06677v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06677">
<div class="article-summary-box-inner">
<span><p>Dialogue State Tracking (DST) aims to keep track of users' intentions during
the course of a conversation. In DST, modelling the relations among domains and
slots is still an under-studied problem. Existing approaches that have
considered such relations generally fall short in: (1) fusing prior slot-domain
membership relations and dialogue-aware dynamic slot relations explicitly, and
(2) generalizing to unseen domains. To address these issues, we propose a novel
\textbf{D}ynamic \textbf{S}chema \textbf{G}raph \textbf{F}usion
\textbf{Net}work (\textbf{DSGFNet}), which generates a dynamic schema graph to
explicitly fuse the prior slot-domain membership relations and dialogue-aware
dynamic slot relations. It also uses the schemata to facilitate knowledge
transfer to new domains. DSGFNet consists of a dialogue utterance encoder, a
schema graph encoder, a dialogue-aware schema graph evolving network, and a
schema graph enhanced dialogue state decoder. Empirical results on benchmark
datasets (i.e., SGD, MultiWOZ2.1, and MultiWOZ2.2), show that DSGFNet
outperforms existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Transformer-based Models for Long Document Classification. (arXiv:2204.06683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06683">
<div class="article-summary-box-inner">
<span><p>The recent literature in text classification is biased towards short text
sequences (e.g., sentences or paragraphs). In real-world applications,
multi-page multi-paragraph documents are common and they cannot be efficiently
encoded by vanilla Transformer-based models. We compare different
Transformer-based Long Document Classification (TrLDC) approaches that aim to
mitigate the computational overhead of vanilla transformers to encode much
longer text, namely sparse attention and hierarchical encoding methods. We
examine several aspects of sparse attention (e.g., size of local attention
window, use of global attention) and hierarchical (e.g., document splitting
strategy) transformers on four document classification datasets covering
different domains. We observe a clear benefit from being able to process longer
text, and, based on our results, we derive practical advice of applying
Transformer-based models on long document classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-NeoX-20B: An Open-Source Autoregressive Language Model. (arXiv:2204.06745v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06745">
<div class="article-summary-box-inner">
<span><p>We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language
model trained on the Pile, whose weights will be made freely and openly
available to the public through a permissive license. It is, to the best of our
knowledge, the largest dense autoregressive model that has publicly available
weights at the time of submission. In this work, we describe \model{}'s
architecture and training and evaluate its performance on a range of
language-understanding, mathematics, and knowledge-based tasks. We find that
GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in
performance when evaluated five-shot than similarly sized GPT-3 and FairSeq
models. We open-source the training and evaluation code, as well as the model
weights, at https://github.com/EleutherAI/gpt-neox.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Top-K Decoding for Non-Autoregressive Semantic Parsing via Intent Conditioning. (arXiv:2204.06748v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06748">
<div class="article-summary-box-inner">
<span><p>Semantic parsing (SP) is a core component of modern virtual assistants like
Google Assistant and Amazon Alexa. While sequence-to-sequence-based
auto-regressive (AR) approaches are common for conversational semantic parsing,
recent studies employ non-autoregressive (NAR) decoders and reduce inference
latency while maintaining competitive parsing quality. However, a major
drawback of NAR decoders is the difficulty of generating top-k (i.e., k-best)
outputs with approaches such as beam search. To address this challenge, we
propose a novel NAR semantic parser that introduces intent conditioning on the
decoder. Inspired by the traditional intent and slot tagging parsers, we
decouple the top-level intent prediction from the rest of a parse. As the
top-level intent largely governs the syntax and semantics of a parse, the
intent conditioning allows the model to better control beam search and improves
the quality and diversity of top-k outputs. We introduce a hybrid
teacher-forcing approach to avoid training and inference mismatch. We evaluate
the proposed NAR on conversational SP datasets, TOP &amp; TOPv2. Like the existing
NAR models, we maintain the O(1) decoding time complexity while generating more
diverse outputs and improving the top-3 exact match (EM) by 2.4 points. In
comparison with AR models, our model speeds up beam search inference by 6.7
times on CPU with competitive top-k EM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-label topic classification for COVID-19 literature with Bioformer. (arXiv:2204.06758v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06758">
<div class="article-summary-box-inner">
<span><p>We describe Bioformer team's participation in the multi-label topic
classification task for COVID-19 literature (track 5 of BioCreative VII). Topic
classification is performed using different BERT models (BioBERT, PubMedBERT,
and Bioformer). We formulate the topic classification task as a sentence pair
classification problem, where the title is the first sentence, and the abstract
is the second sentence. Our results show that Bioformer outperforms BioBERT and
PubMedBERT in this task. Compared to the baseline results, our best model
increased micro, macro, and instance-based F1 score by 8.8%, 15.5%, 7.4%,
respectively. Bioformer achieved the highest micro F1 and macro F1 scores in
this challenge. In post-challenge experiments, we found that pretraining of
Bioformer on COVID-19 articles further improves the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation. (arXiv:2204.06812v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06812">
<div class="article-summary-box-inner">
<span><p>The principal task in supervised neural machine translation (NMT) is to learn
to generate target sentences conditioned on the source inputs from a set of
parallel sentence pairs, and thus produce a model capable of generalizing to
unseen instances. However, it is commonly observed that the generalization
performance of the model is highly influenced by the amount of parallel data
used in training. Although data augmentation is widely used to enrich the
training data, conventional methods with discrete manipulations fail to
generate diverse and faithful training samples. In this paper, we present a
novel data augmentation paradigm termed Continuous Semantic Augmentation
(CsaNMT), which augments each training instance with an adjacency semantic
region that could cover adequate variants of literal expression under the same
meaning. We conduct extensive experiments on both rich-resource and
low-resource settings involving various language pairs, including WMT14
English-{German,French}, NIST Chinese-English and multiple low-resource IWSLT
translation tasks. The provided empirical evidences show that CsaNMT sets a new
level of performance among existing augmentation techniques, improving on the
state-of-the-art by a large margin. The core codes are contained in Appendix E.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Gender Debiasing Affects Internal Model Representations, and Why It Matters. (arXiv:2204.06827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06827">
<div class="article-summary-box-inner">
<span><p>Common studies of gender bias in NLP focus either on extrinsic bias measured
by model performance on a downstream task or on intrinsic bias found in models'
internal representations. However, the relationship between extrinsic and
intrinsic bias is relatively unknown. In this work, we illuminate this
relationship by measuring both quantities together: we debias a model during
downstream fine-tuning, which reduces extrinsic bias, and measure the effect on
intrinsic bias, which is operationalized as bias extractability with
information-theoretic probing. Through experiments on two tasks and multiple
bias metrics, we show that our intrinsic bias metric is a better indicator of
debiasing than (a contextual adaptation of) the standard WEAT metric, and can
also expose cases of superficial debiasing. Our framework provides a
comprehensive perspective on bias in NLP models, which can be applied to deploy
NLP systems in a more informed manner. Our code will be made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shedding New Light on the Language of the Dark Web. (arXiv:2204.06885v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06885">
<div class="article-summary-box-inner">
<span><p>The hidden nature and the limited accessibility of the Dark Web, combined
with the lack of public datasets in this domain, make it difficult to study its
inherent characteristics such as linguistic properties. Previous works on text
classification of Dark Web domain have suggested that the use of deep neural
models may be ineffective, potentially due to the linguistic differences
between the Dark and Surface Webs. However, not much work has been done to
uncover the linguistic characteristics of the Dark Web. This paper introduces
CoDA, a publicly available Dark Web dataset consisting of 10000 web documents
tailored towards text-based Dark Web analysis. By leveraging CoDA, we conduct a
thorough linguistic analysis of the Dark Web and examine the textual
differences between the Dark Web and the Surface Web. We also assess the
performance of various methods of Dark Web page classification. Finally, we
compare CoDA with an existing public Dark Web dataset and evaluate their
suitability for various use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task. (arXiv:2204.06889v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06889">
<div class="article-summary-box-inner">
<span><p>Although transformer-based Neural Language Models demonstrate impressive
performance on a variety of tasks, their generalization abilities are not well
understood. They have been shown to perform strongly on subject-verb number
agreement in a wide array of settings, suggesting that they learned to track
syntactic dependencies during their training even without explicit supervision.
In this paper, we examine the extent to which BERT is able to perform
lexically-independent subject-verb number agreement (NA) on targeted syntactic
templates. To do so, we disrupt the lexical patterns found in naturally
occurring stimuli for each targeted structure in a novel fine-grained analysis
of BERT's behavior. Our results on nonce sentences suggest that the model
generalizes well for simple templates, but fails to perform
lexically-independent syntactic generalization when as little as one attractor
is present.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges for Open-domain Targeted Sentiment Analysis. (arXiv:2204.06893v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06893">
<div class="article-summary-box-inner">
<span><p>Since previous studies on open-domain targeted sentiment analysis are limited
in dataset domain variety and sentence level, we propose a novel dataset
consisting of 6,013 human-labeled data to extend the data domains in topics of
interest and document level. Furthermore, we offer a nested target annotation
schema to extract the complete sentiment information in documents, boosting the
practicality and effectiveness of open-domain targeted sentiment analysis.
Moreover, we leverage the pre-trained model BART in a sequence-to-sequence
generation method for the task. Benchmark results show that there exists large
room for improvement of open-domain targeted sentiment analysis. Meanwhile,
experiments have shown that challenges remain in the effective use of
open-domain data, long documents, the complexity of target structure, and
domain variances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Multi-task Learning Framework for Multi-goal Conversational Recommender Systems. (arXiv:2204.06923v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06923">
<div class="article-summary-box-inner">
<span><p>Recent years witnessed several advances in developing multi-goal
conversational recommender systems (MG-CRS) that can proactively attract users'
interests and naturally lead user-engaged dialogues with multiple
conversational goals and diverse topics. Four tasks are often involved in
MG-CRS, including Goal Planning, Topic Prediction, Item Recommendation, and
Response Generation. Most existing studies address only some of these tasks. To
handle the whole problem of MG-CRS, modularized frameworks are adopted where
each task is tackled independently without considering their interdependencies.
In this work, we propose a novel Unified MultI-goal conversational recommeNDer
system, namely UniMIND. In specific, we unify these four tasks with different
formulations into the same sequence-to-sequence (Seq2Seq) paradigm.
Prompt-based learning strategies are investigated to endow the unified model
with the capability of multi-task learning. Finally, the overall learning and
inference procedure consists of three stages, including multi-task learning,
prompt-based tuning, and inference. Experimental results on two MG-CRS
benchmarks (DuRecDial and TG-ReDial) show that UniMIND achieves
state-of-the-art performance on all tasks with a unified model. Extensive
analyses and discussions are provided for shedding some new perspectives for
MG-CRS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Source HamNoSys Parser for Multilingual Sign Language Encoding. (arXiv:2204.06924v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06924">
<div class="article-summary-box-inner">
<span><p>This paper presents our recent developments in the field of automatic
processing of sign language corpora using the Hamburg Sign Language Annotation
System (HamNoSys). We designed an automated tool to convert HamNoSys
annotations into numerical labels for defined initial features of body and hand
positions. Our proposed numerical multilabels greatly simplify the structure of
HamNoSys annotation without significant loss of gloss meaning. These numerical
multilabels can potentially be used to feed the machine learning models, which
would accelerate the development of vision-based sign language recognition. In
addition, this tool can assist experts in the annotation process to help
identify semantic errors. The code and sample annotations are publicly
available at https://github.com/hearai/parse-hamnosys.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Aspect Detection from Online Unsolicited Customer Reviews. (arXiv:2204.06964v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06964">
<div class="article-summary-box-inner">
<span><p>Within the context of review analytics, aspects are the features of products
and services at which customers target their opinions and sentiments. Aspect
detection helps product owners and service providers to identify shortcomings
and prioritize customers' needs, and hence, maintain revenues and mitigate
customer churn. Existing methods focus on detecting the surface form of an
aspect by training supervised learning methods that fall short when aspects are
latent in reviews. In this paper, we propose an unsupervised method to extract
latent occurrences of aspects. Specifically, we assume that a customer
undergoes a two-stage hypothetical generative process when writing a review:
(1) deciding on an aspect amongst the set of aspects available for the product
or service, and (2) writing the opinion words that are more interrelated to the
chosen aspect from the set of all words available in a language. We employ
latent Dirichlet allocation to learn the latent aspects distributions for
generating the reviews. Experimental results on benchmark datasets show that
our proposed method is able to improve the state of the art when the aspects
are latent with no surface form in reviews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge. (arXiv:2204.06970v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06970">
<div class="article-summary-box-inner">
<span><p>Cognitively plausible visual dialogue models should keep a mental scoreboard
of shared established facts in the dialogue context. We propose a theory-based
evaluation method for investigating to what degree models pretrained on the
VisDial dataset incrementally build representations that appropriately do
scorekeeping. Our conclusion is that the ability to make the distinction
between shared and privately known statements along the dialogue is moderately
present in the analysed models, but not always incrementally consistent, which
may partially be due to the limited need for grounding interactions in the
original task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XLMRQA: Open-Domain Question Answering on Vietnamese Wikipedia-based Textual Knowledge Source. (arXiv:2204.07002v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07002">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) is a natural language understanding task within the
fields of information retrieval and information extraction that has attracted
much attention from the computational linguistics and artificial intelligence
research community in recent years because of the strong development of machine
reading comprehension-based models. A reader-based QA system is a high-level
search engine that can find correct answers to queries or questions in
open-domain or domain-specific texts using machine reading comprehension (MRC)
techniques. The majority of advancements in data resources and machine-learning
approaches in the MRC and QA systems, on the other hand, especially in two
resource-rich languages such as English and Chinese. A low-resource language
like Vietnamese has witnessed a scarcity of research on QA systems. This paper
presents XLMRQA, the first Vietnamese QA system using a supervised
transformer-based reader on the Wikipedia-based textual knowledge source (using
the UIT-ViQuAD corpus), outperforming the two robust QA systems using deep
neural network models: DrQA and BERTserini with 24.46% and 6.28%, respectively.
From the results obtained on the three systems, we analyze the influence of
question types on the performance of the QA systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anti-Asian Hate Speech Detection via Data Augmented Semantic Relation Inference. (arXiv:2204.07010v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07010">
<div class="article-summary-box-inner">
<span><p>With the spreading of hate speech on social media in recent years, automatic
detection of hate speech is becoming a crucial task and has attracted attention
from various communities. This task aims to recognize online posts (e.g.,
tweets) that contain hateful information. The peculiarities of languages in
social media, such as short and poorly written content, lead to the difficulty
of learning semantics and capturing discriminative features of hate speech.
Previous studies have utilized additional useful resources, such as sentiment
hashtags, to improve the performance of hate speech detection. Hashtags are
added as input features serving either as sentiment-lexicons or extra context
information. However, our close investigation shows that directly leveraging
these features without considering their context may introduce noise to
classifiers. In this paper, we propose a novel approach to leverage sentiment
hashtags to enhance hate speech detection in a natural language inference
framework. We design a novel framework SRIC that simultaneously performs two
tasks: (1) semantic relation inference between online posts and sentiment
hashtags, and (2) sentiment classification on these posts. The semantic
relation inference aims to encourage the model to encode sentiment-indicative
information into representations of online posts. We conduct extensive
experiments on two real-world datasets and demonstrate the effectiveness of our
proposed framework compared with state-of-the-art representation learning
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rows from Many Sources: Enriching row completions from Wikidata with a pre-trained Language Model. (arXiv:2204.07014v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07014">
<div class="article-summary-box-inner">
<span><p>Row completion is the task of augmenting a given table of text and numbers
with additional, relevant rows. The task divides into two steps: subject
suggestion, the task of populating the main column; and gap filling, the task
of populating the remaining columns. We present state-of-the-art results for
subject suggestion and gap filling measured on a standard benchmark
(WikiTables). Our idea is to solve this task by harmoniously combining
knowledge base table interpretation and free text generation. We interpret the
table using the knowledge base to suggest new rows and generate metadata like
headers through property linking. To improve candidate diversity, we synthesize
additional rows using free text generation via GPT-3, and crucially, we exploit
the metadata we interpret to produce better prompts for text generation.
Finally, we verify that the additional synthesized content can be linked to the
knowledge base or a trusted web source such as Wikipedia.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Multi-task Learning with Task Dependency for Appeal Judgment Prediction. (arXiv:2204.07046v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07046">
<div class="article-summary-box-inner">
<span><p>Legal Judgment Prediction (LJP) aims to automatically predict judgment
results, such as charges, relevant law articles, and the term of penalty. It
plays a vital role in legal assistant systems and has become a popular research
topic in recent years. This paper concerns a worthwhile but not well-studied
LJP task, Appeal judgment Prediction (AJP), which predicts the judgment of an
appellate court on an appeal case based on the textual description of case
facts and grounds of appeal. There are two significant challenges in practice
to solve the AJP task. One is how to model the appeal judgment procedure
appropriately. The other is how to improve the interpretability of the
prediction results. We propose a Sequential Multi-task Learning Framework with
Task Dependency for Appeal Judgement Prediction (SMAJudge) to address these
challenges. SMAJudge utilizes two sequential components to model the complete
proceeding from the lower court to the appellate court and employs an attention
mechanism to make the prediction more explainable, which handles the challenges
of AJP effectively. Experimental results obtained with a dataset consisting of
more than 30K appeal judgment documents have revealed the effectiveness and
superiority of SMAJudge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">State of the Art in Artificial Intelligence applied to the Legal Domain. (arXiv:2204.07047v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07047">
<div class="article-summary-box-inner">
<span><p>While Artificial Intelligence applied to the legal domain is a topic with
origins in the last century, recent advances in Artificial Intelligence are
posed to revolutionize it. This work presents an overview and contextualizes
the main advances on the field of Natural Language Processing and how these
advances have been used to further the state of the art in legal text analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Evaluation Of Transformer Models For De-Identification Of Clinical Text Data. (arXiv:2204.07056v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07056">
<div class="article-summary-box-inner">
<span><p>Objective: To comparatively evaluate several transformer model architectures
at identifying protected health information (PHI) in the i2b2/UTHealth 2014
clinical text de-identification challenge corpus.
</p>
<p>Methods: The i2b2/UTHealth 2014 corpus contains N=1304 clinical notes
obtained from N=296 patients. Using a transfer learning framework, we fine-tune
several transformer model architectures on the corpus, including: BERT-base,
BERT-large, ROBERTA-base, ROBERTA-large, ALBERT-base and ALBERT-xxlarge. During
fine-tuning we vary the following model hyper-parameters: batch size, number
training epochs, learning rate and weight decay. We fine tune models on a
training data set, we evaluate and select optimally performing models on an
independent validation dataset, and lastly assess generalization performance on
a held-out test dataset. We assess model performance in terms of accuracy,
precision (positive predictive value), recall (sensitivity) and F1 score
(harmonic mean of precision and recall). We are interested in overall model
performance (PHI identified vs. PHI not identified), as well as PHI-specific
model performance.
</p>
<p>Results: We observe that the ROBERTA-large models perform best at identifying
PHI in the i2b2/UTHealth 2014 corpus, achieving &gt;99% overall accuracy and 96.7%
recall/precision on the heldout test corpus. Performance was good across many
PHI classes; however, accuracy/precision/recall decreased for identification of
the following entity classes: professions, organizations, ages, and certain
locations.
</p>
<p>Conclusions: Transformers are a promising model class/architecture for
clinical text de-identification. With minimal hyper-parameter tuning
transformers afford researchers/clinicians the opportunity to obtain (near)
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate Speech Classification Using SVM and Naive BAYES. (arXiv:2204.07057v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07057">
<div class="article-summary-box-inner">
<span><p>The spread of hatred that was formerly limited to verbal communications has
rapidly moved over the Internet. Social media and community forums that allow
people to discuss and express their opinions are becoming platforms for the
spreading of hate messages. Many countries have developed laws to avoid online
hate speech. They hold the companies that run the social media responsible for
their failure to eliminate hate speech. But as online content continues to
grow, so does the spread of hate speech However, manual analysis of hate speech
on online platforms is infeasible due to the huge amount of data as it is
expensive and time consuming. Thus, it is important to automatically process
the online user contents to detect and remove hate speech from online media.
Many recent approaches suffer from interpretability problem which means that it
can be difficult to understand why the systems make the decisions they do.
Through this work, some solutions for the problem of automatic detection of
hate messages were proposed using Support Vector Machine (SVM) and Na\"ive
Bayes algorithms. This achieved near state-of-the-art performance while being
simpler and producing more easily interpretable decisions than other methods.
Empirical evaluation of this technique has resulted in a classification
accuracy of approximately 99% and 50% for SVM and NB respectively over the test
set.
</p>
<p>Keywords: classification; hate speech; feature extraction, algorithm,
supervised learning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue Strategy Adaptation to New Action Sets Using Multi-dimensional Modelling. (arXiv:2204.07082v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07082">
<div class="article-summary-box-inner">
<span><p>A major bottleneck for building statistical spoken dialogue systems for new
domains and applications is the need for large amounts of training data. To
address this problem, we adopt the multi-dimensional approach to dialogue
management and evaluate its potential for transfer learning. Specifically, we
exploit pre-trained task-independent policies to speed up training for an
extended task-specific action set, in which the single summary action for
requesting a slot is replaced by multiple slot-specific request actions. Policy
optimisation and evaluation experiments using an agenda-based user simulator
show that with limited training data, much better performance levels can be
achieved when using the proposed multi-dimensional adaptation method. We
confirm this improvement in a crowd-sourced human user evaluation of our spoken
dialogue system, comparing partially trained policies. The multi-dimensional
system (with adaptation on limited training data in the target scenario)
outperforms the one-dimensional baseline (without adaptation on the same amount
of training data) by 7% perceived success rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Dual Encoder Architectures for Question Answering. (arXiv:2204.07120v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07120">
<div class="article-summary-box-inner">
<span><p>Dual encoders have been used for question-answering (QA) and information
retrieval (IR) tasks with good results. There are two major types of dual
encoders, Siamese Dual Encoders (SDE), with parameters shared across two
encoders, and Asymmetric Dual Encoder (ADE), with two distinctly parameterized
encoders. In this work, we explore the dual encoder architectures for QA
retrieval tasks. By evaluating on MS MARCO and the MultiReQA benchmark, we show
that SDE performs significantly better than ADE. We further propose three
different improved versions of ADEs. Based on the evaluation of QA retrieval
tasks and direct analysis of the embeddings, we demonstrate that sharing
parameters in projection layers would enable ADEs to perform competitively with
SDEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Semantic Aware Pre-training for Few-shot Text Classification. (arXiv:2204.07128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07128">
<div class="article-summary-box-inner">
<span><p>In text classification tasks, useful information is encoded in the label
names. Label semantic aware systems have leveraged this information for
improved text classification performance during fine-tuning and prediction.
However, use of label-semantics during pre-training has not been extensively
explored. We therefore propose Label Semantic Aware Pre-training (LSAP) to
improve the generalization and data efficiency of text classification systems.
LSAP incorporates label semantics into pre-trained generative models (T5 in our
case) by performing secondary pre-training on labeled sentences from a variety
of domains. As domain-general pre-training requires large amounts of data, we
develop a filtering and labeling pipeline to automatically create
sentence-label pairs from unlabeled text. We perform experiments on intent
(ATIS, Snips, TOPv2) and topic classification (AG News, Yahoo! Answers). LSAP
obtains significant accuracy improvements over state-of-the-art models for
few-shot text classification while maintaining performance comparable to state
of the art in high-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable and Robust Self-Learning for Skill Routing in Large-Scale Conversational AI Systems. (arXiv:2204.07135v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07135">
<div class="article-summary-box-inner">
<span><p>Skill routing is an important component in large-scale conversational
systems. In contrast to traditional rule-based skill routing, state-of-the-art
systems use a model-based approach to enable natural conversations. To provide
supervision signal required to train such models, ideas such as human
annotation, replication of a rule-based system, relabeling based on user
paraphrases, and bandit-based learning were suggested. However, these
approaches: (a) do not scale in terms of the number of skills and skill
on-boarding, (b) require a very costly expert annotation/rule-design, (c)
introduce risks in the user experience with each model update. In this paper,
we present a scalable self-learning approach to explore routing alternatives
without causing abrupt policy changes that break the user experience, learn
from the user interaction, and incrementally improve the routing via frequent
model refreshes. To enable such robust frequent model updates, we suggest a
simple and effective approach that ensures controlled policy updates for
individual domains, followed by an off-policy evaluation for making deployment
decisions without any need for lengthy A/B experimentation. We conduct various
offline and online A/B experiments on a commercial large-scale conversational
system to demonstrate the effectiveness of the proposed method in real-world
production settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLUES: A Benchmark for Learning Classifiers using Natural Language Explanations. (arXiv:2204.07142v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07142">
<div class="article-summary-box-inner">
<span><p>Supervised learning has traditionally focused on inductive learning by
observing labeled examples of a task. In contrast, humans have the ability to
learn new concepts from language. Here, we explore training zero-shot
classifiers for structured data purely from language. For this, we introduce
CLUES, a benchmark for Classifier Learning Using natural language ExplanationS,
consisting of a range of classification tasks over structured data along with
natural language supervision in the form of explanations. CLUES consists of 36
real-world and 144 synthetic classification tasks. It contains crowdsourced
explanations describing real-world tasks from multiple teachers and
programmatically generated explanations for the synthetic tasks. To model the
influence of explanations in classifying an example, we develop ExEnt, an
entailment-based model that learns classifiers using explanations. ExEnt
generalizes up to 18% better (relative) on novel tasks than a baseline that
does not use explanations. We delineate key challenges for automated learning
from explanations, addressing which can lead to progress on CLUES in the
future. Code and datasets are available at: https://clues-benchmark.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FREDA: Flexible Relation Extraction Data Annotation. (arXiv:2204.07150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07150">
<div class="article-summary-box-inner">
<span><p>To effectively train accurate Relation Extraction models, sufficient and
properly labeled data is required. Adequately labeled data is difficult to
obtain and annotating such data is a tricky undertaking. Previous works have
shown that either accuracy has to be sacrificed or the task is extremely
time-consuming, if done accurately. We are proposing an approach in order to
produce high-quality datasets for the task of Relation Extraction quickly.
Neural models, trained to do Relation Extraction on the created datasets,
achieve very good results and generalize well to other datasets. In our study,
we were able to annotate 10,022 sentences for 19 relations in a reasonable
amount of time, and trained a commonly used baseline model for each relation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Studying Alignment in a Collaborative Learning Activity via Automatic Methods: The Link Between What We Say and Do. (arXiv:2104.04429v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04429">
<div class="article-summary-box-inner">
<span><p>A dialogue is successful when there is alignment between the speakers at
different linguistic levels. In this work, we consider the dialogue occurring
between interlocutors engaged in a collaborative learning task, where they are
not only evaluated on how well they performed, but also on how much they
learnt. The main contribution of this work is to propose new automatic measures
to study alignment; focusing on verbal (lexical) alignment, and behavioral
alignment (when an instruction given by one was followed with concrete actions
by another). A second contribution of our work is to study how spontaneous
speech phenomena are used in the process of alignment. Lastly, we make public
the dataset to study alignment in educational dialogues. Our results show that
all teams verbally and behaviourally align to some degree regardless of their
performance and learning, and our measures capture that teams that did not
succeed in the task were simply slower to collaborate. Thus we find that teams
that performed better, were faster to align. Furthermore, our methodology
captures a productive period that includes the time where the interlocutors
came up with their best solutions. We also find that well-performing teams
verbalise the marker "oh" more when they are behaviourally aligned, compared to
other times in the dialogue; showing that this marker is an important cue in
alignment. To the best of our knowledge, we are the first to study the role of
"oh" as an information management marker in a behavioral context (i.e. in
connection to actions taken in a physical environment), compared to only a
verbal one. Our measures contribute to the research in the field of educational
dialogue and the intersection between dialogue and collaborative learning
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings. (arXiv:2104.08928v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08928">
<div class="article-summary-box-inner">
<span><p>Unstructured text provides decision-makers with a rich data source in many
domains, ranging from product reviews in retailing to nursing notes in
healthcare. To leverage this information, words are typically translated into
word embeddings -- vectors that encode the semantic relationships between words
-- through unsupervised learning algorithms such as matrix factorization.
However, learning word embeddings from new domains with limited training data
can be challenging, because the meaning/usage may be different in the new
domain, e.g., the word "positive" typically has positive sentiment, but often
has negative sentiment in medical notes since it may imply that a patient is
tested positive for a disease. Intuitively, we expect that only a small number
of domain-specific words may have new meanings/usages. We propose an intuitive
two-stage estimator that exploits this structure via a group-sparse penalty to
efficiently transfer learn domain-specific word embeddings by combining
large-scale text corpora (such as Wikipedia) with limited domain-specific text
data. We bound the generalization error of our estimator, proving that it can
achieve the same accuracy (compared to not transfer learning) with
substantially less domain-specific data when only a small number of embeddings
are altered between domains. Our results provide the first bounds on
group-sparse matrix factorization, which may be of independent interest. We
empirically evaluate the effectiveness of our approach compared to
state-of-the-art fine-tuning heuristics from natural language processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Systematic Style Differences between Unsupervised and Supervised MT and an Application for High-Resource Machine Translation. (arXiv:2106.15818v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15818">
<div class="article-summary-box-inner">
<span><p>Modern unsupervised machine translation (MT) systems reach reasonable
translation quality under clean and controlled data conditions. As the
performance gap between supervised and unsupervised MT narrows, it is
interesting to ask whether the different training methods result in
systematically different output beyond what is visible via quality metrics like
adequacy or BLEU. We compare translations from supervised and unsupervised MT
systems of similar quality, finding that unsupervised output is more fluent and
more structurally different in comparison to human translation than is
supervised MT. We then demonstrate a way to combine the benefits of both
methods into a single system which results in improved adequacy and fluency as
rated by human evaluators. Our results open the door to interesting discussions
about how supervised and unsupervised MT might be different yet
mutually-beneficial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Your fairness may vary: Pretrained language model fairness in toxic text classification. (arXiv:2108.01250v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01250">
<div class="article-summary-box-inner">
<span><p>The popularity of pretrained language models in natural language processing
systems calls for a careful evaluation of such models in down-stream tasks,
which have a higher potential for societal impact. The evaluation of such
systems usually focuses on accuracy measures. Our findings in this paper call
for attention to be paid to fairness measures as well. Through the analysis of
more than a dozen pretrained language models of varying sizes on two toxic text
classification tasks (English), we demonstrate that focusing on accuracy
measures alone can lead to models with wide variation in fairness
characteristics. Specifically, we observe that fairness can vary even more than
accuracy with increasing training data size and different random
initializations. At the same time, we find that little of the fairness
variation is explained by model size, despite claims in the literature. To
improve model fairness without retraining, we show that two post-processing
methods developed for structured, tabular data can be successfully applied to a
range of pretrained language models. Warning: This paper contains samples of
offensive text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twitter User Representation Using Weakly Supervised Graph Embedding. (arXiv:2108.08988v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08988">
<div class="article-summary-box-inner">
<span><p>Social media platforms provide convenient means for users to participate in
multiple online activities on various contents and create fast widespread
interactions. However, this rapidly growing access has also increased the
diverse information, and characterizing user types to understand people's
lifestyle decisions shared in social media is challenging. In this paper, we
propose a weakly supervised graph embedding based framework for understanding
user types. We evaluate the user embedding learned using weak supervision over
well-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.
Experiments on real-world datasets demonstrate that the proposed framework
outperforms the baselines for detecting user types. Finally, we illustrate data
analysis on different types of users (e.g., practitioner vs. promotional) from
our dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our
method for constructing user representation readily generalizes to other
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summ^N: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents. (arXiv:2110.10150v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10150">
<div class="article-summary-box-inner">
<span><p>Text summarization helps readers capture salient information from documents,
news, interviews, and meetings. However, most state-of-the-art pretrained
language models (LM) are unable to efficiently process long text for many
summarization tasks. In this paper, we propose Summ$^N$, a simple, flexible,
and effective multi-stage framework for input texts that are longer than the
maximum context length of typical pretrained LMs. Summ$^N$ first splits the
data samples and generates a coarse summary in multiple stages and then
produces the final fine-grained summary based on it. Our framework can process
input text of arbitrary length by adjusting the number of stages while keeping
the LM input size fixed. Moreover, it can deal with both single-source
documents and dialogues, and it can be used on top of different backbone
abstractive summarization models. To the best of our knowledge, Summ$^N$ is the
first multi-stage split-then-summarize framework for long input summarization.
Our experiments demonstrate that Summ$^N$ outperforms previous state-of-the-art
methods by improving ROUGE scores on three long meeting summarization datasets
AMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long
document summarization dataset GovReport. Our data and code are available at
https://github.com/psunlpgroup/Summ-N.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EventNarrative: A large-scale Event-centric Dataset for Knowledge Graph-to-Text Generation. (arXiv:2111.00276v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00276">
<div class="article-summary-box-inner">
<span><p>We introduce EventNarrative, a knowledge graph-to-text dataset from publicly
available open-world knowledge graphs. Given the recent advances in
event-driven Information Extraction (IE), and that prior research on
graph-to-text only focused on entity-driven KGs, this paper focuses on
event-centric data. However, our data generation system can still be adapted to
other other types of KG data. Existing large-scale datasets in the
graph-to-text area are non-parallel, meaning there is a large disconnect
between the KGs and text. The datasets that have a paired KG and text, are
small scale and manually generated or generated without a rich ontology, making
the corresponding graphs sparse. Furthermore, these datasets contain many
unlinked entities between their KG and text pairs. EventNarrative consists of
approximately 230,000 graphs and their corresponding natural language text, 6
times larger than the current largest parallel dataset. It makes use of a rich
ontology, all of the KGs entities are linked to the text, and our manual
annotations confirm a high data quality. Our aim is two-fold: help break new
ground in event-centric research where data is lacking, and to give researchers
a well-defined, large-scale dataset in order to better evaluate existing and
future knowledge graph-to-text models. We also evaluate two types of baseline
on EventNarrative: a graph-to-text specific model and two state-of-the-art
language models, which previous work has shown to be adaptable to the knowledge
graph-to-text domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VIRT: Improving Representation-based Models for Text Matching through Virtual Interaction. (arXiv:2112.04195v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04195">
<div class="article-summary-box-inner">
<span><p>With the booming of pre-trained transformers, representation-based models
based on Siamese transformer encoders have become mainstream techniques for
efficient text matching. However, these models suffer from severe performance
degradation due to the lack of interaction between the text pair, compared with
interaction-based models. Prior arts attempt to address this through performing
extra interaction for Siamese encoded representations, while the interaction
during encoding is still ignored. To remedy this, we propose a \textit{Virtual}
InteRacTion mechanism (VIRT) to transfer interactive knowledge from
interaction-based models into Siamese encoders through attention map
distillation. As a train-time-only component, VIRT could completely maintain
the high efficiency of the Siamese structure and brings no extra computation
cost during inference. To fully utilize the learned interactive knowledge, we
further design a VIRT-adapted interaction strategy. Experimental results on
multiple text matching datasets demonstrate that our method outperforms
state-of-the-art representation-based models. What's more, VIRT can be easily
integrated into existing representation-based methods to achieve further
improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Materialized Knowledge Bases from Commonsense Transformers. (arXiv:2112.14815v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14815">
<div class="article-summary-box-inner">
<span><p>Starting from the COMET methodology by Bosselut et al. (2019), generating
commonsense knowledge directly from pre-trained language models has recently
received significant attention. Surprisingly, up to now no materialized
resource of commonsense knowledge generated this way is publicly available.
This paper fills this gap, and uses the materialized resources to perform a
detailed analysis of the potential of this approach in terms of precision and
recall. Furthermore, we identify common problem cases, and outline use cases
enabled by materialized resources. We posit that the availability of these
resources is important for the advancement of the field, as it enables an
off-the-shelf-use of the resulting knowledge, as well as further analyses on
its strengths and weaknesses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question rewriting? Assessing its importance for conversational question answering. (arXiv:2201.09146v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09146">
<div class="article-summary-box-inner">
<span><p>In conversational question answering, systems must correctly interpret the
interconnected interactions and generate knowledgeable answers, which may
require the retrieval of relevant information from a background repository.
Recent approaches to this problem leverage neural language models, although
different alternatives can be considered in terms of modules for (a)
representing user questions in context, (b) retrieving the relevant background
information, and (c) generating the answer. This work presents a conversational
question answering system designed specifically for the Search-Oriented
Conversational AI (SCAI) shared task, and reports on a detailed analysis of its
question rewriting module. In particular, we considered different variations of
the question rewriting module to evaluate the influence on the subsequent
components, and performed a careful analysis of the results obtained with the
best system configuration. Our system achieved the best performance in the
shared task and our analysis emphasizes the importance of the conversation
context representation for the overall system performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Frustratingly Simple Approach for End-to-End Image Captioning. (arXiv:2201.12723v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12723">
<div class="article-summary-box-inner">
<span><p>Image Captioning is a fundamental task to join vision and language,
concerning about cross-modal understanding and text generation. Recent years
witness the emerging attention on image captioning. Most of existing works
follow a traditional two-stage training paradigm. Before training the
captioning models, an extra object detector is utilized to recognize the
objects in the image at first. However, they require sizeable datasets with
fine-grained object annotation for training the object detector, which is a
daunting task. In addition, the errors of the object detectors are easy to
propagate to the following captioning models, degenerating models' performance.
To alleviate such defects, we propose a frustratingly simple but highly
effective end-to-end image captioning framework, Visual Conditioned GPT
(VC-GPT), by connecting the pre-trained visual encoder (CLIP-ViT) and language
decoder (GPT2). Different from the vanilla connection method that directly
inserts the cross-attention modules into GPT2, we come up with a self-ensemble
cross-modal fusion mechanism that comprehensively considers both the single-
and cross-modal knowledge. As a result, we do not need extra object detectors
for model training. Experimental results conducted on three popular image
captioning benchmarks (MSCOCO, Flickr30k and NoCaps) demonstrate that our
VC-GPT achieves either the best or the second-best performance across all
evaluation metrics over extensive baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers and the representation of biomedical background knowledge. (arXiv:2202.02432v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02432">
<div class="article-summary-box-inner">
<span><p>BioBERT and BioMegatron are Transformers models adapted for the biomedical
domain based on publicly available biomedical corpora. As such, they have the
potential to encode large-scale biological knowledge. We investigate the
encoding and representation of biological knowledge in these models, and its
potential utility to support inference in cancer precision medicine - namely,
the interpretation of the clinical significance of genomic alterations. We
compare the performance of different transformer baselines; we use probing to
determine the consistency of encodings for distinct entities; and we use
clustering methods to compare and contrast the internal properties of the
embeddings for genes, variants, drugs and diseases. We show that these models
do indeed encode biological knowledge, although some of this is lost in
fine-tuning for specific tasks. Finally, we analyse how the models behave with
regard to biases and imbalances in the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SkillNet: A Sparsely Activated Model for General-Purpose Natural Language Understanding. (arXiv:2203.03312v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03312">
<div class="article-summary-box-inner">
<span><p>Prevailing deep models are single-purpose and overspecialize at individual
tasks. However, when being extended to new tasks, they typically forget
previously learned skills and learn from scratch. We address this issue by
introducing SkillNet, a general-purpose model that stitches together existing
skills to learn new tasks more effectively. The key feature of our approach is
that it is sparsely activated guided by predefined skills. Different from
traditional dense models that always activate all the model parameters,
SkillNet only activates parts of the model parameters whose skills are relevant
to the target task. When learning for a new task, our approach precisely
activates required skills and also provides an option to add new skills. We
evaluate on natural language understandings tasks and have the following
findings. First, with only one model checkpoint, SkillNet performs better than
task-specific fine-tuning and two multi-task learning baselines (i.e., dense
model and Mixture-of-Experts model) on six tasks. Second, sparsely activated
pre-training further improves the overall performance. Third, SkillNet
significantly outperforms baseline systems when being extended to new tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chart-to-Text: A Large-Scale Benchmark for Chart Summarization. (arXiv:2203.06486v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06486">
<div class="article-summary-box-inner">
<span><p>Charts are commonly used for exploring data and communicating insights.
Generating natural language summaries from charts can be very helpful for
people in inferring key insights that would otherwise require a lot of
cognitive and perceptual efforts. We present Chart-to-text, a large-scale
benchmark with two datasets and a total of 44,096 charts covering a wide range
of topics and chart types. We explain the dataset construction process and
analyze the datasets. We also introduce a number of state-of-the-art neural
models as baselines that utilize image captioning and data-to-text generation
techniques to tackle two problem variations: one assumes the underlying data
table of the chart is available while the other needs to extract data from
chart images. Our analysis with automatic and human evaluation shows that while
our best models usually generate fluent summaries and yield reasonable BLEU
scores, they also suffer from hallucinations and factual errors as well as
difficulties in correctly explaining complex patterns and trends in charts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning. (arXiv:2203.13628v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13628">
<div class="article-summary-box-inner">
<span><p>Inspired by the recent progress in self-supervised learning for computer
vision, in this paper, through the DeLoRes learning framework, we introduce two
new general-purpose audio representation learning approaches, the DeLoRes-S and
DeLoRes-M. Our main objective is to make our network learn representations in a
resource-constrained setting (both data and compute), that can generalize well
across a diverse set of downstream tasks. Inspired from the Barlow Twins
objective function, we propose to learn embeddings that are invariant to
distortions of an input audio sample, while making sure that they contain
non-redundant information about the sample. To achieve this, we measure the
cross-correlation matrix between the outputs of two identical networks fed with
distorted versions of an audio segment sampled from an audio file and make it
as close to the identity matrix as possible. We call this the DeLoRes learning
framework, which we employ in different fashions with the DeLoRes-S and
DeLoRes-M. We use a combination of a small subset of the large-scale AudioSet
dataset and FSD50K for self-supervised learning and are able to learn with less
than half the parameters compared to state-of-the-art algorithms. For
evaluation, we transfer these learned representations to 11 downstream
classification tasks, including speech, music, and animal sounds, and achieve
state-of-the-art results on 7 out of 11 tasks on linear evaluation with
DeLoRes-M and show competitive results with DeLoRes-S, even when pre-trained
using only a fraction of the total data when compared to prior art. Our
transfer learning evaluation setup also shows extremely competitive results for
both DeLoRes-S and DeLoRes-M, with DeLoRes-M achieving state-of-the-art in 4
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models. (arXiv:2203.15863v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15863">
<div class="article-summary-box-inner">
<span><p>Large-scale auto-regressive language models pretrained on massive text have
demonstrated their impressive ability to perform new natural language tasks
with only a few text examples, without the need for fine-tuning. Recent studies
further show that such a few-shot learning ability can be extended to the
text-image setting by training an encoder to encode the images into embeddings
functioning like the text embeddings of the language model. Interested in
exploring the possibility of transferring the few-shot learning ability to the
audio-text setting, we propose a novel speech understanding framework,
WavPrompt, where we finetune a wav2vec model to generate a sequence of audio
embeddings understood by the language model. We show that WavPrompt is a
few-shot learner that can perform speech understanding tasks better than a
naive text baseline. We conduct detailed ablation studies on different
components and hyperparameters to empirically identify the best model
configuration. In addition, we conduct a non-speech understanding experiment to
show WavPrompt can extract more information than just the transcriptions. Code
is available at https://github.com/Hertin/WavPrompt
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3M: Multi-loss, Multi-path and Multi-level Neural Networks for speech recognition. (arXiv:2204.03178v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03178">
<div class="article-summary-box-inner">
<span><p>Recently, Conformer based CTC/AED model has become a mainstream architecture
for ASR. In this paper, based on our prior work, we identify and integrate
several approaches to achieve further improvements for ASR tasks, which we
denote as multi-loss, multi-path and multi-level, summarized as "3M" model.
Specifically, multi-loss refers to the joint CTC/AED loss and multi-path
denotes the Mixture-of-Experts(MoE) architecture which can effectively increase
the model capacity without remarkably increasing computation cost. Multi-level
means that we introduce auxiliary loss at multiple level of a deep model to
help training. We evaluate our proposed method on the public WenetSpeech
dataset and experimental results show that the proposed method provides
12.2%-17.6% relative CER improvement over the baseline model trained by Wenet
toolkit. On our large scale dataset of 150k hours corpus, the 3M model has also
shown obvious superiority over the baseline Conformer model. Code is publicly
available at https://github.com/tencent-ailab/3m-asr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Embeddings Are Capable of Capturing Rhythmic Similarity of Words. (arXiv:2204.04833v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04833">
<div class="article-summary-box-inner">
<span><p>Word embedding systems such as Word2Vec and GloVe are well-known in deep
learning approaches to NLP. This is largely due to their ability to capture
semantic relationships between words. In this work we investigated their
usefulness in capturing rhythmic similarity of words instead. The results show
that vectors these embeddings assign to rhyming words are more similar to each
other, compared to the other words. It is also revealed that GloVe performs
relatively better than Word2Vec in this regard. We also proposed a first of its
kind metric for quantifying rhythmic similarity of a pair of words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GERE: Generative Evidence Retrieval for Fact Verification. (arXiv:2204.05511v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05511">
<div class="article-summary-box-inner">
<span><p>Fact verification (FV) is a challenging task which aims to verify a claim
using multiple evidential sentences from trustworthy corpora, e.g., Wikipedia.
Most existing approaches follow a three-step pipeline framework, including
document retrieval, sentence retrieval and claim verification. High-quality
evidences provided by the first two steps are the foundation of the effective
reasoning in the last step. Despite being important, high-quality evidences are
rarely studied by existing works for FV, which often adopt the off-the-shelf
models to retrieve relevant documents and sentences in an
"index-retrieve-then-rank" fashion. This classical approach has clear drawbacks
as follows: i) a large document index as well as a complicated search process
is required, leading to considerable memory and computational overhead; ii)
independent scoring paradigms fail to capture the interactions among documents
and sentences in ranking; iii) a fixed number of sentences are selected to form
the final evidence set. In this work, we propose GERE, the first system that
retrieves evidences in a generative fashion, i.e., generating the document
titles as well as evidence sentence identifiers. This enables us to mitigate
the aforementioned technical issues since: i) the memory and computational cost
is greatly reduced because the document index is eliminated and the heavy
ranking process is replaced by a light generative process; ii) the dependency
between documents and that between sentences could be captured via sequential
generation process; iii) the generative formulation allows us to dynamically
select a precise set of relevant evidences for each claim. The experimental
results on the FEVER dataset show that GERE achieves significant improvements
over the state-of-the-art baselines, with both time-efficiency and
memory-efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection. (arXiv:2204.05515v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05515">
<div class="article-summary-box-inner">
<span><p>Compared with unimodal data, multimodal data can provide more features to
help the model analyze the sentiment of data. Previous research works rarely
consider token-level feature fusion, and few works explore learning the common
features related to sentiment in multimodal data to help the model fuse
multimodal features. In this paper, we propose a Contrastive Learning and
Multi-Layer Fusion (CLMLF) method for multimodal sentiment detection.
Specifically, we first encode text and image to obtain hidden representations,
and then use a multi-layer fusion module to align and fuse the token-level
features of text and image. In addition to the sentiment analysis task, we also
designed two contrastive learning tasks, label based contrastive learning and
data based contrastive learning tasks, which will help the model learn common
features related to sentiment in multimodal data. Extensive experiments
conducted on three publicly available multimodal datasets demonstrate the
effectiveness of our approach for multimodal sentiment detection compared with
existing methods. The codes are available for use at
https://github.com/Link-Li/CLMLF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification. (arXiv:2204.06305v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06305">
<div class="article-summary-box-inner">
<span><p>Prompt-based learning (i.e., prompting) is an emerging paradigm for
exploiting knowledge learned by a pretrained language model. In this paper, we
propose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method
to automatically select label mappings for few-shot text classification with
prompting. Our method exploits one-to-many label mappings and a
statistics-based algorithm to select label mappings given a prompt template.
Our experiments demonstrate that AMuLaP achieves competitive performance on the
GLUE benchmark without human effort or external resources.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating Structural Disparities for Face Models. (arXiv:2204.06562v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06562">
<div class="article-summary-box-inner">
<span><p>In machine learning, disparity metrics are often defined by measuring the
difference in the performance or outcome of a model, across different
sub-populations (groups) of datapoints. Thus, the inputs to disparity
quantification consist of a model's predictions $\hat{y}$, the ground-truth
labels for the predictions $y$, and group labels $g$ for the data points.
Performance of the model for each group is calculated by comparing $\hat{y}$
and $y$ for the datapoints within a specific group, and as a result, disparity
of performance across the different groups can be calculated. In many real
world scenarios however, group labels ($g$) may not be available at scale
during training and validation time, or collecting them might not be feasible
or desirable as they could often be sensitive information. As a result,
evaluating disparity metrics across categorical groups would not be feasible.
On the other hand, in many scenarios noisy groupings may be obtainable using
some form of a proxy, which would allow measuring disparity metrics across
sub-populations. Here we explore performing such analysis on computer vision
models trained on human faces, and on tasks such as face attribute prediction
and affect estimation. Our experiments indicate that embeddings resulting from
an off-the-shelf face recognition model, could meaningfully serve as a proxy
for such estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Character-focused Video Thumbnail Retrieval. (arXiv:2204.06563v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06563">
<div class="article-summary-box-inner">
<span><p>We explore retrieving character-focused video frames as candidates for being
video thumbnails. To evaluate each frame of the video based on the character(s)
present in it, characters (faces) are evaluated in two aspects:
Facial-expression: We train a CNN model to measure whether a face has an
acceptable facial expression for being in a video thumbnail. This model is
trained to distinguish faces extracted from artworks/thumbnails, from faces
extracted from random frames of videos. Prominence and interactions:
Character(s) in the thumbnail should be important character(s) in the video, to
prevent the algorithm from suggesting non-representative frames as candidates.
We use face clustering to identify the characters in the video, and form a
graph in which the prominence (frequency of appearance) of the character(s),
and their interactions (co-occurrence) are captured. We use this graph to infer
the relevance of the characters present in each candidate frame. Once every
face is scored based on the two criteria above, we infer frame level scores by
combining the scores for all the faces within a frame.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OccAM's Laser: Occlusion-based Attribution Maps for 3D Object Detectors on LiDAR Data. (arXiv:2204.06577v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06577">
<div class="article-summary-box-inner">
<span><p>While 3D object detection in LiDAR point clouds is well-established in
academia and industry, the explainability of these models is a largely
unexplored field. In this paper, we propose a method to generate attribution
maps for the detected objects in order to better understand the behavior of
such models. These maps indicate the importance of each 3D point in predicting
the specific objects. Our method works with black-box models: We do not require
any prior knowledge of the architecture nor access to the model's internals,
like parameters, activations or gradients. Our efficient perturbation-based
approach empirically estimates the importance of each point by testing the
model with randomly generated subsets of the input point cloud. Our
sub-sampling strategy takes into account the special characteristics of LiDAR
data, such as the depth-dependent point density. We show a detailed evaluation
of the attribution maps and demonstrate that they are interpretable and highly
informative. Furthermore, we compare the attribution maps of recent 3D object
detection architectures to provide insights into their decision-making
processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Illumination-Invariant Active Camera Relocalization for Fine-Grained Change Detection in the Wild. (arXiv:2204.06580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06580">
<div class="article-summary-box-inner">
<span><p>Active camera relocalization (ACR) is a new problem in computer vision that
significantly reduces the false alarm caused by image distortions due to camera
pose misalignment in fine-grained change detection (FGCD). Despite the fruitful
achievements that ACR can support, it still remains a challenging problem
caused by the unstable results of relative pose estimation, especially for
outdoor scenes, where the lighting condition is out of control, i.e., the twice
observations may have highly varied illuminations. This paper studies an
illumination-invariant active camera relocalization method, it improves both in
relative pose estimation and scale estimation. We use plane segments as an
intermediate representation to facilitate feature matching, thus further
boosting pose estimation robustness and reliability under lighting variances.
Moreover, we construct a linear system to obtain the absolute scale in each ACR
iteration by minimizing the image warping error, thus, significantly reduce the
time consume of ACR process, it is nearly $1.6$ times faster than the
state-of-the-art ACR strategy. Our work greatly expands the feasibility of
real-world fine-grained change monitoring tasks for cultural heritages.
Extensive experiments tests and real-world applications verify the
effectiveness and robustness of the proposed pose estimation method using for
ACR tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Relation Learning for Regression and Its Application to Brain Age Estimation. (arXiv:2204.06598v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06598">
<div class="article-summary-box-inner">
<span><p>Most deep learning models for temporal regression directly output the
estimation based on single input images, ignoring the relationships between
different images. In this paper, we propose deep relation learning for
regression, aiming to learn different relations between a pair of input images.
Four non-linear relations are considered: "cumulative relation", "relative
relation", "maximal relation" and "minimal relation". These four relations are
learned simultaneously from one deep neural network which has two parts:
feature extraction and relation regression. We use an efficient convolutional
neural network to extract deep features from the pair of input images and apply
a Transformer for relation learning. The proposed method is evaluated on a
merged dataset with 6,049 subjects with ages of 0-97 years using 5-fold
cross-validation for the task of brain age estimation. The experimental results
have shown that the proposed method achieved a mean absolute error (MAE) of
2.38 years, which is lower than the MAEs of 8 other state-of-the-art algorithms
with statistical significance (p$&lt;$0.05) in paired T-test (two-side).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Metrical Reconstruction of Human Faces. (arXiv:2204.06607v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06607">
<div class="article-summary-box-inner">
<span><p>Face reconstruction and tracking is a building block of numerous applications
in AR/VR, human-machine interaction, as well as medical applications. Most of
these applications rely on a metrically correct prediction of the shape,
especially, when the reconstructed subject is put into a metrical context
(i.e., when there is a reference object of known size). A metrical
reconstruction is also needed for any application that measures distances and
dimensions of the subject (e.g., to virtually fit a glasses frame).
State-of-the-art methods for face reconstruction from a single image are
trained on large 2D image datasets in a self-supervised fashion. However, due
to the nature of a perspective projection they are not able to reconstruct the
actual face dimensions, and even predicting the average human face outperforms
some of these methods in a metrical sense. To learn the actual shape of a face,
we argue for a supervised training scheme. Since there exists no large-scale 3D
dataset for this task, we annotated and unified small- and medium-scale
databases. The resulting unified dataset is still a medium-scale dataset with
more than 2k identities and training purely on it would lead to overfitting. To
this end, we take advantage of a face recognition network pretrained on a
large-scale 2D image dataset, which provides distinct features for different
faces and is robust to expression, illumination, and camera changes. Using
these features, we train our face shape estimator in a supervised fashion,
inheriting the robustness and generalization of the face recognition network.
Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art
reconstruction methods by a large margin, both on current non-metric benchmarks
as well as on our metric benchmarks (15% and 24% lower average error on NoW,
respectively).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Memory Management for Video Object Segmentation. (arXiv:2204.06626v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06626">
<div class="article-summary-box-inner">
<span><p>Matching-based networks have achieved state-of-the-art performance for video
object segmentation (VOS) tasks by storing every-k frames in an external memory
bank for future inference. Storing the intermediate frames' predictions
provides the network with richer cues for segmenting an object in the current
frame. However, the size of the memory bank gradually increases with the length
of the video, which slows down inference speed and makes it impractical to
handle arbitrary length videos.
</p>
<p>This paper proposes an adaptive memory bank strategy for matching-based
networks for semi-supervised video object segmentation (VOS) that can handle
videos of arbitrary length by discarding obsolete features. Features are
indexed based on their importance in the segmentation of the objects in
previous frames. Based on the index, we discard unimportant features to
accommodate new features. We present our experiments on DAVIS 2016, DAVIS 2017,
and Youtube-VOS that demonstrate that our method outperforms state-of-the-art
that employ first-and-latest strategy with fixed-sized memory banks and
achieves comparable performance to the every-k strategy with increasing-sized
memory banks. Furthermore, experiments show that our method increases inference
speed by up to 80% over the every-k and 35% over first-and-latest strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Approach for Optimum-Path Forest Classification Using Fuzzy Logic. (arXiv:2204.06635v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06635">
<div class="article-summary-box-inner">
<span><p>In the past decades, fuzzy logic has played an essential role in many
research areas. Alongside, graph-based pattern recognition has shown to be of
great importance due to its flexibility in partitioning the feature space using
the background from graph theory. Some years ago, a new framework for both
supervised, semi-supervised, and unsupervised learning named Optimum-Path
Forest (OPF) was proposed with competitive results in several applications,
besides comprising a low computational burden. In this paper, we propose the
Fuzzy Optimum-Path Forest, an improved version of the standard OPF classifier
that learns the samples' membership in an unsupervised fashion, which are
further incorporated during supervised training. Such information is used to
identify the most relevant training samples, thus improving the classification
step. Experiments conducted over twelve public datasets highlight the
robustness of the proposed approach, which behaves similarly to standard OPF in
worst-case scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wassmap: Wasserstein Isometric Mapping for Image Manifold Learning. (arXiv:2204.06645v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06645">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Wasserstein Isometric Mapping (Wassmap), a
parameter-free nonlinear dimensionality reduction technique that provides
solutions to some drawbacks in existing global nonlinear dimensionality
reduction algorithms in imaging applications. Wassmap represents images via
probability measures in Wasserstein space, then uses pairwise quadratic
Wasserstein distances between the associated measures to produce a
low-dimensional, approximately isometric embedding. We show that the algorithm
is able to exactly recover parameters of some image manifolds including those
generated by translations or dilations of a fixed generating measure.
Additionally, we show that a discrete version of the algorithm retrieves
parameters from manifolds generated from discrete measures by providing a
theoretical bridge to transfer recovery results from functional data to
discrete data. Testing of the proposed algorithms on various image data
manifolds show that Wassmap yields good embeddings compared with other global
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A deep learning algorithm for reducing false positives in screening mammography. (arXiv:2204.06671v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06671">
<div class="article-summary-box-inner">
<span><p>Screening mammography improves breast cancer outcomes by enabling early
detection and treatment. However, false positive callbacks for additional
imaging from screening exams cause unnecessary procedures, patient anxiety, and
financial burden. This work demonstrates an AI algorithm that reduces false
positives by identifying mammograms not suspicious for breast cancer. We
trained the algorithm to determine the absence of cancer using 123,248 2D
digital mammograms (6,161 cancers) and performed a retrospective study on
14,831 screening exams (1,026 cancers) from 15 US and 3 UK sites. Retrospective
evaluation of the algorithm on the largest of the US sites (11,592 mammograms,
101 cancers) a) left the cancer detection rate unaffected (p=0.02,
non-inferiority margin 0.25 cancers per 1000 exams), b) reduced callbacks for
diagnostic exams by 31.1% compared to standard clinical readings, c) reduced
benign needle biopsies by 7.4%, and d) reduced screening exams requiring
radiologist interpretation by 41.6% in the simulated clinical workflow. This
work lays the foundation for semi-autonomous breast cancer screening systems
that could benefit patients and healthcare systems by reducing false positives,
unnecessary procedures, patient anxiety, and expenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Understanding of Sketches. (arXiv:2204.06675v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06675">
<div class="article-summary-box-inner">
<span><p>Sketching is used as a ubiquitous tool of expression by novices and experts
alike. In this thesis I explore two methods that help a system provide a
geometric machine-understanding of sketches, and in-turn help a user accomplish
a downstream task.
</p>
<p>The first work deals with interpretation of a 2D-line drawing as a graph
structure, and also illustrates its effectiveness through its physical
reconstruction by a robot. We setup a two-step pipeline to solve the problem.
Formerly, we estimate the vertices of the graph with sub-pixel level accuracy.
We achieve this using a combination of deep convolutional neural networks
learned under a supervised setting for pixel-level estimation followed by the
connected component analysis for clustering. Later we follow it up with a
feedback-loop-based edge estimation method. To complement the
graph-interpretation, we further perform data-interchange to a robot legible
ASCII format, and thus teach a robot to replicate a line drawing.
</p>
<p>In the second work, we test the 3D-geometric understanding of a sketch-based
system without explicit access to the information about 3D-geometry. The
objective is to complete a contour-like sketch of a 3D-object, with
illumination and texture information. We propose a data-driven approach to
learn a conditional distribution modelled as deep convolutional neural networks
to be trained under an adversarial setting; and we validate it against a
human-in-the-loop. The method itself is further supported by synthetic data
generation using constructive solid geometry following a standard graphics
pipeline. In order to validate the efficacy of our method, we design a
user-interface plugged into a popular sketch-based workflow, and setup a simple
task-based exercise, for an artist. Thereafter, we also discover that
form-exploration is an additional utility of our application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MINSU (Mobile Inventory And Scanning Unit):Computer Vision and AI. (arXiv:2204.06681v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06681">
<div class="article-summary-box-inner">
<span><p>The MINSU(Mobile Inventory and Scanning Unit) algorithm uses the
computational vision analysis method to record the residual quantity/fullness
of the cabinet. To do so, it goes through a five-step method: object detection,
foreground subtraction, K-means clustering, percentage estimation, and
counting. The input image goes through the object detection method to analyze
the specific position of the cabinets in terms of coordinates. After doing so,
it goes through the foreground subtraction method to make the image more
focus-able to the cabinet itself by removing the background (some manual work
may have to be done such as selecting the parts that were not grab cut by the
algorithm). In the K-means clustering method, the multi-colored image turns
into a 3 colored monotonous image for quicker and more accurate analysis. At
last, the image goes through percentage estimation and counting. In these two
methods, the proportion that the material inside the cabinet is found in
percentage which then is used to approximate the number of materials inside.
Had this project been successful, the residual quantity management could solve
the problem addressed earlier in the introduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HASA: Hybrid Architecture Search with Aggregation Strategy for Echinococcosis Classification and Ovary Segmentation in Ultrasound Images. (arXiv:2204.06697v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06697">
<div class="article-summary-box-inner">
<span><p>Different from handcrafted features, deep neural networks can automatically
learn task-specific features from data. Due to this data-driven nature, they
have achieved remarkable success in various areas. However, manual design and
selection of suitable network architectures are time-consuming and require
substantial effort of human experts. To address this problem, researchers have
proposed neural architecture search (NAS) algorithms which can automatically
generate network architectures but suffer from heavy computational cost and
instability if searching from scratch. In this paper, we propose a hybrid NAS
framework for ultrasound (US) image classification and segmentation. The hybrid
framework consists of a pre-trained backbone and several searched cells (i.e.,
network building blocks), which takes advantage of the strengths of both NAS
and the expert knowledge from existing convolutional neural networks.
Specifically, two effective and lightweight operations, a mixed depth-wise
convolution operator and a squeeze-and-excitation block, are introduced into
the candidate operations to enhance the variety and capacity of the searched
cells. These two operations not only decrease model parameters but also boost
network performance. Moreover, we propose a re-aggregation strategy for the
searched cells, aiming to further improve the performance for different vision
tasks. We tested our method on two large US image datasets, including a 9-class
echinococcosis dataset containing 9566 images for classification and an ovary
dataset containing 3204 images for segmentation. Ablation experiments and
comparison with other handcrafted or automatically searched architectures
demonstrate that our method can generate more powerful and lightweight models
for the above US image classification and segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Convolutional Neural Networks in Frequency Domain. (arXiv:2204.06718v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06718">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) achieves impressive success in the field
of computer vision during the past few decades. As the core of CNNs, image
convolution operation helps CNNs to achieve good performance on image-related
tasks. However, image convolution is hard to be implemented and parallelized.
In this paper, we propose a novel neural network model, namely CEMNet, that can
be trained in frequency domain. The most important motivation of this research
is that we can use the very simple element-wise multiplication operation to
replace the image convolution in frequency domain based on Cross-Correlation
Theorem. We further introduce Weight Fixation Mechanism to alleviate
over-fitting, and analyze the working behavior of Batch Normalization, Leaky
ReLU and Dropout in frequency domain to design their counterparts for CEMNet.
Also, to deal with complex inputs brought by DFT, we design two branch network
structure for CEMNet. Experimental results imply that CEMNet works well in
frequency domain, and achieve good performance on MNIST and CIFAR-10 databases.
To our knowledge, CEMNet is the first model trained in Fourier Domain that
achieves more than 70\% validation accuracy on CIFAR-10 database.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information fusion approach for biomass estimation in a plateau mountainous forest using a synergistic system comprising UAS-based digital camera and LiDAR. (arXiv:2204.06746v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06746">
<div class="article-summary-box-inner">
<span><p>Forest land plays a vital role in global climate, ecosystems, farming and
human living environments. Therefore, forest biomass estimation methods are
necessary to monitor changes in the forest structure and function, which are
key data in natural resources research. Although accurate forest biomass
measurements are important in forest inventory and assessments, high-density
measurements that involve airborne light detection and ranging (LiDAR) at a low
flight height in large mountainous areas are highly expensive. The objective of
this study was to quantify the aboveground biomass (AGB) of a plateau
mountainous forest reserve using a system that synergistically combines an
unmanned aircraft system (UAS)-based digital aerial camera and LiDAR to
leverage their complementary advantages. In this study, we utilized digital
aerial photogrammetry (DAP), which has the unique advantages of speed, high
spatial resolution, and low cost, to compensate for the deficiency of forestry
inventory using UAS-based LiDAR that requires terrain-following flight for
high-resolution data acquisition. Combined with the sparse LiDAR points
acquired by using a high-altitude and high-speed UAS for terrain extraction,
dense normalized DAP point clouds can be obtained to produce an accurate and
high-resolution canopy height model (CHM). Based on the CHM and spectral
attributes obtained from multispectral images, we estimated and mapped the AGB
of the region of interest with considerable cost efficiency. Our study supports
the development of predictive models for large-scale wall-to-wall AGB mapping
by leveraging the complementarity between DAP and LiDAR measurements. This work
also reveals the potential of utilizing a UAS-based digital camera and LiDAR
synergistically in a plateau mountainous forest area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation with Implicit Pseudo Supervision for Semantic Segmentation. (arXiv:2204.06747v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06747">
<div class="article-summary-box-inner">
<span><p>Pseudo-labelling is a popular technique in unsuper-vised domain adaptation
for semantic segmentation. However, pseudo labels are noisy and inevitably have
confirmation bias due to the discrepancy between source and target domains and
training process. In this paper, we train the model by the pseudo labels which
are implicitly produced by itself to learn new complementary knowledge about
target domain. Specifically, we propose a tri-learning architecture, where
every two branches produce the pseudo labels to train the third one. And we
align the pseudo labels based on the similarity of the probability
distributions for each two branches. To further implicitly utilize the pseudo
labels, we maximize the distances of features for different classes and
minimize the distances for the same classes by triplet loss. Extensive
experiments on GTA5 to Cityscapes and SYNTHIA to Cityscapes tasks show that the
proposed method has considerable improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RecurSeed and CertainMix for Weakly Supervised Semantic Segmentation. (arXiv:2204.06754v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06754">
<div class="article-summary-box-inner">
<span><p>Although weakly supervised semantic segmentation using only image-level
labels (WSSS-IL) is potentially useful, its low performance and implementation
complexity still limit its application. The main causes are (a) non-detection
and (b) false-detection phenomena: (a) The class activation maps refined from
existing WSSS-IL methods still only represent partial regions for large-scale
objects, and (b) for small-scale objects, over-activation causes them to
deviate from the object edges. We propose RecurSeed which alternately reduces
non- and false-detections through recursive iterations, thereby implicitly
finding an optimal junction that minimizes both errors. To maximize the
effectiveness of RecurSeed, we also propose a novel data augmentation (DA)
approach called CertainMix, which virtually creates object masks and further
expresses their edges in combining the segmentation results, thereby obtaining
a new DA method effectively reflecting object existence reliability through the
spatial information. We achieved new state-of-the-art performances on both the
PASCAL VOC 2012 and MS COCO 2014 benchmarks (VOC val 72.4%, COCO val 45.0%).
The code is available at https://github.com/OFRIN/RecurSeed_and_CertainMix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-performance Evolutionary Algorithms for Online Neuron Control. (arXiv:2204.06765v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06765">
<div class="article-summary-box-inner">
<span><p>Recently, optimization has become an emerging tool for neuroscientists to
study neural code. In the visual system, neurons respond to images with graded
and noisy responses. Image patterns eliciting highest responses are diagnostic
of the coding content of the neuron. To find these patterns, we have used
black-box optimizers to search a 4096d image space, leading to the evolution of
images that maximize neuronal responses. Although genetic algorithm (GA) has
been commonly used, there haven't been any systematic investigations to reveal
the best performing optimizer or the underlying principles necessary to improve
them.
</p>
<p>Here, we conducted a large scale in silico benchmark of optimizers for
activation maximization and found that Covariance Matrix Adaptation (CMA)
excelled in its achieved activation. We compared CMA against GA and found that
CMA surpassed the maximal activation of GA by 66% in silico and 44% in vivo. We
analyzed the structure of Evolution trajectories and found that the key to
success was not covariance matrix adaptation, but local search towards
informative dimensions and an effective step size decay. Guided by these
principles and the geometry of the image manifold, we developed SphereCMA
optimizer which competed well against CMA, proving the validity of the
identified principles. Code available at
https://github.com/Animadversio/ActMax-Optimizer-Dev
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViTOL: Vision Transformer for Weakly Supervised Object Localization. (arXiv:2204.06772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06772">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object localization (WSOL) aims at predicting object
locations in an image using only image-level category labels. Common challenges
that image classification models encounter when localizing objects are, (a)
they tend to look at the most discriminative features in an image that confines
the localization map to a very small region, (b) the localization maps are
class agnostic, and the models highlight objects of multiple classes in the
same image and, (c) the localization performance is affected by background
noise. To alleviate the above challenges we introduce the following simple
changes through our proposed method ViTOL. We leverage the vision-based
transformer for self-attention and introduce a patch-based attention dropout
layer (p-ADL) to increase the coverage of the localization map and a gradient
attention rollout mechanism to generate class-dependent attention maps. We
conduct extensive quantitative, qualitative and ablation experiments on the
ImageNet-1K and CUB datasets. We achieve state-of-the-art MaxBoxAcc-V2
localization scores of 70.47% and 73.17% on the two datasets respectively. Code
is available on https://github.com/Saurav-31/ViTOL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual-Inertial Odometry with Online Calibration of Velocity-Control Based Kinematic Motion Models. (arXiv:2204.06776v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06776">
<div class="article-summary-box-inner">
<span><p>Visual-inertial odometry (VIO) is an important technology for autonomous
robots with power and payload constraints. In this paper, we propose a novel
approach for VIO with stereo cameras which integrates and calibrates the
velocity-control based kinematic motion model of wheeled mobile robots online.
Including such a motion model can help to improve the accuracy of VIO. Compared
to several previous approaches proposed to integrate wheel odometer
measurements for this purpose, our method does not require wheel encoders and
can be applied when the robot motion can be modeled with velocity-control based
kinematic motion model. We use radial basis function (RBF) kernels to
compensate for the time delay and deviations between control commands and
actual robot motion. The motion model is calibrated online by the VIO system
and can be used as a forward model for motion control and planning. We evaluate
our approach with data obtained in variously sized indoor environments,
demonstrate improvements over a pure VIO method, and evaluate the prediction
accuracy of the online calibrated model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Shuffle-Mixer: An Efficient Context-Aware Vision Learner of Transformer-MLP Paradigm for Dense Prediction in Medical Volume. (arXiv:2204.06779v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06779">
<div class="article-summary-box-inner">
<span><p>Dense prediction in medical volume provides enriched guidance for clinical
analysis. CNN backbones have met bottleneck due to lack of long-range
dependencies and global context modeling power. Recent works proposed to
combine vision transformer with CNN, due to its strong global capture ability
and learning capability. However, most works are limited to simply applying
pure transformer with several fatal flaws (i.e., lack of inductive bias, heavy
computation and little consideration for 3D data). Therefore, designing an
elegant and efficient vision transformer learner for dense prediction in
medical volume is promising and challenging. In this paper, we propose a novel
3D Shuffle-Mixer network of a new Local Vision Transformer-MLP paradigm for
medical dense prediction. In our network, a local vision transformer block is
utilized to shuffle and learn spatial context from full-view slices of
rearranged volume, a residual axial-MLP is designed to mix and capture
remaining volume context in a slice-aware manner, and a MLP view aggregator is
employed to project the learned full-view rich context to the volume feature in
a view-aware manner. Moreover, an Adaptive Scaled Enhanced Shortcut is proposed
for local vision transformer to enhance feature along spatial and channel
dimensions adaptively, and a CrossMerge is proposed to skip-connects the
multi-scale feature appropriately in the pyramid architecture. Extensive
experiments demonstrate the proposed model outperforms other state-of-the-art
medical dense prediction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Analysis of Deep Learning Methods for SAR Image Classification. (arXiv:2204.06783v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06783">
<div class="article-summary-box-inner">
<span><p>Deep learning methods exhibit outstanding performance in synthetic aperture
radar (SAR) image interpretation tasks. However, these are black box models
that limit the comprehension of their predictions. Therefore, to meet this
challenge, we have utilized explainable artificial intelligence (XAI) methods
for the SAR image classification task. Specifically, we trained
state-of-the-art convolutional neural networks for each polarization format on
OpenSARUrban dataset and then investigate eight explanation methods to analyze
the predictions of the CNN classifiers of SAR images. These XAI methods are
also evaluated qualitatively and quantitatively which shows that Occlusion
achieves the most reliable interpretation performance in terms of
Max-Sensitivity but with a low-resolution explanation heatmap. The explanation
results provide some insights into the internal mechanism of black-box
decisions for SAR image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pyramidal Attention for Saliency Detection. (arXiv:2204.06788v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06788">
<div class="article-summary-box-inner">
<span><p>Salient object detection (SOD) extracts meaningful contents from an input
image. RGB-based SOD methods lack the complementary depth clues; hence,
providing limited performance for complex scenarios. Similarly, RGB-D models
process RGB and depth inputs, but the depth data availability during testing
may hinder the model's practical applicability. This paper exploits only RGB
images, estimates depth from RGB, and leverages the intermediate depth
features. We employ a pyramidal attention structure to extract multi-level
convolutional-transformer features to process initial stage representations and
further enhance the subsequent ones. At each stage, the backbone transformer
model produces global receptive fields and computing in parallel to attain
fine-grained global predictions refined by our residual convolutional attention
decoder for optimal saliency prediction. We report significantly improved
performance against 21 and 40 state-of-the-art SOD methods on eight RGB and
RGB-D datasets, respectively. Consequently, we present a new SOD perspective of
generating RGB-D SOD without acquiring depth data during training and testing
and assist RGB methods with depth clues for improved performance. The code and
trained models are available at
https://github.com/tanveer-hussain/EfficientSOD2
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss. (arXiv:2204.06806v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06806">
<div class="article-summary-box-inner">
<span><p>We introduce YOLO-pose, a novel heatmap-free approach for joint detection,
and 2D multi-person pose estimation in an image based on the popular YOLO
object detection framework. Existing heatmap based two-stage approaches are
sub-optimal as they are not end-to-end trainable and training relies on a
surrogate L1 loss that is not equivalent to maximizing the evaluation metric,
i.e. Object Keypoint Similarity (OKS). Our framework allows us to train the
model end-to-end and optimize the OKS metric itself. The proposed model learns
to jointly detect bounding boxes for multiple persons and their corresponding
2D poses in a single forward pass and thus bringing in the best of both
top-down and bottom-up approaches. Proposed approach doesn't require the
postprocessing of bottom-up approaches to group detected keypoints into a
skeleton as each bounding box has an associated pose, resulting in an inherent
grouping of the keypoints. Unlike top-down approaches, multiple forward passes
are done away with since all persons are localized along with their pose in a
single inference. YOLO-pose achieves new state-of-the-art results on COCO
validation (90.2% AP50) and test-dev set (90.3% AP50), surpassing all existing
bottom-up approaches in a single forward pass without flip test, multi-scale
testing, or any other test time augmentation. All experiments and results
reported in this paper are without any test time augmentation, unlike
traditional approaches that use flip-test and multi-scale testing to boost
performance. Our training codes will be made publicly available at
https://github.com/TexasInstruments/edgeai-yolov5 and
https://github.com/TexasInstruments/edgeai-yolox
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Vertebral Fracture Quantification via Anchor-Free Landmarks Localization. (arXiv:2204.06818v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06818">
<div class="article-summary-box-inner">
<span><p>Vertebral body compression fractures are early signs of osteoporosis. Though
these fractures are visible on Computed Tomography (CT) images, they are
frequently missed by radiologists in clinical settings. Prior research on
automatic methods of vertebral fracture classification proves its reliable
quality; however, existing methods provide hard-to-interpret outputs and
sometimes fail to process cases with severe abnormalities such as highly
pathological vertebrae or scoliosis. We propose a new two-step algorithm to
localize the vertebral column in 3D CT images and then detect individual
vertebrae and quantify fractures in 2D simultaneously. We train neural networks
for both steps using a simple 6-keypoints based annotation scheme, which
corresponds precisely to the current clinical recommendation. Our algorithm has
no exclusion criteria, processes 3D CT in 2 seconds on a single GPU, and
provides an interpretable and verifiable output. The method approaches
expert-level performance and demonstrates state-of-the-art results in vertebrae
3D localization (the average error is 1 mm), vertebrae 2D detection (precision
and recall are 0.99), and fracture identification (ROC AUC at the patient level
is up to 0.96). Our anchor-free vertebra detection network shows excellent
generalizability on a new domain by achieving ROC AUC 0.95, sensitivity 0.85,
specificity 0.9 on a challenging VerSe dataset with many unseen vertebra types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Vehicle Detection in Satellite Video. (arXiv:2204.06828v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06828">
<div class="article-summary-box-inner">
<span><p>This work presents a deep learning approach for vehicle detection in
satellite video. Vehicle detection is perhaps impossible in single EO satellite
images due to the tininess of vehicles (4-10 pixel) and their similarity to the
background. Instead, we consider satellite video which overcomes the lack of
spatial information by temporal consistency of vehicle movement. A new
spatiotemporal model of a compact $3 \times 3$ convolutional, neural network is
proposed which neglects pooling layers and uses leaky ReLUs. Then we use a
reformulation of the output heatmap including Non-Maximum-Suppression (NMS) for
the final segmentation. Empirical results on two new annotated satellite videos
reconfirm the applicability of this approach for vehicle detection. They more
importantly indicate that pre-training on WAMI data and then fine-tuning on few
annotated video frames for a new video is sufficient. In our experiment only
five annotated images yield a $F_1$ score of 0.81 on a new video showing more
complex traffic patterns than the Las Vegas video. Our best result on Las Vegas
is a $F_1$ score of 0.87 which makes the proposed approach a leading method for
this benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Indirect Illumination for Inverse Rendering. (arXiv:2204.06837v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06837">
<div class="article-summary-box-inner">
<span><p>Recent advances in implicit neural representations and differentiable
rendering make it possible to simultaneously recover the geometry and materials
of an object from multi-view RGB images captured under unknown static
illumination. Despite the promising results achieved, indirect illumination is
rarely modeled in previous methods, as it requires expensive recursive path
tracing which makes the inverse rendering computationally intractable. In this
paper, we propose a novel approach to efficiently recovering spatially-varying
indirect illumination. The key insight is that indirect illumination can be
conveniently derived from the neural radiance field learned from input images
instead of being estimated jointly with direct illumination and materials. By
properly modeling the indirect illumination and visibility of direct
illumination, interreflection- and shadow-free albedo can be recovered. The
experiments on both synthetic and real data demonstrate the superior
performance of our approach compared to previous work and its capability to
synthesize realistic renderings under novel viewpoints and illumination. Our
code and data are available at https://zju3dv.github.io/invrender/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniPD: One-Step Person Detection in Top-View Omnidirectional Indoor Scenes. (arXiv:2204.06846v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06846">
<div class="article-summary-box-inner">
<span><p>We propose a one-step person detector for topview omnidirectional indoor
scenes based on convolutional neural networks (CNNs). While state of the art
person detectors reach competitive results on perspective images, missing CNN
architectures as well as training data that follows the distortion of
omnidirectional images makes current approaches not applicable to our data. The
method predicts bounding boxes of multiple persons directly in omnidirectional
images without perspective transformation, which reduces overhead of pre- and
post-processing and enables real-time performance. The basic idea is to utilize
transfer learning to fine-tune CNNs trained on perspective images with data
augmentation techniques for detection in omnidirectional images. We fine-tune
two variants of Single Shot MultiBox detectors (SSDs). The first one uses
Mobilenet v1 FPN as feature extractor (moSSD). The second one uses ResNet50 v1
FPN (resSSD). Both models are pre-trained on Microsoft Common Objects in
Context (COCO) dataset. We fine-tune both models on PASCAL VOC07 and VOC12
datasets, specifically on class person. Random 90-degree rotation and random
vertical flipping are used for data augmentation in addition to the methods
proposed by original SSD. We reach an average precision (AP) of 67.3 % with
moSSD and 74.9 % with resSSD onthe evaluation dataset. To enhance the
fine-tuning process, we add a subset of HDA Person dataset and a subset of
PIROPOdatabase and reduce the number of perspective images to PASCAL VOC07. The
AP rises to 83.2 % for moSSD and 86.3 % for resSSD, respectively. The average
inference speed is 28 ms per image for moSSD and 38 ms per image for resSSD
using Nvidia Quadro P6000. Our method is applicable to other CNN-based object
detectors and can potentially generalize for detecting other objects in
omnidirectional images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensuring accurate stain reproduction in deep generative networks for virtual immunohistochemistry. (arXiv:2204.06849v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06849">
<div class="article-summary-box-inner">
<span><p>Immunohistochemistry is a valuable diagnostic tool for cancer pathology.
However, it requires specialist labs and equipment, is time-intensive, and is
difficult to reproduce. Consequently, a long term aim is to provide a digital
method of recreating physical immunohistochemical stains. Generative
Adversarial Networks have become exceedingly advanced at mapping one image type
to another and have shown promise at inferring immunostains from haematoxylin
and eosin. However, they have a substantial weakness when used with pathology
images as they can fabricate structures that are not present in the original
data. CycleGANs can mitigate invented tissue structures in pathology image
mapping but have a related disposition to generate areas of inaccurate
staining. In this paper, we describe a modification to the loss function of a
CycleGAN to improve its mapping ability for pathology images by enforcing
realistic stain replication while retaining tissue structure. Our approach
improves upon others by considering structure and staining during model
training. We evaluated our network using the Fr\'echet Inception distance,
coupled with a new technique that we propose to appraise the accuracy of
virtual immunohistochemistry. This assesses the overlap between each stain
component in the inferred and ground truth images through colour deconvolution,
thresholding and the Sorensen-Dice coefficient. Our modified loss function
resulted in a Dice coefficient for the virtual stain of 0.78 compared with the
real AE1/AE3 slide. This was superior to the unaltered CycleGAN's score of
0.74. Additionally, our loss function improved the Fr\'echet Inception distance
for the reconstruction to 74.54 from 76.47. We, therefore, describe an advance
in virtual restaining that can extend to other immunostains and tumour types
and deliver reproducible, fast and readily accessible immunohistochemistry
worldwide.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Training to Improve Player and Ball Detection in Soccer. (arXiv:2204.06859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06859">
<div class="article-summary-box-inner">
<span><p>Accurate player and ball detection has become increasingly important in
recent years for sport analytics. As most state-of-the-art methods rely on
training deep learning networks in a supervised fashion, they require huge
amounts of annotated data, which are rarely available. In this paper, we
present a novel generic semi-supervised method to train a network based on a
labeled image dataset by leveraging a large unlabeled dataset of soccer
broadcast videos. More precisely, we design a teacher-student approach in which
the teacher produces surrogate annotations on the unlabeled data to be used
later for training a student which has the same architecture as the teacher.
Furthermore, we introduce three training loss parametrizations that allow the
student to doubt the predictions of the teacher during training depending on
the proposal confidence score. We show that including unlabeled data in the
training process allows to substantially improve the performances of the
detection network trained only on the labeled data. Finally, we provide a
thorough performance study including different proportions of labeled and
unlabeled data, and establish the first benchmark on the new SoccerNet-v3
detection task, with an mAP of 52.3%. Our code is available at
https://github.com/rvandeghen/SST .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Identity-Preserved Motion Retargeting in Video Synthesis by Feature Disentanglement. (arXiv:2204.06862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06862">
<div class="article-summary-box-inner">
<span><p>Most motion retargeting methods in human action video synthesis decompose the
input video to motion (dynamic information) and shape (static information).
However, we observe if the dynamic information is directly transferred to
another subject, it will result in unnatural synthesised motion. This
phenomenon is mainly caused by neglecting subject-dependent information in
motion. To solve the problem, we propose a novel motion retargeting method
which can combine both subject-independent (common motion content) information
from a source video and subject-dependent (individualized identity motion)
information from a target video. So it can synthesize videos with a much
natural appearance along with identity-preserved motion. In the proposed method
two encoders are employed to extract identity and motion content
representations respectively. We employ the adaptive instance normalization
(AdaIN) layer in the generator and the instance normalization (IN) layer in the
motion content encoder to synthesize the new motion. Besides, we also collected
a dataset, named $Chuang101$, with 101 subjects in total. Each subject performs
identical dancing movement, and so it is convenient for feature disentanglement
among motion and identity of each subject. Furthermore, an efficient
quantitative metric for identify information is designed by gait recognition.
The experiments show the proposed method can synthesize videos more naturally
when the subject's identity is preserved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clothes-Changing Person Re-identification with RGB Modality Only. (arXiv:2204.06890v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06890">
<div class="article-summary-box-inner">
<span><p>The key to address clothes-changing person re-identification (re-id) is to
extract clothes-irrelevant features, e.g., face, hairstyle, body shape, and
gait. Most current works mainly focus on modeling body shape from
multi-modality information (e.g., silhouettes and sketches), but do not make
full use of the clothes-irrelevant information in the original RGB images. In
this paper, we propose a Clothes-based Adversarial Loss (CAL) to mine
clothes-irrelevant features from the original RGB images by penalizing the
predictive power of re-id model w.r.t. clothes. Extensive experiments
demonstrate that using RGB images only, CAL outperforms all state-of-the-art
methods on widely-used clothes-changing person re-id benchmarks. Besides,
compared with images, videos contain richer appearance and additional temporal
information, which can be used to model proper spatiotemporal patterns to
assist clothes-changing re-id. Since there is no publicly available
clothes-changing video re-id dataset, we contribute a new dataset named CCVID
and show that there exists much room for improvement in modeling spatiotemporal
information. The code and new dataset are available at:
https://github.com/guxinqian/Simple-CCReID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Sample Extension for Unsupervised Person Re-Identification. (arXiv:2204.06892v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06892">
<div class="article-summary-box-inner">
<span><p>Most existing unsupervised person re-identification (Re-ID) methods use
clustering to generate pseudo labels for model training. Unfortunately,
clustering sometimes mixes different true identities together or splits the
same identity into two or more sub clusters. Training on these noisy clusters
substantially hampers the Re-ID accuracy. Due to the limited samples in each
identity, we suppose there may lack some underlying information to well reveal
the accurate clusters. To discover these information, we propose an Implicit
Sample Extension (\OurWholeMethod) method to generate what we call support
samples around the cluster boundaries. Specifically, we generate support
samples from actual samples and their neighbouring clusters in the embedding
space through a progressive linear interpolation (PLI) strategy. PLI controls
the generation with two critical factors, i.e., 1) the direction from the
actual sample towards its K-nearest clusters and 2) the degree for mixing up
the context information from the K-nearest clusters. Meanwhile, given the
support samples, ISE further uses a label-preserving loss to pull them towards
their corresponding actual samples, so as to compact each cluster.
Consequently, ISE reduces the "sub and mixed" clustering errors, thus improving
the Re-ID performance. Extensive experiments demonstrate that the proposed
method is effective and achieves state-of-the-art performance for unsupervised
person Re-ID. Code is available at:
\url{https://github.com/PaddlePaddle/PaddleClas}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Likelihood Voting with Self-Knowledge Distillation for Weakly Supervised Object Detection. (arXiv:2204.06899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06899">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object detection (WSOD), which is an effective way to train
an object detection model using only image-level annotations, has attracted
considerable attention from researchers. However, most of the existing methods,
which are based on multiple instance learning (MIL), tend to localize instances
to the discriminative parts of salient objects instead of the entire content of
all objects. In this paper, we propose a WSOD framework called the Spatial
Likelihood Voting with Self-knowledge Distillation Network (SLV-SD Net). In
this framework, we introduce a spatial likelihood voting (SLV) module to
converge region proposal localization without bounding box annotations.
Specifically, in every iteration during training, all the region proposals in a
given image act as voters voting for the likelihood of each category in the
spatial dimensions. After dilating the alignment on the area with large
likelihood values, the voting results are regularized as bounding boxes, which
are then used for the final classification and localization. Based on SLV, we
further propose a self-knowledge distillation (SD) module to refine the feature
representations of the given image. The likelihood maps generated by the SLV
module are used to supervise the feature learning of the backbone network,
encouraging the network to attend to wider and more diverse areas of the image.
Extensive experiments on the PASCAL VOC 2007/2012 and MS-COCO datasets
demonstrate the excellent performance of SLV-SD Net. In addition, SLV-SD Net
produces new state-of-the-art results on these benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SoccerNet-Tracking: Multiple Object Tracking Dataset and Benchmark in Soccer Videos. (arXiv:2204.06918v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06918">
<div class="article-summary-box-inner">
<span><p>Tracking objects in soccer videos is extremely important to gather both
player and team statistics, whether it is to estimate the total distance run,
the ball possession or the team formation. Video processing can help automating
the extraction of those information, without the need of any invasive sensor,
hence applicable to any team on any stadium. Yet, the availability of datasets
to train learnable models and benchmarks to evaluate methods on a common
testbed is very limited. In this work, we propose a novel dataset for multiple
object tracking composed of 200 sequences of 30s each, representative of
challenging soccer scenarios, and a complete 45-minutes half-time for long-term
tracking. The dataset is fully annotated with bounding boxes and tracklet IDs,
enabling the training of MOT baselines in the soccer domain and a full
benchmarking of those methods on our segregated challenge sets. Our analysis
shows that multiple player, referee and ball tracking in soccer videos is far
from being solved, with several improvement required in case of fast motion or
in scenarios of severe occlusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis. (arXiv:2204.06929v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06929">
<div class="article-summary-box-inner">
<span><p>Ultrasound (US) imaging is widely used for anatomical structure inspection in
clinical diagnosis. The training of new sonographers and deep learning based
algorithms for US image analysis usually requires a large amount of data.
However, obtaining and labeling large-scale US imaging data are not easy tasks,
especially for diseases with low incidence. Realistic US image synthesis can
alleviate this problem to a great extent. In this paper, we propose a
generative adversarial network (GAN) based image synthesis framework. Our main
contributions include: 1) we present the first work that can synthesize
realistic B-mode US images with high-resolution and customized texture editing
features; 2) to enhance structural details of generated images, we propose to
introduce auxiliary sketch guidance into a conditional GAN. We superpose the
edge sketch onto the object mask and use the composite mask as the network
input; 3) to generate high-resolution US images, we adopt a progressive
training strategy to gradually generate high-resolution images from
low-resolution images. In addition, a feature loss is proposed to minimize the
difference of high-level features between the generated and real images, which
further improves the quality of generated images; 4) the proposed US image
synthesis method is quite universal and can also be generalized to the US
images of other anatomical structures besides the three ones tested in our
study (lung, hip joint, and ovary); 5) extensive experiments on three large US
image datasets are conducted to validate our method. Ablation studies,
customized texture editing, user studies, and segmentation tests demonstrate
promising results of our method in synthesizing realistic US images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Deep Learning to Identify the Critical 3D Structural Features of the Optic Nerve Head for Glaucoma Diagnosis. (arXiv:2204.06931v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06931">
<div class="article-summary-box-inner">
<span><p>Purpose: The optic nerve head (ONH) undergoes complex and deep 3D
morphological changes during the development and progression of glaucoma.
Optical coherence tomography (OCT) is the current gold standard to visualize
and quantify these changes, however the resulting 3D deep-tissue information
has not yet been fully exploited for the diagnosis and prognosis of glaucoma.
To this end, we aimed: (1) To compare the performance of two relatively recent
geometric deep learning techniques in diagnosing glaucoma from a single OCT
scan of the ONH; and (2) To identify the 3D structural features of the ONH that
are critical for the diagnosis of glaucoma.
</p>
<p>Methods: In this study, we included a total of 2,247 non-glaucoma and 2,259
glaucoma scans from 1,725 subjects. All subjects had their ONHs imaged in 3D
with Spectralis OCT. All OCT scans were automatically segmented using deep
learning to identify major neural and connective tissues. Each ONH was then
represented as a 3D point cloud. We used PointNet and dynamic graph
convolutional neural network (DGCNN) to diagnose glaucoma from such 3D ONH
point clouds and to identify the critical 3D structural features of the ONH for
glaucoma diagnosis.
</p>
<p>Results: Both the DGCNN (AUC: 0.97$\pm$0.01) and PointNet (AUC:
0.95$\pm$0.02) were able to accurately detect glaucoma from 3D ONH point
clouds. The critical points formed an hourglass pattern with most of them
located in the inferior and superior quadrant of the ONH.
</p>
<p>Discussion: The diagnostic accuracy of both geometric deep learning
approaches was excellent. Moreover, we were able to identify the critical 3D
structural features of the ONH for glaucoma diagnosis that tremendously
improved the transparency and interpretability of our method. Consequently, our
approach may have strong potential to be used in clinical applications for the
diagnosis and prognosis of a wide range of ophthalmic disorders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEHAVE: Dataset and Method for Tracking Human Object Interactions. (arXiv:2204.06950v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06950">
<div class="article-summary-box-inner">
<span><p>Modelling interactions between humans and objects in natural environments is
central to many applications including gaming, virtual and mixed reality, as
well as human behavior analysis and human-robot collaboration. This challenging
operation scenario requires generalization to vast number of objects, scenes,
and human actions. Unfortunately, there exist no such dataset. Moreover, this
data needs to be acquired in diverse natural environments, which rules out 4D
scanners and marker based capture systems. We present BEHAVE dataset, the first
full body human- object interaction dataset with multi-view RGBD frames and
corresponding 3D SMPL and object fits along with the annotated contacts between
them. We record around 15k frames at 5 locations with 8 subjects performing a
wide range of interactions with 20 common objects. We use this data to learn a
model that can jointly track humans and objects in natural environments with an
easy-to-use portable multi-camera setup. Our key insight is to predict
correspondences from the human and the object to a statistical body model to
obtain human-object contacts during interactions. Our approach can record and
track not just the humans and objects but also their interactions, modeled as
surface contacts, in 3D. Our code and data can be found at:
<a href="http://virtualhumans.mpi-inf.mpg.de/behave">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Deep Learning Meets Chan-Vese Model. (arXiv:2204.06951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06951">
<div class="article-summary-box-inner">
<span><p>The Chan-Vese (CV) model is a classic region-based method in image
segmentation. However, its piecewise constant assumption does not always hold
for practical applications. Many improvements have been proposed but the issue
is still far from well solved. In this work, we propose an unsupervised image
segmentation approach that integrates the CV model with deep neural networks,
which significantly improves the original CV model's segmentation accuracy. Our
basic idea is to apply a deep neural network that maps the image into a latent
space to alleviate the violation of the piecewise constant assumption in image
space. We formulate this idea under the classic Bayesian framework by
approximating the likelihood with an evidence lower bound (ELBO) term while
keeping the prior term in the CV model. Thus, our model only needs the input
image itself and does not require pre-training from external datasets.
Moreover, we extend the idea to multi-phase case and dataset based unsupervised
image segmentation. Extensive experiments validate the effectiveness of our
model and show that the proposed method is noticeably better than other
unsupervised segmentation approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEFM-Nets: Learnable Explicit Feature Map Deep Networks for Segmentation of Histopathological Images of Frozen Sections. (arXiv:2204.06955v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06955">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation of medical images is essential for diagnosis and
treatment of diseases. These problems are solved by highly complex models, such
as deep networks (DN), requiring a large amount of labeled data for training.
Thereby, many DNs possess task- or imaging modality specific architectures with
a decision-making process that is often hard to explain and interpret. Here, we
propose a framework that embeds existing DNs into a low-dimensional subspace
induced by the learnable explicit feature map (LEFM) layer. Compared to the
existing DN, the framework adds one hyperparameter and only modestly increase
the number of learnable parameters. The method is aimed at, but not limited to,
segmentation of low-dimensional medical images, such as color histopathological
images of stained frozen sections. Since features in the LEFM layer are
polynomial functions of the original features, proposed LEFM-Nets contribute to
the interpretability of network decisions. In this work, we combined LEFM with
the known networks: DeepLabv3+, UNet, UNet++ and MA-net. New LEFM-Nets are
applied to the segmentation of adenocarcinoma of a colon in a liver from images
of hematoxylin and eosin (H&amp;E) stained frozen sections. LEFM-Nets are also
tested on nuclei segmentation from images of H&amp;E stained frozen sections of ten
human organs. On the first problem, LEFM-Nets achieved statistically
significant performance improvement in terms of micro balanced accuracy and
$F_1$ score than original networks. LEFM-Nets achieved only better performance
in comparison with the original networks on the second problem. The source code
is available at https://github.com/dsitnik/lefm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The multi-modal universe of fast-fashion: the Visuelle 2.0 benchmark. (arXiv:2204.06972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06972">
<div class="article-summary-box-inner">
<span><p>We present Visuelle 2.0, the first dataset useful for facing diverse
prediction problems that a fast-fashion company has to manage routinely.
Furthermore, we demonstrate how the use of computer vision is substantial in
this scenario. Visuelle 2.0 contains data for 6 seasons / 5355 clothing
products of Nuna Lie, a famous Italian company with hundreds of shops located
in different areas within the country. In particular, we focus on a specific
prediction problem, namely short-observation new product sale forecasting
(SO-fore). SO-fore assumes that the season has started and a set of new
products is on the shelves of the different stores. The goal is to forecast the
sales for a particular horizon, given a short, available past (few weeks),
since no earlier statistics are available. To be successful, SO-fore approaches
should capture this short past and exploit other modalities or exogenous data.
To these aims, Visuelle 2.0 is equipped with disaggregated data at the
item-shop level and multi-modal information for each clothing item, allowing
computer vision approaches to come into play. The main message that we deliver
is that the use of image data with deep networks boosts performances obtained
when using the time series in long-term forecasting scenarios, ameliorating the
WAPE by 8.2% and the MAE by 7.7%. The dataset is available at:
https://humaticslab.github.io/forecasting/visuelle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyDe: The First Open-Source, Python-Based, GPU-Accelerated Hyperspectral Denoising Package. (arXiv:2204.06979v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06979">
<div class="article-summary-box-inner">
<span><p>As with any physical instrument, hyperspectral cameras induce different kinds
of noise in the acquired data. Therefore, Hyperspectral denoising is a crucial
step for analyzing hyperspectral images (HSIs). Conventional computational
methods rarely use GPUs to improve efficiency and are not fully open-source.
Alternatively, deep learning-based methods are often open-source and use GPUs,
but their training and utilization for real-world applications remain
non-trivial for many researchers. Consequently, we propose HyDe: the first
open-source, GPU-accelerated Python-based, hyperspectral image denoising
toolbox, which aims to provide a large set of methods with an easy-to-use
environment. HyDe includes a variety of methods ranging from low-rank
wavelet-based methods to deep neural network (DNN) models. HyDe's interface
dramatically improves the interoperability of these methods and the performance
of the underlying functions. In fact, these methods maintain similar HSI
denoising performance to their original implementations while consuming nearly
ten times less energy. Furthermore, we present a method for training DNNs for
denoising HSIs which are not spatially related to the training dataset, i.e.,
training on ground-level HSIs for denoising HSIs with other perspectives
including airborne, drone-borne, and space-borne. To utilize the trained DNNs,
we show a sliding window method to effectively denoise HSIs which would
otherwise require more than 40 GB. The package can be found at:
\url{https://github.com/Helmholtz-AI-Energy/HyDe}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Image Relational Knowledge Distillation for Semantic Segmentation. (arXiv:2204.06986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06986">
<div class="article-summary-box-inner">
<span><p>Current Knowledge Distillation (KD) methods for semantic segmentation often
guide the student to mimic the teacher's structured information generated from
individual data samples. However, they ignore the global semantic relations
among pixels across various images that are valuable for KD. This paper
proposes a novel Cross-Image Relational KD (CIRKD), which focuses on
transferring structured pixel-to-pixel and pixel-to-region relations among the
whole images. The motivation is that a good teacher network could construct a
well-structured feature space in terms of global pixel dependencies. CIRKD
makes the student mimic better structured semantic relations from the teacher,
thus improving the segmentation performance. Experimental results over
Cityscapes, CamVid and Pascal VOC datasets demonstrate the effectiveness of our
proposed approach against state-of-the-art distillation methods. The code is
available at https://github.com/winycg/CIRKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Atmospheric Turbulence Removal with Complex-Valued Convolutional Neural Network. (arXiv:2204.06989v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06989">
<div class="article-summary-box-inner">
<span><p>Atmospheric turbulence distorts visual imagery and is always problematic for
information interpretation by both human and machine. Most well-developed
approaches to remove atmospheric turbulence distortion are model-based.
However, these methods require high computation and large memory preventing
their feasibility of real-time operation. Deep learning-based approaches have
hence gained more attention but currently work efficiently only on static
scenes. This paper presents a novel learning-based framework offering short
temporal spanning to support dynamic scenes. We exploit complex-valued
convolutions as phase information, altered by atmospheric turbulence, is
captured better than using ordinary real-valued convolutions. Two concatenated
modules are proposed. The first module aims to remove geometric distortions
and, if enough memory, the second module is applied to refine micro details of
the videos. Experimental results show that our proposed framework efficiently
mitigate the atmospheric turbulence distortion and significantly outperforms
the existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Application of Geometric Deep Learning for the Diagnosis of Glaucoma. (arXiv:2204.07004v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07004">
<div class="article-summary-box-inner">
<span><p>Purpose: (1) To assess the performance of geometric deep learning (PointNet)
in diagnosing glaucoma from a single optical coherence tomography (OCT) 3D scan
of the optic nerve head (ONH); (2) To compare its performance to that obtained
with a standard 3D convolutional neural network (CNN), and with a gold-standard
glaucoma parameter, i.e. retinal nerve fiber layer (RNFL) thickness.
</p>
<p>Methods: 3D raster scans of the ONH were acquired with Spectralis OCT for 477
glaucoma and 2,296 non-glaucoma subjects at the Singapore National Eye Centre.
All volumes were automatically segmented using deep learning to identify 7
major neural and connective tissues including the RNFL, the prelamina, and the
lamina cribrosa (LC). Each ONH was then represented as a 3D point cloud with
1,000 points chosen randomly from all tissue boundaries. To simplify the
problem, all ONH point clouds were aligned with respect to the plane and center
of Bruch's membrane opening. Geometric deep learning (PointNet) was then used
to provide a glaucoma diagnosis from a single OCT point cloud. The performance
of our approach was compared to that obtained with a 3D CNN, and with RNFL
thickness.
</p>
<p>Results: PointNet was able to provide a robust glaucoma diagnosis solely from
the ONH represented as a 3D point cloud (AUC=95%). The performance of PointNet
was superior to that obtained with a standard 3D CNN (AUC=87%) and with that
obtained from RNFL thickness alone (AUC=80%).
</p>
<p>Discussion: We provide a proof-of-principle for the application of geometric
deep learning in the field of glaucoma. Our technique requires significantly
less information as input to perform better than a 3D CNN, and with an AUC
superior to that obtained from RNFL thickness alone. Geometric deep learning
may have wide applicability in the field of Ophthalmology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretability of Machine Learning Methods Applied to Neuroimaging. (arXiv:2204.07005v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07005">
<div class="article-summary-box-inner">
<span><p>Deep learning methods have become very popular for the processing of natural
images, and were then successfully adapted to the neuroimaging field. As these
methods are non-transparent, interpretability methods are needed to validate
them and ensure their reliability. Indeed, it has been shown that deep learning
models may obtain high performance even when using irrelevant features, by
exploiting biases in the training set. Such undesirable situations can
potentially be detected by using interpretability methods. Recently, many
methods have been proposed to interpret neural networks. However, this domain
is not mature yet. Machine learning users face two major issues when aiming to
interpret their models: which method to choose, and how to assess its
reliability? Here, we aim at providing answers to these questions by presenting
the most common interpretability methods and metrics developed to assess their
reliability, as well as their applications and benchmarks in the neuroimaging
context. Note that this is not an exhaustive survey: we aimed to focus on the
studies which we found to be the most representative and relevant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Environmental Sound Representation to Robustness of 2D CNN Models Against Adversarial Attacks. (arXiv:2204.07018v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07018">
<div class="article-summary-box-inner">
<span><p>This paper investigates the impact of different standard environmental sound
representations (spectrograms) on the recognition performance and adversarial
attack robustness of a victim residual convolutional neural network, namely
ResNet-18. Our main motivation for focusing on such a front-end classifier
rather than other complex architectures is balancing recognition accuracy and
the total number of training parameters. Herein, we measure the impact of
different settings required for generating more informative Mel-frequency
cepstral coefficient (MFCC), short-time Fourier transform (STFT), and discrete
wavelet transform (DWT) representations on our front-end model. This
measurement involves comparing the classification performance over the
adversarial robustness. We demonstrate an inverse relationship between
recognition accuracy and model robustness against six benchmarking attack
algorithms on the balance of average budgets allocated by the adversary and the
attack cost. Moreover, our experimental results have shown that while the
ResNet-18 model trained on DWT spectrograms achieves a high recognition
accuracy, attacking this model is relatively more costly for the adversary than
other 2D representations. We also report some results on different
convolutional neural network architectures such as ResNet-34, ResNet-56,
AlexNet, and GoogLeNet, SB-CNN, and LSTM-based.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Q-TART: Quickly Training for Adversarial Robustness and in-Transferability. (arXiv:2204.07024v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07024">
<div class="article-summary-box-inner">
<span><p>Raw deep neural network (DNN) performance is not enough; in real-world
settings, computational load, training efficiency and adversarial security are
just as or even more important. We propose to simultaneously tackle
Performance, Efficiency, and Robustness, using our proposed algorithm Q-TART,
Quickly Train for Adversarial Robustness and in-Transferability. Q-TART follows
the intuition that samples highly susceptible to noise strongly affect the
decision boundaries learned by DNNs, which in turn degrades their performance
and adversarial susceptibility. By identifying and removing such samples, we
demonstrate improved performance and adversarial robustness while using only a
subset of the training data. Through our experiments we highlight Q-TART's high
performance across multiple Dataset-DNN combinations, including ImageNet, and
provide insights into the complementary behavior of Q-TART alongside existing
adversarial training approaches to increase robustness by over 1.3% while using
up to 17.9% less training time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomous Satellite Detection and Tracking using Optical Flow. (arXiv:2204.07025v1 [astro-ph.IM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07025">
<div class="article-summary-box-inner">
<span><p>In this paper, an autonomous method of satellite detection and tracking in
images is implemented using optical flow. Optical flow is used to estimate the
image velocities of detected objects in a series of space images. Given that
most objects in an image will be stars, the overall image velocity from star
motion is used to estimate the image's frame-to-frame motion. Objects seen to
be moving with velocity profiles distinct from the overall image velocity are
then classified as potential resident space objects. The detection algorithm is
exercised using both simulated star images and ground-based imagery of
satellites. Finally, this algorithm will be tested and compared using a
commercial and an open-source software approach to provide the reader with two
different options based on their need.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Activation Regression for Continuous Domain Generalization with Applications to Crop Classification. (arXiv:2204.07030v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07030">
<div class="article-summary-box-inner">
<span><p>Geographic variance in satellite imagery impacts the ability of machine
learning models to generalise to new regions. In this paper, we model
geographic generalisation in medium resolution Landsat-8 satellite imagery as a
continuous domain adaptation problem, demonstrating how models generalise
better with appropriate domain knowledge. We develop a dataset spatially
distributed across the entire continental United States, providing macroscopic
insight into the effects of geography on crop classification in multi-spectral
and temporally distributed satellite imagery. Our method demonstrates improved
generalisability from 1) passing geographically correlated climate variables
along with the satellite data to a Transformer model and 2) regressing on the
model features to reconstruct these domain variables. Combined, we provide a
novel perspective on geographic generalisation in satellite imagery and a
simple-yet-effective approach to leverage domain knowledge. Code is available
at: \url{https://github.com/samar-khanna/cropmap}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin-picking. (arXiv:2204.07049v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07049">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an iterative self-training framework for
sim-to-real 6D object pose estimation to facilitate cost-effective robotic
grasping. Given a bin-picking scenario, we establish a photo-realistic
simulator to synthesize abundant virtual data, and use this to train an initial
pose estimation network. This network then takes the role of a teacher model,
which generates pose predictions for unlabeled real data. With these
predictions, we further design a comprehensive adaptive selection scheme to
distinguish reliable results, and leverage them as pseudo labels to update a
student model for pose estimation on real data. To continuously improve the
quality of pseudo labels, we iterate the above steps by taking the trained
student model as a new teacher and re-label real data using the refined teacher
model. We evaluate our method on a public benchmark and our newly-released
dataset, achieving an ADD(-S) improvement of 11.49% and 22.62% respectively.
Our method is also able to improve robotic bin-picking success by 19.54%,
demonstrating the potential of iterative sim-to-real solutions for robotic
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CroCo: Cross-Modal Contrastive learning for localization of Earth Observation data. (arXiv:2204.07052v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07052">
<div class="article-summary-box-inner">
<span><p>It is of interest to localize a ground-based LiDAR point cloud on remote
sensing imagery. In this work, we tackle a subtask of this problem, i.e. to map
a digital elevation model (DEM) rasterized from aerial LiDAR point cloud on the
aerial imagery. We proposed a contrastive learning-based method that trains on
DEM and high-resolution optical imagery and experiment the framework on
different data sampling strategies and hyperparameters. In the best scenario,
the Top-1 score of 0.71 and Top-5 score of 0.81 are obtained. The proposed
method is promising for feature learning from RGB and DEM for localization and
is potentially applicable to other data sources too. Source code will be
released at https://github.com/wtseng530/AVLocalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Egocentric Human-Object Interaction Detection Exploiting Synthetic Data. (arXiv:2204.07061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07061">
<div class="article-summary-box-inner">
<span><p>We consider the problem of detecting Egocentric HumanObject Interactions
(EHOIs) in industrial contexts. Since collecting and labeling large amounts of
real images is challenging, we propose a pipeline and a tool to generate
photo-realistic synthetic First Person Vision (FPV) images automatically
labeled for EHOI detection in a specific industrial scenario. To tackle the
problem of EHOI detection, we propose a method that detects the hands, the
objects in the scene, and determines which objects are currently involved in an
interaction. We compare the performance of our method with a set of
state-of-the-art baselines. Results show that using a synthetic dataset
improves the performance of an EHOI detection system, especially when few real
data are available. To encourage research on this topic, we publicly release
the proposed dataset at the following url:
https://iplab.dmi.unict.it/EHOI_SYNTH/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic Segmentation using Synthetic and Real Data. (arXiv:2204.07069v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07069">
<div class="article-summary-box-inner">
<span><p>Being able to understand the relations between the user and the surrounding
environment is instrumental to assist users in a worksite. For instance,
understanding which objects a user is interacting with from images and video
collected through a wearable device can be useful to inform the worker on the
usage of specific objects in order to improve productivity and prevent
accidents. Despite modern vision systems can rely on advanced algorithms for
object detection, semantic and panoptic segmentation, these methods still
require large quantities of domain-specific labeled data, which can be
difficult to obtain in industrial scenarios. Motivated by this observation, we
propose a pipeline which allows to generate synthetic images from 3D models of
real environments and real objects. The generated images are automatically
labeled and hence effortless to obtain. Exploiting the proposed pipeline, we
generate a dataset comprising synthetic images automatically labeled for
panoptic segmentation. This set is complemented by a small number of manually
labeled real images for fine-tuning. Experiments show that the use of synthetic
images allows to drastically reduce the number of real images needed to obtain
reasonable panoptic segmentation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemiMultiPose: A Semi-supervised Multi-animal Pose Estimation Framework. (arXiv:2204.07072v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07072">
<div class="article-summary-box-inner">
<span><p>Multi-animal pose estimation is essential for studying animals' social
behaviors in neuroscience and neuroethology. Advanced approaches have been
proposed to support multi-animal estimation and achieve state-of-the-art
performance. However, these models rarely exploit unlabeled data during
training even though real world applications have exponentially more unlabeled
frames than labeled frames. Manually adding dense annotations for a large
number of images or videos is costly and labor-intensive, especially for
multiple instances. Given these deficiencies, we propose a novel
semi-supervised architecture for multi-animal pose estimation, leveraging the
abundant structures pervasive in unlabeled frames in behavior videos to enhance
training, which is critical for sparsely-labeled problems. The resulting
algorithm will provide superior multi-animal pose estimation results on three
animal experiments compared to the state-of-the-art baseline and exhibits more
predictive power in sparsely-labeled data regimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Learning for Joint Depth and Image Reconstruction from Diffracted Rotation. (arXiv:2204.07076v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07076">
<div class="article-summary-box-inner">
<span><p>Monocular depth estimation is still an open challenge due to the ill-posed
nature of the problem at hand. Deep learning based techniques have been
extensively studied and proved capable of producing acceptable depth estimation
accuracy even if the lack of meaningful and robust depth cues within single RGB
input images severally limits their performance. Coded aperture-based methods
using phase and amplitude masks encode strong depth cues within 2D images by
means of depth-dependent Point Spread Functions (PSFs) at the price of a
reduced image quality. In this paper, we propose a novel end-to-end learning
approach for depth from diffracted rotation. A phase mask that produces a
Rotating Point Spread Function (RPSF) as a function of defocus is jointly
optimized with the weights of a depth estimation neural network. To this aim,
we introduce a differentiable physical model of the aperture mask and exploit
an accurate simulation of the camera imaging pipeline. Our approach requires a
significantly less complex model and less training data, yet it is superior to
existing methods in the task of monocular depth estimation on indoor
benchmarks. In addition, we address the problem of image degradation by
incorporating a non-blind and non-uniform image deblurring module to recover
the sharp all-in-focus image from its RPSF-blurred counterpart.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Attended Object Detection Using Gaze Data as Annotations. (arXiv:2204.07090v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07090">
<div class="article-summary-box-inner">
<span><p>We consider the problem of detecting and recognizing the objects observed by
visitors (i.e., attended objects) in cultural sites from egocentric vision. A
standard approach to the problem involves detecting all objects and selecting
the one which best overlaps with the gaze of the visitor, measured through a
gaze tracker. Since labeling large amounts of data to train a standard object
detector is expensive in terms of costs and time, we propose a weakly
supervised version of the task which leans only on gaze data and a frame-level
label indicating the class of the attended object. To study the problem, we
present a new dataset composed of egocentric videos and gaze coordinates of
subjects visiting a museum. We hence compare three different baselines for
weakly supervised attended object detection on the collected data. Results show
that the considered approaches achieve satisfactory performance in a weakly
supervised manner, which allows for significant time savings with respect to a
fully supervised detector based on Faster R-CNN. To encourage research on the
topic, we publicly release the code and the dataset at the following url:
https://iplab.dmi.unict.it/WS_OBJ_DET/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Degraded Acacia tree species using deep neural networks on uav drone imagery. (arXiv:2204.07096v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07096">
<div class="article-summary-box-inner">
<span><p>Deep-learning-based image classification and object detection has been
applied successfully to tree monitoring. However, studies of tree crowns and
fallen trees, especially on flood inundated areas, remain largely unexplored.
Detection of degraded tree trunks on natural environments such as water,
mudflats, and natural vegetated areas is challenging due to the mixed colour
image backgrounds. In this paper, Unmanned Aerial Vehicles (UAVs), or drones,
with embedded RGB cameras were used to capture the fallen Acacia Xanthophloea
trees from six designated plots around Lake Nakuru, Kenya. Motivated by the
need to detect fallen trees around the lake, two well-established deep neural
networks, i.e. Faster Region-based Convolution Neural Network (Faster R-CNN)
and Retina-Net were used for fallen tree detection. A total of 7,590
annotations of three classes on 256 x 256 image patches were used for this
study. Experimental results show the relevance of deep learning in this
context, with Retina-Net model achieving 38.9% precision and 57.9% recall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Swin Transformer Channel Attention Network for Image Demosaicing. (arXiv:2204.07098v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07098">
<div class="article-summary-box-inner">
<span><p>Image demosaicing is problem of interpolating full- resolution color images
from raw sensor (color filter array) data. During last decade, deep neural
networks have been widely used in image restoration, and in particular, in
demosaicing, attaining significant performance improvement. In recent years,
vision transformers have been designed and successfully used in various
computer vision applications. One of the recent methods of image restoration
based on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art
performance with a smaller number of parameters than neural network-based
methods. Inspired by the success of SwinIR, we propose in this paper a novel
Swin Transformer-based network for image demosaicing, called RSTCANet. To
extract image features, RSTCANet stacks several residual Swin Transformer
Channel Attention blocks (RSTCAB), introducing the channel attention for each
two successive ST blocks. Extensive experiments demonstrate that RSTCANet out-
performs state-of-the-art image demosaicing methods, and has a smaller number
of parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look Back and Forth: Video Super-Resolution with Explicit Temporal Difference Modeling. (arXiv:2204.07114v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07114">
<div class="article-summary-box-inner">
<span><p>Temporal modeling is crucial for video super-resolution. Most of the video
super-resolution methods adopt the optical flow or deformable convolution for
explicitly motion compensation. However, such temporal modeling techniques
increase the model complexity and might fail in case of occlusion or complex
motion, resulting in serious distortion and artifacts. In this paper, we
propose to explore the role of explicit temporal difference modeling in both LR
and HR space. Instead of directly feeding consecutive frames into a VSR model,
we propose to compute the temporal difference between frames and divide those
pixels into two subsets according to the level of difference. They are
separately processed with two branches of different receptive fields in order
to better extract complementary information. To further enhance the
super-resolution result, not only spatial residual features are extracted, but
the difference between consecutive frames in high-frequency domain is also
computed. It allows the model to exploit intermediate SR results in both future
and past to refine the current SR output. The difference at different time
steps could be cached such that information from further distance in time could
be propagated to the current frame for refinement. Experiments on several video
super-resolution benchmark datasets demonstrate the effectiveness of the
proposed method and its favorable performance against state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeiT III: Revenge of the ViT. (arXiv:2204.07118v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07118">
<div class="article-summary-box-inner">
<span><p>A Vision Transformer (ViT) is a simple neural architecture amenable to serve
several computer vision tasks. It has limited built-in architectural priors, in
contrast to more recent architectures that incorporate priors either about the
input data or of specific tasks. Recent works show that ViTs benefit from
self-supervised pre-training, in particular BerT-like pre-training like BeiT.
In this paper, we revisit the supervised training of ViTs. Our procedure builds
upon and simplifies a recipe introduced for training ResNet-50. It includes a
new simple data-augmentation procedure with only 3 augmentations, closer to the
practice in self-supervised learning. Our evaluations on Image classification
(ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning
and semantic segmentation show that our procedure outperforms by a large margin
previous fully supervised training recipes for ViT. It also reveals that the
performance of our ViT trained with supervision is comparable to that of more
recent architectures. Our results could serve as better baselines for recent
self-supervised approaches demonstrated on ViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIFS: Neural Implicit Function for General Shape Representation. (arXiv:2204.07126v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07126">
<div class="article-summary-box-inner">
<span><p>Recent development of neural implicit function has shown tremendous success
on high-quality 3D shape reconstruction. However, most works divide the space
into inside and outside of the shape, which limits their representing power to
single-layer and watertight shapes. This limitation leads to tedious data
processing (converting non-watertight raw data to watertight) as well as the
incapability of representing general object shapes in the real world. In this
work, we propose a novel method to represent general shapes including
non-watertight shapes and shapes with multi-layer surfaces. We introduce
General Implicit Function for 3D Shape (GIFS), which models the relationships
between every two points instead of the relationships between points and
surfaces. Instead of dividing 3D space into predefined inside-outside regions,
GIFS encodes whether two points are separated by any surface. Experiments on
ShapeNet show that GIFS outperforms previous state-of-the-art methods in terms
of reconstruction quality, rendering efficiency, and visual fidelity. Project
page is available at https://jianglongye.com/gifs .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Siamese Networks for Label-Efficient Learning. (arXiv:2204.07141v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07141">
<div class="article-summary-box-inner">
<span><p>We propose Masked Siamese Networks (MSN), a self-supervised learning
framework for learning image representations. Our approach matches the
representation of an image view containing randomly masked patches to the
representation of the original unmasked image. This self-supervised
pre-training strategy is particularly scalable when applied to Vision
Transformers since only the unmasked patches are processed by the network. As a
result, MSNs improve the scalability of joint-embedding architectures, while
producing representations of a high semantic level that perform competitively
on low-shot image classification. For instance, on ImageNet-1K, with only 5,000
annotated images, our base MSN model achieves 72.4% top-1 accuracy, and with 1%
of ImageNet-1K labels, we achieve 75.7% top-1 accuracy, setting a new
state-of-the-art for self-supervised learning on this benchmark. Our code is
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neighborhood Attention Transformer. (arXiv:2204.07143v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07143">
<div class="article-summary-box-inner">
<span><p>We present Neighborhood Attention Transformer (NAT), an efficient, accurate
and scalable hierarchical transformer that works well on both image
classification and downstream vision tasks. It is built upon Neighborhood
Attention (NA), a simple and flexible attention mechanism that localizes the
receptive field for each query to its nearest neighboring pixels. NA is a
localization of self-attention, and approaches it as the receptive field size
increases. It is also equivalent in FLOPs and memory usage to Swin
Transformer's shifted window attention given the same receptive field size,
while being less constrained. Furthermore, NA includes local inductive biases,
which eliminate the need for extra operations such as pixel shifts.
Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1
accuracy on ImageNet with only 4.3 GFLOPs and 28M parameters, 51.4% mAP on
MS-COCO and 48.4% mIoU on ADE20k. We will open-source our checkpoints, training
script, configurations, and our CUDA kernel at:
https://github.com/SHI-Labs/Neighborhood-Attention-Transformer .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformable Sprites for Unsupervised Video Decomposition. (arXiv:2204.07151v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07151">
<div class="article-summary-box-inner">
<span><p>We describe a method to extract persistent elements of a dynamic scene from
an input video. We represent each scene element as a \emph{Deformable Sprite}
consisting of three components: 1) a 2D texture image for the entire video, 2)
per-frame masks for the element, and 3) non-rigid deformations that map the
texture image into each video frame. The resulting decomposition allows for
applications such as consistent video editing. Deformable Sprites are a type of
video auto-encoder model that is optimized on individual videos, and does not
require training on a large dataset, nor does it rely on pre-trained models.
Moreover, our method does not require object masks or other user input, and
discovers moving objects of a wider variety than previous work. We evaluate our
approach on standard video datasets and show qualitative results on a diverse
array of Internet videos. Code and video results can be found at
https://deformable-sprites.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's in your hands? 3D Reconstruction of Generic Objects in Hands. (arXiv:2204.07153v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07153">
<div class="article-summary-box-inner">
<span><p>Our work aims to reconstruct hand-held objects given a single RGB image. In
contrast to prior works that typically assume known 3D templates and reduce the
problem to 3D pose estimation, our work reconstructs generic hand-held object
without knowing their 3D templates. Our key insight is that hand articulation
is highly predictive of the object shape, and we propose an approach that
conditionally reconstructs the object based on the articulation and the visual
input. Given an image depicting a hand-held object, we first use off-the-shelf
systems to estimate the underlying hand pose and then infer the object shape in
a normalized hand-centric coordinate frame. We parameterized the object by
signed distance which are inferred by an implicit network which leverages the
information from both visual feature and articulation-aware coordinates to
process a query point. We perform experiments across three datasets and show
that our method consistently outperforms baselines and is able to reconstruct a
diverse set of objects. We analyze the benefits and robustness of explicit
articulation conditioning and also show that this allows the hand pose
estimation to further improve in test-time optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MiniViT: Compressing Vision Transformers with Weight Multiplexing. (arXiv:2204.07154v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07154">
<div class="article-summary-box-inner">
<span><p>Vision Transformer (ViT) models have recently drawn much attention in
computer vision due to their high model capability. However, ViT models suffer
from huge number of parameters, restricting their applicability on devices with
limited memory. To alleviate this problem, we propose MiniViT, a new
compression framework, which achieves parameter reduction in vision
transformers while retaining the same performance. The central idea of MiniViT
is to multiplex the weights of consecutive transformer blocks. More
specifically, we make the weights shared across layers, while imposing a
transformation on the weights to increase diversity. Weight distillation over
self-attention is also applied to transfer knowledge from large-scale ViT
models to weight-multiplexed compact models. Comprehensive experiments
demonstrate the efficacy of MiniViT, showing that it can reduce the size of the
pre-trained Swin-B transformer by 48\%, while achieving an increase of 1.0\% in
Top-1 accuracy on ImageNet. Moreover, using a single-layer of parameters,
MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters,
without seriously compromising the performance. Finally, we verify the
transferability of MiniViT by reporting its performance on downstream
benchmarks. Code and models are available at here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Any-resolution Training for High-resolution Image Synthesis. (arXiv:2204.07156v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07156">
<div class="article-summary-box-inner">
<span><p>Generative models operate at fixed resolution, even though natural images
come in a variety of sizes. As high-resolution details are downsampled away,
and low-resolution images are discarded altogether, precious supervision is
lost. We argue that every pixel matters and create datasets with variable-size
images, collected at their native resolutions. Taking advantage of this data is
challenging; high-resolution processing is costly, and current architectures
can only process fixed-resolution data. We introduce continuous-scale training,
a process that samples patches at random scales to train a new generator with
variable output resolutions. First, conditioning the generator on a target
scale allows us to generate higher resolutions images than previously possible,
without adding layers to the model. Second, by conditioning on continuous
coordinates, we can sample patches that still obey a consistent global layout,
which also allows for scalable training at higher resolutions. Controlled FFHQ
experiments show our method takes advantage of the multi-resolution training
data better than discrete multi-scale approaches, achieving better FID scores
and cleaner high-frequency details. We also train on other natural image
domains including churches, mountains, and birds, and demonstrate arbitrary
scale synthesis with both coherent global layouts and realistic local details,
going beyond 2K resolution in our experiments. Our project page is available
at: https://chail.github.io/anyres-gan/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Forecasting of Panoptic Segmentations with Difference Attention. (arXiv:2204.07157v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07157">
<div class="article-summary-box-inner">
<span><p>Forecasting of a representation is important for safe and effective autonomy.
For this, panoptic segmentations have been studied as a compelling
representation in recent work. However, recent state-of-the-art on panoptic
segmentation forecasting suffers from two issues: first, individual object
instances are treated independently of each other; second, individual object
instance forecasts are merged in a heuristic manner. To address both issues, we
study a new panoptic segmentation forecasting model that jointly forecasts all
object instances in a scene using a transformer model based on 'difference
attention.' It further refines the predictions by taking depth estimates into
account. We evaluate the proposed model on the Cityscapes and AIODrive
datasets. We find difference attention to be particularly suitable for
forecasting because the difference of quantities like locations enables a model
to explicitly reason about velocities and acceleration. Because of this, we
attain state-of-the-art on panoptic segmentation forecasting metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Level Set Theory for Neural Implicit Evolution under Explicit Flows. (arXiv:2204.07159v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07159">
<div class="article-summary-box-inner">
<span><p>Coordinate-based neural networks parameterizing implicit surfaces have
emerged as efficient representations of geometry. They effectively act as
parametric level sets with the zero-level set defining the surface of interest.
We present a framework that allows applying deformation operations defined for
triangle meshes onto such implicit surfaces. Several of these operations can be
viewed as energy-minimization problems that induce an instantaneous flow field
on the explicit surface. Our method uses the flow field to deform parametric
implicit surfaces by extending the classical theory of level sets. We also
derive a consolidated view for existing methods on differentiable surface
extraction and rendering, by formalizing connections to the level-set theory.
We show that these methods drift from the theory and that our approach exhibits
improvements for applications like surface smoothing, mean-curvature flow,
inverse rendering and user-defined editing on implicit geometry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate Lung Nodules Segmentation with Detailed Representation Transfer and Soft Mask Supervision. (arXiv:2007.14556v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.14556">
<div class="article-summary-box-inner">
<span><p>Accurate lung lesion segmentation from Computed Tomography (CT) images is
crucial to the analysis and diagnosis of lung diseases such as COVID-19 and
lung cancer. However, the smallness and variety of lung nodules and the lack of
high-quality labeling make the accurate lung nodule segmentation difficult. To
address these issues, we first introduce a novel segmentation mask named Soft
Mask which has richer and more accurate edge details description and better
visualization and develop a universal automatic Soft Mask annotation pipeline
to deal with different datasets correspondingly. Then, a novel Network with
detailed representation transfer and Soft Mask supervision (DSNet) is proposed
to process the input low-resolution images of lung nodules into high-quality
segmentation results. Our DSNet contains a special Detail Representation
Transfer Module (DRTM) for reconstructing the detailed representation to
alleviate the small size of lung nodules images, and an adversarial training
framework with Soft Mask for further improving the accuracy of segmentation.
Extensive experiments validate that our DSNet outperforms other
state-of-the-art methods for accurate lung nodule segmentation and has strong
generalization ability in other accurate medical segmentation tasks with
competitive results. Besides, we provide a new challenging lung nodules
segmentation dataset for further studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Activation Map Adaptation for Effective Knowledge Distillation. (arXiv:2010.13500v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13500">
<div class="article-summary-box-inner">
<span><p>Model compression becomes a recent trend due to the requirement of deploying
neural networks on embedded and mobile devices. Hence, both accuracy and
efficiency are of critical importance. To explore a balance between them, a
knowledge distillation strategy is proposed for general visual representation
learning. It utilizes our well-designed activation map adaptive module to
replace some blocks of the teacher network, exploring the most appropriate
supervisory features adaptively during the training process. Using the
teacher's hidden layer output to prompt the student network to train so as to
transfer effective semantic information.To verify the effectiveness of our
strategy, this paper applied our method to cifar-10 dataset. Results
demonstrate that the method can boost the accuracy of the student network by
0.6% with 6.5% loss reduction, and significantly improve its training speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SVAM: Saliency-guided Visual Attention Modeling by Autonomous Underwater Robots. (arXiv:2011.06252v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06252">
<div class="article-summary-box-inner">
<span><p>This paper presents a holistic approach to saliency-guided visual attention
modeling (SVAM) for use by autonomous underwater robots. Our proposed model,
named SVAM-Net, integrates deep visual features at various scales and semantics
for effective salient object detection (SOD) in natural underwater images. The
SVAM-Net architecture is configured in a unique way to jointly accommodate
bottom-up and top-down learning within two separate branches of the network
while sharing the same encoding layers. We design dedicated spatial attention
modules (SAMs) along these learning pathways to exploit the coarse-level and
fine-level semantic features for SOD at four stages of abstractions. The
bottom-up branch performs a rough yet reasonably accurate saliency estimation
at a fast rate, whereas the deeper top-down branch incorporates a residual
refinement module (RRM) that provides fine-grained localization of the salient
objects. Extensive performance evaluation of SVAM-Net on benchmark datasets
clearly demonstrates its effectiveness for underwater SOD. We also validate its
generalization performance by several ocean trials' data that include test
images of diverse underwater scenes and waterbodies, and also images with
unseen natural objects. Moreover, we analyze its computational feasibility for
robotic deployments and demonstrate its utility in several important use cases
of visual attention modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing a new high-resolution handwritten digits data set with writer characteristics. (arXiv:2011.07946v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07946">
<div class="article-summary-box-inner">
<span><p>The contributions in this article are two-fold. First, we introduce a new
hand-written digit data set that we collected. It contains high-resolution
images of hand-written The contributions in this article are two-fold. First,
we introduce a new handwritten digit data set that we collected. It contains
high-resolution images of handwritten digits together with various writer
characteristics which are not available in the well-known MNIST database. The
multiple writer characteristics gathered are a novelty of our data set and
create new research opportunities. The data set is publicly available online.
Second, we analyse this new data set. We begin with simple supervised tasks. We
assess the predictability of the writer characteristics gathered, the effect of
using some of those characteristics as predictors in classification task and
the effect of higher resolution images on classification accuracy. We also
explore semi-supervised applications; we can leverage the high quantity of
handwritten digits data sets already existing online to improve the accuracy of
various classifications task with noticeable success. Finally, we also
demonstrate the generative perspective offered by this new data set; we are
able to generate images that mimics the writing style of specific writers. The
data set has unique and distinct features and our analysis establishes
benchmarks and showcases some of the new opportunities made possible with this
new data set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Temporal Learning on Monocular Videos for 3D Human Pose Estimation. (arXiv:2012.01511v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01511">
<div class="article-summary-box-inner">
<span><p>In this paper we propose an unsupervised learning method to extract temporal
information on monocular videos, where we detect and encode subject of interest
in each frame and leverage contrastive self-supervised (CSS) learning to
extract rich latent vectors. Instead of simply treating the latent features of
nearby frames as positive pairs and those of temporally-distant ones as
negative pairs as in other CSS approaches, we explicitly disentangle each
latent vector into a time-variant component and a time-invariant one. We then
show that applying CSS only to the time-variant features and encouraging a
gradual transition on them between nearby and away frames while also
reconstructing the input, extract rich temporal features into the time-variant
component, well-suited for human pose estimation. Our approach reduces error by
about 50\% compared to the standard CSS strategies, outperforms other
unsupervised single-view methods and matches the performance of multi-view
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Contrastive Learning for Text-to-Image Generation. (arXiv:2101.04702v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04702">
<div class="article-summary-box-inner">
<span><p>The output of text-to-image synthesis systems should be coherent, clear,
photo-realistic scenes with high semantic fidelity to their conditioned text
descriptions. Our Cross-Modal Contrastive Generative Adversarial Network
(XMC-GAN) addresses this challenge by maximizing the mutual information between
image and text. It does this via multiple contrastive losses which capture
inter-modality and intra-modality correspondences. XMC-GAN uses an attentional
self-modulation generator, which enforces strong text-image correspondence, and
a contrastive discriminator, which acts as a critic as well as a feature
encoder for contrastive learning. The quality of XMC-GAN's output is a major
step up from previous models, as we show on three challenging datasets. On
MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33,
but--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1
for image-text alignment, compared to three other recent models. XMC-GAN also
generalizes to the challenging Localized Narratives dataset (which has longer,
more detailed descriptions), improving state-of-the-art FID from 48.70 to
14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images
data, establishing a strong benchmark FID score of 26.91.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Green View Index and Green View Index best path using Google Street View and deep learning. (arXiv:2104.12627v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12627">
<div class="article-summary-box-inner">
<span><p>Streetscape is an important part of the urban landscape, analysing and
studying them can increase the understanding of the cities' infrastructure,
which can lead to better planning and design of the urban living environment.
In this paper, we used Google Street View to obtain street view images of Osaka
City. The semantic segmentation model HRNet-OCR \cite{HRNet-OCR} is used to
segment the Osaka City street view images and analyse the Green View Index
(GVI) of Osaka City. Based on the GVI value, because of the limitations of
ArcGIS software, we take advantage of adjacency matrix and Floyd algorithm is
used to calculate Green View Index best path. Our analysis not only allows for
the calculation of specific routes for the optimal GVI paths, but also allows
for the visualization and integration of neighbourhood landscape. By
summarising all the data, a more specific and objective analysis of the
landscape in the study area can be carried out and based on this, the available
natural resources can be maximised for a better life.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Adversarial Transferability with Gradient Refining. (arXiv:2105.04834v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04834">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are vulnerable to adversarial examples, which are
crafted by adding human-imperceptible perturbations to original images. Most
existing adversarial attack methods achieve nearly 100% attack success rates
under the white-box setting, but only achieve relatively low attack success
rates under the black-box setting. To improve the transferability of
adversarial examples for the black-box setting, several methods have been
proposed, e.g., input diversity, translation-invariant attack, and
momentum-based attack. In this paper, we propose a method named Gradient
Refining, which can further improve the adversarial transferability by
correcting useless gradients introduced by input diversity through multiple
transformations. Our method is generally applicable to many gradient-based
attack methods combined with input diversity. Extensive experiments are
conducted on the ImageNet dataset and our method can achieve an average
transfer success rate of 82.07% for three different models under single-model
setting, which outperforms the other state-of-the-art methods by a large margin
of 6.0% averagely. And we have applied the proposed method to the competition
CVPR 2021 Unrestricted Adversarial Attacks on ImageNet organized by Alibaba and
won the second place in attack success rates among 1558 teams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Mobile GUI: from Pixel-Words to Screen-Sentences. (arXiv:2105.11941v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11941">
<div class="article-summary-box-inner">
<span><p>The ubiquity of mobile phones makes mobile GUI understanding an important
task. Most previous works in this domain require human-created metadata of
screens (e.g. View Hierarchy) during inference, which unfortunately is often
not available or reliable enough for GUI understanding. Inspired by the
impressive success of Transformers in NLP tasks, targeting for purely
vision-based GUI understanding, we extend the concepts of Words/Sentence to
Pixel-Words/Screen-Sentence, and propose a mobile GUI understanding
architecture: Pixel-Words to Screen-Sentence (PW2SS). In analogy to the
individual Words, we define the Pixel-Words as atomic visual components (text
and graphic components), which are visually consistent and semantically clear
across screenshots of a large variety of design styles. The Pixel-Words
extracted from a screenshot are aggregated into Screen-Sentence with a Screen
Transformer proposed to model their relations. Since the Pixel-Words are
defined as atomic visual components, the ambiguity between their visual
appearance and semantics is dramatically reduced. We are able to make use of
metadata available in training data to auto-generate high-quality annotations
for Pixel-Words. A dataset, RICO-PW, of screenshots with Pixel-Words
annotations is built based on the public RICO dataset, which will be released
to help to address the lack of high-quality training data in this area. We
train a detector to extract Pixel-Words from screenshots on this dataset and
achieve metadata-free GUI understanding during inference. We conduct
experiments and show that Pixel-Words can be well extracted on RICO-PW and well
generalized to a new dataset, P2S-UI, collected by ourselves. The effectiveness
of PW2SS is further verified in the GUI understanding tasks including relation
prediction, clickability prediction, screen retrieval, and app type
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Online Learning System for Wireless Charging Alignment using Surround-view Fisheye Cameras. (arXiv:2105.12763v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12763">
<div class="article-summary-box-inner">
<span><p>Electric Vehicles are increasingly common, with inductive chargepads being
considered a convenient and efficient means of charging electric vehicles.
However, drivers are typically poor at aligning the vehicle to the necessary
accuracy for efficient inductive charging, making the automated alignment of
the two charging plates desirable. In parallel to the electrification of the
vehicular fleet, automated parking systems that make use of surround-view
camera systems are becoming increasingly popular. In this work, we propose a
system based on the surround-view camera architecture to detect, localize, and
automatically align the vehicle with the inductive chargepad. The visual design
of the chargepads is not standardized and not necessarily known beforehand.
Therefore, a system that relies on offline training will fail in some
situations. Thus, we propose a self-supervised online learning method that
leverages the driver's actions when manually aligning the vehicle with the
chargepad and combine it with weak supervision from semantic segmentation and
depth to learn a classifier to auto-annotate the chargepad in the video for
further training. In this way, when faced with a previously unseen chargepad,
the driver needs only manually align the vehicle a single time. As the
chargepad is flat on the ground, it is not easy to detect it from a distance.
Thus, we propose using a Visual SLAM pipeline to learn landmarks relative to
the chargepad to enable alignment from a greater range. We demonstrate the
working system on an automated vehicle as illustrated in the video
https://youtu.be/_cLCmkW4UYo. To encourage further research, we will share a
chargepad dataset used in this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning. (arXiv:2106.06047v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06047">
<div class="article-summary-box-inner">
<span><p>Federated learning is an emerging research paradigm enabling collaborative
training of machine learning models among different organizations while keeping
data private at each institution. Despite recent progress, there remain
fundamental challenges such as the lack of convergence and the potential for
catastrophic forgetting across real-world heterogeneous devices. In this paper,
we demonstrate that self-attention-based architectures (e.g., Transformers) are
more robust to distribution shifts and hence improve federated learning over
heterogeneous data. Concretely, we conduct the first rigorous empirical
investigation of different neural architectures across a range of federated
algorithms, real-world benchmarks, and heterogeneous data splits. Our
experiments show that simply replacing convolutional networks with Transformers
can greatly reduce catastrophic forgetting of previous devices, accelerate
convergence, and reach a better global model, especially when dealing with
heterogeneous data. We release our code and pretrained models at
https://github.com/Liangqiong/ViT-FL-main to encourage future exploration in
robust architectures as an alternative to current research efforts on the
optimization front.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handling Data Heterogeneity with Generative Replay in Collaborative Learning for Medical Imaging. (arXiv:2106.13208v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13208">
<div class="article-summary-box-inner">
<span><p>Collaborative learning, which enables collaborative and decentralized
training of deep neural networks at multiple institutions in a
privacy-preserving manner, is rapidly emerging as a valuable technique in
healthcare applications. However, its distributed nature often leads to
significant heterogeneity in data distributions across institutions. In this
paper, we present a novel generative replay strategy to address the challenge
of data heterogeneity in collaborative learning methods. Different from
traditional methods that directly aggregating the model parameters, we leverage
generative adversarial learning to aggregate the knowledge from all the local
institutions. Specifically, instead of directly training a model for task
performance, we develop a novel dual model architecture: a primary model learns
the desired task, and an auxiliary "generative replay model" allows aggregating
knowledge from the heterogenous clients. The auxiliary model is then
broadcasted to the central sever, to regulate the training of primary model
with an unbiased target distribution. Experimental results demonstrate the
capability of the proposed method in handling heterogeneous data across
institutions. On highly heterogeneous data partitions, our model achieves
~4.88% improvement in the prediction accuracy on a diabetic retinopathy
classification dataset, and ~49.8% reduction of mean absolution value on a Bone
Age prediction dataset, respectively, compared to the state-of-the art
collaborative learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HCR-Net: A deep learning based script independent handwritten character recognition network. (arXiv:2108.06663v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06663">
<div class="article-summary-box-inner">
<span><p>Despite being studied extensively for a few decades, handwritten character
recognition (HCR) is considered a challenging learning problem in pattern
recognition and there is very limited research on script independent models.
This is mainly because of diversity of scripts, focus of the conventional
research on handcrafted feature extraction techniques, and unavailability of
public datasets and codes to reproduce the results. On the other hand, deep
learning has witnessed huge success in different areas of pattern recognition,
including HCR, and provides end-to-end learning but it has been studied for
specific scripts only. In this paper, we have proposed a novel deep learning
architecture which exploits transfer learning and image-augmentation for
end-to-end learning for script independent handwritten character recognition,
called HCR-Net. HCR-Net is based on a novel transfer learning approach for HCR,
where some of lower layers of a pre-trained network are utilized. Due to
transfer learning and image-augmentation, HCR-Net provides faster training,
better performance and better generalizations, and can achieve up to 99\%
results of its final accuracy in just first epoch. The experimental results on
publicly available datasets of Bangla, Punjabi, Hindi, English, Swedish, Urdu,
Farsi, Tibetan, Kannada, Malayalam, Telugu, Marathi, Nepali and Arabic
languages prove the efficacy of HCR-Net and establishes several new benchmarks.
For reproducibility of the results and for the advancements of the HCR
research, complete code is publicly released at
https://github.com/jmdvinodjmd/HCR-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Burst Image Restoration and Enhancement. (arXiv:2110.03680v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03680">
<div class="article-summary-box-inner">
<span><p>Modern handheld devices can acquire burst image sequence in a quick
succession. However, the individual acquired frames suffer from multiple
degradations and are misaligned due to camera shake and object motions. The
goal of Burst Image Restoration is to effectively combine complimentary cues
across multiple burst frames to generate high-quality outputs. Towards this
goal, we develop a novel approach by solely focusing on the effective
information exchange between burst frames, such that the degradations get
filtered out while the actual scene details are preserved and enhanced. Our
central idea is to create a set of pseudo-burst features that combine
complementary information from all the input burst frames to seamlessly
exchange information. However, the pseudo-burst cannot be successfully created
unless the individual burst frames are properly aligned to discount inter-frame
movements. Therefore, our approach initially extracts pre-processed features
from each burst frame and matches them using an edge-boosting burst alignment
module. The pseudo-burst features are then created and enriched using
multi-scale contextual information. Our final step is to adaptively aggregate
information from the pseudo-burst features to progressively increase resolution
in multiple stages while merging the pseudo-burst features. In comparison to
existing works that usually follow a late fusion scheme with single-stage
upsampling, our approach performs favorably, delivering state-of-the-art
performance on burst superresolution, burst low-light image enhancement, and
burst denoising tasks. The source code and pre-trained models are available at
\url{https://github.com/akshaydudhane16/BIPNet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained Deep One-Class Feature Learning For Classifying Imbalanced Medical Images. (arXiv:2111.10610v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10610">
<div class="article-summary-box-inner">
<span><p>Medical image data are usually imbalanced across different classes. One-class
classification has attracted increasing attention to address the data imbalance
problem by distinguishing the samples of the minority class from the majority
class. Previous methods generally aim to either learn a new feature space to
map training samples together or to fit training samples by autoencoder-like
models. These methods mainly focus on capturing either compact or descriptive
features, where the information of the samples of a given one class is not
sufficiently utilized. In this paper, we propose a novel deep learning-based
method to learn compact features by adding constraints on the bottleneck
features, and to preserve descriptive features by training an autoencoder at
the same time. Through jointly optimizing the constraining loss and the
autoencoder's reconstruction loss, our method can learn more relevant features
associated with the given class, making the majority and minority samples more
distinguishable. Experimental results on three clinical datasets (including the
MRI breast images, FFDM breast images and chest X-ray images) obtains
state-of-art performance compared to previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Knowledge-Guided Deep Learning for Imbalanced Medical Image Classification. (arXiv:2111.10620v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10620">
<div class="article-summary-box-inner">
<span><p>Deep learning models have gained remarkable performance on a variety of image
classification tasks. However, many models suffer from limited performance in
clinical or medical settings when data are imbalanced. To address this
challenge, we propose a medical-knowledge-guided one-class classification
approach that leverages domain-specific knowledge of classification tasks to
boost the model's performance. The rationale behind our approach is that some
existing prior medical knowledge can be incorporated into data-driven deep
learning to facilitate model learning. We design a deep learning-based
one-class classification pipeline for imbalanced image classification, and
demonstrate in three use cases how we take advantage of medical knowledge of
each specific classification task by generating additional middle classes to
achieve higher classification performances. We evaluate our approach on three
different clinical image classification tasks (a total of 8459 images) and show
superior model performance when compared to six state-of-the-art methods. All
codes of this work will be publicly available upon acceptance of the paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning. (arXiv:2111.14213v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14213">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) is a promising strategy for performing
privacy-preserving, distributed learning with a network of clients (i.e., edge
devices). However, the data distribution among clients is often non-IID in
nature, making efficient optimization difficult. To alleviate this issue, many
FL algorithms focus on mitigating the effects of data heterogeneity across
clients by introducing a variety of proximal terms, some incurring considerable
compute and/or memory overheads, to restrain local updates with respect to the
global model. Instead, we consider rethinking solutions to data heterogeneity
in FL with a focus on local learning generality rather than proximal
restriction. To this end, we first present a systematic study informed by
second-order indicators to better understand algorithm effectiveness in FL.
Interestingly, we find that standard regularization methods are surprisingly
strong performers in mitigating data heterogeneity effects. Based on our
findings, we further propose a simple and effective method, FedAlign, to
overcome data heterogeneity and the pitfalls of previous methods. FedAlign
achieves competitive accuracy with state-of-the-art FL methods across a variety
of settings while minimizing computation and memory overhead. Code is available
at https://github.com/mmendiet/FedAlign
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis. (arXiv:2112.01148v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01148">
<div class="article-summary-box-inner">
<span><p>In recent years, the security of AI systems has drawn increasing research
attention, especially in the medical imaging realm. To develop a secure medical
image analysis (MIA) system, it is a must to study possible backdoor attacks
(BAs), which can embed hidden malicious behaviors into the system. However,
designing a unified BA method that can be applied to various MIA systems is
challenging due to the diversity of imaging modalities (e.g., X-Ray, CT, and
MRI) and analysis tasks (e.g., classification, detection, and segmentation).
Most existing BA methods are designed to attack natural image classification
models, which apply spatial triggers to training images and inevitably corrupt
the semantics of poisoned pixels, leading to the failures of attacking dense
prediction models. To address this issue, we propose a novel
Frequency-Injection based Backdoor Attack method (FIBA) that is capable of
delivering attacks in various MIA tasks. Specifically, FIBA leverages a trigger
function in the frequency domain that can inject the low-frequency information
of a trigger image into the poisoned image by linearly combining the spectral
amplitude of both images. Since it preserves the semantics of the poisoned
image pixels, FIBA can perform attacks on both classification and dense
prediction models. Experiments on three benchmarks in MIA (i.e., ISIC-2019 for
skin lesion classification, KiTS-19 for kidney tumor segmentation, and EAD-2019
for endoscopic artifact detection), validate the effectiveness of FIBA and its
superiority over state-of-the-art methods in attacking MIA models as well as
bypassing backdoor defense. Source code will be available at
https://github.com/HazardFY/FIBA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distill and De-bias: Mitigating Bias in Face Recognition using Knowledge Distillation. (arXiv:2112.09786v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09786">
<div class="article-summary-box-inner">
<span><p>Face recognition networks generally demonstrate bias with respect to
sensitive attributes like gender, skintone etc. For gender and skintone, we
observe that the regions of the face that a network attends to vary by the
category of an attribute. This might contribute to bias. Building on this
intuition, we propose a novel distillation-based approach called Distill and
De-bias (D&amp;D) to enforce a network to attend to similar face regions,
irrespective of the attribute category. In D&amp;D, we train a teacher network on
images from one category of an attribute; e.g. light skintone. Then distilling
information from the teacher, we train a student network on images of the
remaining category; e.g., dark skintone. A feature-level distillation loss
constrains the student network to generate teacher-like representations. This
allows the student network to attend to similar face regions for all attribute
categories and enables it to reduce bias. We also propose a second distillation
step on top of D&amp;D, called D&amp;D++. For the D&amp;D++ network, we distill the
`un-biasedness' of the D&amp;D network into a new student network, the D&amp;D++
network. We train the new network on all attribute categories; e.g., both light
and dark skintones. This helps us train a network that is less biased for an
attribute, while obtaining higher face verification performance than D&amp;D. We
show that D&amp;D++ outperforms existing baselines in reducing gender and skintone
bias on the IJB-C dataset, while obtaining higher face verification performance
than existing adversarial de-biasing methods. We evaluate the effectiveness of
our proposed methods on two state-of-the-art face recognition networks:
Crystalface and ArcFace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2. (arXiv:2112.14683v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14683">
<div class="article-summary-box-inner">
<span><p>Videos show continuous events, yet most $-$ if not all $-$ video synthesis
frameworks treat them discretely in time. In this work, we think of videos of
what they should be $-$ time-continuous signals, and extend the paradigm of
neural representations to build a continuous-time video generator. For this, we
first design continuous motion representations through the lens of positional
embeddings. Then, we explore the question of training on very sparse videos and
demonstrate that a good generator can be learned by using as few as 2 frames
per clip. After that, we rethink the traditional image + video discriminators
pair and design a holistic discriminator that aggregates temporal information
by simply concatenating frames' features. This decreases the training cost and
provides richer learning signal to the generator, making it possible to train
directly on 1024$^2$ videos for the first time. We build our model on top of
StyleGAN2 and it is just ${\approx}5\%$ more expensive to train at the same
resolution while achieving almost the same image quality. Moreover, our latent
space features similar properties, enabling spatial manipulations that our
method can propagate in time. We can generate arbitrarily long videos at
arbitrary high frame rate, while prior work struggles to generate even 64
frames at a fixed rate. Our model is tested on four modern 256$^2$ and one
1024$^2$-resolution video synthesis benchmarks. In terms of sheer metrics, it
performs on average ${\approx}30\%$ better than the closest runner-up. Project
website: https://universome.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BottleFit: Learning Compressed Representations in Deep Neural Networks for Effective and Efficient Split Computing. (arXiv:2201.02693v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02693">
<div class="article-summary-box-inner">
<span><p>Although mission-critical applications require the use of deep neural
networks (DNNs), their continuous execution at mobile devices results in a
significant increase in energy consumption. While edge offloading can decrease
energy consumption, erratic patterns in channel quality, network and edge
server load can lead to severe disruption of the system's key operations. An
alternative approach, called split computing, generates compressed
representations within the model (called "bottlenecks"), to reduce bandwidth
usage and energy consumption. Prior work has proposed approaches that introduce
additional layers, to the detriment of energy consumption and latency. For this
reason, we propose a new framework called BottleFit, which, in addition to
targeted DNN architecture modifications, includes a novel training strategy to
achieve high accuracy even with strong compression rates. We apply BottleFit on
cutting-edge DNN models in image classification, and show that BottleFit
achieves 77.1% data compression with up to 0.6% accuracy loss on ImageNet
dataset, while state of the art such as SPINN loses up to 6% in accuracy. We
experimentally measure the power consumption and latency of an image
classification application running on an NVIDIA Jetson Nano board (GPU-based)
and a Raspberry PI board (GPU-less). We show that BottleFit decreases power
consumption and latency respectively by up to 49% and 89% with respect to
(w.r.t.) local computing and by 37% and 55% w.r.t. edge offloading. We also
compare BottleFit with state-of-the-art autoencoders-based approaches, and show
that (i) BottleFit reduces power consumption and execution time respectively by
up to 54% and 44% on the Jetson and 40% and 62% on Raspberry PI; (ii) the size
of the head model executed on the mobile device is 83 times smaller. We publish
the code repository for reproducibility of the results in this study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes. (arXiv:2201.07788v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07788">
<div class="article-summary-box-inner">
<span><p>Progress in 3D object understanding has relied on manually canonicalized
shape datasets that contain instances with consistent position and orientation
(3D pose). This has made it hard to generalize these methods to in-the-wild
shapes, eg., from internet model collections or depth sensors. ConDor is a
self-supervised method that learns to Canonicalize the 3D orientation and
position for full and partial 3D point clouds. We build on top of Tensor Field
Networks (TFNs), a class of permutation- and rotation-equivariant, and
translation-invariant 3D networks. During inference, our method takes an unseen
full or partial 3D point cloud at an arbitrary pose and outputs an equivariant
canonical pose. During training, this network uses self-supervision losses to
learn the canonical pose from an un-canonicalized collection of full and
partial 3D point clouds. ConDor can also learn to consistently co-segment
object parts without any supervision. Extensive quantitative results on four
new metrics show that our approach outperforms existing methods while enabling
new applications such as operation on depth images and annotation transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Contrastive Learning with Cluster Ensemble for Unsupervised Person Re-identification. (arXiv:2201.11995v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11995">
<div class="article-summary-box-inner">
<span><p>Unsupervised person re-identification (ReID) aims to match a query image of a
pedestrian to the images in gallery set without supervision labels. The most
popular approaches to tackle unsupervised person ReID are usually performing a
clustering algorithm to yield pseudo labels at first and then exploit the
pseudo labels to train a deep neural network. However, the pseudo labels are
noisy and sensitive to the hyper-parameter(s) in clustering algorithm. In this
paper, we propose a Hybrid Contrastive Learning (HCL) approach for unsupervised
person ReID, which is based on a hybrid between instance-level and
cluster-level contrastive loss functions. Moreover, we present a
Multi-Granularity Clustering Ensemble based Hybrid Contrastive Learning
(MGCE-HCL) approach, which adopts a multi-granularity clustering ensemble
strategy to mine priority information among the pseudo positive sample pairs
and defines a priority-weighted hybrid contrastive loss for better tolerating
the noises in the pseudo positive samples. We conduct extensive experiments on
two benchmark datasets Market-1501 and DukeMTMC-reID. Experimental results
validate the effectiveness of our proposals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Frustratingly Simple Approach for End-to-End Image Captioning. (arXiv:2201.12723v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12723">
<div class="article-summary-box-inner">
<span><p>Image Captioning is a fundamental task to join vision and language,
concerning about cross-modal understanding and text generation. Recent years
witness the emerging attention on image captioning. Most of existing works
follow a traditional two-stage training paradigm. Before training the
captioning models, an extra object detector is utilized to recognize the
objects in the image at first. However, they require sizeable datasets with
fine-grained object annotation for training the object detector, which is a
daunting task. In addition, the errors of the object detectors are easy to
propagate to the following captioning models, degenerating models' performance.
To alleviate such defects, we propose a frustratingly simple but highly
effective end-to-end image captioning framework, Visual Conditioned GPT
(VC-GPT), by connecting the pre-trained visual encoder (CLIP-ViT) and language
decoder (GPT2). Different from the vanilla connection method that directly
inserts the cross-attention modules into GPT2, we come up with a self-ensemble
cross-modal fusion mechanism that comprehensively considers both the single-
and cross-modal knowledge. As a result, we do not need extra object detectors
for model training. Experimental results conducted on three popular image
captioning benchmarks (MSCOCO, Flickr30k and NoCaps) demonstrate that our
VC-GPT achieves either the best or the second-best performance across all
evaluation metrics over extensive baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Neural Network based Framework for Effective Laparoscopic Video Quality Assessment. (arXiv:2202.04517v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04517">
<div class="article-summary-box-inner">
<span><p>Video quality assessment is a challenging problem having a critical
significance in the context of medical imaging. For instance, in laparoscopic
surgery, the acquired video data suffers from different kinds of distortion
that not only hinder surgery performance but also affect the execution of
subsequent tasks in surgical navigation and robotic surgeries. For this reason,
we propose in this paper neural network-based approaches for distortion
classification as well as quality prediction. More precisely, a Residual
Network (ResNet) based approach is firstly developed for simultaneous ranking
and classification task. Then, this architecture is extended to make it
appropriate for the quality prediction task by using an additional Fully
Connected Neural Network (FCNN). To train the overall architecture (ResNet and
FCNN models), transfer learning and end-to-end learning approaches are
investigated. Experimental results, carried out on a new laparoscopic video
quality database, have shown the efficiency of the proposed methods compared to
recent conventional and deep learning based approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations. (arXiv:2202.07800v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07800">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViTs) take all the image patches as tokens and construct
multi-head self-attention (MHSA) among them. Complete leverage of these image
tokens brings redundant computations since not all the tokens are attentive in
MHSA. Examples include that tokens containing semantically meaningless or
distractive image backgrounds do not positively contribute to the ViT
predictions. In this work, we propose to reorganize image tokens during the
feed-forward process of ViT models, which is integrated into ViT during
training. For each forward inference, we identify the attentive image tokens
between MHSA and FFN (i.e., feed-forward network) modules, which is guided by
the corresponding class token attention. Then, we reorganize image tokens by
preserving attentive image tokens and fusing inattentive ones to expedite
subsequent MHSA and FFN computations. To this end, our method EViT improves
ViTs from two perspectives. First, under the same amount of input image tokens,
our method reduces MHSA and FFN computation for efficient inference. For
instance, the inference speed of DeiT-S is increased by 50% while its
recognition accuracy is decreased by only 0.3% for ImageNet classification.
Second, by maintaining the same computational cost, our method empowers ViTs to
take more image tokens as input for recognition accuracy improvement, where the
image tokens are from higher resolution images. An example is that we improve
the recognition accuracy of DeiT-S by 1% for ImageNet classification at the
same computational cost of a vanilla DeiT-S. Meanwhile, our method does not
introduce more parameters to ViTs. Experiments on the standard benchmarks show
the effectiveness of our method. The code is available at
https://github.com/youweiliang/evit
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Self-Supervised Learning of Global and Object-Centric Representations. (arXiv:2203.05997v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05997">
<div class="article-summary-box-inner">
<span><p>Self-supervision allows learning meaningful representations of natural
images, which usually contain one central object. How well does it transfer to
multi-entity scenes? We discuss key aspects of learning structured
object-centric representations with self-supervision and validate our insights
through several experiments on the CLEVR dataset. Regarding the architecture,
we confirm the importance of competition for attention-based object discovery,
where each image patch is exclusively attended by one object. For training, we
show that contrastive losses equipped with matching can be applied directly in
a latent space, avoiding pixel-based reconstruction. However, such an
optimization objective is sensitive to false negatives (recurring objects) and
false positives (matching errors). Careful consideration is thus required
around data augmentation and negative sample selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation. (arXiv:2203.09516v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09516">
<div class="article-summary-box-inner">
<span><p>Powerful priors allow us to perform inference with insufficient information.
In this paper, we propose an autoregressive prior for 3D shapes to solve
multimodal 3D tasks such as shape completion, reconstruction, and generation.
We model the distribution over 3D shapes as a non-sequential autoregressive
distribution over a discretized, low-dimensional, symbolic grid-like latent
representation of 3D shapes. This enables us to represent distributions over 3D
shapes conditioned on information from an arbitrary set of spatially anchored
query locations and thus perform shape completion in such arbitrary settings
(e.g., generating a complete chair given only a view of the back leg). We also
show that the learned autoregressive prior can be leveraged for conditional
tasks such as single-view reconstruction and language-based generation. This is
achieved by learning task-specific naive conditionals which can be approximated
by light-weight models trained on minimal paired data. We validate the
effectiveness of the proposed method using both quantitative and qualitative
evaluation and show that the proposed method outperforms the specialized
state-of-the-art methods trained for individual tasks. The project page with
code and video visualizations can be found at
https://yccyenchicheng.github.io/AutoSDF/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Motion Deblurring and Frame Interpolation with Events. (arXiv:2203.12178v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12178">
<div class="article-summary-box-inner">
<span><p>Slow shutter speed and long exposure time of frame-based cameras often cause
visual blur and loss of inter-frame information, degenerating the overall
quality of captured videos. To this end, we present a unified framework of
event-based motion deblurring and frame interpolation for blurry video
enhancement, where the extremely low latency of events is leveraged to
alleviate motion blur and facilitate intermediate frame prediction.
Specifically, the mapping relation between blurry frames and sharp latent
images is first predicted by a learnable double integral network, and a fusion
network is then proposed to refine the coarse results via utilizing the
information from consecutive blurry inputs and the concurrent events. By
exploring the mutual constraints among blurry frames, latent images, and event
streams, we further propose a self-supervised learning framework to enable
network training with real-world blurry videos and events. Extensive
experiments demonstrate that our method compares favorably against the
state-of-the-art approaches and achieves remarkable performance on both
synthetic and real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection. (arXiv:2203.13954v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13954">
<div class="article-summary-box-inner">
<span><p>The task of Human-Object Interaction~(HOI) detection could be divided into
two core problems, i.e., human-object association and interaction
understanding. In this paper, we reveal and address the disadvantages of the
conventional query-driven HOI detectors from the two aspects. For the
association, previous two-branch methods suffer from complex and costly
post-matching, while single-branch methods ignore the features distinction in
different tasks. We propose Guided-Embedding Network~(GEN) to attain a
two-branch pipeline without post-matching. In GEN, we design an instance
decoder to detect humans and objects with two independent query sets and a
position Guided Embedding~(p-GE) to mark the human and object in the same
position as a pair. Besides, we design an interaction decoder to classify
interactions, where the interaction queries are made of instance Guided
Embeddings (i-GE) generated from the outputs of each instance decoder layer.
For the interaction understanding, previous methods suffer from long-tailed
distribution and zero-shot discovery. This paper proposes a Visual-Linguistic
Knowledge Transfer (VLKT) training strategy to enhance interaction
understanding by transferring knowledge from a visual-linguistic pre-trained
model CLIP. In specific, we extract text embeddings for all labels with CLIP to
initialize the classifier and adopt a mimic loss to minimize the visual feature
distance between GEN and CLIP. As a result, GEN-VLKT outperforms the state of
the art by large margins on multiple datasets, e.g., +5.05 mAP on HICO-Det. The
source codes are available at https://github.com/YueLiao/gen-vlkt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Hyperspectral Images Using SVM with Shape-adaptive Reconstruction and Smoothed Total Variation. (arXiv:2203.15619v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15619">
<div class="article-summary-box-inner">
<span><p>In this work, a novel algorithm called SVM with Shape-adaptive Reconstruction
and Smoothed Total Variation (SaR-SVM-STV) is introduced to classify
hyperspectral images, which makes full use of spatial and spectral information.
The Shape-adaptive Reconstruction (SaR) is introduced to preprocess each pixel
based on the Pearson Correlation between pixels in its shape-adaptive (SA)
region. Support Vector Machines (SVMs) are trained to estimate the pixel-wise
probability maps of each class. Then the Smoothed Total Variation (STV) model
is applied to denoise and generate the final classification map. Experiments
show that SaR-SVM-STV outperforms the SVM-STV method with a few training
labels, demonstrating the significance of reconstructing hyperspectral images
before classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Quality Assessment of UGC Gaming Videos. (arXiv:2204.00128v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00128">
<div class="article-summary-box-inner">
<span><p>In recent years, with the vigorous development of the video game industry,
the proportion of gaming videos on major video websites like YouTube has
dramatically increased. However, relatively little research has been done on
the automatic quality prediction of gaming videos, especially on those that
fall in the category of "User-Generated-Content" (UGC). Since current leading
general-purpose Video Quality Assessment (VQA) models do not perform well on
this type of gaming videos, we have created a new VQA model specifically
designed to succeed on UGC gaming videos, which we call the Gaming Video
Quality Predictor (GAME-VQP). GAME-VQP successfully predicts the unique
statistical characteristics of gaming videos by drawing upon features designed
under modified natural scene statistics models, combined with gaming specific
features learned by a Convolution Neural Network. We study the performance of
GAME-VQP on a very recent large UGC gaming video database called
LIVE-YT-Gaming, and find that it both outperforms other mainstream general VQA
models as well as VQA models specifically designed for gaming videos. The new
model will be made public after paper being accepted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word separation in continuous sign language using isolated signs and post-processing. (arXiv:2204.00923v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00923">
<div class="article-summary-box-inner">
<span><p>Continuous Sign Language Recognition (CSLR) is a long challenging task in
Computer Vision due to the difficulties in detecting the explicit boundaries
between the words in a sign sentence. To deal with this challenge, we propose a
two-stage model. In the first stage, the predictor model, which includes a
combination of CNN, SVD, and LSTM, is trained with the isolated signs. In the
second stage, we apply a post-processing algorithm to the Softmax outputs
obtained from the first part of the model in order to separate the isolated
signs in the continuous signs. Due to the lack of a large dataset, including
both the sign sequences and the corresponding isolated signs, two public
datasets in Isolated Sign Language Recognition (ISLR), RKS-PERSIANSIGN and
ASLVID, are used for evaluation. Results of the continuous sign videos confirm
the efficiency of the proposed model to deal with isolated sign boundaries
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dynamic Correlations in Spatiotemporal Graphs for Motion Prediction. (arXiv:2204.01297v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01297">
<div class="article-summary-box-inner">
<span><p>Human motion prediction is a challenge task due to the dynamic spatiotemporal
graph correlations in different motion sequences. How to efficiently represent
spatiotemporal graph correlations and model dynamic correlation variances
between different motion sequences is a challenge for spatiotemporal graph
representation in motion prediction. In this work, we present Dynamic
SpatioTemporal Graph Convolution (DSTD-GC). The proposed DSTD-GC decomposes
dynamic spatiotemporal graph modeling into a combination of Dynamic Spatial
Graph Convolution (DS-GC) and Dynamic Temporal Graph Convolution (DT-GC). As
human motions are subject to common constraints like body connections and
present dynamic motion patterns from different samples, we present Constrained
Dynamic Correlation Modeling strategy to represent the spatial/temporal graph
as a shared spatial/temporal correlation and a function to extract
temporal-specific /spatial-specific adjustments for each sample. The modeling
strategy represents the spatiotemporal graph with 28.6\% parameters of the
state-of-the-art static decomposition representation while also explicitly
models sample-specific spatiotemporal correlation variances. Moreover, we also
mathematically reformulating spatiotemporal graph convolutions and their
decomposed variants into a unified form and find that DSTD-GC relaxes strict
constraints of other graph convolutions, leading to a stronger representation
capability. Combining DSTD-GC with prior knowledge, we propose a powerful
spatiotemporal graph convolution network called DSTD-GCN which outperforms
state-of-the-art methods on the Human3.6M and CMU Mocap datasets in prediction
accuracy with fewest parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end multi-particle reconstruction in high occupancy imaging calorimeters with graph neural networks. (arXiv:2204.01681v2 [physics.ins-det] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01681">
<div class="article-summary-box-inner">
<span><p>We present an end-to-end reconstruction algorithm to build particle
candidates from detector hits in next-generation granular calorimeters similar
to that foreseen for the high-luminosity upgrade of the CMS detector. The
algorithm exploits a distance-weighted graph neural network, trained with
object condensation, a graph segmentation technique. Through a single-shot
approach, the reconstruction task is paired with energy regression. We describe
the reconstruction performance in terms of efficiency as well as in terms of
energy resolution. In addition, we show the jet reconstruction performance of
our method and discuss its inference computational cost. To our knowledge, this
work is the first-ever example of single-shot calorimetric reconstruction of
${\cal O}(1000)$ particles in high-luminosity conditions with 200 pileup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO. (arXiv:2204.03359v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03359">
<div class="article-summary-box-inner">
<span><p>Image-Text matching (ITM) is a common task for evaluating the quality of
Vision and Language (VL) models. However, existing ITM benchmarks have a
significant limitation. They have many missing correspondences, originating
from the data construction process itself. For example, a caption is only
matched with one image although the caption can be matched with other similar
images, and vice versa. To correct the massive false negatives, we construct
the Extended COCO Validation (ECCV) Caption dataset by supplying the missing
associations with machine and human annotators. We employ five state-of-the-art
ITM models with diverse properties for our annotation process. Our dataset
provides x3.6 positive image-to-caption associations and x8.5 caption-to-image
associations compared to the original MS-COCO. We also propose to use an
informative ranking-based metric, rather than the popular Recall@K(R@K). We
re-evaluate the existing 25 VL models on existing and proposed benchmarks. Our
findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K R@K,
CxC R@1 are highly correlated with each other, while the rankings change when
we shift to the ECCV mAP. Lastly, we delve into the effect of the bias
introduced by the choice of machine annotator. Source code and dataset are
available at https://github.com/naver-ai/eccv-caption
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HunYuan_tvr for Text-Video Retrivial. (arXiv:2204.03382v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03382">
<div class="article-summary-box-inner">
<span><p>Text-Video Retrieval plays an important role in multi-modal understanding and
has attracted increasing attention in recent years. Most existing methods focus
on constructing contrastive pairs between whole videos and complete caption
sentences, while ignoring fine-grained cross-modal relationships, e.g., short
clips and phrases or single frame and word. In this paper, we propose a novel
method, named HunYuan\_tvr, to explore hierarchical cross-modal interactions by
simultaneously exploring video-sentence, clip-phrase, and frame-word
relationships. Considering intrinsic semantic relations between frames,
HunYuan\_tvr first performs self-attention to explore frame-wise correlations
and adaptively clusters correlated frames into clip-level representations.
Then, the clip-wise correlation is explored to aggregate clip representations
into a compact one to describe the video globally. In this way, we can
construct hierarchical video representations for frame-clip-video
granularities, and also explore word-wise correlations to form
word-phrase-sentence embeddings for the text modality. Finally, hierarchical
contrastive learning is designed to explore cross-modal
relationships,~\emph{i.e.,} frame-word, clip-phrase, and video-sentence, which
enables HunYuan\_tvr to achieve a comprehensive multi-modal understanding.
Further boosted by adaptive label denosing and marginal sample enhancement,
HunYuan\_tvr obtains new state-of-the-art results on various benchmarks, e.g.,
Rank@1 of 55.0%, 57.8%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC,
DiDemo, and ActivityNet respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmenting across places: The need for fair transfer learning with satellite imagery. (arXiv:2204.04358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04358">
<div class="article-summary-box-inner">
<span><p>The increasing availability of high-resolution satellite imagery has enabled
the use of machine learning to support land-cover measurement and inform
policy-making. However, labelling satellite images is expensive and is
available for only some locations. This prompts the use of transfer learning to
adapt models from data-rich locations to others. Given the potential for
high-impact applications of satellite imagery across geographies, a systematic
assessment of transfer learning implications is warranted. In this work, we
consider the task of land-cover segmentation and study the fairness
implications of transferring models across locations. We leverage a large
satellite image segmentation benchmark with 5987 images from 18 districts (9
urban and 9 rural). Via fairness metrics we quantify disparities in model
performance along two axes -- across urban-rural locations and across
land-cover classes. Findings show that state-of-the-art models have better
overall accuracy in rural areas compared to urban areas, through unsupervised
domain adaptation methods transfer learning better to urban versus rural areas
and enlarge fairness gaps. In analysis of reasons for these findings, we show
that raw satellite images are overall more dissimilar between source and target
districts for rural than for urban locations. This work highlights the need to
conduct fairness analysis for satellite imagery segmentation models and
motivates the development of methods for fair transfer learning in order not to
introduce disparities between places, particularly urban and rural locations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Graph Variational Autoencoders for Indoor Furniture layout Generation. (arXiv:2204.04867v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04867">
<div class="article-summary-box-inner">
<span><p>We present a structured graph variational autoencoder for generating the
layout of indoor 3D scenes. Given the room type (e.g., living room or library)
and the room layout (e.g., room elements such as floor and walls), our
architecture generates a collection of objects (e.g., furniture items such as
sofa, table and chairs) that is consistent with the room type and layout. This
is a challenging problem because the generated scene should satisfy multiple
constrains, e.g., each object must lie inside the room and two objects cannot
occupy the same volume. To address these challenges, we propose a deep
generative model that encodes these relationships as soft constraints on an
attributed graph (e.g., the nodes capture attributes of room and furniture
elements, such as class, pose and size, and the edges capture geometric
relationships such as relative orientation). The architecture consists of a
graph encoder that maps the input graph to a structured latent space, and a
graph decoder that generates a furniture graph, given a latent code and the
room graph. The latent space is modeled with auto-regressive priors, which
facilitates the generation of highly structured scenes. We also propose an
efficient training procedure that combines matching and constrained learning.
Experiments on the 3D-FRONT dataset show that our method produces scenes that
are diverse and are adapted to the room layout.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning State-of-the-Art with Uncertainties. (arXiv:2204.05173v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05173">
<div class="article-summary-box-inner">
<span><p>With the availability of data, hardware, software ecosystem and relevant
skill sets, the machine learning community is undergoing a rapid development
with new architectures and approaches appearing at high frequency every year.
In this article, we conduct an exemplary image classification study in order to
demonstrate how confidence intervals around accuracy measurements can greatly
enhance the communication of research results as well as impact the reviewing
process. In addition, we explore the hallmarks and limitations of this
approximation. We discuss the relevance of this approach reflecting on a
spotlight publication of ICLR22. A reproducible workflow is made available as
an open-source adjoint to this publication. Based on our discussion, we make
suggestions for improving the authoring and reviewing process of machine
learning articles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViViD++: Vision for Visibility Dataset. (arXiv:2204.06183v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06183">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a dataset capturing diverse visual data formats
that target varying luminance conditions. While RGB cameras provide nourishing
and intuitive information, changes in lighting conditions potentially result in
catastrophic failure for robotic applications based on vision sensors.
Approaches overcoming illumination problems have included developing more
robust algorithms or other types of visual sensors, such as thermal and event
cameras. Despite the alternative sensors' potential, there still are few
datasets with alternative vision sensors. Thus, we provided a dataset recorded
from alternative vision sensors, by handheld or mounted on a car, repeatedly in
the same space but in different conditions. We aim to acquire visible
information from co-aligned alternative vision sensors. Our sensor system
collects data more independently from visible light intensity by measuring the
amount of infrared dissipation, depth by structured reflection, and
instantaneous temporal changes in luminance. We provide these measurements
along with inertial sensors and ground-truth for developing robust visual SLAM
under poor illumination. The full dataset is available at:
https://visibilitydataset.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WSSS4LUAD: Grand Challenge on Weakly-supervised Tissue Semantic Segmentation for Lung Adenocarcinoma. (arXiv:2204.06455v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06455">
<div class="article-summary-box-inner">
<span><p>Lung cancer is the leading cause of cancer death worldwide, and
adenocarcinoma (LUAD) is the most common subtype. Exploiting the potential
value of the histopathology images can promote precision medicine in oncology.
Tissue segmentation is the basic upstream task of histopathology image
analysis. Existing deep learning models have achieved superior segmentation
performance but require sufficient pixel-level annotations, which is
time-consuming and expensive. To enrich the label resources of LUAD and to
alleviate the annotation efforts, we organize this challenge WSSS4LUAD to call
for the outstanding weakly-supervised semantic segmentation (WSSS) techniques
for histopathology images of LUAD. Participants have to design the algorithm to
segment tumor epithelial, tumor-associated stroma and normal tissue with only
patch-level labels. This challenge includes 10,091 patch-level annotations (the
training set) and over 130 million labeled pixels (the validation and test
sets), from 87 WSIs (67 from GDPH, 20 from TCGA). All the labels were generated
by a pathologist-in-the-loop pipeline with the help of AI models and checked by
the label review board. Among 532 registrations, 28 teams submitted the results
in the test phase with over 1,000 submissions. Finally, the first place team
achieved mIoU of 0.8413 (tumor: 0.8389, stroma: 0.7931, normal: 0.8919).
According to the technical reports of the top-tier teams, CAM is still the most
popular approach in WSSS. Cutmix data augmentation has been widely adopted to
generate more reliable samples. With the success of this challenge, we believe
that WSSS approaches with patch-level annotations can be a complement to the
traditional pixel annotations while reducing the annotation efforts. The entire
dataset has been released to encourage more researches on computational
pathology in LUAD and more novel WSSS techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Set Recognition: a Good Closed-Set Classifier is All You Need?. (arXiv:2110.06207v2 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06207">
<div class="article-summary-box-inner">
<span><p>The ability to identify whether or not a test sample belongs to one of the
semantic classes in a classifier's training set is critical to practical
deployment of the model. This task is termed open-set recognition (OSR) and has
received significant attention in recent years. In this paper, we first
demonstrate that the ability of a classifier to make the 'none-of-above'
decision is highly correlated with its accuracy on the closed-set classes. We
find that this relationship holds across loss objectives and architectures, and
further demonstrate the trend both on the standard OSR benchmarks as well as on
a large-scale ImageNet evaluation. Second, we use this correlation to boost the
performance of a maximum logit score OSR 'baseline' by improving its closed-set
accuracy, and with this strong baseline achieve state-of-the-art on a number of
OSR benchmarks. Similarly, we boost the performance of the existing
state-of-the-art method by improving its closed-set accuracy, but the resulting
discrepancy with the strong baseline is marginal. Our third contribution is to
present the 'Semantic Shift Benchmark' (SSB), which better respects the task of
detecting semantic novelty, in contrast to other forms of distribution shift
also considered in related sub-fields, such as out-of-distribution detection.
On this new evaluation, we again demonstrate that there is negligible
difference between the strong baseline and the existing state-of-the-art.
Project Page: https://www.robots.ox.ac.uk/~vgg/research/osr/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization. (arXiv:2204.06504v1 [cs.GR] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06504">
<div class="article-summary-box-inner">
<span><p>Since 2016, we have witnessed the tremendous growth of artificial
intelligence+visualization (AI+VIS) research. However, existing survey papers
on AI+VIS focus on visual analytics and information visualization, not
scientific visualization (SciVis). In this paper, we survey related deep
learning (DL) works in SciVis, specifically in the direction of DL4SciVis:
designing DL solutions for solving SciVis problems. To stay focused, we
primarily consider works that handle scalar and vector field data but exclude
mesh data. We classify and discuss these works along six dimensions: domain
setting, research task, learning type, network architecture, loss function, and
evaluation metric. The paper concludes with a discussion of the remaining gaps
to fill along the discussed dimensions and the grand challenges we need to
tackle as a community. This state-of-the-art survey guides SciVis researchers
in gaining an overview of this emerging topic and points out future directions
to grow this research.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-16 23:07:58.301001977 UTC">2022-04-16 23:07:58 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>