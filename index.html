<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-11-04T01:30:00Z">11-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Keyphrase Completion. (arXiv:2111.01910v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01910">
<div class="article-summary-box-inner">
<span><p>Keyphrase provides accurate information of document content that is highly
compact, concise, full of meanings, and widely used for discourse
comprehension, organization, and text retrieval. Though previous studies have
made substantial efforts for automated keyphrase extraction and generation,
surprisingly, few studies have been made for \textit{keyphrase completion}
(KPC). KPC aims to generate more keyphrases for document (e.g. scientific
publication) taking advantage of document content along with a very limited
number of known keyphrases, which can be applied to improve text indexing
system, etc. In this paper, we propose a novel KPC method with an
encoder-decoder framework. We name it \textit{deep keyphrase completion} (DKPC)
since it attempts to capture the deep semantic meaning of the document content
together with known keyphrases via a deep learning framework. Specifically, the
encoder and the decoder in DKPC play different roles to make full use of the
known keyphrases. The former considers the keyphrase-guiding factors, which
aggregates information of known keyphrases into context. On the contrary, the
latter considers the keyphrase-inhibited factor to inhibit semantically
repeated keyphrase generation. Extensive experiments on benchmark datasets
demonstrate the efficacy of our proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Advantages of Interactive and Non-Interactive Models for Vector-Based Cross-Lingual Information Retrieval. (arXiv:2111.01992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01992">
<div class="article-summary-box-inner">
<span><p>Interactive and non-interactive model are the two de-facto standard
frameworks in vector-based cross-lingual information retrieval (V-CLIR), which
embed queries and documents in synchronous and asynchronous fashions,
respectively. From the retrieval accuracy and computational efficiency
perspectives, each model has its own superiority and shortcoming. In this
paper, we propose a novel framework to leverage the advantages of these two
paradigms. Concretely, we introduce semi-interactive mechanism, which builds
our model upon non-interactive architecture but encodes each document together
with its associated multilingual queries. Accordingly, cross-lingual features
can be better learned like an interactive model. Besides, we further transfer
knowledge from a well-trained interactive model to ours by reusing its word
embeddings and adopting knowledge distillation. Our model is initialized from a
multilingual pre-trained language model M-BERT, and evaluated on two
open-resource CLIR datasets derived from Wikipedia and an in-house dataset
collected from a real-world search engine. Extensive analyses reveal that our
methods significantly boost the retrieval accuracy while maintaining the
computational efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenPrompt: An Open-source Framework for Prompt-learning. (arXiv:2111.01998v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01998">
<div class="article-summary-box-inner">
<span><p>Prompt-learning has become a new paradigm in modern natural language
processing, which directly adapts pre-trained language models (PLMs) to
$cloze$-style prediction, autoregressive modeling, or sequence to sequence
generation, resulting in promising performances on various tasks. However, no
standard implementation framework of prompt-learning is proposed yet, and most
existing prompt-learning codebases, often unregulated, only provide limited
implementations for specific scenarios. Since there are many details such as
templating strategy, initializing strategy, and verbalizing strategy, etc. need
to be considered in prompt-learning, practitioners face impediments to quickly
adapting the desired prompt learning methods to their applications. In this
paper, we present {OpenPrompt}, a unified easy-to-use toolkit to conduct
prompt-learning over PLMs. OpenPrompt is a research-friendly framework that is
equipped with efficiency, modularity, and extendibility, and its combinability
allows the freedom to combine different PLMs, task formats, and prompting
modules in a unified paradigm. Users could expediently deploy prompt-learning
frameworks and evaluate the generalization of them on different NLP tasks
without constraints. OpenPrompt is publicly released at {\url{
https://github.com/thunlp/OpenPrompt}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Speaker Role Identification in Air Traffic Communication Using Deep Learning Approaches. (arXiv:2111.02041v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02041">
<div class="article-summary-box-inner">
<span><p>Automatic spoken instruction understanding (SIU) of the controller-pilot
conversations in the air traffic control (ATC) requires not only recognizing
the words and semantics of the speech but also determining the role of the
speaker. However, few of the published works on the automatic understanding
systems in air traffic communication focus on speaker role identification
(SRI). In this paper, we formulate the SRI task of controller-pilot
communication as a binary classification problem. Furthermore, the text-based,
speech-based, and speech and text based multi-modal methods are proposed to
achieve a comprehensive comparison of the SRI task. To ablate the impacts of
the comparative approaches, various advanced neural network architectures are
applied to optimize the implementation of text-based and speech-based methods.
Most importantly, a multi-modal speaker role identification network (MMSRINet)
is designed to achieve the SRI task by considering both the speech and textual
modality features. To aggregate modality features, the modal fusion module is
proposed to fuse and squeeze acoustic and textual representations by modal
attention mechanism and self-attention pooling layer, respectively. Finally,
the comparative approaches are validated on the ATCSpeech corpus collected from
a real-world ATC environment. The experimental results demonstrate that all the
comparative approaches are worked for the SRI task, and the proposed MMSRINet
shows the competitive performance and robustness than the other methods on both
seen and unseen data, achieving 98.56%, and 98.08% accuracy, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02080">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models such as GPT-3 have the surprising ability to
do in-context learning, where the model learns to do a downstream task simply
by conditioning on a prompt consisting of input-output examples. Without being
explicitly pretrained to do so, the language model learns from these examples
during its forward pass without parameter updates on "out-of-distribution"
prompts. Thus, it is unclear what mechanism enables in-context learning. In
this paper, we study the role of the pretraining distribution on the emergence
of in-context learning under a mathematical setting where the pretraining texts
have long-range coherence. Here, language model pretraining requires inferring
a latent document-level concept from the conditioning text to generate coherent
next tokens. At test time, this mechanism enables in-context learning by
inferring the shared latent concept between prompt examples and applying it to
make a prediction on the test example. Concretely, we prove that in-context
learning occurs implicitly via Bayesian inference of the latent concept when
the pretraining distribution is a mixture of HMMs. This can occur despite the
distribution mismatch between prompts and pretraining data. In contrast to
messy large-scale pretraining datasets for in-context learning in natural
language, we generate a family of small-scale synthetic datasets (GINC) where
Transformer and LSTM language models both exhibit in-context learning. Beyond
the theory which focuses on the effect of the pretraining distribution, we
empirically find that scaling model size improves in-context accuracy even when
the pretraining loss is the same.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task. (arXiv:2111.02086v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02086">
<div class="article-summary-box-inner">
<span><p>This report describes Microsoft's machine translation systems for the WMT21
shared task on large-scale multilingual machine translation. We participated in
all three evaluation tracks including Large Track and two Small Tracks where
the former one is unconstrained and the latter two are fully constrained. Our
model submissions to the shared task were initialized with
DeltaLM\footnote{\url{https://aka.ms/deltalm}}, a generic pre-trained
multilingual encoder-decoder model, and fine-tuned correspondingly with the
vast collected parallel data and allowed data sources according to track
settings, together with applying progressive learning and iterative
back-translation approaches to further improve the performance. Our final
submissions ranked first on three tracks in terms of the automatic evaluation
metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Evaluation and Moderation of Open-domain Dialogue Systems. (arXiv:2111.02110v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02110">
<div class="article-summary-box-inner">
<span><p>In recent years, dialogue systems have attracted significant interests in
both academia and industry. Especially the discipline of open-domain dialogue
systems, aka chatbots, has gained great momentum. Yet, a long standing
challenge that bothers the researchers is the lack of effective automatic
evaluation metrics, which results in significant impediment in the current
research. Common practice in assessing the performance of open-domain dialogue
models involves extensive human evaluation on the final deployed models, which
is both time- and cost- intensive. Moreover, a recent trend in building
open-domain chatbots involve pre-training dialogue models with a large amount
of social media conversation data. However, the information contained in the
social media conversations may be offensive and inappropriate. Indiscriminate
usage of such data can result in insensitive and toxic generative models. This
paper describes the data, baselines and results obtained for the Track 5 at the
Dialogue System Technology Challenge 10 (DSTC10).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. (arXiv:2111.02114v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02114">
<div class="article-summary-box-inner">
<span><p>Multi-modal language-vision models trained on hundreds of millions of
image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable
capability to perform zero- or few-shot learning and transfer even in absence
of per-sample labels on target image data. Despite this trend, to date there
has been no publicly available datasets of sufficient scale for training such
models from scratch. To address this issue, in a community effort we build and
release for public LAION-400M, a dataset with CLIP-filtered 400 million
image-text pairs, their CLIP embeddings and kNN indices that allow efficient
similarity search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lingua Custodia's participation at the WMT 2021 Machine Translation using Terminologies shared task. (arXiv:2111.02120v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02120">
<div class="article-summary-box-inner">
<span><p>This paper describes Lingua Custodia's submission to the WMT21 shared task on
machine translation using terminologies. We consider three directions, namely
English to French, Russian, and Chinese. We rely on a Transformer-based
architecture as a building block, and we explore a method which introduces two
main changes to the standard procedure to handle terminologies. The first one
consists in augmenting the training data in such a way as to encourage the
model to learn a copy behavior when it encounters terminology constraint terms.
The second change is constraint token masking, whose purpose is to ease copy
behavior learning and to improve model generalization. Empirical results show
that our method satisfies most terminology constraints while maintaining high
translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Klarna Product Page Dataset: A RealisticBenchmark for Web Representation Learning. (arXiv:2111.02168v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02168">
<div class="article-summary-box-inner">
<span><p>This paper tackles the under-explored problem of DOM tree element
representation learning. We advance the field of machine learning-based web
automation and hope to spur further research regarding this crucial area with
two contributions. First, we adapt several popular Graph-based Neural Network
models and apply them to embed elements in website DOM trees. Second, we
present a large-scale and realistic dataset of webpages. By providing this
open-access resource, we lower the entry barrier to this area of research. The
dataset contains $51,701$ manually labeled product pages from $8,175$ real
e-commerce websites. The pages can be rendered entirely in a web browser and
are suitable for computer vision applications. This makes it substantially
richer and more diverse than other datasets proposed for element representation
learning, classification and prediction on the web. Finally, using our proposed
dataset, we show that the embeddings produced by a Graph Convolutional Neural
Network outperform representations produced by other state-of-the-art methods
in a web element prediction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition. (arXiv:2111.02172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02172">
<div class="article-summary-box-inner">
<span><p>The audio-video based multimodal emotion recognition has attracted a lot of
attention due to its robust performance. Most of the existing methods focus on
proposing different cross-modal fusion strategies. However, these strategies
introduce redundancy in the features of different modalities without fully
considering the complementary properties between modal information, and these
approaches do not guarantee the non-loss of original semantic information
during intra- and inter-modal interactions. In this paper, we propose a novel
cross-modal fusion network based on self-attention and residual structure
(CFN-SR) for multimodal emotion recognition. Firstly, we perform representation
learning for audio and video modalities to obtain the semantic features of the
two modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed
the features of the two modalities into the cross-modal blocks separately to
ensure efficient complementarity and completeness of information through the
self-attention mechanism and residual structure. Finally, we obtain the output
of emotions by splicing the obtained fused representation with the original
representation. To verify the effectiveness of the proposed method, we conduct
experiments on the RAVDESS dataset. The experimental results show that the
proposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with
26.30M parameters. Our code is available at
https://github.com/skeletonNN/CFN-SR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-DRE: BERT with Deep Recursive Encoder for Natural Language Sentence Matching. (arXiv:2111.02188v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02188">
<div class="article-summary-box-inner">
<span><p>This paper presents a deep neural architecture, for Natural Language Sentence
Matching (NLSM) by adding a deep recursive encoder to BERT so called BERT with
Deep Recursive Encoder (BERT-DRE). Our analysis of model behavior shows that
BERT still does not capture the full complexity of text, so a deep recursive
encoder is applied on top of BERT. Three Bi-LSTM layers with residual
connection are used to design a recursive encoder and an attention module is
used on top of this encoder. To obtain the final vector, a pooling layer
consisting of average and maximum pooling is used. We experiment our model on
four benchmarks, SNLI, FarsTail, MultiNLI, SciTail, and a novel Persian
religious questions dataset. This paper focuses on improving the BERT results
in the NLSM task. In this regard, comparisons between BERT-DRE and BERT are
conducted, and it is shown that in all cases, BERT-DRE outperforms only BERT.
The BERT algorithm on the religious dataset achieved an accuracy of 89.70%, and
BERT-DRE architectures improved to 90.29% using the same dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Implicit Sentiment in Aspect-based Sentiment Analysis with Supervised Contrastive Pre-Training. (arXiv:2111.02194v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02194">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis aims to identify the sentiment polarity of a
specific aspect in product reviews. We notice that about 30% of reviews do not
contain obvious opinion words, but still convey clear human-aware sentiment
orientation, which is known as implicit sentiment. However, recent neural
network-based approaches paid little attention to implicit sentiment entailed
in the reviews. To overcome this issue, we adopt Supervised Contrastive
Pre-training on large-scale sentiment-annotated corpora retrieved from
in-domain language resources. By aligning the representation of implicit
sentiment expressions to those with the same sentiment label, the pre-training
process leads to better capture of both implicit and explicit sentiment
orientation towards aspects in reviews. Experimental results show that our
method achieves state-of-the-art performance on SemEval2014 benchmarks, and
comprehensive analysis validates its effectiveness on learning implicit
sentiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Embedding of Stories Into Collections of Independent Media. (arXiv:2111.02216v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02216">
<div class="article-summary-box-inner">
<span><p>We look at how machine learning techniques that derive properties of items in
a collection of independent media can be used to automatically embed stories
into such collections. To do so, we use models that extract the tempo of songs
to make a music playlist follow a narrative arc. Our work specifies an
open-source tool that uses pre-trained neural network models to extract the
global tempo of a set of raw audio files and applies these measures to create a
narrative-following playlist. This tool is available at
https://github.com/dylanashley/playlist-story-builder/releases/tag/v1.0.0
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Case Study and Qualitative Analysis of Simple Cross-Lingual Opinion Mining. (arXiv:2111.02259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02259">
<div class="article-summary-box-inner">
<span><p>User-generated content from social media is produced in many languages,
making it technically challenging to compare the discussed themes from one
domain across different cultures and regions. It is relevant for domains in a
globalized world, such as market research, where people from two nations and
markets might have different requirements for a product. We propose a simple,
modern, and effective method for building a single topic model with sentiment
analysis capable of covering multiple languages simultanteously, based on a
pre-trained state-of-the-art deep neural network for natural language
understanding. To demonstrate its feasibility, we apply the model to newspaper
articles and user comments of a specific domain, i.e., organic food products
and related consumption behavior. The themes match across languages.
Additionally, we obtain an high proportion of stable and domain-relevant
topics, a meaningful relation between topics and their respective textual
contents, and an interpretable representation for social media documents.
Marketing can potentially benefit from our method, since it provides an
easy-to-use means of addressing specific customer interests from different
market regions around the globe. For reproducibility, we provide the code,
data, and results of our study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SERC: Syntactic and Semantic Sequence based Event Relation Classification. (arXiv:2111.02265v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02265">
<div class="article-summary-box-inner">
<span><p>Temporal and causal relations play an important role in determining the
dependencies between events. Classifying the temporal and causal relations
between events has many applications, such as generating event timelines, event
summarization, textual entailment and question answering. Temporal and causal
relations are closely related and influence each other. So we propose a joint
model that incorporates both temporal and causal features to perform causal
relation classification. We use the syntactic structure of the text for
identifying temporal and causal relations between two events from the text. We
extract parts-of-speech tag sequence, dependency tag sequence and word sequence
from the text. We propose an LSTM based model for temporal and causal relation
classification that captures the interrelations between the three encoded
features. Evaluation of our model on four popular datasets yields promising
results for temporal and causal relation classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Annotator Bias Approximation on Crowdsourced Single-Label Sentiment Analysis. (arXiv:2111.02326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02326">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is often a crowdsourcing task prone to subjective labels
given by many annotators. It is not yet fully understood how the annotation
bias of each annotator can be modeled correctly with state-of-the-art methods.
However, resolving annotator bias precisely and reliably is the key to
understand annotators' labeling behavior and to successfully resolve
corresponding individual misconceptions and wrongdoings regarding the
annotation task. Our contribution is an explanation and improvement for precise
neural end-to-end bias modeling and ground truth estimation, which reduces an
undesired mismatch in that regard of the existing state-of-the-art.
Classification experiments show that it has potential to improve accuracy in
cases where each sample is annotated only by one single annotator. We provide
the whole source code publicly and release an own domain-specific sentiment
dataset containing 10,000 sentences discussing organic food products. These are
crawled from social media and are singly labeled by 10 non-expert annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. (arXiv:2111.02358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02358">
<div class="article-summary-box-inner">
<span><p>We present a unified Vision-Language pretrained Model (VLMo) that jointly
learns a dual encoder and a fusion encoder with a modular Transformer network.
Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,
where each block contains a pool of modality-specific experts and a shared
self-attention layer. Because of the modeling flexibility of MoME, pretrained
VLMo can be fine-tuned as a fusion encoder for vision-language classification
tasks, or used as a dual encoder for efficient image-text retrieval. Moreover,
we propose a stagewise pre-training strategy, which effectively leverages
large-scale image-only and text-only data besides image-text pairs.
Experimental results show that VLMo achieves state-of-the-art results on
various vision-language tasks, including VQA and NLVR2. The code and pretrained
models are available at https://aka.ms/vlmo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HmBlogs: A big general Persian corpus. (arXiv:2111.02362v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02362">
<div class="article-summary-box-inner">
<span><p>This paper introduces the hmBlogs corpus for Persian, as a low resource
language. This corpus has been prepared based on a collection of nearly 20
million blog posts over a period of about 15 years from a space of Persian
blogs and includes more than 6.8 billion tokens. It can be claimed that this
corpus is currently the largest Persian corpus that has been prepared
independently for the Persian language. This corpus is presented in both raw
and preprocessed forms, and based on the preprocessed corpus some word
embedding models are produced. By the provided models, the hmBlogs is compared
with some of the most important corpora available in Persian, and the results
show the superiority of the hmBlogs corpus over the others. These evaluations
also present the importance and effects of corpora, evaluation datasets, model
production methods, different hyperparameters and even the evaluation methods.
In addition to evaluating the corpus and its produced language models, this
research also presents a semantic analogy dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Training End-to-End Vision-and-Language Transformers. (arXiv:2111.02387v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02387">
<div class="article-summary-box-inner">
<span><p>Vision-and-language (VL) pre-training has proven to be highly effective on
various VL downstream tasks. While recent work has shown that fully
transformer-based VL models can be more efficient than previous
region-feature-based methods, their performance on downstream tasks are often
degraded significantly. In this paper, we present METER~(\textbf{M}ultimodal
\textbf{E}nd-to-end \textbf{T}ransform\textbf{ER}), through which we
systematically investigate how to design and pre-train a fully
transformer-based VL model in an end-to-end manner. Specifically, we dissect
the model designs along multiple dimensions: vision encoders (e.g., CLIP-ViT,
Swin transformer), text encoders (e.g., RoBERTa, DeBERTa), multimodal fusion
(e.g., merged attention vs. co-attention), architecture design (e.g.,
encoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked
image modeling). We conduct comprehensive experiments on a wide range of VL
tasks, and provide insights on how to train a performant VL transformer while
maintaining fast inference speed. Notably, METER~achieves an accuracy of
77.64\% on the VQAv2 test-std set using only 4M images for pre-training,
surpassing the state-of-the-art region-feature-based VinVL model by +1.04\%,
and outperforming the previous best fully transformer-based ALBEF model by
+1.6\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Landscape of Relational Syllogistic Logics. (arXiv:1809.00656v2 [math.LO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1809.00656">
<div class="article-summary-box-inner">
<span><p>This paper explores relational syllogistic logics, a family of logical
systems related to reasoning about relations in extensions of the classical
syllogistic. These are all decidable logical systems. We prove completeness
theorems and complexity results for a natural subfamily of relational
syllogistic logics, parametrized by constructors for terms and for sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-level Neural Network for Implicit Causality Detection in Web Texts. (arXiv:1908.07822v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.07822">
<div class="article-summary-box-inner">
<span><p>Mining causality from text is a complex and crucial natural language
understanding task corresponding to the human cognition. Existing studies at
its solution can be grouped into two primary categories: feature engineering
based and neural model based methods. In this paper, we find that the former
has incomplete coverage and inherent errors but provide prior knowledge; while
the latter leverages context information but causal inference of which is
insufficiency. To handle the limitations, we propose a novel causality
detection model named MCDN to explicitly model causal reasoning process, and
furthermore, to exploit the advantages of both methods. Specifically, we adopt
multi-head self-attention to acquire semantic feature at word level and develop
the SCRN to infer causality at segment level. To the best of our knowledge,
with regards to the causality tasks, this is the first time that the Relation
Network is applied. The experimental results show that: 1) the proposed
approach performs prominent performance on causality detection; 2) further
analysis manifests the effectiveness and robustness of MCDN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple and Effective Positional Encoding for Transformers. (arXiv:2104.08698v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08698">
<div class="article-summary-box-inner">
<span><p>Transformer models are permutation equivariant. To supply the order and type
information of the input tokens, position and segment embeddings are usually
added to the input. Recent works proposed variations of positional encodings
with relative position encodings achieving better performance. Our analysis
shows that the gain actually comes from moving positional information to
attention layer from the input. Motivated by this, we introduce Decoupled
Positional Attention for Transformers (DIET), a simple yet effective mechanism
to encode position and segment information into the Transformer models. The
proposed method has faster training and inference time, while achieving
competitive performance on GLUE, XTREME and WMT benchmarks. We further
generalize our method to long-range transformers and show performance gain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Luna: Linear Unified Nested Attention. (arXiv:2106.01540v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01540">
<div class="article-summary-box-inner">
<span><p>The quadratic computational and memory complexities of the Transformer's
attention mechanism have limited its scalability for modeling long sequences.
In this paper, we propose Luna, a linear unified nested attention mechanism
that approximates softmax attention with two nested linear attention functions,
yielding only linear (as opposed to quadratic) time and space complexity.
Specifically, with the first attention function, Luna packs the input sequence
into a sequence of fixed length. Then, the packed sequence is unpacked using
the second attention function. As compared to a more traditional attention
mechanism, Luna introduces an additional sequence with a fixed length as input
and an additional corresponding output, which allows Luna to perform attention
operation linearly, while also storing adequate contextual information. We
perform extensive evaluations on three benchmarks of sequence modeling tasks:
long-context sequence modeling, neural machine translation and masked language
modeling for large-scale pretraining. Competitive or even better experimental
results demonstrate both the effectiveness and efficiency of Luna compared to a
variety
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. (arXiv:2106.12566v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12566">
<div class="article-summary-box-inner">
<span><p>The attention module, which is a crucial component in Transformer, cannot
scale efficiently to long sequences due to its quadratic complexity. Many works
focus on approximating the dot-then-exponentiate softmax function in the
original attention, leading to sub-quadratic or even linear-complexity
Transformer architectures. However, we show that these methods cannot be
applied to more powerful attention modules that go beyond the
dot-then-exponentiate style, e.g., Transformers with relative positional
encoding (RPE). Since in many state-of-the-art models, relative positional
encoding is used as default, designing efficient Transformers that can
incorporate RPE is appealing. In this paper, we propose a novel way to
accelerate attention calculation for Transformers with RPE on top of the
kernelized attention. Based upon the observation that relative positional
encoding forms a Toeplitz matrix, we mathematically show that kernelized
attention with RPE can be calculated efficiently using Fast Fourier Transform
(FFT). With FFT, our method achieves $\mathcal{O}(n\log n)$ time complexity.
Interestingly, we further demonstrate that properly using relative positional
encoding can mitigate the training instability problem of vanilla kernelized
attention. On a wide range of tasks, we empirically show that our models can be
trained from scratch without any optimization issues. The learned model
performs better than many efficient Transformer variants and is faster than
standard Transformer in the long-sequence regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Detoxification using Large Pre-trained Neural Models. (arXiv:2109.08914v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08914">
<div class="article-summary-box-inner">
<span><p>We present two novel unsupervised methods for eliminating toxicity in text.
Our first method combines two recent ideas: (1) guidance of the generation
process with small style-conditional language models and (2) use of
paraphrasing models to perform style transfer. We use a well-performing
paraphraser guided by style-trained language models to keep the text content
and remove toxicity. Our second method uses BERT to replace toxic words with
their non-offensive synonyms. We make the method more flexible by enabling BERT
to replace mask tokens with a variable number of words. Finally, we present the
first large-scale comparative study of style transfer models on the task of
toxicity removal. We compare our models with a number of methods for style
transfer. The models are evaluated in a reference-free way using a combination
of unsupervised style transfer metrics. Both methods we suggest yield new SOTA
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social Media Reveals Urban-Rural Differences in Stress across China. (arXiv:2110.15726v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15726">
<div class="article-summary-box-inner">
<span><p>Modeling differential stress expressions in urban and rural regions in China
can provide a better understanding of the effects of urbanization on
psychological well-being in a country that has rapidly grown economically in
the last two decades. This paper studies linguistic differences in the
experiences and expressions of stress in urban-rural China from Weibo posts
from over 65,000 users across 329 counties using hierarchical mixed-effects
models. We analyzed phrases, topical themes, and psycho-linguistic word choices
in Weibo posts mentioning stress to better understand appraisal differences
surrounding psychological stress in urban and rural communities in China; we
then compared them with large-scale polls from Gallup. After controlling for
socioeconomic and gender differences, we found that rural communities tend to
express stress in emotional and personal themes such as relationships, health,
and opportunity while users in urban areas express stress using relative,
temporal, and external themes such as work, politics, and economics. These
differences exist beyond controlling for GDP and urbanization, indicating a
fundamentally different lifestyle between rural and urban residents in very
specific environments, arguably having different sources of stress. We found
corroborative trends in physical, financial, and social wellness with
urbanization in Gallup polls.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying causal associations in tweets using deep learning: Use case on diabetes-related tweets from 2017-2021. (arXiv:2111.01225v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01225">
<div class="article-summary-box-inner">
<span><p>Objective: Leveraging machine learning methods, we aim to extract both
explicit and implicit cause-effect associations in patient-reported,
diabetes-related tweets and provide a tool to better understand opinion,
feelings and observations shared within the diabetes online community from a
causality perspective. Materials and Methods: More than 30 million
diabetes-related tweets in English were collected between April 2017 and
January 2021. Deep learning and natural language processing methods were
applied to focus on tweets with personal and emotional content. A
cause-effect-tweet dataset was manually labeled and used to train 1) a
fine-tuned Bertweet model to detect causal sentences containing a causal
association 2) a CRF model with BERT based features to extract possible
cause-effect associations. Causes and effects were clustered in a
semi-supervised approach and visualised in an interactive cause-effect-network.
Results: Causal sentences were detected with a recall of 68% in an imbalanced
dataset. A CRF model with BERT based features outperformed a fine-tuned BERT
model for cause-effect detection with a macro recall of 68%. This led to 96,676
sentences with cause-effect associations. "Diabetes" was identified as the
central cluster followed by "Death" and "Insulin". Insulin pricing related
causes were frequently associated with "Death". Conclusions: A novel
methodology was developed to detect causal sentences and identify both explicit
and implicit, single and multi-word cause and corresponding effect as expressed
in diabetes-related tweets leveraging BERT-based architectures and visualised
as cause-effect-network. Extracting causal associations on real-life, patient
reported outcomes in social media data provides a useful complementary source
of information in diabetes research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Text-based Phishing Detection. (arXiv:2111.01676v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01676">
<div class="article-summary-box-inner">
<span><p>This paper reports on an experiment into text-based phishing detection using
readily available resources and without the use of semantics. The developed
algorithm is a modified version of previously published work that works with
the same tools. The results obtained in recognizing phishing emails are
considerably better than the previously reported work; but the rate of text
falsely identified as phishing is slightly worse. It is expected that adding
semantic component will reduce the false positive rate while preserving the
detection accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Classifier Training Efficiency for Automatic Cyberbullying Detection with Feature Density. (arXiv:2111.01689v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01689">
<div class="article-summary-box-inner">
<span><p>We study the effectiveness of Feature Density (FD) using different
linguistically-backed feature preprocessing methods in order to estimate
dataset complexity, which in turn is used to comparatively estimate the
potential performance of machine learning (ML) classifiers prior to any
training. We hypothesise that estimating dataset complexity allows for the
reduction of the number of required experiments iterations. This way we can
optimize the resource-intensive training of ML models which is becoming a
serious issue due to the increases in available dataset sizes and the ever
rising popularity of models based on Deep Neural Networks (DNN). The problem of
constantly increasing needs for more powerful computational resources is also
affecting the environment due to alarmingly-growing amount of CO2 emissions
caused by training of large-scale ML models. The research was conducted on
multiple datasets, including popular datasets, such as Yelp business review
dataset used for training typical sentiment analysis models, as well as more
recent datasets trying to tackle the problem of cyberbullying, which, being a
serious social problem, is also a much more sophisticated problem form the
point of view of linguistic representation. We use cyberbullying datasets
collected for multiple languages, namely English, Japanese and Polish. The
difference in linguistic complexity of datasets allows us to additionally
discuss the efficacy of linguistically-backed word preprocessing.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">3-D PET Image Generation with tumour masks using TGAN. (arXiv:2111.01866v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01866">
<div class="article-summary-box-inner">
<span><p>Training computer-vision related algorithms on medical images for disease
diagnosis or image segmentation is difficult due to the lack of training data,
labeled samples, and privacy concerns. For this reason, a robust generative
method to create synthetic data is highly sought after. However, most
three-dimensional image generators require additional image input or are
extremely memory intensive. To address these issues we propose adapting video
generation techniques for 3-D image generation. Using the temporal GAN (TGAN)
architecture, we show we are able to generate realistic head and neck PET
images. We also show that by conditioning the generator on tumour masks, we are
able to control the geometry and location of the tumour in the generated
images. To test the utility of the synthetic images, we train a segmentation
model using the synthetic images. Synthetic images conditioned on real tumour
masks are automatically segmented, and the corresponding real images are also
segmented. We evaluate the segmentations using the Dice score and find the
segmentation algorithm performs similarly on both datasets (0.65 synthetic
data, 0.70 real data). Various radionomic features are then calculated over the
segmented tumour volumes for each data set. A comparison of the real and
synthetic feature distributions show that seven of eight feature distributions
had statistically insignificant differences (p&gt;0.05). Correlation coefficients
were also calculated between all radionomic features and it is shown that all
of the strong statistical correlations in the real data set are preserved in
the synthetic data set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Body Size and Depth Disambiguation in Multi-Person Reconstruction from Single Images. (arXiv:2111.01884v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01884">
<div class="article-summary-box-inner">
<span><p>We address the problem of multi-person 3D body pose and shape estimation from
a single image. While this problem can be addressed by applying single-person
approaches multiple times for the same scene, recent works have shown the
advantages of building upon deep architectures that simultaneously reason about
all people in the scene in a holistic manner by enforcing, e.g., depth order
constraints or minimizing interpenetration among reconstructed bodies. However,
existing approaches are still unable to capture the size variability of people
caused by the inherent body scale and depth ambiguity. In this work, we tackle
this challenge by devising a novel optimization scheme that learns the
appropriate body scale and relative camera pose, by enforcing the feet of all
people to remain on the ground floor. A thorough evaluation on MuPoTS-3D and
3DPW datasets demonstrates that our approach is able to robustly estimate the
body translation and shape of multiple people while retrieving their spatial
arrangement, consistently improving current state-of-the-art, especially in
scenes with people of very different heights
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A dataset for multi-sensor drone detection. (arXiv:2111.01888v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01888">
<div class="article-summary-box-inner">
<span><p>The use of small and remotely controlled unmanned aerial vehicles (UAVs), or
drones, has increased in recent years. This goes in parallel with misuse
episodes, with an evident threat to the safety of people or facilities. As a
result, the detection of UAV has also emerged as a research topic. Most studies
on drone detection fail to specify the type of acquisition device, the drone
type, the detection range, or the dataset. The lack of proper UAV detection
studies employing thermal infrared cameras is also an issue, despite its
success with other targets. Besides, we have not found any previous study that
addresses the detection task as a function of distance to the target. Sensor
fusion is indicated as an open research issue as well, although research in
this direction is scarce too. To counteract the mentioned issues and allow
fundamental studies with a common public benchmark, we contribute with an
annotated multi-sensor database for drone detection that includes infrared and
visible videos and audio files. The database includes three different drones,
of different sizes and other flying objects that can be mistakenly detected as
drones, such as birds, airplanes or helicopters. In addition to using several
different sensors, the number of classes is higher than in previous studies. To
allow studies as a function of the sensor-to-target distance, the dataset is
divided into three categories (Close, Medium, Distant) according to the
industry-standard Detect, Recognize and Identify (DRI) requirements, built on
the Johnson criteria. Given that the drones must be flown within visual range
due to regulations, the largest sensor-to-target distance for a drone is 200 m,
and acquisitions are made in daylight. The data has been obtained at three
airports in Sweden: Halmstad Airport (IATA code: HAD/ICAO code: ESMT),
Gothenburg City Airport (GSE/ESGP) and Malm\"o Airport (MMX/ESMS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A high performance fingerprint liveness detection method based on quality related features. (arXiv:2111.01898v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01898">
<div class="article-summary-box-inner">
<span><p>A new software-based liveness detection approach using a novel fingerprint
parameterization based on quality related features is proposed. The system is
tested on a highly challenging database comprising over 10,500 real and fake
images acquired with five sensors of different technologies and covering a wide
range of direct attack scenarios in terms of materials and procedures followed
to generate the gummy fingers. The proposed solution proves to be robust to the
multi-scenario dataset, and presents an overall rate of 90% correctly
classified samples. Furthermore, the liveness detection method presented has
the added advantage over previously studied techniques of needing just one
image from a finger to decide whether it is real or fake. This last
characteristic provides the method with very valuable features as it makes it
less intrusive, more user friendly, faster and reduces its implementation
costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning for identification and face, gender, expression recognition under constraints. (arXiv:2111.01930v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01930">
<div class="article-summary-box-inner">
<span><p>Biometric recognition based on the full face is an extensive research area.
However, using only partially visible faces, such as in the case of
veiled-persons, is a challenging task. Deep convolutional neural network (CNN)
is used in this work to extract the features from veiled-person face images. We
found that the sixth and the seventh fully connected layers, FC6 and FC7
respectively, in the structure of the VGG19 network provide robust features
with each of these two layers containing 4096 features. The main objective of
this work is to test the ability of deep learning based automated computer
system to identify not only persons, but also to perform recognition of gender,
age, and facial expressions such as eye smile. Our experimental results
indicate that we obtain high accuracy for all the tasks. The best recorded
accuracy values are up to 99.95% for identifying persons, 99.9% for gender
recognition, 99.9% for age recognition and 80.9% for facial expression (eye
smile) recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting spatio-temporal layouts for compositional action recognition. (arXiv:2111.01936v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01936">
<div class="article-summary-box-inner">
<span><p>Recognizing human actions is fundamentally a spatio-temporal reasoning
problem, and should be, at least to some extent, invariant to the appearance of
the human and the objects involved. Motivated by this hypothesis, in this work,
we take an object-centric approach to action recognition. Multiple works have
studied this setting before, yet it remains unclear (i) how well a carefully
crafted, spatio-temporal layout-based method can recognize human actions, and
(ii) how, and when, to fuse the information from layout and appearance-based
models. The main focus of this paper is compositional/few-shot action
recognition, where we advocate the usage of multi-head attention (proven to be
effective for spatial reasoning) over spatio-temporal layouts, i.e.,
configurations of object bounding boxes. We evaluate different schemes to
inject video appearance information to the system, and benchmark our approach
on background cluttered action recognition. On the Something-Else and Action
Genome datasets, we demonstrate (i) how to extend multi-head attention for
spatio-temporal layout-based action recognition, (ii) how to improve the
performance of appearance-based models by fusion with layout-based models,
(iii) that even on non-compositional background-cluttered video datasets, a
fusion between layout- and appearance-based models improves the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarially Perturbed Wavelet-based Morphed Face Generation. (arXiv:2111.01965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01965">
<div class="article-summary-box-inner">
<span><p>Morphing is the process of combining two or more subjects in an image in
order to create a new identity which contains features of both individuals.
Morphed images can fool Facial Recognition Systems (FRS) into falsely accepting
multiple people, leading to failures in national security. As morphed image
synthesis becomes easier, it is vital to expand the research community's
available data to help combat this dilemma. In this paper, we explore
combination of two methods for morphed image generation, those of geometric
transformation (warping and blending to create morphed images) and photometric
perturbation. We leverage both methods to generate high-quality adversarially
perturbed morphs from the FERET, FRGC, and FRLL datasets. The final images
retain high similarity to both input subjects while resulting in minimal
artifacts in the visual domain. Images are synthesized by fusing the wavelet
sub-bands from the two look-alike subjects, and then adversarially perturbed to
create highly convincing imagery to deceive both humans and deep morph
detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Glimpse Network: A Robust and Efficient Classification Architecture based on Recurrent Downsampled Attention. (arXiv:2111.02018v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02018">
<div class="article-summary-box-inner">
<span><p>Most feedforward convolutional neural networks spend roughly the same efforts
for each pixel. Yet human visual recognition is an interaction between eye
movements and spatial attention, which we will have several glimpses of an
object in different regions. Inspired by this observation, we propose an
end-to-end trainable Multi-Glimpse Network (MGNet) which aims to tackle the
challenges of high computation and the lack of robustness based on recurrent
downsampled attention mechanism. Specifically, MGNet sequentially selects
task-relevant regions of an image to focus on and then adaptively combines all
collected information for the final prediction. MGNet expresses strong
resistance against adversarial attacks and common corruptions with less
computation. Also, MGNet is inherently more interpretable as it explicitly
informs us where it focuses during each iteration. Our experiments on
ImageNet100 demonstrate the potential of recurrent downsampled attention
mechanisms to improve a single feedforward manner. For example, MGNet improves
4.76% accuracy on average in common corruptions with only 36.9% computational
cost. Moreover, while the baseline incurs an accuracy drop to 7.6%, MGNet
manages to maintain 44.2% accuracy in the same PGD attack strength with
ResNet-50 backbone. Our code is available at
https://github.com/siahuat0727/MGNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Advancements in Self-Supervised Paradigms for Visual Feature Representation. (arXiv:2111.02042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02042">
<div class="article-summary-box-inner">
<span><p>We witnessed a massive growth in the supervised learning paradigm in the past
decade. Supervised learning requires a large amount of labeled data to reach
state-of-the-art performance. However, labeling the samples requires a lot of
human annotation. To avoid the cost of labeling data, self-supervised methods
were proposed to make use of largely available unlabeled data. This study
conducts a comprehensive and insightful survey and analysis of recent
developments in the self-supervised paradigm for feature representation. In
this paper, we investigate the factors affecting the usefulness of
self-supervision under different settings. We present some of the key insights
concerning two different approaches in self-supervision, generative and
contrastive methods. We also investigate the limitations of supervised
adversarial training and how self-supervision can help overcome those
limitations. We then move on to discuss the limitations and challenges in
effectively using self-supervision for visual tasks. Finally, we highlight some
open problems and point out future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Categorical Difference and Related Brain Regions of the Attentional Blink Effect. (arXiv:2111.02044v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02044">
<div class="article-summary-box-inner">
<span><p>Attentional blink (AB) is a biological effect, showing that for 200 to 500ms
after paying attention to one visual target, it is difficult to notice another
target that appears next, and attentional blink magnitude (ABM) is a indicating
parameter to measure the degree of this effect. Researchers have shown that
different categories of images can access the consciousness of human mind
differently, and produce different ranges of ABM values. So in this paper, we
compare two different types of images, categorized as animal and object, by
predicting ABM values directly from image features extracted from convolutional
neural network (CNN), and indirectly from functional magnetic resonance imaging
(fMRI) data. First, for two sets of images, we separately extract their average
features from layers of Alexnet, a classic model of CNN, then input the
features into a trained linear regression model to predict ABM values, and we
find higher-level instead of lower-level image features determine the
categorical difference in AB effect, and mid-level image features predict ABM
values more correctly than low-level and high-level image features. Then we
employ fMRI data from different brain regions collected when the subjects
viewed 50 test images to predict ABM values, and conclude that brain regions
covering relatively broader areas, like LVC, HVC and VC, perform better than
other smaller brain regions, which means AB effect is more related to synthetic
impact of several visual brain regions than only one particular visual regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Point Set Resampling via Gradient Fields. (arXiv:2111.02045v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02045">
<div class="article-summary-box-inner">
<span><p>3D point clouds acquired by scanning real-world objects or scenes have found
a wide range of applications including immersive telepresence, autonomous
driving, surveillance, etc. They are often perturbed by noise or suffer from
low density, which obstructs downstream tasks such as surface reconstruction
and understanding. In this paper, we propose a novel paradigm of point set
resampling for restoration, which learns continuous gradient fields of point
clouds that converge points towards the underlying surface. In particular, we
represent a point cloud via its gradient field -- the gradient of the
log-probability density function, and enforce the gradient field to be
continuous, thus guaranteeing the continuity of the model for solvable
optimization. Based on the continuous gradient fields estimated via a proposed
neural network, resampling a point cloud amounts to performing gradient-based
Markov Chain Monte Carlo (MCMC) on the input noisy or sparse point cloud.
Further, we propose to introduce regularization into the gradient-based MCMC
during point cloud restoration, which essentially refines the intermediate
resampled point cloud iteratively and accommodates various priors in the
resampling process. Extensive experimental results demonstrate that the
proposed point set resampling achieves the state-of-the-art performance in
representative restoration tasks including point cloud denoising and
upsampling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Image Feature Biases Exhibited by Deep CNN Models. (arXiv:2111.02058v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02058">
<div class="article-summary-box-inner">
<span><p>In recent years, convolutional neural networks (CNNs) have been applied
successfully in many fields. However, such deep neural models are still
regarded as black box in most tasks. One of the fundamental issues underlying
this problem is understanding which features are most influential in image
recognition tasks and how they are processed by CNNs. It is widely accepted
that CNN models combine low-level features to form complex shapes until the
object can be readily classified, however, several recent studies have argued
that texture features are more important than other features. In this paper, we
assume that the importance of certain features varies depending on specific
tasks, i.e., specific tasks exhibit a feature bias. We designed two
classification tasks based on human intuition to train deep neural models to
identify anticipated biases. We devised experiments comprising many tasks to
test these biases for the ResNet and DenseNet models. From the results, we
conclude that (1) the combined effect of certain features is typically far more
influential than any single feature; (2) in different tasks, neural models can
perform different biases, that is, we can design a specific task to make a
neural model biased toward a specific anticipated feature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep-Learning-Based Single-Image Height Reconstruction from Very-High-Resolution SAR Intensity Data. (arXiv:2111.02061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02061">
<div class="article-summary-box-inner">
<span><p>Originally developed in fields such as robotics and autonomous driving with
image-based navigation in mind, deep learning-based single-image depth
estimation (SIDE) has found great interest in the wider image analysis
community. Remote sensing is no exception, as the possibility to estimate
height maps from single aerial or satellite imagery bears great potential in
the context of topographic reconstruction. A few pioneering investigations have
demonstrated the general feasibility of single image height prediction from
optical remote sensing images and motivate further studies in that direction.
With this paper, we present the first-ever demonstration of deep learning-based
single image height prediction for the other important sensor modality in
remote sensing: synthetic aperture radar (SAR) data. Besides the adaptation of
a convolutional neural network (CNN) architecture for SAR intensity images, we
present a workflow for the generation of training data, and extensive
experimental results for different SAR imaging modes and test sites. Since we
put a particular emphasis on transferability, we are able to confirm that deep
learning-based single-image height estimation is not only possible, but also
transfers quite well to unseen data, even if acquired by different imaging
modes and imaging parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event and Activity Recognition in Video Surveillance for Cyber-Physical Systems. (arXiv:2111.02064v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02064">
<div class="article-summary-box-inner">
<span><p>This chapter aims to aid the development of Cyber-Physical Systems (CPS) in
automated understanding of events and activities in various applications of
video-surveillance. These events are mostly captured by drones, CCTVs or novice
and unskilled individuals on low-end devices. Being unconstrained, these videos
are immensely challenging due to a number of quality factors. We present an
extensive account of the various approaches taken to solve the problem over the
years. This ranges from methods as early as Structure from Motion (SFM) based
approaches to recent solution frameworks involving deep neural networks. We
show that the long-term motion patterns alone play a pivotal role in the task
of recognizing an event. Consequently each video is significantly represented
by a fixed number of key-frames using a graph-based approach. Only the temporal
features are exploited using a hybrid Convolutional Neural Network (CNN) +
Recurrent Neural Network (RNN) architecture. The results we obtain are
encouraging as they outperform standard temporal CNNs and are at par with those
using spatial information along with motion cues. Further exploring multistream
models, we conceive a multi-tier fusion strategy for the spatial and temporal
wings of a network. A consolidated representation of the respective individual
prediction vectors on video and frame levels is obtained using a biased
conflation technique. The fusion strategy endows us with greater rise in
precision on each stage as compared to the state-of-the-art methods, and thus a
powerful consensus is achieved in classification. Results are recorded on four
benchmark datasets widely used in the domain of action recognition, namely CCV,
HMDB, UCF-101 and KCV. It is inferable that focusing on better classification
of the video sequences certainly leads to robust actuation of a system designed
for event surveillance and object cum activity tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Progressive Prototype Network for Generalized Zero-Shot Learning. (arXiv:2111.02073v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02073">
<div class="article-summary-box-inner">
<span><p>Generalized Zero-Shot Learning (GZSL) aims to recognize new categories with
auxiliary semantic information,e.g., category attributes. In this paper, we
handle the critical issue of domain shift problem, i.e., confusion between seen
and unseen categories, by progressively improving cross-domain transferability
and category discriminability of visual representations. Our approach, named
Dual Progressive Prototype Network (DPPN), constructs two types of prototypes
that record prototypical visual patterns for attributes and categories,
respectively. With attribute prototypes, DPPN alternately searches
attribute-related local regions and updates corresponding attribute prototypes
to progressively explore accurate attribute-region correspondence. This enables
DPPN to produce visual representations with accurate attribute localization
ability, which benefits the semantic-visual alignment and representation
transferability. Besides, along with progressive attribute localization, DPPN
further projects category prototypes into multiple spaces to progressively
repel visual representations from different categories, which boosts category
discriminability. Both attribute and category prototypes are collaboratively
learned in a unified framework, which makes visual representations of DPPN
transferable and distinctive. Experiments on four benchmarks prove that DPPN
effectively alleviates the domain shift problem in GZSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceQvec: Vector Quality Assessment for Face Biometrics based on ISO Compliance. (arXiv:2111.02078v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02078">
<div class="article-summary-box-inner">
<span><p>In this paper we develop FaceQvec, a software component for estimating the
conformity of facial images with each of the points contemplated in the ISO/IEC
19794-5, a quality standard that defines general quality guidelines for face
images that would make them acceptable or unacceptable for use in official
documents such as passports or ID cards. This type of tool for quality
assessment can help to improve the accuracy of face recognition, as well as to
identify which factors are affecting the quality of a given face image and to
take actions to eliminate or reduce those factors, e.g., with postprocessing
techniques or re-acquisition of the image. FaceQvec consists of the automation
of 25 individual tests related to different points contemplated in the
aforementioned standard, as well as other characteristics of the images that
have been considered to be related to facial quality. We first include the
results of the quality tests evaluated on a development dataset captured under
realistic conditions. We used those results to adjust the decision threshold of
each test. Then we checked again their accuracy on a evaluation database that
contains new face images not seen during development. The evaluation results
demonstrate the accuracy of the individual tests for checking compliance with
ISO/IEC 19794-5. FaceQvec is available online
(https://github.com/uam-biometrics/FaceQvec).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Influence of image noise on crack detection performance of deep convolutional neural networks. (arXiv:2111.02079v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02079">
<div class="article-summary-box-inner">
<span><p>Development of deep learning techniques to analyse image data is an expansive
and emerging field. The benefits of tracking, identifying, measuring, and
sorting features of interest from image data has endless applications for
saving cost, time, and improving safety. Much research has been conducted on
classifying cracks from image data using deep convolutional neural networks;
however, minimal research has been conducted to study the efficacy of network
performance when noisy images are used. This paper will address the problem and
is dedicated to investigating the influence of image noise on network accuracy.
The methods used incorporate a benchmark image data set, which is purposely
deteriorated with two types of noise, followed by treatment with image
enhancement pre-processing techniques. These images, including their native
counterparts, are then used to train and validate two different networks to
study the differences in accuracy and performance. Results from this research
reveal that noisy images have a moderate to high impact on the network's
capability to accurately classify images despite the application of image
pre-processing. A new index has been developed for finding the most efficient
method for classification in terms of computation timing and accuracy.
Consequently, AlexNet was selected as the most efficient model based on the
proposed index.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. (arXiv:2111.02114v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02114">
<div class="article-summary-box-inner">
<span><p>Multi-modal language-vision models trained on hundreds of millions of
image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable
capability to perform zero- or few-shot learning and transfer even in absence
of per-sample labels on target image data. Despite this trend, to date there
has been no publicly available datasets of sufficient scale for training such
models from scratch. To address this issue, in a community effort we build and
release for public LAION-400M, a dataset with CLIP-filtered 400 million
image-text pairs, their CLIP embeddings and kNN indices that allow efficient
similarity search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient 3D Deep LiDAR Odometry. (arXiv:2111.02135v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02135">
<div class="article-summary-box-inner">
<span><p>An efficient 3D point cloud learning architecture, named PWCLO-Net, for LiDAR
odometry is first proposed in this paper. In this architecture, the
projection-aware representation of the 3D point cloud is proposed to organize
the raw 3D point cloud into an ordered data form to achieve efficiency. The
Pyramid, Warping, and Cost volume (PWC) structure for the LiDAR odometry task
is built to estimate and refine the pose in a coarse-to-fine approach
hierarchically and efficiently. A projection-aware attentive cost volume is
built to directly associate two discrete point clouds and obtain embedding
motion patterns. Then, a trainable embedding mask is proposed to weigh the
local motion patterns to regress the overall pose and filter outlier points.
The trainable pose warp-refinement module is iteratively used with embedding
mask optimized hierarchically to make the pose estimation more robust for
outliers. The entire architecture is holistically optimized end-to-end to
achieve adaptive learning of cost volume and mask, and all operations involving
point cloud sampling and grouping are accelerated by projection-aware 3D
feature learning methods. The superior performance and effectiveness of our
LiDAR odometry architecture are demonstrated on KITTI odometry dataset. Our
method outperforms all recent learning-based methods and even the
geometry-based approach, LOAM with mapping optimization, on most sequences of
KITTI odometry dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Entropy-guided Reinforced Partial Convolutional Network for Zero-Shot Learning. (arXiv:2111.02139v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02139">
<div class="article-summary-box-inner">
<span><p>Zero-Shot Learning (ZSL) aims to transfer learned knowledge from observed
classes to unseen classes via semantic correlations. A promising strategy is to
learn a global-local representation that incorporates global information with
extra localities (i.e., small parts/regions of inputs). However, existing
methods discover localities based on explicit features without digging into the
inherent properties and relationships among regions. In this work, we propose a
novel Entropy-guided Reinforced Partial Convolutional Network (ERPCNet), which
extracts and aggregates localities progressively based on semantic relevance
and visual correlations without human-annotated regions. ERPCNet uses
reinforced partial convolution and entropy guidance; it not only discovers
global-cooperative localities dynamically but also converges faster for policy
gradient optimization. We conduct extensive experiments to demonstrate
ERPCNet's performance through comparisons with state-of-the-art methods under
ZSL and Generalized Zero-Shot Learning (GZSL) settings on four benchmark
datasets. We also show ERPCNet is time efficient and explainable through
visualization analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond PRNU: Learning Robust Device-Specific Fingerprint for Source Camera Identification. (arXiv:2111.02144v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02144">
<div class="article-summary-box-inner">
<span><p>Source camera identification tools assist image forensic investigators to
associate an image in question with a suspect camera. Various techniques have
been developed based on the analysis of the subtle traces left in the images
during the acquisition. The Photo Response Non Uniformity (PRNU) noise pattern
caused by sensor imperfections has been proven to be an effective way to
identify the source camera. The existing literature suggests that the PRNU is
the only fingerprint that is device-specific and capable of identifying the
exact source device. However, the PRNU is susceptible to camera settings, image
content, image processing operations, and counter-forensic attacks. A forensic
investigator unaware of counter-forensic attacks or incidental image
manipulations is at the risk of getting misled. The spatial synchronization
requirement during the matching of two PRNUs also represents a major limitation
of the PRNU. In recent years, deep learning based approaches have been
successful in identifying source camera models. However, the identification of
individual cameras of the same model through these data-driven approaches
remains unsatisfactory. In this paper, we bring to light the existence of a new
robust data-driven device-specific fingerprint in digital images which is
capable of identifying the individual cameras of the same model. It is
discovered that the new device fingerprint is location-independent, stochastic,
and globally available, which resolve the spatial synchronization issue. Unlike
the PRNU, which resides in the high-frequency band, the new device fingerprint
is extracted from the low and mid-frequency bands, which resolves the fragility
issue that the PRNU is unable to contend with. Our experiments on various
datasets demonstrate that the new fingerprint is highly resilient to image
manipulations such as rotation, gamma correction, and aggressive JPEG
compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Klarna Product Page Dataset: A RealisticBenchmark for Web Representation Learning. (arXiv:2111.02168v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02168">
<div class="article-summary-box-inner">
<span><p>This paper tackles the under-explored problem of DOM tree element
representation learning. We advance the field of machine learning-based web
automation and hope to spur further research regarding this crucial area with
two contributions. First, we adapt several popular Graph-based Neural Network
models and apply them to embed elements in website DOM trees. Second, we
present a large-scale and realistic dataset of webpages. By providing this
open-access resource, we lower the entry barrier to this area of research. The
dataset contains $51,701$ manually labeled product pages from $8,175$ real
e-commerce websites. The pages can be rendered entirely in a web browser and
are suitable for computer vision applications. This makes it substantially
richer and more diverse than other datasets proposed for element representation
learning, classification and prediction on the web. Finally, using our proposed
dataset, we show that the embeddings produced by a Graph Convolutional Neural
Network outperform representations produced by other state-of-the-art methods
in a web element prediction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition. (arXiv:2111.02172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02172">
<div class="article-summary-box-inner">
<span><p>The audio-video based multimodal emotion recognition has attracted a lot of
attention due to its robust performance. Most of the existing methods focus on
proposing different cross-modal fusion strategies. However, these strategies
introduce redundancy in the features of different modalities without fully
considering the complementary properties between modal information, and these
approaches do not guarantee the non-loss of original semantic information
during intra- and inter-modal interactions. In this paper, we propose a novel
cross-modal fusion network based on self-attention and residual structure
(CFN-SR) for multimodal emotion recognition. Firstly, we perform representation
learning for audio and video modalities to obtain the semantic features of the
two modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed
the features of the two modalities into the cross-modal blocks separately to
ensure efficient complementarity and completeness of information through the
self-attention mechanism and residual structure. Finally, we obtain the output
of emotions by splicing the obtained fused representation with the original
representation. To verify the effectiveness of the proposed method, we conduct
experiments on the RAVDESS dataset. The experimental results show that the
proposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with
26.30M parameters. Our code is available at
https://github.com/skeletonNN/CFN-SR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminator Synthesis: On reusing the other half of Generative Adversarial Networks. (arXiv:2111.02175v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02175">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks have long since revolutionized the world of
computer vision and, tied to it, the world of art. Arduous efforts have gone
into fully utilizing and stabilizing training so that outputs of the Generator
network have the highest possible fidelity, but little has gone into using the
Discriminator after training is complete. In this work, we propose to use the
latter and show a way to use the features it has learned from the training
dataset to both alter an image and generate one from scratch. We name this
method Discriminator Dreaming, and the full code can be found at
https://github.com/PDillis/stylegan3-fun.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Image Compression for Machine Perception. (arXiv:2111.02249v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02249">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that learned image compression strategies can
outperform standard hand-crafted compression algorithms that have been
developed over decades of intensive research on the rate-distortion trade-off.
With growing applications of computer vision, high quality image reconstruction
from a compressible representation is often a secondary objective. Compression
that ensures high accuracy on computer vision tasks such as image segmentation,
classification, and detection therefore has the potential for significant
impact across a wide variety of settings. In this work, we develop a framework
that produces a compression format suitable for both human perception and
machine perception. We show that representations can be learned that
simultaneously optimize for compression and performance on core vision tasks.
Our approach allows models to be trained directly from compressed
representations, and this approach yields increased performance on new tasks
and in low-shot learning settings. We present results that improve upon
segmentation and detection performance compared to standard high quality JPGs,
but with representations that are four to ten times smaller in terms of bits
per pixel. Further, unlike naive compression methods, at a level ten times
smaller than standard JEPGs, segmentation and detection models trained from our
format suffer only minor degradation in performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Cue Adaptive Emotion Recognition Network. (arXiv:2111.02273v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02273">
<div class="article-summary-box-inner">
<span><p>Expressing and identifying emotions through facial and physical expressions
is a significant part of social interaction. Emotion recognition is an
essential task in computer vision due to its various applications and mainly
for allowing a more natural interaction between humans and machines. The common
approaches for emotion recognition focus on analyzing facial expressions and
requires the automatic localization of the face in the image. Although these
methods can correctly classify emotion in controlled scenarios, such techniques
are limited when dealing with unconstrained daily interactions. We propose a
new deep learning approach for emotion recognition based on adaptive multi-cues
that extract information from context and body poses, which humans commonly use
in social interaction and communication. We compare the proposed approach with
the state-of-art approaches in the CAER-S dataset, evaluating different
components in a pipeline that reached an accuracy of 89.30%
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparison of Deep Learning Models for the Prediction of Hand Hygiene Videos. (arXiv:2111.02322v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02322">
<div class="article-summary-box-inner">
<span><p>This paper presents a comparison of various deep learning models such as
Exception, Resnet-50, and Inception V3 for the classification and prediction of
hand hygiene gestures, which were recorded in accordance with the World Health
Organization (WHO) guidelines. The dataset consists of six hand hygiene
movements in a video format, gathered for 30 participants. The network consists
of pre-trained models with image net weights and a modified head of the model.
An accuracy of 37% (Xception model), 33% (Inception V3), and 72% (ResNet-50) is
achieved in the classification report after the training of the models for 25
epochs. ResNet-50 model clearly outperforms with correct class predictions. The
major speed limitation can be overcome with the use of fast processing GPU for
future work. A complete hand hygiene dataset along with other generic gestures
such as one-hand movements (linear hand motion; circular hand rotation) will be
tested with ResNet-50 architecture and the variants for health care workers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ML-PersRef: A Machine Learning-based Personalized Multimodal Fusion Approach for Referencing Outside Objects From a Moving Vehicle. (arXiv:2111.02327v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02327">
<div class="article-summary-box-inner">
<span><p>Over the past decades, the addition of hundreds of sensors to modern vehicles
has led to an exponential increase in their capabilities. This allows for novel
approaches to interaction with the vehicle that go beyond traditional
touch-based and voice command approaches, such as emotion recognition, head
rotation, eye gaze, and pointing gestures. Although gaze and pointing gestures
have been used before for referencing objects inside and outside vehicles, the
multimodal interaction and fusion of these gestures have so far not been
extensively studied. We propose a novel learning-based multimodal fusion
approach for referencing outside-the-vehicle objects while maintaining a long
driving route in a simulated environment. The proposed multimodal approaches
outperform single-modality approaches in multiple aspects and conditions.
Moreover, we also demonstrate possible ways to exploit behavioral differences
between users when completing the referencing task to realize an adaptable
personalized system for each driver. We propose a personalization technique
based on the transfer-of-learning concept for exceedingly small data sizes to
enhance prediction and adapt to individualistic referencing behavior. Our code
is publicly available at https://github.com/amr-gomaa/ML-PersRef.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LTD: Low Temperature Distillation for Robust Adversarial Training. (arXiv:2111.02331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02331">
<div class="article-summary-box-inner">
<span><p>Adversarial training has been widely used to enhance the robustness of the
neural network models against adversarial attacks. However, there still a
notable gap between the nature accuracy and the robust accuracy. We found one
of the reasons is the commonly used labels, one-hot vectors, hinder the
learning process for image recognition. In this paper, we proposed a method,
called Low Temperature Distillation (LTD), which is based on the knowledge
distillation framework to generate the desired soft labels. Unlike the previous
work, LTD uses relatively low temperature in the teacher model, and employs
different, but fixed, temperatures for the teacher model and the student model.
Moreover, we have investigated the methods to synergize the use of nature data
and adversarial ones in LTD. Experimental results show that without extra
unlabeled data, the proposed method combined with the previous work can achieve
57.72\% and 30.36\% robust accuracy on CIFAR-10 and CIFAR-100 dataset
respectively, which is about 1.21\% improvement of the state-of-the-art methods
in average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HS3: Learning with Proper Task Complexity in Hierarchically Supervised Semantic Segmentation. (arXiv:2111.02333v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02333">
<div class="article-summary-box-inner">
<span><p>While deeply supervised networks are common in recent literature, they
typically impose the same learning objective on all transitional layers despite
their varying representation powers.
</p>
<p>In this paper, we propose Hierarchically Supervised Semantic Segmentation
(HS3), a training scheme that supervises intermediate layers in a segmentation
network to learn meaningful representations by varying task complexity. To
enforce a consistent performance vs. complexity trade-off throughout the
network, we derive various sets of class clusters to supervise each
transitional layer of the network. Furthermore, we devise a fusion framework,
HS3-Fuse, to aggregate the hierarchical features generated by these layers,
which can provide rich semantic contexts and further enhance the final
segmentation. Extensive experiments show that our proposed HS3 scheme
considerably outperforms vanilla deep supervision with no added inference cost.
Our proposed HS3-Fuse framework further improves segmentation predictions and
achieves state-of-the-art results on two large segmentation benchmarks: NYUD-v2
and Cityscapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. (arXiv:2111.02358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02358">
<div class="article-summary-box-inner">
<span><p>We present a unified Vision-Language pretrained Model (VLMo) that jointly
learns a dual encoder and a fusion encoder with a modular Transformer network.
Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,
where each block contains a pool of modality-specific experts and a shared
self-attention layer. Because of the modeling flexibility of MoME, pretrained
VLMo can be fine-tuned as a fusion encoder for vision-language classification
tasks, or used as a dual encoder for efficient image-text retrieval. Moreover,
we propose a stagewise pre-training strategy, which effectively leverages
large-scale image-only and text-only data besides image-text pairs.
Experimental results show that VLMo achieves state-of-the-art results on
various vision-language tasks, including VQA and NLVR2. The code and pretrained
models are available at https://aka.ms/vlmo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subpixel Heatmap Regression for Facial Landmark Localization. (arXiv:2111.02360v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02360">
<div class="article-summary-box-inner">
<span><p>Deep Learning models based on heatmap regression have revolutionized the task
of facial landmark localization with existing models working robustly under
large poses, non-uniform illumination and shadows, occlusions and
self-occlusions, low resolution and blur. However, despite their wide adoption,
heatmap regression approaches suffer from discretization-induced errors related
to both the heatmap encoding and decoding process. In this work we show that
these errors have a surprisingly large negative impact on facial alignment
accuracy. To alleviate this problem, we propose a new approach for the heatmap
encoding and decoding process by leveraging the underlying continuous
distribution. To take full advantage of the newly proposed encoding-decoding
mechanism, we also introduce a Siamese-based training that enforces heatmap
consistency across various geometric image transformations. Our approach offers
noticeable gains across multiple datasets setting a new state-of-the-art result
in facial landmark localization. Code alongside the pretrained models will be
made available at https://www.adrianbulat.com/face-alignment
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Salient Object Detection via Contrastive Features and Attention Modules. (arXiv:2111.02368v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02368">
<div class="article-summary-box-inner">
<span><p>Video salient object detection aims to find the most visually distinctive
objects in a video. To explore the temporal dependencies, existing methods
usually resort to recurrent neural networks or optical flow. However, these
approaches require high computational cost, and tend to accumulate inaccuracies
over time. In this paper, we propose a network with attention modules to learn
contrastive features for video salient object detection without the high
computational temporal modeling techniques. We develop a non-local
self-attention scheme to capture the global information in the video frame. A
co-attention formulation is utilized to combine the low-level and high-level
features. We further apply the contrastive learning to improve the feature
representations, where foreground region pairs from the same video are pulled
together, and foreground-background region pairs are pushed away in the latent
space. The intra-frame contrastive loss helps separate the foreground and
background features, and the inter-frame contrastive loss improves the temporal
consistency. We conduct extensive experiments on several benchmark datasets for
video salient object detection and unsupervised video object segmentation, and
show that the proposed method requires less computation, and performs favorably
against the state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Training End-to-End Vision-and-Language Transformers. (arXiv:2111.02387v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02387">
<div class="article-summary-box-inner">
<span><p>Vision-and-language (VL) pre-training has proven to be highly effective on
various VL downstream tasks. While recent work has shown that fully
transformer-based VL models can be more efficient than previous
region-feature-based methods, their performance on downstream tasks are often
degraded significantly. In this paper, we present METER~(\textbf{M}ultimodal
\textbf{E}nd-to-end \textbf{T}ransform\textbf{ER}), through which we
systematically investigate how to design and pre-train a fully
transformer-based VL model in an end-to-end manner. Specifically, we dissect
the model designs along multiple dimensions: vision encoders (e.g., CLIP-ViT,
Swin transformer), text encoders (e.g., RoBERTa, DeBERTa), multimodal fusion
(e.g., merged attention vs. co-attention), architecture design (e.g.,
encoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked
image modeling). We conduct comprehensive experiments on a wide range of VL
tasks, and provide insights on how to train a performant VL transformer while
maintaining fast inference speed. Notably, METER~achieves an accuracy of
77.64\% on the VQAv2 test-std set using only 4M images for pre-training,
surpassing the state-of-the-art region-feature-based VinVL model by +1.04\%,
and outperforming the previous best fully transformer-based ALBEF model by
+1.6\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FAST: Searching for a Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation. (arXiv:2111.02394v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02394">
<div class="article-summary-box-inner">
<span><p>We propose an accurate and efficient scene text detection framework, termed
FAST (i.e., faster arbitrarily-shaped text detector). Different from recent
advanced text detectors that used hand-crafted network architectures and
complicated post-processing, resulting in low inference speed, FAST has two new
designs. (1) We search the network architecture by designing a network search
space and reward function carefully tailored for text detection, leading to
more powerful features than most networks that are searched for image
classification. (2) We design a minimalist representation (only has 1-channel
output) to model text with arbitrary shape, as well as a GPU-parallel
post-processing to efficiently assemble text lines with negligible time
overhead. Benefiting from these two designs, FAST achieves an excellent
trade-off between accuracy and efficiency on several challenging datasets. For
example, FAST-A0 yields 81.4% F-measure at 152 FPS on Total-Text, outperforming
the previous fastest method by 1.5 points and 70 FPS in terms of accuracy and
speed. With TensorRT optimization, the inference speed can be further
accelerated to over 600 FPS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Shared Latent Variables for Robots to Imitate Human Movements and Understand their Physical Limitations. (arXiv:1810.04879v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1810.04879">
<div class="article-summary-box-inner">
<span><p>Assistive robotics and particularly robot coaches may be very helpful for
rehabilitation healthcare. In this context, we propose a method based on
Gaussian Process Latent Variable Model (GP-LVM) to transfer knowledge between a
physiotherapist, a robot coach and a patient. Our model is able to map visual
human body features to robot data in order to facilitate the robot learning and
imitation. In addition , we propose to extend the model to adapt robots'
understanding to patient's physical limitations during the assessment of
rehabilitation exercises. Experimental evaluation demonstrates promising
results for both robot imitation and model adaptation according to the
patients' limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Evaluation: Fine-Grained CNN vs. Traditional CNN Classifiers. (arXiv:2003.11154v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.11154">
<div class="article-summary-box-inner">
<span><p>To make the best use of the underlying minute and subtle differences,
fine-grained classifiers collect information about inter-class variations. The
task is very challenging due to the small differences between the colors,
viewpoint, and structure in the same class entities. The classification becomes
more difficult due to the similarities between the differences in viewpoint
with other classes and differences with its own. In this work, we investigate
the performance of the landmark general CNN classifiers, which presented
top-notch results on large scale classification datasets, on the fine-grained
datasets, and compare it against state-of-the-art fine-grained classifiers. In
this paper, we pose two specific questions: (i) Do the general CNN classifiers
achieve comparable results to fine-grained classifiers? (ii) Do general CNN
classifiers require any specific information to improve upon the fine-grained
ones? Throughout this work, we train the general CNN classifiers without
introducing any aspect that is specific to fine-grained datasets. We show an
extensive evaluation on six datasets to determine whether the fine-grained
classifier is able to elevate the baseline in their experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness. (arXiv:2006.13726v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.13726">
<div class="article-summary-box-inner">
<span><p>Evaluating the robustness of a defense model is a challenging task in
adversarial robustness research. Obfuscated gradients, a type of gradient
masking, have previously been found to exist in many defense methods and cause
a false signal of robustness. In this paper, we identify a more subtle
situation called Imbalanced Gradients that can also cause overestimated
adversarial robustness. The phenomenon of imbalanced gradients occurs when the
gradient of one term of the margin loss dominates and pushes the attack towards
to a suboptimal direction. To exploit imbalanced gradients, we formulate a
Margin Decomposition (MD) attack that decomposes a margin loss into individual
terms and then explores the attackability of these terms separately via a
two-stage process. We also propose a MultiTargeted and an ensemble version of
our MD attack. By investigating 17 defense models proposed since 2018, we find
that 6 models are susceptible to imbalanced gradients and our MD attack can
decrease their robustness evaluated by the best baseline standalone attack by
another 2%. We also provide an in-depth analysis of the likely causes of
imbalanced gradients and effective countermeasures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine versus Human Attention in Deep Reinforcement Learning Tasks. (arXiv:2010.15942v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.15942">
<div class="article-summary-box-inner">
<span><p>Deep reinforcement learning (RL) algorithms are powerful tools for solving
visuomotor decision tasks. However, the trained models are often difficult to
interpret, because they are represented as end-to-end deep neural networks. In
this paper, we shed light on the inner workings of such trained models by
analyzing the pixels that they attend to during task execution, and comparing
them with the pixels attended to by humans executing the same tasks. To this
end, we investigate the following two questions that, to the best of our
knowledge, have not been previously studied. 1) How similar are the visual
representations learned by RL agents and humans when performing the same task?
and, 2) How do similarities and differences in these learned representations
explain RL agents' performance on these tasks? Specifically, we compare the
saliency maps of RL agents against visual attention models of human experts
when learning to play Atari games. Further, we analyze how hyperparameters of
the deep RL algorithm affect the learned representations and saliency maps of
the trained agents. The insights provided have the potential to inform novel
algorithms for closing the performance gap between human experts and RL agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Red Blood Cell Segmentation with Overlapping Cell Separation and Classification on Imbalanced Dataset. (arXiv:2012.01321v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01321">
<div class="article-summary-box-inner">
<span><p>Automated red blood cell (RBC) classification on blood smear images helps
hematologists to analyze RBC lab results in a reduced time and cost. However,
overlapping cells can cause incorrect predicted results, and so they have to be
separated into multiple single RBCs before classifying. To classify multiple
classes with deep learning, imbalance problems are common in medical imaging
because normal samples are always higher than rare disease samples. This paper
presents a new method to segment and classify RBCs from blood smear images,
specifically to tackle cell overlapping and data imbalance problems. Focusing
on overlapping cell separation, our segmentation process first estimates
ellipses to represent RBCs. The method detects the concave points and then
finds the ellipses using directed ellipse fitting. The accuracy from 20 blood
smear images was 0.889. Classification requires balanced training datasets.
However, some RBC types are rare. The imbalance ratio of this dataset was
34.538 for 12 RBC classes from 20,875 individual RBC samples. The use of
machine learning for RBC classification with an imbalanced dataset is hence
more challenging than many other applications. We analyzed techniques to deal
with this problem. The best accuracy and F1-score were 0.921 and 0.8679,
respectively, using EfficientNet-B1 with augmentation. Experimental results
showed that the weight balancing technique with augmentation had the potential
to deal with imbalance problems by improving the F1-score on minority classes,
while data augmentation significantly improved the overall classification
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attack Agnostic Detection of Adversarial Examples via Random Subspace Analysis. (arXiv:2012.06405v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.06405">
<div class="article-summary-box-inner">
<span><p>Whilst adversarial attack detection has received considerable attention, it
remains a fundamentally challenging problem from two perspectives. First, while
threat models can be well-defined, attacker strategies may still vary widely
within those constraints. Therefore, detection should be considered as an
open-set problem, standing in contrast to most current detection approaches.
These methods take a closed-set view and train binary detectors, thus biasing
detection toward attacks seen during detector training. Second, limited
information is available at test time and typically confounded by nuisance
factors including the label and underlying content of the image. We address
these challenges via a novel strategy based on random subspace analysis. We
present a technique that utilizes properties of random projections to
characterize the behavior of clean and adversarial examples across a diverse
set of subspaces. The self-consistency (or inconsistency) of model activations
is leveraged to discern clean from adversarial examples. Performance
evaluations demonstrate that our technique ($AUC\in[0.92, 0.98]$) outperforms
competing detection strategies ($AUC\in[0.30,0.79]$), while remaining truly
agnostic to the attack strategy (for both targeted/untargeted attacks). It also
requires significantly less calibration data (composed only of clean examples)
than competing approaches to achieve this performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GTA: Global Temporal Attention for Video Action Understanding. (arXiv:2012.08510v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08510">
<div class="article-summary-box-inner">
<span><p>Self-attention learns pairwise interactions to model long-range dependencies,
yielding great improvements for video action recognition. In this paper, we
seek a deeper understanding of self-attention for temporal modeling in videos.
We first demonstrate that the entangled modeling of spatio-temporal information
by flattening all pixels is sub-optimal, failing to capture temporal
relationships among frames explicitly. To this end, we introduce Global
Temporal Attention (GTA), which performs global temporal attention on top of
spatial attention in a decoupled manner. We apply GTA on both pixels and
semantically similar regions to capture temporal relationships at different
levels of spatial granularity. Unlike conventional self-attention that computes
an instance-specific attention matrix, GTA directly learns a global attention
matrix that is intended to encode temporal structures that generalize across
different samples. We further augment GTA with a cross-channel multi-head
fashion to exploit channel interactions for better temporal modeling. Extensive
experiments on 2D and 3D networks demonstrate that our approach consistently
enhances temporal modeling and provides state-of-the-art performance on three
video action recognition datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LS-HDIB: A Large Scale Handwritten Document Image Binarization Dataset. (arXiv:2101.11674v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11674">
<div class="article-summary-box-inner">
<span><p>Handwritten document image binarization is challenging due to high
variability in the written content and complex background attributes such as
page style, paper quality, stains, shadow gradients, and non-uniform
illumination. While the traditional thresholding methods do not effectively
generalize on such challenging real-world scenarios, deep learning-based
methods have performed relatively well when provided with sufficient training
data. However, the existing datasets are limited in size and diversity. This
work proposes LS-HDIB - a large-scale handwritten document image binarization
dataset containing over a million document images that span numerous real-world
scenarios. Additionally, we introduce a novel technique that uses a combination
of adaptive thresholding and seamless cloning methods to create the dataset
with accurate ground truths. Through an extensive quantitative and qualitative
evaluation over eight different deep learning based models, we demonstrate the
enhancement in the performance of these models when trained on the LS-HDIB
dataset and tested on unseen images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SWAD: Domain Generalization by Seeking Flat Minima. (arXiv:2102.08604v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08604">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) methods aim to achieve generalizability to an
unseen target domain by using only training data from the source domains.
Although a variety of DG methods have been proposed, a recent study shows that
under a fair evaluation protocol, called DomainBed, the simple empirical risk
minimization (ERM) approach works comparable to or even outperforms previous
methods. Unfortunately, simply solving ERM on a complex, non-convex loss
function can easily lead to sub-optimal generalizability by seeking sharp
minima. In this paper, we theoretically show that finding flat minima results
in a smaller domain generalization gap. We also propose a simple yet effective
method, named Stochastic Weight Averaging Densely (SWAD), to find flat minima.
SWAD finds flatter minima and suffers less from overfitting than does the
vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy.
SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS,
VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large
margins of +1.6% averagely on out-of-domain accuracy. We also compare SWAD with
conventional generalization methods, such as data augmentation and consistency
regularization methods, to verify that the remarkable performance improvements
are originated from by seeking flat minima, not from better in-domain
generalizability. Last but not least, SWAD is readily adaptable to existing DG
methods without modification; the combination of SWAD and an existing DG method
further improves DG performances. Source code is available at
https://github.com/khanrc/swad.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stronger NAS with Weaker Predictors. (arXiv:2102.10490v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10490">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) often trains and evaluates a large number of
architectures. Recent predictor-based NAS approaches attempt to alleviate such
heavy computation costs with two key steps: sampling some
architecture-performance pairs and fitting a proxy accuracy predictor. Given
limited samples, these predictors, however, are far from accurate to locate top
architectures due to the difficulty of fitting the huge search space. This
paper reflects on a simple yet crucial question: if our final goal is to find
the best architecture, do we really need to model the whole space well?. We
propose a paradigm shift from fitting the whole architecture space using one
strong predictor, to progressively fitting a search path towards the
high-performance sub-space through a set of weaker predictors. As a key
property of the weak predictors, their probabilities of sampling better
architectures keep increasing. Hence we only sample a few well-performed
architectures guided by the previously learned predictor and estimate a new
better weak predictor. This embarrassingly easy framework, dubbed WeakNAS,
produces coarse-to-fine iteration to gradually refine the ranking of sampling
space. Extensive experiments demonstrate that WeakNAS costs fewer samples to
find top-performance architectures on NAS-Bench-101 and NAS-Bench-201. Compared
to state-of-the-art (SOTA) predictor-based NAS methods, WeakNAS outperforms all
with notable margins, e.g., requiring at least 7.5x less samples to find global
optimal on NAS-Bench-101. WeakNAS can also absorb their ideas to boost
performance more. Further, WeakNAS strikes the new SOTA result of 81.3% in the
ImageNet MobileNet Search Space. The code is available at
https://github.com/VITA-Group/WeakNAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03743">
<div class="article-summary-box-inner">
<span><p>We investigate the reasons for the performance degradation incurred with
batch-independent normalization. We find that the prototypical techniques of
layer normalization and instance normalization both induce the appearance of
failure modes in the neural network's pre-activations: (i) layer normalization
induces a collapse towards channel-wise constant functions; (ii) instance
normalization induces a lack of variability in instance statistics, symptomatic
of an alteration of the expressivity. To alleviate failure mode (i) without
aggravating failure mode (ii), we introduce the technique "Proxy Normalization"
that normalizes post-activations using a proxy distribution. When combined with
layer normalization or group normalization, this batch-independent
normalization emulates batch normalization's behavior and consistently matches
or exceeds its performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Evaluation of Deep Active Learning on Image Classification Tasks. (arXiv:2106.15324v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15324">
<div class="article-summary-box-inner">
<span><p>With the goal of making deep learning more label-efficient, a growing number
of papers have been studying active learning (AL) for deep models. However,
there are a number of issues in the prevalent experimental settings, mainly
stemming from a lack of unified implementation and benchmarking. Issues in the
current literature include sometimes contradictory observations on the
performance of different AL algorithms, unintended exclusion of important
generalization approaches such as data augmentation and SGD for optimization, a
lack of study of evaluation facets like the labeling efficiency of AL, and
little or no clarity on the scenarios in which AL outperforms random sampling
(RS). In this work, we present a unified re-implementation of state-of-the-art
AL algorithms in the context of image classification via our new open-source AL
toolkit DISTIL, and we carefully study these issues as facets of effective
evaluation. On the positive side, we show that AL techniques are $2\times$ to
$4\times$ more label-efficient compared to RS with the use of data
augmentation. Surprisingly, when data augmentation is included, there is no
longer a consistent gain in using BADGE, a state-of-the-art approach, over
simple uncertainty sampling. We then do a careful analysis of how existing
approaches perform with varying amounts of redundancy and number of examples
per class. Finally, we provide several insights for AL practitioners to
consider in future work, such as the effect of the AL batch size, the effect of
initialization, the importance of retraining the model at every round, and
other insights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-Aware Multi-Grid Deep Homography Estimation with Contextual Correlation. (arXiv:2107.02524v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02524">
<div class="article-summary-box-inner">
<span><p>Homography estimation is an important task in computer vision applications,
such as image stitching, video stabilization, and camera calibration.
Traditional homography estimation methods heavily depend on the quantity and
distribution of feature correspondences, leading to poor robustness in
low-texture scenes. The learning solutions, on the contrary, try to learn
robust deep features but demonstrate unsatisfying performance in the scenes
with low overlap rates. In this paper, we address these two problems
simultaneously by designing a contextual correlation layer (CCL). The CCL can
efficiently capture the long-range correlation within feature maps and can be
flexibly used in a learning framework. In addition, considering that a single
homography can not represent the complex spatial transformation in
depth-varying images with parallax, we propose to predict multi-grid homography
from global to local. Moreover, we equip our network with a depth perception
capability, by introducing a novel depth-aware shape-preserved loss. Extensive
experiments demonstrate the superiority of our method over state-of-the-art
solutions in the synthetic benchmark dataset and real-world dataset. The codes
and models will be available at
https://github.com/nie-lang/Multi-Grid-Deep-Homography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization via Gradient Surgery. (arXiv:2108.01621v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01621">
<div class="article-summary-box-inner">
<span><p>In real-life applications, machine learning models often face scenarios where
there is a change in data distribution between training and test domains. When
the aim is to make predictions on distributions different from those seen at
training, we incur in a domain generalization problem. Methods to address this
issue learn a model using data from multiple source domains, and then apply
this model to the unseen target domain. Our hypothesis is that when training
with multiple domains, conflicting gradients within each mini-batch contain
information specific to the individual domains which is irrelevant to the
others, including the test domain. If left untouched, such disagreement may
degrade generalization performance. In this work, we characterize the
conflicting gradients emerging in domain shift scenarios and devise novel
gradient agreement strategies based on gradient surgery to alleviate their
effect. We validate our approach in image classification tasks with three
multi-domain datasets, showing the value of the proposed agreement strategy in
enhancing the generalization capability of deep learning models in domain shift
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Data Uncertainty in Object Tracking Algorithms. (arXiv:2109.10521v2 [eess.SY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10521">
<div class="article-summary-box-inner">
<span><p>Methodologies for incorporating the uncertainties characteristic of
data-driven object detectors into object tracking algorithms are explored.
Object tracking methods rely on measurement error models, typically in the form
of measurement noise, false positive rates, and missed detection rates. Each of
these quantities, in general, can be dependent on object or measurement
location. However, for detections generated from neural-network processed
camera inputs, these measurement error statistics are not sufficient to
represent the primary source of errors, namely a dissimilarity between run-time
sensor input and the training data upon which the detector was trained. To this
end, we investigate incorporating data uncertainty into object tracking methods
such as to improve the ability to track objects, and particularly those which
out-of-distribution w.r.t. training data. The proposed methodologies are
validated on an object tracking benchmark as well on experiments with a real
autonomous aircraft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDRVideo-GAN: Deep Generative HDR Video Reconstruction. (arXiv:2110.11795v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11795">
<div class="article-summary-box-inner">
<span><p>High dynamic range (HDR) videos provide a more visually realistic experience
than the standard low dynamic range (LDR) videos. Despite having significant
progress in HDR imaging, it is still a challenging task to capture high-quality
HDR video with a conventional off-the-shelf camera. Existing approaches rely
entirely on using dense optical flow between the neighboring LDR sequences to
reconstruct an HDR frame. However, they lead to inconsistencies in color and
exposure over time when applied to alternating exposures with noisy frames. In
this paper, we propose an end-to-end GAN-based framework for HDR video
reconstruction from LDR sequences with alternating exposures. We first extract
clean LDR frames from noisy LDR video with alternating exposures with a
denoising network trained in a self-supervised setting. Using optical flow, we
then align the neighboring alternating-exposure frames to a reference frame and
then reconstruct high-quality HDR frames in a complete adversarial setting. To
further improve the robustness and quality of generated frames, we incorporate
temporal stability-based regularization term along with content and style-based
losses in the cost function during the training procedure. Experimental results
demonstrate that our framework achieves state-of-the-art performance and
generates superior quality HDR frames of a video over the existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaskSplit: Self-supervised Meta-learning for Few-shot Semantic Segmentation. (arXiv:2110.12207v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12207">
<div class="article-summary-box-inner">
<span><p>Just like other few-shot learning problems, few-shot segmentation aims to
minimize the need for manual annotation, which is particularly costly in
segmentation tasks. Even though the few-shot setting reduces this cost for
novel test classes, there is still a need to annotate the training data. To
alleviate this need, we propose a self-supervised training approach for
learning few-shot segmentation models. We first use unsupervised saliency
estimation to obtain pseudo-masks on images. We then train a simple prototype
based model over different splits of pseudo masks and augmentations of images.
Our extensive experiments show that the proposed approach achieves promising
results, highlighting the potential of self-supervised training. To the best of
our knowledge this is the first work that addresses unsupervised few-shot
segmentation problem on natural images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masking Modalities for Cross-modal Video Retrieval. (arXiv:2111.01300v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01300">
<div class="article-summary-box-inner">
<span><p>Pre-training on large scale unlabelled datasets has shown impressive
performance improvements in the fields of computer vision and natural language
processing. Given the advent of large-scale instructional video datasets, a
common strategy for pre-training video encoders is to use the accompanying
speech as weak supervision. However, as speech is used to supervise the
pre-training, it is never seen by the video encoder, which does not learn to
process that modality. We address this drawback of current pre-training
methods, which fail to exploit the rich cues in spoken language. Our proposal
is to pre-train a video encoder using all the available video modalities as
supervision, namely, appearance, sound, and transcribed speech. We mask an
entire modality in the input and predict it using the other two modalities.
This encourages each modality to collaborate with the others, and our video
encoder learns to process appearance and audio as well as speech. We show the
superior performance of our "modality masking" pre-training approach for video
retrieval on the How2R, YouCook2 and Condensed Movies datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Split Vision Transformer for COVID-19 CXR Diagnosis using Task-Agnostic Training. (arXiv:2111.01338v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01338">
<div class="article-summary-box-inner">
<span><p>Federated learning, which shares the weights of the neural network across
clients, is gaining attention in the healthcare sector as it enables training
on a large corpus of decentralized data while maintaining data privacy. For
example, this enables neural network training for COVID-19 diagnosis on chest
X-ray (CXR) images without collecting patient CXR data across multiple
hospitals. Unfortunately, the exchange of the weights quickly consumes the
network bandwidth if highly expressive network architecture is employed.
So-called split learning partially solves this problem by dividing a neural
network into a client and a server part, so that the client part of the network
takes up less extensive computation resources and bandwidth. However, it is not
clear how to find the optimal split without sacrificing the overall network
performance. To amalgamate these methods and thereby maximize their distinct
strengths, here we show that the Vision Transformer, a recently developed deep
learning architecture with straightforward decomposable configuration, is
ideally suitable for split learning without sacrificing performance. Even under
the non-independent and identically distributed data distribution which
emulates a real collaboration between hospitals using CXR datasets from
multiple sources, the proposed framework was able to attain performance
comparable to data-centralized training. In addition, the proposed framework
along with heterogeneous multi-task clients also improves individual task
performances including the diagnosis of COVID-19, eliminating the need for
sharing large weights with innumerable parameters. Our results affirm the
suitability of Transformer for collaborative learning in medical imaging and
pave the way forward for future real-world implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Vision Transformers Perform Convolution?. (arXiv:2111.01353v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01353">
<div class="article-summary-box-inner">
<span><p>Several recent studies have demonstrated that attention-based networks, such
as Vision Transformer (ViT), can outperform Convolutional Neural Networks
(CNNs) on several computer vision tasks without using convolutional layers.
This naturally leads to the following questions: Can a self-attention layer of
ViT express any convolution operation? In this work, we prove that a single ViT
layer with image patches as the input can perform any convolution operation
constructively, where the multi-head attention mechanism and the relative
positional encoding play essential roles. We further provide a lower bound on
the number of heads for Vision Transformers to express CNNs. Corresponding with
our analysis, experimental results show that the construction in our proof can
help inject convolutional bias into Transformers and significantly improve the
performance of ViT in low data regimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HHP-Net: A light Heteroscedastic neural network for Head Pose estimation with uncertainty. (arXiv:2111.01440v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01440">
<div class="article-summary-box-inner">
<span><p>In this paper we introduce a novel method to estimate the head pose of people
in single images starting from a small set of head keypoints. To this purpose,
we propose a regression model that exploits keypoints computed automatically by
2D pose estimation algorithms and outputs the head pose represented by yaw,
pitch, and roll. Our model is simple to implement and more efficient with
respect to the state of the art -- faster in inference and smaller in terms of
memory occupancy -- with comparable accuracy. Our method also provides a
measure of the heteroscedastic uncertainties associated with the three angles,
through an appropriately designed loss function; we show there is a correlation
between error and uncertainty values, thus this extra source of information may
be used in subsequent computational steps. As an example application, we
address social interaction analysis in images: we propose an algorithm for a
quantitative estimation of the level of interaction between people, starting
from their head poses and reasoning on their mutual positions. The code is
available at https://github.com/cantarinigiorgio/HHP-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Knowledge Distillation From the Perspective of Model Calibration. (arXiv:2111.01684v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01684">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed dramatically improvements in the knowledge
distillation, which can generate a compact student model for better efficiency
while retaining the model effectiveness of the teacher model. Previous studies
find that: more accurate teachers do not necessary make for better teachers due
to the mismatch of abilities. In this paper, we aim to analysis the phenomenon
from the perspective of model calibration. We found that the larger teacher
model may be too over-confident, thus the student model cannot effectively
imitate. While, after the simple model calibration of the teacher model, the
size of the teacher model has a positive correlation with the performance of
the student model.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-11-04 23:02:25.359010606 UTC">2021-11-04 23:02:25 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.6</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>