<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-26T01:30:00Z">07-26</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">PanGu-Coder: Program Synthesis with Function-Level Language Modeling. (arXiv:2207.11280v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11280">
<div class="article-summary-box-inner">
<span><p>We present PanGu-Coder, a pretrained decoder-only language model adopting the
PanGu-Alpha architecture for text-to-code generation, i.e. the synthesis of
programming language solutions given a natural language problem description. We
train PanGu-Coder using a two-stage strategy: the first stage employs Causal
Language Modelling (CLM) to pre-train on raw programming language data, while
the second stage uses a combination of Causal Language Modelling and Masked
Language Modelling (MLM) training objectives that focus on the downstream task
of text-to-code generation and train on loosely curated pairs of natural
language program definitions and code functions. Finally, we discuss
PanGu-Coder-FT, which is fine-tuned on a combination of competitive programming
problems and code with continuous integration tests. We evaluate PanGu-Coder
with a focus on whether it generates functionally correct programs and
demonstrate that it achieves equivalent or better performance than similarly
sized models, such as CodeX, while attending a smaller context window and
training on less data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities. (arXiv:2207.11345v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11345">
<div class="article-summary-box-inner">
<span><p>As for other forms of AI, speech recognition has recently been examined with
respect to performance disparities across different user cohorts. One approach
to achieve fairness in speech recognition is to (1) identify speaker cohorts
that suffer from subpar performance and (2) apply fairness mitigation measures
targeting the cohorts discovered. In this paper, we report on initial findings
with both discovery and mitigation of performance disparities using data from a
product-scale AI assistant speech recognition system. We compare cohort
discovery based on geographic and demographic information to a more scalable
method that groups speakers without human labels, using speaker embedding
technology. For fairness mitigation, we find that oversampling of
underrepresented cohorts, as well as modeling speaker cohort membership by
additional input variables, reduces the gap between top- and bottom-performing
cohorts, without deteriorating overall recognition accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Grounded Conversational Data Augmentation with Generative Conversational Networks. (arXiv:2207.11363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11363">
<div class="article-summary-box-inner">
<span><p>While rich, open-domain textual data are generally available and may include
interesting phenomena (humor, sarcasm, empathy, etc.) most are designed for
language processing tasks, and are usually in a non-conversational format. In
this work, we take a step towards automatically generating conversational data
using Generative Conversational Networks, aiming to benefit from the breadth of
available language and knowledge data, and train open domain social
conversational agents. We evaluate our approach on conversations with and
without knowledge on the Topical Chat dataset using automatic metrics and human
evaluators. Our results show that for conversations without knowledge
grounding, GCN can generalize from the seed data, producing novel conversations
that are less relevant but more engaging and for knowledge-grounded
conversations, it can produce more knowledge-focused, fluent, and engaging
conversations. Specifically, we show that for open-domain conversations with
10\% of seed data, our approach performs close to the baseline that uses 100%
of the data, while for knowledge-grounded conversations, it achieves the same
using only 1% of the data, on human ratings of engagingness, fluency, and
relevance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations. (arXiv:2207.11401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11401">
<div class="article-summary-box-inner">
<span><p>Visual Entailment with natural language explanations aims to infer the
relationship between a text-image pair and generate a sentence to explain the
decision-making process. Previous methods rely mainly on a pre-trained
vision-language model to perform the relation inference and a language model to
generate the corresponding explanation. However, the pre-trained
vision-language models mainly build token-level alignment between text and
image yet ignore the high-level semantic alignment between the phrases (chunks)
and visual contents, which is critical for vision-language reasoning. Moreover,
the explanation generator based only on the encoded joint representation does
not explicitly consider the critical decision-making points of relation
inference. Thus the generated explanations are less faithful to visual-language
reasoning. To mitigate these problems, we propose a unified Chunk-aware
Alignment and Lexical Constraint based method, dubbed as CALeC. It contains a
Chunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical
Constraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence
structure inherent in language and various image regions to build chunk-aware
semantic alignment. Relation inferrer uses an attention-based reasoning network
to incorporate the token-level and chunk-level vision-language representations.
LeCG utilizes lexical constraints to expressly incorporate the words or chunks
focused by the relation inferrer into explanation generation, improving the
faithfulness and informativeness of the explanations. We conduct extensive
experiments on three datasets, and experimental results indicate that CALeC
significantly outperforms other competitor models on inference accuracy and
quality of generated explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Document-level Relation Extraction by Entity Knowledge Injection. (arXiv:2207.11433v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11433">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (RE) aims to identify the relations
between entities throughout an entire document. It needs complex reasoning
skills to synthesize various knowledge such as coreferences and commonsense.
Large-scale knowledge graphs (KGs) contain a wealth of real-world facts, and
can provide valuable knowledge to document-level RE. In this paper, we propose
an entity knowledge injection framework to enhance current document-level RE
models. Specifically, we introduce coreference distillation to inject
coreference knowledge, endowing an RE model with the more general capability of
coreference reasoning. We also employ representation reconciliation to inject
factual knowledge and aggregate KG representations and document representations
into a unified space. The experiments on two benchmark datasets validate the
generalization of our entity knowledge injection framework and the consistent
improvement to several document-level RE models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facing Changes: Continual Entity Alignment for Growing Knowledge Graphs. (arXiv:2207.11436v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11436">
<div class="article-summary-box-inner">
<span><p>Entity alignment is a basic and vital technique in knowledge graph (KG)
integration. Over the years, research on entity alignment has resided on the
assumption that KGs are static, which neglects the nature of growth of
real-world KGs. As KGs grow, previous alignment results face the need to be
revisited while new entity alignment waits to be discovered. In this paper, we
propose and dive into a realistic yet unexplored setting, referred to as
continual entity alignment. To avoid retraining an entire model on the whole
KGs whenever new entities and triples come, we present a continual alignment
method for this task. It reconstructs an entity's representation based on
entity adjacency, enabling it to generate embeddings for new entities quickly
and inductively using their existing neighbors. It selects and replays partial
pre-aligned entity pairs to train only parts of KGs while extracting
trustworthy alignment for knowledge augmentation. As growing KGs inevitably
contain non-matchable entities, different from previous works, the proposed
method employs bidirectional nearest neighbor matching to find new entity
alignment and update old alignment. Furthermore, we also construct new datasets
by simulating the growth of multilingual DBpedia. Extensive experiments
demonstrate that our continual alignment method is more effective than
baselines based on retraining or inductive learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\mu\text{KG}$: A Library for Multi-source Knowledge Graph Embeddings and Applications. (arXiv:2207.11442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11442">
<div class="article-summary-box-inner">
<span><p>This paper presents $\mu\text{KG}$, an open-source Python library for
representation learning over knowledge graphs. $\mu\text{KG}$ supports joint
representation learning over multi-source knowledge graphs (and also a single
knowledge graph), multiple deep learning libraries (PyTorch and TensorFlow2),
multiple embedding tasks (link prediction, entity alignment, entity typing, and
multi-source link prediction), and multiple parallel computing modes
(multi-process and multi-GPU computing). It currently implements 26 popular
knowledge graph embedding models and supports 16 benchmark datasets.
$\mu\text{KG}$ provides advanced implementations of embedding techniques with
simplified pipelines of different tasks. It also comes with high-quality
documentation for ease of use. $\mu\text{KG}$ is more comprehensive than
existing knowledge graph embedding libraries. It is useful for a thorough
comparison and analysis of various embedding models and tasks. We show that the
jointly learned embeddings can greatly help knowledge-powered downstream tasks,
such as multi-hop knowledge graph question answering. We will stay abreast of
the latest developments in the related fields and incorporate them into
$\mu\text{KG}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Catch Me If You Can: Deceiving Stance Detection and Geotagging Models to Protect Privacy of Individuals on Twitter. (arXiv:2207.11500v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11500">
<div class="article-summary-box-inner">
<span><p>The recent advances in natural language processing have yielded many exciting
developments in text analysis and language understanding models; however, these
models can also be used to track people, bringing severe privacy concerns. In
this work, we investigate what individuals can do to avoid being detected by
those models while using social media platforms. We ground our investigation in
two exposure-risky tasks, stance detection and geotagging. We explore a variety
of simple techniques for modifying text, such as inserting typos in salient
words, paraphrasing, and adding dummy social media posts. Our experiments show
that the performance of BERT-based models fined tuned for stance detection
decreases significantly due to typos, but it is not affected by paraphrasing.
Moreover, we find that typos have minimal impact on state-of-the-art geotagging
models due to their increased reliance on social networks; however, we show
that users can deceive those models by interacting with different users,
reducing their performance by almost 50%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vaccine Discourse on Twitter During the COVID-19 Pandemic. (arXiv:2207.11521v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11521">
<div class="article-summary-box-inner">
<span><p>Since the onset of the COVID-19 pandemic, vaccines have been an important
topic in public discourse. The discussions around vaccines are polarized as
some see them as an important measure to end the pandemic, and others are
hesitant or find them harmful. This study investigates posts related to
COVID-19 vaccines on Twitter and focuses on those which have a negative stance
toward vaccines. A dataset of 16,713,238 English tweets related to COVID-19
vaccines was collected covering the period from March 1, 2020, to July 31,
2021. We used the Scikit-learn Python library to apply a support vector machine
(SVM) classifier to identify the tweets with a negative stance toward the
COVID-19 vaccines. A total of 5,163 tweets were used to train the classifier,
out of which a subset of 2,484 tweets were manually annotated by us and made
publicly available. We used the BERTtopic model to extract and investigate the
topics discussed within the negative tweets and how they changed over time. We
show that the negativity with respect to COVID-19 vaccines has decreased over
time along with the vaccine roll-outs. We identify 37 topics of discussion and
present their respective importance over time. We show that popular topics
consist of conspiratorial discussions such as 5G towers and microchips, but
also contain legitimate concerns around vaccination safety and side effects as
well as concerns about policies. Our study shows that even unpopular opinions
or conspiracy theories can become widespread when paired with a widely popular
discussion topic such as COVID-19 vaccines. Understanding the concerns and the
discussed topics and how they change over time is essential for policymakers
and public health authorities to provide better and in-time information and
policies, to facilitate vaccination of the population in future similar crises.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supporting peace negotiations in the Yemen war through machine learning. (arXiv:2207.11528v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11528">
<div class="article-summary-box-inner">
<span><p>Today's conflicts are becoming increasingly complex, fluid and fragmented,
often involving a host of national and international actors with multiple and
often divergent interests. This development poses significant challenges for
conflict mediation, as mediators struggle to make sense of conflict dynamics,
such as the range of conflict parties and the evolution of their political
positions, the distinction between relevant and less relevant actors in
peace-making, or the identification of key conflict issues and their
interdependence. International peace efforts appear ill-equipped to
successfully address these challenges. While technology is already being
experimented with and used in a range of conflict related fields, such as
conflict predicting or information gathering, less attention has been given to
how technology can contribute to conflict mediation. This case study
contributes to emerging research on the use of state-of-the-art machine
learning technologies and techniques in conflict mediation processes. Using
dialogue transcripts from peace negotiations in Yemen, this study shows how
machine-learning can effectively support mediating teams by providing them with
tools for knowledge management, extraction and conflict analysis. Apart from
illustrating the potential of machine learning tools in conflict mediation, the
paper also emphasises the importance of interdisciplinary and participatory,
co-creation methodology for the development of context-sensitive and targeted
tools and to ensure meaningful and responsible implementation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Reasoning Behind Classification Predictions with BERT for Fake News Detection. (arXiv:2207.11562v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11562">
<div class="article-summary-box-inner">
<span><p>Fake news detection has become a major task to solve as there has been an
increasing number of fake news on the internet in recent years. Although many
classification models have been proposed based on statistical learning methods
showing good results, reasoning behind the classification performances may not
be enough. In the self-supervised learning studies, it has been highlighted
that a quality of representation (embedding) space matters and directly affects
a downstream task performance. In this study, a quality of the representation
space is analyzed visually and analytically in terms of linear separability for
different classes on a real and fake news dataset. To further add
interpretability to a classification model, a modification of Class Activation
Mapping (CAM) is proposed. The modified CAM provides a CAM score for each word
token, where the CAM score on a word token denotes a level of focus on that
word token to make the prediction. Finally, it is shown that the naive BERT
model topped with a learnable linear layer is enough to achieve robust
performance while being compatible with CAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context based lemmatizer for Polish language. (arXiv:2207.11565v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11565">
<div class="article-summary-box-inner">
<span><p>Lemmatization is the process of grouping together the inflected forms of a
word so they can be analysed as a single item, identified by the word's lemma,
or dictionary form. In computational linguistics, lemmatisation is the
algorithmic process of determining the lemma of a word based on its intended
meaning. Unlike stemming, lemmatisation depends on correctly identifying the
intended part of speech and meaning of a word in a sentence, as well as within
the larger context surrounding that sentence. As a result, developing efficient
lemmatisation algorithm is the complex task. In recent years it can be observed
that deep learning models used for this task outperform other methods including
machine learning algorithms. In this paper the polish lemmatizer based on
Google T5 model is presented. The training was run with different context
lengths. The model achieves the best results for polish language lemmatisation
process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis. (arXiv:2207.11652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11652">
<div class="article-summary-box-inner">
<span><p>Existing studies on multimodal sentiment analysis heavily rely on textual
modality and unavoidably induce the spurious correlations between textual words
and sentiment labels. This greatly hinders the model generalization ability. To
address this problem, we define the task of out-of-distribution (OOD)
multimodal sentiment analysis. This task aims to estimate and mitigate the bad
effect of textual modality for strong OOD generalization. To this end, we
embrace causal inference, which inspects the causal relationships via a causal
graph. From the graph, we find that the spurious correlations are attributed to
the direct effect of textual modality on the model prediction while the
indirect one is more reliable by considering multimodal semantics. Inspired by
this, we devise a model-agnostic counterfactual framework for multimodal
sentiment analysis, which captures the direct effect of textual modality via an
extra text model and estimates the indirect one by a multimodal model. During
the inference, we first estimate the direct effect by the counterfactual
inference, and then subtract it from the total effect of all modalities to
obtain the indirect effect for reliable prediction. Extensive experiments show
the superior effectiveness and generalization ability of our proposed
framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoWeird: Weird Translational Scoring Function Identified by Random Search. (arXiv:2207.11673v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11673">
<div class="article-summary-box-inner">
<span><p>Scoring function (SF) measures the plausibility of triplets in knowledge
graphs. Different scoring functions can lead to huge differences in link
prediction performances on different knowledge graphs. In this report, we
describe a weird scoring function found by random search on the open graph
benchmark (OGB). This scoring function, called AutoWeird, only uses tail entity
and relation in a triplet to compute its plausibility score. Experimental
results show that AutoWeird achieves top-1 performance on ogbl-wikikg2 data
set, but has much worse performance than other methods on ogbl-biokg data set.
By analyzing the tail entity distribution and evaluation protocol of these two
data sets, we attribute the unexpected success of AutoWeird on ogbl-wikikg2 to
inappropriate evaluation and concentrated tail entity distribution. Such
results may motivate further research on how to accurately evaluate the
performance of different link prediction methods for knowledge graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Mandarin Speech Recogntion with Block-augmented Transformer. (arXiv:2207.11697v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11697">
<div class="article-summary-box-inner">
<span><p>Recently Convolution-augmented Transformer (Conformer) has shown promising
results in Automatic Speech Recognition (ASR), outperforming the previous best
published Transformer Transducer. In this work, we believe that the output
information of each block in the encoder and decoder is not completely
inclusive, in other words, their output information may be complementary. We
study how to take advantage of the complementary information of each block in a
parameter-efficient way, and it is expected that this may lead to more robust
performance. Therefore we propose the Block-augmented Transformer for speech
recognition, named Blockformer. We have implemented two block ensemble methods:
the base Weighted Sum of the Blocks Output (Base-WSBO), and the
Squeeze-and-Excitation module to Weighted Sum of the Blocks Output (SE-WSBO).
Experiments have proved that the Blockformer significantly outperforms the
state-of-the-art Conformer-based models on AISHELL-1, our model achieves a CER
of 4.35\% without using a language model and 4.10\% with an external language
model on the testset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11716">
<div class="article-summary-box-inner">
<span><p>Semantic similarity analysis and modeling is a fundamentally acclaimed task
in many pioneering applications of natural language processing today. Owing to
the sensation of sequential pattern recognition, many neural networks like RNNs
and LSTMs have achieved satisfactory results in semantic similarity modeling.
However, these solutions are considered inefficient due to their inability to
process information in a non-sequential manner, thus leading to the improper
extraction of context. Transformers function as the state-of-the-art
architecture due to their advantages like non-sequential data processing and
self-attention. In this paper, we perform semantic similarity analysis and
modeling on the U.S Patent Phrase to Phrase Matching Dataset using both
traditional and transformer-based techniques. We experiment upon four different
variants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by
performing K-Fold Cross-Validation. The experimental results demonstrate our
methodology's enhanced performance compared to traditional techniques, with an
average Pearson correlation score of 0.79.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anti-Overestimation Dialogue Policy Learning for Task-Completion Dialogue System. (arXiv:2207.11762v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11762">
<div class="article-summary-box-inner">
<span><p>A dialogue policy module is an essential part of task-completion dialogue
systems. Recently, increasing interest has focused on reinforcement learning
(RL)-based dialogue policy. Its favorable performance and wise action decisions
rely on an accurate estimation of action values. The overestimation problem is
a widely known issue of RL since its estimate of the maximum action value is
larger than the ground truth, which results in an unstable learning process and
suboptimal policy. This problem is detrimental to RL-based dialogue policy
learning. To mitigate this problem, this paper proposes a dynamic partial
average estimator (DPAV) of the ground truth maximum action value. DPAV
calculates the partial average between the predicted maximum action value and
minimum action value, where the weights are dynamically adaptive and
problem-dependent. We incorporate DPAV into a deep Q-network as the dialogue
policy and show that our method can achieve better or comparable results
compared to top baselines on three dialogue datasets of different domains with
a lower computational load. In addition, we also theoretically prove the
convergence and derive the upper and lower bounds of the bias compared with
those of other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Sentiment-Aware Conversational Agent. (arXiv:2207.11774v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11774">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an end-to-end sentiment-aware conversational agent
based on two models: a reply sentiment prediction model, which leverages the
context of the dialogue to predict an appropriate sentiment for the agent to
express in its reply; and a text generation model, which is conditioned on the
predicted sentiment and the context of the dialogue, to produce a reply that is
both context and sentiment appropriate. Additionally, we propose to use a
sentiment classification model to evaluate the sentiment expressed by the agent
during the development of the model. This allows us to evaluate the agent in an
automatic way. Both automatic and human evaluation results show that explicitly
guiding the text generation model with a pre-defined set of sentences leads to
clear improvements, both regarding the expressed sentiment and the quality of
the generated text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancements to the BOUN Treebank Reflecting the Agglutinative Nature of Turkish. (arXiv:2207.11782v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11782">
<div class="article-summary-box-inner">
<span><p>In this study, we aim to offer linguistically motivated solutions to resolve
the issues of the lack of representation of null morphemes, highly productive
derivational processes, and syncretic morphemes of Turkish in the BOUN Treebank
without diverging from the Universal Dependencies framework.
</p>
<p>In order to tackle these issues, new annotation conventions were introduced
by splitting certain lemmas and employing the MISC (miscellaneous) tab in the
UD framework to denote derivation. Representational capabilities of the
re-annotated treebank were tested on a LSTM-based dependency parser and an
updated version of the BoAT Tool is introduced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArmanEmo: A Persian Dataset for Text-based Emotion Detection. (arXiv:2207.11808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11808">
<div class="article-summary-box-inner">
<span><p>With the recent proliferation of open textual data on social media platforms,
Emotion Detection (ED) from Text has received more attention over the past
years. It has many applications, especially for businesses and online service
providers, where emotion detection techniques can help them make informed
commercial decisions by analyzing customers/users' feelings towards their
products and services. In this study, we introduce ArmanEmo, a human-labeled
emotion dataset of more than 7000 Persian sentences labeled for seven
categories. The dataset has been collected from different resources, including
Twitter, Instagram, and Digikala (an Iranian e-commerce company) comments.
Labels are based on Ekman's six basic emotions (Anger, Fear, Happiness, Hatred,
Sadness, Wonder) and another category (Other) to consider any other emotion not
included in Ekman's model. Along with the dataset, we have provided several
baseline models for emotion classification focusing on the state-of-the-art
transformer-based language models. Our best model achieves a macro-averaged F1
score of 75.39 percent across our test dataset. Moreover, we also conduct
transfer learning experiments to compare our proposed dataset's generalization
against other Persian emotion datasets. Results of these experiments suggest
that our dataset has superior generalizability among the existing Persian
emotion datasets. ArmanEmo is publicly available for non-commercial use at
https://github.com/Arman-Rayan-Sharif/arman-text-emotion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gumbel-Attention for Multi-modal Machine Translation. (arXiv:2103.08862v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08862">
<div class="article-summary-box-inner">
<span><p>Multi-modal machine translation (MMT) improves translation quality by
introducing visual information. However, the existing MMT model ignores the
problem that the image will bring information irrelevant to the text, causing
much noise to the model and affecting the translation quality. This paper
proposes a novel Gumbel-Attention for multi-modal machine translation, which
selects the text-related parts of the image features. Specifically, different
from the previous attention-based method, we first use a differentiable method
to select the image information and automatically remove the useless parts of
the image features. Experiments prove that our method retains the image
features related to the text, and the remaining parts help the MMT model
generates better translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network. (arXiv:2106.07352v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07352">
<div class="article-summary-box-inner">
<span><p>We present an instance-based nearest neighbor approach to entity linking. In
contrast to most prior entity retrieval systems which represent each entity
with a single vector, we build a contextualized mention-encoder that learns to
place similar mentions of the same entity closer in vector space than mentions
of different entities. This approach allows all mentions of an entity to serve
as "class prototypes" as inference involves retrieving from the full set of
labeled entity mentions in the training set and applying the nearest mention
neighbor's entity label. Our model is trained on a large multilingual corpus of
mention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor
inference on an index of 700 million mentions. It is simpler to train, gives
more interpretable predictions, and outperforms all other systems on two
multilingual entity linking benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TREND: Trigger-Enhanced Relation-Extraction Network for Dialogues. (arXiv:2108.13811v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13811">
<div class="article-summary-box-inner">
<span><p>The goal of dialogue relation extraction (DRE) is to identify the relation
between two entities in a given dialogue. During conversations, speakers may
expose their relations to certain entities by explicit or implicit clues, such
evidences called "triggers". However, trigger annotations may not be always
available for the target data, so it is challenging to leverage such
information for enhancing the performance. Therefore, this paper proposes to
learn how to identify triggers from the data with trigger annotations and then
transfers the trigger-finding capability to other datasets for better
performance. The experiments show that the proposed approach is capable of
improving relation extraction performance of unseen relations and also
demonstrate the transferability of our proposed trigger-finding model across
different domains and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Irrationality of Neural Rationale Models. (arXiv:2110.07550v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07550">
<div class="article-summary-box-inner">
<span><p>Neural rationale models are popular for interpretable predictions of NLP
tasks. In these, a selector extracts segments of the input text, called
rationales, and passes these segments to a classifier for prediction. Since the
rationale is the only information accessible to the classifier, it is plausibly
defined as the explanation. Is such a characterization unconditionally correct?
In this paper, we argue to the contrary, with both philosophical perspectives
and empirical evidence suggesting that rationale models are, perhaps, less
rational and interpretable than expected. We call for more rigorous and
comprehensive evaluations of these models to ensure desired properties of
interpretability are indeed achieved. The code can be found at
https://github.com/yimingz89/Neural-Rationale-Analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Well Do You Know Your Audience? Toward Socially-aware Question Generation. (arXiv:2110.08445v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08445">
<div class="article-summary-box-inner">
<span><p>When writing, a person may need to anticipate questions from their audience,
but different social groups may ask very different types of questions. If
someone is writing about a problem they want to resolve, what kind of follow-up
question will a domain expert ask, and could the writer better address the
expert's information needs by rewriting their original post? In this paper, we
explore the task of socially-aware question generation. We collect a data set
of questions and posts from social media, including background information
about the question-askers' social groups. We find that different social groups,
such as experts and novices, consistently ask different types of questions. We
train several text-generation models that incorporate social information, and
we find that a discrete social-representation model outperforms the text-only
model when different social groups ask highly different questions from one
another. Our work provides a framework for developing text generation models
that can help writers anticipate the information expectations of highly
different social groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Distillation: Speeding Up Text Classification by Using Bigger Student Models. (arXiv:2110.08536v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08536">
<div class="article-summary-box-inner">
<span><p>Distilling state-of-the-art transformer models into lightweight student
models is an effective way to reduce computation cost at inference time. The
student models are typically compact transformers with fewer parameters, while
expensive operations such as self-attention persist. Therefore, the improved
inference speed may still be unsatisfactory for real-time or high-volume use
cases. In this paper, we aim to further push the limit of inference speed by
distilling teacher models into bigger, sparser student models -- bigger in that
they scale up to billions of parameters; sparser in that most of the model
parameters are n-gram embeddings. Our experiments on six single-sentence text
classification tasks show that these student models retain 97% of the
RoBERTa-Large teacher performance on average, and meanwhile achieve up to 600x
speed-up on both GPUs and CPUs at inference time. Further investigation reveals
that our pipeline is also helpful for sentence-pair classification tasks, and
in domain generalization settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13214">
<div class="article-summary-box-inner">
<span><p>Current visual question answering (VQA) tasks mainly consider answering
human-annotated questions for natural images. However, aside from natural
images, abstract diagrams with semantic richness are still understudied in
visual understanding and reasoning research. In this work, we introduce a new
challenge of Icon Question Answering (IconQA) with the goal of answering a
question in an icon image context. We release IconQA, a large-scale dataset
that consists of 107,439 questions and three sub-tasks: multi-image-choice,
multi-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by
real-world diagram word problems that highlight the importance of abstract
diagram understanding and comprehensive cognitive reasoning. Thus, IconQA
requires not only perception skills like object recognition and text
understanding, but also diverse cognitive reasoning skills, such as geometric
reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate
potential IconQA models to learn semantic representations for icon images, we
further release an icon dataset Icon645 which contains 645,687 colored icons on
377 classes. We conduct extensive user studies and blind experiments and
reproduce a wide range of advanced VQA methods to benchmark the IconQA task.
Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid
cross-modal Transformer with input diagram embeddings pre-trained on the icon
dataset. IconQA and Icon645 are available at https://iconqa.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Structured Inference with Randomization. (arXiv:2112.03638v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03638">
<div class="article-summary-box-inner">
<span><p>Deep discrete structured models have seen considerable progress recently, but
traditional inference using dynamic programming (DP) typically works with a
small number of states (less than hundreds), which severely limits model
capacity. At the same time, across machine learning, there is a recent trend of
using randomized truncation techniques to accelerate computations involving
large sums. Here, we propose a family of randomized dynamic programming (RDP)
algorithms for scaling structured models to tens of thousands of latent states.
Our method is widely applicable to classical DP-based inference (partition,
marginal, reparameterization, entropy) and different graph structures (chains,
trees, and more general hypergraphs). It is also compatible with automatic
differentiation: it can be integrated with neural networks seamlessly and
learned with gradient-based optimizers. Our core technique approximates the
sum-product by restricting and reweighting DP on a small subset of nodes, which
reduces computation by orders of magnitude. We further achieve low bias and
variance via Rao-Blackwellization and importance sampling. Experiments over
different graphs demonstrate the accuracy and efficiency of our approach.
Furthermore, when using RDP for training a structured variational autoencoder
with a scaled inference network, we achieve better test likelihood than
baselines and successfully prevent posterior collapse. code at:
https://github.com/FranxYao/RDP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal data matters: language model pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10113">
<div class="article-summary-box-inner">
<span><p>The massive amount of electronic health records (EHR) has created enormous
potential in improving healthcare. Clinical codes (structured data) and
clinical narratives (unstructured data) are two important textual modalities in
EHR. Clinical codes convey diagnostic and treatment information during the
hospital, and clinical notes carry narratives of clinical providers for patient
encounters. They do not exist in isolation and can complement each other in
most real-life clinical scenarios. However, most existing EHR-oriented studies
either focus on a particular modality or integrate data from different
modalities in a straightforward manner, which ignores the intrinsic
interactions between them. To address these issues, we proposed a Medical
Multimodal Pre-trained Language Model, named MedM-PLM, to learn enhanced EHR
representations over structured and unstructured data. In MedM-PLM, two
Transformer-based neural network components are firstly adopted to learn
representative characteristics from each modality. A cross-modal module is then
introduced to model their interactions. We pre-trained MedM-PLM on the
MIMIC-III dataset and verified the effectiveness of the model on three
downstream clinical tasks, i.e., medication recommendation, 30-day readmission
prediction and ICD coding. Extensive experiments demonstrate the power of
MedM-PLM compared with state-of-the-art methods. Further analyses and
visualizations show the robustness of our model, which could potentially
provide more comprehensive interpretations for clinical decision-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Wikipedia Help Offline Reinforcement Learning?. (arXiv:2201.12122v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12122">
<div class="article-summary-box-inner">
<span><p>Fine-tuning reinforcement learning (RL) models has been challenging because
of a lack of large scale off-the-shelf datasets as well as high variance in
transferability among different environments. Recent work has looked at
tackling offline RL from the perspective of sequence modeling with improved
results as result of the introduction of the Transformer architecture. However,
when the model is trained from scratch, it suffers from slow convergence
speeds. In this paper, we look to take advantage of this formulation of
reinforcement learning as sequence modeling and investigate the transferability
of pre-trained sequence models on other domains (vision, language) when
finetuned on offline RL tasks (control, games). To this end, we also propose
techniques to improve transfer between these domains. Results show consistent
performance gains in terms of both convergence speed and reward on a variety of
environments, accelerating training by 3-6x and achieving state-of-the-art
performance in a variety of tasks using Wikipedia-pretrained and GPT2 language
models. We hope that this work not only brings light to the potentials of
leveraging generic sequence modeling techniques and pre-trained models for RL,
but also inspires future work on sharing knowledge between generative modeling
tasks of completely different domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility. (arXiv:2202.02312v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02312">
<div class="article-summary-box-inner">
<span><p>Vision-language navigation (VLN), in which an agent follows language
instruction in a visual environment, has been studied under the premise that
the input command is fully feasible in the environment. Yet in practice, a
request may not be possible due to language ambiguity or environment changes.
To study VLN with unknown command feasibility, we introduce a new dataset
Mobile app Tasks with Iterative Feedback (MoTIF), where the goal is to complete
a natural language command in a mobile app. Mobile apps provide a scalable
domain to study real downstream uses of VLN methods. Moreover, mobile app
commands provide instruction for interactive navigation, as they result in
action sequences with state changes via clicking, typing, or swiping. MoTIF is
the first to include feasibility annotations, containing both binary
feasibility labels and fine-grained labels for why tasks are unsatisfiable. We
further collect follow-up questions for ambiguous queries to enable research on
task uncertainty resolution. Equipped with our dataset, we propose the new
problem of feasibility prediction, in which a natural language instruction and
multimodal app environment are used to predict command feasibility. MoTIF
provides a more realistic app dataset as it contains many diverse environments,
high-level goals, and longer action sequences than prior work. We evaluate
interactive VLN methods using MoTIF, quantify the generalization ability of
current approaches to new app environments, and measure the effect of task
feasibility on navigation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. (arXiv:2202.04800v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04800">
<div class="article-summary-box-inner">
<span><p>Humans have remarkable capacity to reason abductively and hypothesize about
what lies beyond the literal content of an image. By identifying concrete
visual clues scattered throughout a scene, we almost can't help but draw
probable inferences beyond the literal scene based on our everyday experience
and knowledge about the world. For example, if we see a "20 mph" sign alongside
a road, we might assume the street sits in a residential area (rather than on a
highway), even if no houses are pictured. Can machines perform similar visual
reasoning?
</p>
<p>We present Sherlock, an annotated corpus of 103K images for testing machine
capacity for abductive reasoning beyond literal image contents. We adopt a
free-viewing paradigm: participants first observe and identify salient clues
within images (e.g., objects, actions) and then provide a plausible inference
about the scene, given the clue. In total, we collect 363K (clue, inference)
pairs, which form a first-of-its-kind abductive visual reasoning dataset. Using
our corpus, we test three complementary axes of abductive reasoning. We
evaluate the capacity of models to: i) retrieve relevant inferences from a
large candidate corpus; ii) localize evidence for inferences via bounding
boxes, and iii) compare plausible inferences to match human judgments on a
newly-collected diagnostic corpus of 19K Likert-scale judgments. While we find
that fine-tuning CLIP-RN50x64 with a multitask objective outperforms strong
baselines, significant headroom exists between model performance and human
agreement. Data, models, and leaderboard available at
<a href="http://visualabduction.com/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Semantic Embeddings for Ontology Subsumption Prediction. (arXiv:2202.09791v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09791">
<div class="article-summary-box-inner">
<span><p>Automating ontology construction and curation is an important but challenging
task in knowledge engineering and artificial intelligence. Prediction by
machine learning techniques such as contextual semantic embedding is a
promising direction, but the relevant research is still preliminary especially
for expressive ontologies in Web Ontology Language (OWL). In this paper, we
present a new subsumption prediction method named BERTSubs for classes of OWL
ontology. It exploits the pre-trained language model BERT to compute contextual
embeddings of a class, where customized templates are proposed to incorporate
the class context (e.g., neighbouring classes) and the logical existential
restriction. BERTSubs is quite general, being able to predict multiple kinds of
subsumers including named classes and existential restrictions from the same
ontology or another ontology. Extensive evaluation on five real-world
ontologies for three different subsumption tasks has shown the effectiveness of
the templates and that BERTSubs can dramatically outperform the baselines that
use (literal-aware) knowledge graph embeddings, non-contextual word embeddings
and the state-of-the-art OWL ontology embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniDU: Towards A Unified Generative Dialogue Understanding Framework. (arXiv:2204.04637v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04637">
<div class="article-summary-box-inner">
<span><p>With the development of pre-trained language models, remarkable success has
been witnessed in dialogue understanding (DU). However, current DU approaches
usually employ independent models for each distinct DU task without considering
shared knowledge across different DU tasks. In this paper, we propose a unified
generative dialogue understanding framework, named {\em UniDU}, to achieve
effective information exchange across diverse DU tasks. Here, we reformulate
all DU tasks into a unified prompt-based generative model paradigm. More
importantly, a novel model-agnostic multi-task training strategy (MATS) is
introduced to dynamically adapt the weights of diverse tasks for best knowledge
sharing during training, based on the nature and available data of each task.
Experiments on ten DU datasets covering five fundamental DU tasks show that the
proposed UniDU framework largely outperforms task-specific well-designed
methods on all tasks. MATS also reveals the knowledge-sharing structure of
these tasks. Finally, UniDU obtains promising performance in the unseen
dialogue domain, showing the great potential for generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Redwood: Using Collision Detection to Grow a Large-Scale Intent Classification Dataset. (arXiv:2204.05483v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05483">
<div class="article-summary-box-inner">
<span><p>Dialog systems must be capable of incorporating new skills via updates over
time in order to reflect new use cases or deployment scenarios. Similarly,
developers of such ML-driven systems need to be able to add new training data
to an already-existing dataset to support these new skills. In intent
classification systems, problems can arise if training data for a new skill's
intent overlaps semantically with an already-existing intent. We call such
cases collisions. This paper introduces the task of intent collision detection
between multiple datasets for the purposes of growing a system's skillset. We
introduce several methods for detecting collisions, and evaluate our methods on
real datasets that exhibit collisions. To highlight the need for intent
collision detection, we show that model performance suffers if new data is
added in such a way that does not arbitrate colliding intents. Finally, we use
collision detection to construct and benchmark a new dataset, Redwood, which is
composed of 451 ntent categories from 13 original intent classification
datasets, making it the largest publicly available intent classification
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction. (arXiv:2206.05238v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05238">
<div class="article-summary-box-inner">
<span><p>The most prominent tasks in emotion analysis are to assign emotions to texts
and to understand how emotions manifest in language. An observation for NLP is
that emotions can be communicated implicitly by referring to events, appealing
to an empathetic, intersubjective understanding of events, even without
explicitly mentioning an emotion name. In psychology, the class of emotion
theories known as appraisal theories aims at explaining the link between events
and emotions. Appraisals can be formalized as variables that measure a
cognitive evaluation by people living through an event that they consider
relevant. They include the assessment if an event is novel, if the person
considers themselves to be responsible, if it is in line with the own goals,
and many others. Such appraisals explain which emotions are developed based on
an event, e.g., that a novel situation can induce surprise or one with
uncertain consequences could evoke fear. We analyze the suitability of
appraisal theories for emotion analysis in text with the goal of understanding
if appraisal concepts can reliably be reconstructed by annotators, if they can
be predicted by text classifiers, and if appraisal concepts help to identify
emotion categories. To achieve that, we compile a corpus by asking people to
textually describe events that triggered particular emotions and to disclose
their appraisals. Then, we ask readers to reconstruct emotions and appraisals
from the text. This setup allows us to measure if emotions and appraisals can
be recovered purely from text and provides a human baseline. Our comparison of
text classification methods to human annotators shows that both can reliably
detect emotions and appraisals with similar performance. Therefore, appraisals
constitute an alternative computational emotion analysis paradigm and further
improve the categorization of emotions in text with joint models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. (arXiv:2207.01780v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01780">
<div class="article-summary-box-inner">
<span><p>Program synthesis or code generation aims to generate a program that
satisfies a problem specification. Recent approaches using large-scale
pretrained language models (LMs) have shown promising results, yet they have
some critical limitations. In particular, they often follow a standard
supervised fine-tuning procedure to train a code generation model only from the
pairs of natural-language problem descriptions and ground-truth programs. Such
paradigm largely ignores some important but potentially useful signals in the
problem specification such as unit tests, which thus often results in poor
performance when solving complex unseen coding tasks. To address the
limitations, we propose "CodeRL", a new framework for program synthesis tasks
through pretrained LMs and deep reinforcement learning (RL). Specifically,
during training, we treat the code-generating LM as an actor network, and
introduce a critic network that is trained to predict the functional
correctness of generated programs and provide dense feedback signals to the
actor. During inference, we introduce a new generation procedure with a
critical sampling strategy that allows a model to automatically regenerate
programs based on feedback from example unit tests and critic scores. For the
model backbones, we extended the encoder-decoder architecture of CodeT5 with
enhanced learning objectives, larger model sizes, and better pretraining data.
Our method not only achieves new SOTA results on the challenging APPS
benchmark, but also shows strong zero-shot transfer capability with new SOTA
results on the simpler MBPP benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Whois? Deep Author Name Disambiguation using Bibliographic Data. (arXiv:2207.04772v2 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04772">
<div class="article-summary-box-inner">
<span><p>As the number of authors is increasing exponentially over years, the number
of authors sharing the same names is increasing proportionally. This makes it
challenging to assign newly published papers to their adequate authors.
Therefore, Author Name Ambiguity (ANA) is considered a critical open problem in
digital libraries. This paper proposes an Author Name Disambiguation (AND)
approach that links author names to their real-world entities by leveraging
their co-authors and domain of research. To this end, we use a collection from
the DBLP repository that contains more than 5 million bibliographic records
authored by around 2.6 million co-authors. Our approach first groups authors
who share the same last names and same first name initials. The author within
each group is identified by capturing the relation with his/her co-authors and
area of research, which is represented by the titles of the validated
publications of the corresponding author. To this end, we train a neural
network model that learns from the representations of the co-authors and
titles. We validated the effectiveness of our approach by conducting extensive
experiments on a large dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ILASR: Privacy-Preserving Incremental Learning for Automatic Speech Recognition at Production Scale. (arXiv:2207.09078v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09078">
<div class="article-summary-box-inner">
<span><p>Incremental learning is one paradigm to enable model building and updating at
scale with streaming data. For end-to-end automatic speech recognition (ASR)
tasks, the absence of human annotated labels along with the need for privacy
preserving policies for model building makes it a daunting challenge. Motivated
by these challenges, in this paper we use a cloud based framework for
production systems to demonstrate insights from privacy preserving incremental
learning for automatic speech recognition (ILASR). By privacy preserving, we
mean, usage of ephemeral data which are not human annotated. This system is a
step forward for production levelASR models for incremental/continual learning
that offers near real-time test-bed for experimentation in the cloud for
end-to-end ASR, while adhering to privacy-preserving policies. We show that the
proposed system can improve the production models significantly(3%) over a new
time period of six months even in the absence of human annotated labels with
varying levels of weak supervision and large batch sizes in incremental
learning. This improvement is 20% over test sets with new words and phrases in
the new time period. We demonstrate the effectiveness of model building in a
privacy-preserving incremental fashion for ASR while further exploring the
utility of having an effective teacher model and use of large batch sizes.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Brain tumor detection using artificial convolutional neural networks. (arXiv:2207.11248v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11248">
<div class="article-summary-box-inner">
<span><p>In this paper, a convolutional neural network (CNN) was used to classify NMR
images of human brains with 4 different types of tumors: meningioma, glioma and
pituitary gland tumors. During the training phase of this project, an accuracy
of 100% was obtained, meanwhile, in the evaluation phase the precision was 96%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rich Feature Distillation with Feature Affinity Module for Efficient Image Dehazing. (arXiv:2207.11250v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11250">
<div class="article-summary-box-inner">
<span><p>Single-image haze removal is a long-standing hurdle for computer vision
applications. Several works have been focused on transferring advances from
image classification, detection, and segmentation to the niche of image
dehazing, primarily focusing on contrastive learning and knowledge
distillation. However, these approaches prove computationally expensive,
raising concern regarding their applicability to on-the-edge use-cases. This
work introduces a simple, lightweight, and efficient framework for single-image
haze removal, exploiting rich "dark-knowledge" information from a lightweight
pre-trained super-resolution model via the notion of heterogeneous knowledge
distillation. We designed a feature affinity module to maximize the flow of
rich feature semantics from the super-resolution teacher to the student
dehazing network. In order to evaluate the efficacy of our proposed framework,
its performance as a plug-and-play setup to a baseline model is examined. Our
experiments are carried out on the RESIDE-Standard dataset to demonstrate the
robustness of our framework to the synthetic and real-world domains. The
extensive qualitative and quantitative results provided establish the
effectiveness of the framework, achieving gains of upto 15\% (PSNR) while
reducing the model size by $\sim$20 times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation. (arXiv:2207.11325v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11325">
<div class="article-summary-box-inner">
<span><p>In order to cope with the increasing demand for labeling data and privacy
issues with human detection, synthetic data has been used as a substitute and
showing promising results in human detection and tracking tasks. We participate
in the 7th Workshop on Benchmarking Multi-Target Tracking (BMTT), themed on
"How Far Can Synthetic Data Take us"? Our solution, PieTrack, is developed
based on synthetic data without using any pre-trained weights. We propose a
self-supervised domain adaptation method that enables mitigating the domain
shift issue between the synthetic (e.g., MOTSynth) and real data (e.g., MOT17)
without involving extra human labels. By leveraging the proposed multi-scale
ensemble inference, we achieved a final HOTA score of 58.7 on the MOT17 testing
set, ranked third place in the challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Swin Transformers for Egocentric Video Understanding @ Ego4D Challenges 2022. (arXiv:2207.11329v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11329">
<div class="article-summary-box-inner">
<span><p>We implemented Video Swin Transformer as a base architecture for the tasks of
Point-of-No-Return temporal localization and Object State Change
Classification. Our method achieved competitive performance on both challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Graph Reasoning for Multi-person 3D Pose Estimation. (arXiv:2207.11341v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11341">
<div class="article-summary-box-inner">
<span><p>Multi-person 3D pose estimation is a challenging task because of occlusion
and depth ambiguity, especially in the cases of crowd scenes. To solve these
problems, most existing methods explore modeling body context cues by enhancing
feature representation with graph neural networks or adding structural
constraints. However, these methods are not robust for their single-root
formulation that decoding 3D poses from a root node with a pre-defined graph.
In this paper, we propose GR-M3D, which models the \textbf{M}ulti-person
\textbf{3D} pose estimation with dynamic \textbf{G}raph \textbf{R}easoning. The
decoding graph in GR-M3D is predicted instead of pre-defined. In particular, It
firstly generates several data maps and enhances them with a scale and depth
aware refinement module (SDAR). Then multiple root keypoints and dense decoding
paths for each person are estimated from these data maps. Based on them,
dynamic decoding graphs are built by assigning path weights to the decoding
paths, while the path weights are inferred from those enhanced data maps. And
this process is named dynamic graph reasoning (DGR). Finally, the 3D poses are
decoded according to dynamic decoding graphs for each detected person. GR-M3D
can adjust the structure of the decoding graph implicitly by adopting soft path
weights according to input data, which makes the decoding graphs be adaptive to
different input persons to the best extent and more capable of handling
occlusion and depth ambiguity than previous methods. We empirically show that
the proposed bottom-up approach even outperforms top-down methods and achieves
state-of-the-art results on three 3D pose datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Impartial Take to the CNN vs Transformer Robustness Contest. (arXiv:2207.11347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11347">
<div class="article-summary-box-inner">
<span><p>Following the surge of popularity of Transformers in Computer Vision, several
studies have attempted to determine whether they could be more robust to
distribution shifts and provide better uncertainty estimates than Convolutional
Neural Networks (CNNs). The almost unanimous conclusion is that they are, and
it is often conjectured more or less explicitly that the reason of this
supposed superiority is to be attributed to the self-attention mechanism. In
this paper we perform extensive empirical analyses showing that recent
state-of-the-art CNNs (particularly, ConvNeXt) can be as robust and reliable or
even sometimes more than the current state-of-the-art Transformers. However,
there is no clear winner. Therefore, although it is tempting to state the
definitive superiority of one family of architectures over another, they seem
to enjoy similar extraordinary performances on a variety of tasks while also
suffering from similar vulnerabilities such as texture, background, and
simplicity biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep neural network heatmaps capture Alzheimer's disease patterns reported in a large meta-analysis of neuroimaging studies. (arXiv:2207.11352v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11352">
<div class="article-summary-box-inner">
<span><p>Deep neural networks currently provide the most advanced and accurate machine
learning models to distinguish between structural MRI scans of subjects with
Alzheimer's disease and healthy controls. Unfortunately, the subtle brain
alterations captured by these models are difficult to interpret because of the
complexity of these multi-layer and non-linear models. Several heatmap methods
have been proposed to address this issue and analyze the imaging patterns
extracted from the deep neural networks, but no quantitative comparison between
these methods has been carried out so far. In this work, we explore these
questions by deriving heatmaps from Convolutional Neural Networks (CNN) trained
using T1 MRI scans of the ADNI data set, and by comparing these heatmaps with
brain maps corresponding to Support Vector Machines (SVM) coefficients. Three
prominent heatmap methods are studied: Layer-wise Relevance Propagation (LRP),
Integrated Gradients (IG), and Guided Grad-CAM (GGC). Contrary to prior studies
where the quality of heatmaps was visually or qualitatively assessed, we
obtained precise quantitative measures by computing overlap with a ground-truth
map from a large meta-analysis that combined 77 voxel-based morphometry (VBM)
studies independently from ADNI. Our results indicate that all three heatmap
methods were able to capture brain regions covering the meta-analysis map and
achieved better results than SVM coefficients. Among them, IG produced the
heatmaps with the best overlap with the independent meta-analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Egocentric scene context for human-centric environment understanding from video. (arXiv:2207.11365v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11365">
<div class="article-summary-box-inner">
<span><p>First-person video highlights a camera-wearer's activities in the context of
their persistent environment. However, current video understanding approaches
reason over visual features from short video clips that are detached from the
underlying physical space and only capture what is directly seen. We present an
approach that links egocentric video and camera pose over time by learning
representations that are predictive of the camera-wearer's (potentially unseen)
local surroundings to facilitate human-centric environment understanding. We
train such models using videos from agents in simulated 3D environments where
the environment is fully observable, and test them on real-world videos of
house tours from unseen environments. We show that by grounding videos in their
physical environment, our models surpass traditional scene classification
models at predicting which room a camera-wearer is in (where frame-level
information is insufficient), and can leverage this grounding to localize video
moments corresponding to environment-centric queries, outperforming prior
methods. Project page: <a href="http://vision.cs.utexas.edu/projects/ego-scene-context/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural-Sim: Learning to Generate Training Data with NeRF. (arXiv:2207.11368v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11368">
<div class="article-summary-box-inner">
<span><p>Training computer vision models usually requires collecting and labeling vast
amounts of imagery under a diverse set of scene configurations and properties.
This process is incredibly time-consuming, and it is challenging to ensure that
the captured data distribution maps well to the target domain of an application
scenario. Recently, synthetic data has emerged as a way to address both of
these issues. However, existing approaches either require human experts to
manually tune each scene property or use automatic methods that provide little
to no control; this requires rendering large amounts of random data variations,
which is slow and is often suboptimal for the target domain. We present the
first fully differentiable synthetic data pipeline that uses Neural Radiance
Fields (NeRFs) in a closed-loop with a target application's loss function. Our
approach generates data on-demand, with no human labor, to maximize accuracy
for a target task. We illustrate the effectiveness of our method on synthetic
and real-world object detection tasks. We also introduce a new
"YCB-in-the-Wild" dataset and benchmark that provides a test scenario for
object detection with varied poses in real-world environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Different Annotation Strategies for Deployment of Parking Spaces Classification Systems. (arXiv:2207.11372v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11372">
<div class="article-summary-box-inner">
<span><p>When using vision-based approaches to classify individual parking spaces
between occupied and empty, human experts often need to annotate the locations
and label a training set containing images collected in the target parking lot
to fine-tune the system. We propose investigating three annotation types
(polygons, bounding boxes, and fixed-size squares), providing different data
representations of the parking spaces. The rationale is to elucidate the best
trade-off between handcraft annotation precision and model performance. We also
investigate the number of annotated parking spaces necessary to fine-tune a
pre-trained model in the target parking lot. Experiments using the PKLot
dataset show that it is possible to fine-tune a model to the target parking lot
with less than 1,000 labeled samples, using low precision annotations such as
fixed-size squares.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Perceptually Aligned Gradients Imply Adversarial Robustness?. (arXiv:2207.11378v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11378">
<div class="article-summary-box-inner">
<span><p>In the past decade, deep learning-based networks have achieved unprecedented
success in numerous tasks, including image classification. Despite this
remarkable achievement, recent studies have demonstrated that such networks are
easily fooled by small malicious perturbations, also known as adversarial
examples. This security weakness led to extensive research aimed at obtaining
robust models. Beyond the clear robustness benefits of such models, it was also
observed that their gradients with respect to the input align with human
perception. Several works have identified Perceptually Aligned Gradients (PAG)
as a byproduct of robust training, but none have considered it as a standalone
phenomenon nor studied its own implications. In this work, we focus on this
trait and test whether Perceptually Aligned Gradients imply Robustness. To this
end, we develop a novel objective to directly promote PAG in training
classifiers and examine whether models with such gradients are more robust to
adversarial attacks. Extensive experiments on CIFAR-10 and STL validate that
such models have improved robust performance, exposing the surprising
bidirectional connection between PAG and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Aspect Information Fusion Model For ABAW4 Multi-task Challenge. (arXiv:2207.11389v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11389">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose the solution to the Multi-Task Learning (MTL)
Challenge of the 4th Affective Behavior Analysis in-the-wild (ABAW)
competition. The task of ABAW is to predict frame-level emotion descriptors
from videos: discrete emotional state; valence and arousal; and action units.
Although researchers have proposed several approaches and achieved promising
results in ABAW, current works in this task rarely consider interactions
between different emotion descriptors. To this end, we propose a novel end to
end architecture to achieve full integration of different types of information.
Experimental results demonstrate the effectiveness of our proposed solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Pneumonia: Attention-Based Contrastive Learning for Class-Imbalanced Pneumonia Lesion Recognition in Chest X-rays. (arXiv:2207.11393v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11393">
<div class="article-summary-box-inner">
<span><p>Computer-aided X-ray pneumonia lesion recognition is important for accurate
diagnosis of pneumonia. With the emergence of deep learning, the identification
accuracy of pneumonia has been greatly improved, but there are still some
challenges due to the fuzzy appearance of chest X-rays. In this paper, we
propose a deep learning framework named Attention-Based Contrastive Learning
for Class-Imbalanced X-Ray Pneumonia Lesion Recognition (denoted as Deep
Pneumonia). We adopt self-supervised contrastive learning strategy to pre-train
the model without using extra pneumonia data for fully mining the limited
available dataset. In order to leverage the location information of the lesion
area that the doctor has painstakingly marked, we propose mask-guided hard
attention strategy and feature learning with contrastive regulation strategy
which are applied on the attention map and the extracted features respectively
to guide the model to focus more attention on the lesion area where contains
more discriminative features for improving the recognition performance. In
addition, we adopt Class-Balanced Loss instead of traditional Cross-Entropy as
the loss function of classification to tackle the problem of serious class
imbalance between different classes of pneumonia in the dataset. The
experimental results show that our proposed framework can be used as a reliable
computer-aided pneumonia diagnosis system to assist doctors to better diagnose
pneumonia cases accurately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Orientation and Context Entangled Network for Retinal Vessel Segmentation. (arXiv:2207.11396v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11396">
<div class="article-summary-box-inner">
<span><p>Most of the existing deep learning based methods for vessel segmentation
neglect two important aspects of retinal vessels, one is the orientation
information of vessels, and the other is the contextual information of the
whole fundus region. In this paper, we propose a robust Orientation and Context
Entangled Network (denoted as OCE-Net), which has the capability of extracting
complex orientation and context information of the blood vessels. To achieve
complex orientation aware, a Dynamic Complex Orientation Aware Convolution
(DCOA Conv) is proposed to extract complex vessels with multiple orientations
for improving the vessel continuity. To simultaneously capture the global
context information and emphasize the important local information, a Global and
Local Fusion Module (GLFM) is developed to simultaneously model the long-range
dependency of vessels and focus sufficient attention on local thin vessels. A
novel Orientation and Context Entangled Non-local (OCE-NL) module is proposed
to entangle the orientation and context information together. In addition, an
Unbalanced Attention Refining Module (UARM) is proposed to deal with the
unbalanced pixel numbers of background, thick and thin vessels. Extensive
experiments were performed on several commonly used datasets (DRIVE, STARE and
CHASEDB1) and some more challenging datasets (AV-WIDE, UoA-DR, RFMiD and UK
Biobank). The ablation study shows that the proposed method achieves promising
performance on maintaining the continuity of thin vessels and the comparative
experiments demonstrate that our OCE-Net can achieve state-of-the-art
performance on retinal vessel segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations. (arXiv:2207.11401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11401">
<div class="article-summary-box-inner">
<span><p>Visual Entailment with natural language explanations aims to infer the
relationship between a text-image pair and generate a sentence to explain the
decision-making process. Previous methods rely mainly on a pre-trained
vision-language model to perform the relation inference and a language model to
generate the corresponding explanation. However, the pre-trained
vision-language models mainly build token-level alignment between text and
image yet ignore the high-level semantic alignment between the phrases (chunks)
and visual contents, which is critical for vision-language reasoning. Moreover,
the explanation generator based only on the encoded joint representation does
not explicitly consider the critical decision-making points of relation
inference. Thus the generated explanations are less faithful to visual-language
reasoning. To mitigate these problems, we propose a unified Chunk-aware
Alignment and Lexical Constraint based method, dubbed as CALeC. It contains a
Chunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical
Constraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence
structure inherent in language and various image regions to build chunk-aware
semantic alignment. Relation inferrer uses an attention-based reasoning network
to incorporate the token-level and chunk-level vision-language representations.
LeCG utilizes lexical constraints to expressly incorporate the words or chunks
focused by the relation inferrer into explanation generation, improving the
faithfulness and informativeness of the explanations. We conduct extensive
experiments on three datasets, and experimental results indicate that CALeC
significantly outperforms other competitor models on inference accuracy and
quality of generated explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PS-NeRF: Neural Inverse Rendering for Multi-view Photometric Stereo. (arXiv:2207.11406v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11406">
<div class="article-summary-box-inner">
<span><p>Traditional multi-view photometric stereo (MVPS) methods are often composed
of multiple disjoint stages, resulting in noticeable accumulated errors. In
this paper, we present a neural inverse rendering method for MVPS based on
implicit representation. Given multi-view images of a non-Lambertian object
illuminated by multiple unknown directional lights, our method jointly
estimates the geometry, materials, and lights. Our method first employs
multi-light images to estimate per-view surface normal maps, which are used to
regularize the normals derived from the neural radiance field. It then jointly
optimizes the surface normals, spatially-varying BRDFs, and lights based on a
shadow-aware differentiable rendering layer. After optimization, the
reconstructed object can be used for novel-view rendering, relighting, and
material editing. Experiments on both synthetic and real datasets demonstrate
that our method achieves far more accurate shape reconstruction than existing
MVPS and neural rendering methods. Our code and model can be found at
https://ywq.github.io/psnerf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Halftoning with Multi-Agent Deep Reinforcement Learning. (arXiv:2207.11408v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11408">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have recently succeeded in digital halftoning using
vanilla convolutional layers with high parallelism. However, existing deep
methods fail to generate halftones with a satisfying blue-noise property and
require complex training schemes. In this paper, we propose a halftoning method
based on multi-agent deep reinforcement learning, called HALFTONERS, which
learns a shared policy to generate high-quality halftone images. Specifically,
we view the decision of each binary pixel value as an action of a virtual
agent, whose policy is trained by a low-variance policy gradient. Moreover, the
blue-noise property is achieved by a novel anisotropy suppressing loss
function. Experiments show that our halftoning method produces high-quality
halftones while staying relatively fast.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Satellite Detection in Unresolved Space Imagery for Space Domain Awareness Using Neural Networks. (arXiv:2207.11412v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11412">
<div class="article-summary-box-inner">
<span><p>This work utilizes a MobileNetV2 Convolutional Neural Network (CNN) for fast,
mobile detection of satellites, and rejection of stars, in cluttered unresolved
space imagery. First, a custom database is created using imagery from a
synthetic satellite image program and labeled with bounding boxes over
satellites for "satellite-positive" images. The CNN is then trained on this
database and the inference is validated by checking the accuracy of the model
on an external dataset constructed of real telescope imagery. In doing so, the
trained CNN provides a method of rapid satellite identification for subsequent
utilization in ground-based orbit estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection and Initial Assessment of Lunar Landing Sites Using Neural Networks. (arXiv:2207.11413v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11413">
<div class="article-summary-box-inner">
<span><p>Robotic and human lunar landings are a focus of future NASA missions.
Precision landing capabilities are vital to guarantee the success of the
mission, and the safety of the lander and crew. During the approach to the
surface there are multiple challenges associated with Hazard Relative
Navigation to ensure safe landings. This paper will focus on a passive
autonomous hazard detection and avoidance sub-system to generate an initial
assessment of possible landing regions for the guidance system. The system uses
a single camera and the MobileNetV2 neural network architecture to detect and
discern between safe landing sites and hazards such as rocks, shadows, and
craters. Then a monocular structure from motion will recreate the surface to
provide slope and roughness analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arbitrary Style Transfer with Structure Enhancement by Combining the Global and Local Loss. (arXiv:2207.11438v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11438">
<div class="article-summary-box-inner">
<span><p>Arbitrary style transfer generates an artistic image which combines the
structure of a content image and the artistic style of the artwork by using
only one trained network. The image representation used in this method contains
content structure representation and the style patterns representation, which
is usually the features representation of high-level in the pre-trained
classification networks. However, the traditional classification networks were
designed for classification which usually focus on high-level features and
ignore other features. As the result, the stylized images distribute style
elements evenly throughout the image and make the overall image structure
unrecognizable. To solve this problem, we introduce a novel arbitrary style
transfer method with structure enhancement by combining the global and local
loss. The local structure details are represented by Lapstyle and the global
structure is controlled by the image depth. Experimental results demonstrate
that our method can generate higher-quality images with impressive visual
effects on several common datasets, comparing with other state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Spatio-Temporal Debiasing for Video Scene Graph Generation. (arXiv:2207.11441v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11441">
<div class="article-summary-box-inner">
<span><p>Video scene graph generation (VidSGG) aims to parse the video content into
scene graphs, which involves modeling the spatio-temporal contextual
information in the video. However, due to the long-tailed training data in
datasets, the generalization performance of existing VidSGG models can be
affected by the spatio-temporal conditional bias problem. In this work, from
the perspective of meta-learning, we propose a novel Meta Video Scene Graph
Generation (MVSGG) framework to address such a bias problem. Specifically, to
handle various types of spatio-temporal conditional biases, our framework first
constructs a support set and a group of query sets from the training data,
where the data distribution of each query set is different from that of the
support set w.r.t. a type of conditional bias. Then, by performing a novel meta
training and testing process to optimize the model to obtain good testing
performance on these query sets after training on the support set, our
framework can effectively guide the model to learn to well generalize against
biases. Extensive experiments demonstrate the efficacy of our proposed
framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BuyTheDips: PathLoss for improved topology-preserving deep learning-based image segmentation. (arXiv:2207.11446v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11446">
<div class="article-summary-box-inner">
<span><p>Capturing the global topology of an image is essential for proposing an
accurate segmentation of its domain. However, most of existing segmentation
methods do not preserve the initial topology of the given input, which is
detrimental for numerous downstream object-based tasks. This is all the more
true for deep learning models which most work at local scales. In this paper,
we propose a new topology-preserving deep image segmentation method which
relies on a new leakage loss: the Pathloss. Our method is an extension of the
BALoss [1], in which we want to improve the leakage detection for better
recovering the closeness property of the image segmentation. This loss allows
us to correctly localize and fix the critical points (a leakage in the
boundaries) that could occur in the predictions, and is based on a
shortest-path search algorithm. This way, loss minimization enforces
connectivity only where it is necessary and finally provides a good
localization of the boundaries of the objects in the image. Moreover, according
to our research, our Pathloss learns to preserve stronger elongated structure
compared to methods without using topology-preserving loss. Training with our
topological loss function, our method outperforms state-of-the-art
topology-aware methods on two representative datasets of different natures:
Electron Microscopy and Historical Map.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UC-OWOD: Unknown-Classified Open World Object Detection. (arXiv:2207.11455v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11455">
<div class="article-summary-box-inner">
<span><p>Open World Object Detection (OWOD) is a challenging computer vision problem
that requires detecting unknown objects and gradually learning the identified
unknown classes. However, it cannot distinguish unknown instances as multiple
unknown classes. In this work, we propose a novel OWOD problem called
Unknown-Classified Open World Object Detection (UC-OWOD). UC-OWOD aims to
detect unknown instances and classify them into different unknown classes.
Besides, we formulate the problem and devise a two-stage object detector to
solve UC-OWOD. First, unknown label-aware proposal and unknown-discriminative
classification head are used to detect known and unknown objects. Then,
similarity-based unknown classification and unknown clustering refinement
modules are constructed to distinguish multiple unknown classes. Moreover, two
novel evaluation protocols are designed to evaluate unknown-class detection.
Abundant experiments and visualizations prove the effectiveness of the proposed
method. Code is available at https://github.com/JohnWuzh/UC-OWOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2207.11463v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11463">
<div class="article-summary-box-inner">
<span><p>Recently, most handwritten mathematical expression recognition (HMER) methods
adopt the encoder-decoder networks, which directly predict the markup sequences
from formula images with the attention mechanism. However, such methods may
fail to accurately read formulas with complicated structure or generate long
markup sequences, as the attention results are often inaccurate due to the
large variance of writing styles or spatial layouts. To alleviate this problem,
we propose an unconventional network for HMER named Counting-Aware Network
(CAN), which jointly optimizes two tasks: HMER and symbol counting.
Specifically, we design a weakly-supervised counting module that can predict
the number of each symbol class without the symbol-level position annotations,
and then plug it into a typical attention-based encoder-decoder model for HMER.
Experiments on the benchmark datasets for HMER validate that both joint
optimization and counting results are beneficial for correcting the prediction
errors of encoder-decoder models, and CAN consistently outperforms the
state-of-the-art methods. In particular, compared with an encoder-decoder model
for HMER, the extra time cost caused by the proposed counting module is
marginal. The source code is available at https://github.com/LBH1024/CAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Object Placement via Dual-path Graph Completion. (arXiv:2207.11464v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11464">
<div class="article-summary-box-inner">
<span><p>Object placement aims to place a foreground object over a background image
with a suitable location and size. In this work, we treat object placement as a
graph completion problem and propose a novel graph completion module (GCM). The
background scene is represented by a graph with multiple nodes at different
spatial locations with various receptive fields. The foreground object is
encoded as a special node that should be inserted at a reasonable place in this
graph. We also design a dual-path framework upon the structure of GCM to fully
exploit annotated composite images. With extensive experiments on OPA dataset,
our method proves to significantly outperform existing methods in generating
plausible object placement without loss of diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CompNVS: Novel View Synthesis with Scene Completion. (arXiv:2207.11467v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11467">
<div class="article-summary-box-inner">
<span><p>We introduce a scalable framework for novel view synthesis from RGB-D images
with largely incomplete scene coverage. While generative neural approaches have
demonstrated spectacular results on 2D images, they have not yet achieved
similar photorealistic results in combination with scene completion where a
spatial 3D scene understanding is essential. To this end, we propose a
generative pipeline performing on a sparse grid-based neural scene
representation to complete unobserved scene parts via a learned distribution of
scenes in a 2.5D-3D-2.5D manner. We process encoded image features in 3D space
with a geometry completion network and a subsequent texture inpainting network
to extrapolate the missing area. Photorealistic image sequences can be finally
obtained via consistency-relevant differentiable rendering. Comprehensive
experiments show that the graphical outputs of our method outperform the state
of the art, especially within unobserved scene parts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Scene Text Erasing with Self-Supervision. (arXiv:2207.11469v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11469">
<div class="article-summary-box-inner">
<span><p>Scene text erasing seeks to erase text contents from scene images and current
state-of-the-art text erasing models are trained on large-scale synthetic data.
Although data synthetic engines can provide vast amounts of annotated training
samples, there are differences between synthetic and real-world data. In this
paper, we employ self-supervision for feature representation on unlabeled
real-world scene text images. A novel pretext task is designed to keep
consistent among text stroke masks of image variants. We design the Progressive
Erasing Network in order to remove residual texts. The scene text is erased
progressively by leveraging the intermediate generated results which provide
the foundation for subsequent higher quality results. Experiments show that our
method significantly improves the generalization of the text erasing task and
achieves state-of-the-art performance on public benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Labeling Tool. (arXiv:2207.11479v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11479">
<div class="article-summary-box-inner">
<span><p>Training and testing supervised object detection models require a large
collection of images with ground truth labels. Labels define object classes in
the image, as well as their locations, shape, and possibly other information
such as pose. The labeling process has proven extremely time consuming, even
with the presence of manpower. We introduce a novel labeling tool for 2D images
as well as 3D triangular meshes: 3D Labeling Tool (3DLT). This is a standalone,
feature-heavy and cross-platform software that does not require installation
and can run on Windows, macOS and Linux-based distributions. Instead of
labeling the same object on every image separately like current tools, we use
depth information to reconstruct a triangular mesh from said images and label
the object only once on the aforementioned mesh. We use registration to
simplify 3D labeling, outlier detection to improve 2D bounding box calculation
and surface reconstruction to expand labeling possibility to large point
clouds. Our tool is tested against state of the art methods and it greatly
surpasses them in terms of speed while preserving accuracy and ease of use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Emotion Recognition with Modality-Pairwise Unsupervised Contrastive Loss. (arXiv:2207.11482v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11482">
<div class="article-summary-box-inner">
<span><p>Emotion recognition is involved in several real-world applications. With an
increase in available modalities, automatic understanding of emotions is being
performed more accurately. The success in Multimodal Emotion Recognition (MER),
primarily relies on the supervised learning paradigm. However, data annotation
is expensive, time-consuming, and as emotion expression and perception depends
on several factors (e.g., age, gender, culture) obtaining labels with a high
reliability is hard. Motivated by these, we focus on unsupervised feature
learning for MER. We consider discrete emotions, and as modalities text, audio
and vision are used. Our method, as being based on contrastive loss between
pairwise modalities, is the first attempt in MER literature. Our end-to-end
feature learning approach has several differences (and advantages) compared to
existing MER methods: i) it is unsupervised, so the learning is lack of data
labelling cost; ii) it does not require data spatial augmentation, modality
alignment, large number of batch size or epochs; iii) it applies data fusion
only at inference; and iv) it does not require backbones pre-trained on emotion
recognition task. The experiments on benchmark datasets show that our method
outperforms several baseline approaches and unsupervised learning methods
applied in MER. Particularly, it even surpasses a few supervised MER
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphFit: Learning Multi-scale Graph-Convolutional Representation for Point Cloud Normal Estimation. (arXiv:2207.11484v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11484">
<div class="article-summary-box-inner">
<span><p>We propose a precise and efficient normal estimation method that can deal
with noise and nonuniform density for unstructured 3D point clouds. Unlike
existing approaches that directly take patches and ignore the local
neighborhood relationships, which make them susceptible to challenging regions
such as sharp edges, we propose to learn graph convolutional feature
representation for normal estimation, which emphasizes more local neighborhood
geometry and effectively encodes intrinsic relationships. Additionally, we
design a novel adaptive module based on the attention mechanism to integrate
point features with their neighboring features, hence further enhancing the
robustness of the proposed normal estimator against point density variations.
To make it more distinguishable, we introduce a multi-scale architecture in the
graph block to learn richer geometric features. Our method outperforms
competitors with the state-of-the-art accuracy on various benchmark datasets,
and is quite robust against noise, outliers, as well as the density variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Pointly-Supervised Instance Segmentation. (arXiv:2207.11493v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11493">
<div class="article-summary-box-inner">
<span><p>The requirement of expensive annotations is a major burden for training a
well-performed instance segmentation model. In this paper, we present an
economic active learning setting, named active pointly-supervised instance
segmentation (APIS), which starts with box-level annotations and iteratively
samples a point within the box and asks if it falls on the object. The key of
APIS is to find the most desirable points to maximize the segmentation accuracy
with limited annotation budgets. We formulate this setting and propose several
uncertainty-based sampling strategies. The model developed with these
strategies yields consistent performance gain on the challenging MS-COCO
dataset, compared against other learning strategies. The results suggest that
APIS, integrating the advantages of active learning and point-based
supervision, is an effective learning paradigm for label-efficient instance
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intelligent 3D Network Protocol for Multimedia Data Classification using Deep Learning. (arXiv:2207.11504v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11504">
<div class="article-summary-box-inner">
<span><p>In videos, the human's actions are of three-dimensional (3D) signals. These
videos investigate the spatiotemporal knowledge of human behavior. The
promising ability is investigated using 3D convolution neural networks (CNNs).
The 3D CNNs have not yet achieved high output for their well-established
two-dimensional (2D) equivalents in still photographs. Board 3D Convolutional
Memory and Spatiotemporal fusion face training difficulty preventing 3D CNN
from accomplishing remarkable evaluation. In this paper, we implement Hybrid
Deep Learning Architecture that combines STIP and 3D CNN features to enhance
the performance of 3D videos effectively. After implementation, the more
detailed and deeper charting for training in each circle of space-time fusion.
The training model further enhances the results after handling complicated
evaluations of models. The video classification model is used in this
implemented model. Intelligent 3D Network Protocol for Multimedia Data
Classification using Deep Learning is introduced to further understand
spacetime association in human endeavors. In the implementation of the result,
the well-known dataset, i.e., UCF101 to, evaluates the performance of the
proposed hybrid technique. The results beat the proposed hybrid technique that
substantially beats the initial 3D CNNs. The results are compared with
state-of-the-art frameworks from literature for action recognition on UCF101
with an accuracy of 95%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSBNet: Improving Visual Recognition Efficiency by Adaptive Sampling. (arXiv:2207.11511v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11511">
<div class="article-summary-box-inner">
<span><p>Downsampling is widely adopted to achieve a good trade-off between accuracy
and latency for visual recognition. Unfortunately, the commonly used pooling
layers are not learned, and thus cannot preserve important information. As
another dimension reduction method, adaptive sampling weights and processes
regions that are relevant to the task, and is thus able to better preserve
useful information. However, the use of adaptive sampling has been limited to
certain layers. In this paper, we show that using adaptive sampling in the
building blocks of a deep neural network can improve its efficiency. In
particular, we propose SSBNet which is built by inserting sampling layers
repeatedly into existing networks like ResNet. Experiment results show that the
proposed SSBNet can achieve competitive image classification and object
detection performance on ImageNet and COCO datasets. For example, the
SSB-ResNet-RS-200 achieved 82.6% accuracy on ImageNet dataset, which is 0.6%
higher than the baseline ResNet-RS-152 with a similar complexity. Visualization
shows the advantage of SSBNet in allowing different layers to focus on
different positions, and ablation studies further validate the advantage of
adaptive sampling over uniform methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Hybrid Architecture and Pseudo-label for Semi-supervised Abdominal Organ Segmentation. (arXiv:2207.11512v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11512">
<div class="article-summary-box-inner">
<span><p>Abdominal organ segmentation has many important clinical applications, such
as organ quantification, surgical planning, and disease diagnosis. However,
manually annotating organs from CT scans is time-consuming and labor-intensive.
Semi-supervised learning has shown the potential to alleviate this challenge by
learning from a large set of unlabeled images and limited labeled samples. In
this work, we follow the self-training strategy and employ a hybrid
architecture (PHTrans) with CNN and Transformer for both teacher and student
models to generate precise pseudo-labels. Afterward, we introduce them with
label data together into a two-stage segmentation framework with lightweight
PHTrans for training to improve the performance and generalization ability of
the model while remaining efficient. Experiments on the validation set of
FLARE2022 demonstrate that our method achieves excellent segmentation
performance as well as fast and low-resource model inference. The average DSC
and HSD are 0.8956 and 0.9316, respectively. Under our development
environments, the average inference time is 18.62 s, the average maximum GPU
memory is 1995.04 MB, and the area under the GPU memory-time curve and the
average area under the CPU utilization-time curve are 23196.84 and 319.67.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models. (arXiv:2207.11514v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11514">
<div class="article-summary-box-inner">
<span><p>We study open-world 3D scene understanding, a family of tasks that require
agents to reason about their 3D environment with an open-set vocabulary and
out-of-domain visual inputs - a critical skill for robots to operate in the
unstructured 3D world. Towards this end, we propose Semantic Abstraction
(SemAbs), a framework that equips 2D Vision-Language Models (VLMs) with new 3D
spatial capabilities, while maintaining their zero-shot robustness. We achieve
this abstraction using relevancy maps extracted from CLIP, and learn 3D spatial
and geometric reasoning skills on top of those abstractions in a
semantic-agnostic manner. We demonstrate the usefulness of SemAbs on two
open-world 3D scene understanding tasks: 1) completing partially observed
objects and 2) localizing hidden objects from language descriptions.
Experiments show that SemAbs can generalize to novel vocabulary,
materials/lighting, classes, and domains (i.e., real-world scans) from training
on limited 3D synthetic data. Code and data will be available at
https://semantic-abstraction.cs.columbia.edu/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Marior: Margin Removal and Iterative Content Rectification for Document Dewarping in the Wild. (arXiv:2207.11515v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11515">
<div class="article-summary-box-inner">
<span><p>Camera-captured document images usually suffer from perspective and geometric
deformations. It is of great value to rectify them when considering poor visual
aesthetics and the deteriorated performance of OCR systems. Recent
learning-based methods intensively focus on the accurately cropped document
image. However, this might not be sufficient for overcoming practical
challenges, including document images either with large marginal regions or
without margins. Due to this impracticality, users struggle to crop documents
precisely when they encounter large marginal regions. Simultaneously, dewarping
images without margins is still an insurmountable problem. To the best of our
knowledge, there is still no complete and effective pipeline for rectifying
document images in the wild. To address this issue, we propose a novel approach
called Marior (Margin Removal and \Iterative Content Rectification). Marior
follows a progressive strategy to iteratively improve the dewarping quality and
readability in a coarse-to-fine manner. Specifically, we divide the pipeline
into two modules: margin removal module (MRM) and iterative content
rectification module (ICRM). First, we predict the segmentation mask of the
input image to remove the margin, thereby obtaining a preliminary result. Then
we refine the image further by producing dense displacement flows to achieve
content-aware rectification. We determine the number of refinement iterations
adaptively. Experiments demonstrate the state-of-the-art performance of our
method on public benchmarks. The resources are available at
https://github.com/ZZZHANG-jx/Marior for further comparison.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Monotonic Pixel-Level Modulation. (arXiv:2207.11517v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11517">
<div class="article-summary-box-inner">
<span><p>Continuous one-to-many mapping is a less investigated yet important task in
both low-level visions and neural image translation. In this paper, we present
a new formulation called MonoPix, an unsupervised and contrastive continuous
modulation model, and take a step further to enable a pixel-level spatial
control which is critical but can not be properly handled previously. The key
feature of this work is to model the monotonicity between controlling signals
and the domain discriminator with a novel contrastive modulation framework and
corresponding monotonicity constraints. We have also introduced a selective
inference strategy with logarithmic approximation complexity and support fast
domain adaptations. The state-of-the-art performance is validated on a variety
of continuous mapping tasks, including AFHQ cat-dog and Yosemite summer-winter
translation. The introduced approach also helps to provide a new solution for
many low-level tasks like low-light enhancement and natural noise generation,
which is beyond the long-established practice of one-to-one training and
inference. Code is available at https://github.com/lukun199/MonoPix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition. (arXiv:2207.11518v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11518">
<div class="article-summary-box-inner">
<span><p>The teacher-free online Knowledge Distillation (KD) aims to train an ensemble
of multiple student models collaboratively and distill knowledge from each
other. Although existing online KD methods achieve desirable performance, they
often focus on class probabilities as the core knowledge type, ignoring the
valuable feature representational information. We present a Mutual Contrastive
Learning (MCL) framework for online KD. The core idea of MCL is to perform
mutual interaction and transfer of contrastive distributions among a cohort of
networks in an online manner. Our MCL can aggregate cross-network embedding
information and maximize the lower bound to the mutual information between two
networks. This enables each network to learn extra contrastive knowledge from
others, leading to better feature representations, thus improving the
performance of visual recognition tasks. Beyond the final layer, we extend MCL
to several intermediate layers assisted by auxiliary feature refinement
modules. This further enhances the ability of representation learning for
online KD. Experiments on image classification and transfer learning to visual
recognition tasks show that MCL can lead to consistent performance gains
against state-of-the-art online KD approaches. The superiority demonstrates
that MCL can guide the network to generate better feature representations. Our
code is publicly available at https://github.com/winycg/MCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unstructured Road Segmentation using Hypercolumn based Random Forests of Local experts. (arXiv:2207.11523v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11523">
<div class="article-summary-box-inner">
<span><p>Monocular vision based road detection methods are mostly based on machine
learning methods, relying on classification and feature extraction accuracy,
and suffer from appearance, illumination and weather changes. Traditional
methods introduce the predictions into conditional random fields or markov
random fields models to improve the intermediate predictions based on
structure. These methods are optimization based and therefore resource heavy
and slow, making it unsuitable for real time applications. We propose a method
to detect and segment roads with a random forest classifier of local experts
with superpixel based machine-learned features. The random forest takes in
machine learnt descriptors from a pre-trained convolutional neural network -
VGG-16. The features are also pooled into their respective superpixels,
allowing for local structure to be continuous. We compare our algorithm against
Nueral Network based methods and Traditional approaches (based on Hand-crafted
features), on both Structured Road (CamVid and Kitti) and Unstructured Road
Datasets. Finally, we introduce a Road Scene Dataset with 1000 annotated
images, and verify that our algorithm works well in non-urban and rural road
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-driven Neural Gesture Reenactment with Video Motion Graphs. (arXiv:2207.11524v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11524">
<div class="article-summary-box-inner">
<span><p>Human speech is often accompanied by body gestures including arm and hand
gestures. We present a method that reenacts a high-quality video with gestures
matching a target speech audio. The key idea of our method is to split and
re-assemble clips from a reference video through a novel video motion graph
encoding valid transitions between clips. To seamlessly connect different clips
in the reenactment, we propose a pose-aware video blending network which
synthesizes video frames around the stitched frames between two clips.
Moreover, we developed an audio-based gesture searching algorithm to find the
optimal order of the reenacted frames. Our system generates reenactments that
are consistent with both the audio rhythms and the speech content. We evaluate
our synthesized video quality quantitatively, qualitatively, and with user
studies, demonstrating that our method produces videos of much higher quality
and consistency with the target audio compared to previous work and baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Validation of AI and non-AI Methods in MRI Volumetry to Diagnose Parkinsonian Syndromes. (arXiv:2207.11534v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11534">
<div class="article-summary-box-inner">
<span><p>Automated segmentation and volumetry of brain magnetic resonance imaging
(MRI) scans are essential for the diagnosis of Parkinson's disease (PD) and
Parkinson's plus syndromes (P-plus). To enhance the diagnostic performance, we
adopt deep learning (DL) models in brain segmentation and compared their
performance with the gold-standard non-DL method. We collected brain MRI scans
of healthy controls (n=105) and patients with PD (n=105), multiple systemic
atrophy (n=132), and progressive supranuclear palsy (n=69) at Samsung Medical
Center from January 2017 to December 2020. Using the gold-standard non-DL
model, FreeSurfer (FS), we segmented six brain structures: midbrain, pons,
caudate, putamen, pallidum, and third ventricle, and considered them as
annotating data for DL models, the representative V-Net and UNETR. The Dice
scores and area under the curve (AUC) for differentiating normal, PD, and
P-plus cases were calculated. The segmentation times of V-Net and UNETR for the
six brain structures per patient were 3.48 +- 0.17 and 48.14 +- 0.97 s,
respectively, being at least 300 times faster than FS (15,735 +- 1.07 s). Dice
scores of both DL models were sufficiently high (&gt;0.85), and their AUCs for
disease classification were superior to that of FS. For classification of
normal vs. P-plus and PD vs. multiple systemic atrophy (cerebellar type), the
DL models and FS showed AUCs above 0.8. DL significantly reduces the analysis
time without compromising the performance of brain segmentation and
differential diagnosis. Our findings may contribute to the adoption of DL brain
MRI segmentation in clinical settings and advance brain research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HPS-Det: Dynamic Sample Assignment with Hyper-Parameter Search for Object Detection. (arXiv:2207.11539v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11539">
<div class="article-summary-box-inner">
<span><p>Sample assignment plays a prominent part in modern object detection
approaches. However, most existing methods rely on manual design to assign
positive / negative samples, which do not explicitly establish the
relationships between sample assignment and object detection performance. In
this work, we propose a novel dynamic sample assignment scheme based on
hyper-parameter search. We first define the number of positive samples assigned
to each ground truth as the hyper-parameters and employ a surrogate
optimization algorithm to derive the optimal choices. Then, we design a dynamic
sample assignment procedure to dynamically select the optimal number of
positives at each training iteration. Experiments demonstrate that the
resulting HPS-Det brings improved performance over different object detection
baselines. Moreover, We analyze the hyper-parameter reusability when
transferring between different datasets and between different backbones for
object detection, which exhibits the superiority and versatility of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Support Few-Shot Semantic Segmentation. (arXiv:2207.11549v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11549">
<div class="article-summary-box-inner">
<span><p>Existing few-shot segmentation methods have achieved great progress based on
the support-query matching framework. But they still heavily suffer from the
limited coverage of intra-class variations from the few-shot supports provided.
Motivated by the simple Gestalt principle that pixels belonging to the same
object are more similar than those to different objects of same class, we
propose a novel self-support matching strategy to alleviate this problem, which
uses query prototypes to match query features, where the query prototypes are
collected from high-confidence query predictions. This strategy can effectively
capture the consistent underlying characteristics of the query objects, and
thus fittingly match query features. We also propose an adaptive self-support
background prototype generation module and self-support loss to further
facilitate the self-support matching procedure. Our self-support network
substantially improves the prototype quality, benefits more improvement from
stronger backbones and more supports, and achieves SOTA on multiple datasets.
Codes are at \url{https://github.com/fanq15/SSP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Resolution Swin Transformer for Automatic Medical Image Segmentation. (arXiv:2207.11553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11553">
<div class="article-summary-box-inner">
<span><p>The Resolution of feature maps is critical for medical image segmentation.
Most of the existing Transformer-based networks for medical image segmentation
are U-Net-like architecture that contains an encoder that utilizes a sequence
of Transformer blocks to convert the input medical image from high-resolution
representation into low-resolution feature maps and a decoder that gradually
recovers the high-resolution representation from low-resolution feature maps.
Unlike previous studies, in this paper, we utilize the network design style
from the High-Resolution Network (HRNet), replace the convolutional layers with
Transformer blocks, and continuously exchange information from the different
resolution feature maps that are generated by Transformer blocks. The newly
Transformer-based network presented in this paper is denoted as High-Resolution
Swin Transformer Network (HRSTNet). Extensive experiments illustrate that
HRSTNet can achieve comparable performance with the state-of-the-art
Transformer-based U-Net-like architecture on Brain Tumor Segmentation(BraTS)
2021 and the liver dataset from Medical Segmentation Decathlon. The code of
HRSTNet will be publicly available at https://github.com/auroua/HRSTNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Open Set 3D Learning: A Benchmark on Object Point Clouds. (arXiv:2207.11554v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11554">
<div class="article-summary-box-inner">
<span><p>In the last years, there has been significant progress in the field of 3D
learning on classification, detection and segmentation problems. The vast
majority of the existing studies focus on canonical closed-set conditions,
neglecting the intrinsic open nature of the real-world. This limits the
abilities of autonomous systems involved in safety-critical applications that
require managing novel and unknown signals. In this context exploiting 3D data
can be a valuable asset since it conveys rich information about the geometry of
sensed objects and scenes. This paper provides the first broad study on Open
Set 3D learning. We introduce a novel testbed with settings of increasing
difficulty in terms of category semantic shift and cover both in-domain
(synthetic-to-synthetic) and cross-domain (synthetic-to-real) scenarios.
Moreover, we investigate the related out-of-distribution and Open Set 2D
literature to understand if and how their most recent approaches are effective
on 3D data. Our extensive benchmark positions several algorithms in the same
coherent picture, revealing their strengths and limitations. The results of our
analysis may serve as a reliable foothold for future tailored Open Set 3D
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robots Enact Malignant Stereotypes. (arXiv:2207.11569v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11569">
<div class="article-summary-box-inner">
<span><p>Stereotypes, bias, and discrimination have been extensively documented in
Machine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural
Language Processing (NLP) [6], or both, in the case of large image and caption
models such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias
manifests in robots that physically and autonomously act within the world. We
audit one of several recently published CLIP-powered robotic manipulation
methods, presenting it with objects that have pictures of human faces on the
surface which vary across race and gender, alongside task descriptions that
contain terms associated with common stereotypes. Our experiments definitively
show robots acting out toxic stereotypes with respect to gender, race, and
scientifically-discredited physiognomy, at scale. Furthermore, the audited
methods are less likely to recognize Women and People of Color. Our
interdisciplinary sociotechnical analysis synthesizes across fields and
applications such as Science Technology and Society (STS), Critical Studies,
History, Safety, Robotics, and AI. We find that robots powered by large
datasets and Dissolution Models (sometimes called "foundation models", e.g.
CLIP) that contain humans risk physically amplifying malignant stereotypes in
general; and that merely correcting disparities will be insufficient for the
complexity and scale of the problem. Instead, we recommend that robot learning
methods that physically manifest stereotypes or other harmful outcomes be
paused, reworked, or even wound down when appropriate, until outcomes can be
proven safe, effective, and just. Finally, we discuss comprehensive policy
changes and the potential of new interdisciplinary research on topics like
Identity Safety Assessment Frameworks and Design Justice to better understand
and address these harms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning of Echocardiogram Videos Enables Data-Efficient Clinical Diagnosis. (arXiv:2207.11581v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11581">
<div class="article-summary-box-inner">
<span><p>Given the difficulty of obtaining high-quality labels for medical image
recognition tasks, there is a need for deep learning techniques that can be
adequately fine-tuned on small labeled data sets. Recent advances in
self-supervised learning techniques have shown that such an in-domain
representation learning approach can provide a strong initialization for
supervised fine-tuning, proving much more data-efficient than standard transfer
learning from a supervised pretraining task. However, these applications are
not adapted to applications to medical diagnostics captured in a video format.
With this progress in mind, we developed a self-supervised learning approach
catered to echocardiogram videos with the goal of learning strong
representations for downstream fine-tuning on the task of diagnosing aortic
stenosis (AS), a common and dangerous disease of the aortic valve. When
fine-tuned on 1% of the training data, our best self-supervised learning model
achieves 0.818 AUC (95% CI: 0.794, 0.840), while the standard transfer learning
approach reaches 0.644 AUC (95% CI: 0.610, 0.677). We also find that our
self-supervised model attends more closely to the aortic valve when predicting
severe AS as demonstrated by saliency map visualizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defining an action of SO(d)-rotations on images generated by projections of d-dimensional objects: Applications to pose inference with Geometric VAEs. (arXiv:2207.11582v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11582">
<div class="article-summary-box-inner">
<span><p>Recent advances in variational autoencoders (VAEs) have enabled learning
latent manifolds as compact Lie groups, such as $SO(d)$. Since this approach
assumes that data lies on a subspace that is homeomorphic to the Lie group
itself, we here investigate how this assumption holds in the context of images
that are generated by projecting a $d$-dimensional volume with unknown pose in
$SO(d)$. Upon examining different theoretical candidates for the group and
image space, we show that the attempt to define a group action on the data
space generally fails, as it requires more specific geometric constraints on
the volume. Using geometric VAEs, our experiments confirm that this constraint
is key to proper pose inference, and we discuss the potential of these results
for applications and future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Artisan: A Semantic-Aware and Controllable CLIPstyler. (arXiv:2207.11598v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11598">
<div class="article-summary-box-inner">
<span><p>Recall that most of the current image style transfer methods require the user
to give an image of a particular style and then extract that styling feature
and texture to generate the style of an image, but there are still some
problems: the user may not have a reference style image, or it may be difficult
to summarise the desired style in mind with just one image. The recently
proposed CLIPstyler has solved this problem, which is able to perform style
transfer based only on the provided description of the style image. Although
CLIPstyler can achieve good performance when landscapes or portraits appear
alone, it can blur the people and lose the original semantics when people and
landscapes coexist. Based on these issues, we demonstrate a novel framework
that uses a pre-trained CLIP text-image embedding model and guides image style
transfer through an FCN semantic segmentation network. Specifically, we solve
the portrait over-styling problem for both selfies and real-world landscape
with human subjects photos, enhance the contrast between the effect of style
transfer in portrait and landscape, and make the degree of image style transfer
in different semantic parts fully controllable. Our Generative Artisan resolve
the failure case of CLIPstyler and yield both qualitative and quantitative
methods to prove ours have much better results than CLIPstyler in both selfies
and real-world landscape with human subjects photos. This improvement makes it
possible to commercialize our framework for business scenarios such as
retouching graphics software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Deblurring using Dual Camera Fusion on Mobile Phones. (arXiv:2207.11617v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11617">
<div class="article-summary-box-inner">
<span><p>Motion blur of fast-moving subjects is a longstanding problem in photography
and very common on mobile phones due to limited light collection efficiency,
particularly in low-light conditions. While we have witnessed great progress in
image deblurring in recent years, most methods require significant
computational power and have limitations in processing high-resolution photos
with severe local motions. To this end, we develop a novel face deblurring
system based on the dual camera fusion technique for mobile phones. The system
detects subject motion to dynamically enable a reference camera, e.g.,
ultrawide angle camera commonly available on recent premium phones, and
captures an auxiliary photo with faster shutter settings. While the main shot
is low noise but blurry, the reference shot is sharp but noisy. We learn ML
models to align and fuse these two shots and output a clear photo without
motion blur. Our algorithm runs efficiently on Google Pixel 6, which takes 463
ms overhead per shot. Our experiments demonstrate the advantage and robustness
of our system against alternative single-image, multi-frame, face-specific, and
video deblurring algorithms as well as commercial products. To the best of our
knowledge, our work is the first mobile solution for face motion deblurring
that works reliably and robustly over thousands of images in diverse motion and
lighting conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simplistic and Cost-Effective Design for Real-World Development of an Ambient Assisted Living System for Fall Detection and Indoor Localization: Proof of Concept. (arXiv:2207.11623v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11623">
<div class="article-summary-box-inner">
<span><p>Falls, highly common in the constantly increasing global aging population,
can have a variety of negative effects on their health, well-being, and quality
of life, including restricting their capabilities to conduct Activities of
Daily Living (ADLs), which are crucial for one's sustenance. Timely assistance
during falls is highly necessary, which involves tracking the indoor location
of the elderly during their diverse navigational patterns associated with ADLs
to detect the precise location of a fall. With the decreasing caregiver
population on a global scale, it is important that the future of intelligent
living environments can detect falls during ADLs while being able to track the
indoor location of the elderly in the real world. To address these challenges,
this work proposes a cost-effective and simplistic design paradigm for an
Ambient Assisted Living system that can capture multimodal components of user
behaviors during ADLs that are necessary for performing fall detection and
indoor localization in a simultaneous manner in the real world. Proof of
concept results from real-world experiments are presented to uphold the
effective working of the system. The findings from two comparison studies with
prior works in this field are also presented to uphold the novelty of this
work. The first comparison study shows how the proposed system outperforms
prior works in the areas of indoor localization and fall detection in terms of
the effectiveness of its software design and hardware design. The second
comparison study shows that the cost for the development of this system is the
least as compared to prior works in these fields, which involved real-world
development of the underlining systems, thereby upholding its cost-effective
nature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-temporal Analysis for Automated Concrete Workability Estimation. (arXiv:2207.11635v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11635">
<div class="article-summary-box-inner">
<span><p>Concrete workability measure is mostly determined based on subjective
assessment of a certified assessor with visual inspections. The potential human
error in measuring the workability and the resulting unnecessary adjustments
for the workability is a major challenge faced by the construction industry,
leading to significant costs, material waste and delay. In this paper, we try
to apply computer vision techniques to observe the concrete mixing process and
estimate the workability. Specifically, we collected the video data and then
built three different deep neural networks for spatial-temporal regression. The
pilot study demonstrates a practical application with computer vision
techniques to estimate the concrete workability during the mixing process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explored An Effective Methodology for Fine-Grained Snake Recognition. (arXiv:2207.11637v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11637">
<div class="article-summary-box-inner">
<span><p>Fine-Grained Visual Classification (FGVC) is a longstanding and fundamental
problem in computer vision and pattern recognition, and underpins a diverse set
of real-world applications. This paper describes our contribution at
SnakeCLEF2022 with FGVC. Firstly, we design a strong multimodal backbone to
utilize various meta-information to assist in fine-grained identification.
Secondly, we provide new loss functions to solve the long tail distribution
with dataset. Then, in order to take full advantage of unlabeled datasets, we
use self-supervised learning and supervised learning joint training to provide
pre-trained model. Moreover, some effective data process tricks also are
considered in our experiments. Last but not least, fine-tuned in downstream
task with hard mining, ensambled kinds of model performance. Extensive
experiments demonstrate that our method can effectively improve the performance
of fine-grained recognition. Our method can achieve a macro f1 score 92.7% and
89.4% on private and public dataset, respectively, which is the 1st place among
the participators on private leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DCT Approximations Based on Chen's Factorization. (arXiv:2207.11638v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11638">
<div class="article-summary-box-inner">
<span><p>In this paper, two 8-point multiplication-free DCT approximations based on
the Chen's factorization are proposed and their fast algorithms are also
derived. Both transformations are assessed in terms of computational cost,
error energy, and coding gain. Experiments with a JPEG-like image compression
scheme are performed and results are compared with competing methods. The
proposed low-complexity transforms are scaled according to Jridi-Alfalou-Meher
algorithm to effect 16- and 32-point approximations. The new sets of
transformations are embedded into an HEVC reference software to provide a fully
HEVC-compliant video coding scheme. We show that approximate transforms can
outperform traditional transforms and state-of-the-art methods at a very low
complexity cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Scene Inference under Noise-Blur Dual Corruptions. (arXiv:2207.11643v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11643">
<div class="article-summary-box-inner">
<span><p>Scene inference under low-light is a challenging problem due to severe noise
in the captured images. One way to reduce noise is to use longer exposure
during the capture. However, in the presence of motion (scene or camera
motion), longer exposures lead to motion blur, resulting in loss of image
information. This creates a trade-off between these two kinds of image
degradations: motion blur (due to long exposure) vs. noise (due to short
exposure), also referred as a dual image corruption pair in this paper. With
the rise of cameras capable of capturing multiple exposures of the same scene
simultaneously, it is possible to overcome this trade-off. Our key observation
is that although the amount and nature of degradation varies for these
different image captures, the semantic content remains the same across all
images. To this end, we propose a method to leverage these multi exposure
captures for robust inference under low-light and motion. Our method builds on
a feature consistency loss to encourage similar results from these individual
captures, and uses the ensemble of their final predictions for robust visual
recognition. We demonstrate the effectiveness of our approach on simulated
images as well as real captures with multiple exposures, and across the tasks
of object detection and image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Regularization of Event-based Learning by Reversing and Drifting. (arXiv:2207.11659v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11659">
<div class="article-summary-box-inner">
<span><p>Event camera has an enormous potential in challenging scenes for its
advantages of high temporal resolution, high dynamic range, low power
consumption, and no motion blur. However, event-based learning is hindered by
insufficient generalization ability. In this paper, we first analyze the
influence of different brightness variations on event data. Then we propose two
novel augmentation methods: EventReverse and EventDrift. By reversing and
drifting events to their corresponding positions in the spatiotemporal or
polarity domain, the proposed methods generate samples affected by different
brightness variations, which improves the robustness of event-based learning
and results in a better generalization. Extensive experiments on N-CARS,
N-Caltech101 and CIFAR10-DVS datasets demonstrate that our method is general
and remarkably effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAR: Masked Autoencoders for Efficient Action Recognition. (arXiv:2207.11660v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11660">
<div class="article-summary-box-inner">
<span><p>Standard approaches for video recognition usually operate on the full input
videos, which is inefficient due to the widely present spatio-temporal
redundancy in videos. Recent progress in masked video modelling, i.e.,
VideoMAE, has shown the ability of vanilla Vision Transformers (ViT) to
complement spatio-temporal contexts given only limited visual contents.
Inspired by this, we propose propose Masked Action Recognition (MAR), which
reduces the redundant computation by discarding a proportion of patches and
operating only on a part of the videos. MAR contains the following two
indispensable components: cell running masking and bridging classifier.
Specifically, to enable the ViT to perceive the details beyond the visible
patches easily, cell running masking is presented to preserve the
spatio-temporal correlations in videos, which ensures the patches at the same
spatial location can be observed in turn for easy reconstructions.
Additionally, we notice that, although the partially observed features can
reconstruct semantically explicit invisible patches, they fail to achieve
accurate classification. To address this, a bridging classifier is proposed to
bridge the semantic gap between the ViT encoded features for reconstruction and
the features specialized for classification. Our proposed MAR reduces the
computational cost of ViT by 53% and extensive experiments show that MAR
consistently outperforms existing ViT models with a notable margin. Especially,
we found a ViT-Large trained by MAR outperforms the ViT-Huge trained by a
standard training scheme by convincing margins on both Kinetics-400 and
Something-Something v2 datasets, while our computation overhead of ViT-Large is
only 14.5% of ViT-Huge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Associative Plasticity between Synapses to Enhance Learning of Spiking Neural Networks. (arXiv:2207.11670v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11670">
<div class="article-summary-box-inner">
<span><p>Spiking Neural Networks (SNNs) are the third generation of artificial neural
networks that enable energy-efficient implementation on neuromorphic hardware.
However, the discrete transmission of spikes brings significant challenges to
the robust and high-performance learning mechanism. Most existing works focus
solely on learning between neurons but ignore the influence between synapses,
resulting in a loss of robustness and accuracy. To address this problem, we
propose a robust and effective learning mechanism by modeling the associative
plasticity between synapses (APBS) observed from the physiological phenomenon
of associative long-term potentiation (ALTP). With the proposed APBS method,
synapses of the same neuron interact through a shared factor when concurrently
stimulated by other neurons. In addition, we propose a spatiotemporal cropping
and flipping (STCF) method to improve the generalization ability of our
network. Extensive experiments demonstrate that our approaches achieve superior
performance on static CIFAR-10 datasets and state-of-the-art performance on
neuromorphic MNIST-DVS, CIFAR10-DVS datasets by a lightweight convolution
network. To our best knowledge, this is the first time to explore a learning
method between synapses and an extended approach for neuromorphic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnable Privacy-Preserving Anonymization for Pedestrian Images. (arXiv:2207.11677v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11677">
<div class="article-summary-box-inner">
<span><p>This paper studies a novel privacy-preserving anonymization problem for
pedestrian images, which preserves personal identity information (PII) for
authorized models and prevents PII from being recognized by third parties.
Conventional anonymization methods unavoidably cause semantic information loss,
leading to limited data utility. Besides, existing learned anonymization
techniques, while retaining various identity-irrelevant utilities, will change
the pedestrian identity, and thus are unsuitable for training robust
re-identification models. To explore the privacy-utility trade-off for
pedestrian images, we propose a joint learning reversible anonymization
framework, which can reversibly generate full-body anonymous images with little
performance drop on person re-identification tasks. The core idea is that we
adopt desensitized images generated by conventional methods as the initial
privacy-preserving supervision and jointly train an anonymization encoder with
a recovery decoder and an identity-invariant model. We further propose a
progressive training strategy to improve the performance, which iteratively
upgrades the initial anonymization supervision. Experiments further demonstrate
the effectiveness of our anonymized pedestrian images for privacy protection,
which boosts the re-identification performance while preserving privacy. Code
is available at \url{https://github.com/whuzjw/privacy-reid}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FD-MAR: Fourier Dual-domain Network for CT Metal Artifact Reduction. (arXiv:2207.11678v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11678">
<div class="article-summary-box-inner">
<span><p>The presence of high-density objects such as metal implants and dental
fillings can introduce severely streak-like artifacts in computed tomography
(CT) images, greatly limiting subsequent diagnosis. Although various deep
neural networks-based methods have been proposed for metal artifact reduction
(MAR), they usually suffer from poor performance due to limited exploitation of
global context in the sinogram domain, secondary artifacts introduced in the
image domain, and the requirement of precise metal masks. To address these
issues, this paper explores fast Fourier convolution for MAR in both sinogram
and image domains, and proposes a Fourier dual-domain network for MAR, termed
FD-MAR. Specifically, we first propose a Fourier sinogram restoration network,
which can leverage sinogram-wide receptive context to fill in the
metal-corrupted region from uncorrupted region and, hence, is robust to the
metal trace. Second, we propose a Fourier refinement network in the image
domain, which can refine the reconstructed images in a local-to-global manner
by exploring image-wide context information. As a result, the proposed FD-MAR
can explore the sinogram- and image-wide receptive fields for MAR. By
optimizing FD-MAR with a composite loss function, extensive experimental
results demonstrate the superiority of the proposed FD-MAR over the
state-of-the-art MAR methods in terms of quantitative metrics and visual
comparison. Notably, FD-MAR does not require precise metal masks, which is of
great importance in clinical routine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affective Behaviour Analysis Using Pretrained Model with Facial Priori. (arXiv:2207.11679v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11679">
<div class="article-summary-box-inner">
<span><p>Affective behaviour analysis has aroused researchers' attention due to its
broad applications. However, it is labor exhaustive to obtain accurate
annotations for massive face images. Thus, we propose to utilize the prior
facial information via Masked Auto-Encoder (MAE) pretrained on unlabeled face
images. Furthermore, we combine MAE pretrained Vision Transformer (ViT) and
AffectNet pretrained CNN to perform multi-task emotion recognition. We notice
that expression and action unit (AU) scores are pure and intact features for
valence-arousal (VA) regression. As a result, we utilize AffectNet pretrained
CNN to extract expression scores concatenating with expression and AU scores
from ViT to obtain the final VA features. Moreover, we also propose a
co-training framework with two parallel MAE pretrained ViT for expression
recognition tasks. In order to make the two views independent, we random mask
most patches during the training process. Then, JS divergence is performed to
make the predictions of the two views as consistent as possible. The results on
ABAW4 show that our methods are effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Graph Neural Networks for Image Style Transfer. (arXiv:2207.11681v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11681">
<div class="article-summary-box-inner">
<span><p>State-of-the-art parametric and non-parametric style transfer approaches are
prone to either distorted local style patterns due to global statistics
alignment, or unpleasing artifacts resulting from patch mismatching. In this
paper, we study a novel semi-parametric neural style transfer framework that
alleviates the deficiency of both parametric and non-parametric stylization.
The core idea of our approach is to establish accurate and fine-grained
content-style correspondences using graph neural networks (GNNs). To this end,
we develop an elaborated GNN model with content and style local patches as the
graph vertices. The style transfer procedure is then modeled as the
attention-based heterogeneous message passing between the style and content
nodes in a learnable manner, leading to adaptive many-to-one style-content
correlations at the local patch level. In addition, an elaborated deformable
graph convolutional operation is introduced for cross-scale style-content
matching. Experimental results demonstrate that the proposed semi-parametric
image stylization approach yields encouraging results on the challenging style
patterns, preserving both global appearance and exquisite details. Furthermore,
by controlling the number of edges at the inference stage, the proposed method
also triggers novel functionalities like diversified patch-based stylization
with a single model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PCA: Semi-supervised Segmentation with Patch Confidence Adversarial Training. (arXiv:2207.11683v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11683">
<div class="article-summary-box-inner">
<span><p>Deep learning based semi-supervised learning (SSL) methods have achieved
strong performance in medical image segmentation, which can alleviate doctors'
expensive annotation by utilizing a large amount of unlabeled data. Unlike most
existing semi-supervised learning methods, adversarial training based methods
distinguish samples from different sources by learning the data distribution of
the segmentation map, leading the segmenter to generate more accurate
predictions. We argue that the current performance restrictions for such
approaches are the problems of feature extraction and learning preference. In
this paper, we propose a new semi-supervised adversarial method called Patch
Confidence Adversarial Training (PCA) for medical image segmentation. Rather
than single scalar classification results or pixel-level confidence maps, our
proposed discriminator creates patch confidence maps and classifies them at the
scale of the patches. The prediction of unlabeled data learns the pixel
structure and context information in each patch to get enough gradient
feedback, which aids the discriminator in convergent to an optimal state and
improves semi-supervised segmentation performance. Furthermore, at the
discriminator's input, we supplement semantic information constraints on
images, making it simpler for unlabeled data to fit the expected data
distribution. Extensive experiments on the Automated Cardiac Diagnosis
Challenge (ACDC) 2017 dataset and the Brain Tumor Segmentation (BraTS) 2019
challenge dataset show that our method outperforms the state-of-the-art
semi-supervised methods, which demonstrates its effectiveness for medical image
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kernel Relative-prototype Spectral Filtering for Few-shot Learning. (arXiv:2207.11685v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11685">
<div class="article-summary-box-inner">
<span><p>Few-shot learning performs classification tasks and regression tasks on
scarce samples. As one of the most representative few-shot learning models,
Prototypical Network represents each class as sample average, or a prototype,
and measures the similarity of samples and prototypes by Euclidean distance. In
this paper, we propose a framework of spectral filtering (shrinkage) for
measuring the difference between query samples and prototypes, or namely the
relative prototypes, in a reproducing kernel Hilbert space (RKHS). In this
framework, we further propose a method utilizing Tikhonov regularization as the
filter function for few-shot classification. We conduct several experiments to
verify our method utilizing different kernels based on the miniImageNet
dataset, tiered-ImageNet dataset and CIFAR-FS dataset. The experimental results
show that the proposed model can perform the state-of-the-art. In addition, the
experimental results show that the proposed shrinkage method can boost the
performance. Source code is available at https://github.com/zhangtao2022/DSFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proving Common Mechanisms Shared by Twelve Methods of Boosting Adversarial Transferability. (arXiv:2207.11694v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11694">
<div class="article-summary-box-inner">
<span><p>Although many methods have been proposed to enhance the transferability of
adversarial perturbations, these methods are designed in a heuristic manner,
and the essential mechanism for improving adversarial transferability is still
unclear. This paper summarizes the common mechanism shared by twelve previous
transferability-boosting methods in a unified view, i.e., these methods all
reduce game-theoretic interactions between regional adversarial perturbations.
To this end, we focus on the attacking utility of all interactions between
regional adversarial perturbations, and we first discover and prove the
negative correlation between the adversarial transferability and the attacking
utility of interactions. Based on this discovery, we theoretically prove and
empirically verify that twelve previous transferability-boosting methods all
reduce interactions between regional adversarial perturbations. More crucially,
we consider the reduction of interactions as the essential reason for the
enhancement of adversarial transferability. Furthermore, we design the
interaction loss to directly penalize interactions between regional adversarial
perturbations during attacking. Experimental results show that the interaction
loss significantly improves the transferability of adversarial perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11699">
<div class="article-summary-box-inner">
<span><p>Significant progress has been witnessed in learning-based Multi-view Stereo
(MVS) of supervised and unsupervised settings. To combine their respective
merits in accuracy and completeness, meantime reducing the demand for expensive
labeled data, this paper explores a novel semi-supervised setting of
learning-based MVS problem that only a tiny part of the MVS data is attached
with dense depth ground truth. However, due to huge variation of scenarios and
flexible setting in views, semi-supervised MVS problem (Semi-MVS) may break the
basic assumption in classic semi-supervised learning, that unlabeled data and
labeled data share the same label space and data distribution. To handle these
issues, we propose a novel semi-supervised MVS framework, namely SE-MVS. For
the simple case that the basic assumption works in MVS data, consistency
regularization encourages the model predictions to be consistent between
original sample and randomly augmented sample via constraints on KL divergence.
For further troublesome case that the basic assumption is conflicted in MVS
data, we propose a novel style consistency loss to alleviate the negative
effect caused by the distribution gap. The visual style of unlabeled sample is
transferred to labeled sample to shrink the gap, and the model prediction of
generated sample is further supervised with the label in original labeled
sample. The experimental results on DTU, BlendedMVS, GTA-SFM, and
Tanks\&amp;Temples datasets show the superior performance of the proposed method.
With the same settings in backbone network, our proposed SE-MVS outperforms its
fully-supervised and unsupervised baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Test-Time Adaptation via Shift-agnostic Weight Regularization and Nearest Source Prototypes. (arXiv:2207.11707v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11707">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel test-time adaptation strategy that adjusts the
model pre-trained on the source domain using only unlabeled online data from
the target domain to alleviate the performance degradation due to the
distribution shift between the source and target domains. Adapting the entire
model parameters using the unlabeled online data may be detrimental due to the
erroneous signals from an unsupervised objective. To mitigate this problem, we
propose a shift-agnostic weight regularization that encourages largely updating
the model parameters sensitive to distribution shift while slightly updating
those insensitive to the shift, during test-time adaptation. This
regularization enables the model to quickly adapt to the target domain without
performance degradation by utilizing the benefit of a high learning rate. In
addition, we present an auxiliary task based on nearest source prototypes to
align the source and target features, which helps reduce the distribution shift
and leads to further performance improvement. We show that our method exhibits
state-of-the-art performance on various standard benchmarks and even
outperforms its supervised counterpart.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keypoint-less Camera Calibration for Sports Field Registration in Soccer. (arXiv:2207.11709v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11709">
<div class="article-summary-box-inner">
<span><p>Sports field registration in broadcast videos is typically interpreted as the
task of homography estimation, which provides a mapping between a planar field
and the corresponding visible area of the image. In contrast to previous
approaches, we consider the task as a camera calibration problem. First, we
introduce a differentiable objective function that is able to learn the camera
pose and focal length from segment correspondences (e.g., lines, point clouds),
based on pixel-level annotations for segments of a known calibration object,
i.e., the sports field. The calibration module iteratively minimizes the
segment reprojection error induced by the estimated camera parameters. Second,
we propose a novel approach for 3D sports field registration from broadcast
soccer images. The calibration module does not require any training data and
compared to the typical solution, which subsequently refines an initial
estimation, our solution does it in one step. The proposed method is evaluated
for sports field registration on two datasets and achieves superior results
compared to two state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIPS: Text-Induced Pose Synthesis. (arXiv:2207.11718v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11718">
<div class="article-summary-box-inner">
<span><p>In computer vision, human pose synthesis and transfer deal with probabilistic
image generation of a person in a previously unseen pose from an already
available observation of that person. Though researchers have recently proposed
several methods to achieve this task, most of these techniques derive the
target pose directly from the desired target image on a specific dataset,
making the underlying process challenging to apply in real-world scenarios as
the generation of the target image is the actual aim. In this paper, we first
present the shortcomings of current pose transfer algorithms and then propose a
novel text-based pose transfer technique to address those issues. We divide the
problem into three independent stages: (a) text to pose representation, (b)
pose refinement, and (c) pose rendering. To the best of our knowledge, this is
one of the first attempts to develop a text-based pose transfer framework where
we also introduce a new dataset DF-PASS, by adding descriptive pose annotations
for the images of the DeepFashion dataset. The proposed method generates
promising results with significant qualitative and quantitative scores in our
experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Feature Learning for Realistic Cloth-Changing Gait Recognition. (arXiv:2207.11720v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11720">
<div class="article-summary-box-inner">
<span><p>Gait recognition is instrumental in crime prevention and social security, for
it can be conducted at a long distance without the cooperation of subjects.
However, existing datasets and methods cannot deal with the most challenging
problem in realistic gait recognition effectively: walking in different clothes
(CL). In order to tackle this problem, we propose two benchmarks: CASIA-BN-RCC
and OUMVLP-RCC, to simulate the cloth-changing condition in practice. The two
benchmarks can force the algorithm to realize cross-view and cross-cloth with
two sub-datasets. Furthermore, we propose a new framework that can be applied
with off-the-shelf backbones to improve its performance in the Realistic
Cloth-Changing problem with Progressive Feature Learning. Specifically, in our
framework, we design Progressive Mapping and Progressive Uncertainty to extract
the cross-view features and then extract cross-cloth features on the basis. In
this way, the features from the cross-view sub-dataset can first dominate the
feature space and relieve the uneven distribution caused by the adverse effect
from the cross-cloth sub-dataset. The experiments on our benchmarks show that
our framework can effectively improve the recognition performance in CL
conditions. Our codes and datasets will be released after accepted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-guided Multi-Mask Image Harmonization. (arXiv:2207.11722v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11722">
<div class="article-summary-box-inner">
<span><p>Previous harmonization methods focus on adjusting one inharmonious region in
an image based on an input mask. They may face problems when dealing with
different perturbations on different semantic regions without available input
masks. To deal with the problem that one image has been pasted with several
foregrounds coming from different images and needs to harmonize them towards
different domain directions without any mask as input, we propose a new
semantic-guided multi-mask image harmonization task. Different from the
previous single-mask image harmonization task, each inharmonious image is
perturbed with different methods according to the semantic segmentation masks.
Two challenging benchmarks, HScene and HLIP, are constructed based on $150$ and
$19$ semantic classes, respectively. Furthermore, previous baselines focus on
regressing the exact value for each pixel of the harmonized images. The
generated results are in the `black box' and cannot be edited. In this work, we
propose a novel way to edit the inharmonious images by predicting a series of
operator masks. The masks indicate the level and the position to apply a
certain image editing operation, which could be the brightness, the saturation,
and the color in a specific dimension. The operator masks provide more
flexibility for users to edit the image further. Extensive experiments verify
that the operator mask-based network can further improve those state-of-the-art
methods which directly regress RGB images when the perturbations are
structural. Experiments have been conducted on our constructed benchmarks to
verify that our proposed operator mask-based framework can locate and modify
the inharmonious regions in more complex scenes. Our code and models are
available at
https://github.com/XuqianRen/Semantic-guided-Multi-mask-Image-Harmonization.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Internal and External Constraints for Unrolling Shutter in Videos. (arXiv:2207.11725v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11725">
<div class="article-summary-box-inner">
<span><p>Videos obtained by rolling-shutter (RS) cameras result in spatially-distorted
frames. These distortions become significant under fast camera/scene motions.
Undoing effects of RS is sometimes addressed as a spatial problem, where
objects need to be rectified/displaced in order to generate their correct
global shutter (GS) frame. However, the cause of the RS effect is inherently
temporal, not spatial. In this paper we propose a space-time solution to the RS
problem. We observe that despite the severe differences between their xy
frames, a RS video and its corresponding GS video tend to share the exact same
xt slices -- up to a known sub-frame temporal shift. Moreover, they share the
same distribution of small 2D xt-patches, despite the strong temporal aliasing
within each video. This allows to constrain the GS output video using
video-specific constraints imposed by the RS input video. Our algorithm is
composed of 3 main components: (i) Dense temporal upsampling between
consecutive RS frames using an off-the-shelf method, (which was trained on
regular video sequences), from which we extract GS "proposals". (ii) Learning
to correctly merge an ensemble of such GS "proposals" using a dedicated
MergeNet. (iii) A video-specific zero-shot optimization which imposes the
similarity of xt-patches between the GS output video and the RS input video.
Our method obtains state-of-the-art results on benchmark datasets, both
numerically and visually, despite being trained on a small synthetic RS/GS
dataset. Moreover, it generalizes well to new complex RS videos with motion
types outside the distribution of the training set (e.g., complex non-rigid
motions) -- videos which competing methods trained on much more data cannot
handle well. We attribute these generalization capabilities to the combination
of external and internal constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can we achieve robustness from data alone?. (arXiv:2207.11727v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11727">
<div class="article-summary-box-inner">
<span><p>Adversarial training and its variants have come to be the prevailing methods
to achieve adversarially robust classification using neural networks. However,
its increased computational cost together with the significant gap between
standard and robust performance hinder progress and beg the question of whether
we can do better. In this work, we take a step back and ask: Can models achieve
robustness via standard training on a suitably optimized set? To this end, we
devise a meta-learning method for robust classification, that optimizes the
dataset prior to its deployment in a principled way, and aims to effectively
remove the non-robust parts of the data. We cast our optimization method as a
multi-step PGD procedure on kernel regression, with a class of kernels that
describe infinitely wide neural nets (Neural Tangent Kernels - NTKs).
Experiments on MNIST and CIFAR-10 demonstrate that the datasets we produce
enjoy very high robustness against PGD attacks, when deployed in both kernel
regression classifiers and neural networks. However, this robustness is
somewhat fallacious, as alternative attacks manage to fool the models, which we
find to be the case for previous similar works in the literature as well. We
discuss potential reasons for this and outline further avenues of research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Super Resolution of MR Images Using CNNs and Vision Transformers. (arXiv:2207.11748v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11748">
<div class="article-summary-box-inner">
<span><p>State of the art magnetic resonance (MR) image super-resolution methods (ISR)
using convolutional neural networks (CNNs) leverage limited contextual
information due to the limited spatial coverage of CNNs. Vision transformers
(ViT) learn better global context that is helpful in generating superior
quality HR images. We combine local information of CNNs and global information
from ViTs for image super resolution and output super resolved images that have
superior quality than those produced by state of the art methods. We include
extra constraints through multiple novel loss functions that preserve structure
and texture information from the low resolution to high resolution images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-Guided Auxiliary Training Improves 3D Object Detector. (arXiv:2207.11753v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11753">
<div class="article-summary-box-inner">
<span><p>Detecting 3D objects from point clouds is a practical yet challenging task
that has attracted increasing attention recently. In this paper, we propose a
Label-Guided auxiliary training method for 3D object detection (LG3D), which
serves as an auxiliary network to enhance the feature learning of existing 3D
object detectors. Specifically, we propose two novel modules: a
Label-Annotation-Inducer that maps annotations and point clouds in bounding
boxes to task-specific representations and a Label-Knowledge-Mapper that
assists the original features to obtain detection-critical representations. The
proposed auxiliary network is discarded in inference and thus has no extra
computational cost at test time. We conduct extensive experiments on both
indoor and outdoor datasets to verify the effectiveness of our approach. For
example, our proposed LG3D improves VoteNet by 2.5% and 3.1% mAP on the SUN
RGB-D and ScanNetV2 datasets, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Generalizable Light Field Networks from Few Images. (arXiv:2207.11757v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11757">
<div class="article-summary-box-inner">
<span><p>We explore a new strategy for few-shot novel view synthesis based on a neural
light field representation. Given a target camera pose, an implicit neural
network maps each ray to its target pixel's color directly. The network is
conditioned on local ray features generated by coarse volumetric rendering from
an explicit 3D feature volume. This volume is built from the input images using
a 3D ConvNet. Our method achieves competitive performances on synthetic and
real MVS data with respect to state-of-the-art neural radiance field based
competition, while offering a 100 times faster rendering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-Temporal Federated Learning for Lifelong Person Re-identification on Distributed Edges. (arXiv:2207.11759v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11759">
<div class="article-summary-box-inner">
<span><p>Data drift is a thorny challenge when deploying person re-identification
(ReID) models into real-world devices, where the data distribution is
significantly different from that of the training environment and keeps
changing. To tackle this issue, we propose a federated spatial-temporal
incremental learning approach, named FedSTIL, which leverages both lifelong
learning and federated learning to continuously optimize models deployed on
many distributed edge clients. Unlike previous efforts, FedSTIL aims to mine
spatial-temporal correlations among the knowledge learnt from different edge
clients. Specifically, the edge clients first periodically extract general
representations of drifted data to optimize their local models. Then, the
learnt knowledge from edge clients will be aggregated by centralized parameter
server, where the knowledge will be selectively and attentively distilled from
spatial- and temporal-dimension with carefully designed mechanisms. Finally,
the distilled informative spatial-temporal knowledge will be sent back to
correlated edge clients to further improve the recognition accuracy of each
edge client with a lifelong learning method. Extensive experiments on a mixture
of five real-world datasets demonstrate that our method outperforms others by
nearly 4% in Rank-1 accuracy, while reducing communication cost by 62%. All
implementation codes are publicly available on
https://github.com/MSNLAB/Federated-Lifelong-Person-ReID
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis. (arXiv:2207.11770v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11770">
<div class="article-summary-box-inner">
<span><p>Talking head synthesis is an emerging technology with wide applications in
film dubbing, virtual avatars and online education. Recent NeRF-based methods
generate more natural talking videos, as they better capture the 3D structural
information of faces. However, a specific model needs to be trained for each
identity with a large dataset. In this paper, we propose Dynamic Facial
Radiance Fields (DFRF) for few-shot talking head synthesis, which can rapidly
generalize to an unseen identity with few training data. Different from the
existing NeRF-based methods which directly encode the 3D geometry and
appearance of a specific person into the network, our DFRF conditions face
radiance field on 2D appearance images to learn the face prior. Thus the facial
radiance field can be flexibly adjusted to the new identity with few reference
images. Additionally, for better modeling of the facial deformations, we
propose a differentiable face warping module conditioned on audio signals to
deform all reference images to the query space. Extensive experiments show that
with only tens of seconds of training clip available, our proposed DFRF can
synthesize natural and high-quality audio-driven talking head videos for novel
identities with only 40k iterations. We highly recommend readers view our
supplementary video for intuitive comparisons. Code is available in
https://sstzal.github.io/DFRF/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Denoising Using Convolutional Autoencoder. (arXiv:2207.11771v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11771">
<div class="article-summary-box-inner">
<span><p>With the inexorable digitalisation of the modern world, every subset in the
field of technology goes through major advancements constantly. One such subset
is digital images which are ever so popular. Images can not always be as
visually pleasing or clear as you would want them to be and are often distorted
or obscured with noise. A number of techniques to enhance images have come up
as the years passed, all with their own respective pros and cons. In this
paper, we look at one such particular technique which accomplishes this task
with the help of a neural network model commonly known as an autoencoder. We
construct different architectures for the model and compare results in order to
decide the one best suited for the task. The characteristics and working of the
model are discussed briefly knowing which can help set a path for future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Semi-Supervised Contrastive Learning for Contamination-Resistant Anomaly Detection. (arXiv:2207.11789v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11789">
<div class="article-summary-box-inner">
<span><p>Anomaly detection aims at identifying deviant samples from the normal data
distribution. Contrastive learning has provided a successful way to sample
representation that enables effective discrimination on anomalies. However,
when contaminated with unlabeled abnormal samples in training set under
semi-supervised settings, current contrastive-based methods generally 1) ignore
the comprehensive relation between training data, leading to suboptimal
performance, and 2) require fine-tuning, resulting in low efficiency. To
address the above two issues, in this paper, we propose a novel hierarchical
semi-supervised contrastive learning (HSCL) framework, for
contamination-resistant anomaly detection. Specifically, HSCL hierarchically
regulates three complementary relations: sample-to-sample, sample-to-prototype,
and normal-to-abnormal relations, enlarging the discrimination between normal
and abnormal samples with a comprehensive exploration of the contaminated data.
Besides, HSCL is an end-to-end learning approach that can efficiently learn
discriminative representations without fine-tuning. HSCL achieves
state-of-the-art performance in multiple scenarios, such as one-class
classification and cross-dataset detection. Extensive ablation studies further
verify the effectiveness of each considered relation. The code is available at
https://github.com/GaoangW/HSCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PatchRD: Detail-Preserving Shape Completion by Learning Patch Retrieval and Deformation. (arXiv:2207.11790v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11790">
<div class="article-summary-box-inner">
<span><p>This paper introduces a data-driven shape completion approach that focuses on
completing geometric details of missing regions of 3D shapes. We observe that
existing generative methods lack the training data and representation capacity
to synthesize plausible, fine-grained details with complex geometry and
topology. Our key insight is to copy and deform patches from the partial input
to complete missing regions. This enables us to preserve the style of local
geometric features, even if it drastically differs from the training data. Our
fully automatic approach proceeds in two stages. First, we learn to retrieve
candidate patches from the input shape. Second, we select and deform some of
the retrieved candidates to seamlessly blend them into the complete shape. This
method combines the advantages of the two most common completion methods:
similarity-based single-instance completion, and completion by learning a shape
space. We leverage repeating patterns by retrieving patches from the partial
input, and learn global structural priors by using a neural network to guide
the retrieval and deformation steps. Experimental results show our approach
considerably outperforms baselines across multiple datasets and shape
categories. Code and data are available at https://github.com/GitBoSun/PatchRD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal 3D Shape Generation and Manipulation. (arXiv:2207.11795v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11795">
<div class="article-summary-box-inner">
<span><p>Creating and editing the shape and color of 3D objects require tremendous
human effort and expertise. Compared to direct manipulation in 3D interfaces,
2D interactions such as sketches and scribbles are usually much more natural
and intuitive for the users. In this paper, we propose a generic multi-modal
generative model that couples the 2D modalities and implicit 3D representations
through shared latent spaces. With the proposed model, versatile 3D generation
and manipulation are enabled by simply propagating the editing from a specific
2D controlling modality through the latent spaces. For example, editing the 3D
shape by drawing a sketch, re-colorizing the 3D surface via painting color
scribbles on the 2D rendering, or generating 3D shapes of a certain category
given one or a few reference images. Unlike prior works, our model does not
require re-training or fine-tuning per editing task and is also conceptually
simple, easy to implement, robust to input domain shifts, and flexible to
diverse reconstruction on partial 2D inputs. We evaluate our framework on two
representative 2D modalities of grayscale line sketches and rendered color
images, and demonstrate that our method enables various shape manipulation and
generation tasks with these 2D modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions. (arXiv:2207.11805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11805">
<div class="article-summary-box-inner">
<span><p>Action understanding has evolved into the era of fine granularity, as most
human behaviors in real life have only minor differences. To detect these
fine-grained actions accurately in a label-efficient way, we tackle the problem
of weakly-supervised fine-grained temporal action detection in videos for the
first time. Without the careful design to capture subtle differences between
fine-grained actions, previous weakly-supervised models for general action
detection cannot perform well in the fine-grained setting. We propose to model
actions as the combinations of reusable atomic actions which are automatically
discovered from data through self-supervised clustering, in order to capture
the commonality and individuality of fine-grained actions. The learnt atomic
actions, represented by visual concepts, are further mapped to fine and coarse
action labels leveraging the semantic label hierarchy. Our approach constructs
a visual representation hierarchy of four levels: clip level, atomic action
level, fine action class level and coarse action class level, with supervision
at each level. Extensive experiments on two large-scale fine-grained video
datasets, FineAction and FineGym, show the benefit of our proposed
weakly-supervised model for fine-grained action detection, and it achieves
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VizWiz-FewShot: Locating Objects in Images Taken by People With Visual Impairments. (arXiv:2207.11810v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11810">
<div class="article-summary-box-inner">
<span><p>We introduce a few-shot localization dataset originating from photographers
who authentically were trying to learn about the visual content in the images
they took. It includes nearly 10,000 segmentations of 100 categories in over
4,500 images that were taken by people with visual impairments. Compared to
existing few-shot object detection and instance segmentation datasets, our
dataset is the first to locate holes in objects (e.g., found in 12.3\% of our
segmentations), it shows objects that occupy a much larger range of sizes
relative to the images, and text is over five times more common in our objects
(e.g., found in 22.4\% of our segmentations). Analysis of three modern few-shot
localization algorithms demonstrates that they generalize poorly to our new
dataset. The algorithms commonly struggle to locate objects with holes, very
small and very large objects, and objects lacking text. To encourage a larger
community to work on these unsolved challenges, we publicly share our annotated
few-shot dataset at https://vizwiz.org .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object State Change Classification in Egocentric Videos using the Divided Space-Time Attention Mechanism. (arXiv:2207.11814v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11814">
<div class="article-summary-box-inner">
<span><p>This report describes our submission called "TarHeels" for the Ego4D: Object
State Change Classification Challenge. We use a transformer-based video
recognition model and leverage the Divided Space-Time Attention mechanism for
classifying object state change in egocentric videos. Our submission achieves
the second-best performance in the challenge. Furthermore, we perform an
ablation study to show that identifying object state change in egocentric
videos requires temporal modeling ability. Lastly, we present several positive
and negative examples to visualize our model's predictions. The code is
publicly available at: https://github.com/md-mohaiminul/ObjectStateChange
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inter-model Interpretability: Self-supervised Models as a Case Study. (arXiv:2207.11837v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11837">
<div class="article-summary-box-inner">
<span><p>Since early machine learning models, metrics such as accuracy and precision
have been the de facto way to evaluate and compare trained models. However, a
single metric number doesn't fully capture the similarities and differences
between models, especially in the computer vision domain. A model with high
accuracy on a certain dataset might provide a lower accuracy on another
dataset, without any further insights. To address this problem we build on a
recent interpretability technique called Dissect to introduce
\textit{inter-model interpretability}, which determines how models relate or
complement each other based on the visual concepts they have learned (such as
objects and materials). Towards this goal, we project 13 top-performing
self-supervised models into a Learned Concepts Embedding (LCE) space that
reveals proximities among models from the perspective of learned concepts. We
further crossed this information with the performance of these models on four
computer vision tasks and 15 datasets. The experiment allowed us to categorize
the models into three categories and revealed for the first time the type of
visual concepts different tasks requires. This is a step forward for designing
cross-task learning algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAVCHOI: Detecting Suspicious Activities using Dense Video Captioning with Human Object Interactions. (arXiv:2207.11838v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11838">
<div class="article-summary-box-inner">
<span><p>Detecting suspicious activities in surveillance videos has been a
longstanding problem, which can further lead to difficulties in detecting
crimes. The authors propose a novel approach for detecting and summarizing the
suspicious activities going on in the surveillance videos. They also create
ground truth summaries for the UCF-Crime video dataset. Further, the authors
test existing state-of-the-art algorithms for Dense Video Captioning for a
subset of this dataset and propose a model for this task by leveraging
Human-Object Interaction models for the Visual features. They observe that this
formulation for Dense Captioning achieves large gains over earlier approaches
by a significant margin. The authors also perform an ablative analysis of the
dataset and the model and report their findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Dive into Deep Cluster. (arXiv:2207.11839v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11839">
<div class="article-summary-box-inner">
<span><p>Deep Learning has demonstrated a significant improvement against traditional
machine learning approaches in different domains such as image and speech
recognition. Their success on benchmark datasets is transferred to the
real-world through pretrained models by practitioners. Pretraining visual
models using supervised learning requires a significant amount of expensive
data annotation. To tackle this limitation, DeepCluster - a simple and scalable
unsupervised pretraining of visual representations - has been proposed.
However, the underlying work of the model is not yet well understood. In this
paper, we analyze DeepCluster internals and exhaustively evaluate the impact of
various hyperparameters over a wide range of values on three different
datasets. Accordingly, we propose an explanation of why the algorithm works in
practice. We also show that DeepCluster convergence and performance highly
depend on the interplay between the quality of the randomly initialized filters
of the convolutional layer and the selected number of clusters. Furthermore, we
demonstrate that continuous clustering is not critical for DeepCluster
convergence. Therefore, early stopping of the clustering phase will reduce the
training time and allow the algorithm to scale to large datasets. Finally, we
derive plausible hyperparameter selection criteria in a semi-supervised
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Image Rescaling using Dual Latent Variables in Invertible Neural Network. (arXiv:2207.11844v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11844">
<div class="article-summary-box-inner">
<span><p>Normalizing flow models have been used successfully for generative image
super-resolution (SR) by approximating complex distribution of natural images
to simple tractable distribution in latent space through Invertible Neural
Networks (INN). These models can generate multiple realistic SR images from one
low-resolution (LR) input using randomly sampled points in the latent space,
simulating the ill-posed nature of image upscaling where multiple
high-resolution (HR) images correspond to the same LR. Lately, the invertible
process in INN has also been used successfully by bidirectional image rescaling
models like IRN and HCFlow for joint optimization of downscaling and inverse
upscaling, resulting in significant improvements in upscaled image quality.
While they are optimized for image downscaling too, the ill-posed nature of
image downscaling, where one HR image could be downsized to multiple LR images
depending on different interpolation kernels and resampling methods, is not
considered. A new downscaling latent variable, in addition to the original one
representing uncertainties in image upscaling, is introduced to model
variations in the image downscaling process. This dual latent variable
enhancement is applicable to different image rescaling models and it is shown
in extensive experiments that it can improve image upscaling accuracy
consistently without sacrificing image quality in downscaled LR images. It is
also shown to be effective in enhancing other INN-based models for image
restoration applications like image hiding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Gait Database for Normal Walk Collected by Smartphone Accelerometer. (arXiv:1905.03109v3 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.03109">
<div class="article-summary-box-inner">
<span><p>Gait recognition is the characterization of unique biometric patterns
associated with each individual which can be utilized to identify a person
without direct contact. A public gait database with a relatively large number
of subjects can provide a great opportunity for future studies to build and
validate gait authentication models. The goal of this study is to introduce a
comprehensive gait database of 93 human subjects who walked between two
endpoints (320 meters) during two different sessions and record their gait data
using two smartphones, one attached to the right thigh and another one on the
left side of the waist. This data is collected to be utilized by a deep
learning-based method that requires enough time points. The metadata including
age, gender, smoking, daily exercise time, height, and weight of an individual
is recorded. this data set is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Test-time Augmentation for Content-based Image Retrieval. (arXiv:2002.01642v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.01642">
<div class="article-summary-box-inner">
<span><p>Off-the-shelf convolutional neural network features achieve outstanding
results in many image retrieval tasks. However, their invariance to target data
is pre-defined by the network architecture and training data. Existing image
retrieval approaches require fine-tuning or modification of pre-trained
networks to adapt to variations unique to the target data. In contrast, our
method enhances the invariance of off-the-shelf features by aggregating
features extracted from images augmented at test-time, with augmentations
guided by a policy learned through reinforcement learning. The learned policy
assigns different magnitudes and weights to the selected transformations, which
are selected from a list of image transformations. Policies are evaluated using
a metric learning protocol to learn the optimal policy. The model converges
quickly and the cost of each policy iteration is minimal as we propose an
off-line caching technique to greatly reduce the computational cost of
extracting features from augmented images. Experimental results on large
trademark retrieval (METU trademark dataset) and landmark retrieval (ROxford5k
and RParis6k scene datasets) tasks show that the learned ensemble of
transformations is highly effective for improving performance, and is
practical, and transferable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining How Deep Neural Networks Forget by Deep Visualization. (arXiv:2005.01004v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.01004">
<div class="article-summary-box-inner">
<span><p>Explaining the behaviors of deep neural networks, usually considered as black
boxes, is critical especially when they are now being adopted over diverse
aspects of human life. Taking the advantages of interpretable machine learning
(interpretable ML), this paper proposes a novel tool called Catastrophic
Forgetting Dissector (or CFD) to explain catastrophic forgetting in continual
learning settings. We also introduce a new method called Critical Freezing
based on the observations of our tool. Experiments on ResNet articulate how
catastrophic forgetting happens, particularly showing which components of this
famous network are forgetting. Our new continual learning algorithm defeats
various recent techniques by a significant margin, proving the capability of
the investigation. Critical freezing not only attacks catastrophic forgetting
but also exposes explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SegGroup: Seg-Level Supervision for 3D Instance and Semantic Segmentation. (arXiv:2012.10217v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10217">
<div class="article-summary-box-inner">
<span><p>Most existing point cloud instance and semantic segmentation methods rely
heavily on strong supervision signals, which require point-level labels for
every point in the scene. However, such strong supervision suffers from large
annotation costs, arousing the need to study efficient annotating. In this
paper, we discover that the locations of instances matter for both instance and
semantic 3D scene segmentation. By fully taking advantage of locations, we
design a weakly-supervised point cloud segmentation method that only requires
clicking on one point per instance to indicate its location for annotation.
With over-segmentation for pre-processing, we extend these location annotations
into segments as seg-level labels. We further design a segment grouping network
(SegGroup) to generate point-level pseudo labels under seg-level labels by
hierarchically grouping the unlabeled segments into the relevant nearby labeled
segments, so that existing point-level supervised segmentation models can
directly consume these pseudo labels for training. Experimental results show
that our seg-level supervised method (SegGroup) achieves comparable results
with the fully annotated point-level supervised methods. Moreover, it
outperforms the recent weakly-supervised methods given a fixed annotation
budget. Code is available at https://github.com/AnTao97/SegGroup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLASTER: Clustering with Reinforcement Learning for Zero-Shot Action Recognition. (arXiv:2101.07042v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07042">
<div class="article-summary-box-inner">
<span><p>Zero-shot action recognition is the task of recognizingaction classes without
visual examples, only with a seman-tic embedding which relates unseen to seen
classes. Theproblem can be seen as learning a function which general-izes well
to instances of unseen classes without losing dis-crimination between classes.
Neural networks can modelthe complex boundaries between visual classes, which
ex-plains their success as supervised models. However, inzero-shot learning,
these highly specialized class bound-aries may not transfer well from seen to
unseen classes.In this paper we propose a centroid-based representation,which
clusters visual and semantic representation, consid-ers all training samples at
once, and in this way generaliz-ing well to instances from unseen classes. We
optimize theclustering using Reinforcement Learning which we show iscritical
for our approach to work. We call the proposedmethod CLASTER and observe that
it consistently outper-forms the state-of-the-art in all standard datasets,
includ-ing UCF101, HMDB51 and Olympic Sports; both in thestandard zero-shot
evaluation and the generalized zero-shotlearning. Further, we show that our
model performs com-petitively in the image domain as well, outperforming
thestate-of-the-art in many settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lidar Point Cloud Guided Monocular 3D Object Detection. (arXiv:2104.09035v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09035">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection is a challenging task in the self-driving and
computer vision community. As a common practice, most previous works use
manually annotated 3D box labels, where the annotating process is expensive. In
this paper, we find that the precisely and carefully annotated labels may be
unnecessary in monocular 3D detection, which is an interesting and
counterintuitive finding. Using rough labels that are randomly disturbed, the
detector can achieve very close accuracy compared to the one using the
ground-truth labels. We delve into this underlying mechanism and then
empirically find that: concerning the label accuracy, the 3D location part in
the label is preferred compared to other parts of labels. Motivated by the
conclusions above and considering the precise LiDAR 3D measurement, we propose
a simple and effective framework, dubbed LiDAR point cloud guided monocular 3D
object detection (LPCG). This framework is capable of either reducing the
annotation costs or considerably boosting the detection accuracy without
introducing extra annotation costs. Specifically, It generates pseudo labels
from unlabeled LiDAR point clouds. Thanks to accurate LiDAR 3D measurements in
3D space, such pseudo labels can replace manually annotated labels in the
training of monocular 3D detectors, since their 3D location information is
precise. LPCG can be applied into any monocular 3D detector to fully use
massive unlabeled data in a self-driving system. As a result, in KITTI
benchmark, we take the first place on both monocular 3D and BEV
(bird's-eye-view) detection with a significant margin. In Waymo benchmark, our
method using 10% labeled data achieves comparable accuracy to the baseline
detector using 100% labeled data. The codes are released at
https://github.com/SPengLiang/LPCG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TextAdaIN: Paying Attention to Shortcut Learning in Text Recognizers. (arXiv:2105.03906v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03906">
<div class="article-summary-box-inner">
<span><p>Leveraging the characteristics of convolutional layers, neural networks are
extremely effective for pattern recognition tasks. However in some cases, their
decisions are based on unintended information leading to high performance on
standard benchmarks but also to a lack of generalization to challenging testing
conditions and unintuitive failures. Recent work has termed this "shortcut
learning" and addressed its presence in multiple domains. In text recognition,
we reveal another such shortcut, whereby recognizers overly depend on local
image statistics. Motivated by this, we suggest an approach to regulate the
reliance on local statistics that improves text recognition performance.
</p>
<p>Our method, termed TextAdaIN, creates local distortions in the feature map
which prevent the network from overfitting to local statistics. It does so by
viewing each feature map as a sequence of elements and deliberately mismatching
fine-grained feature statistics between elements in a mini-batch. Despite
TextAdaIN's simplicity, extensive experiments show its effectiveness compared
to other, more complicated methods. TextAdaIN achieves state-of-the-art results
on standard handwritten text recognition benchmarks. It generalizes to multiple
architectures and to the domain of scene text recognition. Furthermore, we
demonstrate that integrating TextAdaIN improves robustness towards more
challenging testing conditions. The official Pytorch implementation can be
found at https://github.com/amazon-research/textadain-robust-recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KVT: k-NN Attention for Boosting Vision Transformers. (arXiv:2106.00515v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00515">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) have dominated computer vision for
years, due to its ability in capturing locality and translation invariance.
Recently, many vision transformer architectures have been proposed and they
show promising performance. A key component in vision transformers is the
fully-connected self-attention which is more powerful than CNNs in modelling
long range dependencies. However, since the current dense self-attention uses
all image patches (tokens) to compute attention matrix, it may neglect locality
of images patches and involve noisy tokens (e.g., clutter background and
occlusion), leading to a slow training process and potential degradation of
performance. To address these problems, we propose the $k$-NN attention for
boosting vision transformers. Specifically, instead of involving all the tokens
for attention matrix calculation, we only select the top-$k$ similar tokens
from the keys for each query to compute the attention map. The proposed $k$-NN
attention naturally inherits the local bias of CNNs without introducing
convolutional operations, as nearby tokens tend to be more similar than others.
In addition, the $k$-NN attention allows for the exploration of long range
correlation and at the same time filters out irrelevant tokens by choosing the
most similar tokens from the entire image. Despite its simplicity, we verify,
both theoretically and empirically, that $k$-NN attention is powerful in
speeding up training and distilling noise from input tokens. Extensive
experiments are conducted by using 11 different vision transformer
architectures to verify that the proposed $k$-NN attention can work with any
existing transformer architectures to improve its prediction performance. The
codes are available at \url{https://github.com/damo-cv/KVT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">2D vs. 3D LiDAR-based Person Detection on Mobile Robots. (arXiv:2106.11239v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11239">
<div class="article-summary-box-inner">
<span><p>Person detection is a crucial task for mobile robots navigating in
human-populated environments. LiDAR sensors are promising for this task, thanks
to their accurate depth measurements and large field of view. Two types of
LiDAR sensors exist: the 2D LiDAR sensors, which scan a single plane, and the
3D LiDAR sensors, which scan multiple planes, thus forming a volume. How do
they compare for the task of person detection? To answer this, we conduct a
series of experiments, using the public, large-scale JackRabbot dataset and the
state-of-the-art 2D and 3D LiDAR-based person detectors (DR-SPAAM and
CenterPoint respectively). Our experiments include multiple aspects, ranging
from the basic performance and speed comparison, to more detailed analysis on
localization accuracy and robustness against distance and scene clutter. The
insights from these experiments highlight the strengths and weaknesses of 2D
and 3D LiDAR sensors as sources for person detection, and are especially
valuable for designing mobile robots that will operate in close proximity to
surrounding humans (e.g. service or social robot).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Self-supervised Augmented Knowledge Distillation. (arXiv:2107.13715v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13715">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation often involves how to define and transfer knowledge
from teacher to student effectively. Although recent self-supervised
contrastive knowledge achieves the best performance, forcing the network to
learn such knowledge may damage the representation learning of the original
class recognition task. We therefore adopt an alternative self-supervised
augmented task to guide the network to learn the joint distribution of the
original recognition task and self-supervised auxiliary task. It is
demonstrated as a richer knowledge to improve the representation power without
losing the normal classification capability. Moreover, it is incomplete that
previous methods only transfer the probabilistic knowledge between the final
layers. We propose to append several auxiliary classifiers to hierarchical
intermediate feature maps to generate diverse self-supervised knowledge and
perform the one-to-one transfer to teach the student network thoroughly. Our
method significantly surpasses the previous SOTA SSKD with an average
improvement of 2.56\% on CIFAR-100 and an improvement of 0.77\% on ImageNet
across widely used network pairs. Codes are available at
https://github.com/winycg/HSAKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation. (arXiv:2107.13824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13824">
<div class="article-summary-box-inner">
<span><p>In recent years, sparse voxel-based methods have become the state-of-the-arts
for 3D semantic segmentation of indoor scenes, thanks to the powerful 3D CNNs.
Nevertheless, being oblivious to the underlying geometry, voxel-based methods
suffer from ambiguous features on spatially close objects and struggle with
handling complex and irregular geometries due to the lack of geodesic
information. In view of this, we present Voxel-Mesh Network (VMNet), a novel 3D
deep architecture that operates on the voxel and mesh representations
leveraging both the Euclidean and geodesic information. Intuitively, the
Euclidean information extracted from voxels can offer contextual cues
representing interactions between nearby objects, while the geodesic
information extracted from meshes can help separate objects that are spatially
close but have disconnected surfaces. To incorporate such information from the
two domains, we design an intra-domain attentive module for effective feature
aggregation and an inter-domain attentive module for adaptive feature fusion.
Experimental results validate the effectiveness of VMNet: specifically, on the
challenging ScanNet dataset for large-scale segmentation of indoor scenes, it
outperforms the state-of-the-art SparseConvNet and MinkowskiNet (74.6% vs 72.5%
and 73.6% in mIoU) with a simpler network structure (17M vs 30M and 38M
parameters). Code release: https://github.com/hzykent/VMNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Scene Decoration from a Single Photograph. (arXiv:2108.01806v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01806">
<div class="article-summary-box-inner">
<span><p>Furnishing and rendering indoor scenes has been a long-standing task for
interior design, where artists create a conceptual design for the space, build
a 3D model of the space, decorate, and then perform rendering. Although the
task is important, it is tedious and requires tremendous effort. In this paper,
we introduce a new problem of domain-specific indoor scene image synthesis,
namely neural scene decoration. Given a photograph of an empty indoor space and
a list of decorations with layout determined by user, we aim to synthesize a
new image of the same space with desired furnishing and decorations. Neural
scene decoration can be applied to create conceptual interior designs in a
simple yet effective manner. Our attempt to this research problem is a novel
scene generation architecture that transforms an empty scene and an object
layout into a realistic furnished scene photograph. We demonstrate the
performance of our proposed method by comparing it with conditional image
synthesis baselines built upon prevailing image translation approaches both
qualitatively and quantitatively. We conduct extensive experiments to further
validate the plausibility and aesthetics of our generated scenes. Our
implementation is available at
\url{https://github.com/hkust-vgd/neural_scene_decoration}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors. (arXiv:2109.00182v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00182">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel local descriptor-based framework, called
You Only Hypothesize Once (YOHO), for the registration of two unaligned point
clouds. In contrast to most existing local descriptors which rely on a fragile
local reference frame to gain rotation invariance, the proposed descriptor
achieves the rotation invariance by recent technologies of group equivariant
feature learning, which brings more robustness to point density and noise.
Meanwhile, the descriptor in YOHO also has a rotation equivariant part, which
enables us to estimate the registration from just one correspondence
hypothesis. Such property reduces the searching space for feasible
transformations, thus greatly improves both the accuracy and the efficiency of
YOHO. Extensive experiments show that YOHO achieves superior performances with
much fewer needed RANSAC iterations on four widely-used datasets, the
3DMatch/3DLoMatch datasets, the ETH dataset and the WHU-TLS dataset. More
details are shown in our project page: https://hpwang-whu.github.io/YOHO/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution. (arXiv:2109.03075v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03075">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) is an effective framework that aims to transfer
meaningful information from a large teacher to a smaller student. Generally, KD
often involves how to define and transfer knowledge. Previous KD methods often
focus on mining various forms of knowledge, for example, feature maps and
refined information. However, the knowledge is derived from the primary
supervised task and thus is highly task-specific. Motivated by the recent
success of self-supervised representation learning, we propose an auxiliary
self-supervision augmented task to guide networks to learn more meaningful
features. Therefore, we can derive soft self-supervision augmented
distributions as richer dark knowledge from this task for KD. Unlike previous
knowledge, this distribution encodes joint knowledge from supervised and
self-supervised feature learning. Beyond knowledge exploration, we propose to
append several auxiliary branches at various hidden layers, to fully take
advantage of hierarchical feature maps. Each auxiliary branch is guided to
learn self-supervision augmented task and distill this distribution from
teacher to student. Overall, we call our KD method as Hierarchical
Self-Supervision Augmented Knowledge Distillation (HSSAKD). Experiments on
standard image classification show that both offline and online HSSAKD achieves
state-of-the-art performance in the field of KD. Further transfer experiments
on object detection further verify that HSSAKD can guide the network to learn
better features. The code is available at https://github.com/winycg/HSAKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clustering performance analysis using a new correlation-based cluster validity index. (arXiv:2109.11172v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11172">
<div class="article-summary-box-inner">
<span><p>There are various cluster validity indices used for evaluating clustering
results. One of the main objectives of using these indices is to seek the
optimal unknown number of clusters. Some indices work well for clusters with
different densities, sizes, and shapes. Yet, one shared weakness of those
validity indices is that they often provide only one optimal number of
clusters. That number is unknown in real-world problems, and there might be
more than one possible option. We develop a new cluster validity index based on
a correlation between an actual distance between a pair of data points and a
centroid distance of clusters that the two points occupy. Our proposed index
constantly yields several local peaks and overcomes the previously stated
weakness. Several experiments in different scenarios, including UCI real-world
data sets, have been conducted to compare the proposed validity index with
several well-known ones. An R package related to this new index called NCvalid
is available at https://github.com/nwiroonsri/NCvalid.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization. (arXiv:2109.14549v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14549">
<div class="article-summary-box-inner">
<span><p>Developing robust vision-guided controllers for quadrupedal robots in complex
environments, with various obstacles, dynamical surroundings and uneven
terrains, is very challenging. While Reinforcement Learning (RL) provides a
promising paradigm for agile locomotion skills with vision inputs in
simulation, it is still very challenging to deploy the RL policy in the real
world. Our key insight is that aside from the discrepancy in the domain gap, in
visual appearance between the simulation and the real world, the latency from
the control pipeline is also a major cause of difficulty. In this paper, we
propose Multi-Modal Delay Randomization (MMDR) to address this issue when
training RL agents. Specifically, we simulate the latency of real hardware by
using past observations, sampled with randomized periods, for both
proprioception and vision. We train the RL policy for end-to-end control in a
physical simulator without any predefined controller or reference motion, and
directly deploy it on the real A1 quadruped robot running in the wild. We
evaluate our method in different outdoor environments with complex terrains and
obstacles. We demonstrate the robot can smoothly maneuver at a high speed,
avoid the obstacles, and show significant improvement over the baselines. Our
project page with videos is at https://mehooz.github.io/mmdr-wild/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Synthetic Anomalies for Self-Supervised Anomaly Detection and Localization. (arXiv:2109.15222v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15222">
<div class="article-summary-box-inner">
<span><p>We introduce a simple and intuitive self-supervision task, Natural Synthetic
Anomalies (NSA), for training an end-to-end model for anomaly detection and
localization using only normal training data. NSA integrates Poisson image
editing to seamlessly blend scaled patches of various sizes from separate
images. This creates a wide range of synthetic anomalies which are more similar
to natural sub-image irregularities than previous data-augmentation strategies
for self-supervised anomaly detection. We evaluate the proposed method using
natural and medical images. Our experiments with the MVTec AD dataset show that
a model trained to localize NSA anomalies generalizes well to detecting
real-world a priori unknown types of manufacturing defects. Our method achieves
an overall detection AUROC of 97.2 outperforming all previous methods that
learn without the use of additional datasets. Code available at
https://github.com/hmsch/natural-synthetic-anomalies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Construction Site Safety Monitoring and Excavator Activity Analysis System. (arXiv:2110.03083v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03083">
<div class="article-summary-box-inner">
<span><p>With the recent advancements in deep learning and computer vision, the
AI-powered construction machine such as autonomous excavator has made
significant progress. Safety is the most important section in modern
construction, where construction machines are more and more automated. In this
paper, we propose a vision-based excavator perception, activity analysis, and
safety monitoring system. Our perception system could detect multi-class
construction machines and humans in real-time while estimating the poses and
actions of the excavator. Then, we present a novel safety monitoring and
excavator activity analysis system based on the perception result. To evaluate
the performance of our method, we collect a dataset using the Autonomous
Excavator System (AES) including multi-class of objects in different lighting
conditions with human annotations. We also evaluate our method on a benchmark
construction dataset. The results showed our YOLO v5 multi-class objects
detection model improved inference speed by 8 times (YOLO v5 x-large) to 34
times (YOLO v5 small) compared with Faster R-CNN/ YOLO v3 model. Furthermore,
the accuracy of YOLO v5 models is improved by 2.7% (YOLO v5 x-large) while
model size is reduced by 63.9% (YOLO v5 x-large) to 93.9% (YOLO v5 small). The
experimental results show that the proposed action recognition approach
outperforms the state-of-the-art approaches on top-1 accuracy by about 5.18%.
The proposed real-time safety monitoring system is not only designed for our
Autonomous Excavator System (AES) in solid waste scenes, it can also be applied
to general construction scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating generative networks using Gaussian mixtures of image features. (arXiv:2110.05240v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05240">
<div class="article-summary-box-inner">
<span><p>We develop a measure for evaluating the performance of generative networks
given two sets of images. A popular performance measure currently used to do
this is the Fr\'echet Inception Distance (FID). FID assumes that images
featurized using the penultimate layer of Inception-v3 follow a Gaussian
distribution, an assumption which cannot be violated if we wish to use FID as a
metric. However, we show that Inception-v3 features of the ImageNet dataset are
not Gaussian; in particular, every single marginal is not Gaussian. To remedy
this problem, we model the featurized images using Gaussian mixture models
(GMMs) and compute the 2-Wasserstein distance restricted to GMMs. We define a
performance measure, which we call WaM, on two sets of images by using
Inception-v3 (or another classifier) to featurize the images, estimate two
GMMs, and use the restricted $2$-Wasserstein distance to compare the GMMs. We
experimentally show the advantages of WaM over FID, including how FID is more
sensitive than WaM to imperceptible image perturbations. By modelling the
non-Gaussian features obtained from Inception-v3 as GMMs and using a GMM
metric, we can more accurately evaluate generative network performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attack across Datasets. (arXiv:2110.07718v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07718">
<div class="article-summary-box-inner">
<span><p>Existing transfer attack methods commonly assume that the attacker knows the
training set (e.g., the label set, the input size) of the black-box victim
models, which is usually unrealistic because in some cases the attacker cannot
know this information. In this paper, we define a Generalized Transferable
Attack (GTA) problem where the attacker doesn't know this information and is
acquired to attack any randomly encountered images that may come from unknown
datasets. To solve the GTA problem, we propose a novel Image Classification
Eraser (ICE) that trains a particular attacker to erase classification
information of any images from arbitrary datasets. Experiments on several
datasets demonstrate that ICE greatly outperforms existing transfer attacks on
GTA, and show that ICE uses similar texture-like noises to perturb different
images from different datasets. Moreover, fast fourier transformation analysis
indicates that the main components in each ICE noise are three sine waves for
the R, G, and B image channels. Inspired by this interesting finding, we then
design a novel Sine Attack (SA) method to optimize the three sine waves.
Experiments show that SA performs comparably to ICE, indicating that the three
sine waves are effective and enough to break DNNs under the GTA setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Generating Identifiable Virtual Faces. (arXiv:2110.07986v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07986">
<div class="article-summary-box-inner">
<span><p>Face anonymization with generative models have become increasingly prevalent
since they sanitize private information by generating virtual face images,
ensuring both privacy and image utility. Such virtual face images are usually
not identifiable after the removal or protection of the original identity. In
this paper, we formalize and tackle the problem of generating identifiable
virtual face images. Our virtual face images are visually different from the
original ones for privacy protection. In addition, they are bound with new
virtual identities, which can be directly used for face recognition. We propose
an Identifiable Virtual Face Generator (IVFG) to generate the virtual face
images. The IVFG projects the latent vectors of the original face images into
virtual ones according to a user specific key, based on which the virtual face
images are generated. To make the virtual face images identifiable, we propose
a multi-task learning objective as well as a triplet styled training strategy
to learn the IVFG. We evaluate the performance of our virtual face images using
different face recognizers on diffident face image datasets, all of which
demonstrate the effectiveness of the IVFG for generate identifiable virtual
face images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NYU-VPR: Long-Term Visual Place Recognition Benchmark with View Direction and Data Anonymization Influences. (arXiv:2110.09004v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09004">
<div class="article-summary-box-inner">
<span><p>Visual place recognition (VPR) is critical in not only localization and
mapping for autonomous driving vehicles, but also in assistive navigation for
the visually impaired population. To enable a long-term VPR system on a large
scale, several challenges need to be addressed. First, different applications
could require different image view directions, such as front views for
self-driving cars while side views for the low vision people. Second, VPR in
metropolitan scenes can often cause privacy concerns due to the imaging of
pedestrian and vehicle identity information, calling for the need for data
anonymization before VPR queries and database construction. Both factors could
lead to VPR performance variations that are not well understood yet. To study
their influences, we present the NYU-VPR dataset that contains more than
200,000 images over a 2km by 2km area near the New York University campus,
taken within the whole year of 2016. We present benchmark results on several
popular VPR algorithms showing that side views are significantly more
challenging for current VPR methods while the influence of data anonymization
is almost negligible, together with our hypothetical explanations and in-depth
analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13214">
<div class="article-summary-box-inner">
<span><p>Current visual question answering (VQA) tasks mainly consider answering
human-annotated questions for natural images. However, aside from natural
images, abstract diagrams with semantic richness are still understudied in
visual understanding and reasoning research. In this work, we introduce a new
challenge of Icon Question Answering (IconQA) with the goal of answering a
question in an icon image context. We release IconQA, a large-scale dataset
that consists of 107,439 questions and three sub-tasks: multi-image-choice,
multi-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by
real-world diagram word problems that highlight the importance of abstract
diagram understanding and comprehensive cognitive reasoning. Thus, IconQA
requires not only perception skills like object recognition and text
understanding, but also diverse cognitive reasoning skills, such as geometric
reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate
potential IconQA models to learn semantic representations for icon images, we
further release an icon dataset Icon645 which contains 645,687 colored icons on
377 classes. We conduct extensive user studies and blind experiments and
reproduce a wide range of advanced VQA methods to benchmark the IconQA task.
Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid
cross-modal Transformer with input diagram embeddings pre-trained on the icon
dataset. IconQA and Icon645 are available at https://iconqa.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Item Fashion Recommender: Towards Cross-Domain Recommendations. (arXiv:2111.00758v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00758">
<div class="article-summary-box-inner">
<span><p>Nowadays, recommender systems and search engines play an integral role in
fashion e-commerce. Still, many challenges lie ahead, and this study tries to
tackle some. This article first suggests a content-based fashion recommender
system that uses a parallel neural network to take a single fashion item shop
image as input and make in-shop recommendations by listing similar items
available in the store. Next, the same structure is enhanced to personalize the
results based on user preferences. This work then introduces a background
augmentation technique that makes the system more robust to out-of-domain
queries, enabling it to make street-to-shop recommendations using only a
training set of catalog shop images. Moreover, the last contribution of this
paper is a new evaluation metric for recommendation tasks called
objective-guided human score. This method is an entirely customizable framework
that produces interpretable, comparable scores from subjective evaluations of
human scorers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Semi-supervised Video Object Segmentation Problem from a Cyclic Perspective. (arXiv:2111.01323v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01323">
<div class="article-summary-box-inner">
<span><p>Modern video object segmentation (VOS) algorithms have achieved remarkably
high performance in a sequential processing order, while most of currently
prevailing pipelines still show some obvious inadequacy like accumulative
error, unknown robustness or lack of proper interpretation tools. In this
paper, we place the semi-supervised video object segmentation problem into a
cyclic workflow and find the defects above can be collectively addressed via
the inherent cyclic property of semi-supervised VOS systems. Firstly, a cyclic
mechanism incorporated to the standard sequential flow can produce more
consistent representations for pixel-wise correspondance. Relying on the
accurate reference mask in the starting frame, we show that the error
propagation problem can be mitigated. Next, a simple gradient correction
module, which naturally extends the offline cyclic pipeline to an online
manner, can highlight the high-frequent and detailed part of results to further
improve the segmentation quality while keeping feasible computation cost.
Meanwhile such correction can protect the network from severe performance
degration resulted from interference signals. Finally we develop cycle
effective receptive field (cycle-ERF) based on gradient correction process to
provide a new perspective into analyzing object-specific regions of interests.
We conduct comprehensive comparison and detailed analysis on challenging
benchmarks of DAVIS16, DAVIS17 and Youtube-VOS, demonstrating that the cyclic
mechanism is helpful to enhance segmentation quality, improve the robustness of
VOS systems, and further provide qualitative comparison and interpretation on
how different VOS algorithms work. The code of this project can be found at
https://github.com/lyxok1/STM-Training
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sliced Recursive Transformer. (arXiv:2111.05297v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05297">
<div class="article-summary-box-inner">
<span><p>We present a neat yet effective recursive operation on vision transformers
that can improve parameter utilization without involving additional parameters.
This is achieved by sharing weights across the depth of transformer networks.
The proposed method can obtain a substantial gain (~2%) simply using naive
recursive operation, requires no special or sophisticated knowledge for
designing principles of networks, and introduces minimal computational overhead
to the training procedure. To reduce the additional computation caused by
recursive operation while maintaining the superior accuracy, we propose an
approximating method through multiple sliced group self-attentions across
recursive layers which can reduce the cost consumption by 10~30% with minimal
performance loss. We call our model Sliced Recursive Transformer (SReT), a
novel and parameter-efficient vision transformer design that is compatible with
a broad range of other designs for efficient ViT architectures. Our best model
establishes significant improvement on ImageNet-1K over state-of-the-art
methods while containing fewer parameters. The proposed weight sharing
mechanism by sliced recursion structure allows us to build a transformer with
more than 100 or even 1000 shared layers with ease while keeping a compact size
(13~15M), to avoid optimization difficulties when the model is too large. The
flexible scalability has shown great potential for scaling up models and
constructing extremely deep vision transformers. Code is available at
https://github.com/szq0214/SReT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Document Generator for Annotation-free Layout Recognition. (arXiv:2111.06016v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06016">
<div class="article-summary-box-inner">
<span><p>Analyzing the layout of a document to identify headers, sections, tables,
figures etc. is critical to understanding its content. Deep learning based
approaches for detecting the layout structure of document images have been
promising. However, these methods require a large number of annotated examples
during training, which are both expensive and time consuming to obtain. We
describe here a synthetic document generator that automatically produces
realistic documents with labels for spatial positions, extents and categories
of the layout elements. The proposed generative process treats every physical
component of a document as a random variable and models their intrinsic
dependencies using a Bayesian Network graph. Our hierarchical formulation using
stochastic templates allow parameter sharing between documents for retaining
broad themes and yet the distributional characteristics produces visually
unique samples, thereby capturing complex and diverse layouts. We empirically
illustrate that a deep layout detection model trained purely on the synthetic
documents can match the performance of a model that uses real documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SWAT: Spatial Structure Within and Among Tokens. (arXiv:2111.13677v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13677">
<div class="article-summary-box-inner">
<span><p>Modeling visual data as tokens (i.e., image patches) using attention
mechanisms, feed-forward networks or convolutions has been highly effective in
recent years. Such methods usually have a common pipeline: a tokenization
method, followed by a set of layers/blocks for information mixing, both within
and among tokens. When image patches are converted into tokens, they are often
flattened, discarding the spatial structure within each patch. As a result, any
processing that follows (eg: multi-head self-attention) may fail to recover
and/or benefit from such information. In this paper, we argue that models can
have significant gains when spatial structure is preserved during tokenization,
and is explicitly used during the mixing stage. We propose two key
contributions: (1) Structure-aware Tokenization and, (2) Structure-aware
Mixing, both of which can be combined with existing models with minimal effort.
We introduce a family of models (SWAT), showing improvements over the likes of
DeiT, MLP-Mixer and Swin Transformer, across multiple benchmarks including
ImageNet classification and ADE20K segmentation. Our code and models will be
released online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OOD-CV: A Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images. (arXiv:2111.14341v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14341">
<div class="article-summary-box-inner">
<span><p>Enhancing the robustness of vision algorithms in real-world scenarios is
challenging. One reason is that existing robustness benchmarks are limited, as
they either rely on synthetic data or ignore the effects of individual nuisance
factors. We introduce OOD-CV, a benchmark dataset that includes
out-of-distribution examples of 10 object categories in terms of pose, shape,
texture, context and the weather conditions, and enables benchmarking models
for image classification, object detection, and 3D pose estimation. In addition
to this novel dataset, we contribute extensive experiments using popular
baseline methods, which reveal that: 1. Some nuisance factors have a much
stronger negative effect on the performance compared to others, also depending
on the vision task. 2. Current approaches to enhance robustness have only
marginal effects, and can even reduce robustness. 3. We do not observe
significant differences between convolutional and transformer architectures. We
believe our dataset provides a rich testbed to study robustness and will help
push forward research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Neural Networks. (arXiv:2112.00891v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00891">
<div class="article-summary-box-inner">
<span><p>Video data is often repetitive; for example, the contents of adjacent frames
are usually strongly correlated. Such redundancy occurs at multiple levels of
complexity, from low-level pixel values to textures and high-level semantics.
We propose Event Neural Networks (EvNets), which leverage this redundancy to
achieve considerable computation savings during video inference. A defining
characteristic of EvNets is that each neuron has state variables that provide
it with long-term memory, which allows low-cost, high-accuracy inference even
in the presence of significant camera motion. We show that it is possible to
transform a wide range of neural networks into EvNets without re-training. We
demonstrate our method on state-of-the-art architectures for both high- and
low-level visual processing, including pose recognition, object detection,
optical flow, and image enhancement. We observe roughly an order-of-magnitude
reduction in computational costs compared to conventional networks, with
minimal reductions in model accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransFGU: A Top-down Approach to Fine-Grained Unsupervised Semantic Segmentation. (arXiv:2112.01515v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01515">
<div class="article-summary-box-inner">
<span><p>Unsupervised semantic segmentation aims to obtain high-level semantic
representation on low-level visual features without manual annotations. Most
existing methods are bottom-up approaches that try to group pixels into regions
based on their visual cues or certain predefined rules. As a result, it is
difficult for these bottom-up approaches to generate fine-grained semantic
segmentation when coming to complicated scenes with multiple objects and some
objects sharing similar visual appearance. In contrast, we propose the first
top-down unsupervised semantic segmentation framework for fine-grained
segmentation in extremely complicated scenarios. Specifically, we first obtain
rich high-level structured semantic concept information from large-scale vision
data in a self-supervised learning manner, and use such information as a prior
to discover potential semantic categories presented in target datasets.
Secondly, the discovered high-level semantic categories are mapped to low-level
pixel features by calculating the class activate map (CAM) with respect to
certain discovered semantic representation. Lastly, the obtained CAMs serve as
pseudo labels to train the segmentation module and produce the final semantic
segmentation. Experimental results on multiple semantic segmentation benchmarks
show that our top-down unsupervised segmentation is robust to both
object-centric and scene-centric datasets under different semantic granularity
levels, and outperforms all the current state-of-the-art bottom-up methods. Our
code is available at \url{https://github.com/damo-cv/TransFGU}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration. (arXiv:2112.01740v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01740">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection has attracted increasing attention and rapidly
progressed in recent years. However, the requirement of an exhaustive offline
fine-tuning stage in existing methods is time-consuming and significantly
hinders their usage in online applications such as autonomous exploration of
low-power robots. We find that their major limitation is that the little but
valuable information from a few support images is not fully exploited. To solve
this problem, we propose a brand new architecture, AirDet, and surprisingly
find that, by learning class-agnostic relation with the support images in all
modules, including cross-scale object proposal network, shots aggregation
module, and localization network, AirDet without fine-tuning achieves
comparable or even better results than many fine-tuned methods, reaching up to
30-40% improvements. We also present solid results of onboard tests on
real-world exploration data from the DARPA Subterranean Challenge, which
strongly validate the feasibility of AirDet in robotics. To the best of our
knowledge, AirDet is the first feasible few-shot detection method for
autonomous exploration of low-power robots. The code and pre-trained models are
released at https://github.com/Jaraxxus-Me/AirDet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROCA: Robust CAD Model Retrieval and Alignment from a Single Image. (arXiv:2112.01988v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01988">
<div class="article-summary-box-inner">
<span><p>We present ROCA, a novel end-to-end approach that retrieves and aligns 3D CAD
models from a shape database to a single input image. This enables 3D
perception of an observed scene from a 2D RGB observation, characterized as a
lightweight, compact, clean CAD representation. Core to our approach is our
differentiable alignment optimization based on dense 2D-3D object
correspondences and Procrustes alignment. ROCA can thus provide a robust CAD
alignment while simultaneously informing CAD retrieval by leveraging the 2D-3D
correspondences to learn geometrically similar CAD models. Experiments on
challenging, real-world imagery from ScanNet show that ROCA significantly
improves on state of the art, from 9.5% to 17.6% in retrieval-aware CAD
alignment accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coupling Vision and Proprioception for Navigation of Legged Robots. (arXiv:2112.02094v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02094">
<div class="article-summary-box-inner">
<span><p>We exploit the complementary strengths of vision and proprioception to
develop a point-goal navigation system for legged robots, called VP-Nav. Legged
systems are capable of traversing more complex terrain than wheeled robots, but
to fully utilize this capability, we need a high-level path planner in the
navigation system to be aware of the walking capabilities of the low-level
locomotion policy in varying environments. We achieve this by using
proprioceptive feedback to ensure the safety of the planned path by sensing
unexpected obstacles like glass walls, terrain properties like slipperiness or
softness of the ground and robot properties like extra payload that are likely
missed by vision. The navigation system uses onboard cameras to generate an
occupancy map and a corresponding cost map to reach the goal. A fast marching
planner then generates a target path. A velocity command generator takes this
as input to generate the desired velocity for the walking policy. A safety
advisor module adds sensed unexpected obstacles to the occupancy map and
environment-determined speed limits to the velocity command generator. We show
superior performance compared to wheeled robot baselines, and ablation studies
which have disjoint high-level planning and low-level control. We also show the
real-world deployment of VP-Nav on a quadruped robot with onboard sensors and
computation. Videos at https://navigation-locomotion.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLT: Bidirectional Layout Transformer for Controllable Layout Generation. (arXiv:2112.05112v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05112">
<div class="article-summary-box-inner">
<span><p>Creating visual layouts is a critical step in graphic design. Automatic
generation of such layouts is essential for scalable and diverse visual
designs. To advance conditional layout generation, we introduce BLT, a
bidirectional layout transformer. BLT differs from previous work on
transformers in adopting non-autoregressive transformers. In training, BLT
learns to predict the masked attributes by attending to surrounding attributes
in two directions. During inference, BLT first generates a draft layout from
the input and then iteratively refines it into a high-quality layout by masking
out low-confident attributes. The masks generated in both training and
inference are controlled by a new hierarchical sampling policy. We verify the
proposed model on six benchmarks of diverse design tasks. Experimental results
demonstrate two benefits compared to the state-of-the-art layout transformer
models. First, our model empowers layout transformers to fulfill controllable
layout generation. Second, it achieves up to 10x speedup in generating a layout
at inference time than the layout transformer baseline. Code is released at
https://shawnkx.github.io/blt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering. (arXiv:2112.05504v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05504">
<div class="article-summary-box-inner">
<span><p>Neural radiance fields (NeRF) has achieved outstanding performance in
modeling 3D objects and controlled scenes, usually under a single scale. In
this work, we focus on multi-scale cases where large changes in imagery are
observed at drastically different scales. This scenario vastly exists in
real-world 3D environments, such as city scenes, with views ranging from
satellite level that captures the overview of a city, to ground level imagery
showing complex details of an architecture; and can also be commonly identified
in landscape and delicate minecraft 3D models. The wide span of viewing
positions within these scenes yields multi-scale renderings with very different
levels of detail, which poses great challenges to neural radiance field and
biases it towards compromised results. To address these issues, we introduce
BungeeNeRF, a progressive neural radiance field that achieves level-of-detail
rendering across drastically varied scales. Starting from fitting distant views
with a shallow base block, as training progresses, new blocks are appended to
accommodate the emerging details in the increasingly closer views. The strategy
progressively activates high-frequency channels in NeRF's positional encoding
inputs and successively unfolds more complex details as the training proceeds.
We demonstrate the superiority of BungeeNeRF in modeling diverse multi-scale
scenes with drastically varying views on multiple data sources (city models,
synthetic, and drone captured data) and its support for high-quality rendering
in different levels of detail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality. (arXiv:2112.05892v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05892">
<div class="article-summary-box-inner">
<span><p>Group Activity Recognition detects the activity collectively performed by a
group of actors, which requires compositional reasoning of actors and objects.
We approach the task by modeling the video as tokens that represent the
multi-scale semantic concepts in the video. We propose COMPOSER, a Multiscale
Transformer based architecture that performs attention-based reasoning over
tokens at each scale and learns group activity compositionally. In addition,
prior works suffer from scene biases with privacy and ethical concerns. We only
use the keypoint modality which reduces scene biases and prevents acquiring
detailed visual data that may contain private or biased information of users.
We improve the multiscale representations in COMPOSER by clustering the
intermediate scale representations, while maintaining consistent cluster
assignments between scales. Finally, we use techniques such as auxiliary
prediction and data augmentations tailored to the keypoint signals to aid model
training. We demonstrate the model's strength and interpretability on two
widely-used datasets (Volleyball and Collective Activity). COMPOSER achieves up
to +5.4% improvement with just the keypoint modality. Code is available at
https://github.com/hongluzhou/composer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EgoBody: Human Body Shape and Motion of Interacting People from Head-Mounted Devices. (arXiv:2112.07642v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07642">
<div class="article-summary-box-inner">
<span><p>Understanding social interactions from egocentric views is crucial for many
applications, ranging from assistive robotics to AR/VR. Key to reasoning about
interactions is to understand the body pose and motion of the interaction
partner from the egocentric view. However, research in this area is severely
hindered by the lack of datasets. Existing datasets are limited in terms of
either size, capture/annotation modalities, ground-truth quality, or
interaction diversity. We fill this gap by proposing EgoBody, a novel
large-scale dataset for human pose, shape and motion estimation from egocentric
views, during interactions in complex 3D scenes. We employ Microsoft HoloLens2
headsets to record rich egocentric data streams (including RGB, depth, eye
gaze, head and hand tracking). To obtain accurate 3D ground truth, we calibrate
the headset with a multi-Kinect rig and fit expressive SMPL-X body meshes to
multi-view RGB-D frames, reconstructing 3D human shapes and poses relative to
the scene, over time. We collect 125 sequences, spanning diverse interaction
scenarios, and propose the first benchmark for 3D full-body pose and shape
estimation of the social partner from egocentric views. We extensively evaluate
state-of-the-art methods, highlight their limitations in the egocentric
scenario, and address such limitations leveraging our high-quality annotations.
Data and code are available at
https://sanweiliti.github.io/egobody/egobody.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Image Synthesis and Editing: A Survey. (arXiv:2112.13592v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13592">
<div class="article-summary-box-inner">
<span><p>As information exists in various modalities in real world, effective
interaction and fusion among multimodal information plays a key role for the
creation and perception of multimodal data in computer vision and deep learning
research. With superb power in modelling the interaction among multimodal
information, multimodal image synthesis and editing has become a hot research
topic in recent years. Instead of providing explicit guidance for network
training, multimodal guidance offers intuitive and flexible means for image
synthesis and editing. On the other hand, this field is also facing several
challenges in alignment of features with inherent modality gaps, synthesis of
high-resolution images, faithful evaluation metrics, etc. In this survey, we
comprehensively contextualize the advance of the recent multimodal image
synthesis and editing and formulate taxonomies according to data modality and
model architectures. We start with an introduction to different types of
guidance modalities in image synthesis and editing. We then describe multimodal
image synthesis and editing approaches extensively with detailed frameworks
including Generative Adversarial Networks (GANs), Auto-regressive models,
Diffusion models, Neural Radiance Fields (NeRF) and other methods. This is
followed by a comprehensive description of benchmark datasets and corresponding
evaluation metrics as widely adopted in multimodal image synthesis and editing,
as well as detailed comparisons of various synthesis methods with analysis of
respective advantages and limitations. Finally, we provide insights about the
current research challenges and possible directions for future research. We
hope this survey could lay a sound and valuable foundation for future
development of multimodal image synthesis and editing. A project associated
with this survey is available at https://github.com/fnzhan/MISE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invariance encoding in sliced-Wasserstein space for image classification with limited training data. (arXiv:2201.02980v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02980">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks (CNNs) are broadly considered to be
state-of-the-art generic end-to-end image classification systems. However, they
are known to underperform when training data are limited and thus require data
augmentation strategies that render the method computationally expensive and
not always effective. Rather than using a data augmentation strategy to encode
invariances as typically done in machine learning, here we propose to
mathematically augment a nearest subspace classification model in
sliced-Wasserstein space by exploiting certain mathematical properties of the
Radon Cumulative Distribution Transform (R-CDT), a recently introduced image
transform. We demonstrate that for a particular type of learning problem, our
mathematical solution has advantages over data augmentation with deep CNNs in
terms of classification accuracy and computational complexity, and is
particularly effective under a limited training data setting. The method is
simple, effective, computationally efficient, non-iterative, and requires no
parameters to be tuned. Python code implementing our method is available at
https://github.com/rohdelab/mathematical_augmentation. Our method is integrated
as a part of the software package PyTransKit, which is available at
https://github.com/rohdelab/PyTransKit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization via Frequency-domain-based Feature Disentanglement and Interaction. (arXiv:2201.08029v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08029">
<div class="article-summary-box-inner">
<span><p>Adaptation to out-of-distribution data is a meta-challenge for all
statistical learning algorithms that strongly rely on the i.i.d. assumption. It
leads to unavoidable labor costs and confidence crises in realistic
applications. For that, domain generalization aims at mining domain-irrelevant
knowledge from multiple source domains that can generalize to unseen target
domains. In this paper, by leveraging the frequency domain of an image, we
uniquely work with two key observations: (i) the high-frequency information of
an image depicts object edge structure, which preserves high-level semantic
information of the object is naturally consistent across different domains, and
(ii) the low-frequency component retains object smooth structure, while this
information is susceptible to domain shifts. Motivated by the above
observations, we introduce (i) an encoder-decoder structure to disentangle
high- and low-frequency feature of an image, (ii) an information interaction
mechanism to ensure the helpful knowledge from both two parts can cooperate
effectively, and (iii) a novel data augmentation technique that works on the
frequency domain to encourage the robustness of frequency-wise feature
disentangling. The proposed method obtains state-of-the-art performance on
three widely used domain generalization benchmarks (Digit-DG, Office-Home, and
PACS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism. (arXiv:2202.00232v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00232">
<div class="article-summary-box-inner">
<span><p>This work introduces an attention mechanism for image classifiers and the
corresponding deep neural network (DNN) architecture, dubbed ISNet. During
training, the ISNet uses segmentation targets to learn how to find the image's
region of interest and concentrate its attention on it. The proposal is based
on a novel concept, background relevance minimization in explanation heatmaps.
It can be applied to virtually any classification neural network architecture,
without any extra computational cost at run-time. Capable of ignoring the
background, the resulting single DNN can substitute the common pipeline of a
segmenter followed by a classifier, being faster and lighter. We tested the
ISNet with three applications: COVID-19 and tuberculosis detection in chest
X-rays, and facial attribute estimation. The first two tasks employed mixed
training databases, and fostered shortcut learning. By focusing on lungs and
ignoring sources of bias in the background, the ISNet reduced the problem.
Thus, it improved generalization to external (out-of-distribution) test
datasets in the biomedical classification problems, surpassing a standard
classifier, a multi-task DNN (performing classification and segmentation), an
attention-gated neural network, and the standard segmentation-classification
pipeline. Facial attribute estimation demonstrated that ISNet could precisely
focus on faces, being also applicable to natural images. ISNet presents an
accurate, fast, and light methodology to ignore backgrounds and improve
generalization in diverse domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactron: Embodied Adaptive Object Detection. (arXiv:2202.00660v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00660">
<div class="article-summary-box-inner">
<span><p>Over the years various methods have been proposed for the problem of object
detection. Recently, we have witnessed great strides in this domain owing to
the emergence of powerful deep neural networks. However, there are typically
two main assumptions common among these approaches. First, the model is trained
on a fixed training set and is evaluated on a pre-recorded test set. Second,
the model is kept frozen after the training phase, so no further updates are
performed after the training is finished. These two assumptions limit the
applicability of these methods to real-world settings. In this paper, we
propose Interactron, a method for adaptive object detection in an interactive
setting, where the goal is to perform object detection in images observed by an
embodied agent navigating in different environments. Our idea is to continue
training during inference and adapt the model at test time without any explicit
supervision via interacting with the environment. Our adaptive object detection
model provides a 7.2 point improvement in AP (and 12.7 points in AP50) over
DETR, a recent, high-performance object detector. Moreover, we show that our
object detection model adapts to environments with completely different
appearance characteristics, and performs well in them. The code is available
at: https://github.com/allenai/interactron .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Audio-Visual Separation of Dynamic Sound Sources. (arXiv:2202.00850v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00850">
<div class="article-summary-box-inner">
<span><p>We explore active audio-visual separation for dynamic sound sources, where an
embodied agent moves intelligently in a 3D environment to continuously isolate
the time-varying audio stream being emitted by an object of interest. The agent
hears a mixed stream of multiple audio sources (e.g., multiple people
conversing and a band playing music at a noisy party). Given a limited time
budget, it needs to extract the target sound accurately at every step using
egocentric audio-visual observations. We propose a reinforcement learning agent
equipped with a novel transformer memory that learns motion policies to control
its camera and microphone to recover the dynamic target audio, using
self-attention to make high-quality estimates for current timesteps and also
simultaneously improve its past estimates. Using highly realistic acoustic
SoundSpaces simulations in real-world scanned Matterport3D environments, we
show that our model is able to learn efficient behavior to carry out continuous
separation of a dynamic audio target. Project:
https://vision.cs.utexas.edu/projects/active-av-dynamic-separation/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility. (arXiv:2202.02312v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02312">
<div class="article-summary-box-inner">
<span><p>Vision-language navigation (VLN), in which an agent follows language
instruction in a visual environment, has been studied under the premise that
the input command is fully feasible in the environment. Yet in practice, a
request may not be possible due to language ambiguity or environment changes.
To study VLN with unknown command feasibility, we introduce a new dataset
Mobile app Tasks with Iterative Feedback (MoTIF), where the goal is to complete
a natural language command in a mobile app. Mobile apps provide a scalable
domain to study real downstream uses of VLN methods. Moreover, mobile app
commands provide instruction for interactive navigation, as they result in
action sequences with state changes via clicking, typing, or swiping. MoTIF is
the first to include feasibility annotations, containing both binary
feasibility labels and fine-grained labels for why tasks are unsatisfiable. We
further collect follow-up questions for ambiguous queries to enable research on
task uncertainty resolution. Equipped with our dataset, we propose the new
problem of feasibility prediction, in which a natural language instruction and
multimodal app environment are used to predict command feasibility. MoTIF
provides a more realistic app dataset as it contains many diverse environments,
high-level goals, and longer action sequences than prior work. We evaluate
interactive VLN methods using MoTIF, quantify the generalization ability of
current approaches to new app environments, and measure the effect of task
feasibility on navigation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. (arXiv:2202.04800v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04800">
<div class="article-summary-box-inner">
<span><p>Humans have remarkable capacity to reason abductively and hypothesize about
what lies beyond the literal content of an image. By identifying concrete
visual clues scattered throughout a scene, we almost can't help but draw
probable inferences beyond the literal scene based on our everyday experience
and knowledge about the world. For example, if we see a "20 mph" sign alongside
a road, we might assume the street sits in a residential area (rather than on a
highway), even if no houses are pictured. Can machines perform similar visual
reasoning?
</p>
<p>We present Sherlock, an annotated corpus of 103K images for testing machine
capacity for abductive reasoning beyond literal image contents. We adopt a
free-viewing paradigm: participants first observe and identify salient clues
within images (e.g., objects, actions) and then provide a plausible inference
about the scene, given the clue. In total, we collect 363K (clue, inference)
pairs, which form a first-of-its-kind abductive visual reasoning dataset. Using
our corpus, we test three complementary axes of abductive reasoning. We
evaluate the capacity of models to: i) retrieve relevant inferences from a
large candidate corpus; ii) localize evidence for inferences via bounding
boxes, and iii) compare plausible inferences to match human judgments on a
newly-collected diagnostic corpus of 19K Likert-scale judgments. While we find
that fine-tuning CLIP-RN50x64 with a multitask objective outperforms strong
baselines, significant headroom exists between model performance and human
agreement. Data, models, and leaderboard available at
<a href="http://visualabduction.com/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robotic Telekinesis: Learning a Robotic Hand Imitator by Watching Humans on Youtube. (arXiv:2202.10448v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10448">
<div class="article-summary-box-inner">
<span><p>We build a system that enables any human to control a robot hand and arm,
simply by demonstrating motions with their own hand. The robot observes the
human operator via a single RGB camera and imitates their actions in real-time.
Human hands and robot hands differ in shape, size, and joint structure, and
performing this translation from a single uncalibrated camera is a highly
underconstrained problem. Moreover, the retargeted trajectories must
effectively execute tasks on a physical robot, which requires them to be
temporally smooth and free of self-collisions. Our key insight is that while
paired human-robot correspondence data is expensive to collect, the internet
contains a massive corpus of rich and diverse human hand videos. We leverage
this data to train a system that understands human hands and retargets a human
video stream into a robot hand-arm trajectory that is smooth, swift, safe, and
semantically similar to the guiding demonstration. We demonstrate that it
enables previously untrained people to teleoperate a robot on various dexterous
manipulation tasks. Our low-cost, glove-free, marker-free remote teleoperation
system makes robot teaching more accessible and we hope that it can aid robots
in learning to act autonomously in the real world. Videos at
https://robotic-telekinesis.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Learning Contrastive Representations for Learning with Noisy Labels. (arXiv:2203.01785v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01785">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are able to memorize noisy labels easily with a softmax
cross-entropy (CE) loss. Previous studies attempted to address this issue focus
on incorporating a noise-robust loss function to the CE loss. However, the
memorization issue is alleviated but still remains due to the non-robust CE
loss. To address this issue, we focus on learning robust contrastive
representations of data on which the classifier is hard to memorize the label
noise under the CE loss. We propose a novel contrastive regularization function
to learn such representations over noisy data where label noise does not
dominate the representation learning. By theoretically investigating the
representations induced by the proposed regularization function, we reveal that
the learned representations keep information related to true labels and discard
information related to corrupted labels. Moreover, our theoretical results also
indicate that the learned representations are robust to the label noise. The
effectiveness of this method is demonstrated with experiments on benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation. (arXiv:2203.04568v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04568">
<div class="article-summary-box-inner">
<span><p>The success of Transformer in computer vision has attracted increasing
attention in the medical imaging community. Especially for medical image
segmentation, many excellent hybrid architectures based on convolutional neural
networks (CNNs) and Transformer have been presented and achieve impressive
performance. However, most of these methods, which embed modular Transformer
into CNNs, struggle to reach their full potential. In this paper, we propose a
novel hybrid architecture for medical image segmentation called PHTrans, which
parallelly hybridizes Transformer and CNN in main building blocks to produce
hierarchical representations from global and local features and adaptively
aggregate them, aiming to fully exploit their strengths to obtain better
segmentation performance. Specifically, PHTrans follows the U-shaped
encoder-decoder design and introduces the parallel hybird module in deep
stages, where convolution blocks and the modified 3D Swin Transformer learn
local features and global dependencies separately, then a sequence-to-volume
operation unifies the dimensions of the outputs to achieve feature aggregation.
Extensive experimental results on both Multi-Atlas Labeling Beyond the Cranial
Vault and Automated Cardiac Diagnosis Challeng datasets corroborate its
effectiveness, consistently outperforming state-of-the-art methods. The code is
available at: https://github.com/lseventeen/PHTrans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PD-Flow: A Point Cloud Denoising Framework with Normalizing Flows. (arXiv:2203.05940v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05940">
<div class="article-summary-box-inner">
<span><p>Point cloud denoising aims to restore clean point clouds from raw
observations corrupted by noise and outliers while preserving the fine-grained
details. We present a novel deep learning-based denoising model, that
incorporates normalizing flows and noise disentanglement techniques to achieve
high denoising accuracy. Unlike existing works that extract features of point
clouds for point-wise correction, we formulate the denoising process from the
perspective of distribution learning and feature disentanglement. By
considering noisy point clouds as a joint distribution of clean points and
noise, the denoised results can be derived from disentangling the noise
counterpart from latent point representation, and the mapping between Euclidean
and latent spaces is modeled by normalizing flows. We evaluate our method on
synthesized 3D models and real-world datasets with various noise settings.
Qualitative and quantitative results show that our method outperforms previous
state-of-the-art deep learning-based approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Worst Case Matters for Few-Shot Recognition. (arXiv:2203.06574v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06574">
<div class="article-summary-box-inner">
<span><p>Few-shot recognition learns a recognition model with very few (e.g., 1 or 5)
images per category, and current few-shot learning methods focus on improving
the average accuracy over many episodes. We argue that in real-world
applications we may often only try one episode instead of many, and hence
maximizing the worst-case accuracy is more important than maximizing the
average accuracy. We empirically show that a high average accuracy not
necessarily means a high worst-case accuracy. Since this objective is not
accessible, we propose to reduce the standard deviation and increase the
average accuracy simultaneously. In turn, we devise two strategies from the
bias-variance tradeoff perspective to implicitly reach this goal: a simple yet
effective stability regularization (SR) loss together with model ensemble to
reduce variance during fine-tuning, and an adaptability calibration mechanism
to reduce the bias. Extensive experiments on benchmark datasets demonstrate the
effectiveness of the proposed strategies, which outperforms current
state-of-the-art methods with a significant margin in terms of not only
average, but also worst-case accuracy. Our code is available at
https://github.com/heekhero/ACSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SocialVAE: Human Trajectory Prediction using Timewise Latents. (arXiv:2203.08207v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08207">
<div class="article-summary-box-inner">
<span><p>Predicting pedestrian movement is critical for human behavior analysis and
also for safe and efficient human-agent interactions. However, despite
significant advancements, it is still challenging for existing approaches to
capture the uncertainty and multimodality of human navigation decision making.
In this paper, we propose SocialVAE, a novel approach for human trajectory
prediction. The core of SocialVAE is a timewise variational autoencoder
architecture that exploits stochastic recurrent neural networks to perform
prediction, combined with a social attention mechanism and a backward posterior
approximation to allow for better extraction of pedestrian navigation
strategies. We show that SocialVAE improves current state-of-the-art
performance on several pedestrian trajectory prediction benchmarks, including
the ETH/UCY benchmark, Stanford Drone Dataset, and SportVU NBA movement
dataset. Code is available at: https://github.com/xupei0610/SocialVAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Historical Document Image Datasets. (arXiv:2203.08504v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08504">
<div class="article-summary-box-inner">
<span><p>This paper presents a systematic literature review of image datasets for
document image analysis, focusing on historical documents, such as handwritten
manuscripts and early prints. Finding appropriate datasets for historical
document analysis is a crucial prerequisite to facilitate research using
different machine learning algorithms. However, because of the very large
variety of the actual data (e.g., scripts, tasks, dates, support systems, and
amount of deterioration), the different formats for data and label
representation, and the different evaluation processes and benchmarks, finding
appropriate datasets is a difficult task. This work fills this gap, presenting
a meta-study on existing datasets. After a systematic selection process
(according to PRISMA guidelines), we select 56 studies that are chosen based on
different factors, such as the year of publication, number of methods
implemented in the article, reliability of the chosen algorithms, dataset size,
and journal outlet. We summarize each study by assigning it to one of three
pre-defined tasks: document classification, layout structure, or semantic
analysis. We present the statistics, document type, language, tasks, input
visual aspects, and ground truth information for every dataset. In addition, we
provide the benchmark tasks and results from these papers or recent
competitions. We further discuss gaps and challenges in this domain. We
advocate for providing conversion tools to common formats (e.g., COCO format
for computer vision tasks) and always providing a set of evaluation metrics,
instead of just one, to make results comparable across studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PressureVision: Estimating Hand Pressure from a Single RGB Image. (arXiv:2203.10385v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10385">
<div class="article-summary-box-inner">
<span><p>People often interact with their surroundings by applying pressure with their
hands. While hand pressure can be measured by placing pressure sensors between
the hand and the environment, doing so can alter contact mechanics, interfere
with human tactile perception, require costly sensors, and scale poorly to
large environments. We explore the possibility of using a conventional RGB
camera to infer hand pressure, enabling machine perception of hand pressure
from uninstrumented hands and surfaces. The central insight is that the
application of pressure by a hand results in informative appearance changes.
Hands share biomechanical properties that result in similar observable
phenomena, such as soft-tissue deformation, blood distribution, hand pose, and
cast shadows. We collected videos of 36 participants with diverse skin tone
applying pressure to an instrumented planar surface. We then trained a deep
model (PressureVisionNet) to infer a pressure image from a single RGB image.
Our model infers pressure for participants outside of the training data and
outperforms baselines. We also show that the output of our model depends on the
appearance of the hand and cast shadows near contact regions. Overall, our
results suggest the appearance of a previously unobserved human hand can be
used to accurately infer applied pressure. Data, code, and models are available
online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer. (arXiv:2203.10638v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10638">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the application of Vehicle-to-Everything (V2X)
communication to improve the perception performance of autonomous vehicles. We
present a robust cooperative perception framework with V2X communication using
a novel vision Transformer. Specifically, we build a holistic attention model,
namely V2X-ViT, to effectively fuse information across on-road agents (i.e.,
vehicles and infrastructure). V2X-ViT consists of alternating layers of
heterogeneous multi-agent self-attention and multi-scale window self-attention,
which captures inter-agent interaction and per-agent spatial relationships.
These key modules are designed in a unified Transformer architecture to handle
common V2X challenges, including asynchronous information sharing, pose errors,
and heterogeneity of V2X components. To validate our approach, we create a
large-scale V2X perception dataset using CARLA and OpenCDA. Extensive
experimental results demonstrate that V2X-ViT sets new state-of-the-art
performance for 3D object detection and achieves robust performance even under
harsh, noisy environments. The code is available at
https://github.com/DerrickXuNu/v2x-vit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HM: Hybrid Masking for Few-Shot Segmentation. (arXiv:2203.12826v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12826">
<div class="article-summary-box-inner">
<span><p>We study few-shot semantic segmentation that aims to segment a target object
from a query image when provided with a few annotated support images of the
target class. Several recent methods resort to a feature masking (FM) technique
to discard irrelevant feature activations which eventually facilitates the
reliable prediction of segmentation mask. A fundamental limitation of FM is the
inability to preserve the fine-grained spatial details that affect the accuracy
of segmentation mask, especially for small target objects. In this paper, we
develop a simple, effective, and efficient approach to enhance feature masking
(FM). We dub the enhanced FM as hybrid masking (HM). Specifically, we
compensate for the loss of fine-grained spatial details in FM technique by
investigating and leveraging a complementary basic input masking method.
Experiments have been conducted on three publicly available benchmarks with
strong few-shot segmentation (FSS) baselines. We empirically show improved
performance against the current state-of-the-art methods by visible margins
across different benchmarks. Our code and trained models are available at:
https://github.com/moonsh/HM-Hybrid-Masking
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Visual Navigation Perspective for Category-Level Object Pose Estimation. (arXiv:2203.13572v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13572">
<div class="article-summary-box-inner">
<span><p>This paper studies category-level object pose estimation based on a single
monocular image. Recent advances in pose-aware generative models have paved the
way for addressing this challenging task using analysis-by-synthesis. The idea
is to sequentially update a set of latent variables, e.g., pose, shape, and
appearance, of the generative model until the generated image best agrees with
the observation. However, convergence and efficiency are two challenges of this
inference procedure. In this paper, we take a deeper look at the inference of
analysis-by-synthesis from the perspective of visual navigation, and
investigate what is a good navigation policy for this specific task. We
evaluate three different strategies, including gradient descent, reinforcement
learning and imitation learning, via thorough comparisons in terms of
convergence, robustness and efficiency. Moreover, we show that a simple hybrid
approach leads to an effective and efficient solution. We further compare these
strategies to state-of-the-art methods, and demonstrate superior performance on
synthetic and real-world datasets leveraging off-the-shelf pose-aware
generative models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Active Speaker Detection. (arXiv:2203.14250v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14250">
<div class="article-summary-box-inner">
<span><p>Recent advances in the Active Speaker Detection (ASD) problem build upon a
two-stage process: feature extraction and spatio-temporal context aggregation.
In this paper, we propose an end-to-end ASD workflow where feature learning and
contextual predictions are jointly learned. Our end-to-end trainable network
simultaneously learns multi-modal embeddings and aggregates spatio-temporal
context. This results in more suitable feature representations and improved
performance in the ASD task. We also introduce interleaved graph neural network
(iGNN) blocks, which split the message passing according to the main sources of
context in the ASD problem. Experiments show that the aggregated features from
the iGNN blocks are more suitable for ASD, resulting in state-of-the art
performance. Finally, we design a weakly-supervised strategy, which
demonstrates that the ASD problem can also be approached by utilizing
audiovisual data but relying exclusively on audio annotations. We achieve this
by modelling the direct relationship between the audio signal and the possible
sound sources (speakers), as well as introducing a contrastive loss. All the
resources of this project will be made available at:
https://github.com/fuankarion/end-to-end-asd.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Human-Object Interaction Concepts via Self-Compositional Learning. (arXiv:2203.14272v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14272">
<div class="article-summary-box-inner">
<span><p>A comprehensive understanding of human-object interaction (HOI) requires
detecting not only a small portion of predefined HOI concepts (or categories)
but also other reasonable HOI concepts, while current approaches usually fail
to explore a huge portion of unknown HOI concepts (i.e., unknown but reasonable
combinations of verbs and objects). In this paper, 1) we introduce a novel and
challenging task for a comprehensive HOI understanding, which is termed as HOI
Concept Discovery; and 2) we devise a self-compositional learning framework (or
SCL) for HOI concept discovery. Specifically, we maintain an online updated
concept confidence matrix during training: 1) we assign pseudo-labels for all
composite HOI instances according to the concept confidence matrix for
self-training; and 2) we update the concept confidence matrix using the
predictions of all composite HOI instances. Therefore, the proposed method
enables the learning on both known and unknown HOI concepts. We perform
extensive experiments on several popular HOI datasets to demonstrate the
effectiveness of the proposed method for HOI concept discovery, object
affordance recognition and HOI detection. For example, the proposed
self-compositional learning framework significantly improves the performance of
1) HOI concept discovery by over 10% on HICO-DET and over 3% on V-COCO,
respectively; 2) object affordance recognition by over 9% mAP on MS-COCO and
HICO-DET; and 3) rare-first and non-rare-first unknown HOI detection relatively
over 30% and 20%, respectively. Code is publicly available at
https://github.com/zhihou7/HOI-CL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth. (arXiv:2203.15174v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15174">
<div class="article-summary-box-inner">
<span><p>Conventional self-supervised monocular depth prediction methods are based on
a static environment assumption, which leads to accuracy degradation in dynamic
scenes due to the mismatch and occlusion problems introduced by object motions.
Existing dynamic-object-focused methods only partially solved the mismatch
problem at the training loss level. In this paper, we accordingly propose a
novel multi-frame monocular depth prediction method to solve these problems at
both the prediction and supervision loss levels. Our method, called
DynamicDepth, is a new framework trained via a self-supervised cycle consistent
learning scheme. A Dynamic Object Motion Disentanglement (DOMD) module is
proposed to disentangle object motions to solve the mismatch problem. Moreover,
novel occlusion-aware Cost Volume and Re-projection Loss are designed to
alleviate the occlusion effects of object motions. Extensive analyses and
experiments on the Cityscapes and KITTI datasets show that our method
significantly outperforms the state-of-the-art monocular depth prediction
methods, especially in the areas of dynamic objects. Code is available at
https://github.com/AutoAILab/DynamicDepth
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acknowledging the Unknown for Multi-label Learning with Single Positive Labels. (arXiv:2203.16219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16219">
<div class="article-summary-box-inner">
<span><p>Due to the difficulty of collecting exhaustive multi-label annotations,
multi-label datasets often contain partial labels. We consider an extreme of
this weakly supervised learning problem, called single positive multi-label
learning (SPML), where each multi-label training image has only one positive
label. Traditionally, all unannotated labels are assumed as negative labels in
SPML, which introduces false negative labels and causes model training to be
dominated by assumed negative labels. In this work, we choose to treat all
unannotated labels from an alternative perspective, i.e. acknowledging they are
unknown. Hence, we propose entropy-maximization (EM) loss to attain a special
gradient regime for providing proper supervision signals. Moreover, we propose
asymmetric pseudo-labeling (APL), which adopts asymmetric-tolerance strategies
and a self-paced procedure, to cooperate with EM loss and then provide more
precise supervision. Experiments show that our method significantly improves
performance and achieves state-of-the-art results on all four benchmarks. Code
is available at https://github.com/Correr-Zhou/SPML-AckTheUnknown.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeqTR: A Simple yet Universal Network for Visual Grounding. (arXiv:2203.16265v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16265">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a simple yet universal network termed SeqTR for
visual grounding tasks, e.g., phrase localization, referring expression
comprehension (REC) and segmentation (RES). The canonical paradigms for visual
grounding often require substantial expertise in designing network
architectures and loss functions, making them hard to generalize across tasks.
To simplify and unify the modeling, we cast visual grounding as a point
prediction problem conditioned on image and text inputs, where either the
bounding box or binary mask is represented as a sequence of discrete coordinate
tokens. Under this paradigm, visual grounding tasks are unified in our SeqTR
network without task-specific branches or heads, e.g., the convolutional mask
decoder for RES, which greatly reduces the complexity of multi-task modeling.
In addition, SeqTR also shares the same optimization objective for all tasks
with a simple cross-entropy loss, further reducing the complexity of deploying
hand-crafted loss functions. Experiments on five benchmark datasets demonstrate
that the proposed SeqTR outperforms (or is on par with) the existing
state-of-the-arts, proving that a simple yet universal approach for visual
grounding is indeed feasible. Source code is available at
https://github.com/sean-zhuh/SeqTR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17261">
<div class="article-summary-box-inner">
<span><p>Recent research explosion on Neural Radiance Field (NeRF) shows the
encouraging potential to represent complex scenes with neural networks. One
major drawback of NeRF is its prohibitive inference time: Rendering a single
pixel requires querying the NeRF network hundreds of times. To resolve it,
existing efforts mainly attempt to reduce the number of required sampled
points. However, the problem of iterative sampling still exists. On the other
hand, Neural Light Field (NeLF) presents a more straightforward representation
over NeRF in novel view synthesis -- the rendering of a pixel amounts to one
single forward pass without ray-marching. In this work, we present a deep
residual MLP network (88 layers) to effectively learn the light field. We show
the key to successfully learning such a deep NeLF network is to have sufficient
data, for which we transfer the knowledge from a pre-trained NeRF model via
data distillation. Extensive experiments on both synthetic and real-world
scenes show the merits of our method over other counterpart algorithms. On the
synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x
runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average
PSNR improvement) rendering quality than NeRF without any customized
parallelism requirement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Digitizing Historical Balance Sheet Data: A Practitioner's Guide. (arXiv:2204.00052v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00052">
<div class="article-summary-box-inner">
<span><p>This paper discusses how to successfully digitize large-scale historical
micro-data by augmenting optical character recognition (OCR) engines with pre-
and post-processing methods. Although OCR software has improved dramatically in
recent years due to improvements in machine learning, off-the-shelf OCR
applications still present high error rates which limit their applications for
accurate extraction of structured information. Complementing OCR with
additional methods can however dramatically increase its success rate, making
it a powerful and cost-efficient tool for economic historians. This paper
showcases these methods and explains why they are useful. We apply them against
two large balance sheet datasets and introduce quipucamayoc, a Python package
containing these methods in a unified framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00833">
<div class="article-summary-box-inner">
<span><p>Pixel synthesis is a promising research paradigm for image generation, which
can well exploit pixel-wise prior knowledge for generation. However, existing
methods still suffer from excessive memory footprint and computation overhead.
In this paper, we propose a progressive pixel synthesis network towards
efficient image generation, coined as PixelFolder. Specifically, PixelFolder
formulates image generation as a progressive pixel regression problem and
synthesizes images via a multi-stage structure, which can greatly reduce the
overhead caused by large tensor transformations. In addition, we introduce
novel pixel folding operations to further improve model efficiency while
maintaining pixel-wise prior knowledge for end-to-end regression. With these
innovative designs, we greatly reduce the expenditure of pixel synthesis, e.g.,
reducing 89% computation and 53% parameters compared with the latest pixel
synthesis method CIPS. To validate our approach, we conduct extensive
experiments on two benchmark datasets, namely FFHQ and LSUN Church. The
experimental results show that with much less expenditure, PixelFolder obtains
new state-of-the-art (SOTA) performance on two benchmark datasets, i.e., 3.77
FID and 2.45 FID on FFHQ and LSUN Church, respectively.Meanwhile, PixelFolder
is also more efficient than the SOTA methods like StyleGAN2, reducing about 72%
computation and 31% parameters, respectively. These results greatly validate
the effectiveness of the proposed PixelFolder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Vision Transformers by Revisiting High-frequency Components. (arXiv:2204.00993v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00993">
<div class="article-summary-box-inner">
<span><p>The transformer models have shown promising effectiveness in dealing with
various vision tasks. However, compared with training Convolutional Neural
Network (CNN) models, training Vision Transformer (ViT) models is more
difficult and relies on the large-scale training set. To explain this
observation we make a hypothesis that \textit{ViT models are less effective in
capturing the high-frequency components of images than CNN models}, and verify
it by a frequency analysis. Inspired by this finding, we first investigate the
effects of existing techniques for improving ViT models from a new frequency
perspective, and find that the success of some techniques (e.g., RandAugment)
can be attributed to the better usage of the high-frequency components. Then,
to compensate for this insufficient ability of ViT models, we propose HAT,
which directly augments high-frequency components of images via adversarial
training. We show that HAT can consistently boost the performance of various
ViT models (e.g., +1.2% for ViT-B, +0.5% for Swin-B), and especially enhance
the advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the
superiority can also be maintained on out-of-distribution data and transferred
to downstream tasks. The code is available at:
https://github.com/jiawangbai/HAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaxViT: Multi-Axis Vision Transformer. (arXiv:2204.01697v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01697">
<div class="article-summary-box-inner">
<span><p>Transformers have recently gained significant attention in the computer
vision community. However, the lack of scalability of self-attention mechanisms
with respect to image size has limited their wide adoption in state-of-the-art
vision backbones. In this paper we introduce an efficient and scalable
attention model we call multi-axis attention, which consists of two aspects:
blocked local and dilated global attention. These design choices allow
global-local spatial interactions on arbitrary input resolutions with only
linear complexity. We also present a new architectural element by effectively
blending our proposed attention model with convolutions, and accordingly
propose a simple hierarchical vision backbone, dubbed MaxViT, by simply
repeating the basic building block over multiple stages. Notably, MaxViT is
able to ''see'' globally throughout the entire network, even in earlier,
high-resolution stages. We demonstrate the effectiveness of our model on a
broad spectrum of vision tasks. On image classification, MaxViT achieves
state-of-the-art performance under various settings: without extra data, MaxViT
attains 86.5% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our
model achieves 88.7% top-1 accuracy. For downstream tasks, MaxViT as a backbone
delivers favorable performance on object detection as well as visual aesthetic
assessment. We also show that our proposed model expresses strong generative
modeling capability on ImageNet, demonstrating the superior potential of MaxViT
blocks as a universal vision module. The source code and trained models will be
available at https://github.com/google-research/maxvit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer. (arXiv:2204.03638v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03638">
<div class="article-summary-box-inner">
<span><p>Videos are created to express emotion, exchange information, and share
experiences. Video synthesis has intrigued researchers for a long time. Despite
the rapid progress driven by advances in visual synthesis, most existing
studies focus on improving the frames' quality and the transitions between
them, while little progress has been made in generating longer videos. In this
paper, we present a method that builds on 3D-VQGAN and transformers to generate
videos with thousands of frames. Our evaluation shows that our model trained on
16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse,
and Taichi-HD datasets can generate diverse, coherent, and high-quality long
videos. We also showcase conditional extensions of our approach for generating
meaningful long videos by incorporating temporal information with text and
audio. Videos and code can be found at
https://songweige.github.io/projects/tats/index.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories. (arXiv:2204.04153v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04153">
<div class="article-summary-box-inner">
<span><p>Tracking pixels in videos is typically studied as an optical flow estimation
problem, where every pixel is described with a displacement vector that locates
it in the next frame. Even though wider temporal context is freely available,
prior efforts to take this into account have yielded only small gains over
2-frame methods. In this paper, we revisit Sand and Teller's "particle video"
approach, and study pixel tracking as a long-range motion estimation problem,
where every pixel is described with a trajectory that locates it in multiple
future frames. We re-build this classic approach using components that drive
the current state-of-the-art in flow and object tracking, such as dense cost
maps, iterative optimization, and learned appearance updates. We train our
models using long-range amodal point trajectories mined from existing optical
flow data that we synthetically augment with multi-frame occlusions. We test
our approach in trajectory estimation benchmarks and in keypoint label
propagation tasks, and compare favorably against state-of-the-art optical flow
and feature tracking methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring. (arXiv:2204.07820v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07820">
<div class="article-summary-box-inner">
<span><p>Blind image deblurring (BID) remains a challenging and significant task.
Benefiting from the strong fitting ability of deep learning, paired data-driven
supervised BID method has obtained great progress. However, paired data are
usually synthesized by hand, and the realistic blurs are more complex than
synthetic ones, which makes the supervised methods inept at modeling realistic
blurs and hinders their real-world applications. As such, unsupervised deep BID
method without paired data offers certain advantages, but current methods still
suffer from some drawbacks, e.g., bulky model size, long inference time, and
strict image resolution and domain requirements. In this paper, we propose a
lightweight and real-time unsupervised BID baseline, termed Frequency-domain
Contrastive Loss Constrained Lightweight CycleGAN (shortly, FCL-GAN), with
attractive properties, i.e., no image domain limitation, no image resolution
limitation, 25x lighter than SOTA, and 5x faster than SOTA. To guarantee the
lightweight property and performance superiority, two new collaboration units
called lightweight domain conversion unit(LDCU) and parameter-free
frequency-domain contrastive unit(PFCU) are designed. LDCU mainly implements
inter-domain conversion in lightweight manner. PFCU further explores the
similarity measure, external difference and internal connection between the
blurred domain and sharp domain images in frequency domain, without involving
extra parameters. Extensive experiments on several image datasets demonstrate
the effectiveness of our FCL-GAN in terms of performance, model size and
reference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency. (arXiv:2204.10310v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10310">
<div class="article-summary-box-inner">
<span><p>Approaches for single-view reconstruction typically rely on viewpoint
annotations, silhouettes, the absence of background, multiple views of the same
instance, a template shape, or symmetry. We avoid all such supervision and
assumptions by explicitly leveraging the consistency between images of
different object instances. As a result, our method can learn from large
collections of unlabelled images depicting the same object category. Our main
contributions are two ways for leveraging cross-instance consistency: (i)
progressive conditioning, a training strategy to gradually specialize the model
from category to instances in a curriculum learning fashion; and (ii) neighbor
reconstruction, a loss enforcing consistency between instances having similar
shape or texture. Also critical to the success of our method are: our
structured autoencoding architecture decomposing an image into explicit shape,
texture, pose, and background; an adapted formulation of differential
rendering; and a new optimization scheme alternating between 3D and pose
learning. We compare our approach, UNICORN, both on the diverse synthetic
ShapeNet dataset - the classical benchmark for methods requiring multiple views
as supervision - and on standard real-image benchmarks (Pascal3D+ Car, CUB) for
which most methods require known templates and silhouette annotations. We also
showcase applicability to more challenging real-world collections (CompCars,
LSUN), where silhouettes are not available and images are not cropped around
the object.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where in the World is this Image? Transformer-based Geo-localization in the Wild. (arXiv:2204.13861v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13861">
<div class="article-summary-box-inner">
<span><p>Predicting the geographic location (geo-localization) from a single
ground-level RGB image taken anywhere in the world is a very challenging
problem. The challenges include huge diversity of images due to different
environmental scenarios, drastic variation in the appearance of the same
location depending on the time of the day, weather, season, and more
importantly, the prediction is made from a single image possibly having only a
few geo-locating cues. For these reasons, most existing works are restricted to
specific cities, imagery, or worldwide landmarks. In this work, we focus on
developing an efficient solution to planet-scale single-image geo-localization.
To this end, we propose TransLocator, a unified dual-branch transformer network
that attends to tiny details over the entire image and produces robust feature
representation under extreme appearance variations. TransLocator takes an RGB
image and its semantic segmentation map as inputs, interacts between its two
parallel branches after each transformer layer, and simultaneously performs
geo-localization and scene recognition in a multi-task fashion. We evaluate
TransLocator on four benchmark datasets - Im2GPS, Im2GPS3k, YFCC4k, YFCC26k and
obtain 5.5%, 14.1%, 4.9%, 9.9% continent-level accuracy improvement over the
state-of-the-art. TransLocator is also validated on real-world test images and
found to be more effective than previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking. (arXiv:2205.02301v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02301">
<div class="article-summary-box-inner">
<span><p>Estimating human motion from video is an active research area due to its many
potential applications. Most state-of-the-art methods predict human shape and
posture estimates for individual images and do not leverage the temporal
information available in video. Many "in the wild" sequences of human motion
are captured by a moving camera, which adds the complication of conflated
camera and human motion to the estimation. We therefore present BodySLAM, a
monocular SLAM system that jointly estimates the position, shape, and posture
of human bodies, as well as the camera trajectory. We also introduce a novel
human motion model to constrain sequential body postures and observe the scale
of the scene. Through a series of experiments on video sequences of human
motion captured by a moving monocular camera, we demonstrate that BodySLAM
improves estimates of all human body parameters and camera poses when compared
to estimating these separately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-CLOP: CLIP-Guided Collage and Photomontage. (arXiv:2205.03146v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03146">
<div class="article-summary-box-inner">
<span><p>The unabated mystique of large-scale neural networks, such as the CLIP dual
image-and-text encoder, popularized automatically generated art. Increasingly
more sophisticated generators enhanced the artworks' realism and visual
appearance, and creative prompt engineering enabled stylistic expression.
Guided by an artist-in-the-loop ideal, we design a gradient-based generator to
produce collages. It requires the human artist to curate libraries of image
patches and to describe (with prompts) the whole image composition, with the
option to manually adjust the patches' positions during generation, thereby
allowing humans to reclaim some control of the process and achieve greater
creative freedom. We explore the aesthetic potentials of high-resolution
collages, and provide an open-source Google Colab as an artistic tool.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation. (arXiv:2205.03962v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03962">
<div class="article-summary-box-inner">
<span><p>Virtual facial avatars will play an increasingly important role in immersive
communication, games and the metaverse, and it is therefore critical that they
be inclusive. This requires accurate recovery of the appearance, represented by
albedo, regardless of age, sex, or ethnicity. While significant progress has
been made on estimating 3D facial geometry, albedo estimation has received less
attention. The task is fundamentally ambiguous because the observed color is a
function of albedo and lighting, both of which are unknown. We find that
current methods are biased towards light skin tones due to (1) strongly biased
priors that prefer lighter pigmentation and (2) algorithmic solutions that
disregard the light/albedo ambiguity. To address this, we propose a new
evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation
and, hence, fairness. Specifically, we create the first facial albedo
evaluation benchmark where subjects are balanced in terms of skin color, and
measure accuracy using the Individual Typology Angle (ITA) metric. We then
address the light/albedo ambiguity by building on a key observation: the image
of the full scene -- as opposed to a cropped image of the face -- contains
important information about lighting that can be used for disambiguation. TRUST
regresses facial albedo by conditioning both on the face region and a global
illumination signal obtained from the scene image. Our experimental results
show significant improvement compared to state-of-the-art methods on albedo
estimation, both in terms of accuracy and fairness. The evaluation benchmark
and code will be made available for research purposes at
https://trust.is.tue.mpg.de.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder. (arXiv:2205.06803v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06803">
<div class="article-summary-box-inner">
<span><p>Although generative facial prior and geometric prior have recently
demonstrated high-quality results for blind face restoration, producing
fine-grained facial details faithful to inputs remains a challenging problem.
Motivated by the classical dictionary-based methods and the recent vector
quantization (VQ) technique, we propose a VQ-based face restoration method -
VQFR. VQFR takes advantage of high-quality low-level feature banks extracted
from high-quality faces and can thus help recover realistic facial details.
However, the simple application of the VQ codebook cannot achieve good results
with faithful details and identity preservation. Therefore, we further
introduce two special network designs. 1). We first investigate the compression
patch size in the VQ codebook and find that the VQ codebook designed with a
proper compression patch size is crucial to balance the quality and fidelity.
2). To further fuse low-level features from inputs while not "contaminating"
the realistic details generated from the VQ codebook, we proposed a parallel
decoder consisting of a texture decoder and a main decoder. Those two decoders
then interact with a texture warping module with deformable convolution.
Equipped with the VQ codebook as a facial detail dictionary and the parallel
decoder design, the proposed VQFR can largely enhance the restored quality of
facial details while keeping the fidelity to previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Segmentation in Real-World Images via Spelke Object Inference. (arXiv:2205.08515v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08515">
<div class="article-summary-box-inner">
<span><p>Self-supervised, category-agnostic segmentation of real-world images is a
challenging open problem in computer vision. Here, we show how to learn static
grouping priors from motion self-supervision by building on the cognitive
science concept of a Spelke Object: a set of physical stuff that moves
together. We introduce the Excitatory-Inhibitory Segment Extraction Network
(EISEN), which learns to extract pairwise affinity graphs for static scenes
from motion-based training signals. EISEN then produces segments from
affinities using a novel graph propagation and competition network. During
training, objects that undergo correlated motion (such as robot arms and the
objects they move) are decoupled by a bootstrapping process: EISEN explains
away the motion of objects it has already learned to segment. We show that
EISEN achieves a substantial improvement in the state of the art for
self-supervised image segmentation on challenging synthetic and real-world
robotics datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physically-Based Editing of Indoor Scene Lighting from a Single Image. (arXiv:2205.09343v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09343">
<div class="article-summary-box-inner">
<span><p>We present a method to edit complex indoor lighting from a single image with
its predicted depth and light source segmentation masks. This is an extremely
challenging problem that requires modeling complex light transport, and
disentangling HDR lighting from material and geometry with only a partial LDR
observation of the scene. We tackle this problem using two novel components: 1)
a holistic scene reconstruction method that estimates scene reflectance and
parametric 3D lighting, and 2) a neural rendering framework that re-renders the
scene from our predictions. We use physically-based indoor light
representations that allow for intuitive editing, and infer both visible and
invisible light sources. Our neural rendering framework combines
physically-based direct illumination and shadow rendering with deep networks to
approximate global illumination. It can capture challenging lighting effects,
such as soft shadows, directional lighting, specular materials, and
interreflections. Previous single image inverse rendering methods usually
entangle scene lighting and geometry and only support applications like object
insertion. Instead, by combining parametric 3D lighting estimation with neural
scene rendering, we demonstrate the first automatic method to achieve full
scene relighting, including light source insertion, removal, and replacement,
from a single image. All source code and data will be publicly released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Gender Prediction Based on Deep Transfer Learning from Panoramic Radiograph Images. (arXiv:2205.09850v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09850">
<div class="article-summary-box-inner">
<span><p>Panoramic Dental Radiography (PDR) image processing is one of the most widely
used methods for gender determination in forensic medicine. Deep learning
models are widely used in automated analysis of radiological images today due
to their high processing speed, accuracy and stability. A few approach using
transfer learning is proposed to gender-classify PDR images. In this study,
DenseNet121 convolutional neural network (CNN) classifier, which is one of the
pre-trained Deep learning architectures, was used. The proposed DenseNet121
network has been expanded and fine-tuned with several additional layers before
the final layer to increase its ability to understand more complex patterns
from data. At the end of this stage, it has been trained with the dental
dataset containing PDR images and has become more experienced. K-fold cross
validation method is adopted to increase the accuracy of the proposed
DenseNet121 model.In this study the best performance was achieved for the 4,800
test datasets with a classification accuracy of 97.25%. The proposed model,
along with Grad-CAM based analysis also revealed that the mandible
circumference and teeth are the most significant areas to consider in gender
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient Skin Lesion Segmentation via Dilated Scale-Wise Feature Fusion Network. (arXiv:2205.10272v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10272">
<div class="article-summary-box-inner">
<span><p>Skin lesion detection in dermoscopic images is essential in the accurate and
early diagnosis of skin cancer by a computerized apparatus. Current skin lesion
segmentation approaches show poor performance in challenging circumstances such
as indistinct lesion boundaries, low contrast between the lesion and the
surrounding area, or heterogeneous background that causes over/under
segmentation of the skin lesion. To accurately recognize the lesion from the
neighboring regions, we propose a dilated scale-wise feature fusion network
based on convolution factorization. Our network is designed to simultaneously
extract features at different scales which are systematically fused for better
detection. The proposed model has satisfactory accuracy and efficiency. Various
experiments for lesion segmentation are performed along with comparisons with
the state-of-the-art models. Our proposed model consistently showcases
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10667">
<div class="article-summary-box-inner">
<span><p>Traditionally, extracting patterns from eye movement data relies on
statistics of different macro-events such as fixations and saccades. This
requires an additional preprocessing step to separate the eye movement
subtypes, often with a number of parameters on which the classification results
depend. Besides that, definitions of such macro events are formulated in
different ways by different researchers.
</p>
<p>We propose an application of a new class of features to the quantitative
analysis of personal eye movement trajectories structure. This new class of
features based on algebraic topology allows extracting patterns from different
modalities of gaze such as time series of coordinates and amplitudes, heatmaps,
and point clouds in a unified way at all scales from micro to macro. We
experimentally demonstrate the competitiveness of the new class of features
with the traditional ones and their significant synergy while being used
together for the person authentication task on the recently published eye
movement trajectories dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SiSPRNet: End-to-End Learning for Single-Shot Phase Retrieval. (arXiv:2205.11434v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11434">
<div class="article-summary-box-inner">
<span><p>With the success of deep learning methods in many image processing tasks,
deep learning approaches have also been introduced to the phase retrieval
problem recently. These approaches are different from the traditional iterative
optimization methods in that they usually require only one intensity
measurement and can reconstruct phase images in real-time. However, because of
tremendous domain discrepancy, the quality of the reconstructed images given by
these approaches still has much room to improve to meet the general application
requirements. In this paper, we design a novel deep neural network structure
named SiSPRNet for phase retrieval based on a single Fourier intensity
measurement. To effectively utilize the spectral information of the
measurements, we propose a new feature extraction unit using the Multi-Layer
Perceptron (MLP) as the front end. It allows all pixels of the input intensity
image to be considered together for exploring their global representation. The
size of the MLP is carefully designed to facilitate the extraction of the
representative features while reducing noises and outliers. A dropout layer is
also equipped to mitigate the possible overfitting problem in training the MLP.
To promote the global correlation in the reconstructed images, a self-attention
mechanism is introduced to the Up-sampling and Reconstruction (UR) blocks of
the proposed SiSPRNet. These UR blocks are inserted into a residual learning
structure to prevent the weak information flow and vanishing gradient problems
due to their complex layer structure. Extensive evaluations of the proposed
model are performed using different testing datasets of phase-only images and
images with linearly related magnitude and phase. Experiments were conducted on
an optical experimentation platform to understand the performance of different
deep learning methods when working in a practical environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust 3D Object Detection in Cold Weather Conditions. (arXiv:2205.11925v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11925">
<div class="article-summary-box-inner">
<span><p>Adverse weather conditions can negatively affect LiDAR-based object
detectors. In this work, we focus on the phenomenon of vehicle gas exhaust
condensation in cold weather conditions. This everyday effect can influence the
estimation of object sizes, orientations and introduce ghost object detections,
compromising the reliability of the state of the art object detectors. We
propose to solve this problem by using data augmentation and a novel training
loss term. To effectively train deep neural networks, a large set of labeled
data is needed. In case of adverse weather conditions, this process can be
extremely laborious and expensive. We address this issue in two steps: First,
we present a gas exhaust data generation method based on 3D surface
reconstruction and sampling which allows us to generate large sets of gas
exhaust clouds from a small pool of labeled data. Second, we introduce a point
cloud augmentation process that can be used to add gas exhaust to datasets
recorded in good weather conditions. Finally, we formulate a new training loss
term that leverages the augmented point cloud to increase object detection
robustness by penalizing predictions that include noise. In contrast to other
works, our method can be used with both grid-based and point-based detectors.
Moreover, since our approach does not require any network architecture changes,
inference times remain unchanged. Experimental results on real data show that
our proposed method greatly increases robustness to gas exhaust and noisy data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RADNet: Ensemble Model for Robust Glaucoma Classification in Color Fundus Images. (arXiv:2205.12902v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12902">
<div class="article-summary-box-inner">
<span><p>Glaucoma is one of the most severe eye diseases, characterized by rapid
progression and leading to irreversible blindness. It is often the case that
diagnostics is carried out when one's sight has already significantly degraded
due to the lack of noticeable symptoms at early stage of the disease. Regular
glaucoma screenings of the population shall improve early-stage detection,
however the desirable frequency of etymological checkups is often not feasible
due to the excessive load imposed by manual diagnostics on limited number of
specialists. Considering the basic methodology to detect glaucoma is to analyze
fundus images for the optic-disc-to-optic-cup ratio, Machine Learning
algorithms can offer sophisticated methods for image processing and
classification. In our work, we propose an advanced image pre-processing
technique combined with a multi-view network of deep classification models to
categorize glaucoma. Our Glaucoma Automated Retinal Detection Network (GARDNet)
has been successfully tested on Rotterdam EyePACS AIROGS dataset with an AUC of
0.92, and then additionally fine-tuned and tested on RIM-ONE DL dataset with an
AUC of 0.9308 outperforming the state-of-the-art of 0.9272. Our code will be
made available on GitHub upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03491">
<div class="article-summary-box-inner">
<span><p>Explaining is a human knowledge transfer process regarding a phenomenon
between an explainer and an explainee. Each word used to explain this
phenomenon must be carefully selected by the explainer in accordance with the
current explainee phenomenon-related knowledge level and the phenomenon itself
in order to have a high understanding from the explainee of the phenomenon.
Nowadays, deep models, especially graph neural networks, have a major place in
daily life even in critical applications. In such context, those models need to
have a human high interpretability also referred as being explainable, in order
to improve usage trustability of them in sensitive cases. Explaining is also a
human dependent task and methods that explain deep model behavior must include
these social-related concerns for providing profitable and quality
explanations. Current explaining methods often occlude such social aspect for
providing their explanations and only focus on the signal aspect of the
question. In this contribution we propose a reliable social-aware explaining
method suited for graph neural network that includes this social feature as a
modular concept generator and by both leveraging signal and graph domain aspect
thanks to an eigencentrality concept ordering approach. Besides our method
takes into account the human-dependent aspect underlying any explanation
process, we also reach high score regarding state-of-the-art objective metrics
assessing explanation methods for graph neural networks models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition. (arXiv:2206.04790v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04790">
<div class="article-summary-box-inner">
<span><p>We address the problem of data augmentation for video action recognition.
Standard augmentation strategies in video are hand-designed and sample the
space of possible augmented data points either at random, without knowing which
augmented points will be better, or through heuristics. We propose to learn
what makes a good video for action recognition and select only high-quality
samples for augmentation. In particular, we choose video compositing of a
foreground and a background video as the data augmentation process, which
results in diverse and realistic new samples. We learn which pairs of videos to
augment without having to actually composite them. This reduces the space of
possible augmentations, which has two advantages: it saves computational cost
and increases the accuracy of the final trained classifier, as the augmented
pairs are of higher quality than average. We present experimental results on
the entire spectrum of training settings: few-shot, semi-supervised and fully
supervised. We observe consistent improvements across all of them over prior
work and baselines on Kinetics, UCF101, HMDB51, and achieve a new
state-of-the-art on settings with limited data. We see improvements of up to
8.6% in the semi-supervised setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects. (arXiv:2206.07219v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07219">
<div class="article-summary-box-inner">
<span><p>The recent development of deep learning combined with compressed sensing
enables fast reconstruction of undersampled MR images and has achieved
state-of-the-art performance for Cartesian k-space trajectories. However,
non-Cartesian trajectories such as the radial trajectory need to be transformed
onto a Cartesian grid in each iteration of the network training, slowing down
the training process and posing inconvenience and delay during training.
Multiple iterations of nonuniform Fourier transform in the networks offset the
deep learning advantage of fast inference. Current approaches typically either
work on image-to-image networks or grid the non-Cartesian trajectories before
the network training to avoid the repeated gridding process. However, the
image-to-image networks cannot ensure the k-space data consistency in the
reconstructed images and the pre-processing of non-Cartesian k-space leads to
gridding errors which cannot be compensated by the network training. Inspired
by the Transformer network to handle long-range dependencies in sequence
transduction tasks, we propose to rearrange the radial spokes to sequential
data based on the chronological order of acquisition and use the Transformer to
predict unacquired radial spokes from acquired ones. We propose novel data
augmentation methods to generate a large amount of training data from a limited
number of subjects. The network can be generated to different anatomical
structures. Experimental results show superior performance of the proposed
framework compared to state-of-the-art deep neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Captioning based on Feature Refinement and Reflective Decoding. (arXiv:2206.07986v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07986">
<div class="article-summary-box-inner">
<span><p>Image captioning is the process of automatically generating a description of
an image in natural language. Image captioning is one of the significant
challenges in image understanding since it requires not only recognizing
salient objects in the image but also their attributes and the way they
interact. The system must then generate a syntactically and semantically
correct caption that describes the image content in natural language. With the
significant progress in deep learning models and their ability to effectively
encode large sets of images and generate correct sentences, several
neural-based captioning approaches have been proposed recently, each trying to
achieve better accuracy and caption quality. This paper introduces an
encoder-decoder-based image captioning system in which the encoder extracts
spatial features from the image using ResNet-101. This stage is followed by a
refining model, which uses an attention-on-attention mechanism to extract the
visual features of the target image objects, then determine their interactions.
The decoder consists of an attention-based recurrent module and a reflective
attention module, which collaboratively apply attention to the visual and
textual features to enhance the decoder's ability to model long-term sequential
dependencies. Extensive experiments performed on Flickr30K, show the
effectiveness of the proposed approach and the high quality of the generated
captions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Annotation Need in Self-Explanatory Models for Lung Nodule Diagnosis. (arXiv:2206.13608v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13608">
<div class="article-summary-box-inner">
<span><p>Feature-based self-explanatory methods explain their classification in terms
of human-understandable features. In the medical imaging community, this
semantic matching of clinical knowledge adds significantly to the
trustworthiness of the AI. However, the cost of additional annotation of
features remains a pressing issue. We address this problem by proposing
cRedAnno, a data-/annotation-efficient self-explanatory approach for lung
nodule diagnosis. cRedAnno considerably reduces the annotation need by
introducing self-supervised contrastive learning to alleviate the burden of
learning most parameters from annotation, replacing end-to-end training with
two-stage training. When training with hundreds of nodule samples and only 1%
of their annotations, cRedAnno achieves competitive accuracy in predicting
malignancy, meanwhile significantly surpassing most previous works in
predicting nodule attributes. Visualisation of the learned space further
indicates that the correlation between the clustering of malignancy and nodule
attributes coincides with clinical knowledge. Our complete code is open-source
available: https://github.com/diku-dk/credanno.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Timestamp-Supervised Action Segmentation with Graph Convolutional Networks. (arXiv:2206.15031v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15031">
<div class="article-summary-box-inner">
<span><p>We introduce a novel approach for temporal activity segmentation with
timestamp supervision. Our main contribution is a graph convolutional network,
which is learned in an end-to-end manner to exploit both frame features and
connections between neighboring frames to generate dense framewise labels from
sparse timestamp labels. The generated dense framewise labels can then be used
to train the segmentation model. In addition, we propose a framework for
alternating learning of both the segmentation model and the graph convolutional
model, which first initializes and then iteratively refines the learned models.
Detailed experiments on four public datasets, including 50 Salads, GTEA,
Breakfast, and Desktop Assembly, show that our method is superior to the
multi-layer perceptron baseline, while performing on par with or better than
the state of the art in temporal activity segmentation with timestamp
supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-term Leap Attention, Short-term Periodic Shift for Video Classification. (arXiv:2207.05526v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05526">
<div class="article-summary-box-inner">
<span><p>Video transformer naturally incurs a heavier computation burden than a static
vision transformer, as the former processes $T$ times longer sequence than the
latter under the current attention of quadratic complexity $(T^2N^2)$. The
existing works treat the temporal axis as a simple extension of spatial axes,
focusing on shortening the spatio-temporal sequence by either generic pooling
or local windowing without utilizing temporal redundancy.
</p>
<p>However, videos naturally contain redundant information between neighboring
frames; thereby, we could potentially suppress attention on visually similar
frames in a dilated manner. Based on this hypothesis, we propose the LAPS, a
long-term ``\textbf{\textit{Leap Attention}}'' (LA), short-term
``\textbf{\textit{Periodic Shift}}'' (\textit{P}-Shift) module for video
transformers, with $(2TN^2)$ complexity. Specifically, the ``LA'' groups
long-term frames into pairs, then refactors each discrete pair via attention.
The ``\textit{P}-Shift'' exchanges features between temporal neighbors to
confront the loss of short-term dynamics. By replacing a vanilla 2D attention
with the LAPS, we could adapt a static transformer into a video one, with zero
extra parameters and neglectable computation overhead ($\sim$2.6\%).
Experiments on the standard Kinetics-400 benchmark demonstrate that our LAPS
transformer could achieve competitive performances in terms of accuracy, FLOPs,
and Params among CNN and transformer SOTAs. We open-source our project in
\sloppy
\href{https://github.com/VideoNetworks/LAPS-transformer}{\textit{\color{magenta}{https://github.com/VideoNetworks/LAPS-transformer}}} .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransGrasp: Grasp Pose Estimation of a Category of Objects by Transferring Grasps from Only One Labeled Instance. (arXiv:2207.07861v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07861">
<div class="article-summary-box-inner">
<span><p>Grasp pose estimation is an important issue for robots to interact with the
real world. However, most of existing methods require exact 3D object models
available beforehand or a large amount of grasp annotations for training. To
avoid these problems, we propose TransGrasp, a category-level grasp pose
estimation method that predicts grasp poses of a category of objects by
labeling only one object instance. Specifically, we perform grasp pose transfer
across a category of objects based on their shape correspondences and propose a
grasp pose refinement module to further fine-tune grasp pose of grippers so as
to ensure successful grasps. Experiments demonstrate the effectiveness of our
method on achieving high-quality grasps with the transferred grasp poses. Our
code is available at https://github.com/yanjh97/TransGrasp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDM:Visual Explanations for Neural Networks via Multiple Dynamic Mask. (arXiv:2207.08046v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08046">
<div class="article-summary-box-inner">
<span><p>The Class Activation Maps(CAM) lookup of a neural network can tell us what
regions the neural network is focusing on when making a decision.We propose an
algorithm Multiple Dynamic Mask (MDM), which is a general saliency graph query
method with interpretability of inference process. The algorithm is based on an
assumption: when a picture is input into a trained neural network, only the
activation features related to classification will affect the classification
results of the neural network, and the features unrelated to classification
will hardly affect the classification results of the network. MDM: A
learning-based end-to-end algorithm for finding regions of interest for neural
network classification.It has the following advantages: 1. It has the
interpretability of the reasoning process, and the reasoning process conforms
to human cognition. 2. It is universal, it can be used for any neural network
and does not depend on the internal structure of the neural network. 3. The
search performance is better. The algorithm is based on learning and has the
ability to adapt to different data and networks. The performance is better than
the method proposed in the previous paper. For the MDM saliency map search
algorithm, we experimentally compared ResNet and DenseNet as the trained neural
network. The recent advanced saliency map search method and the results of MDM
on the performance indicators of each search effect item, the performance of
MDM has reached the state of the art. We applied the MDM method to the
interpretable neural network ProtoPNet and XProtoNet, which improved the
model's interpretability prototype search performance. And we visualize the
effect of convolutional neural architecture and Transformer architecture in
saliency map search, illustrating the interpretability and generality of MDM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latency-Aware Collaborative Perception. (arXiv:2207.08560v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08560">
<div class="article-summary-box-inner">
<span><p>Collaborative perception has recently shown great potential to improve
perception capabilities over single-agent perception. Existing collaborative
perception methods usually consider an ideal communication environment.
However, in practice, the communication system inevitably suffers from latency
issues, causing potential performance degradation and high risks in
safety-critical applications, such as autonomous driving. To mitigate the
effect caused by the inevitable latency, from a machine learning perspective,
we present the first latency-aware collaborative perception system, which
actively adapts asynchronous perceptual features from multiple agents to the
same time stamp, promoting the robustness and effectiveness of collaboration.
To achieve such a feature-level synchronization, we propose a novel latency
compensation module, called SyncNet, which leverages feature-attention
symbiotic estimation and time modulation techniques. Experiments results show
that the proposed latency aware collaborative perception system with SyncNet
can outperforms the state-of-the-art collaborative perception method by 15.6%
in the communication latency scenario and keep collaborative perception being
superior to single agent perception under severe latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Partition Implicit with Surface Codes for 3D Representation. (arXiv:2207.08631v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08631">
<div class="article-summary-box-inner">
<span><p>Deep implicit functions have shown remarkable shape modeling ability in
various 3D computer vision tasks. One drawback is that it is hard for them to
represent a 3D shape as multiple parts. Current solutions learn various
primitives and blend the primitives directly in the spatial space, which still
struggle to approximate the 3D shape accurately. To resolve this problem, we
introduce a novel implicit representation to represent a single 3D shape as a
set of parts in the latent space, towards both highly accurate and plausibly
interpretable shape modeling. Our insight here is that both the part learning
and the part blending can be conducted much easier in the latent space than in
the spatial space. We name our method Latent Partition Implicit (LPI), because
of its ability of casting the global shape modeling into multiple local part
modeling, which partitions the global shape unity. LPI represents a shape as
Signed Distance Functions (SDFs) using surface codes. Each surface code is a
latent code representing a part whose center is on the surface, which enables
us to flexibly employ intrinsic attributes of shapes or additional surface
properties. Eventually, LPI can reconstruct both the shape and the parts on the
shape, both of which are plausible meshes. LPI is a multi-level representation,
which can partition a shape into different numbers of parts after training. LPI
can be learned without ground truth signed distances, point normals or any
supervision for part partition. LPI outperforms the latest methods under the
widely used benchmarks in terms of reconstruction accuracy and modeling
interpretability. Our code, data and models are available at
https://github.com/chenchao15/LPI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Recognition based on Multi-Task Learning Framework in the ABAW4 Challenge. (arXiv:2207.09373v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09373">
<div class="article-summary-box-inner">
<span><p>This paper presents our submission to the Multi-Task Learning (MTL) Challenge
of the 4th Affective Behavior Analysis in-the-wild (ABAW) competition. Based on
visual feature representations, we utilize three types of temporal encoder to
capture the temporal context information in the video, including the
transformer based encoder, LSTM based encoder and GRU based encoder. With the
temporal context-aware representations, we employ multi-task framework to
predict the valence, arousal, expression and AU values of the images. In
addition, smoothing processing is applied to refine the initial valence and
arousal predictions, and a model ensemble strategy is used to combine multiple
results from different model setups. Our system achieves the performance of
$1.742$ on MTL Challenge validation dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model. (arXiv:2207.10040v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10040">
<div class="article-summary-box-inner">
<span><p>Image restoration algorithms for atmospheric turbulence are known to be much
more challenging to design than traditional ones such as blur or noise because
the distortion caused by the turbulence is an entanglement of spatially varying
blur, geometric distortion, and sensor noise. Existing CNN-based restoration
methods built upon convolutional kernels with static weights are insufficient
to handle the spatially dynamical atmospheric turbulence effect. To address
this problem, in this paper, we propose a physics-inspired transformer model
for imaging through atmospheric turbulence. The proposed network utilizes the
power of transformer blocks to jointly extract a dynamical turbulence
distortion map and restore a turbulence-free image. In addition, recognizing
the lack of a comprehensive dataset, we collect and present two new real-world
turbulence datasets that allow for evaluation with both classical objective
metrics (e.g., PSNR and SSIM) and a new task-driven metric using text
recognition accuracy. Both real testing sets and all related code will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SplitMixer: Fat Trimmed From MLP-like Models. (arXiv:2207.10255v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10255">
<div class="article-summary-box-inner">
<span><p>We present SplitMixer, a simple and lightweight isotropic MLP-like
architecture, for visual recognition. It contains two types of interleaving
convolutional operations to mix information across spatial locations (spatial
mixing) and channels (channel mixing). The first one includes sequentially
applying two depthwise 1D kernels, instead of a 2D kernel, to mix spatial
information. The second one is splitting the channels into overlapping or
non-overlapping segments, with or without shared parameters, and applying our
proposed channel mixing approaches or 3D convolution to mix channel
information. Depending on design choices, a number of SplitMixer variants can
be constructed to balance accuracy, the number of parameters, and speed. We
show, both theoretically and experimentally, that SplitMixer performs on par
with the state-of-the-art MLP-like models while having a significantly lower
number of parameters and FLOPS. For example, without strong data augmentation
and optimization, SplitMixer achieves around 94% accuracy on CIFAR-10 with only
0.28M parameters, while ConvMixer achieves the same accuracy with about 0.6M
parameters. The well-known MLP-Mixer achieves 85.45% with 17.1M parameters. On
CIFAR-100 dataset, SplitMixer achieves around 73% accuracy, on par with
ConvMixer, but with about 52% fewer parameters and FLOPS. We hope that our
results spark further research towards finding more efficient vision
architectures and facilitate the development of MLP-like models. Code is
available at https://github.com/aliborji/splitmixer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeshLoc: Mesh-Based Visual Localization. (arXiv:2207.10762v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10762">
<div class="article-summary-box-inner">
<span><p>Visual localization, i.e., the problem of camera pose estimation, is a
central component of applications such as autonomous robots and augmented
reality systems. A dominant approach in the literature, shown to scale to large
scenes and to handle complex illumination and seasonal changes, is based on
local features extracted from images. The scene representation is a sparse
Structure-from-Motion point cloud that is tied to a specific local feature.
Switching to another feature type requires an expensive feature matching step
between the database images used to construct the point cloud. In this work, we
thus explore a more flexible alternative based on dense 3D meshes that does not
require features matching between database images to build the scene
representation. We show that this approach can achieve state-of-the-art
results. We further show that surprisingly competitive results can be obtained
when extracting features on renderings of these meshes, without any neural
rendering stage, and even when rendering raw scene geometry without color or
texture. Our results show that dense 3D model-based representations are a
promising alternative to existing representations and point to interesting and
challenging directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Random Occlusion and Multi-Layer Projection for Deep Multi-Camera Pedestrian Localization. (arXiv:2207.10895v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10895">
<div class="article-summary-box-inner">
<span><p>Although deep-learning based methods for monocular pedestrian detection have
made great progress, they are still vulnerable to heavy occlusions. Using
multi-view information fusion is a potential solution but has limited
applications, due to the lack of annotated training samples in existing
multi-view datasets, which increases the risk of overfitting. To address this
problem, a data augmentation method is proposed to randomly generate 3D
cylinder occlusions, on the ground plane, which are of the average size of
pedestrians and projected to multiple views, to relieve the impact of
overfitting in the training. Moreover, the feature map of each view is
projected to multiple parallel planes at different heights, by using
homographies, which allows the CNNs to fully utilize the features across the
height of each pedestrian to infer the locations of pedestrians on the ground
plane. The proposed 3DROM method has a greatly improved performance in
comparison with the state-of-the-art deep-learning based methods for multi-view
pedestrian detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11095">
<div class="article-summary-box-inner">
<span><p>Speckle filtering is generally a prerequisite to the analysis of synthetic
aperture radar (SAR) images. Tremendous progress has been achieved in the
domain of single-image despeckling. Latest techniques rely on deep neural
networks to restore the various structures and textures peculiar to SAR images.
The availability of time series of SAR images offers the possibility of
improving speckle filtering by combining different speckle realizations over
the same area. The supervised training of deep neural networks requires
ground-truth speckle-free images. Such images can only be obtained indirectly
through some form of averaging, by spatial or temporal integration, and are
imperfect. Given the potential of very high quality restoration reachable by
multi-temporal speckle filtering, the limitations of ground-truth images need
to be circumvented. We extend a recent self-supervised training strategy for
single-look complex SAR images, called MERLIN, to the case of multi-temporal
filtering. This requires modeling the sources of statistical dependencies in
the spatial and temporal dimensions as well as between the real and imaginary
components of the complex amplitudes. Quantitative analysis on datasets with
simulated speckle indicates a clear improvement of speckle reduction when
additional SAR images are included. Our method is then applied to stacks of
TerraSAR-X images and shown to outperform competing multi-temporal speckle
filtering approaches. The code of the trained models is made freely available
on the Gitlab of the IMAGES team of the LTCI Lab, T\'el\'ecom Paris Institut
Polytechnique de Paris
(https://gitlab.telecom-paris.fr/ring/multi-temporal-merlin/).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STEFANN: Scene Text Editor using Font Adaptive Neural Network. (arXiv:1903.01192v2 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1903.01192">
<div class="article-summary-box-inner">
<span><p>Textual information in a captured scene plays an important role in scene
interpretation and decision making. Though there exist methods that can
successfully detect and interpret complex text regions present in a scene, to
the best of our knowledge, there is no significant prior work that aims to
modify the textual information in an image. The ability to edit text directly
on images has several advantages including error correction, text restoration
and image reusability. In this paper, we propose a method to modify text in an
image at character-level. We approach the problem in two stages. At first, the
unobserved character (target) is generated from an observed character (source)
being modified. We propose two different neural network architectures - (a)
FANnet to achieve structural consistency with source font and (b) Colornet to
preserve source color. Next, we replace the source character with the generated
character maintaining both geometric and visual consistency with neighboring
characters. Our method works as a unified platform for modifying text in
images. We present the effectiveness of our method on COCO-Text and ICDAR
datasets both qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-26 23:08:02.683608864 UTC">2022-07-26 23:08:02 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>