<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-02T01:30:00Z">06-02</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models. (arXiv:2206.00052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00052">
<div class="article-summary-box-inner">
<span><p>Pre-trained programming language (PL) models (such as CodeT5, CodeBERT,
GraphCodeBERT, etc.,) have the potential to automate software engineering tasks
involving code understanding and code generation. However, these models are not
robust to changes in the input and thus, are potentially susceptible to
adversarial attacks. We propose, CodeAttack, a simple yet effective black-box
attack model that uses code structure to generate imperceptible, effective, and
minimally perturbed adversarial code samples. We demonstrate the
vulnerabilities of the state-of-the-art PL models to code-specific adversarial
attacks. We evaluate the transferability of CodeAttack on several code-code
(translation and repair) and code-NL (summarization) tasks across different
programming languages. CodeAttack outperforms state-of-the-art adversarial NLP
attack models to achieve the best overall performance while being more
efficient and imperceptible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Mixture-of-Expert Approach to RL-based Dialogue Management. (arXiv:2206.00059v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00059">
<div class="article-summary-box-inner">
<span><p>Despite recent advancements in language models (LMs), their application to
dialogue management (DM) problems and ability to carry on rich conversations
remain a challenge. We use reinforcement learning (RL) to develop a dialogue
agent that avoids being short-sighted (outputting generic utterances) and
maximizes overall user satisfaction. Most existing RL approaches to DM train
the agent at the word-level, and thus, have to deal with a combinatorially
complex action space even for a medium-size vocabulary. As a result, they
struggle to produce a successful and engaging dialogue even if they are
warm-started with a pre-trained LM. To address this issue, we develop a
RL-based DM using a novel mixture of expert language model (MoE-LM) that
consists of (i) a LM capable of learning diverse semantics for conversation
histories, (ii) a number of {\em specialized} LMs (or experts) capable of
generating utterances corresponding to a particular attribute or personality,
and (iii) a RL-based DM that performs dialogue planning with the utterances
generated by the experts. Our MoE approach provides greater flexibility to
generate sensible utterances with different intents and allows RL to focus on
conversational-level DM. We compare it with SOTA baselines on open-domain
dialogues and demonstrate its effectiveness both in terms of the diversity and
sensibility of the generated utterances and the overall DM performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VALHALLA: Visual Hallucination for Machine Translation. (arXiv:2206.00100v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00100">
<div class="article-summary-box-inner">
<span><p>Designing better machine translation systems by considering auxiliary inputs
such as images has attracted much attention in recent years. While existing
methods show promising performance over the conventional text-only translation
systems, they typically require paired text and image as input during
inference, which limits their applicability to real-world scenarios. In this
paper, we introduce a visual hallucination framework, called VALHALLA, which
requires only source sentences at inference time and instead uses hallucinated
visual representations for multimodal machine translation. In particular, given
a source sentence an autoregressive hallucination transformer is used to
predict a discrete visual representation from the input text, and the combined
text and hallucinated representations are utilized to obtain the target
translation. We train the hallucination transformer jointly with the
translation transformer using standard backpropagation with cross-entropy
losses while being guided by an additional loss that encourages consistency
between predictions using either ground-truth or hallucinated visual
representations. Extensive experiments on three standard translation datasets
with a diverse set of language pairs demonstrate the effectiveness of our
approach over both text-only baselines and state-of-the-art methods. Project
page: <a href="http://www.svcl.ucsd.edu/projects/valhalla.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IGLU Gridworld: Simple and Fast Environment for Embodied Dialog Agents. (arXiv:2206.00142v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00142">
<div class="article-summary-box-inner">
<span><p>We present the IGLU Gridworld: a reinforcement learning environment for
building and evaluating language conditioned embodied agents in a scalable way.
The environment features visual agent embodiment, interactive learning through
collaboration, language conditioned RL, and combinatorically hard task (3d
blocks building) space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding How People Rate Their Conversations. (arXiv:2206.00167v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00167">
<div class="article-summary-box-inner">
<span><p>User ratings play a significant role in spoken dialogue systems. Typically,
such ratings tend to be averaged across all users and then utilized as feedback
to improve the system or personalize its behavior. While this method can be
useful to understand broad, general issues with the system and its behavior, it
does not take into account differences between users that affect their ratings.
In this work, we conduct a study to better understand how people rate their
interactions with conversational agents. One macro-level characteristic that
has been shown to correlate with how people perceive their inter-personal
communication is personality. We specifically focus on agreeableness and
extraversion as variables that may explain variation in ratings and therefore
provide a more meaningful signal for training or personalization. In order to
elicit those personality traits during an interaction with a conversational
agent, we designed and validated a fictional story, grounded in prior work in
psychology. We then implemented the story into an experimental conversational
agent that allowed users to opt-in to hearing the story. Our results suggest
that for human-conversational agent interactions, extraversion may play a role
in user ratings, but more data is needed to determine if the relationship is
significant. Agreeableness, on the other hand, plays a statistically
significant role in conversation ratings: users who are more agreeable are more
likely to provide a higher rating for their interaction. In addition, we found
that users who opted to hear the story were, in general, more likely to rate
their conversational experience higher than those who did not.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering the Hidden Vocabulary of DALLE-2. (arXiv:2206.00169v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00169">
<div class="article-summary-box-inner">
<span><p>We discover that DALLE-2 seems to have a hidden vocabulary that can be used
to generate images with absurd prompts. For example, it seems that
\texttt{Apoploe vesrreaitais} means birds and \texttt{Contarra ccetnxniams
luryca tanniounons} (sometimes) means bugs or pests. We find that these prompts
are often consistent in isolation but also sometimes in combinations. We
present our black-box method to discover words that seem random but have some
correspondence to visual concepts. This creates important security and
interpretability challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Order-sensitive Shapley Values for Evaluating Conceptual Soundness of NLP Models. (arXiv:2206.00192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00192">
<div class="article-summary-box-inner">
<span><p>Previous works show that deep NLP models are not always conceptually sound:
they do not always learn the correct linguistic concepts. Specifically, they
can be insensitive to word order. In order to systematically evaluate models
for their conceptual soundness with respect to word order, we introduce a new
explanation method for sequential data: Order-sensitive Shapley Values (OSV).
We conduct an extensive empirical evaluation to validate the method and surface
how well various deep NLP models learn word order. Using synthetic data, we
first show that OSV is more faithful in explaining model behavior than
gradient-based methods. Second, applying to the HANS dataset, we discover that
the BERT-based NLI model uses only the word occurrences without word orders.
Although simple data augmentation improves accuracy on HANS, OSV shows that the
augmented model does not fundamentally improve the model's learning of order.
Third, we discover that not all sentiment analysis models learn negation
properly: some fail to capture the correct syntax of the negation construct.
Finally, we show that pretrained language models such as BERT may rely on the
absolute positions of subject words to learn long-range Subject-Verb Agreement.
With each NLP task, we also demonstrate how OSV can be leveraged to generate
adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption. (arXiv:2206.00216v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00216">
<div class="article-summary-box-inner">
<span><p>As more and more pre-trained language models adopt on-cloud deployment, the
privacy issues grow quickly, mainly for the exposure of plain-text user data
(e.g., search history, medical record, bank account). Privacy-preserving
inference of transformer models is on the demand of cloud service users. To
protect privacy, it is an attractive choice to compute only with ciphertext in
homomorphic encryption (HE). However, enabling pre-trained models inference on
ciphertext data is difficult due to the complex computations in transformer
blocks, which are not supported by current HE tools yet. In this work, we
introduce $\textit{THE-X}$, an approximation approach for transformers, which
enables privacy-preserving inference of pre-trained models developed by popular
frameworks. $\textit{THE-X}$ proposes a workflow to deal with complex
computation in transformer networks, including all the non-polynomial functions
like GELU, softmax, and LayerNorm. Experiments reveal our proposed
$\textit{THE-X}$ can enable transformer inference on encrypted data for
different downstream tasks, all with negligible performance drop but enjoying
the theory-guaranteed privacy-preserving advantage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Group-level Gender Bias in Professional Evaluations: The Case of Medical Student End-of-Shift Feedback. (arXiv:2206.00234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00234">
<div class="article-summary-box-inner">
<span><p>Although approximately 50% of medical school graduates today are women,
female physicians tend to be underrepresented in senior positions, make less
money than their male counterparts and receive fewer promotions. There is a
growing body of literature demonstrating gender bias in various forms of
evaluation in medicine, but this work was mainly conducted by looking for
specific words using fixed dictionaries such as LIWC and focused on
recommendation letters. We use a dataset of written and quantitative
assessments of medical student performance on individual shifts of work,
collected across multiple institutions, to investigate the extent to which
gender bias exists in a day-to-day context for medical students. We investigate
differences in the narrative comments given to male and female students by both
male or female faculty assessors, using a fine-tuned BERT model. This allows us
to examine whether groups are written about in systematically different ways,
without relying on hand-crafted wordlists or topic models. We compare these
results to results from the traditional LIWC method and find that, although we
find no evidence of group-level gender bias in this dataset, terms related to
family and children are used more in feedback given to women.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IDANI: Inference-time Domain Adaptation via Neuron-level Interventions. (arXiv:2206.00259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00259">
<div class="article-summary-box-inner">
<span><p>Large pre-trained models are usually fine-tuned on downstream task data, and
tested on unseen data. When the train and test data come from different
domains, the model is likely to struggle, as it is not adapted to the test
domain. We propose a new approach for domain adaptation (DA), using
neuron-level interventions: We modify the representation of each test example
in specific neurons, resulting in a counterfactual example from the source
domain, which the model is more familiar with. The modified example is then fed
back into the model. While most other DA methods are applied during training
time, ours is applied during inference only, making it more efficient and
applicable. Our experiments show that our method improves performance on unseen
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InducT-GCN: Inductive Graph Convolutional Networks for Text Classification. (arXiv:2206.00265v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00265">
<div class="article-summary-box-inner">
<span><p>Text classification aims to assign labels to textual units by making use of
global information. Recent studies have applied graph neural network (GNN) to
capture the global word co-occurrence in a corpus. Existing approaches require
that all the nodes (training and test) in a graph are present during training,
which are transductive and do not naturally generalise to unseen nodes. To make
those models inductive, they use extra resources, like pretrained word
embedding. However, high-quality resource is not always available and hard to
train. Under the extreme settings with no extra resource and limited amount of
training set, can we still learn an inductive graph-based text classification
model? In this paper, we introduce a novel inductive graph-based text
classification framework, InducT-GCN (InducTive Graph Convolutional Networks
for Text classification). Compared to transductive models that require test
documents in training, we construct a graph based on the statistics of training
documents only and represent document vectors with a weighted sum of word
vectors. We then conduct one-directional GCN propagation during testing. Across
five text classification benchmarks, our InducT-GCN outperformed
state-of-the-art methods that are either transductive in nature or pre-trained
additional resources. We also conducted scalability testing by gradually
increasing the data size and revealed that our InducT-GCN can reduce the time
and space complexity. The code is available on:
https://github.com/usydnlp/InductTGCN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MORE: A Metric Learning Based Framework for Open-domain Relation Extraction. (arXiv:2206.00289v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00289">
<div class="article-summary-box-inner">
<span><p>Open relation extraction (OpenRE) is the task of extracting relation schemes
from open-domain corpora. Most existing OpenRE methods either do not fully
benefit from high-quality labeled corpora or can not learn semantic
representation directly, affecting downstream clustering efficiency. To address
these problems, in this work, we propose a novel learning framework named MORE
(Metric learning-based Open Relation Extraction). The framework utilizes deep
metric learning to obtain rich supervision signals from labeled data and drive
the neural model to learn semantic relational representation directly.
Experiments result in two real-world datasets show that our method outperforms
other state-of-the-art baselines. Our source code is available on Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Layer Normalizations and Residual Connections in Transformers. (arXiv:2206.00330v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00330">
<div class="article-summary-box-inner">
<span><p>In the perspective of a layer normalization (LN) position, the architecture
of Transformers can be categorized into two types: Post-LN and Pre-LN. Recent
Transformers prefer to select Pre-LN because the training in Post-LN with deep
Transformers, e.g., ten or more layers, often becomes unstable, resulting in
useless models. However, in contrast, Post-LN has also consistently achieved
better performance than Pre-LN in relatively shallow Transformers, e.g., six or
fewer layers. This study first investigates the reason for these discrepant
observations empirically and theoretically and discovers 1, the LN in Post-LN
is the source of the vanishing gradient problem that mainly leads the unstable
training whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger
gradient norms in higher layers during the back-propagation that may lead an
effective training. Exploiting the new findings, we propose a method that can
equip both higher stability and effective training by a simple modification
from Post-LN. We conduct experiments on a wide range of text generation tasks
and demonstrate that our method outperforms Pre-LN, and stable training
regardless of the shallow or deep layer settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical character recognition quality affects perceived usefulness of historical newspaper clippings. (arXiv:2206.00369v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00369">
<div class="article-summary-box-inner">
<span><p>Introduction. We study effect of different quality optical character
recognition in interactive information retrieval with a collection of one
digitized historical Finnish newspaper. Method. This study is based on the
simulated interactive information retrieval work task model. Thirty-two users
made searches to an article collection of Finnish newspaper Uusi Suometar
1869-1918 with ca. 1.45 million auto segmented articles. Our article search
database had two versions of each article with different quality optical
character recognition. Each user performed six pre-formulated and six
self-formulated short queries and evaluated subjectively the top-10 results
using graded relevance scale of 0-3 without knowing about the optical character
recognition quality differences of the otherwise identical articles. Analysis.
Analysis of the user evaluations was performed by comparing mean averages of
evaluations scores in user sessions. Differences of query results were detected
by analysing lengths of returned articles in pre-formulated and self-formulated
queries and number of different documents retrieved overall in these two
sessions. Results. The main result of the study is that improved optical
character recognition quality affects perceived usefulness of historical
newspaper articles positively. Conclusions. We were able to show that
improvement in optical character recognition quality of documents leads to
higher mean relevance evaluation scores of query results in our historical
newspaper collection. To the best of our knowledge this simulated interactive
user-task is the first one showing empirically that users' subjective relevance
assessments are affected by a change in the quality of optically read text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate Speech in Different Social Contexts. (arXiv:2206.00372v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00372">
<div class="article-summary-box-inner">
<span><p>Social media platforms and online streaming services have spawned a new breed
of Hate Speech (HS). Due to the massive amount of user-generated content on
these sites, modern machine learning techniques are found to be feasible and
cost-effective to tackle this problem. However, linguistically diverse datasets
covering different social contexts in which offensive language is typically
used are required to train generalizable models. In this paper, we identify the
shortcomings of existing Bangla HS datasets and introduce a large manually
labeled dataset BD-SHS that includes HS in different social contexts. The
labeling criteria were prepared following a hierarchical annotation process,
which is the first of its kind in Bangla HS to the best of our knowledge. The
dataset includes more than 50,200 offensive comments crawled from online social
networking sites and is at least 60% larger than any existing Bangla HS
datasets. We present the benchmark result of our dataset by training different
NLP models resulting in the best one achieving an F1-score of 91.0%. In our
experiments, we found that a word embedding trained exclusively using 1.47
million comments from social media and streaming sites consistently resulted in
better modeling of HS detection in comparison to other pre-trained embeddings.
Our dataset and all accompanying codes is publicly available at
github.com/naurosromim/hate-speech-dataset-for-Bengali-social-media
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Use of NLP-Based Text Representation Techniques to Support Requirement Engineering Tasks: A Systematic Mapping Review. (arXiv:2206.00421v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00421">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) is widely used to support the automation of
different Requirements Engineering (RE) tasks. Most of the proposed approaches
start with various NLP steps that analyze requirements statements, extract
their linguistic information, and convert them to easy-to-process
representations, such as lists of features or embedding-based vector
representations. These NLP-based representations are usually used at a later
stage as inputs for machine learning techniques or rule-based methods. Thus,
requirements representations play a major role in determining the accuracy of
different approaches. In this paper, we conducted a survey in the form of a
systematic literature mapping (classification) to find out (1) what are the
representations used in RE tasks literature, (2) what is the main focus of
these works, (3) what are the main research directions in this domain, and (4)
what are the gaps and potential future directions. After compiling an initial
pool of 2,227 papers, and applying a set of inclusion/exclusion criteria, we
obtained a final pool containing 104 relevant papers. Our survey shows that the
research direction has changed from the use of lexical and syntactic features
to the use of advanced embedding techniques, especially in the last two years.
Using advanced embedding representations has proved its effectiveness in most
RE tasks (such as requirement analysis, extracting requirements from reviews
and forums, and semantic-level quality tasks). However, representations that
are based on lexical and syntactic features are still more appropriate for
other RE tasks (such as modeling and syntax-level quality tasks) since they
provide the required information for the rules and regular expressions used
when handling these tasks. In addition, we identify four gaps in the existing
literature, why they matter, and how future research can begin to address them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What a Creole Wants, What a Creole Needs. (arXiv:2206.00437v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00437">
<div class="article-summary-box-inner">
<span><p>In recent years, the natural language processing (NLP) community has given
increased attention to the disparity of efforts directed towards high-resource
languages over low-resource ones. Efforts to remedy this delta often begin with
translations of existing English datasets into other languages. However, this
approach ignores that different language communities have different needs. We
consider a group of low-resource languages, Creole languages. Creoles are both
largely absent from the NLP literature, and also often ignored by society at
large due to stigma, despite these languages having sizable and vibrant
communities. We demonstrate, through conversations with Creole experts and
surveys of Creole-speaking communities, how the things needed from language
technology can change dramatically from one language to another, even when the
languages are considered to be very similar to each other, as with Creoles. We
discuss the prominent themes arising from these conversations, and ultimately
demonstrate that useful language technology cannot be built without involving
the relevant community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vietnamese Hate and Offensive Detection using PhoBERT-CNN and Social Media Streaming Data. (arXiv:2206.00524v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00524">
<div class="article-summary-box-inner">
<span><p>Society needs to develop a system to detect hate and offense to build a
healthy and safe environment. However, current research in this field still
faces four major shortcomings, including deficient pre-processing techniques,
indifference to data imbalance issues, modest performance models, and lacking
practical applications. This paper focused on developing an intelligent system
capable of addressing these shortcomings. Firstly, we proposed an efficient
pre-processing technique to clean comments collected from Vietnamese social
media. Secondly, a novel hate speech detection (HSD) model, which is the
combination of a pre-trained PhoBERT model and a Text-CNN model, was proposed
for solving tasks in Vietnamese. Thirdly, EDA techniques are applied to deal
with imbalanced data to improve the performance of classification models.
Besides, various experiments were conducted as baselines to compare and
investigate the proposed model's performance against state-of-the-art methods.
The experiment results show that the proposed PhoBERT-CNN model outperforms
SOTA methods and achieves an F1-score of 67,46% and 98,45% on two benchmark
datasets, ViHSD and HSD-VLSP, respectively. Finally, we also built a streaming
HSD application to demonstrate the practicality of our proposed system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Diversity in Back Translation for Low-Resource Machine Translation. (arXiv:2206.00564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00564">
<div class="article-summary-box-inner">
<span><p>Back translation is one of the most widely used methods for improving the
performance of neural machine translation systems. Recent research has sought
to enhance the effectiveness of this method by increasing the 'diversity' of
the generated translations. We argue that the definitions and metrics used to
quantify 'diversity' in previous work have been insufficient. This work puts
forward a more nuanced framework for understanding diversity in training data,
splitting it into lexical diversity and syntactic diversity. We present novel
metrics for measuring these different aspects of diversity and carry out
empirical analysis into the effect of these types of diversity on final neural
machine translation model performance for low-resource
English$\leftrightarrow$Turkish and mid-resource
English$\leftrightarrow$Icelandic. Our findings show that generating back
translation using nucleus sampling results in higher final model performance,
and that this method of generation has high levels of both lexical and
syntactic diversity. We also find evidence that lexical diversity is more
important than syntactic for back translation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. (arXiv:2206.00621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00621">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce Cross-View Language Modeling, a simple and
effective language model pre-training framework that unifies cross-lingual
cross-modal pre-training with shared architectures and objectives. Our approach
is motivated by a key observation that cross-lingual and cross-modal
pre-training share the same goal of aligning two different views of the same
object into a common semantic space. To this end, the cross-view language
modeling framework considers both multi-modal data (i.e., image-caption pairs)
and multi-lingual data (i.e., parallel sentence pairs) as two different views
of the same object, and trains the model to align the two views by maximizing
the mutual information between them with conditional masked language modeling
and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal
Language Model, with the cross-view language modeling framework. Empirical
results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual
image-text retrieval datasets show that while conceptually simpler, CCLM
significantly outperforms the prior state-of-the-art with an average absolute
improvement of over 10%. Notably, CCLM is the first multi-lingual multi-modal
model that surpasses the translate-test performance of representative English
vision-language models by zero-shot cross-lingual transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin. (arXiv:2206.00648v1 [q-fin.ST])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00648">
<div class="article-summary-box-inner">
<span><p>Bitcoin, with its ever-growing popularity, has demonstrated extreme price
volatility since its origin. This volatility, together with its decentralised
nature, make Bitcoin highly subjective to speculative trading as compared to
more traditional assets. In this paper, we propose a multimodal model for
predicting extreme price fluctuations. This model takes as input a variety of
correlated assets, technical indicators, as well as Twitter content. In an
in-depth study, we explore whether social media discussions from the general
public on Bitcoin have predictive power for extreme price movements. A dataset
of 5,000 tweets per day containing the keyword `Bitcoin' was collected from
2015 to 2021. This dataset, called PreBit, is made available online. In our
hybrid model, we use sentence-level FinBERT embeddings, pretrained on financial
lexicons, so as to capture the full contents of the tweets and feed it to the
model in an understandable way. By combining these embeddings with a
Convolutional Neural Network, we built a predictive model for significant
market movements. The final multimodal ensemble model includes this NLP model
together with a model based on candlestick data, technical indicators and
correlated asset prices. In an ablation study, we explore the contribution of
the individual modalities. Finally, we propose and backtest a trading strategy
based on the predictions of our models with varying prediction threshold and
show that it can used to build a profitable trading strategy with a reduced
risk over a `hold' or moving average strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Sense Language Modelling. (arXiv:2012.05776v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05776">
<div class="article-summary-box-inner">
<span><p>The effectiveness of a language model is influenced by its token
representations, which must encode contextual information and handle the same
word form having a plurality of meanings (polysemy). Currently, none of the
common language modelling architectures explicitly model polysemy. We propose a
language model which not only predicts the next word, but also its sense in
context. We argue that this higher prediction granularity may be useful for end
tasks such as assistive writing, and allow for more a precise linking of
language models with knowledge bases. We find that multi-sense language
modelling requires architectures that go beyond standard language models, and
here propose a structured prediction framework that decomposes the task into a
word followed by a sense prediction task. To aid sense prediction, we utilise a
Graph Attention Network, which encodes definitions and example uses of word
senses. Overall, we find that multi-sense language modelling is a highly
challenging task, and suggest that future work focus on the creation of more
annotated training datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">'Just because you are right, doesn't mean I am wrong': Overcoming a Bottleneck in the Development and Evaluation of Open-Ended Visual Question Answering (VQA) Tasks. (arXiv:2103.15022v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15022">
<div class="article-summary-box-inner">
<span><p>GQA~\citep{hudson2019gqa} is a dataset for real-world visual reasoning and
compositional question answering. We found that many answers predicted by the
best vision-language models on the GQA dataset do not match the ground-truth
answer but still are semantically meaningful and correct in the given context.
In fact, this is the case with most existing visual question answering (VQA)
datasets where they assume only one ground-truth answer for each question. We
propose Alternative Answer Sets (AAS) of ground-truth answers to address this
limitation, which is created automatically using off-the-shelf NLP tools. We
introduce a semantic metric based on AAS and modify top VQA solvers to support
multiple plausible answers for a question. We implement this approach on the
GQA dataset and show the performance improvements. Code and data are available
in this link \url{https://github.com/luomancs/alternative_answer_set.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning. (arXiv:2108.00356v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00356">
<div class="article-summary-box-inner">
<span><p>Masked language models (MLMs) are pre-trained with a denoising objective that
is in a mismatch with the objective of downstream fine-tuning. We propose
pragmatic masking and surrogate fine-tuning as two complementing strategies
that exploit social cues to drive pre-trained representations toward a broad
set of concepts useful for a wide class of social meaning tasks. We test our
models on $15$ different Twitter datasets for social meaning detection. Our
methods achieve $2.34\%$ $F_1$ over a competitive baseline, while outperforming
domain-specific language models pre-trained on large datasets. Our methods also
excel in few-shot learning: with only $5\%$ of training data (severely
few-shot), our methods enable an impressive $68.54\%$ average $F_1$. The
methods are also language agnostic, as we show in a zero-shot setting involving
six datasets from three different languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-3 Models are Poor Few-Shot Learners in the Biomedical Domain. (arXiv:2109.02555v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02555">
<div class="article-summary-box-inner">
<span><p>Deep neural language models have set new breakthroughs in many tasks of
Natural Language Processing (NLP). Recent work has shown that deep transformer
language models (pretrained on large amounts of texts) can achieve high levels
of task-specific few-shot performance comparable to state-of-the-art models.
However, the ability of these large language models in few-shot transfer
learning has not yet been explored in the biomedical domain. We investigated
the performance of two powerful transformer language models, i.e. GPT-3 and
BioBERT, in few-shot settings on various biomedical NLP tasks. The experimental
results showed that, to a great extent, both the models underperform a language
model fine-tuned on the full training data. Although GPT-3 had already achieved
near state-of-the-art results in few-shot knowledge transfer on open-domain NLP
tasks, it could not perform as effectively as BioBERT, which is orders of
magnitude smaller than GPT-3. Regarding that BioBERT was already pretrained on
large biomedical text corpora, our study suggests that language models may
largely benefit from in-domain pretraining in task-specific few-shot learning.
However, in-domain pretraining seems not to be sufficient; novel pretraining
and few-shot learning strategies are required in the biomedical NLP domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08475">
<div class="article-summary-box-inner">
<span><p>Visual dialog, which aims to hold a meaningful conversation with humans about
a given image, is a challenging task that requires models to reason the complex
dependencies among visual content, dialog history, and current questions. Graph
neural networks are recently applied to model the implicit relations between
objects in an image or dialog. However, they neglect the importance of 1)
coreference relations among dialog history and dependency relations between
words for the question representation; and 2) the representation of the image
based on the fully represented question. Therefore, we propose a novel
relation-aware graph-over-graph network (GoG) for visual dialog. Specifically,
GoG consists of three sequential graphs: 1) H-Graph, which aims to capture
coreference relations among dialog history; 2) History-aware Q-Graph, which
aims to fully understand the question through capturing dependency relations
between words based on coreference resolution on the dialog history; and 3)
Question-aware I-Graph, which aims to capture the relations between objects in
an image based on fully question representation. As an additional feature
representation module, we add GoG to the existing visual dialogue model.
Experimental results show that our model outperforms the strong baseline in
both generative and discriminative settings by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Cross-Utterance Language Modeling for Conversational Speech Recognition. (arXiv:2111.03333v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03333">
<div class="article-summary-box-inner">
<span><p>Conversational speech normally is embodied with loose syntactic structures at
the utterance level but simultaneously exhibits topical coherence relations
across consecutive utterances. Prior work has shown that capturing longer
context information with a recurrent neural network or long short-term memory
language model (LM) may suffer from the recent bias while excluding the
long-range context. In order to capture the long-term semantic interactions
among words and across utterances, we put forward disparate conversation
history fusion methods for language modeling in automatic speech recognition
(ASR) of conversational speech. Furthermore, a novel audio-fusion mechanism is
introduced, which manages to fuse and utilize the acoustic embeddings of a
current utterance and the semantic content of its corresponding conversation
history in a cooperative way. To flesh out our ideas, we frame the ASR N-best
hypothesis rescoring task as a prediction problem, leveraging BERT, an iconic
pre-trained LM, as the ingredient vehicle to facilitate selection of the oracle
hypothesis from a given N-best hypothesis list. Empirical experiments conducted
on the AMI benchmark dataset seem to demonstrate the feasibility and efficacy
of our methods in relation to some current top-of-line methods. The proposed
methods not only achieve significant inference time reduction but also improve
the ASR performance for conversational speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. (arXiv:2111.08276v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08276">
<div class="article-summary-box-inner">
<span><p>Most existing methods in vision language pre-training rely on object-centric
features extracted through object detection and make fine-grained alignments
between the extracted features and texts. It is challenging for these methods
to learn relations among multiple objects. To this end, we propose a new method
called X-VLM to perform `multi-grained vision language pre-training.' The key
to learning multi-grained alignments is to locate visual concepts in the image
given the associated texts, and in the meantime align the texts with the visual
concepts, where the alignments are in multi-granularity. Experimental results
show that X-VLM effectively leverages the learned multi-grained alignments to
many downstream vision language tasks and consistently outperforms
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radiology Report Generation with a Learned Knowledge Base and Multi-modal Alignment. (arXiv:2112.15011v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15011">
<div class="article-summary-box-inner">
<span><p>In clinics, a radiology report is crucial for guiding a patient's treatment.
However, writing radiology reports is a heavy burden for radiologists. To this
end, we present an automatic, multi-modal approach for report generation from a
chest x-ray. Our approach, motivated by the observation that the descriptions
in radiology reports are highly correlated with specific information of the
x-ray images, features two distinct modules: (i) Learned knowledge base: To
absorb the knowledge embedded in the radiology reports, we build a knowledge
base that can automatically distil and restore medical knowledge from textual
embedding without manual labour; (ii) Multi-modal alignment: to promote the
semantic alignment among reports, disease labels, and images, we explicitly
utilize textual embedding to guide the learning of the visual feature space. We
evaluate the performance of the proposed model using metrics from both natural
language generation and clinic efficacy on the public IU-Xray and MIMIC-CXR
datasets. Our ablation study shows that each module contributes to improving
the quality of generated reports. Furthermore, with the assistance of both
modules, our approach outperforms state-of-the-art methods over almost all the
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Knowledge Graph Embeddings based Approach for Author Name Disambiguation using Literals. (arXiv:2201.09555v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09555">
<div class="article-summary-box-inner">
<span><p>Scholarly data is growing continuously containing information about the
articles from a plethora of venues including conferences, journals, etc. Many
initiatives have been taken to make scholarly data available as Knowledge
Graphs (KGs). These efforts to standardize these data and make them accessible
have also led to many challenges such as exploration of scholarly articles,
ambiguous authors, etc. This study more specifically targets the problem of
Author Name Disambiguation (AND) on Scholarly KGs and presents a novel
framework, Literally Author Name Disambiguation (LAND), which utilizes
Knowledge Graph Embeddings (KGEs) using multimodal literal information
generated from these KGs. This framework is based on three components: 1)
Multimodal KGEs, 2) A blocking procedure, and finally, 3) Hierarchical
Agglomerative Clustering. Extensive experiments have been conducted on two
newly created KGs: (i) KG containing information from Scientometrics Journal
from 1978 onwards (OC-782K), and (ii) a KG extracted from a well-known
benchmark for AND provided by AMiner (AMiner-534K). The results show that our
proposed architecture outperforms our baselines of 8-14% in terms of the F1
score and shows competitive performances on a challenging benchmark such as
AMiner. The code and the datasets are publicly available through Github:
https://github.com/sntcristian/and-kge and
Zenodo:https://doi.org/10.5281/zenodo.6309855 respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain of Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11903">
<div class="article-summary-box-inner">
<span><p>We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. (arXiv:2202.03052v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03052">
<div class="article-summary-box-inner">
<span><p>In this work, we pursue a unified paradigm for multimodal pretraining to
break the scaffolds of complex task/modality-specific customization. We propose
OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task
Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks,
including image generation, visual grounding, image captioning, image
classification, language modeling, etc., in a simple sequence-to-sequence
learning framework. OFA follows the instruction-based learning in both
pretraining and finetuning stages, requiring no extra task-specific layers for
downstream tasks. In comparison with the recent state-of-the-art vision &amp;
language models that rely on extremely large cross-modal datasets, OFA is
pretrained on only 20M publicly available image-text pairs. Despite its
simplicity and relatively small-scale training data, OFA achieves new SOTAs in
a series of cross-modal tasks while attaining highly competitive performances
on uni-modal tasks. Our further analysis indicates that OFA can also
effectively transfer to unseen tasks and unseen domains. Our code and models
are publicly available at https://github.com/OFA-Sys/OFA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Language Modeling with Sparse all-MLP. (arXiv:2203.06850v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06850">
<div class="article-summary-box-inner">
<span><p>All-MLP architectures have attracted increasing interest as an alternative to
attention-based models. In NLP, recent work like gMLP shows that all-MLPs can
match Transformers in language modeling, but still lag behind in downstream
tasks. In this work, we analyze the limitations of MLPs in expressiveness, and
propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature
and input (token) dimensions. Such sparse all-MLPs significantly increase model
capacity and expressiveness while keeping the compute constant. We address
critical challenges in incorporating conditional computation with two routing
strategies. The proposed sparse all-MLP improves language modeling perplexity
and obtains up to 2$\times$ improvement in training efficiency compared to both
Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH
Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its
zero-shot in-context learning performance on six downstream tasks, and find
that it surpasses Transformer-based MoEs and dense Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition. (arXiv:2203.06925v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06925">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition task is one of the core tasks of information
extraction.Word ambiguity and word abbreviation are important reasons for the
low recognition rate of named entities. In this paper, we propose a novel named
entity recognition model WCL-BBCD (Word Contrastive Learning with
BERT-BiLSTM-CRF-DBpedia) incorporating the idea of contrastive learning. The
model first trains the sentence pairs in the text, calculate similarity between
words in sentence pairs by cosine similarity, and fine-tunes the BERT model
used for the named entity recognition task through the similarity, so as to
alleviate word ambiguity. Then, the fine-tuned BERT model is combined with the
BiLSTM-CRF model to perform the named entity recognition task. Finally, the
recognition results are corrected in combination with prior knowledge such as
knowledge graphs, so as to alleviate the recognition caused by word
abbreviations low-rate problem. Experimental results show that our model
outperforms other similar model methods on the CoNLL-2003 English dataset and
OntoNotes V5 English dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships. (arXiv:2203.14260v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14260">
<div class="article-summary-box-inner">
<span><p>Understanding realistic visual scene images together with language
descriptions is a fundamental task towards generic visual understanding.
Previous works have shown compelling comprehensive results by building
hierarchical structures for visual scenes (e.g., scene graphs) and natural
languages (e.g., dependency trees), individually. However, how to construct a
joint vision-language (VL) structure has barely been investigated. More
challenging but worthwhile, we introduce a new task that targets on inducing
such a joint VL structure in an unsupervised manner. Our goal is to bridge the
visual scene graphs and linguistic dependency trees seamlessly. Due to the lack
of VL structural data, we start by building a new dataset VLParse. Rather than
using labor-intensive labeling from scratch, we propose an automatic alignment
procedure to produce coarse structures followed by human refinement to produce
high-quality ones. Moreover, we benchmark our dataset by proposing a
contrastive learning (CL)-based framework VLGAE, short for Vision-Language
Graph Autoencoder. Our model obtains superior performance on two derived tasks,
i.e., language grammar induction and VL phrase grounding. Ablations show the
effectiveness of both visual cues and dependency relationships on fine-grained
VL structure construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clozer: Adaptable Data Augmentation for Cloze-style Reading Comprehension. (arXiv:2203.16027v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16027">
<div class="article-summary-box-inner">
<span><p>Task-adaptive pre-training (TAPT) alleviates the lack of labelled data and
provides performance lift by adapting unlabelled data to downstream task.
Unfortunately, existing adaptations mainly involve deterministic rules that
cannot generalize well. Here, we propose Clozer, a sequence-tagging based cloze
answer extraction method used in TAPT that is extendable for adaptation on any
cloze-style machine reading comprehension (MRC) downstream tasks. We experiment
on multiple-choice cloze-style MRC tasks, and show that Clozer performs
significantly better compared to the oracle and state-of-the-art in escalating
TAPT effectiveness in lifting model performance, and prove that Clozer is able
to recognize the gold answers independently of any heuristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taming Continuous Posteriors for Latent Variational Dialogue Policies. (arXiv:2205.07633v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07633">
<div class="article-summary-box-inner">
<span><p>Utilizing amortized variational inference for latent-action reinforcement
learning (RL) has been shown to be an effective approach in Task-oriented
Dialogue (ToD) systems for optimizing dialogue success. Until now, categorical
posteriors have been argued to be one of the main drivers of performance. In
this work we revisit Gaussian variational posteriors for latent-action RL and
show that they can yield even better performance than categoricals. We achieve
this by simplifying the training procedure and propose ways to regularize the
latent dialogue policy to retain good response coherence. Using continuous
latent representations our model achieves state of the art dialogue success
rate on the MultiWOZ benchmark, and also compares well to categorical latent
methods in response coherence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialog Inpainting: Turning Documents into Dialogs. (arXiv:2205.09073v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09073">
<div class="article-summary-box-inner">
<span><p>Many important questions (e.g. "How to eat healthier?") require conversation
to establish context and explore in depth. However, conversational question
answering (ConvQA) systems have long been stymied by scarce training data that
is expensive to collect. To address this problem, we propose a new technique
for synthetically generating diverse and high-quality dialog data: dialog
inpainting. Our approach takes the text of any document and transforms it into
a two-person dialog between the writer and an imagined reader: we treat
sentences from the article as utterances spoken by the writer, and then use a
dialog inpainter to predict what the imagined reader asked or said in between
each of the writer's utterances. By applying this approach to passages from
Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets
totalling 19 million diverse information-seeking dialogs -- 1,000x larger than
the largest existing ConvQA dataset. Furthermore, human raters judge the answer
adequacy and conversationality of WikiDialog to be as good or better than
existing manually-collected datasets. Using our inpainted data to pre-train
ConvQA retrieval systems, we significantly advance state-of-the-art across
three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains
on standard evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing. (arXiv:2205.09607v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09607">
<div class="article-summary-box-inner">
<span><p>Semantic parsing is the task of producing structured meaning representations
for natural language sentences. Recent research has pointed out that the
commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle to
generalize systematically, i.e. to handle examples that require recombining
known knowledge in novel settings. In this work, we show that better systematic
generalization can be achieved by producing the meaning representation directly
as a graph and not as a sequence. To this end we propose LAGr (Label Aligned
Graphs), a general framework to produce semantic parses by independently
predicting node and edge labels for a complete multi-layer input-aligned graph.
The strongly-supervised LAGr algorithm requires aligned graphs as inputs,
whereas weakly-supervised LAGr infers alignments for originally unaligned
target graphs using approximate maximum-a-posteriori inference. Experiments
demonstrate that LAGr achieves significant improvements in systematic
generalization upon the baseline seq2seq parsers in both strongly- and
weakly-supervised settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training. (arXiv:2205.10471v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10471">
<div class="article-summary-box-inner">
<span><p>Keyphrase generation is the task of automatically predicting keyphrases given
a piece of long text. Despite its recent flourishing, keyphrase generation on
non-English languages haven't been vastly investigated. In this paper, we call
attention to a new setting named multilingual keyphrase generation and we
contribute two new datasets, EcommerceMKP and AcademicMKP, covering six
languages. Technically, we propose a retrieval-augmented method for
multilingual keyphrase generation to mitigate the data shortage problem in
non-English languages. The retrieval-augmented model leverages keyphrase
annotations in English datasets to facilitate generating keyphrases in
low-resource languages. Given a non-English passage, a cross-lingual dense
passage retrieval module finds relevant English passages. Then the associated
English keyphrases serve as external knowledge for keyphrase generation in the
current language. Moreover, we develop a retriever-generator iterative training
algorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual
passage retriever. Comprehensive experiments and ablations show that the
proposed approach outperforms all baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Light-Weight Answer Text Retrieval in Dialogue Systems. (arXiv:2205.14226v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14226">
<div class="article-summary-box-inner">
<span><p>Dialogue systems can benefit from being able to search through a corpus of
text to find information relevant to user requests, especially when
encountering a request for which no manually curated response is available. The
state-of-the-art technology for neural dense retrieval or re-ranking involves
deep learning models with hundreds of millions of parameters. However, it is
difficult and expensive to get such models to operate at an industrial scale,
especially for cloud services that often need to support a big number of
individually customized dialogue systems, each with its own text corpus. We
report our work on enabling advanced neural dense retrieval systems to operate
effectively at scale on relatively inexpensive hardware. We compare with
leading alternative industrial solutions and show that we can provide a
solution that is effective, fast, and cost-efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anchor Prediction: A Topic Modeling Approach. (arXiv:2205.14631v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14631">
<div class="article-summary-box-inner">
<span><p>Networks of documents connected by hyperlinks, such as Wikipedia, are
ubiquitous. Hyperlinks are inserted by the authors to enrich the text and
facilitate the navigation through the network. However, authors tend to insert
only a fraction of the relevant hyperlinks, mainly because this is a time
consuming task. In this paper we address an annotation, which we refer to as
anchor prediction. Even though it is conceptually close to link prediction or
entity linking, it is a different task that require developing a specific
method to solve it. Given a source document and a target document, this task
consists in automatically identifying anchors in the source document, i.e words
or terms that should carry a hyperlink pointing towards the target document. We
propose a contextualized relational topic model, CRTM, that models directed
links between documents as a function of the local context of the anchor in the
source document and the whole content of the target document. The model can be
used to predict anchors in a source document, given the target document,
without relying on a dictionary of previously seen mention or title, nor any
external knowledge graph. Authors can benefit from CRTM, by letting it
automatically suggest hyperlinks, given a new document and the set of target
document to connect to. It can also benefit to readers, by dynamically
inserting hyperlinks between the documents they're reading. Experiments
conducted on several Wikipedia corpora (in English, Italian and German)
highlight the practical usefulness of anchor prediction and demonstrate the
relevancy of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking. (arXiv:2205.15503v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15503">
<div class="article-summary-box-inner">
<span><p>Current natural language interaction for self-tracking tools largely depends
on bespoke implementation optimized for a specific tracking theme and data
format, which is neither generalizable nor scalable to a tremendous design
space of self-tracking. However, training machine learning models in the
context of self-tracking is challenging due to the wide variety of tracking
topics and data formats. In this paper, we propose a novel NLP task for
self-tracking that extracts close- and open-ended information from a
retrospective activity log described as a plain text, and a domain-agnostic,
GPT-3-based NLU framework that performs this task. The framework augments the
prompt using synthetic samples to transform the task into 10-shot learning, to
address a cold-start problem in bootstrapping a new tracking topic. Our
preliminary evaluation suggests that our approach significantly outperforms the
baseline QA models. Going further, we discuss future application domains toward
which the NLP and HCI researchers can collaborate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Transformers for Product Matching -- Experiments and a New Benchmark in Polish. (arXiv:2205.15712v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15712">
<div class="article-summary-box-inner">
<span><p>Product matching corresponds to the task of matching identical products
across different data sources. It typically employs available product features
which, apart from being multimodal, i.e., comprised of various data types,
might be non-homogeneous and incomplete. The paper shows that pre-trained,
multilingual Transformer models, after fine-tuning, are suitable for solving
the product matching problem using textual features both in English and Polish
languages. We tested multilingual mBERT and XLM-RoBERTa models in English on
Web Data Commons - training dataset and gold standard for large-scale product
matching. The obtained results show that these models perform similarly to the
latest solutions tested on this set, and in some cases, the results were even
better.
</p>
<p>Additionally, we prepared a new dataset entirely in Polish and based on
offers in selected categories obtained from several online stores for the
research purpose. It is the first open dataset for product matching tasks in
Polish, which allows comparing the effectiveness of the pre-trained models.
Thus, we also showed the baseline results obtained by the fine-tuned mBERT and
XLM-RoBERTa models on the Polish datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues. (arXiv:2205.15951v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15951">
<div class="article-summary-box-inner">
<span><p>Movies reflect society and also hold power to transform opinions. Social
biases and stereotypes present in movies can cause extensive damage due to
their reach. These biases are not always found to be the need of storyline but
can creep in as the author's bias. Movie production houses would prefer to
ascertain that the bias present in a script is the story's demand. Today, when
deep learning models can give human-level accuracy in multiple tasks, having an
AI solution to identify the biases present in the script at the writing stage
can help them avoid the inconvenience of stalled release, lawsuits, etc. Since
AI solutions are data intensive and there exists no domain specific data to
address the problem of biases in scripts, we introduce a new dataset of movie
scripts that are annotated for identity bias. The dataset contains dialogue
turns annotated for (i) bias labels for seven categories, viz., gender,
race/ethnicity, religion, age, occupation, LGBTQ, and other, which contains
biases like body shaming, personality bias, etc. (ii) labels for sensitivity,
stereotype, sentiment, emotion, emotion intensity, (iii) all labels annotated
with context awareness, (iv) target groups and reason for bias labels and (v)
expert-driven group-validation process for high quality annotations. We also
report various baseline performances for bias identification and category
detection on our dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cluster-based Evaluation of Automatically Generated Text. (arXiv:2205.16001v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.16001">
<div class="article-summary-box-inner">
<span><p>While probabilistic language generators have improved dramatically over the
last few years, the automatic evaluation metrics used to assess them have not
kept pace with this progress. In the domain of language generation, a good
metric must correlate highly with human judgements. Yet, with few exceptions,
there is a lack of such metrics in the literature. In this work, we analyse the
general paradigm of language generator evaluation. We first discuss the
computational and qualitative issues with using automatic evaluation metrics
that operate on probability distributions over strings, the backbone of most
language generators. We then propose the use of distributions over clusters
instead, where we cluster strings based on their text embeddings (obtained from
a pretrained language model). While we find the biases introduced by this
substitution to be quite strong, we observe that, empirically, this methodology
leads to metric estimators with higher correlation with human judgements, while
simultaneously reducing estimator variance. We finish the paper with a probing
analysis, which leads us to conclude that -- by encoding syntactic- and
coherence-level features of text, while ignoring surface-level features --
these clusters may simply be better equipped to evaluate state-of-the-art
language models.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrated Bagging Deep Learning for Image Semantic Segmentation: A Case Study on COVID-19 Chest X-ray Image. (arXiv:2206.00002v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00002">
<div class="article-summary-box-inner">
<span><p>Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes
coronavirus disease 2019 (COVID-19). Imaging tests such as chest X-ray (CXR)
and computed tomography (CT) can provide useful information to clinical staff
for facilitating a diagnosis of COVID-19 in a more efficient and comprehensive
manner. As a breakthrough of artificial intelligence (AI), deep learning has
been applied to perform COVID-19 infection region segmentation and disease
classification by analyzing CXR and CT data. However, prediction uncertainty of
deep learning models for these tasks, which is very important to
safety-critical applications like medical image processing, has not been
comprehensively investigated. In this work, we propose a novel ensemble deep
learning model through integrating bagging deep learning and model calibration
to not only enhance segmentation performance, but also reduce prediction
uncertainty. The proposed method has been validated on a large dataset that is
associated with CXR image segmentation. Experimental results demonstrate that
the proposed method can improve the segmentation performance, as well as
decrease prediction uncertainties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterization of 3D Printers and X-Ray Computerized Tomography. (arXiv:2206.00041v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00041">
<div class="article-summary-box-inner">
<span><p>The 3D printing process flow requires several inputs for the best printing
quality. These settings may vary from sample to sample, printer to printer, and
depend upon users' previous experience. The involved operational parameters for
3D Printing are varied to test the optimality. Thirty-eight samples are printed
using four commercially available 3D printers, namely: (a) Ultimaker 2
Extended+, (b) Delta Wasp, (c) Raise E2, and (d) ProJet MJP. The sample
profiles contain uniform and non-uniform distribution of the assorted size of
cubes and spheres with a known amount of porosity. These samples are scanned
using X-Ray Computed Tomography system. Functional Imaging analysis is
performed using AI-based segmentation codes to (a) characterize these 3D
printers and (b) find Three-dimensional surface roughness of three teeth and
one sandstone pebble (from riverbed) with naturally deposited layers is also
compared with printed sample values. Teeth has best quality. It is found that
ProJet MJP gives the best quality of printed samples with the least amount of
surface roughness and almost near to the actual porosity value. As expected,
100% infill density value, best spatial resolution for printing or Layer
height, and minimum nozzle speed give the best quality of 3D printing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PandA: Unsupervised Learning of Parts and Appearances in the Feature Maps of GANs. (arXiv:2206.00048v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00048">
<div class="article-summary-box-inner">
<span><p>Recent advances in the understanding of Generative Adversarial Networks
(GANs) have led to remarkable progress in visual editing and synthesis tasks,
capitalizing on the rich semantics that are embedded in the latent spaces of
pre-trained GANs. However, existing methods are often tailored to specific GAN
architectures and are limited to either discovering global semantic directions
that do not facilitate localized control, or require some form of supervision
through manually provided regions or segmentation masks. In this light, we
present an architecture-agnostic approach that jointly discovers factors
representing spatial parts and their appearances in an entirely unsupervised
fashion. These factors are obtained by applying a semi-nonnegative tensor
factorization on the feature maps, which in turn enables context-aware local
image editing with pixel-level control. In addition, we show that the
discovered appearance factors correspond to saliency maps that localize
concepts of interest, without using any labels. Experiments on a wide range of
GAN architectures and datasets show that, in comparison to the state of the
art, our method is far more efficient in terms of training time and, most
importantly, provides much more accurate localized control. Our code is
available at: https://github.com/james-oldfield/PandA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing feature fusion strategies for Deep Learning-based kidney stone identification. (arXiv:2206.00069v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00069">
<div class="article-summary-box-inner">
<span><p>This contribution presents a deep-learning method for extracting and fusing
image information acquired from different viewpoints with the aim to produce
more discriminant object features. Our approach was specifically designed to
mimic the morpho-constitutional analysis used by urologists to visually
classify kidney stones by inspecting the sections and surfaces of their
fragments. Deep feature fusion strategies improved the results of single view
extraction backbone models by more than 10\% in terms of precision of the
kidney stones classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FHIST: A Benchmark for Few-shot Classification of Histological Images. (arXiv:2206.00092v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00092">
<div class="article-summary-box-inner">
<span><p>Few-shot learning has recently attracted wide interest in image
classification, but almost all the current public benchmarks are focused on
natural images. The few-shot paradigm is highly relevant in medical-imaging
applications due to the scarcity of labeled data, as annotations are expensive
and require specialized expertise. However, in medical imaging, few-shot
learning research is sparse, limited to private data sets and is at its early
stage. In particular, the few-shot setting is of high interest in histology due
to the diversity and fine granularity of cancer related tissue classification
tasks, and the variety of data-preparation techniques. This paper introduces a
highly diversified public benchmark, gathered from various public datasets, for
few-shot histology data classification. We build few-shot tasks and
base-training data with various tissue types, different levels of domain shifts
stemming from various cancer sites, and different class-granularity levels,
thereby reflecting realistic scenarios. We evaluate the performances of
state-of-the-art few-shot learning methods on our benchmark, and observe that
simple fine-tuning and regularization methods achieve better results than the
popular meta-learning and episodic-training paradigm. Furthermore, we introduce
three scenarios based on the domain shifts between the source and target
histology data: near-domain, middle-domain and out-domain. Our experiments
display the potential of few-shot learning in histology classification, with
state-of-art few shot learning methods approaching the supervised-learning
baselines in the near-domain setting. In our out-domain setting, for 5-way
5-shot, the best performing method reaches 60% accuracy. We believe that our
work could help in building realistic evaluations and fair comparisons of
few-shot learning methods and will further encourage research in the few-shot
paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VALHALLA: Visual Hallucination for Machine Translation. (arXiv:2206.00100v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00100">
<div class="article-summary-box-inner">
<span><p>Designing better machine translation systems by considering auxiliary inputs
such as images has attracted much attention in recent years. While existing
methods show promising performance over the conventional text-only translation
systems, they typically require paired text and image as input during
inference, which limits their applicability to real-world scenarios. In this
paper, we introduce a visual hallucination framework, called VALHALLA, which
requires only source sentences at inference time and instead uses hallucinated
visual representations for multimodal machine translation. In particular, given
a source sentence an autoregressive hallucination transformer is used to
predict a discrete visual representation from the input text, and the combined
text and hallucinated representations are utilized to obtain the target
translation. We train the hallucination transformer jointly with the
translation transformer using standard backpropagation with cross-entropy
losses while being guided by an additional loss that encourages consistency
between predictions using either ground-truth or hallucinated visual
representations. Extensive experiments on three standard translation datasets
with a diverse set of language pairs demonstrate the effectiveness of our
approach over both text-only baselines and state-of-the-art methods. Project
page: <a href="http://www.svcl.ucsd.edu/projects/valhalla.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning pipeline for image classification on mobile phones. (arXiv:2206.00105v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00105">
<div class="article-summary-box-inner">
<span><p>This article proposes and documents a machine-learning framework and tutorial
for classifying images using mobile phones. Compared to computers, the
performance of deep learning model performance degrades when deployed on a
mobile phone and requires a systematic approach to find a model that performs
optimally on both computers and mobile phones. By following the proposed
pipeline, which consists of various computational tools, simple procedural
recipes, and technical considerations, one can bring the power of deep learning
medical image classification to mobile devices, potentially unlocking new
domains of applications. The pipeline is demonstrated on four different
publicly available datasets: COVID X-rays, COVID CT scans, leaves, and
colorectal cancer. We used two application development frameworks: TensorFlow
Lite (real-time testing) and Flutter (digital image testing) to test the
proposed pipeline. We found that transferring deep learning models to a mobile
phone is limited by hardware and classification accuracy drops. To address this
issue, we proposed this pipeline to find an optimized model for mobile phones.
Finally, we discuss additional applications and computational concerns related
to deploying deep-learning models on phones, including real-time analysis and
image preprocessing. We believe the associated documentation and code can help
physicians and medical experts develop medical image classification
applications for distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Glo-In-One: Holistic Glomerular Detection, Segmentation, and Lesion Characterization with Large-scale Web Image Mining. (arXiv:2206.00123v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00123">
<div class="article-summary-box-inner">
<span><p>The quantitative detection, segmentation, and characterization of glomeruli
from high-resolution whole slide imaging (WSI) play essential roles in the
computer-assisted diagnosis and scientific research in digital renal pathology.
Historically, such comprehensive quantification requires extensive programming
skills in order to be able to handle heterogeneous and customized computational
tools. To bridge the gap of performing glomerular quantification for
non-technical users, we develop the Glo-In-One toolkit to achieve holistic
glomerular detection, segmentation, and characterization via a single line of
command. Additionally, we release a large-scale collection of 30,000 unlabeled
glomerular images to further facilitate the algorithmic development of
self-supervised deep learning. The inputs of the Glo-In-One toolkit are WSIs,
while the outputs are (1) WSI-level multi-class circle glomerular detection
results (which can be directly manipulated with ImageScope), (2) glomerular
image patches with segmentation masks, and (3) different lesion types. To
leverage the performance of the Glo-In-One toolkit, we introduce
self-supervised deep learning to glomerular quantification via large-scale web
image mining. The GGS fine-grained classification model achieved a decent
performance compared with baseline supervised methods while only using 10% of
the annotated data. The glomerular detection achieved an average precision of
0.627 with circle representations, while the glomerular segmentation achieved a
0.955 patch-wise Dice Similarity Coefficient (DSC).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection. (arXiv:2206.00148v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00148">
<div class="article-summary-box-inner">
<span><p>Over the past few years there has been major progress in the field of
synthetic data generation using simulation based techniques. These methods use
high-end graphics engines and physics-based ray-tracing rendering in order to
represent the world in 3D and create highly realistic images. Datagen has
specialized in the generation of high-quality 3D humans, realistic 3D
environments and generation of realistic human motion. This technology has been
developed into a data generation platform which we used for these experiments.
This work demonstrates the use of synthetic photo-realistic in-cabin data to
train a Driver Monitoring System that uses a lightweight neural network to
detect whether the driver's hands are on the wheel. We demonstrate that when
only a small amount of real data is available, synthetic data can be a simple
way to boost performance. Moreover, we adopt the data-centric approach and show
how performing error analysis and generating the missing edge-cases in our
platform boosts performance. This showcases the ability of human-centric
synthetic data to generalize well to the real world, and help train algorithms
in computer vision settings where data from the target domain is scarce or hard
to collect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAGER: Progressive Attribute-Guided Extendable Robust Image Generation. (arXiv:2206.00162v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00162">
<div class="article-summary-box-inner">
<span><p>This work presents a generative modeling approach based on successive
subspace learning (SSL). Unlike most generative models in the literature, our
method does not utilize neural networks to analyze the underlying source
distribution and synthesize images. The resulting method, called the
progressive attribute-guided extendable robust image generative (PAGER) model,
has advantages in mathematical transparency, progressive content generation,
lower training time, robust performance with fewer training samples, and
extendibility to conditional image generation. PAGER consists of three modules:
core generator, resolution enhancer, and quality booster. The core generator
learns the distribution of low-resolution images and performs unconditional
image generation. The resolution enhancer increases image resolution via
conditional generation. Finally, the quality booster adds finer details to
generated images. Extensive experiments on MNIST, Fashion-MNIST, and CelebA
datasets are conducted to demonstrate generative performance of PAGER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering the Hidden Vocabulary of DALLE-2. (arXiv:2206.00169v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00169">
<div class="article-summary-box-inner">
<span><p>We discover that DALLE-2 seems to have a hidden vocabulary that can be used
to generate images with absurd prompts. For example, it seems that
\texttt{Apoploe vesrreaitais} means birds and \texttt{Contarra ccetnxniams
luryca tanniounons} (sometimes) means bugs or pests. We find that these prompts
are often consistent in isolation but also sometimes in combinations. We
present our black-box method to discover words that seem random but have some
correspondence to visual concepts. This creates important security and
interpretability challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Sequential Contexts using Transformer for 3D Hand Pose Estimation. (arXiv:2206.00171v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00171">
<div class="article-summary-box-inner">
<span><p>3D hand pose estimation (HPE) is the process of locating the joints of the
hand in 3D from any visual input. HPE has recently received an increased amount
of attention due to its key role in a variety of human-computer interaction
applications. Recent HPE methods have demonstrated the advantages of employing
videos or multi-view images, allowing for more robust HPE systems. Accordingly,
in this study, we propose a new method to perform Sequential learning with
Transformer for Hand Pose (SeTHPose) estimation. Our SeTHPose pipeline begins
by extracting visual embeddings from individual hand images. We then use a
transformer encoder to learn the sequential context along time or viewing
angles and generate accurate 2D hand joint locations. Then, a graph
convolutional neural network with a U-Net configuration is used to convert the
2D hand joint locations to 3D poses. Our experiments show that SeTHPose
performs well on both hand sequence varieties, temporal and angular. Also,
SeTHPose outperforms other methods in the field to achieve new state-of-the-art
results on two public available sequential datasets, STB and MuViHand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Labeling Where Adapting Fails: Cross-Domain Semantic Segmentation with Point Supervision via Active Selection. (arXiv:2206.00181v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00181">
<div class="article-summary-box-inner">
<span><p>Training models dedicated to semantic segmentation requires a large amount of
pixel-wise annotated data. Due to their costly nature, these annotations might
not be available for the task at hand. To alleviate this problem, unsupervised
domain adaptation approaches aim at aligning the feature distributions between
the labeled source and the unlabeled target data. While these strategies lead
to noticeable improvements, their effectiveness remains limited. To guide the
domain adaptation task more efficiently, previous works attempted to include
human interactions in this process under the form of sparse single-pixel
annotations in the target data. In this work, we propose a new domain
adaptation framework for semantic segmentation with annotated points via active
selection. First, we conduct an unsupervised domain adaptation of the model;
from this adaptation, we use an entropy-based uncertainty measurement for
target points selection. Finally, to minimize the domain gap, we propose a
domain adaptation framework utilizing these target points annotated by human
annotators. Experimental results on benchmark datasets show the effectiveness
of our methods against existing unsupervised domain adaptation approaches. The
propose pipeline is generic and can be included as an extra module to existing
domain adaptation strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Soft-Masked Attention. (arXiv:2206.00182v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00182">
<div class="article-summary-box-inner">
<span><p>Transformers have become prevalent in computer vision due to their
performance and flexibility in modelling complex operations. Of particular
significance is the 'cross-attention' operation, which allows a vector
representation (e.g. of an object in an image) to be learned by attending to an
arbitrarily sized set of input features. Recently, "Masked Attention" was
proposed in which a given object representation only attends to those image
pixel features for which the segmentation mask of that object is active. This
specialization of attention proved beneficial for various image and video
segmentation tasks. In this paper, we propose another specialization of
attention which enables attending over `soft-masks' (those with continuous mask
probabilities instead of binary values), and is also differentiable through
these mask probabilities, thus allowing the mask used for attention to be
learned within the network without requiring direct loss supervision. This can
be useful for several applications. Specifically, we employ our "Differentiable
Soft-Masked Attention" for the task of Weakly-Supervised Video Object
Segmentation (VOS), where we develop a transformer-based network for VOS which
only requires a single annotated image frame for training, but can also benefit
from cycle consistency training on a video with just one annotated frame.
Although there is no loss for masks in unlabeled frames, the network is still
able to segment objects in those frames due to our novel attention formulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAFA: Class-Aware Feature Alignment for Test-Time Adaptation. (arXiv:2206.00205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00205">
<div class="article-summary-box-inner">
<span><p>Despite recent advancements in deep learning, deep networks still suffer from
performance degradation when they face new and different data from their
training distributions. Addressing such a problem, test-time adaptation (TTA)
aims to adapt a model to unlabeled test data on test time while making
predictions simultaneously. TTA applies to pretrained networks without
modifying their training procedures, which enables to utilize the already
well-formed source distribution for adaptation. One possible approach is to
align the representation space of test samples to the source distribution
(\textit{i.e.,} feature alignment). However, performing feature alignments in
TTA is especially challenging in that the access to labeled source data is
restricted during adaptation. That is, a model does not have a chance to learn
test data in a class-discriminative manner, which was feasible in other
adaptation tasks (\textit{e.g.,} unsupervised domain adaptation) via supervised
loss on the source data. Based on such an observation, this paper proposes
\emph{a simple yet effective} feature alignment loss, termed as Class-Aware
Feature Alignment (CAFA), which 1) encourages a model to learn target
representations in a class-discriminative manner and 2) effectively mitigates
the distribution shifts in test time, simultaneously. Our method does not
require any hyper-parameters or additional losses, which are required in the
previous approaches. We conduct extensive experiments and show our proposed
method consistently outperforms existing baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiDAR-MIMO: Efficient Uncertainty Estimation for LiDAR-based 3D Object Detection. (arXiv:2206.00214v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00214">
<div class="article-summary-box-inner">
<span><p>The estimation of uncertainty in robotic vision, such as 3D object detection,
is an essential component in developing safe autonomous systems aware of their
own performance. However, the deployment of current uncertainty estimation
methods in 3D object detection remains challenging due to timing and
computational constraints. To tackle this issue, we propose LiDAR-MIMO, an
adaptation of the multi-input multi-output (MIMO) uncertainty estimation method
to the LiDAR-based 3D object detection task. Our method modifies the original
MIMO by performing multi-input at the feature level to ensure the detection,
uncertainty estimation, and runtime performance benefits are retained despite
the limited capacity of the underlying detector and the large computational
costs of point cloud processing. We compare LiDAR-MIMO with MC dropout and
ensembles as baselines and show comparable uncertainty estimation results with
only a small number of output heads. Further, LiDAR-MIMO can be configured to
be twice as fast as MC dropout and ensembles, while achieving higher mAP than
MC dropout and approaching that of ensembles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-domain Detection Transformer based on Spatial-aware and Semantic-aware Token Alignment. (arXiv:2206.00222v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00222">
<div class="article-summary-box-inner">
<span><p>Detection transformers like DETR have recently shown promising performance on
many object detection tasks, but the generalization ability of those methods is
still quite challenging for cross-domain adaptation scenarios. To address the
cross-domain issue, a straightforward way is to perform token alignment with
adversarial training in transformers. However, its performance is often
unsatisfactory as the tokens in detection transformers are quite diverse and
represent different spatial and semantic information. In this paper, we propose
a new method called Spatial-aware and Semantic-aware Token Alignment (SSTA) for
cross-domain detection transformers. In particular, we take advantage of the
characteristics of cross-attention as used in detection transformer and propose
the spatial-aware token alignment (SpaTA) and the semantic-aware token
alignment (SemTA) strategies to guide the token alignment across domains. For
spatial-aware token alignment, we can extract the information from the
cross-attention map (CAM) to align the distribution of tokens according to
their attention to object queries. For semantic-aware token alignment, we
inject the category information into the cross-attention map and construct
domain embedding to guide the learning of a multi-class discriminator so as to
model the category relationship and achieve category-level token alignment
during the entire adaptation process. We conduct extensive experiments on
several widely-used benchmarks, and the results clearly show the effectiveness
of our proposed method over existing state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Augmentation Module in Contrastive Learning: Learning Hierarchical Augmentation Invariance with Expanded Views. (arXiv:2206.00227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00227">
<div class="article-summary-box-inner">
<span><p>A data augmentation module is utilized in contrastive learning to transform
the given data example into two views, which is considered essential and
irreplaceable. However, the predetermined composition of multiple data
augmentations brings two drawbacks. First, the artificial choice of
augmentation types brings specific representational invariances to the model,
which have different degrees of positive and negative effects on different
downstream tasks. Treating each type of augmentation equally during training
makes the model learn non-optimal representations for various downstream tasks
and limits the flexibility to choose augmentation types beforehand. Second, the
strong data augmentations used in classic contrastive learning methods may
bring too much invariance in some cases, and fine-grained information that is
essential to some downstream tasks may be lost. This paper proposes a general
method to alleviate these two problems by considering where and what to
contrast in a general contrastive learning framework. We first propose to learn
different augmentation invariances at different depths of the model according
to the importance of each data augmentation instead of learning
representational invariances evenly in the backbone. We then propose to expand
the contrast content with augmentation embeddings to reduce the misleading
effects of strong data augmentations. Experiments based on several baseline
methods demonstrate that we learn better representations for various benchmarks
on classification, detection, and segmentation downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fair Comparison between Efficient Attentions. (arXiv:2206.00244v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00244">
<div class="article-summary-box-inner">
<span><p>Transformers have been successfully used in various fields and are becoming
the standard tools in computer vision. However, self-attention, a core
component of transformers, has a quadratic complexity problem, which limits the
use of transformers in various vision tasks that require dense prediction. Many
studies aiming at solving this problem have been reported proposed. However, no
comparative study of these methods using the same scale has been reported due
to different model configurations, training schemes, and new methods. In our
paper, we validate these efficient attention models on the ImageNet1K
classification task by changing only the attention operation and examining
which efficient attention is better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Deep Learning Classifier by Detection of Prototypical Parts on Kidney Stones Images. (arXiv:2206.00252v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00252">
<div class="article-summary-box-inner">
<span><p>Identifying the type of kidney stones can allow urologists to determine their
formation cause, improving the early prescription of appropriate treatments to
diminish future relapses. However, currently, the associated ex-vivo diagnosis
(known as morpho-constitutional analysis, MCA) is time-consuming, expensive,
and requires a great deal of experience, as it requires a visual analysis
component that is highly operator dependant. Recently, machine learning methods
have been developed for in-vivo endoscopic stone recognition. Shallow methods
have been demonstrated to be reliable and interpretable but exhibit low
accuracy, while deep learning-based methods yield high accuracy but are not
explainable. However, high stake decisions require understandable
computer-aided diagnosis (CAD) to suggest a course of action based on
reasonable evidence, rather than merely prescribe one. Herein, we investigate
means for learning part-prototypes (PPs) that enable interpretable models. Our
proposal suggests a classification for a kidney stone patch image and provides
explanations in a similar way as those used on the MCA method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaGO-LOAM: Robust Ground-Optimized LiDAR Odometry. (arXiv:2206.00266v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00266">
<div class="article-summary-box-inner">
<span><p>Numerous researchers have conducted studies to achieve fast and robust
ground-optimized LiDAR odometry methods for terrestrial mobile platforms. In
particular, ground-optimized LiDAR odometry usually employs ground segmentation
as a preprocessing method. This is because most of the points in a 3D point
cloud captured by a 3D LiDAR sensor on a terrestrial platform are from the
ground. However, the effect of the performance of ground segmentation on LiDAR
odometry is still not closely examined. In this paper, a robust
ground-optimized LiDAR odometry framework is proposed to facilitate the study
to check the effect of ground segmentation on LiDAR SLAM based on the
state-of-the-art (SOTA) method. By using our proposed odometry framework, it is
easy and straightforward to test whether ground segmentation algorithms help
extract well-described features and thus improve SLAM performance. In addition,
by leveraging the SOTA ground segmentation method called Patchwork, which shows
robust ground segmentation even in complex and uneven urban environments with
little performance perturbation, a novel ground-optimized LiDAR odometry is
proposed, called PaGO-LOAM. The methods were tested using the KITTI odometry
dataset. \textit{PaGO-LOAM} shows robust and accurate performance compared with
the baseline method. Our code is available at
https://github.com/url-kaist/AlterGround-LeGO-LOAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision GNN: An Image is Worth Graph of Nodes. (arXiv:2206.00272v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00272">
<div class="article-summary-box-inner">
<span><p>Network architecture plays a key role in the deep learning-based computer
vision system. The widely-used convolutional neural network and transformer
treat the image as a grid or sequence structure, which is not flexible to
capture irregular and complex objects. In this paper, we propose to represent
the image as a graph structure and introduce a new Vision GNN (ViG)
architecture to extract graph-level feature for visual tasks. We first split
the image to a number of patches which are viewed as nodes, and construct a
graph by connecting the nearest neighbors. Based on the graph representation of
images, we build our ViG model to transform and exchange information among all
the nodes. ViG consists of two basic modules: Grapher module with graph
convolution for aggregating and updating graph information, and FFN module with
two linear layers for node feature transformation. Both isotropic and pyramid
architectures of ViG are built with different model sizes. Extensive
experiments on image recognition and object detection tasks demonstrate the
superiority of our ViG architecture. We hope this pioneering study of GNN on
general visual tasks will provide useful inspiration and experience for future
research. The PyTroch code will be available at
https://github.com/huawei-noah/CV-Backbones and the MindSpore code will be
avaiable at https://gitee.com/mindspore/models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-Teaching: Weakly Semi-Supervised Object Detection with Point Annotations. (arXiv:2206.00274v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00274">
<div class="article-summary-box-inner">
<span><p>Point annotations are considerably more time-efficient than bounding box
annotations. However, how to use cheap point annotations to boost the
performance of semi-supervised object detection remains largely unsolved. In
this work, we present Point-Teaching, a weakly semi-supervised object detection
framework to fully exploit the point annotations. Specifically, we propose a
Hungarian-based point matching method to generate pseudo labels for point
annotated images. We further propose multiple instance learning (MIL)
approaches at the level of images and points to supervise the object detector
with point annotations. Finally, we propose a simple-yet-effective data
augmentation, termed point-guided copy-paste, to reduce the impact of the
unmatched points. Experiments demonstrate the effectiveness of our method on a
few datasets and various data regimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Bounding Box Annotation with Small Training Data Sets for Industrial Manufacturing. (arXiv:2206.00280v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00280">
<div class="article-summary-box-inner">
<span><p>In the past few years, object detection has attracted a lot of attention in
the context of human-robot collaboration and Industry 5.0 due to enormous
quality improvements in deep learning technologies. In many applications,
object detection models have to be able to quickly adapt to a changing
environment, i.e., to learn new objects. A crucial but challenging prerequisite
for this is the automatic generation of new training data which currently still
limits the broad application of object detection methods in industrial
manufacturing. In this work, we discuss how to adapt state-of-the-art object
detection methods for the task of automatic bounding box annotation for the use
case where the background is homogeneous and the object's label is provided by
a human. We compare an adapted version of Faster R-CNN and the Scaled Yolov4-p5
architecture and show that both can be trained to distinguish unknown objects
from a complex but homogeneous background using only a small amount of training
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Needle In A Haystack, Fast: Benchmarking Image Perceptual Similarity Metrics At Scale. (arXiv:2206.00282v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00282">
<div class="article-summary-box-inner">
<span><p>The advent of the internet, followed shortly by the social media made it
ubiquitous in consuming and sharing information between anyone with access to
it. The evolution in the consumption of media driven by this change, led to the
emergence of images as means to express oneself, convey information and
convince others efficiently. With computer vision algorithms progressing
radically over the last decade, it is become easier and easier to study at
scale the role of images in the flow of information online. While the research
questions and overall pipelines differ radically, almost all start with a
crucial first step - evaluation of global perceptual similarity between
different images. That initial step is crucial for overall pipeline performance
and processes most images. A number of algorithms are available and currently
used to perform it, but so far no comprehensive review was available to guide
the choice of researchers as to the choice of an algorithm best suited to their
question, assumptions and computational resources. With this paper we aim to
fill this gap, showing that classical computer vision methods are not
necessarily the best approach, whereas a pair of relatively little used methods
- Dhash perceptual hash and SimCLR v2 ResNets achieve excellent performance,
scale well and are computationally efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Multi-Purpose Cross-Attention Based Image Alignment Block for Edge Devices. (arXiv:2206.00291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00291">
<div class="article-summary-box-inner">
<span><p>Image alignment, also known as image registration, is a critical block used
in many computer vision problems. One of the key factors in alignment is
efficiency, as inefficient aligners can cause significant overhead to the
overall problem. In the literature, there are some blocks that appear to do the
alignment operation, although most do not focus on efficiency. Therefore, an
image alignment block which can both work in time and/or space and can work on
edge devices would be beneficial for almost all networks dealing with multiple
images. Given its wide usage and importance, we propose an efficient,
cross-attention-based, multi-purpose image alignment block (XABA) suitable to
work within edge devices. Using cross-attention, we exploit the relationships
between features extracted from images. To make cross-attention feasible for
real-time image alignment problems and handle large motions, we provide a
pyramidal block based cross-attention scheme. This also captures local
relationships besides reducing memory requirements and number of operations.
Efficient XABA models achieve real-time requirements of running above 20 FPS
performance on NVIDIA Jetson Xavier with 30W power consumption compared to
other powerful computers. Used as a sub-block in a larger network, XABA also
improves multi-image super-resolution network performance in comparison to
other alignment methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised Denoising of Diffusion-Weighted Magnetic Resonance Images Using a Convolutional Neural Network and Transfer Learning. (arXiv:2206.00305v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00305">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a method for denoising diffusion-weighted images
(DWI) of the brain using a convolutional neural network trained on realistic,
synthetic MR data. We compare our results to averaging of repeated scans, a
widespread method used in clinics to improve signal-to-noise ratio of MR
images. To obtain training data for transfer learning, we model, in a
data-driven fashion, the effects of echo-planar imaging (EPI): Nyquist ghosting
and ramp sampling. We introduce these effects to the digital phantom of brain
anatomy (BrainWeb). Instead of simulating pseudo-random noise with a defined
probability distribution, we perform noise scans with a brain-DWI-designed
protocol to obtain realistic noise maps. We combine them with the simulated,
noise-free EPI images. We also measure the Point Spread Function in a DW image
of an AJR-approved geometrical phantom and inter-scan movement in a brain scan
of a healthy volunteer. Their influence on image denoising and averaging of
repeated images is investigated at different signal-to-noise ratio levels.
Denoising performance is evaluated quantitatively using the simulated EPI
images and qualitatively in real EPI DWI of the brain. We show that the
application of our method allows for a significant reduction in scan time by
lowering the number of repeated scans. Visual comparisons made in the acquired
brain images indicate that the denoised single-repetition images are less noisy
than multi-repetition averaged images. We also analyse the convolutional neural
network denoiser and point out the challenges accompanying this denoising
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-Efficient Online Continual Object Detection in Streaming Video. (arXiv:2206.00309v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00309">
<div class="article-summary-box-inner">
<span><p>To thrive in evolving environments, humans are capable of continual
acquisition and transfer of new knowledge, from a continuous video stream, with
minimal supervisions, while retaining previously learnt experiences. In
contrast to human learning, most standard continual learning benchmarks focus
on learning from static iid images in fully supervised settings. Here, we
examine a more realistic and challenging
problem$\unicode{x2014}$Label-Efficient Online Continual Object Detection
(LEOCOD) in video streams. By addressing this problem, it would greatly benefit
many real-world applications with reduced annotation costs and retraining time.
To tackle this problem, we seek inspirations from complementary learning
systems (CLS) in human brains and propose a computational model, dubbed as
Efficient-CLS. Functionally correlated with the hippocampus and the neocortex
in CLS, Efficient-CLS posits a memory encoding mechanism involving
bidirectional interaction between fast and slow learners via synaptic weight
transfers and pattern replays. We test Efficient-CLS and competitive baselines
in two challenging real-world video stream datasets. Like humans, Efficient-CLS
learns to detect new object classes incrementally from a continuous temporal
stream of non-repeating video with minimal forgetting. Remarkably, with only
25% annotated video frames, our Efficient-CLS still leads among all comparative
models, which are trained with 100% annotations on all video frames. The data
and source code will be publicly available at
https://github.com/showlab/Efficient-CLS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining. (arXiv:2206.00311v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00311">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a model pretraining technique, named MaskOCR, for
text recognition. Our text recognition architecture is an encoder-decoder
transformer: the encoder extracts the patch-level representations, and the
decoder recognizes the text from the representations. Our approach pretrains
both the encoder and the decoder in a sequential manner. (i) We pretrain the
encoder in a self-supervised manner over a large set of unlabeled real text
images. We adopt the masked image modeling approach, which shows the
effectiveness for general images, expecting that the representations take on
semantics. (ii) We pretrain the decoder over a large set of synthesized text
images in a supervised manner and enhance the language modeling capability of
the decoder by randomly masking some text image patches occupied by characters
input to the encoder and accordingly the representations input to the decoder.
Experiments show that the proposed MaskOCR approach achieves superior results
on the benchmark datasets, including Chinese and English text images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CellCentroidFormer: Combining Self-attention and Convolution for Cell Detection. (arXiv:2206.00338v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00338">
<div class="article-summary-box-inner">
<span><p>Cell detection in microscopy images is important to study how cells move and
interact with their environment. Most recent deep learning-based methods for
cell detection use convolutional neural networks (CNNs). However, inspired by
the success in other computer vision applications, vision transformers (ViTs)
are also used for this purpose. We propose a novel hybrid CNN-ViT model for
cell detection in microscopy images to exploit the advantages of both types of
deep learning models. We employ an efficient CNN, that was pre-trained on the
ImageNet dataset, to extract image features and utilize transfer learning to
reduce the amount of required training data. Extracted image features are
further processed by a combination of convolutional and transformer layers, so
that the convolutional layers can focus on local information and the
transformer layers on global information. Our centroid-based cell detection
method represents cells as ellipses and is end-to-end trainable. Furthermore,
we show that our proposed model can outperform a fully convolutional baseline
model on four different 2D microscopy datasets. Code is available at:
https://github.com/roydenwa/cell-centroid-former
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards view-invariant vehicle speed detection from driving simulator images. (arXiv:2206.00343v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00343">
<div class="article-summary-box-inner">
<span><p>The use of cameras for vehicle speed measurement is much more cost effective
compared to other technologies such as inductive loops, radar or laser.
However, accurate speed measurement remains a challenge due to the inherent
limitations of cameras to provide accurate range estimates. In addition,
classical vision-based methods are very sensitive to extrinsic calibration
between the camera and the road. In this context, the use of data-driven
approaches appears as an interesting alternative. However, data collection
requires a complex and costly setup to record videos under real traffic
conditions from the camera synchronized with a high-precision speed sensor to
generate the ground truth speed values. It has recently been demonstrated that
the use of driving simulators (e.g., CARLA) can serve as a robust alternative
for generating large synthetic datasets to enable the application of deep
learning techniques for vehicle speed estimation for a single camera. In this
paper, we study the same problem using multiple cameras in different virtual
locations and with different extrinsic parameters. We address the question of
whether complex 3D-CNN architectures are capable of implicitly learning
view-invariant speeds using a single model, or whether view-specific models are
more appropriate. The results are very promising as they show that a single
model with data from multiple views reports even better accuracy than
camera-specific models, paving the way towards a view-invariant vehicle speed
measurement system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning as a Means To Reduce the Need for Labeled Data in Medical Image Analysis. (arXiv:2206.00344v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00344">
<div class="article-summary-box-inner">
<span><p>One of the largest problems in medical image processing is the lack of
annotated data. Labeling medical images often requires highly trained experts
and can be a time-consuming process. In this paper, we evaluate a method of
reducing the need for labeled data in medical image object detection by using
self-supervised neural network pretraining. We use a dataset of chest X-ray
images with bounding box labels for 13 different classes of anomalies. The
networks are pretrained on a percentage of the dataset without labels and then
fine-tuned on the rest of the dataset. We show that it is possible to achieve
similar performance to a fully supervised model in terms of mean average
precision and accuracy with only 60\% of the labeled data. We also show that it
is possible to increase the maximum performance of a fully-supervised model by
adding a self-supervised pretraining step, and this effect can be observed with
even a small amount of unlabeled data for pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Deep Learning for Skin Lesion Segmentation. (arXiv:2206.00356v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00356">
<div class="article-summary-box-inner">
<span><p>Skin cancer is a major public health problem that could benefit from
computer-aided diagnosis to reduce the burden of this common disease. Skin
lesion segmentation from images is an important step toward achieving this
goal. However, the presence of natural and artificial artifacts (e.g., hair and
air bubbles), intrinsic factors (e.g., lesion shape and contrast), and
variations in image acquisition conditions make skin lesion segmentation a
challenging task. Recently, various researchers have explored the applicability
of deep learning models to skin lesion segmentation. In this survey, we
cross-examine 134 research papers that deal with deep learning based
segmentation of skin lesions. We analyze these works along several dimensions,
including input data (datasets, preprocessing, and synthetic data generation),
model design (architecture, modules, and losses), and evaluation aspects (data
annotation requirements and segmentation performance). We discuss these
dimensions both from the viewpoint of select seminal works, and from a
systematic viewpoint, examining how those choices have influenced current
trends, and how their limitations should be addressed. We summarize all
examined works in a comprehensive table to facilitate comparisons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepCluE: Enhanced Image Clustering via Multi-layer Ensembles in Deep Neural Networks. (arXiv:2206.00359v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00359">
<div class="article-summary-box-inner">
<span><p>Deep clustering has recently emerged as a promising technique for complex
image clustering. Despite the significant progress, previous deep clustering
works mostly tend to construct the final clustering by utilizing a single layer
of representation, e.g., by performing $K$-means on the last fully-connected
layer or by associating some clustering loss to a specific layer. However, few
of them have considered the possibilities and potential benefits of jointly
leveraging multi-layer representations for enhancing the deep clustering
performance. In light of this, this paper presents a Deep Clustering via
Ensembles (DeepCluE) approach, which bridges the gap between deep clustering
and ensemble clustering by harnessing the power of multiple layers in deep
neural networks. Particularly, we utilize a weight-sharing convolutional neural
network as the backbone, which is trained with both the instance-level
contrastive learning (via an instance projector) and the cluster-level
contrastive learning (via a cluster projector) in an unsupervised manner.
Thereafter, multiple layers of feature representations are extracted from the
trained network, upon which a set of diversified base clusterings can be
generated via a highly efficient clusterer. Then, the reliability of the
clusters in multiple base clusterings is automatically estimated by exploiting
an entropy-based criterion, based on which the multiple base clusterings are
further formulated into a weighted-cluster bipartite graph. By partitioning
this bipartite graph via transfer cut, the final image clustering result can
therefore be obtained. Experimental results on six image datasets confirm the
advantages of our DeepCluE approach over the state-of-the-art deep clustering
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Elucidating the Design Space of Diffusion-Based Generative Models. (arXiv:2206.00364v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00364">
<div class="article-summary-box-inner">
<span><p>We argue that the theory and practice of diffusion-based generative models
are currently unnecessarily convoluted and seek to remedy the situation by
presenting a design space that clearly separates the concrete design choices.
This lets us identify several changes to both the sampling and training
processes, as well as preconditioning of the score networks. Together, our
improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a
class-conditional setting and 1.97 in an unconditional setting, with much
faster sampling (35 network evaluations per image) than prior designs. To
further demonstrate their modular nature, we show that our design changes
dramatically improve both the efficiency and quality obtainable with
pre-trained score networks from previous work, including improving the FID of
an existing ImageNet-64 model from 2.07 to near-SOTA 1.55.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Strongly Augmented Contrastive Clustering. (arXiv:2206.00380v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00380">
<div class="article-summary-box-inner">
<span><p>Deep clustering has attracted increasing attention in recent years due to its
capability of joint representation learning and clustering via deep neural
networks. In its latest developments, the contrastive learning has emerged as
an effective technique to substantially enhance the deep clustering
performance. However, the existing contrastive learning based deep clustering
algorithms mostly focus on some carefully-designed augmentations (often with
limited transformations to preserve the structure), referred to as weak
augmentations, but cannot go beyond the weak augmentations to explore the more
opportunities in stronger augmentations (with more aggressive transformations
or even severe distortions). In this paper, we present an end-to-end deep
clustering approach termed strongly augmented contrastive clustering (SACC),
which extends the conventional two-augmentation-view paradigm to multiple views
and jointly leverages strong and weak augmentations for strengthened deep
clustering. Particularly, we utilize a backbone network with triply-shared
weights, where a strongly augmented view and two weakly augmented views are
incorporated. Based on the representations produced by the backbone, the
weak-weak view pair and the strong-weak view pairs are simultaneously exploited
for the instance-level contrastive learning (via an instance projector) and the
cluster-level contrastive learning (via a cluster projector), which, together
with the backbone, can be jointly optimized in a purely unsupervised manner.
Experimental results on five challenging image datasets have shown the superior
performance of the proposed SACC approach over the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generalized Supervised Contrastive Learning Framework. (arXiv:2206.00384v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00384">
<div class="article-summary-box-inner">
<span><p>Based on recent remarkable achievements of contrastive learning in
self-supervised representation learning, supervised contrastive learning
(SupCon) has successfully extended the batch contrastive approaches to the
supervised context and outperformed cross-entropy on various datasets on
ResNet. In this work, we present GenSCL: a generalized supervised contrastive
learning framework that seamlessly adapts modern image-based regularizations
(such as Mixup-Cutmix) and knowledge distillation (KD) to SupCon by our
generalized supervised contrastive loss. Generalized supervised contrastive
loss is a further extension of supervised contrastive loss measuring
cross-entropy between the similarity of labels and that of latent features.
Then a model can learn to what extent contrastives should be pulled closer to
an anchor in the latent space. By explicitly and fully leveraging label
information, GenSCL breaks the boundary between conventional positives and
negatives, and any kind of pre-trained teacher classifier can be utilized.
ResNet-50 trained in GenSCL with Mixup-Cutmix and KD achieves state-of-the-art
accuracies of 97.6% and 84.7% on CIFAR10 and CIFAR100 without external data,
which significantly improves the results reported in the original SupCon (1.6%
and 8.2%, respectively). Pytorch implementation is available at
https://t.ly/yuUO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiVAE: Photorealistic Images Synthesis with Denoising Diffusion Decoder. (arXiv:2206.00386v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00386">
<div class="article-summary-box-inner">
<span><p>Recently most successful image synthesis models are multi stage process to
combine the advantages of different methods, which always includes a VAE-like
model for faithfully reconstructing embedding to image and a prior model to
generate image embedding. At the same time, diffusion models have shown be
capacity to generate high-quality synthetic images. Our work proposes a VQ-VAE
architecture model with a diffusion decoder (DiVAE) to work as the
reconstructing component in image synthesis. We explore how to input image
embedding into diffusion model for excellent performance and find that simple
modification on diffusion's UNet can achieve it. Training on ImageNet, Our
model achieves state-of-the-art results and generates more photorealistic
images specifically. In addition, we apply the DiVAE with an Auto-regressive
generator on conditional synthesis tasks to perform more human-feeling and
detailed samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comparative study between vision transformers and CNNs in digital pathology. (arXiv:2206.00389v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00389">
<div class="article-summary-box-inner">
<span><p>Recently, vision transformers were shown to be capable of outperforming
convolutional neural networks when pretrained on sufficient amounts of data. In
comparison to convolutional neural networks, vision transformers have a weaker
inductive bias and therefore allow a more flexible feature detection. Due to
their promising feature detection, this work explores vision transformers for
tumor detection in digital pathology whole slide images in four tissue types,
and for tissue type identification. We compared the patch-wise classification
performance of the vision transformer DeiT-Tiny to the state-of-the-art
convolutional neural network ResNet18. Due to the sparse availability of
annotated whole slide images, we further compared both models pretrained on
large amounts of unlabeled whole-slide images using state-of-the-art
self-supervised approaches. The results show that the vision transformer
performed slightly better than the ResNet18 for three of four tissue types for
tumor detection while the ResNet18 performed slightly better for the remaining
tasks. The aggregated predictions of both models on slide level were
correlated, indicating that the models captured similar imaging features. All
together, the vision transformer models performed on par with the ResNet18
while requiring more effort to train. In order to surpass the performance of
convolutional neural networks, vision transformers might require more
challenging tasks to benefit from their weak inductive bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Generalisable Audio Representations for Audio-Visual Navigation. (arXiv:2206.00393v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00393">
<div class="article-summary-box-inner">
<span><p>In audio-visual navigation (AVN), an intelligent agent needs to navigate to a
constantly sound-making object in complex 3D environments based on its audio
and visual perceptions. While existing methods attempt to improve the
navigation performance with preciously designed path planning or intricate task
settings, none has improved the model generalisation on unheard sounds with
task settings unchanged. We thus propose a contrastive learning-based method to
tackle this challenge by regularising the audio encoder, where the
sound-agnostic goal-driven latent representations can be learnt from various
audio signals of different classes. In addition, we consider two data
augmentation strategies to enrich the training sounds. We demonstrate that our
designs can be easily equipped to existing AVN frameworks to obtain an
immediate performance gain (13.4%$\uparrow$ in SPL on Replica and
12.2%$\uparrow$ in SPL on MP3D). Our project is available at
https://AV-GeN.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Invariant Visual Representations for Compositional Zero-Shot Learning. (arXiv:2206.00415v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00415">
<div class="article-summary-box-inner">
<span><p>Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions
using knowledge learned from seen attribute-object compositions in the training
set. Previous works mainly project an image and a composition into a common
embedding space to measure their compatibility score. However, both attributes
and objects share the visual representations learned above, leading the model
to exploit spurious correlations and bias towards seen pairs. Instead, we
reconsider CZSL as an out-of-distribution generalization problem. If an object
is treated as a domain, we can learn object-invariant features to recognize the
attributes attached to any object reliably. Similarly, attribute-invariant
features can also be learned when recognizing the objects with attributes as
domains. Specifically, we propose an invariant feature learning framework to
align different domains at the representation and gradient levels to capture
the intrinsic characteristics associated with the tasks. Experiments on two
CZSL benchmarks demonstrate that the proposed method significantly outperforms
the previous state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Gaussian Grasp Maps for Generative Grasping Models. (arXiv:2206.00432v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00432">
<div class="article-summary-box-inner">
<span><p>Generalising robotic grasping to previously unseen objects is a key task in
general robotic manipulation. The current method for training many antipodal
generative grasping models rely on a binary ground truth grasp map generated
from the centre thirds of correctly labelled grasp rectangles. However, these
binary maps do not accurately reflect the positions in which a robotic arm can
correctly grasp a given object. We propose a continuous Gaussian representation
of annotated grasps to generate ground truth training data which achieves a
higher success rate on a simulated robotic grasping benchmark. Three modern
generative grasping networks are trained with either binary or Gaussian grasp
maps, along with recent advancements from the robotic grasping literature, such
as discretisation of grasp angles into bins and an attentional loss function.
Despite negligible difference according to the standard rectangle metric,
Gaussian maps better reproduce the training data and therefore improve success
rates when tested on the same simulated robot arm by avoiding collisions with
the object: achieving 87.94\% accuracy. Furthermore, the best performing model
is shown to operate with a high success rate when transferred to a real robotic
arm, at high inference speeds, without the need for transfer learning. The
system is then shown to be capable of performing grasps on an antagonistic
physical object dataset benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CD$^2$: Fine-grained 3D Mesh Reconstruction with Twice Chamfer Distance. (arXiv:2206.00447v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00447">
<div class="article-summary-box-inner">
<span><p>Monocular 3D reconstruction is to reconstruct the shape of object and its
other detailed information from a single RGB image. In 3D reconstruction,
polygon mesh is the most prevalent expression form obtained from deep learning
models, with detailed surface information and low computational cost. However,
some state-of-the-art works fail to generate well-structured meshes, these
meshes have two severe problems which we call Vertices Clustering and Illegal
Twist. By delving into the mesh deformation procedure, we pinpoint the
inadequate usage of Chamfer Distance(CD) metric in deep learning model. In this
paper, we initially demonstrate the problems resulting from CD with visual
examples and quantitative analyses. To solve these problems, we propose a
fine-grained reconstruction method CD$^2$ with Chamfer distance adopted twice
to perform a plausible and adaptive deformation. Extensive experiments on two
3D datasets and the comparison of our newly proposed mesh quality metrics
demonstrate that our CD$^2$ outperforms others by generating better-structured
meshes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A robust and lightweight deep attention multiple instance learning algorithm for predicting genetic alterations. (arXiv:2206.00455v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00455">
<div class="article-summary-box-inner">
<span><p>Deep-learning models based on whole-slide digital pathology images (WSIs)
become increasingly popular for predicting molecular biomarkers. Instance-based
models has been the mainstream strategy for predicting genetic alterations
using WSIs although bag-based models along with self-attention mechanism-based
algorithms have been proposed for other digital pathology applications. In this
paper, we proposed a novel Attention-based Multiple Instance Mutation Learning
(AMIML) model for predicting gene mutations. AMIML was comprised of successive
1-D convolutional layers, a decoder, and a residual weight connection to
facilitate further integration of a lightweight attention mechanism to detect
the most predictive image patches. Using data for 24 clinically relevant genes
from four cancer cohorts in The Cancer Genome Atlas (TCGA) studies (UCEC, BRCA,
GBM and KIRC), we compared AMIML with one popular instance-based model and four
recently published bag-based models (e.g., CHOWDER, HE2RNA, etc.). AMIML
demonstrated excellent robustness, not only outperforming all the five baseline
algorithms in the vast majority of the tested genes (17 out of 24), but also
providing near-best-performance for the other seven genes. Conversely, the
performance of the baseline published algorithms varied across different
cancers/genes. In addition, compared to the published models for genetic
alterations, AMIML provided a significant improvement for predicting a wide
range of genes (e.g., KMT2C, TP53, and SETD2 for KIRC; ERBB2, BRCA1, and BRCA2
for BRCA; JAK1, POLE, and MTOR for UCEC) as well as produced outstanding
predictive models for other clinically relevant gene mutations, which have not
been reported in the current literature. Furthermore, with the flexible and
interpretable attention-based MIL pooling mechanism, AMIML could further
zero-in and detect predictive image patches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PanopticDepth: A Unified Framework for Depth-aware Panoptic Segmentation. (arXiv:2206.00468v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00468">
<div class="article-summary-box-inner">
<span><p>This paper presents a unified framework for depth-aware panoptic segmentation
(DPS), which aims to reconstruct 3D scene with instance-level semantics from
one single image. Prior works address this problem by simply adding a dense
depth regression head to panoptic segmentation (PS) networks, resulting in two
independent task branches. This neglects the mutually-beneficial relations
between these two tasks, thus failing to exploit handy instance-level semantic
cues to boost depth accuracy while also producing sub-optimal depth maps. To
overcome these limitations, we propose a unified framework for the DPS task by
applying a dynamic convolution technique to both the PS and depth prediction
tasks. Specifically, instead of predicting depth for all pixels at a time, we
generate instance-specific kernels to predict depth and segmentation masks for
each instance. Moreover, leveraging the instance-wise depth estimation scheme,
we add additional instance-level depth cues to assist with supervising the
depth learning via a new depth loss. Extensive experiments on Cityscapes-DPS
and SemKITTI-DPS show the effectiveness and promise of our method. We hope our
unified solution to DPS can lead a new paradigm in this area. Code is available
at https://github.com/NaiyuGao/PanopticDepth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Principal Component Learning: Modeling Similarity by Augmentation Overlap. (arXiv:2206.00471v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00471">
<div class="article-summary-box-inner">
<span><p>Traditional self-supervised contrastive learning methods learn embeddings by
pulling views of the same sample together and pushing views of different
samples away. Since views of a sample are usually generated via data
augmentations, the semantic relationship between samples is ignored. Based on
the observation that semantically similar samples are more likely to have
similar augmentations, we propose to measure similarity via the distribution of
augmentations, i.e., how much the augmentations of two samples overlap. To
handle the dimensional and computational complexity, we propose a novel
Contrastive Principal Component Learning (CPCL) method composed of a
contrastive-like loss and an on-the-fly projection loss to efficiently perform
PCA on the augmentation feature, which encodes the augmentation distribution.
By CPCL, the learned low-dimensional embeddings theoretically preserve the
similarity of augmentation distribution between samples. Empirical results show
our method can achieve competitive results against various traditional
contrastive learning methods on different benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer. (arXiv:2206.00481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00481">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViTs) enabled the use of transformer architecture on
vision tasks showing impressive performances when trained on big datasets.
However, on relatively small datasets, ViTs are less accurate given their lack
of inductive bias. To this end, we propose a simple but still effective
self-supervised learning (SSL) strategy to train ViTs, that without any
external annotation, can significantly improve the results. Specifically, we
define a set of SSL tasks based on relations of image patches that the model
has to solve before or jointly during the downstream training. Differently from
ViT, our RelViT model optimizes all the output tokens of the transformer
encoder that are related to the image patches, thus exploiting more training
signal at each training step. We investigated our proposed methods on several
image benchmarks finding that RelViT improves the SSL state-of-the-art methods
by a large margin, especially on small datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attack-Agnostic Adversarial Detection. (arXiv:2206.00489v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00489">
<div class="article-summary-box-inner">
<span><p>The growing number of adversarial attacks in recent years gives attackers an
advantage over defenders, as defenders must train detectors after knowing the
types of attacks, and many models need to be maintained to ensure good
performance in detecting any upcoming attacks. We propose a way to end the
tug-of-war between attackers and defenders by treating adversarial attack
detection as an anomaly detection problem so that the detector is agnostic to
the attack. We quantify the statistical deviation caused by adversarial
perturbations in two aspects. The Least Significant Component Feature (LSCF)
quantifies the deviation of adversarial examples from the statistics of benign
samples and Hessian Feature (HF) reflects how adversarial examples distort the
landscape of the model's optima by measuring the local loss curvature.
Empirical results show that our method can achieve an overall ROC AUC of 94.9%,
89.7%, and 94.6% on CIFAR10, CIFAR100, and SVHN, respectively, and has
comparable performance to adversarial detectors trained with adversarial
examples on most of the attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Room Wireframe Detection from a Single View. (arXiv:2206.00491v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00491">
<div class="article-summary-box-inner">
<span><p>Reconstruction of indoor surfaces with limited texture information or with
repeated textures, a situation common in walls and ceilings, may be difficult
with a monocular Structure from Motion system. We propose a Semantic Room
Wireframe Detection task to predict a Semantic Wireframe from a single
perspective image. Such predictions may be used with shape priors to estimate
the Room Layout and aid reconstruction. To train and test the proposed
algorithm we create a new set of annotations from the simulated Structured3D
dataset. We show qualitatively that the SRW-Net handles complex room geometries
better than previous Room Layout Estimation algorithms while quantitatively
out-performing the baseline in non-semantic Wireframe Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proximally Sensitive Error for Anomaly Detection and Feature Learning. (arXiv:2206.00506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00506">
<div class="article-summary-box-inner">
<span><p>Mean squared error (MSE) is one of the most widely used metrics to expression
differences between multi-dimensional entities, including images. However, MSE
is not locally sensitive as it does not take into account the spatial
arrangement of the (pixel) differences, which matters for structured data types
like images. Such spatial arrangements carry information about the source of
the differences; therefore, an error function that also incorporates the
location of errors can lead to a more meaningful distance measure. We introduce
Proximally Sensitive Error (PSE), through which we suggest that a regional
emphasis in the error measure can 'highlight' semantic differences between
images over syntactic/random deviations. We demonstrate that this emphasis can
be leveraged upon for the task of anomaly/occlusion detection. We further
explore its utility as a loss function to help a model focus on learning
representations of semantic objects instead of minimizing syntactic
reconstruction noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Landslide4Sense: Reference Benchmark Data and Deep Learning Models for Landslide Detection. (arXiv:2206.00515v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00515">
<div class="article-summary-box-inner">
<span><p>This study introduces \textit{Landslide4Sense}, a reference benchmark for
landslide detection from remote sensing. The repository features 3,799 image
patches fusing optical layers from Sentinel-2 sensors with the digital
elevation model and slope layer derived from ALOS PALSAR. The added
topographical information facilitates an accurate detection of landslide
borders, which recent researches have shown to be challenging using optical
data alone. The extensive data set supports deep learning (DL) studies in
landslide detection and the development and validation of methods for the
systematic update of landslide inventories. The benchmark data set has been
collected at four different times and geographical locations: Iburi (September
2018), Kodagu (August 2018), Gorkha (April 2015), and Taiwan (August 2009).
Each image pixel is labelled as belonging to a landslide or not, incorporating
various sources and thorough manual annotation. We then evaluate the landslide
detection performance of 11 state-of-the-art DL segmentation models: U-Net,
ResU-Net, PSPNet, ContextNet, DeepLab-v2, DeepLab-v3+, FCN-8s, LinkNet, FRRN-A,
FRRN-B, and SQNet. All models were trained from scratch on patches from one
quarter of each study area and tested on independent patches from the other
three quarters. Our experiments demonstrate that ResU-Net outperformed the
other models for the landslide detection task. We make the multi-source
landslide benchmark data (Landslide4Sense) and the tested DL models publicly
available at \url{www.landslide4sense.org}, establishing an important resource
for remote sensing, computer vision, and machine learning communities in
studies of image classification in general and applications to landslide
detection in particular.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Amodal Cityscapes: A New Dataset, its Generation, and an Amodal Semantic Segmentation Challenge Baseline. (arXiv:2206.00527v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00527">
<div class="article-summary-box-inner">
<span><p>Amodal perception terms the ability of humans to imagine the entire shapes of
occluded objects. This gives humans an advantage to keep track of everything
that is going on, especially in crowded situations. Typical perception
functions, however, lack amodal perception abilities and are therefore at a
disadvantage in situations with occlusions. Complex urban driving scenarios
often experience many different types of occlusions and, therefore, amodal
perception for automated vehicles is an important task to investigate. In this
paper, we consider the task of amodal semantic segmentation and propose a
generic way to generate datasets to train amodal semantic segmentation methods.
We use this approach to generate an amodal Cityscapes dataset. Moreover, we
propose and evaluate a method as baseline on Amodal Cityscapes, showing its
applicability for amodal semantic segmentation in automotive environment
perception. We provide the means to re-generate this dataset on github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines. (arXiv:2206.00535v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00535">
<div class="article-summary-box-inner">
<span><p>Deepfakes pose a serious threat to our digital society by fueling the spread
of misinformation. It is essential to develop techniques that both detect them,
and effectively alert the human user to their presence. Here, we introduce a
novel deepfake detection framework that meets both of these needs. Our approach
learns to generate attention maps of video artifacts, semi-supervised on human
annotations. These maps make two contributions. First, they improve the
accuracy and generalizability of a deepfake classifier, demonstrated across
several deepfake detection datasets. Second, they allow us to generate an
intuitive signal for the human user, in the form of "Deepfake Caricatures":
transformations of the original deepfake video where attended artifacts are
exacerbated to improve human recognition. Our approach, based on a mixture of
human and artificial supervision, aims to further the development of
countermeasures against fake visual content, and grants humans the ability to
make their own judgment when presented with dubious visual media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of loss function in Deep Learning methods for accurate retinal vessel segmentation. (arXiv:2206.00536v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00536">
<div class="article-summary-box-inner">
<span><p>The retinal vessel network studied through fundus images contributes to the
diagnosis of multiple diseases not only found in the eye. The segmentation of
this system may help the specialized task of analyzing these images by
assisting in the quantification of morphological characteristics. Due to its
relevance, several Deep Learning-based architectures have been tested for
tackling this problem automatically. However, the impact of loss function
selection on the segmentation of the intricate retinal blood vessel system
hasn't been systematically evaluated. In this work, we present the comparison
of the loss functions Binary Cross Entropy, Dice, Tversky, and Combo loss using
the deep learning architectures (i.e. U-Net, Attention U-Net, and Nested UNet)
with the DRIVE dataset. Their performance is assessed using four metrics: the
AUC, the mean squared error, the dice score, and the Hausdorff distance. The
models were trained with the same number of parameters and epochs. Using dice
score and AUC, the best combination was SA-UNet with Combo loss, which had an
average of 0.9442 and 0.809 respectively. The best average of Hausdorff
distance and mean square error were obtained using the Nested U-Net with the
Dice loss function, which had an average of 6.32 and 0.0241 respectively. The
results showed that there is a significant difference in the selection of loss
function
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Fully Convolutional Transformer for Medical Image Segmentation. (arXiv:2206.00566v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00566">
<div class="article-summary-box-inner">
<span><p>We propose a novel transformer model, capable of segmenting medical images of
varying modalities. Challenges posed by the fine grained nature of medical
image analysis mean that the adaptation of the transformer for their analysis
is still at nascent stages. The overwhelming success of the UNet lay in its
ability to appreciate the fine-grained nature of the segmentation task, an
ability which existing transformer based models do not currently posses. To
address this shortcoming, we propose The Fully Convolutional Transformer (FCT),
which builds on the proven ability of Convolutional Neural Networks to learn
effective image representations, and combines them with the ability of
Transformers to effectively capture long-term dependencies in its inputs. The
FCT is the first fully convolutional Transformer model in medical imaging
literature. It processes its input in two stages, where first, it learns to
extract long range semantic dependencies from the input image, and then learns
to capture hierarchical global attributes from the features. FCT is compact,
accurate and robust. Our results show that it outperforms all existing
transformer architectures by large margins across multiple medical image
segmentation datasets of varying data modalities without the need for any
pre-training. FCT outperforms its immediate competitor on the ACDC dataset by
1.3%, on the Synapse dataset by 4.4%, on the Spleen dataset by 1.2% and on ISIC
2017 dataset by 1.1% on the dice metric, with up to five times fewer
parameters. Our code, environments and models will be available via GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dog nose print matching with dual global descriptor based on Contrastive Learning. (arXiv:2206.00580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00580">
<div class="article-summary-box-inner">
<span><p>Recent studies in biometric-based identification tasks have shown that deep
learning methods can achieve better performance. These methods generally
extract the global features as descriptor to represent the original image.
Nonetheless, it does not perform well for biometric identification under
fine-grained tasks. The main reason is that the single image descriptor
contains insufficient information to represent image. In this paper, we present
a dual global descriptor model, which combines multiple global descriptors to
exploit multi level image features. Moreover, we utilize a contrastive loss to
enlarge the distance between image representations of confusing classes. The
proposed framework achieves the top2 on the CVPR2022 Biometrics Workshop Pet
Biometric Challenge. The source code and trained models are publicly available
at: https://github.com/flyingsheepbin/pet-biometrics
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Higher-Order Attention Networks. (arXiv:2206.00606v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00606">
<div class="article-summary-box-inner">
<span><p>This paper introduces higher-order attention networks (HOANs), a novel class
of attention-based neural networks defined on a generalized higher-order domain
called a combinatorial complex (CC). Similar to hypergraphs, CCs admit
arbitrary set-like relations between a collection of abstract entities.
Simultaneously, CCs permit the construction of hierarchical higher-order
relations analogous to those supported by cell complexes. Thus, CCs effectively
generalize both hypergraphs and cell complexes and combine their desirable
characteristics. By exploiting the rich combinatorial nature of CCs, HOANs
define a new class of message-passing attention-based networks that unifies
higher-order neural networks. Our evaluation on tasks related to mesh shape
analysis and graph learning demonstrates that HOANs attain competitive, and in
some examples superior, predictive performance in comparison to
state-of-the-art neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Choice of Data for Efficient Training and Validation of End-to-End Driving Models. (arXiv:2206.00608v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00608">
<div class="article-summary-box-inner">
<span><p>The emergence of data-driven machine learning (ML) has facilitated
significant progress in many complicated tasks such as highly-automated
driving. While much effort is put into improving the ML models and learning
algorithms in such applications, little focus is put into how the training data
and/or validation setting should be designed. In this paper we investigate the
influence of several data design choices regarding training and validation of
deep driving models trainable in an end-to-end fashion. Specifically, (i) we
investigate how the amount of training data influences the final driving
performance, and which performance limitations are induced through currently
used mechanisms to generate training data. (ii) Further, we show by correlation
analysis, which validation design enables the driving performance measured
during validation to generalize well to unknown test environments. (iii)
Finally, we investigate the effect of random seeding and non-determinism,
giving insights which reported improvements can be deemed significant. Our
evaluations using the popular CARLA simulator provide recommendations regarding
data generation and driving route selection for an efficient future development
of end-to-end driving models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-stream spatiotemporal networks with feature sharing for monitoring animals in the home cage. (arXiv:2206.00614v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00614">
<div class="article-summary-box-inner">
<span><p>This paper presents a spatiotemporal deep learning approach for mouse
behavioural classification in the home cage. Using a series of dual-stream
architectures with assorted modifications to increase performance, we introduce
a novel feature-sharing approach that jointly processes the streams at regular
intervals throughout the network. Using a publicly available labelled dataset
of singly-housed mice, we achieve a prediction accuracy of 86.47% using an
ensemble of Inception-based networks that utilize feature sharing. We also
demonstrate through ablation studies that for all models, the feature-sharing
architectures consistently perform better than conventional ones having
separate streams. The best performing models were further evaluated on other
activity datasets, both mouse and human, and achieved state-of-the-art results.
Future work will investigate the effectiveness of feature sharing in
behavioural classification in the unsupervised anomaly detection domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. (arXiv:2206.00621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00621">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce Cross-View Language Modeling, a simple and
effective language model pre-training framework that unifies cross-lingual
cross-modal pre-training with shared architectures and objectives. Our approach
is motivated by a key observation that cross-lingual and cross-modal
pre-training share the same goal of aligning two different views of the same
object into a common semantic space. To this end, the cross-view language
modeling framework considers both multi-modal data (i.e., image-caption pairs)
and multi-lingual data (i.e., parallel sentence pairs) as two different views
of the same object, and trains the model to align the two views by maximizing
the mutual information between them with conditional masked language modeling
and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal
Language Model, with the cross-view language modeling framework. Empirical
results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual
image-text retrieval datasets show that while conceptually simpler, CCLM
significantly outperforms the prior state-of-the-art with an average absolute
improvement of over 10%. Notably, CCLM is the first multi-lingual multi-modal
model that surpasses the translate-test performance of representative English
vision-language models by zero-shot cross-lingual transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP4IDC: CLIP for Image Difference Captioning. (arXiv:2206.00629v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00629">
<div class="article-summary-box-inner">
<span><p>Image Difference Captioning (IDC) aims at generating sentences to describe
the differences between two similar-looking images. The conventional approaches
learn captioning models on the offline-extracted visual features and the
learning can not be propagated back to the fixed feature extractors pre-trained
on image classification datasets. Accordingly, potential improvements can be
made by fine-tuning the visual features for: 1) narrowing the gap when
generalizing the visual extractor trained on image classification to IDC, and
2) relating the extracted visual features to the descriptions of the
corresponding changes. We thus propose CLIP4IDC to transfer a CLIP model for
the IDC task to attain these improvements. Different from directly fine-tuning
CLIP to generate sentences, a task-specific domain adaptation is used to
improve the extracted features. Specifically, the target is to train CLIP on
raw pixels to relate the image pairs to the described changes. Afterwards, a
vanilla Transformer is trained for IDC on the features extracted by the vision
encoder of CLIP. Experiments on three IDC benchmark datasets, CLEVR-Change,
Spot-the-Diff and Image-Editing-Request, demonstrate the effectiveness of
CLIP4IDC. Our code and models will be released at
https://github.com/sushizixin/CLIP4IDC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Voxel-based Representation with Transformer for 3D Object Detection. (arXiv:2206.00630v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00630">
<div class="article-summary-box-inner">
<span><p>In this work, we present a unified framework for multi-modality 3D object
detection, named UVTR. The proposed method aims to unify multi-modality
representations in the voxel space for accurate and robust single- or
cross-modality 3D detection. To this end, the modality-specific space is first
designed to represent different inputs in the voxel feature space. Different
from previous work, our approach preserves the voxel space without height
compression to alleviate semantic ambiguity and enable spatial interactions.
Benefit from the unified manner, cross-modality interaction is then proposed to
make full use of inherent properties from different sensors, including
knowledge transfer and modality fusion. In this way, geometry-aware expressions
in point clouds and context-rich features in images are well utilized for
better performance and robustness. The transformer decoder is applied to
efficiently sample features from the unified space with learnable positions,
which facilitates object-level interactions. In general, UVTR presents an early
attempt to represent different modalities in a unified framework. It surpasses
previous work in single- and multi-modality entries and achieves leading
performance in the nuScenes test set with 69.7%, 55.1%, and 71.1% NDS for
LiDAR, camera, and multi-modality inputs, respectively. Code is made available
at https://github.com/dvlab-research/UVTR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extreme Floorplan Reconstruction by Structure-Hallucinating Transformer Cascades. (arXiv:2206.00645v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00645">
<div class="article-summary-box-inner">
<span><p>This paper presents an extreme floorplan reconstruction task, a new benchmark
for the task, and a neural architecture as a solution. Given a partial
floorplan reconstruction inferred or curated from panorama images, the task is
to reconstruct a complete floorplan including invisible architectural
structures. The proposed neural network 1) encodes an input partial floorplan
into a set of latent vectors by convolutional neural networks and a
Transformer; and 2) reconstructs an entire floorplan while hallucinating
invisible rooms and doors by cascading Transformer decoders. Qualitative and
quantitative evaluations demonstrate effectiveness of our approach over the
benchmark of 701 houses, outperforming the state-of-the-art reconstruction
techniques. We will share our code, models, and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction. (arXiv:2206.00665v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00665">
<div class="article-summary-box-inner">
<span><p>In recent years, neural implicit surface reconstruction methods have become
popular for multi-view 3D reconstruction. In contrast to traditional multi-view
stereo methods, these approaches tend to produce smoother and more complete
reconstructions due to the inductive smoothness bias of neural networks.
State-of-the-art neural implicit methods allow for high-quality reconstructions
of simple scenes from many input views. Yet, their performance drops
significantly for larger and more complex scenes and scenes captured from
sparse viewpoints. This is caused primarily by the inherent ambiguity in the
RGB reconstruction loss that does not provide enough constraints, in particular
in less-observed and textureless areas. Motivated by recent advances in the
area of monocular geometry prediction, we systematically explore the utility
these cues provide for improving neural implicit surface reconstruction. We
demonstrate that depth and normal cues, predicted by general-purpose monocular
estimators, significantly improve reconstruction quality and optimization time.
Further, we analyse and investigate multiple design choices for representing
neural implicit surfaces, ranging from monolithic MLP models over single-grid
to multi-resolution grid representations. We observe that geometric monocular
priors improve performance both for small-scale single-object as well as
large-scale multi-object scenes, independent of the choice of representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anchor Pruning for Object Detection. (arXiv:2104.00432v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00432">
<div class="article-summary-box-inner">
<span><p>This paper proposes anchor pruning for object detection in one-stage
anchor-based detectors. While pruning techniques are widely used to reduce the
computational cost of convolutional neural networks, they tend to focus on
optimizing the backbone networks where often most computations are. In this
work we demonstrate an additional pruning technique, specifically for object
detection: anchor pruning. With more efficient backbone networks and a growing
trend of deploying object detectors on embedded systems where post-processing
steps such as non-maximum suppression can be a bottleneck, the impact of the
anchors used in the detection head is becoming increasingly more important. In
this work, we show that many anchors in the object detection head can be
removed without any loss in accuracy. With additional retraining, anchor
pruning can even lead to improved accuracy. Extensive experiments on SSD and MS
COCO show that the detection head can be made up to 44% more efficient while
simultaneously increasing accuracy. Further experiments on RetinaNet and PASCAL
VOC show the general effectiveness of our approach. We also introduce
`overanchorized' models that can be used together with anchor pruning to
eliminate hyperparameters related to the initial shape of anchors. Code and
models are available at https://github.com/Mxbonn/anchor_pruning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutions for Spatial Interaction Modeling. (arXiv:2104.07182v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07182">
<div class="article-summary-box-inner">
<span><p>In many different fields interactions between objects play a critical role in
determining their behavior. Graph neural networks (GNNs) have emerged as a
powerful tool for modeling interactions, although often at the cost of adding
considerable complexity and latency. In this paper, we consider the problem of
spatial interaction modeling in the context of predicting the motion of actors
around autonomous vehicles, and investigate alternative approaches to GNNs. We
revisit convolutions and show that they can demonstrate comparable performance
to graph networks in modeling spatial interactions with lower latency, thus
providing an effective and efficient alternative in time-critical systems.
Moreover, we propose a novel interaction loss to further improve the
interaction modeling of the considered methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FoveaTer: Foveated Transformer for Image Classification. (arXiv:2105.14173v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14173">
<div class="article-summary-box-inner">
<span><p>Many animals and humans process the visual field with a varying spatial
resolution (foveated vision) and use peripheral processing to make eye
movements and point the fovea to acquire high-resolution information about
objects of interest. This architecture results in computationally efficient
rapid scene exploration. Recent progress in self-attention-based vision
Transformers allow global interactions between feature locations and result in
increased robustness to adversarial attacks. However, the Transformer models do
not explicitly model the foveated properties of the visual system nor the
interaction between eye movements and the classification task. We propose
foveated Transformer (FoveaTer) model, which uses pooling regions and eye
movements to perform object classification tasks. Our proposed model pools the
image features using squared pooling regions, an approximation to the
biologically-inspired foveated architecture. It decides on subsequent fixation
locations based on the attention assigned by the Transformer to various
locations from past and present fixations. It dynamically allocates more
fixation/computational resources to more challenging images before making the
final object category decision. We compare FoveaTer against a Full-resolution
baseline model, which does not contain any pooling. On the ImageNet dataset,
the Foveated model with Dynamic-stop achieves an accuracy of $1.9\%$ below the
full-resolution model with a throughput gain of $51\%$. Using a Foveated model
with Dynamic-stop and the Full-resolution model, the ensemble outperforms the
baseline Full-resolution by $0.2\%$ with a throughput gain of $7.7\%$. We also
demonstrate our model's robustness against adversarial attacks. Finally, we
compare the Foveated model to human performance in a scene categorization task
and show similar dependence of accuracy with number of exploratory fixations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization. (arXiv:2109.02934v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02934">
<div class="article-summary-box-inner">
<span><p>Learning robust models that generalize well under changes in the data
distribution is critical for real-world applications. To this end, there has
been a growing surge of interest to learn simultaneously from multiple training
domains - while enforcing different types of invariance across those domains.
Yet, all existing approaches fail to show systematic benefits under controlled
evaluation protocols. In this paper, we introduce a new regularization - named
Fishr - that enforces domain invariance in the space of the gradients of the
loss: specifically, the domain-level variances of gradients are matched across
training domains. Our approach is based on the close relations between the
gradient covariance, the Fisher Information and the Hessian of the loss: in
particular, we show that Fishr eventually aligns the domain-level loss
landscapes locally around the final weights. Extensive experiments demonstrate
the effectiveness of Fishr for out-of-distribution generalization. Notably,
Fishr improves the state of the art on the DomainBed benchmark and performs
consistently better than Empirical Risk Minimization. Our code is available at
https://github.com/alexrame/fishr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08475">
<div class="article-summary-box-inner">
<span><p>Visual dialog, which aims to hold a meaningful conversation with humans about
a given image, is a challenging task that requires models to reason the complex
dependencies among visual content, dialog history, and current questions. Graph
neural networks are recently applied to model the implicit relations between
objects in an image or dialog. However, they neglect the importance of 1)
coreference relations among dialog history and dependency relations between
words for the question representation; and 2) the representation of the image
based on the fully represented question. Therefore, we propose a novel
relation-aware graph-over-graph network (GoG) for visual dialog. Specifically,
GoG consists of three sequential graphs: 1) H-Graph, which aims to capture
coreference relations among dialog history; 2) History-aware Q-Graph, which
aims to fully understand the question through capturing dependency relations
between words based on coreference resolution on the dialog history; and 3)
Question-aware I-Graph, which aims to capture the relations between objects in
an image based on fully question representation. As an additional feature
representation module, we add GoG to the existing visual dialogue model.
Experimental results show that our model outperforms the strong baseline in
both generative and discriminative settings by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiC-Net: Learning Efficient Spatio-Temporal Relation for Text-Video Retrieval. (arXiv:2110.15609v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15609">
<div class="article-summary-box-inner">
<span><p>The task of text-video retrieval aims to understand the correspondence
between language and vision, has gained increasing attention in recent years.
Previous studies either adopt off-the-shelf 2D/3D-CNN and then use average/max
pooling to directly capture spatial features with aggregated temporal
information as global video embeddings, or introduce graph-based models and
expert knowledge to learn local spatial-temporal relations. However, the
existing methods have two limitations: 1) The global video representations
learn video temporal information in a simple average/max pooling manner and do
not fully explore the temporal information between every two frames. 2) The
graph-based local video representations are handcrafted, it depends heavily on
expert knowledge and empirical feedback, which may not be able to effectively
mine the higher-level fine-grained visual relations. These limitations result
in their inability to distinguish videos with the same visual components but
with different relations. To solve this problem, we propose a novel cross-modal
retrieval framework, Bi-Branch Complementary Network (BiC-Net), which modifies
transformer architecture to effectively bridge text-video modalities in a
complementary manner via combining local spatial-temporal relation and global
temporal information. Specifically, local video representations are encoded
using multiple transformer blocks and additional residual blocks to learn
spatio-temporal relation features, calling the module a Spatio-Temporal
Residual transformer (SRT). Meanwhile, Global video representations are encoded
using a multi-layer transformer block to learn global temporal features.
Finally, we align the spatio-temporal relation and global temporal features
with the text feature on two embedding spaces for cross-modal text-video
retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intensity Mapping Functions For HDR Panorama Imaging: Weighted Histogram Averaging. (arXiv:2111.07283v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07283">
<div class="article-summary-box-inner">
<span><p>It is challenging to stitch multiple images with different exposures due to
possible color distortion and loss of details in the brightest and darkest
regions of input images. In this paper, a novel intensity mapping algorithm is
first proposed by introducing a new concept of weighted histogram averaging
(WHA). The proposed WHA algorithm leverages the correspondence between the
histogram bins of two images which are built up by using the non-decreasing
property of the intensity mapping functions (IMFs). The WHA algorithm is then
adopted to synthesize a set of differently exposed panorama images. The
intermediate panorama images are finally fused via a state-of-the-art
multi-scale exposure fusion (MEF) algorithm to produce the final panorama
image. Extensive experiments indicate that the proposed WHA algorithm
significantly surpasses the related state-of-the-art intensity mapping methods.
The proposed high dynamic range (HDR) stitching algorithm also preserves
details in the brightest and darkest regions of the input images well. The
related materials will be publicly accessible at
https://github.com/yilun-xu/WHA for reproducible research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. (arXiv:2111.08276v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08276">
<div class="article-summary-box-inner">
<span><p>Most existing methods in vision language pre-training rely on object-centric
features extracted through object detection and make fine-grained alignments
between the extracted features and texts. It is challenging for these methods
to learn relations among multiple objects. To this end, we propose a new method
called X-VLM to perform `multi-grained vision language pre-training.' The key
to learning multi-grained alignments is to locate visual concepts in the image
given the associated texts, and in the meantime align the texts with the visual
concepts, where the alignments are in multi-granularity. Experimental results
show that X-VLM effectively leverages the learned multi-grained alignments to
many downstream vision language tasks and consistently outperforms
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BA-Net: Bridge Attention for Deep Convolutional Neural Networks. (arXiv:2112.04150v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04150">
<div class="article-summary-box-inner">
<span><p>In recent years, channel attention mechanism has been widely investigated due
to its great potential in improving the performance of deep convolutional
neural networks (CNNs) in many vision tasks. However, in most of the existing
methods, only the output of the adjacent convolution layer is fed into the
attention layer for calculating the channel weights. Information from other
convolution layers has been ignored. With these observations, a simple
strategy, named Bridge Attention Net (BA-Net), is proposed in this paper for
better performance with channel attention mechanisms. The core idea of this
design is to bridge the outputs of the previous convolution layers through skip
connections for channel weights generation. Based on our experiment and theory
analysis, we find that features from previous layers also contribute to the
weights significantly. The Comprehensive evaluation demonstrates that the
proposed approach achieves state-of-the-art(SOTA) performance compared with the
existing methods in accuracy and speed. which shows that Bridge Attention
provides a new perspective on the design of neural network architectures with
great potential in improving performance. The code is available at
https://github.com/zhaoy376/Bridge-Attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forensic Analysis of Synthetically Generated Western Blot Images. (arXiv:2112.08739v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08739">
<div class="article-summary-box-inner">
<span><p>The widespread diffusion of synthetically generated content is a serious
threat that needs urgent countermeasures. As a matter of fact, the generation
of synthetic content is not restricted to multimedia data like videos,
photographs or audio sequences, but covers a significantly vast area that can
include biological images as well, such as western blot and microscopic images.
In this paper, we focus on the detection of synthetically generated western
blot images. These images are largely explored in the biomedical literature and
it has been already shown they can be easily counterfeited with few hopes to
spot manipulations by visual inspection or by using standard forensics
detectors. To overcome the absence of publicly available data for this task, we
create a new dataset comprising more than 14K original western blot images and
24K synthetic western blot images, generated using four different
state-of-the-art generation methods. We investigate different strategies to
detect synthetic western blots, exploring binary classification methods as well
as one-class detectors. In both scenarios, we never exploit synthetic western
blot images at training stage. The achieved results show that synthetically
generated western blot images can be spot with good accuracy, even though the
exploited detectors are not optimized over synthetic versions of these
scientific images. We also test the robustness of the developed detectors
against post-processing operations commonly performed on scientific images,
showing that we can be robust to JPEG compression and that some generative
models are easily recognizable, despite the application of editing might alter
the artifacts they leave.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis. (arXiv:2112.10325v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10325">
<div class="article-summary-box-inner">
<span><p>Due to the constraints of the imaging device and high cost in operation time,
computer tomography (CT) scans are usually acquired with low intra-slice
resolution. Improving the intra-slice resolution is beneficial to the disease
diagnosis for both human experts and computer-aided systems. To this end, this
paper builds a novel medical slice synthesis to increase the between-slice
resolution. Considering that the ground-truth intermediate medical slices are
always absent in clinical practice, we introduce the incremental cross-view
mutual distillation strategy to accomplish this task in the self-supervised
learning manner. Specifically, we model this problem from three different
views: slice-wise interpolation from axial view and pixel-wise interpolation
from coronal and sagittal views. Under this circumstance, the models learned
from different views can distill valuable knowledge to guide the learning
processes of each other. We can repeat this process to make the models
synthesize intermediate slice data with increasing inter-slice resolution. To
demonstrate the effectiveness of the proposed approach, we conduct
comprehensive experiments on a large-scale CT dataset. Quantitative and
qualitative comparison results show that our method outperforms
state-of-the-art algorithms by clear margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2. (arXiv:2112.14683v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14683">
<div class="article-summary-box-inner">
<span><p>Videos show continuous events, yet most $-$ if not all $-$ video synthesis
frameworks treat them discretely in time. In this work, we think of videos of
what they should be $-$ time-continuous signals, and extend the paradigm of
neural representations to build a continuous-time video generator. For this, we
first design continuous motion representations through the lens of positional
embeddings. Then, we explore the question of training on very sparse videos and
demonstrate that a good generator can be learned by using as few as 2 frames
per clip. After that, we rethink the traditional image + video discriminators
pair and design a holistic discriminator that aggregates temporal information
by simply concatenating frames' features. This decreases the training cost and
provides richer learning signal to the generator, making it possible to train
directly on 1024$^2$ videos for the first time. We build our model on top of
StyleGAN2 and it is just ${\approx}5\%$ more expensive to train at the same
resolution while achieving almost the same image quality. Moreover, our latent
space features similar properties, enabling spatial manipulations that our
method can propagate in time. We can generate arbitrarily long videos at
arbitrary high frame rate, while prior work struggles to generate even 64
frames at a fixed rate. Our model is tested on four modern 256$^2$ and one
1024$^2$-resolution video synthesis benchmarks. In terms of sheer metrics, it
performs on average ${\approx}30\%$ better than the closest runner-up. Project
website: https://universome.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radiology Report Generation with a Learned Knowledge Base and Multi-modal Alignment. (arXiv:2112.15011v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15011">
<div class="article-summary-box-inner">
<span><p>In clinics, a radiology report is crucial for guiding a patient's treatment.
However, writing radiology reports is a heavy burden for radiologists. To this
end, we present an automatic, multi-modal approach for report generation from a
chest x-ray. Our approach, motivated by the observation that the descriptions
in radiology reports are highly correlated with specific information of the
x-ray images, features two distinct modules: (i) Learned knowledge base: To
absorb the knowledge embedded in the radiology reports, we build a knowledge
base that can automatically distil and restore medical knowledge from textual
embedding without manual labour; (ii) Multi-modal alignment: to promote the
semantic alignment among reports, disease labels, and images, we explicitly
utilize textual embedding to guide the learning of the visual feature space. We
evaluate the performance of the proposed model using metrics from both natural
language generation and clinic efficacy on the public IU-Xray and MIMIC-CXR
datasets. Our ablation study shows that each module contributes to improving
the quality of generated reports. Furthermore, with the assistance of both
modules, our approach outperforms state-of-the-art methods over almost all the
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Self-Supervised Pretext Tasks for Active Learning. (arXiv:2201.07459v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07459">
<div class="article-summary-box-inner">
<span><p>Labeling a large set of data is expensive. Active learning aims to tackle
this problem by asking to annotate only the most informative data from the
unlabeled set. We propose a novel active learning approach that utilizes
self-supervised pretext tasks and a unique data sampler to select data that are
both difficult and representative. We discover that the loss of a simple
self-supervised pretext task, such as rotation prediction, is closely
correlated to the downstream task loss. Before the active learning iterations,
the pretext task learner is trained on the unlabeled set, and the unlabeled
data are sorted and split into batches by their pretext task losses. In each
active learning iteration, the main task model is used to sample the most
uncertain data in a batch to be annotated. We evaluate our method on various
image classification and segmentation benchmarks and achieve compelling
performances on CIFAR10, Caltech-101, ImageNet, and Cityscapes. We further show
that our method performs well on imbalanced datasets, and can be an effective
solution to the cold-start problem where active learning performance is
affected by the randomly sampled initial labeled set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Dual Contouring. (arXiv:2202.01999v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01999">
<div class="article-summary-box-inner">
<span><p>We introduce neural dual contouring (NDC), a new data-driven approach to mesh
reconstruction based on dual contouring (DC). Like traditional DC, it produces
exactly one vertex per grid cell and one quad for each grid edge intersection,
a natural and efficient structure for reproducing sharp features. However,
rather than computing vertex locations and edge crossings with hand-crafted
functions that depend directly on difficult-to-obtain surface gradients, NDC
uses a neural network to predict them. As a result, NDC can be trained to
produce meshes from signed or unsigned distance fields, binary voxel grids, or
point clouds (with or without normals); and it can produce open surfaces in
cases where the input represents a sheet or partial surface. During experiments
with five prominent datasets, we find that NDC, when trained on one of the
datasets, generalizes well to the others. Furthermore, NDC provides better
surface reconstruction accuracy, feature preservation, output complexity,
triangle quality, and inference time in comparison to previous learned (e.g.,
neural marching cubes, convolutional occupancy networks) and traditional (e.g.,
Poisson) methods. Code and data are available at
https://github.com/czq142857/NDC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. (arXiv:2202.03052v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03052">
<div class="article-summary-box-inner">
<span><p>In this work, we pursue a unified paradigm for multimodal pretraining to
break the scaffolds of complex task/modality-specific customization. We propose
OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task
Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks,
including image generation, visual grounding, image captioning, image
classification, language modeling, etc., in a simple sequence-to-sequence
learning framework. OFA follows the instruction-based learning in both
pretraining and finetuning stages, requiring no extra task-specific layers for
downstream tasks. In comparison with the recent state-of-the-art vision &amp;
language models that rely on extremely large cross-modal datasets, OFA is
pretrained on only 20M publicly available image-text pairs. Despite its
simplicity and relatively small-scale training data, OFA achieves new SOTAs in
a series of cross-modal tasks while attaining highly competitive performances
on uni-modal tasks. Our further analysis indicates that OFA can also
effectively transfer to unseen tasks and unseen domains. Our code and models
are publicly available at https://github.com/OFA-Sys/OFA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-based Network for Deformable Medical Image Registration. (arXiv:2202.12104v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12104">
<div class="article-summary-box-inner">
<span><p>Deformable medical image registration plays an important role in clinical
diagnosis and treatment. Recently, the deep learning (DL) based image
registration methods have been widely investigated and showed excellent
performance in computational speed. However, these methods cannot provide
enough registration accuracy because of insufficient ability in representing
both the global and local features of the moving and fixed images. To address
this issue, this paper has proposed the transformer based image registration
method. This method uses the distinctive transformer to extract the global and
local image features for generating the deformation fields, based on which the
registered image is produced in an unsupervised way. Our method can improve the
registration accuracy effectively by means of self-attention mechanism and
bi-level information flow. Experimental results on such brain MR image datasets
as LPBA40 and OASIS-1 demonstrate that compared with several traditional and DL
based registration methods, our method provides higher registration accuracy in
terms of dice values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth Completion using Geometry-Aware Embedding. (arXiv:2203.10912v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10912">
<div class="article-summary-box-inner">
<span><p>Exploiting internal spatial geometric constraints of sparse LiDARs is
beneficial to depth completion, however, has been not explored well. This paper
proposes an efficient method to learn geometry-aware embedding, which encodes
the local and global geometric structure information from 3D points, e.g.,
scene layout, object's sizes and shapes, to guide dense depth estimation.
Specifically, we utilize the dynamic graph representation to model generalized
geometric relationship from irregular point clouds in a flexible and efficient
manner. Further, we joint this embedding and corresponded RGB appearance
information to infer missing depths of the scene with well structure-preserved
details. The key to our method is to integrate implicit 3D geometric
representation into a 2D learning architecture, which leads to a better
trade-off between the performance and efficiency. Extensive experiments
demonstrate that the proposed method outperforms previous works and could
reconstruct fine depths with crisp boundaries in regions that are over-smoothed
by them. The ablation study gives more insights into our method that could
achieve significant gains with a simple design, while having better
generalization capability and stability. The code is available at
https://github.com/Wenchao-Du/GAENet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12023">
<div class="article-summary-box-inner">
<span><p>Many promising applications of supervised machine learning face hurdles in
the acquisition of labeled data in sufficient quantity and quality, creating an
expensive bottleneck. To overcome such limitations, techniques that do not
depend on ground truth labels have been studied, including weak supervision and
generative modeling. While these techniques would seem to be usable in concert,
improving one another, how to build an interface between them is not
well-understood. In this work, we propose a model fusing programmatic weak
supervision and generative adversarial networks and provide theoretical
justification motivating this fusion. The proposed approach captures discrete
latent variables in the data alongside the weak supervision derived label
estimate. Alignment of the two allows for better modeling of sample-dependent
accuracies of the weak supervision sources, improving the estimate of
unobserved labels. It is the first approach to enable data augmentation through
weakly supervised synthetic images and pseudolabels. Additionally, its learned
latent variables can be inspected qualitatively. The model outperforms baseline
weak supervision label models on a number of multiclass image classification
datasets, improves the quality of generated images, and further improves
end-model performance through data augmentation with synthetic samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships. (arXiv:2203.14260v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14260">
<div class="article-summary-box-inner">
<span><p>Understanding realistic visual scene images together with language
descriptions is a fundamental task towards generic visual understanding.
Previous works have shown compelling comprehensive results by building
hierarchical structures for visual scenes (e.g., scene graphs) and natural
languages (e.g., dependency trees), individually. However, how to construct a
joint vision-language (VL) structure has barely been investigated. More
challenging but worthwhile, we introduce a new task that targets on inducing
such a joint VL structure in an unsupervised manner. Our goal is to bridge the
visual scene graphs and linguistic dependency trees seamlessly. Due to the lack
of VL structural data, we start by building a new dataset VLParse. Rather than
using labor-intensive labeling from scratch, we propose an automatic alignment
procedure to produce coarse structures followed by human refinement to produce
high-quality ones. Moreover, we benchmark our dataset by proposing a
contrastive learning (CL)-based framework VLGAE, short for Vision-Language
Graph Autoencoder. Our model obtains superior performance on two derived tasks,
i.e., language grammar induction and VL phrase grounding. Ablations show the
effectiveness of both visual cues and dependency relationships on fine-grained
VL structure construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Min-Max Similarity: A Contrastive Semi-Supervised Deep Learning Network for Surgical Tools Segmentation. (arXiv:2203.15177v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15177">
<div class="article-summary-box-inner">
<span><p>Segmentation of images is a popular topic in medical AI. This is mainly due
to the difficulty to obtain a significant number of pixel-level annotated data
to train a neural network. To address this issue, we proposed a semi-supervised
segmentation network based on contrastive learning. In contrast to the previous
state-of-the-art, we introduce Min-Max Similarity (MMS), a contrastive learning
form of dual-view training by employing classifiers and projectors to build
all-negative, and positive and negative feature pairs respectively to formulate
the learning problem as solving min-max similarity problem. The all-negative
pairs are used to supervise the networks learning from different views and make
sure to capture general features, and the consistency of unlabeled predictions
is measured by pixel-wise contrastive loss between positive and negative pairs.
To quantitative and qualitative evaluate our proposed method, we test it on two
public endoscopy surgical tool segmentation datasets and one cochlear implant
surgery dataset which we manually annotate the cochlear implant in surgical
videos. The segmentation performance (dice coefficients) indicates that our
proposed method outperforms state-of-the-art semi-supervised and fully
supervised segmentation algorithms consistently. And our semi-supervised
segmentation algorithm can successfully recognize unknown surgical tools and
provide good predictions. Also, our MMS could achieve about 40 frames per
second (fps) and suitable to deal with the real-time video segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Semantic Segmentation with Error Localization Network. (arXiv:2204.02078v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02078">
<div class="article-summary-box-inner">
<span><p>This paper studies semi-supervised learning of semantic segmentation, which
assumes that only a small portion of training images are labeled and the others
remain unlabeled. The unlabeled images are usually assigned pseudo labels to be
used in training, which however often causes the risk of performance
degradation due to the confirmation bias towards errors on the pseudo labels.
We present a novel method that resolves this chronic issue of pseudo labeling.
At the heart of our method lies error localization network (ELN), an auxiliary
module that takes an image and its segmentation prediction as input and
identifies pixels whose pseudo labels are likely to be wrong. ELN enables
semi-supervised learning to be robust against inaccurate pseudo labels by
disregarding label noises during training and can be naturally integrated with
self-training and contrastive learning. Moreover, we introduce a new learning
strategy for ELN that simulates plausible and diverse segmentation errors
during training of ELN to enhance its generalization. Our method is evaluated
on PASCAL VOC 2012 and Cityscapes, where it outperforms all existing methods in
every evaluation setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Vision Transformers for Joint SAR-optical Representation Learning. (arXiv:2204.05381v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05381">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) has attracted much interest in remote sensing
and earth observation due to its ability to learn task-agnostic representations
without human annotation. While most of the existing SSL works in remote
sensing utilize ConvNet backbones and focus on a single modality, we explore
the potential of vision transformers (ViTs) for joint SAR-optical
representation learning. Based on DINO, a state-of-the-art SSL algorithm that
distills knowledge from two augmented views of an input image, we combine SAR
and optical imagery by concatenating all channels to a unified input.
Subsequently, we randomly mask out channels of one modality as a data
augmentation strategy. While training, the model gets fed optical-only,
SAR-only, and SAR-optical image pairs learning both inner- and intra-modality
representations. Experimental results employing the BigEarthNet-MM dataset
demonstrate the benefits of both, the ViT backbones and the proposed multimodal
SSL algorithm DINO-MM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors. (arXiv:2204.08612v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08612">
<div class="article-summary-box-inner">
<span><p>Deepfakes utilise Artificial Intelligence (AI) techniques to create synthetic
media where the likeness of one person is replaced with another. There are
growing concerns that deepfakes can be maliciously used to create misleading
and harmful digital contents. As deepfakes become more common, there is a dire
need for deepfake detection technology to help spot deepfake media. Present
deepfake detection models are able to achieve outstanding accuracy (&gt;90%).
However, most of them are limited to within-dataset scenario, where the same
dataset is used for training and testing. Most models do not generalise well
enough in cross-dataset scenario, where models are tested on unseen datasets
from another source. Furthermore, state-of-the-art deepfake detection models
rely on neural network-based classification models that are known to be
vulnerable to adversarial attacks. Motivated by the need for a robust deepfake
detection model, this study adapts metamorphic testing (MT) principles to help
identify potential factors that could influence the robustness of the examined
model, while overcoming the test oracle problem in this domain. Metamorphic
testing is specifically chosen as the testing technique as it fits our demand
to address learning-based system testing with probabilistic outcomes from
largely black-box components, based on potentially large input domains. We
performed our evaluations on MesoInception-4 and TwoStreamNet models, which are
the state-of-the-art deepfake detection models. This study identified makeup
application as an adversarial attack that could fool deepfake detectors. Our
experimental results demonstrate that both the MesoInception-4 and TwoStreamNet
models degrade in their performance by up to 30\% when the input data is
perturbed with makeup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Structure For Building A Robust Model. (arXiv:2204.11596v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11596">
<div class="article-summary-box-inner">
<span><p>As deep learning applications, especially programs of computer vision, are
increasingly deployed in our lives, we have to think more urgently about the
security of these applications.One effective way to improve the security of
deep learning models is to perform adversarial training, which allows the model
to be compatible with samples that are deliberately created for use in
attacking the model.Based on this, we propose a simple architecture to build a
model with a certain degree of robustness, which improves the robustness of the
trained network by adding an adversarial sample detection network for
cooperative training. At the same time, we design a new data sampling strategy
that incorporates multiple existing attacks, allowing the model to adapt to
many different adversarial attacks with a single training.We conducted some
experiments to test the effectiveness of this design based on Cifar10 dataset,
and the results indicate that it has some degree of positive effect on the
robustness of the model.Our code could be found at
https://github.com/dowdyboy/simple_structure_for_robust_model .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNNs are Myopic. (arXiv:2205.10760v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10760">
<div class="article-summary-box-inner">
<span><p>We claim that Convolutional Neural Networks (CNNs) learn to classify images
using only small seemingly unrecognizable tiles. We show experimentally that
CNNs trained only using such tiles can match or even surpass the performance of
CNNs trained on full images. Conversely, CNNs trained on full images show
similar predictions on small tiles. We also propose the first a priori
theoretical model for convolutional data sets that seems to explain this
behavior. This gives additional support to the long standing suspicion that
CNNs do not need to understand the global structure of images to achieve
state-of-the-art accuracies. Surprisingly it also suggests that over-fitting is
not needed either.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12693">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has achieved a great success in the representation
learning of visual and textual data. However, the current methods are mainly
validated on the well-curated datasets, which do not exhibit the real-world
long-tailed distribution. Recent attempts to consider self-supervised
long-tailed learning are made by rebalancing in the loss perspective or the
model perspective, resembling the paradigms in the supervised long-tailed
learning. Nevertheless, without the aid of labels, these explorations have not
shown the expected significant promise due to the limitation in tail sample
discovery or the heuristic structure design. Different from previous works, we
explore this direction from an alternative perspective, i.e., the data
perspective, and propose a novel Boosted Contrastive Learning (BCL) method.
Specifically, BCL leverages the memorization effect of deep neural networks to
automatically drive the information discrepancy of the sample views in
contrastive learning, which is more efficient to enhance the long-tailed
learning in the label-unaware context. Extensive experiments on a range of
benchmark datasets demonstrate the effectiveness of BCL over several
state-of-the-art methods. Our code is available at
https://github.com/Zhihan-Zhou/Boosted-Contrastive-Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matryoshka Representations for Adaptive Deployment. (arXiv:2205.13147v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13147">
<div class="article-summary-box-inner">
<span><p>Learned representations are a central component in modern ML systems, serving
a multitude of downstream tasks. When training such representations, it is
often the case that computational and statistical constraints for each
downstream task are unknown. In this context rigid, fixed capacity
representations can be either over or under-accommodating to the task at hand.
This leads us to ask: can we design a flexible representation that can adapt to
multiple downstream tasks with varying computational resources? Our main
contribution is Matryoshka Representation Learning (MRL) which encodes
information at different granularities and allows a single embedding to adapt
to the computational constraints of downstream tasks. MRL minimally modifies
existing representation learning pipelines and imposes no additional cost
during inference and deployment. MRL learns coarse-to-fine representations that
are at least as accurate and rich as independently trained low-dimensional
representations. The flexibility within the learned Matryoshka Representations
offer: (a) up to 14x smaller embedding size for ImageNet-1K classification at
the same level of accuracy; (b) up to 14x real-world speed-ups for large-scale
retrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for
long-tail few-shot classification, all while being as robust as the original
representations. Finally, we show that MRL extends seamlessly to web-scale
datasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet),
vision + language (ALIGN) and language (BERT). MRL code and pretrained models
are open-sourced at https://github.com/RAIVNLab/MRL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13943">
<div class="article-summary-box-inner">
<span><p>Masked image modeling (MIM), an emerging self-supervised pre-training method,
has shown impressive success across numerous downstream vision tasks with
Vision transformers (ViT). Its underlying idea is simple: a portion of the
input image is randomly masked out and then reconstructed via the pre-text
task. However, why MIM works well is not well explained, and previous studies
insist that MIM primarily works for the Transformer family but is incompatible
with CNNs. In this paper, we first study interactions among patches to
understand what knowledge is learned and how it is acquired via the MIM task.
We observe that MIM essentially teaches the model to learn better middle-level
interactions among patches and extract more generalized features. Based on this
fact, we propose an Architecture-Agnostic Masked Image Modeling framework
(A$^2$MIM), which is compatible with not only Transformers but also CNNs in a
unified way. Extensive experiments on popular benchmarks show that our A$^2$MIM
learns better representations and endows the backbone model with the stronger
capability to transfer to various downstream tasks for both Transformers and
CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIT: A Generative Image-to-text Transformer for Vision and Language. (arXiv:2205.14100v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14100">
<div class="article-summary-box-inner">
<span><p>In this paper, we design and train a Generative Image-to-text Transformer,
GIT, to unify vision-language tasks such as image/video captioning and question
answering. While generative models provide a consistent network architecture
between pre-training and fine-tuning, existing work typically contains complex
structures (uni/multi-modal encoder/decoder) and depends on external modules
such as object detectors/taggers and optical character recognition (OCR). In
GIT, we simplify the architecture as one image encoder and one text decoder
under a single language modeling task. We also scale up the pre-training data
and the model size to boost the model performance. Without bells and whistles,
our GIT establishes new state of the arts on 12 challenging benchmarks with a
large margin. For instance, our model surpasses the human performance for the
first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a
new scheme of generation-based image classification and scene text recognition,
achieving decent performance on standard benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Masked Autoencoders Learn Transferable Representations. (arXiv:2205.14204v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14204">
<div class="article-summary-box-inner">
<span><p>Building scalable models to learn from diverse, multimodal data remains an
open challenge. For vision-language data, the dominant approaches are based on
contrastive learning objectives that train a separate encoder for each
modality. While effective, contrastive learning approaches introduce sampling
bias depending on the data augmentations used, which can degrade performance on
downstream tasks. Moreover, these methods are limited to paired image-text
data, and cannot leverage widely-available unpaired data. In this paper, we
investigate whether a large multimodal model trained purely via masked token
prediction, without using modality-specific encoders or contrastive learning,
can learn transferable representations for downstream tasks. We propose a
simple and scalable network architecture, the Multimodal Masked Autoencoder
(M3AE), which learns a unified encoder for both vision and language data via
masked token prediction. We provide an empirical study of M3AE trained on a
large-scale image-text dataset, and find that M3AE is able to learn
generalizable representations that transfer well to downstream tasks.
Surprisingly, we find that M3AE benefits from a higher text mask ratio
(50-90%), in contrast to BERT whose standard masking ratio is 15%, due to the
joint training of two data modalities. We also provide qualitative analysis
showing that the learned representation incorporates meaningful information
from both image and language. Lastly, we demonstrate the scalability of M3AE
with larger model size and training time, and its flexibility to train on both
paired image-text data as well as unpaired data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis. (arXiv:2205.14375v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14375">
<div class="article-summary-box-inner">
<span><p>Gains in the ability to generalize on image analysis tasks for neural
networks have come at the cost of increased number of parameters and layers,
dataset sizes, training and test computations, and GPU RAM. We introduce a new
architecture -- WaveMix-Lite -- that can generalize on par with contemporary
transformers and convolutional neural networks (CNNs) while needing fewer
resources. WaveMix-Lite uses 2D-discrete wavelet transform to efficiently mix
spatial information from pixels. WaveMix-Lite seems to be a versatile and
scalable architectural framework that can be used for multiple vision tasks,
such as image classification and semantic segmentation, without requiring
significant architectural changes, unlike transformers and CNNs. It is able to
meet or exceed several accuracy benchmarks while training on a single GPU. For
instance, it achieves state-of-the-art accuracy on five EMNIST datasets,
outperforms CNNs and transformers in ImageNet-1K (64$\times$64 images), and
achieves an mIoU of 75.32 % on Cityscapes validation set, while using less than
one-fifth the number parameters and half the GPU RAM of comparable CNNs or
transformers. Our experiments show that while the convolutional elements of
neural architectures exploit the shift-invariance property of images, new types
of layers (e.g., wavelet transform) can exploit additional properties of
images, such as scale-invariance and finite spatial extents of objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cervical Glandular Cell Detection from Whole Slide Image with Out-Of-Distribution Data. (arXiv:2205.14625v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14625">
<div class="article-summary-box-inner">
<span><p>Cervical glandular cell (GC) detection is a key step in computer-aided
diagnosis for cervical adenocarcinomas screening. It is challenging to
accurately recognize GCs in cervical smears in which squamous cells are the
major. Widely existing Out-Of-Distribution (OOD) data in the entire smear leads
decreasing reliability of machine learning system for GC detection. Although,
the State-Of-The-Art (SOTA) deep learning model can outperform pathologists in
preselected regions of interest, the mass False Positive (FP) prediction with
high probability is still unsolved when facing such gigapixel whole slide
image. This paper proposed a novel PolarNet based on the morphological prior
knowledge of GC trying to solve the FP problem via a self-attention mechanism
in eight-neighbor. It estimates the polar orientation of nucleus of GC. As a
plugin module, PolarNet can guide the deep feature and predicted confidence of
general object detection models. In experiments, we discovered that general
models based on four different frameworks can reject FP in small image set and
increase the mean of average precision (mAP) by $\text{0.007}\sim\text{0.015}$
in average, where the highest exceeds the recent cervical cell detection model
0.037. By plugging PolarNet, the deployed C++ program improved by 8.8\% on
accuracy of top-20 GC detection from external WSIs, while sacrificing 14.4 s of
computational time. Code is available in
https://github.com/Chrisa142857/PolarNet-GCdet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Audio Pattern Recognition for Asthma Medication Adherence: Evaluation with the RDA Benchmark Suite. (arXiv:2205.15360v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15360">
<div class="article-summary-box-inner">
<span><p>Asthma is a common, usually long-term respiratory disease with negative
impact on society and the economy worldwide. Treatment involves using medical
devices (inhalers) that distribute medication to the airways, and its
efficiency depends on the precision of the inhalation technique. Health
monitoring systems equipped with sensors and embedded with sound signal
detection enable the recognition of drug actuation and could be powerful tools
for reliable audio content analysis. This paper revisits audio pattern
recognition and machine learning techniques for asthma medication adherence
assessment and presents the Respiratory and Drug Actuation (RDA)
Suite(https://gitlab.com/vvr/monitoring-medication-adherence/rda-benchmark) for
benchmarking and further research. The RDA Suite includes a set of tools for
audio processing, feature extraction and classification and is provided along
with a dataset consisting of respiratory and drug actuation sounds. The
classification models in RDA are implemented based on conventional and advanced
machine learning and deep network architectures. This study provides a
comparative evaluation of the implemented approaches, examines potential
improvements and discusses challenges and future tendencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gator: Customizable Channel Pruning of Neural Networks with Gating. (arXiv:2205.15404v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15404">
<div class="article-summary-box-inner">
<span><p>The rise of neural network (NN) applications has prompted an increased
interest in compression, with a particular focus on channel pruning, which does
not require any additional hardware. Most pruning methods employ either
single-layer operations or global schemes to determine which channels to remove
followed by fine-tuning of the network. In this paper we present Gator, a
channel-pruning method which temporarily adds learned gating mechanisms for
pruning of individual channels, and which is trained with an additional
auxiliary loss, aimed at reducing the computational cost due to memory,
(theoretical) speedup (in terms of FLOPs), and practical, hardware-specific
speedup. Gator introduces a new formulation of dependencies between NN layers
which, in contrast to most previous methods, enables pruning of non-sequential
parts, such as layers on ResNet's highway, and even removing entire ResNet
blocks. Gator's pruning for ResNet-50 trained on ImageNet produces
state-of-the-art (SOTA) results, such as 50% FLOPs reduction with only
0.4%-drop in top-5 accuracy. Also, Gator outperforms previous pruning models,
in terms of GPU latency by running 1.4 times faster. Furthermore, Gator
achieves improved top-5 accuracy results, compared to MobileNetV2 and
SqueezeNet, for similar runtimes. The source code of this work is available at:
https://github.com/EliPassov/gator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector. (arXiv:2205.15469v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15469">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel end-to-end group collaborative learning
network, termed GCoNet+, which can effectively and efficiently (250 fps)
identify co-salient objects in natural scenes. The proposed GCoNet+ achieves
the new state-of-the-art performance for co-salient object detection (CoSOD)
through mining consensus representations based on the following two essential
criteria: 1) intra-group compactness to better formulate the consistency among
co-salient objects by capturing their inherent shared attributes using our
novel group affinity module (GAM); 2) inter-group separability to effectively
suppress the influence of noisy objects on the output by introducing our new
group collaborating module (GCM) conditioning on the inconsistent consensus. To
further improve the accuracy, we design a series of simple yet effective
components as follows: i) a recurrent auxiliary classification module (RACM)
promoting the model learning at the semantic level; ii) a confidence
enhancement module (CEM) helping the model to improve the quality of the final
predictions; and iii) a group-based symmetric triplet (GST) loss guiding the
model to learn more discriminative features. Extensive experiments on three
challenging benchmarks, i.e., CoCA, CoSOD3k, and CoSal2015, demonstrate that
our GCoNet+ outperforms the existing 12 cutting-edge models. Code has been
released at https://github.com/ZhengPeng7/GCoNet_plus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning for Building Damage Assessment from Large-scale xBD Satellite Imagery Benchmark Datasets. (arXiv:2205.15688v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15688">
<div class="article-summary-box-inner">
<span><p>In the field of post-disaster assessment, for timely and accurate rescue and
localization after a disaster, people need to know the location of damaged
buildings. In deep learning, some scholars have proposed methods to make
automatic and highly accurate building damage assessments by remote sensing
images, which are proved to be more efficient than assessment by domain
experts. However, due to the lack of a large amount of labeled data, these
kinds of tasks can suffer from being able to do an accurate assessment, as the
efficiency of deep learning models relies highly on labeled data. Although
existing semi-supervised and unsupervised studies have made breakthroughs in
this area, none of them has completely solved this problem. Therefore, we
propose adopting a self-supervised comparative learning approach to address the
task without the requirement of labeled data. We constructed a novel asymmetric
twin network architecture and tested its performance on the xBD dataset.
Experiment results of our model show the improvement compared to baseline and
commonly used methods. We also demonstrated the potential of self-supervised
methods for building damage recognition awareness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Iterative Recovery from Nonlinear Observations using Generative Models. (arXiv:2205.15749v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15749">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to estimate the direction of an underlying signal from
its nonlinear observations following the semi-parametric single index model
(SIM). Unlike conventional compressed sensing where the signal is assumed to be
sparse, we assume that the signal lies in the range of an $L$-Lipschitz
continuous generative model with bounded $k$-dimensional inputs. This is mainly
motivated by the tremendous success of deep generative models in various real
applications. Our reconstruction method is non-iterative (though approximating
the projection step may use an iterative procedure) and highly efficient, and
it is shown to attain the near-optimal statistical rate of order $\sqrt{(k \log
L)/m}$, where $m$ is the number of measurements. We consider two specific
instances of the SIM, namely noisy $1$-bit and cubic measurement models, and
perform experiments on image datasets to demonstrate the efficacy of our
method. In particular, for the noisy $1$-bit measurement model, we show that
our non-iterative method significantly outperforms a state-of-the-art iterative
method in terms of both accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SymFormer: End-to-end symbolic regression using transformer-based architecture. (arXiv:2205.15764v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15764">
<div class="article-summary-box-inner">
<span><p>Many real-world problems can be naturally described by mathematical formulas.
The task of finding formulas from a set of observed inputs and outputs is
called symbolic regression. Recently, neural networks have been applied to
symbolic regression, among which the transformer-based ones seem to be the most
promising. After training the transformer on a large number of formulas (in the
order of days), the actual inference, i.e., finding a formula for new, unseen
data, is very fast (in the order of seconds). This is considerably faster than
state-of-the-art evolutionary methods. The main drawback of transformers is
that they generate formulas without numerical constants, which have to be
optimized separately, so yielding suboptimal results. We propose a
transformer-based approach called SymFormer, which predicts the formula by
outputting the individual symbols and the corresponding constants
simultaneously. This leads to better performance in terms of fitting the
available data. In addition, the constants provided by SymFormer serve as a
good starting point for subsequent tuning via gradient descent to further
improve the performance. We show on a set of benchmarks that SymFormer
outperforms two state-of-the-art methods while having faster inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D$^2$NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video. (arXiv:2205.15838v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15838">
<div class="article-summary-box-inner">
<span><p>Given a monocular video, segmenting and decoupling dynamic objects while
recovering the static environment is a widely studied problem in machine
intelligence. Existing solutions usually approach this problem in the image
domain, limiting their performance and understanding of the environment. We
introduce Decoupled Dynamic Neural Radiance Field (D$^2$NeRF), a
self-supervised approach that takes a monocular video and learns a 3D scene
representation which decouples moving objects, including their shadows, from
the static background. Our method represents the moving objects and the static
background by two separate neural radiance fields with only one allowing for
temporal changes. A naive implementation of this approach leads to the dynamic
component taking over the static one as the representation of the former is
inherently more general and prone to overfitting. To this end, we propose a
novel loss to promote correct separation of phenomena. We further propose a
shadow field network to detect and decouple dynamically moving shadows. We
introduce a new dataset containing various dynamic objects and shadows and
demonstrate that our method can achieve better performance than
state-of-the-art approaches in decoupling dynamic and static 3D objects,
occlusion and shadow removal, and image segmentation for moving objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Competitive Method for Dog Nose-print Re-identification. (arXiv:2205.15934v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15934">
<div class="article-summary-box-inner">
<span><p>Vision-based pattern identification (such as face, fingerprint, iris etc.)
has been successfully applied in human biometrics for a long history. However,
dog nose-print authentication is a challenging problem since the lack of a
large amount of labeled data. For that, this paper presents our proposed
methods for dog nose-print authentication (Re-ID) task in CVPR 2022 pet
biometric challenge. First, considering the problem that each class only with
few samples in the training set, we propose an automatic offline data
augmentation strategy. Then, for the difference in sample styles between the
training and test datasets, we employ joint cross-entropy, triplet and
pair-wise circle losses function for network optimization. Finally, with
multiple models ensembled adopted, our methods achieve 86.67\% AUC on the test
set. Codes are available at https://github.com/muzishen/Pet-ReID-IMAG.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-02 23:09:01.101996028 UTC">2022-06-02 23:09:01 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>