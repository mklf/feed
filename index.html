<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-31T01:30:00Z">03-31</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">LinkBERT: Pretraining Language Models with Document Links. (arXiv:2203.15827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15827">
<div class="article-summary-box-inner">
<span><p>Language model (LM) pretraining can learn various knowledge from text
corpora, helping downstream tasks. However, existing methods such as BERT model
a single document, and do not capture dependencies or knowledge that span
across documents. In this work, we propose LinkBERT, an LM pretraining method
that leverages links between documents, e.g., hyperlinks. Given a text corpus,
we view it as a graph of documents and create LM inputs by placing linked
documents in the same context. We then pretrain the LM with two joint
self-supervised objectives: masked language modeling and our new proposal,
document relation prediction. We show that LinkBERT outperforms BERT on various
downstream tasks across two domains: the general domain (pretrained on
Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with
citation links). LinkBERT is especially effective for multi-hop reasoning and
few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our
biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on
BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT,
as well as code and data at https://github.com/michiyasunaga/LinkBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seq-2-Seq based Refinement of ASR Output for Spoken Name Capture. (arXiv:2203.15833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15833">
<div class="article-summary-box-inner">
<span><p>Person name capture from human speech is a difficult task in human-machine
conversations. In this paper, we propose a novel approach to capture the person
names from the caller utterances in response to the prompt "say and spell your
first/last name". Inspired from work on spell correction, disfluency removal
and text normalization, we propose a lightweight Seq-2-Seq system which
generates a name spell from a varying user input. Our proposed method
outperforms the strong baseline which is based on LM-driven rule-based
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoregressive Co-Training for Learning Discrete Speech Representations. (arXiv:2203.15840v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15840">
<div class="article-summary-box-inner">
<span><p>While several self-supervised approaches for learning discrete speech
representation have been proposed, it is unclear how these seemingly similar
approaches relate to each other. In this paper, we consider a generative model
with discrete latent variables that learns a discrete representation for
speech. The objective of learning the generative model is formulated as
information-theoretic co-training. Besides the wide generality, the objective
can be optimized with several approaches, subsuming HuBERT-like training and
vector quantization for learning discrete representation. Empirically, we find
that the proposed approach learns discrete representation that is highly
correlated with phonetic units, more correlated than HuBERT-like training and
vector quantization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics. (arXiv:2203.15858v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15858">
<div class="article-summary-box-inner">
<span><p>Current practices in metric evaluation focus on one single dataset, e.g.,
Newstest dataset in each year's WMT Metrics Shared Task. However, in this
paper, we qualitatively and quantitatively show that the performances of
metrics are sensitive to data. The ranking of metrics varies when the
evaluation is conducted on different datasets. Then this paper further
investigates two potential hypotheses, i.e., insignificant data points and the
deviation of Independent and Identically Distributed (i.i.d) assumption, which
may take responsibility for the issue of data variance. In conclusion, our
findings suggest that when evaluating automatic translation metrics,
researchers should take data variance into account and be cautious to claim the
result on a single dataset, because it may leads to inconsistent results with
most of other datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualizing the Relationship Between Encoded Linguistic Information and Task Performance. (arXiv:2203.15860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15860">
<div class="article-summary-box-inner">
<span><p>Probing is popular to analyze whether linguistic information can be captured
by a well-trained deep neural model, but it is hard to answer how the change of
the encoded linguistic information will affect task performance. To this end,
we study the dynamic relationship between the encoded linguistic information
and task performance from the viewpoint of Pareto Optimality. Its key idea is
to obtain a set of models which are Pareto-optimal in terms of both objectives.
From this viewpoint, we propose a method to optimize the Pareto-optimal models
by formalizing it as a multi-objective optimization problem. We conduct
experiments on two popular NLP tasks, i.e., machine translation and language
modeling, and investigate the relationship between several kinds of linguistic
information and task performances. Experimental results demonstrate that the
proposed method is better than a baseline method. Our empirical findings
suggest that some syntactic information is helpful for NLP tasks whereas
encoding more syntactic information does not necessarily lead to better
performance, because the model architecture is also an important factor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models. (arXiv:2203.15863v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15863">
<div class="article-summary-box-inner">
<span><p>Large-scale auto-regressive language models pretrained on massive text have
demonstrated their impressive ability to perform new natural language tasks
with only a few text examples, without the need for fine-tuning. Recent studies
further show that such a few-shot learning ability can be extended to the
text-image setting by training an encoder to encode the images into embeddings
functioning like the text embeddings of the language model. Interested in
exploring the possibility of transferring the few-shot learning ability to the
audio-text setting, we propose a novel speech understanding framework,
WavPrompt, where we finetune a wav2vec model to generate a sequence of audio
embeddings understood by the language model. We show that WavPrompt is a
few-shot learner that can perform speech understanding tasks better than a
naive text baseline. We conduct detailed ablation studies on different
components and hyperparameters to empirically identify the best model
configuration. In addition, we conduct a non-speech understanding experiment to
show WavPrompt can extract more information than just the transcriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Retrieval from Contextual Descriptions. (arXiv:2203.15867v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15867">
<div class="article-summary-box-inner">
<span><p>The ability to integrate context, including perceptual and temporal cues,
plays a pivotal role in grounding the meaning of a linguistic utterance. In
order to measure to what extent current vision-and-language models master this
ability, we devise a new multimodal challenge, Image Retrieval from Contextual
Descriptions (ImageCoDe). In particular, models are tasked with retrieving the
correct image from a set of 10 minimally contrastive candidates based on a
contextual description. As such, each description contains only the details
that help distinguish between images. Because of this, descriptions tend to be
complex in terms of syntax and discourse and require drawing pragmatic
inferences. Images are sourced from both static pictures and video frames. We
benchmark several state-of-the-art models, including both cross-encoders such
as ViLBERT and bi-encoders such as CLIP, on ImageCoDe. Our results reveal that
these models dramatically lag behind human performance: the best variant
achieves an accuracy of 20.9 on video frames and 59.4 on static pictures,
compared with 90.8 in humans. Furthermore, we experiment with new model
variants that are better equipped to incorporate visual and temporal context
into their representations, which achieve modest gains. Our hope is that
ImageCoDE will foster progress in grounded language understanding by
encouraging models to focus on fine-grained visual differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization. (arXiv:2203.15917v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15917">
<div class="article-summary-box-inner">
<span><p>Text normalization (TN) systems in production are largely rule-based using
weighted finite-state transducers (WFST). However, WFST-based systems struggle
with ambiguous input when the normalized form is context-dependent. On the
other hand, neural text normalization systems can take context into account but
they suffer from unrecoverable errors and require labeled normalization
datasets, which are hard to collect. We propose a new hybrid approach that
combines the benefits of rule-based and neural systems. First, a
non-deterministic WFST outputs all normalization candidates, and then a neural
language model picks the best one -- similar to shallow fusion for automatic
speech recognition. While the WFST prevents unrecoverable errors, the language
model resolves contextual ambiguity. The approach is easy to extend and we show
it is effective. It achieves comparable or better results than existing
state-of-the-art TN models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment. (arXiv:2203.15937v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15937">
<div class="article-summary-box-inner">
<span><p>Current leading mispronunciation detection and diagnosis (MDD) systems
achieve promising performance via end-to-end phoneme recognition. One challenge
of such end-to-end solutions is the scarcity of human-annotated phonemes on
natural L2 speech. In this work, we leverage unlabeled L2 speech via a
pseudo-labeling (PL) procedure and extend the fine-tuning approach based on
pre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec
2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples
plus the created pseudo-labeled L2 speech samples. Our pseudo labels are
dynamic and are produced by an ensemble of the online model on-the-fly, which
ensures that our model is robust to pseudo label noise. We show that
fine-tuning with pseudo labels gains a 5.35% phoneme error rate reduction and
2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning
baseline. The proposed PL method is also shown to outperform conventional
offline PL methods. Compared to the state-of-the-art MDD systems, our MDD
solution achieves a more accurate and consistent phonetic error diagnosis. In
addition, we conduct an open test on a separate UTD-4Accents dataset, where our
system recognition outputs show a strong correlation with human perception,
based on accentedness and intelligibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-driven Fact-aware Abstractive Summarization of Biomedical Literature. (arXiv:2203.15959v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15959">
<div class="article-summary-box-inner">
<span><p>As part of the large number of scientific articles being published every
year, the publication rate of biomedical literature has been increasing.
Consequently, there has been considerable effort to harness and summarize the
massive amount of biomedical research articles. While transformer-based
encoder-decoder models in a vanilla source document-to-summary setting have
been extensively studied for abstractive summarization in different domains,
their major limitations continue to be entity hallucination (a phenomenon where
generated summaries constitute entities not related to or present in source
article(s)) and factual inconsistency. This problem is exacerbated in a
biomedical setting where named entities and their semantics (which can be
captured through a knowledge base) constitute the essence of an article. The
use of named entities and facts mined from background knowledge bases
pertaining to the named entities to guide abstractive summarization has not
been studied in biomedical article summarization literature. In this paper, we
propose an entity-driven fact-aware framework for training end-to-end
transformer-based encoder-decoder models for abstractive summarization of
biomedical articles. We call the proposed approach, whose building block is a
transformer-based model, EFAS, Entity-driven Fact-aware Abstractive
Summarization. We conduct experiments using five state-of-the-art
transformer-based models (two of which are specifically designed for long
document summarization) and demonstrate that injecting knowledge into the
training/inference phase of these models enables the models to achieve
significantly better performance than the standard source document-to-summary
setting in terms of entity-level factual accuracy, N-gram novelty, and semantic
equivalence while performing comparably on ROUGE metrics. The proposed approach
is evaluated on ICD-11-Summ-1000, and PubMed-50k.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Domain Adaptation for ASR with Full Self-Supervision. (arXiv:2203.15966v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15966">
<div class="article-summary-box-inner">
<span><p>Cross-device federated learning (FL) protects user privacy by collaboratively
training a model on user devices, therefore eliminating the need for
collecting, storing, and manually labeling user data. Previous works have
considered cross-device FL for automatic speech recognition (ASR), however,
there are a few important challenges that have not been fully addressed. These
include the lack of ground-truth ASR transcriptions, and the scarcity of
compute resource and network bandwidth on edge devices. In this paper, we
address these two challenges. First, we propose a federated learning system to
support on-device ASR adaptation with full self-supervision, which uses
self-labeling together with data augmentation and filtering techniques. The
proposed system can improve a strong Emformer-Transducer based ASR model
pretrained on out-of-domain data, using in-domain audios without any
ground-truth transcriptions. Second, to reduce the training cost, we propose a
self-restricted RNN Transducer (SR-RNN-T) loss, a new variant of
alignment-restricted RNN-T that uses Viterbi forced-alignment from
self-supervision. To further reduce the compute and network cost, we
systematically explore adapting only a subset of weights in the
Emformer-Transducer. Our best training recipe achieves a 12.9% relative WER
reduction over the strong out-of-domain baseline, which equals 70% of the
reduction achievable with full human supervision and centralized training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale Speaker Diarization with Dynamic Scale Weighting. (arXiv:2203.15974v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15974">
<div class="article-summary-box-inner">
<span><p>Speaker diarization systems are challenged by a trade-off between the
temporal resolution and the fidelity of the speaker representation. By
obtaining a superior temporal resolution with an enhanced accuracy, a
multi-scale approach is a way to cope with such a trade-off. In this paper, we
propose a more advanced multi-scale diarization system based on a multi-scale
diarization decoder. There are two main contributions in this study that
significantly improve the diarization performance. First, we use multi-scale
clustering as an initialization to estimate the number of speakers and obtain
the average speaker representation vector for each speaker and each scale.
Next, we propose the use of 1-D convolutional neural networks that dynamically
determine the importance of each scale at each time step. To handle a variable
number of speakers and overlapping speech, the proposed system can estimate the
number of existing speakers. Our proposed system achieves a state-of-art
performance on the CALLHOME and AMI MixHeadset datasets, with 3.92% and 1.05%
diarization error rates, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models. (arXiv:2203.15996v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15996">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have been prevailed in natural language
processing and become the backbones of many NLP tasks, but the demands for
computational resources have limited their applications. In this paper, we
introduce TextPruner, an open-source model pruning toolkit designed for
pre-trained language models, targeting fast and easy model compression.
TextPruner offers structured post-training pruning methods, including
vocabulary pruning and transformer pruning, and can be applied to various
models and tasks. We also propose a self-supervised pruning method that can be
applied without the labeled data. Our experiments with several NLP tasks
demonstrate the ability of TextPruner to reduce the model size without
re-training the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clozer: Adaptable Data Augmentation for Cloze-style Reading Comprehension. (arXiv:2203.16027v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16027">
<div class="article-summary-box-inner">
<span><p>Task-adaptive pre-training (TAPT) alleviates the lack of labelled data and
provides performance lift by adapting unlabelled data to downstream task.
Unfortunately, existing adaptations mainly involve deterministic rules that
cannot generalize well. Here, we propose Clozer, a sequence-tagging based cloze
answer extraction method used in TAPT that is extendable for adaptation on any
cloze-style machine reading comprehension (MRC) downstream tasks. We experiment
on multiple-choice cloze-style MRC tasks, and show that Clozer performs
significantly better compared to the oracle and state-of-the-art in escalating
TAPT effectiveness in lifting model performance, and prove that Clozer is able
to recognize the gold answers independently of any heuristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Span Classification with Structured Information for Disfluency Detection in Spoken Utterances. (arXiv:2203.16028v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16028">
<div class="article-summary-box-inner">
<span><p>Existing approaches in disfluency detection focus on solving a token-level
classification task for identifying and removing disfluencies in text.
Moreover, most works focus on leveraging only contextual information captured
by the linear sequences in text, thus ignoring the structured information in
text which is efficiently captured by dependency trees. In this paper, building
on the span classification paradigm of entity recognition, we propose a novel
architecture for detecting disfluencies in transcripts from spoken utterances,
incorporating both contextual information through transformers and
long-distance structured information captured by dependency trees, through
graph convolutional networks (GCNs). Experimental results show that our
proposed model achieves state-of-the-art results on the widely used English
Switchboard for disfluency detection and outperforms prior-art by a significant
margin. We make all our codes publicly available on GitHub
(https://github.com/Sreyan88/Disfluency-Detection-with-Span-Classification)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks. (arXiv:2203.16040v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16040">
<div class="article-summary-box-inner">
<span><p>Because the performance of speech separation is excellent for speech in which
two speakers completely overlap, research attention has been shifted to dealing
with more realistic scenarios. However, domain mismatch between training/test
situations due to factors, such as speaker, content, channel, and environment,
remains a severe problem for speech separation. Speaker and environment
mismatches have been studied in the existing literature. Nevertheless, there
are few studies on speech content and channel mismatches. Moreover, the impacts
of language and channel in these studies are mostly tangled. In this study, we
create several datasets for various experiments. The results show that the
impacts of different languages are small enough to be ignored compared to the
impacts of different channels. In our experiments, training on data recorded by
Android phones leads to the best generalizability. Moreover, we provide a new
solution for channel mismatch by evaluating projection, where the channel
similarity can be measured and used to effectively select additional training
data to improve the performance of in-the-wild test data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Graph Convolutional Networks for Text Classification. (arXiv:2203.16060v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16060">
<div class="article-summary-box-inner">
<span><p>Graph Convolutional Networks (GCN) have been effective at tasks that have
rich relational structure and can preserve global structure information of a
dataset in graph embeddings. Recently, many researchers focused on examining
whether GCNs could handle different Natural Language Processing tasks,
especially text classification. While applying GCNs to text classification is
well-studied, its graph construction techniques, such as node/edge selection
and their feature representation, and the optimal GCN learning mechanism in
text classification is rather neglected. In this paper, we conduct a
comprehensive analysis of the role of node and edge embeddings in a graph and
its GCN learning techniques in text classification. Our analysis is the first
of its kind and provides useful insights into the importance of each graph
node/edge construction mechanism when applied at the GCN training/testing in
different text classification benchmarks, as well as under its semi-supervised
environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview of Indian Language Datasets used for Text Summarization. (arXiv:2203.16127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16127">
<div class="article-summary-box-inner">
<span><p>In this paper, we survey Text Summarization (TS) datasets in Indian
Lan-guages (ILs), which are also low-resource languages (LRLs). We seek to
answer one primary question: is the pool of Indian Language Text Summarization
(ILTS) dataset growing or is there a serious resource poverty? To an-swer the
primary question, we pose two sub-questions that we seek about ILTS datasets:
first, what characteristics: format and domain do ILTS da-tasets have? Second,
how different are those characteristics of ILTS datasets from high-resource
languages (HRLs) particularly English. The survey of ILTS and English datasets
reveals two similarities and one contrast. The two similarities are: first, the
domain of dataset commonly is news (Hermann et al., 2015). The second
similarity is the format of the dataset which is both extractive and
abstractive. The contrast is in how the research in dataset development has
progressed. ILs face a slow speed of development and public release of datasets
as compared with English. We conclude that the relatively lower number of ILTS
datasets is because of two reasons: first, absence of a dedicated forum for
developing TS tools. And second, lack of shareable standard datasets in the
public domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling. (arXiv:2203.16169v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16169">
<div class="article-summary-box-inner">
<span><p>This work presents a new resource for borrowing identification and analyzes
the performance and errors of several models on this task. We introduce a new
annotated corpus of Spanish newswire rich in unassimilated lexical borrowings
-- words from one language that are introduced into another without
orthographic adaptation -- and use it to evaluate how several sequence labeling
models (CRF, BiLSTM-CRF, and Transformer-based models) perform. The corpus
contains 370,000 tokens and is larger, more borrowing-dense, OOV-rich, and
topic-varied than previous corpora available for this task. Our results show
that a BiLSTM-CRF model fed with subword embeddings along with either
Transformer-based embeddings pretrained on codeswitched data or a combination
of contextualized word embeddings outperforms results obtained by a
multilingual BERT-based model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-MLM: Improved Contrastive Learning for Self-supervised Multi-lingual Knowledge Retrieval. (arXiv:2203.16187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16187">
<div class="article-summary-box-inner">
<span><p>Contrastive learning (CL) has become a ubiquitous approach for several
natural language processing (NLP) downstream tasks, especially for question
answering (QA). However, the major challenge, how to efficiently train the
knowledge retrieval model in an unsupervised manner, is still unresolved.
Recently the commonly used methods are composed of CL and masked language model
(MLM). Unexpectedly, MLM ignores the sentence-level training, and CL also
neglects extraction of the internal info from the query. To optimize the CL
hardly obtain internal information from the original query, we introduce a
joint training method by combining CL and Auto-MLM for self-supervised
multi-lingual knowledge retrieval. First, we acquire the fixed dimensional
sentence vector. Then, mask some words among the original sentences with random
strategy. Finally, we generate a new token representation for predicting the
masked tokens. Experimental results show that our proposed approach
consistently outperforms all the previous SOTA methods on both AliExpress $\&amp;$
LAZADA service corpus and openly available corpora in 8 languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic generation of semantic corpora for improving intent estimation of taxonomy-driven search engines. (arXiv:2203.16230v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16230">
<div class="article-summary-box-inner">
<span><p>With the increasing demand of intelligent systems capable of operating in
different user contexts (e.g. users on the move) the correct interpretation of
the user-need by such systems has become crucial to give a consistent answer to
the user query. The most effective techniques which are used to address such
task are in the fields of natural language processing and semantic expansion of
terms. Such systems are aimed at estimating the actual meaning of input
queries, addressing the concepts of the words which are expressed within the
user questions. The aim of this paper is to demonstrate which semantic relation
impacts the most in semantic expansion-based retrieval systems and to identify
the best tradeoff between accuracy and noise introduction when combining such
relations. The evaluations are made building a simple natural language
processing system capable of querying any taxonomy-driven domain, making use of
the combination of different semantic expansions as knowledge resources. The
proposed evaluation employs a wide and varied taxonomy as a use-case,
exploiting its labels as basis for the expansions. To build the knowledge
resources several corpora have been produced and integrated as gazetteers into
the NLP infrastructure with the purpose of estimating the pseudo-queries
corresponding to the taxonomy labels, considered as the possible intents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-autoregressive Translation with Dependency-Aware Decoder. (arXiv:2203.16266v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16266">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive translation (NAT) models suffer from inferior translation
quality due to removal of dependency on previous target tokens from inputs to
the decoder. In this paper, we propose a novel and general approach to enhance
the target dependency within the NAT decoder from two perspectives: decoder
input and decoder self-attention. First, we transform the initial decoder input
from the source language space to the target language space through a novel
attentive transformation process. The transformation reassembles the decoder
input based on target token embeddings and conditions the final output on the
target-side information. Second, before NAT training, we introduce an effective
forward-backward pre-training phase, implemented with different triangle
attention masks. This pre-training phase enables the model to gradually learn
bidirectional dependencies for the final NAT decoding process. Experimental
results demonstrate that the proposed approaches consistently improve highly
competitive NAT models on four WMT translation directions by up to 1.88 BLEU
score, while overall maintaining inference latency comparable to other fully
NAT models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Pipeline for Zero-Shot Data-to-Text Generation. (arXiv:2203.16279v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16279">
<div class="article-summary-box-inner">
<span><p>In data-to-text (D2T) generation, training on in-domain data leads to
overfitting to the data representation and repeating training data noise. We
examine how to avoid finetuning pretrained language models (PLMs) on D2T
generation datasets while still taking advantage of surface realization
capabilities of PLMs. Inspired by pipeline approaches, we propose to generate
text by transforming single-item descriptions with a sequence of modules
trained on general-domain text-based operations: ordering, aggregation, and
paragraph compression. We train PLMs for performing these operations on a
synthetic corpus WikiFluent which we build from English Wikipedia. Our
experiments on two major triple-to-text datasets -- WebNLG and E2E -- show that
our approach enables D2T generation from RDF triples in zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rainbow Keywords: Efficient Incremental Learning for Online Spoken Keyword Spotting. (arXiv:2203.16361v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16361">
<div class="article-summary-box-inner">
<span><p>Catastrophic forgetting is a thorny challenge when updating keyword spotting
(KWS) models after deployment. This problem will be more challenging if KWS
models are further required for edge devices due to their limited memory. To
alleviate such an issue, we propose a novel diversity-aware incremental
learning method named Rainbow Keywords (RK). Specifically, the proposed RK
approach introduces a diversity-aware sampler to select a diverse set from
historical and incoming keywords by calculating classification uncertainty. As
a result, the RK approach can incrementally learn new tasks without forgetting
prior knowledge. Besides, the RK approach also proposes data augmentation and
knowledge distillation loss function for efficient memory management on the
edge device. Experimental results show that the proposed RK approach achieves
4.2% absolute improvement in terms of average accuracy over the best baseline
on Google Speech Command dataset with less required memory. The scripts are
available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis. (arXiv:2203.16369v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16369">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a
specific aspect in the given sentence. While pre-trained language models such
as BERT have achieved great success, incorporating dynamic semantic changes
into ABSA remains challenging. To this end, in this paper, we propose to
address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method
designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we
first take the Stack-BERT layers as a primary encoder to grasp the overall
semantic of the sentence and then fine-tune it by incorporating a lightweight
Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention
to a small region of the sentences at each step and re-weigh the vitally
important words for better aspect-aware sentiment understanding. Finally,
experimental results on three benchmark datasets demonstrate the effectiveness
and the rationality of our proposed model and provide good interpretable
insights for future semantic modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TubeDETR: Spatio-Temporal Video Grounding with Transformers. (arXiv:2203.16434v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16434">
<div class="article-summary-box-inner">
<span><p>We consider the problem of localizing a spatio-temporal tube in a video
corresponding to a given text query. This is a challenging task that requires
the joint and efficient modeling of temporal, spatial and multi-modal
interactions. To address this task, we propose TubeDETR, a transformer-based
architecture inspired by the recent success of such models for text-conditioned
object detection. Our model notably includes: (i) an efficient video and text
encoder that models spatial multi-modal interactions over sparsely sampled
frames and (ii) a space-time decoder that jointly performs spatio-temporal
localization. We demonstrate the advantage of our proposed components through
an extensive ablation study. We also evaluate our full approach on the
spatio-temporal video grounding task and demonstrate improvements over the
state of the art on the challenging VidSTG and HC-STVG benchmarks. Code and
trained models are publicly available at
https://antoyang.github.io/tubedetr.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models. (arXiv:2203.16474v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16474">
<div class="article-summary-box-inner">
<span><p>Eye tracking data during reading is a useful source of information to
understand the cognitive processes that take place during language
comprehension processes. Different languages account for different brain
triggers , however there seems to be some uniform indicators. In this paper, we
describe our submission to the CMCL 2022 shared task on predicting human
reading patterns for multi-lingual dataset. Our model uses text representations
from transformers and some hand engineered features with a regression layer on
top to predict statistical measures of mean and standard deviation for 2 main
eye-tracking features. We train an end to end model to extract meaningful
information from different languages and test our model on two seperate
datasets. We compare different transformer models and show ablation studies
affecting model performance. Our final submission ranked 4th place for
SubTask-1 and 1st place for SubTask-2 for the shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16487">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Generalized Aggressive Decoding (GAD) -- a novel
approach to accelerating autoregressive translation with no quality loss,
through the collaboration of autoregressive and non-autoregressive translation
(NAT) of the Transformer. At each decoding iteration, GAD aggressively decodes
a number of tokens in parallel as a draft through NAT and then verifies them in
the autoregressive manner, where only the tokens that pass the verification are
kept as decoded tokens. GAD can achieve the same performance as autoregressive
translation but perform much more efficiently because both NAT drafting and
autoregressive verification are fast due to parallel computing. We conduct
experiments in the WMT14 English-German translation task and confirm that the
vanilla GAD yields exactly the same results as greedy decoding with about 3x
speedup, and that its variant (GAD++) with an advanced verification strategy
not only outperforms the greedy translation and even achieves the comparable
translation quality with the beam search result, but also further improves the
decoding speed, resulting in an around 5x speedup over autoregressive
translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Spoken Dialogue Language Modeling. (arXiv:2203.16502v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16502">
<div class="article-summary-box-inner">
<span><p>We introduce dGSLM, the first "textless" model able to generate audio samples
of naturalistic spoken dialogues. It uses recent work on unsupervised spoken
unit discovery coupled with a dual-tower transformer architecture with
cross-attention trained on 2000 hours of two-channel raw conversational audio
(Fisher dataset) without any text or labels. It is able to generate speech,
laughter and other paralinguistic signals in the two channels simultaneously
and reproduces naturalistic turn taking. Generation samples can be found at:
https://speechbot.github.io/dgslm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vakyansh: ASR Toolkit for Low Resource Indic languages. (arXiv:2203.16512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16512">
<div class="article-summary-box-inner">
<span><p>We present Vakyansh, an end to end toolkit for Speech Recognition in Indic
languages. India is home to almost 121 languages and around 125 crore speakers.
Yet most of the languages are low resource in terms of data and pretrained
models. Through Vakyansh, we introduce automatic data pipelines for data
creation, model training, model evaluation and deployment. We create 14,000
hours of speech data in 23 Indic languages and train wav2vec 2.0 based
pretrained models. These pretrained models are then finetuned to create state
of the art speech recognition models for 18 Indic languages which are followed
by language models and punctuation restoration models. We open source all these
resources with a mission that this will inspire the speech community to develop
speech first applications using our ASR models in Indic languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotional Conversation Generation with Heterogeneous Graph Neural Network. (arXiv:2012.04882v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04882">
<div class="article-summary-box-inner">
<span><p>The successful emotional conversation system depends on sufficient perception
and appropriate expression of emotions. In a real-life conversation, humans
firstly instinctively perceive emotions from multi-source information,
including the emotion flow hidden in dialogue history, facial expressions,
audio, and personalities of speakers. Then, they convey suitable emotions
according to their personalities, but these multiple types of information are
insufficiently exploited in emotional conversation fields. To address this
issue, in this paper, we propose a heterogeneous graph-based model for
emotional conversation generation. Firstly, we design a Heterogeneous
Graph-Based Encoder to represent the conversation content (i.e., the dialogue
history, its emotion flow, facial expressions, audio, and speakers'
personalities) with a heterogeneous graph neural network, and then predict
suitable emotions for feedback. Secondly, we employ an
Emotion-Personality-Aware Decoder to generate a response relevant to the
conversation context as well as with appropriate emotions, through taking the
encoded graph representations, the predicted emotions by the encoder and the
personality of the current speaker as inputs. Experiments on both automatic and
human evaluation show that our method can effectively perceive emotions from
multi-source knowledge and generate a satisfactory response. Furthermore, based
on the up-to-date text generator BART, our model still can achieve consistent
improvement, which significantly outperforms some existing state-of-the-art
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10407">
<div class="article-summary-box-inner">
<span><p>The ability to quickly learn from a small quantity oftraining data widens the
range of machine learning applications. In this paper, we propose a
data-efficient image captioning model, VisualGPT, which leverages the
linguistic knowledge from a large pretrained language model(LM). A crucial
challenge is to balance between the use of visual information in the image and
prior linguistic knowledge acquired from pretraining. We designed a novel
self-resurrecting encoder-decoder attention mechanism to quickly adapt the
pretrained LM as the language decoder ona small amount of in-domain training
data. The proposed self-resurrecting activation unit produces sparse
activations but has reduced susceptibility to zero gradients. We train the
proposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual
Captions training data. Under these conditions, we outperform the best baseline
model by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual
Captions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,
a medical report generation dataset. To the best of our knowledge, this is the
first work that improves data efficiency of image captioning by utilizing LM
pretrained on unimodal data. Our code is available at:
https://github.com/Vision-CAIR/VisualGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey. (arXiv:2105.04387v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04387">
<div class="article-summary-box-inner">
<span><p>Dialogue systems are a popular natural language processing (NLP) task as it
is promising in real-life applications. It is also a complicated task since
many NLP tasks deserving study are involved. As a result, a multitude of novel
works on this task are carried out, and most of them are deep learning based
due to the outstanding performance. In this survey, we mainly focus on the deep
learning based dialogue systems. We comprehensively review state-of-the-art
research outcomes in dialogue systems and analyze them from two angles: model
type and system type. Specifically, from the angle of model type, we discuss
the principles, characteristics, and applications of different models that are
widely used in dialogue systems. This will help researchers acquaint these
models and see how they are applied in state-of-the-art frameworks, which is
rather helpful when designing a new dialogue system. From the angle of system
type, we discuss task-oriented and open-domain dialogue systems as two streams
of research, providing insight into the hot topics related. Furthermore, we
comprehensively review the evaluation methods and datasets for dialogue systems
to pave the way for future research. Finally, some possible research trends are
identified based on the recent research outcomes. To the best of our knowledge,
this survey is the most comprehensive and up-to-date one at present for deep
learning based dialogue systems, extensively covering the popular techniques.
We speculate that this work is a good starting point for academics who are new
to the dialogue systems or those who want to quickly grasp up-to-date
techniques in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Corpus-based Open-Domain Event Type Induction. (arXiv:2109.03322v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03322">
<div class="article-summary-box-inner">
<span><p>Traditional event extraction methods require predefined event types and their
corresponding annotations to learn event extractors. These prerequisites are
often hard to be satisfied in real-world applications. This work presents a
corpus-based open-domain event type induction method that automatically
discovers a set of event types from a given corpus. As events of the same type
could be expressed in multiple ways, we propose to represent each event type as
a cluster of &lt;predicate sense, object head&gt; pairs. Specifically, our method (1)
selects salient predicates and object heads, (2) disambiguates predicate senses
using only a verb sense dictionary, and (3) obtains event types by jointly
embedding and clustering &lt;predicate sense, object head&gt; pairs in a latent
spherical space. Our experiments, on three datasets from different domains,
show our method can discover salient and high-quality event types, according to
both automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTC Variations Through New WFST Topologies. (arXiv:2110.03098v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03098">
<div class="article-summary-box-inner">
<span><p>This paper presents novel Weighted Finite-State Transducer (WFST) topologies
to implement Connectionist Temporal Classification (CTC)-like algorithms for
automatic speech recognition. Three new CTC variants are proposed: (1) the
"compact-CTC", in which direct transitions between units are replaced with
&lt;epsilon&gt; back-off transitions; (2) the "minimal-CTC", that only adds &lt;blank&gt;
self-loops when used in WFST-composition; and (3) the "selfless-CTC" variants,
which disallows self-loop for non-blank units. Compact-CTC allows for 1.5 times
smaller WFST decoding graphs and reduces memory consumption by two times when
training CTC models with the LF-MMI objective without hurting the recognition
accuracy. Minimal-CTC reduces graph size and memory consumption by two and four
times for the cost of a small accuracy drop. Using selfless-CTC can improve the
accuracy for wide context window models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Describe Solutions for Bug Reports Based on Developer Discussions. (arXiv:2110.04353v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04353">
<div class="article-summary-box-inner">
<span><p>When a software bug is reported, developers engage in a discussion to
collaboratively resolve it. While the solution is likely formulated within the
discussion, it is often buried in a large amount of text, making it difficult
to comprehend and delaying its implementation. To expedite bug resolution, we
propose generating a concise natural language description of the solution by
synthesizing relevant content within the discussion, which encompasses both
natural language and source code. We build a corpus for this task using a novel
technique for obtaining noisy supervision from repository changes linked to bug
reports, with which we establish benchmarks. We also design two systems for
generating a description during an ongoing discussion by classifying when
sufficient context for performing the task emerges in real-time. With automated
and human evaluation, we find this task to form an ideal testbed for complex
reasoning in long, bimodal dialogue context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06537">
<div class="article-summary-box-inner">
<span><p>The conventional wisdom behind learning deep classification models is to
focus on bad-classified examples and ignore well-classified examples that are
far from the decision boundary. For instance, when training with cross-entropy
loss, examples with higher likelihoods (i.e., well-classified examples)
contribute smaller gradients in back-propagation. However, we theoretically
show that this common practice hinders representation learning, energy
optimization, and margin growth. To counteract this deficiency, we propose to
reward well-classified examples with additive bonuses to revive their
contribution to the learning process. This counterexample theoretically
addresses these three issues. We empirically support this claim by directly
verifying the theoretical results or significant performance improvement with
our counterexample on diverse tasks, including image classification, graph
classification, and machine translation. Furthermore, this paper shows that we
can deal with complex scenarios, such as imbalanced classification, OOD
detection, and applications under adversarial attacks because our idea can
solve these three issues. Code is available at:
https://github.com/lancopku/well-classified-examples-are-underestimated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-guided Counterfactual Generation for QA. (arXiv:2110.07596v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07596">
<div class="article-summary-box-inner">
<span><p>Deep NLP models have been shown to learn spurious correlations, leaving them
brittle to input perturbations. Recent work has shown that counterfactual or
contrastive data -- i.e. minimally perturbed inputs -- can reveal these
weaknesses, and that data augmentation using counterfactuals can help
ameliorate them. Proposed techniques for generating counterfactuals rely on
human annotations, perturbations based on simple heuristics, and meaning
representation frameworks. We focus on the task of creating counterfactuals for
question answering, which presents unique challenges related to world
knowledge, semantic diversity, and answerability. To address these challenges,
we develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual
evaluation and training data with minimal human supervision. Using an
open-domain QA framework and question generation model trained on original task
data, we create counterfactuals that are fluent, semantically diverse, and
automatically labeled. Data augmentation with RGF counterfactuals improves
performance on out-of-domain and challenging evaluation sets over and above
existing methods, in both the reading comprehension and open-domain QA
settings. Moreover, we find that RGF data leads to significant improvements in
a model's robustness to local perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models. (arXiv:2110.08151v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08151">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown that multilingual pretrained language models can be
effectively improved with cross-lingual alignment information from Wikipedia
entities. However, existing methods only exploit entity information in
pretraining and do not explicitly use entities in downstream tasks. In this
study, we explore the effectiveness of leveraging entity representations for
downstream cross-lingual tasks. We train a multilingual language model with 24
languages with entity representations and show the model consistently
outperforms word-based pretrained models in various cross-lingual transfer
tasks. We also analyze the model and the key insight is that incorporating
entity representations into the input allows us to extract more
language-agnostic features. We also evaluate the model with a multilingual
cloze prompt task with the mLAMA dataset. We show that entity-based prompt
elicits correct factual knowledge more likely than using only word
representations. Our source code and pretrained models are available at
https://github.com/studio-ousia/luke.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimum Description Length Recurrent Neural Networks. (arXiv:2111.00600v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00600">
<div class="article-summary-box-inner">
<span><p>We train neural networks to optimize a Minimum Description Length score,
i.e., to balance between the complexity of the network and its accuracy at a
task. We show that networks optimizing this objective function master tasks
involving memory challenges and go beyond context-free languages. These
learners master languages such as $a^nb^n$, $a^nb^nc^n$, $a^nb^{2n}$,
$a^nb^mc^{n+m}$, and they perform addition. Moreover, they often do so with
100% accuracy. The networks are small, and their inner workings are
transparent. We thus provide formal proofs that their perfect accuracy holds
not only on a given test set, but for any input sequence. To our knowledge, no
other connectionist model has been shown to capture the underlying grammars for
these languages in full generality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLAVA: A Foundational Language And Vision Alignment Model. (arXiv:2112.04482v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04482">
<div class="article-summary-box-inner">
<span><p>State-of-the-art vision and vision-and-language models rely on large-scale
visio-linguistic pretraining for obtaining good performance on a variety of
downstream tasks. Generally, such models are often either cross-modal
(contrastive) or multi-modal (with earlier fusion) but not both; and they often
only target specific modalities or tasks. A promising direction would be to use
a single holistic universal model, as a "foundation", that targets all
modalities at once -- a true vision and language foundation model should be
good at vision tasks, language tasks, and cross- and multi-modal vision and
language tasks. We introduce FLAVA as such a model and demonstrate impressive
performance on a wide range of 35 tasks spanning these target modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforced Abstractive Summarization with Adaptive Length Controlling. (arXiv:2112.07534v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07534">
<div class="article-summary-box-inner">
<span><p>Document summarization, as a fundamental task in natural language generation,
aims to generate a short and coherent summary for a given document.
Controllable summarization, especially of the length, is an important issue for
some practical applications, especially how to trade-off the length constraint
and information integrity. In this paper, we propose an \textbf{A}daptive
\textbf{L}ength \textbf{C}ontrolling \textbf{O}ptimization (\textbf{ALCO})
method to leverage two-stage abstractive summarization model via reinforcement
learning. ALCO incorporates length constraint into the stage of sentence
extraction to penalize the overlength extracted sentences. Meanwhile, a
saliency estimation mechanism is designed to preserve the salient information
in the generated sentences. A series of experiments have been conducted on a
wildly-used benchmark dataset \textit{CNN/Daily Mail}. The results have shown
that ALCO performs better than the popular baselines in terms of length
controllability and content preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcing Semantic-Symmetry for Document Summarization. (arXiv:2112.07583v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07583">
<div class="article-summary-box-inner">
<span><p>Document summarization condenses a long document into a short version with
salient information and accurate semantic descriptions. The main issue is how
to make the output summary semantically consistent with the input document. To
reach this goal, recently, researchers have focused on supervised end-to-end
hybrid approaches, which contain an extractor module and abstractor module.
Among them, the extractor identifies the salient sentences from the input
document, and the abstractor generates a summary from the salient sentences.
This model successfully keeps the consistency between the generated summary and
the reference summary via various strategies (e.g., reinforcement learning).
There are two semantic gaps when training the hybrid model (one is between
document and extracted sentences, and the other is between extracted sentences
and summary). However, they are not explicitly considered in the existing
methods, which usually results in a semantic bias of summary. To mitigate the
above issue, in this paper, a new \textbf{r}einforcing
s\textbf{e}mantic-\textbf{sy}mmetry learning \textbf{m}odel is proposed for
document summarization (\textbf{ReSyM}). ReSyM introduces a
semantic-consistency reward in the extractor to bridge the first gap. A
semantic dual-reward is designed to bridge the second gap in the abstractor.
The whole document summarization process is implemented via reinforcement
learning with a hybrid reward mechanism (combining the above two rewards).
Moreover, a comprehensive sentence representation learning method is presented
to sufficiently capture the information from the original document. A series of
experiments have been conducted on two wildly used benchmark datasets CNN/Daily
Mail and BigPatent. The results have shown the superiority of ReSyM by
comparing it with the state-of-the-art baselines in terms of various evaluation
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic-Aware Encoding for Extractive Summarization. (arXiv:2112.09572v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09572">
<div class="article-summary-box-inner">
<span><p>Document summarization provides an instrument for faster understanding the
collection of text documents and has several real-life applications. With the
growth of online text data, numerous summarization models have been proposed
recently. The Sequence-to-Sequence (Seq2Seq) based neural summarization model
is the most widely used in the summarization field due to its high performance.
This is because semantic information and structure information in the text is
adequately considered when encoding. However, the existing extractive
summarization models pay little attention to and use the central topic
information to assist the generation of summaries, which leads to models not
ensuring the generated summary under the primary topic. A lengthy document can
span several topics, and a single summary cannot do justice to all the topics.
Therefore, the key to generating a high-quality summary is determining the
central topic and building a summary based on it, especially for a long
document. We propose a topic-aware encoding for document summarization to deal
with this issue. This model effectively combines syntactic-level and
topic-level information to build a comprehensive sentence representation.
Specifically, a neural topic model is added in the neural-based sentence-level
representation learning to adequately consider the central topic information
for capturing the critical content in the original document. The experimental
results on three public datasets show that our model outperforms the
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relationship extraction for knowledge graph creation from biomedical literature. (arXiv:2201.01647v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01647">
<div class="article-summary-box-inner">
<span><p>Biomedical research is growing at such an exponential pace that scientists,
researchers, and practitioners are no more able to cope with the amount of
published literature in the domain. The knowledge presented in the literature
needs to be systematized in such a way that claims and hypotheses can be easily
found, accessed, and validated. Knowledge graphs can provide such a framework
for semantic knowledge representation from literature. However, in order to
build a knowledge graph, it is necessary to extract knowledge as relationships
between biomedical entities and normalize both entities and relationship types.
In this paper, we present and compare a few rule-based and machine
learning-based (Naive Bayes, Random Forests as examples of traditional machine
learning methods and DistilBERT and T5-based models as examples of modern deep
learning transformers) methods for scalable relationship extraction from
biomedical literature, and for the integration into the knowledge graphs. We
examine how resilient are these various methods to unbalanced and fairly small
datasets, showing that transformer-based models handle well both small
datasets, due to pre-training on large C4 dataset, as well as unbalanced data.
The best performing model was the DistilBERT-based model fine-tuned on balanced
data, with a reported F1-score of 0.89.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Effectiveness of Pinyin-Character Dual-Decoding for End-to-End Mandarin Chinese ASR. (arXiv:2201.10792v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10792">
<div class="article-summary-box-inner">
<span><p>End-to-end automatic speech recognition (ASR) has achieved promising results.
However, most existing end-to-end ASR methods neglect the use of specific
language characteristics. For Mandarin Chinese ASR tasks, there exist mutual
promotion relationship between Pinyin and Character where Chinese characters
can be romanized by Pinyin. Based on the above intuition, we first investigate
types of end-to-end encoder-decoder based models in the single-input
dual-output (SIDO) multi-task framework, after which a novel asynchronous
decoding with fuzzy Pinyin sampling method is proposed according to the
one-to-one correspondence characteristics between Pinyin and Character.
Furthermore, we proposed a two-stage training strategy to make training more
stable and converge faster. The results on the test sets of AISHELL-1 dataset
show that the proposed enhanced dual-decoder model without a language model is
improved by a big margin compared to strong baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media. (arXiv:2203.04356v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04356">
<div class="article-summary-box-inner">
<span><p>Social media is a breeding ground for threat narratives and related
conspiracy theories. In these, an outside group threatens the integrity of an
inside group, leading to the emergence of sharply defined group identities:
Insiders -- agents with whom the authors identify and Outsiders -- agents who
threaten the insiders. Inferring the members of these groups constitutes a
challenging new NLP task: (i) Information is distributed over many
poorly-constructed posts; (ii) Threats and threat agents are highly contextual,
with the same post potentially having multiple agents assigned to membership in
either group; (iii) An agent's identity is often implicit and transitive; and
(iv) Phrases used to imply Outsider status often do not follow common negative
sentiment patterns. To address these challenges, we define a novel
Insider-Outsider classification task. Because we are not aware of any
appropriate existing datasets or attendant models, we introduce a labeled
dataset (CT5K) and design a model (NP2IO) to address this task. NP2IO leverages
pretrained language modeling to classify Insiders and Outsiders. NP2IO is shown
to be robust, generalizing to noun phrases not seen during training, and
exceeding the performance of non-trivial baseline models by $20\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation. (arXiv:2203.06386v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06386">
<div class="article-summary-box-inner">
<span><p>The recent large-scale vision-language pre-training (VLP) of dual-stream
architectures (e.g., CLIP) with a tremendous amount of image-text pair data,
has shown its superiority on various multimodal alignment tasks. Despite its
success, the resulting models are not capable of multimodal generative tasks
due to the weak text encoder. To tackle this problem, we propose to augment the
dual-stream VLP model with a textual pre-trained language model (PLM) via
vision-language knowledge distillation (VLKD), enabling the capability for
multimodal generation. VLKD is pretty data- and computation-efficient compared
to the pre-training from scratch. Experimental results show that the resulting
model has strong zero-shot performance on multimodal generation tasks, such as
open-ended visual question answering and image captioning. For example, it
achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous
state-of-the-art zero-shot model with $7\times$ fewer parameters. Furthermore,
the original textual language understanding and generation ability of the PLM
is maintained after VLKD, which makes our model versatile for both multimodal
and unimodal tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data. (arXiv:2203.11476v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11476">
<div class="article-summary-box-inner">
<span><p>Human speakers encode information into raw speech which is then decoded by
the listeners. This complex relationship between encoding (production) and
decoding (perception) is often modeled separately. Here, we test how encoding
and decoding of lexical semantic information can emerge automatically from raw
speech in unsupervised generative deep convolutional networks that combine the
production and perception principles of speech. We introduce, to our knowledge,
the most challenging objective in unsupervised lexical learning: a network that
must learn unique representations for lexical items with no direct access to
training data. We train several models (ciwGAN and fiwGAN <a href="/abs/2006.02951">arXiv:2006.02951</a>) and
test how the networks classify acoustic lexical items in unobserved test data.
Strong evidence in favor of lexical learning and a causal relationship between
latent codes and meaningful sublexical units emerge. The architecture that
combines the production and perception principles is thus able to learn to
decode unique information from raw acoustic data without accessing real
training data directly. We propose a technique to explore lexical (holistic)
and sublexical (featural) learned representations in the classifier network.
The results bear implications for unsupervised speech technology, as well as
for unsupervised semantic modeling as language models increasingly bypass text
and operate from raw acoustics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11480">
<div class="article-summary-box-inner">
<span><p>Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual CheckList: Generation and Evaluation. (arXiv:2203.12865v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12865">
<div class="article-summary-box-inner">
<span><p>The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation
of NLP systems has revealed high failure rates for basic capabilities for
multiple state-of-the-art and commercial models. However, the CheckList
creation process is manual which creates a bottleneck towards creation of
multilingual CheckLists catering 100s of languages. In this work, we explore
multiple approaches to generate and evaluate the quality of Multilingual
CheckList. We device an algorithm -- Automated Multilingual Checklist
Generation (AMCG) for automatically transferring a CheckList from a source to a
target language that relies on a reasonable machine translation system. We then
compare the CheckList generated by AMCG with CheckLists generated with
different levels of human intervention. Through in-depth crosslingual
experiments between English and Hindi, and broad multilingual experiments
spanning 11 languages, we show that the automatic approach can provide accurate
estimates of failure rates of a model across capabilities, as would a
human-verified CheckList, and better than CheckLists generated by humans from
scratch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion. (arXiv:2203.13224v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13224">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have recently been shown to generate more factual
responses by employing modularity (Zhou et al., 2021) in combination with
retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et
al. (2021) to include internet search as a module. Our SeeKeR (Search
engine-&gt;Knowledge-&gt;Response) method thus applies a single LM to three modular
tasks in succession: search, generating knowledge, and generating a final
response. We show that, when using SeeKeR as a dialogue model, it outperforms
the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain
knowledge-grounded conversations for the same number of parameters, in terms of
consistency, knowledge and per-turn engagingness. SeeKeR applied to topical
prompt completions as a standard language model outperforms GPT2 (Radford et
al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,
despite GPT3 being a vastly larger model. Our code and models are made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Conversational Paradigm for Program Synthesis. (arXiv:2203.13474v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13474">
<div class="article-summary-box-inner">
<span><p>Program synthesis strives to generate a computer program as a solution to a
given problem specification. We propose a conversational program synthesis
approach via large language models, which addresses the challenges of searching
over a vast program space and user intent specification faced in prior
approaches. Our new approach casts the process of writing a specification and
program as a multi-turn conversation between a user and a system. It treats
program synthesis as a sequence prediction problem, in which the specification
is expressed in natural language and the desired program is conditionally
sampled. We train a family of large language models, called CodeGen, on natural
language and programming language data. With weak supervision in the data and
the scaling up of data size and model size, conversational capacities emerge
from the simple autoregressive language modeling. To study the model behavior
on conversational program synthesis, we develop a multi-turn programming
benchmark (MTPB), where solving each problem requires multi-step synthesis via
multi-turn conversation between the user and the model. Our findings show the
emergence of conversational capabilities and the effectiveness of the proposed
conversational program synthesis paradigm. In addition, our model CodeGen (with
up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the
HumanEval benchmark. We make the training library JaxFormer including
checkpoints available as open source contribution:
https://github.com/salesforce/CodeGen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Roadmap for Big Model. (arXiv:2203.14101v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14101">
<div class="article-summary-box-inner">
<span><p>With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&amp;Interpretability, Commonsense Reasoning, Reliability&amp;Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation. (arXiv:2203.15319v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15319">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation (NMT) has reached a level of maturity to be
recognized as the premier method for the translation between different
languages and aroused interest in different research areas, including software
engineering. A key step to validate the robustness of the NMT models consists
in evaluating the performance of the models on adversarial inputs, i.e., inputs
obtained from the original ones by adding small amounts of perturbation.
However, when dealing with the specific task of the code generation (i.e., the
generation of code starting from a description in natural language), it has not
yet been defined an approach to validate the robustness of the NMT models. In
this work, we address the problem by identifying a set of perturbations and
metrics tailored for the robustness assessment of such models. We present a
preliminary experimental evaluation, showing what type of perturbations affect
the model the most and deriving useful insights for future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heuristic-based Inter-training to Improve Few-shot Multi-perspective Dialog Summarization. (arXiv:2203.15590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15590">
<div class="article-summary-box-inner">
<span><p>Many organizations require their customer-care agents to manually summarize
their conversations with customers. These summaries are vital for decision
making purposes of the organizations. The perspective of the summary that is
required to be created depends on the application of the summaries. With this
work, we study the multi-perspective summarization of customer-care
conversations between support agents and customers. We observe that there are
different heuristics that are associated with summaries of different
perspectives, and explore these heuristics to create weak-labeled data for
intermediate training of the models before fine-tuning with scarce human
annotated summaries. Most importantly, we show that our approach supports
models to generate multi-perspective summaries with a very small amount of
annotated data. For example, our approach achieves 94\% of the performance
(Rouge-2) of a model trained with the original data, by training only with 7\%
of the original data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlloST: Low-resource Speech Translation without Source Transcription. (arXiv:2105.00171v3 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00171">
<div class="article-summary-box-inner">
<span><p>The end-to-end architecture has made promising progress in speech translation
(ST). However, the ST task is still challenging under low-resource conditions.
Most ST models have shown unsatisfactory results, especially in the absence of
word information from the source speech utterance. In this study, we survey
methods to improve ST performance without using source transcription, and
propose a learning framework that utilizes a language-independent universal
phone recognizer. The framework is based on an attention-based
sequence-to-sequence model, where the encoder generates the phonetic embeddings
and phone-aware acoustic representations, and the decoder controls the fusion
of the two embedding streams to produce the target token sequence. In addition
to investigating different fusion strategies, we explore the specific usage of
byte pair encoding (BPE), which compresses a phone sequence into a
syllable-like segmented sequence. Due to the conversion of symbols, a segmented
sequence represents not only pronunciation but also language-dependent
information lacking in phones. Experiments conducted on the Fisher
Spanish-English and Taigi-Mandarin drama corpora show that our method
outperforms the conformer-based baseline, and the performance is close to that
of the existing best method using source transcription.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">An EEG-Based Multi-Modal Emotion Database with Both Posed and Authentic Facial Actions for Emotion Analysis. (arXiv:2203.15829v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15829">
<div class="article-summary-box-inner">
<span><p>Emotion is an experience associated with a particular pattern of
physiological activity along with different physiological, behavioral and
cognitive changes. One behavioral change is facial expression, which has been
studied extensively over the past few decades. Facial behavior varies with a
person's emotion according to differences in terms of culture, personality,
age, context, and environment. In recent years, physiological activities have
been used to study emotional responses. A typical signal is the
electroencephalogram (EEG), which measures brain activity. Most of existing
EEG-based emotion analysis has overlooked the role of facial expression
changes. There exits little research on the relationship between facial
behavior and brain signals due to the lack of dataset measuring both EEG and
facial action signals simultaneously. To address this problem, we propose to
develop a new database by collecting facial expressions, action units, and EEGs
simultaneously. We recorded the EEGs and face videos of both posed facial
actions and spontaneous expressions from 29 participants with different ages,
genders, ethnic backgrounds. Differing from existing approaches, we designed a
protocol to capture the EEG signals by evoking participants' individual action
units explicitly. We also investigated the relation between the EEG signals and
facial action units. As a baseline, the database has been evaluated through the
experiments on both posed and spontaneous emotion recognition with images
alone, EEG alone, and EEG fused with images, respectively. The database will be
released to the research community to advance the state of the art for
automatic emotion recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACR Loss: Adaptive Coordinate-based Regression Loss for Face Alignment. (arXiv:2203.15835v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15835">
<div class="article-summary-box-inner">
<span><p>Although deep neural networks have achieved reasonable accuracy in solving
face alignment, it is still a challenging task, specifically when we deal with
facial images, under occlusion, or extreme head poses. Heatmap-based Regression
(HBR) and Coordinate-based Regression (CBR) are among the two mainly used
methods for face alignment. CBR methods require less computer memory, though
their performance is less than HBR methods. In this paper, we propose an
Adaptive Coordinate-based Regression (ACR) loss to improve the accuracy of CBR
for face alignment. Inspired by the Active Shape Model (ASM), we generate
Smooth-Face objects, a set of facial landmark points with less variations
compared to the ground truth landmark points. We then introduce a method to
estimate the level of difficulty in predicting each landmark point for the
network by comparing the distribution of the ground truth landmark points and
the corresponding Smooth-Face objects. Our proposed ACR Loss can adaptively
modify its curvature and the influence of the loss based on the difficulty
level of predicting each landmark point in a face. Accordingly, the ACR Loss
guides the network toward challenging points than easier points, which improves
the accuracy of the face alignment task. Our extensive evaluation shows the
capabilities of the proposed ACR Loss in predicting facial landmark points in
various facial images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VPTR: Efficient Transformers for Video Prediction. (arXiv:2203.15836v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15836">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new Transformer block for video future frames
prediction based on an efficient local spatial-temporal separation attention
mechanism. Based on this new Transformer block, a fully autoregressive video
future frames prediction Transformer is proposed. In addition, a
non-autoregressive video prediction Transformer is also proposed to increase
the inference speed and reduce the accumulated inference errors of its
autoregressive counterpart. In order to avoid the prediction of very similar
future frames, a contrastive feature loss is applied to maximize the mutual
information between predicted and ground-truth future frame features. This work
is the first that makes a formal comparison of the two types of attention-based
video future frames prediction models over different scenarios. The proposed
models reach a performance competitive with more complex state-of-the-art
models. The source code is available at \emph{https://github.com/XiYe20/VPTR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NNLander-VeriF: A Neural Network Formal Verification Framework for Vision-Based Autonomous Aircraft Landing. (arXiv:2203.15841v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15841">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the problem of formally verifying a Neural Network
(NN) based autonomous landing system. In such a system, a NN controller
processes images from a camera to guide the aircraft while approaching the
runway. A central challenge for the safety and liveness verification of
vision-based closed-loop systems is the lack of mathematical models that
captures the relation between the system states (e.g., position of the
aircraft) and the images processed by the vision-based NN controller. Another
challenge is the limited abilities of state-of-the-art NN model checkers. Such
model checkers can reason only about simple input-output robustness properties
of neural networks. This limitation creates a gap between the NN model checker
abilities and the need to verify a closed-loop system while considering the
aircraft dynamics, the perception components, and the NN controller. To this
end, this paper presents NNLander-VeriF, a framework to verify vision-based NN
controllers used for autonomous landing. NNLander-VeriF addresses the
challenges above by exploiting geometric models of perspective cameras to
obtain a mathematical model that captures the relation between the aircraft
states and the inputs to the NN controller. By converting this model into a NN
(with manually assigned weights) and composing it with the NN controller, one
can capture the relation between aircraft states and control actions using one
augmented NN. Such an augmented NN model leads to a natural encoding of the
closed-loop verification into several NN robustness queries, which
state-of-the-art NN model checkers can handle. Finally, we evaluate our
framework to formally verify the properties of a trained NN and we show its
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Inertial Localization. (arXiv:2203.15851v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15851">
<div class="article-summary-box-inner">
<span><p>This paper proposes the inertial localization problem, the task of estimating
the absolute location from a sequence of inertial sensor measurements. This is
an exciting and unexplored area of indoor localization research, where we
present a rich dataset with 53 hours of inertial sensor data and the associated
ground truth locations. We developed a solution, dubbed neural inertial
localization (NILoc) which 1) uses a neural inertial navigation technique to
turn inertial sensor history to a sequence of velocity vectors; then 2) employs
a transformer-based neural architecture to find the device location from the
sequence of velocities. We only use an IMU sensor, which is energy efficient
and privacy preserving compared to WiFi, cameras, and other data sources. Our
approach is significantly faster and achieves competitive results even compared
with state-of-the-art methods that require a floorplan and run 20 to 30 times
slower. We share our code, model and data at https://sachini.github.io/niloc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OdontoAI: A human-in-the-loop labeled data set and an online platform to boost research on dental panoramic radiographs. (arXiv:2203.15856v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15856">
<div class="article-summary-box-inner">
<span><p>Deep learning has remarkably advanced in the last few years, supported by
large labeled data sets. These data sets are precious yet scarce because of the
time-consuming labeling procedures, discouraging researchers from producing
them. This scarcity is especially true in dentistry, where deep learning
applications are still in an embryonic stage. Motivated by this background, we
address in this study the construction of a public data set of dental panoramic
radiographs. Our objects of interest are the teeth, which are segmented and
numbered, as they are the primary targets for dentists when screening a
panoramic radiograph. We benefited from the human-in-the-loop (HITL) concept to
expedite the labeling procedure, using predictions from deep neural networks as
provisional labels, later verified by human annotators. All the gathering and
labeling procedures of this novel data set is thoroughly analyzed. The results
were consistent and behaved as expected: At each HITL iteration, the model
predictions improved. Our results demonstrated a 51% labeling time reduction
using HITL, saving us more than 390 continuous working hours. In a novel online
platform, called OdontoAI, created to work as task central for this novel data
set, we released 4,000 images, from which 2,000 have their labels publicly
available for model fitting. The labels of the other 2,000 images are private
and used for model evaluation considering instance and semantic segmentation
and numbering. To the best of our knowledge, this is the largest-scale publicly
available data set for panoramic radiographs, and the OdontoAI is the first
platform of its kind in dentistry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image Caption Generation Models. (arXiv:2203.15859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15859">
<div class="article-summary-box-inner">
<span><p>Neural image caption generation (NICG) models have received massive attention
from the research community due to their excellent performance in visual
understanding. Existing work focuses on improving NICG model accuracy while
efficiency is less explored. However, many real-world applications require
real-time feedback, which highly relies on the efficiency of NICG models.
Recent research observed that the efficiency of NICG models could vary for
different inputs. This observation brings in a new attack surface of NICG
models, i.e., An adversary might be able to slightly change inputs to cause the
NICG models to consume more computational resources. To further understand such
efficiency-oriented threats, we propose a new attack approach, NICGSlowDown, to
evaluate the efficiency robustness of NICG models. Our experimental results
show that NICGSlowDown can generate images with human-unnoticeable
perturbations that will increase the NICG model latency up to 483.86%. We hope
this research could raise the community's concern about the efficiency
robustness of NICG models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation. (arXiv:2203.15865v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15865">
<div class="article-summary-box-inner">
<span><p>Supervised approaches to 3D pose estimation from single images are remarkably
effective when labeled data is abundant. Therefore, much of the recent
attention has shifted towards semi and (or) weakly supervised learning.
Generating an effective form of supervision with little annotations still poses
major challenges in crowded scenes. However, since it is easy to observe a
scene from multiple cameras, we propose to impose multi-view geometrical
constraints by means of a differentiable triangulation and to use it as form of
self-supervision during training when no labels are available. We therefore
train a 2D pose estimator in such a way that its predictions correspond to the
re-projection of the triangulated 3D one and train an auxiliary network on them
to produce the final 3D poses. We complement the triangulation with a weighting
mechanism that nullify the impact of noisy predictions caused by self-occlusion
or occlusion from other subjects. Our experimental results on Human3.6M and
MPI-INF-3DHP substantiate the significance of our weighting strategy where we
obtain state-of-the-art results in the semi and weakly supervised learning
setup. We also contribute a new multi-player sports dataset that features
occlusion, and show the effectiveness of our algorithm over baseline
triangulation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Retrieval from Contextual Descriptions. (arXiv:2203.15867v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15867">
<div class="article-summary-box-inner">
<span><p>The ability to integrate context, including perceptual and temporal cues,
plays a pivotal role in grounding the meaning of a linguistic utterance. In
order to measure to what extent current vision-and-language models master this
ability, we devise a new multimodal challenge, Image Retrieval from Contextual
Descriptions (ImageCoDe). In particular, models are tasked with retrieving the
correct image from a set of 10 minimally contrastive candidates based on a
contextual description. As such, each description contains only the details
that help distinguish between images. Because of this, descriptions tend to be
complex in terms of syntax and discourse and require drawing pragmatic
inferences. Images are sourced from both static pictures and video frames. We
benchmark several state-of-the-art models, including both cross-encoders such
as ViLBERT and bi-encoders such as CLIP, on ImageCoDe. Our results reveal that
these models dramatically lag behind human performance: the best variant
achieves an accuracy of 20.9 on video frames and 59.4 on static pictures,
compared with 90.8 in humans. Furthermore, we experiment with new model
variants that are better equipped to incorporate visual and temporal context
into their representations, which achieve modest gains. Our hope is that
ImageCoDE will foster progress in grounded language understanding by
encouraging models to focus on fine-grained visual differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A deep learning model for burn depth classification using ultrasound imaging. (arXiv:2203.15879v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15879">
<div class="article-summary-box-inner">
<span><p>Identification of burn depth with sufficient accuracy is a challenging
problem. This paper presents a deep convolutional neural network to classify
burn depth based on altered tissue morphology of burned skin manifested as
texture patterns in the ultrasound images. The network first learns a
low-dimensional manifold of the unburned skin images using an encoder-decoder
architecture that reconstructs it from ultrasound images of burned skin. The
encoder is then re-trained to classify burn depths. The encoder-decoder network
is trained using a dataset comprised of B-mode ultrasound images of unburned
and burned ex vivo porcine skin samples. The classifier is developed using
B-mode images of burned in situ skin samples obtained from freshly euthanized
postmortem pigs. The performance metrics obtained from 20-fold cross-validation
show that the model can identify deep-partial thickness burns, which is the
most difficult to diagnose clinically, with 99% accuracy, 98% sensitivity, and
100% specificity. The diagnostic accuracy of the classifier is further
illustrated by the high area under the curve values of 0.99 and 0.95,
respectively, for the receiver operating characteristic and precision-recall
curves. A post hoc explanation indicates that the classifier activates the
discriminative textural features in the B-mode images for burn classification.
The proposed model has the potential for clinical utility in assisting the
clinical assessment of burn depths using a widely available clinical imaging
device.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proactive Image Manipulation Detection. (arXiv:2203.15880v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15880">
<div class="article-summary-box-inner">
<span><p>Image manipulation detection algorithms are often trained to discriminate
between images manipulated with particular Generative Models (GMs) and
genuine/real images, yet generalize poorly to images manipulated with GMs
unseen in the training. Conventional detection algorithms receive an input
image passively. By contrast, we propose a proactive scheme to image
manipulation detection. Our key enabling technique is to estimate a set of
templates which when added onto the real image would lead to more accurate
manipulation detection. That is, a template protected real image, and its
manipulated version, is better discriminated compared to the original real
image vs. its manipulated one. These templates are estimated using certain
constraints based on the desired properties of templates. For image
manipulation detection, our proposed approach outperforms the prior work by an
average precision of 16% for CycleGAN and 32% for GauGAN. Our approach is
generalizable to a variety of GMs showing an improvement over prior work by an
average precision of 10% averaged across 12 GMs. Our code is available at
https://www.github.com/vishal3477/proactive_IMD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Detect Mobile Objects from LiDAR Scans Without Labels. (arXiv:2203.15882v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15882">
<div class="article-summary-box-inner">
<span><p>Current 3D object detectors for autonomous driving are almost entirely
trained on human-annotated data. Although of high quality, the generation of
such data is laborious and costly, restricting them to a few specific locations
and object types. This paper proposes an alternative approach entirely based on
unlabeled data, which can be collected cheaply and in abundance almost
everywhere on earth. Our approach leverages several simple common sense
heuristics to create an initial set of approximate seed labels. For example,
relevant traffic participants are generally not persistent across multiple
traversals of the same route, do not fly, and are never under ground. We
demonstrate that these seed labels are highly effective to bootstrap a
surprisingly accurate detector through repeated self-training without a single
human annotated label.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Equilibrium Assisted Block Sparse Coding of Inter-dependent Signals: Application to Hyperspectral Imaging. (arXiv:2203.15901v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15901">
<div class="article-summary-box-inner">
<span><p>In this study, the problem of computing a sparse representation for datasets
of inter-dependent signals, given a fixed dictionary, is considered. A dataset
of inter-dependent signals is defined as a matrix whose columns demonstrate
strong dependencies. A computational efficient sparse coding optimization
problem is derived by employing regularization terms that are adapted to the
properties of the signals of interest. Exploiting the merits of the learnable
regularization techniques, a neural network is employed to act as structure
prior and reveal the underlying signal interdependencies. To solve the
optimization problem Deep unrolling and Deep equilibrium based algorithms are
developed, forming highly interpretable and concise deep-learning-based
architectures, that process the input dataset in a block-by-block fashion.
Extensive simulation results, in the context of hyperspectral image denoising,
are provided, that demonstrate that the proposed algorithms outperform
significantly other sparse coding approaches and exhibit superior performance
against recent state-of-the-art deep-learning-based denoising models. In a
wider perspective, our work provides a unique bridge between a classic
approach, that is the sparse representation theory, and modern representation
tools that are based on deep learning modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled3D: Learning a 3D Generative Model with Disentangled Geometry and Appearance from Monocular Images. (arXiv:2203.15926v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15926">
<div class="article-summary-box-inner">
<span><p>Learning 3D generative models from a dataset of monocular images enables
self-supervised 3D reasoning and controllable synthesis. State-of-the-art 3D
generative models are GANs which use neural 3D volumetric representations for
synthesis. Images are synthesized by rendering the volumes from a given camera.
These models can disentangle the 3D scene from the camera viewpoint in any
generated image. However, most models do not disentangle other factors of image
formation, such as geometry and appearance. In this paper, we design a 3D GAN
which can learn a disentangled model of objects, just from monocular
observations. Our model can disentangle the geometry and appearance variations
in the scene, i.e., we can independently sample from the geometry and
appearance spaces of the generative model. This is achieved using a novel
non-rigid deformable scene formulation. A 3D volume which represents an object
instance is computed as a non-rigidly deformed canonical 3D volume. Our method
learns the canonical volume, as well as its deformations, jointly during
training. This formulation also helps us improve the disentanglement between
the 3D scene and the camera viewpoints using a novel pose regularization loss
defined on the 3D deformation field. In addition, we further model the inverse
deformations, enabling the computation of dense correspondences between images
generated by our model. Finally, we design an approach to embed real images
into the latent space of our disentangled generative model, enabling editing of
real images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Leaf Segmentation under Complex Lighting Conditions. (arXiv:2203.15943v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15943">
<div class="article-summary-box-inner">
<span><p>As an essential prerequisite task in image-based plant phenotyping, leaf
segmentation has garnered increasing attention in recent years. While
self-supervised learning is emerging as an effective alternative to various
computer vision tasks, its adaptation for image-based plant phenotyping remains
rather unexplored. In this work, we present a self-supervised leaf segmentation
framework consisting of a self-supervised semantic segmentation model, a
color-based leaf segmentation algorithm, and a self-supervised color correction
model. The self-supervised semantic segmentation model groups the semantically
similar pixels by iteratively referring to the self-contained information,
allowing the pixels of the same semantic object to be jointly considered by the
color-based leaf segmentation algorithm for identifying the leaf regions.
Additionally, we propose to use a self-supervised color correction model for
images taken under complex illumination conditions. Experimental results on
datasets of different plant species demonstrate the potential of the proposed
self-supervised framework in achieving effective and generalizable leaf
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Learning Neural Representations from Shadows. (arXiv:2203.15946v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15946">
<div class="article-summary-box-inner">
<span><p>We present a method that learns neural scene representations from only
shadows present in the scene. While traditional shape-from-shadow (SfS)
algorithms reconstruct geometry from shadows, they assume a fixed scanning
setup and fail to generalize to complex scenes. Neural rendering algorithms, on
the other hand, rely on photometric consistency between RGB images but largely
ignore physical cues such as shadows, which have been shown to provide valuable
information about the scene. We observe that shadows are a powerful cue that
can constrain neural scene representations to learn SfS, and even outperform
NeRF to reconstruct otherwise hidden geometry. We propose a graphics-inspired
differentiable approach to render accurate shadows with volumetric rendering,
predicting a shadow map that can be compared to the ground truth shadow. Even
with just binary shadow maps, we show that neural rendering can localize the
object and estimate coarse geometry. Our approach reveals that sparse cues in
images can be used to estimate geometry using differentiable volumetric
rendering. Moreover, our framework is highly generalizable and can work
alongside existing 3D reconstruction techniques that otherwise only use
photometric consistency. Our code is made available in our supplementary
materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-resolution Face Swapping via Latent Semantics Disentanglement. (arXiv:2203.15958v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15958">
<div class="article-summary-box-inner">
<span><p>We present a novel high-resolution face swapping method using the inherent
prior knowledge of a pre-trained GAN model. Although previous research can
leverage generative priors to produce high-resolution results, their quality
can suffer from the entangled semantics of the latent space. We explicitly
disentangle the latent semantics by utilizing the progressive nature of the
generator, deriving structure attributes from the shallow layers and appearance
attributes from the deeper ones. Identity and pose information within the
structure attributes are further separated by introducing a landmark-driven
structure transfer latent direction. The disentangled latent code produces rich
generative features that incorporate feature blending to produce a plausible
swapping result. We further extend our method to video face swapping by
enforcing two spatio-temporal constraints on the latent space and the image
space. Extensive experiments demonstrate that the proposed method outperforms
state-of-the-art image/video face swapping methods in terms of hallucination
quality and consistency. Code can be found at:
https://github.com/cnnlstm/FSLSD_HiRes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Active Speaker Faces for Diarization in TV shows. (arXiv:2203.15961v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15961">
<div class="article-summary-box-inner">
<span><p>Speaker diarization is one of the critical components of computational media
intelligence as it enables a character-level analysis of story portrayals and
media content understanding. Automated audio-based speaker diarization of
entertainment media poses challenges due to the diverse acoustic conditions
present in media content, be it background music, overlapping speakers, or
sound effects. At the same time, speaking faces in the visual modality provide
complementary information and not prone to the errors seen in the audio
modality. In this paper, we address the problem of speaker diarization in TV
shows using the active speaker faces. We perform face clustering on the active
speaker faces and show superior speaker diarization performance compared to the
state-of-the-art audio-based diarization methods. We additionally report a
systematic analysis of the impact of active speaker face detection quality on
the diarization performance. We also observe that a moderately well-performing
active speaker system could outperform the audio-based diarization systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSMNet: Position-aware Stereo Merging Network for Room Layout Estimation. (arXiv:2203.15965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15965">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new deep learning-based method for estimating
room layout given a pair of 360 panoramas. Our system, called Position-aware
Stereo Merging Network or PSMNet, is an end-to-end joint layout-pose estimator.
PSMNet consists of a Stereo Pano Pose (SP2) transformer and a novel
Cross-Perspective Projection (CP2) layer. The stereo-view SP2 transformer is
used to implicitly infer correspondences between views, and can handle noisy
poses. The pose-aware CP2 layer is designed to render features from the
adjacent view to the anchor (reference) view, in order to perform view fusion
and estimate the visible layout. Our experiments and analysis validate our
method, which significantly outperforms the state-of-the-art layout estimators,
especially for large and complex room spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deeply Interleaved Two-Stream Encoder for Referring Video Segmentation. (arXiv:2203.15969v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15969">
<div class="article-summary-box-inner">
<span><p>Referring video segmentation aims to segment the corresponding video object
described by the language expression. To address this task, we first design a
two-stream encoder to extract CNN-based visual features and transformer-based
linguistic features hierarchically, and a vision-language mutual guidance
(VLMG) module is inserted into the encoder multiple times to promote the
hierarchical and progressive fusion of multi-modal features. Compared with the
existing multi-modal fusion methods, this two-stream encoder takes into account
the multi-granularity linguistic context, and realizes the deep interleaving
between modalities with the help of VLGM. In order to promote the temporal
alignment between frames, we further propose a language-guided multi-scale
dynamic filtering (LMDF) module to strengthen the temporal coherence, which
uses the language-guided spatial-temporal features to generate a set of
position-specific dynamic filters to more flexibly and effectively update the
feature of current frame. Extensive experiments on four datasets verify the
effectiveness of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Deep Homography Estimation. (arXiv:2203.15982v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15982">
<div class="article-summary-box-inner">
<span><p>We propose Iterative Homography Network, namely IHN, a new deep homography
estimation architecture. Different from previous works that achieve iterative
refinement by network cascading or untrainable IC-LK iterator, the iterator of
IHN has tied weights and is completely trainable. IHN achieves state-of-the-art
accuracy on several datasets including challenging scenes. We propose 2
versions of IHN: (1) IHN for static scenes, (2) IHN-mov for dynamic scenes with
moving objects. Both versions can be arranged in 1-scale for efficiency or
2-scale for accuracy. We show that the basic 1-scale IHN already outperforms
most of the existing methods. On a variety of datasets, the 2-scale IHN
outperforms all competitors by a large gap. We introduce IHN-mov by producing
an inlier mask to further improve the estimation accuracy of moving-objects
scenes. We experimentally show that the iterative framework of IHN can achieve
95% error reduction while considerably saving network parameters. When
processing sequential image pairs, IHN can achieve 32.7 fps, which is about 8x
the speed of IC-LK iterator. Source code is available at
https://github.com/imdumpl78/IHN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VI-IKD: High-Speed Accurate Off-Road Navigation using Learned Visual-Inertial Inverse Kinodynamics. (arXiv:2203.15983v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15983">
<div class="article-summary-box-inner">
<span><p>One of the key challenges in high speed off road navigation on ground
vehicles is that the kinodynamics of the vehicle terrain interaction can differ
dramatically depending on the terrain. Previous approaches to addressing this
challenge have considered learning an inverse kinodynamics (IKD) model,
conditioned on inertial information of the vehicle to sense the kinodynamic
interactions. In this paper, we hypothesize that to enable accurate high-speed
off-road navigation using a learned IKD model, in addition to inertial
information from the past, one must also anticipate the kinodynamic
interactions of the vehicle with the terrain in the future. To this end, we
introduce Visual-Inertial Inverse Kinodynamics (VI-IKD), a novel learning based
IKD model that is conditioned on visual information from a terrain patch ahead
of the robot in addition to past inertial information, enabling it to
anticipate kinodynamic interactions in the future. We validate the
effectiveness of VI-IKD in accurate high-speed off-road navigation
experimentally on a scale 1/5 UT-AlphaTruck off-road autonomous vehicle in both
indoor and outdoor environments and show that compared to other
state-of-the-art approaches, VI-IKD enables more accurate and robust off-road
navigation on a variety of different terrains at speeds of up to 3.5 m/s.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Object Classification via Self-Supervised Pose Alignment. (arXiv:2203.15987v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15987">
<div class="article-summary-box-inner">
<span><p>Semantic patterns of fine-grained objects are determined by subtle appearance
difference of local parts, which thus inspires a number of part-based methods.
However, due to uncontrollable object poses in images, distinctive details
carried by local regions can be spatially distributed or even self-occluded,
leading to a large variation on object representation. For discounting pose
variations, this paper proposes to learn a novel graph based object
representation to reveal a global configuration of local parts for
self-supervised pose alignment across classes, which is employed as an
auxiliary feature regularization on a deep representation learning
network.Moreover, a coarse-to-fine supervision together with the proposed
pose-insensitive constraint on shallow-to-deep sub-networks encourages
discriminative features in a curriculum learning manner. We evaluate our method
on three popular fine-grained object classification benchmarks, consistently
achieving the state-of-the-art performance. Source codes are available at
https://github.com/yangxh11/P2P-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Sound of Bounding-Boxes. (arXiv:2203.15991v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15991">
<div class="article-summary-box-inner">
<span><p>In the task of audio-visual sound source separation, which leverages visual
information for sound source separation, identifying objects in an image is a
crucial step prior to separating the sound source. However, existing methods
that assign sound on detected bounding boxes suffer from a problem that their
approach heavily relies on pre-trained object detectors. Specifically, when
using these existing methods, it is required to predetermine all the possible
categories of objects that can produce sound and use an object detector
applicable to all such categories. To tackle this problem, we propose a fully
unsupervised method that learns to detect objects in an image and separate
sound source simultaneously. As our method does not rely on any pre-trained
detector, our method is applicable to arbitrary categories without any
additional annotation. Furthermore, although being fully unsupervised, we found
that our method performs comparably in separation accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleFool: Fooling Video Classification Systems via Style Transfer. (arXiv:2203.16000v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16000">
<div class="article-summary-box-inner">
<span><p>Video classification systems are vulnerable to adversarial attacks, which can
create severe security problems in video verification. Current black-box
attacks need a large number of queries to succeed, resulting in high
computational overhead in the process of attack. On the other hand, attacks
with restricted perturbations are ineffective against defenses such as
denoising or adversarial training. In this paper, we focus on unrestricted
perturbations and propose StyleFool, a black-box video adversarial attack via
style transfer to fool the video classification system. StyleFool first
utilizes color theme proximity to select the best style image, which helps
avoid unnatural details in the stylized videos. Meanwhile, the target class
confidence is additionally considered in targeted attack to influence the
output distribution of the classifier by moving the stylized video closer to or
even across the decision boundary. A gradient-free method is then employed to
further optimize the adversarial perturbation. We carry out extensive
experiments to evaluate StyleFool on two standard datasets, UCF-101 and
HMDB-51. The experimental results suggest that StyleFool outperforms the
state-of-the-art adversarial attacks in terms of both number of queries and
robustness against existing defenses. We identify that 50% of the stylized
videos in untargeted attack do not need any query since they can already fool
the video classification model. Furthermore, we evaluate the
indistinguishability through a user study to show that the adversarial samples
of StyleFool look imperceptible to human eyes, despite unrestricted
perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Sampler: Almost-Universal yet Task-Oriented Sampling for Point Clouds. (arXiv:2203.16001v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16001">
<div class="article-summary-box-inner">
<span><p>Sampling is a key operation in point-cloud task and acts to increase
computational efficiency and tractability by discarding redundant points.
Universal sampling algorithms (e.g., Farthest Point Sampling) work without
modification across different tasks, models, and datasets, but by their very
nature are agnostic about the downstream task/model. As such, they have no
implicit knowledge about which points would be best to keep and which to
reject. Recent work has shown how task-specific point cloud sampling (e.g.,
SampleNet) can be used to outperform traditional sampling approaches by
learning which points are more informative. However, these learnable samplers
face two inherent issues: i) overfitting to a model rather than a task, and
\ii) requiring training of the sampling network from scratch, in addition to
the task network, somewhat countering the original objective of down-sampling
to increase efficiency. In this work, we propose an almost-universal sampler,
in our quest for a sampler that can learn to preserve the most useful points
for a particular task, yet be inexpensive to adapt to different tasks, models,
or datasets. We first demonstrate how training over multiple models for the
same task (e.g., shape reconstruction) significantly outperforms the vanilla
SampleNet in terms of accuracy by not overfitting the sample network to a
particular task network. Second, we show how we can train an almost-universal
meta-sampler across multiple tasks. This meta-sampler can then be rapidly
fine-tuned when applied to different datasets, networks, or even different
tasks, thus amortizing the initial cost of training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ITTR: Unpaired Image-to-Image Translation with Transformers. (arXiv:2203.16015v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16015">
<div class="article-summary-box-inner">
<span><p>Unpaired image-to-image translation is to translate an image from a source
domain to a target domain without paired training data. By utilizing CNN in
extracting local semantics, various techniques have been developed to improve
the translation performance. However, CNN-based generators lack the ability to
capture long-range dependency to well exploit global semantics. Recently,
Vision Transformers have been widely investigated for recognition tasks. Though
appealing, it is inappropriate to simply transfer a recognition-based vision
transformer to image-to-image translation due to the generation difficulty and
the computation limitation. In this paper, we propose an effective and
efficient architecture for unpaired Image-to-Image Translation with
Transformers (ITTR). It has two main designs: 1) hybrid perception block (HPB)
for token mixing from different receptive fields to utilize global semantics;
2) dual pruned self-attention (DPSA) to sharply reduce the computational
complexity. Our ITTR outperforms the state-of-the-arts for unpaired
image-to-image translation on six benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReplaceBlock: An improved regularization method based on background information. (arXiv:2203.16029v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16029">
<div class="article-summary-box-inner">
<span><p>Attention mechanism, being frequently used to train networks for better
feature representations, can effectively disentangle the target object from
irrelevant objects in the background. Given an arbitrary image, we find that
the background's irrelevant objects are most likely to occlude/block the target
object. We propose, based on this finding, a ReplaceBlock to simulate the
situations when the target object is partially occluded by the objects that are
deemed as background. Specifically, ReplaceBlock erases the target object in
the image, and then generates a feature map with only irrelevant objects and
background by the model. Finally, some regions in the background feature map
are used to replace some regions of the target object in the original image
feature map. In this way, ReplaceBlock can effectively simulate the feature map
of the occluded image. The experimental results show that ReplaceBlock works
better than DropBlock in regularizing convolutional networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Deep is Your Art: An Experimental Study on the Limits of Artistic Understanding in a Single-Task, Single-Modality Neural Network. (arXiv:2203.16031v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16031">
<div class="article-summary-box-inner">
<span><p>Mathematical modeling and aesthetic rule extraction of works of art are
complex activities. This is because art is a multidimensional, subjective
discipline. Perception and interpretation of art are, to many extents, relative
and open-ended rather than measurable. Following the explainable Artificial
Intelligence paradigm, this paper investigated in a human-understandable
fashion the limits to which a single-task, single-modality benchmark computer
vision model performs in classifying contemporary 2D visual arts. It is
important to point out that this work does not introduce an interpreting method
to open the black box of Deep Neural Networks, instead it uses existing
evaluating metrics derived from the confusion matrix to try to uncover the
mechanism with which Deep Neural Networks understand art. To achieve so,
VGG-11, pre-trained on ImageNet and discriminatively fine-tuned, was used on
handcrafted small-data datasets designed from real-world photography gallery
shows. We demonstrated that the artwork's Exhibited Properties or formal
factors such as shape and color, rather than Non-Exhibited Properties or
content factors such as history and intention, have much higher potential to be
the determinant when art pieces have very similar Exhibited Properties. We also
showed that a single-task and single-modality model's understanding of art is
inadequate as it largely ignores Non-Exhibited Properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monitored Distillation for Positive Congruent Depth Completion. (arXiv:2203.16034v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16034">
<div class="article-summary-box-inner">
<span><p>We propose a method to infer a dense depth map from a single image, its
calibration, and the associated sparse point cloud. In order to leverage
existing models that produce putative depth maps (teacher models), we propose
an adaptive knowledge distillation approach that yields a positive congruent
training process, where a student model avoids learning the error modes of the
teachers. We consider the scenario of a blind ensemble where we do not have
access to ground truth for model selection nor training. The crux of our
method, termed Monitored Distillation, lies in a validation criterion that
allows us to learn from teachers by choosing predictions that best minimize the
photometric reprojection error for a given image. The result of which is a
distilled depth map and a confidence map, or "monitor", for how well a
prediction from a particular teacher fits the observed image. The monitor
adaptively weights the distilled depth where, if all of the teachers exhibit
high residuals, the standard unsupervised image reconstruction loss takes over
as the supervisory signal. On indoor scenes (VOID), we outperform blind
ensembling baselines by 13.3% and unsupervised methods by 20.3%; we boast a 79%
model size reduction while maintaining comparable performance to the best
supervised method. For outdoors (KITTI), we tie for 5th overall on the
benchmark despite not using ground truth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Learning of Semantic Correspondence with Pseudo-Labels. (arXiv:2203.16038v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16038">
<div class="article-summary-box-inner">
<span><p>Establishing dense correspondences across semantically similar images remains
a challenging task due to the significant intra-class variations and background
clutters. Traditionally, a supervised learning was used for training the
models, which required tremendous manually-labeled data, while some methods
suggested a self-supervised or weakly-supervised learning to mitigate the
reliance on the labeled data, but with limited performance. In this paper, we
present a simple, but effective solution for semantic correspondence that
learns the networks in a semi-supervised manner by supplementing few
ground-truth correspondences via utilization of a large amount of confident
correspondences as pseudo-labels, called SemiMatch. Specifically, our framework
generates the pseudo-labels using the model's prediction itself between source
and weakly-augmented target, and uses pseudo-labels to learn the model again
between source and strongly-augmented target, which improves the robustness of
the model. We also present a novel confidence measure for pseudo-labels and
data augmentation tailored for semantic correspondence. In experiments,
SemiMatch achieves state-of-the-art performance on various benchmarks,
especially on PF-Willow by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Iterative Co-Training Transductive Framework for Zero Shot Learning. (arXiv:2203.16041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16041">
<div class="article-summary-box-inner">
<span><p>In zero-shot learning (ZSL) community, it is generally recognized that
transductive learning performs better than inductive one as the unseen-class
samples are also used in its training stage. How to generate pseudo labels for
unseen-class samples and how to use such usually noisy pseudo labels are two
critical issues in transductive learning. In this work, we introduce an
iterative co-training framework which contains two different base ZSL models
and an exchanging module. At each iteration, the two different ZSL models are
co-trained to separately predict pseudo labels for the unseen-class samples,
and the exchanging module exchanges the predicted pseudo labels, then the
exchanged pseudo-labeled samples are added into the training sets for the next
iteration. By such, our framework can gradually boost the ZSL performance by
fully exploiting the potential complementarity of the two models'
classification capabilities. In addition, our co-training framework is also
applied to the generalized ZSL (GZSL), in which a semantic-guided OOD detector
is proposed to pick out the most likely unseen-class samples before class-level
classification to alleviate the bias problem in GZSL. Extensive experiments on
three benchmarks show that our proposed methods could significantly outperform
about $31$ state-of-the-art ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Threshold Matters in WSSS: Manipulating the Activation for the Robust and Accurate Segmentation Model Against Thresholds. (arXiv:2203.16045v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16045">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised semantic segmentation (WSSS) has recently gained much
attention for its promise to train segmentation models only with image-level
labels. Existing WSSS methods commonly argue that the sparse coverage of CAM
incurs the performance bottleneck of WSSS. This paper provides analytical and
empirical evidence that the actual bottleneck may not be sparse coverage but a
global thresholding scheme applied after CAM. Then, we show that this issue can
be mitigated by satisfying two conditions; 1) reducing the imbalance in the
foreground activation and 2) increasing the gap between the foreground and the
background activation. Based on these findings, we propose a novel activation
manipulation network with a per-pixel classification loss and a label
conditioning module. Per-pixel classification naturally induces two-level
activation in activation maps, which can penalize the most discriminative
parts, promote the less discriminative parts, and deactivate the background
regions. Label conditioning imposes that the output label of pseudo-masks
should be any of true image-level labels; it penalizes the wrong activation
assigned to non-target classes. Based on extensive analysis and evaluations, we
demonstrate that each component helps produce accurate pseudo-masks, achieving
the robustness against the choice of the global threshold. Finally, our model
achieves state-of-the-art records on both PASCAL VOC 2012 and MS COCO 2014
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressively Generating Better Initial Guesses Towards Next Stages for High-Quality Human Motion Prediction. (arXiv:2203.16051v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16051">
<div class="article-summary-box-inner">
<span><p>This paper presents a high-quality human motion prediction method that
accurately predicts future human poses given observed ones. Our method is based
on the observation that a good initial guess of the future poses is very
helpful in improving the forecasting accuracy. This motivates us to propose a
novel two-stage prediction framework, including an init-prediction network that
just computes the good guess and then a formal-prediction network that predicts
the target future poses based on the guess. More importantly, we extend this
idea further and design a multi-stage prediction framework where each stage
predicts initial guess for the next stage, which brings more performance gain.
To fulfill the prediction task at each stage, we propose a network comprising
Spatial Dense Graph Convolutional Networks (S-DGCN) and Temporal Dense Graph
Convolutional Networks (T-DGCN). Alternatively executing the two networks helps
extract spatiotemporal features over the global receptive field of the whole
pose sequence. All the above design choices cooperating together make our
method outperform previous approaches by large margins: 6%-7% on Human3.6M,
5%-10% on CMU-MoCap, and 13%-16% on 3DPW.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Facial Skin Feature Detection for Everyone. (arXiv:2203.16056v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16056">
<div class="article-summary-box-inner">
<span><p>Automatic assessment and understanding of facial skin condition have several
applications, including the early detection of underlying health problems,
lifestyle and dietary treatment, skin-care product recommendation, etc. Selfies
in the wild serve as an excellent data resource to democratize skin quality
assessment, but suffer from several data collection challenges.The key to
guaranteeing an accurate assessment is accurate detection of different skin
features. We present an automatic facial skin feature detection method that
works across a variety of skin tones and age groups for selfies in the wild. To
be specific, we annotate the locations of acne, pigmentation, and wrinkle for
selfie images with different skin tone colors, severity levels, and lighting
conditions. The annotation is conducted in a two-phase scheme with the help of
a dermatologist to train volunteers for annotation. We employ Unet++ as the
network architecture for feature detection. This work shows that the two-phase
annotation scheme can robustly detect the accurate locations of acne,
pigmentation, and wrinkle for selfie images with different ethnicities, skin
tone colors, severity levels, age groups, and lighting conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised 360$^{\circ}$ Room Layout Estimation. (arXiv:2203.16057v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16057">
<div class="article-summary-box-inner">
<span><p>We present the first self-supervised method to train panoramic room layout
estimation models without any labeled data. Unlike per-pixel dense depth that
provides abundant correspondence constraints, layout representation is sparse
and topological, hindering the use of self-supervised reprojection consistency
on images. To address this issue, we propose Differentiable Layout View
Rendering, which can warp a source image to the target camera pose given the
estimated layout from the target image. As each rendered pixel is
differentiable with respect to the estimated layout, we can now train the
layout estimation model by minimizing reprojection loss. Besides, we introduce
regularization losses to encourage Manhattan alignment, ceiling-floor
alignment, cycle consistency, and layout stretch consistency, which further
improve our predictions. Finally, we present the first self-supervised results
on ZilloIndoor and MatterportLayout datasets. Our approach also shows promising
solutions in data-scarce scenarios and active learning, which would have an
immediate value in the real estate virtual tour software. Code is available at
https://github.com/joshua049/Stereo-360-Layout.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval. (arXiv:2203.16062v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16062">
<div class="article-summary-box-inner">
<span><p>Evaluation measures have a crucial impact on the direction of research.
Therefore, it is of utmost importance to develop appropriate and reliable
evaluation measures for new applications where conventional measures are not
well suited. Video Moment Retrieval (VMR) is one such application, and the
current practice is to use R@$K,\theta$ for evaluating VMR systems. However,
this measure has two disadvantages. First, it is rank-insensitive: It ignores
the rank positions of successfully localised moments in the top-$K$ ranked list
by treating the list as a set. Second, it binarizes the Intersection over Union
(IoU) of each retrieved video moment using the threshold $\theta$ and thereby
ignoring fine-grained localisation quality of ranked moments.
</p>
<p>We propose an alternative measure for evaluating VMR, called Average Max IoU
(AxIoU), which is free from the above two problems. We show that AxIoU
satisfies two important axioms for VMR evaluation, namely, \textbf{Invariance
against Redundant Moments} and \textbf{Monotonicity with respect to the Best
Moment}, and also that R@$K,\theta$ satisfies the first axiom only. We also
empirically examine how AxIoU agrees with R@$K,\theta$, as well as its
stability with respect to change in the test data and human-annotated temporal
boundaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pay Attention to Hidden States for Video Deblurring: Ping-Pong Recurrent Neural Networks and Selective Non-Local Attention. (arXiv:2203.16063v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16063">
<div class="article-summary-box-inner">
<span><p>Video deblurring models exploit information in the neighboring frames to
remove blur caused by the motion of the camera and the objects. Recurrent
Neural Networks~(RNNs) are often adopted to model the temporal dependency
between frames via hidden states. When motion blur is strong, however, hidden
states are hard to deliver proper information due to the displacement between
different frames. While there have been attempts to update the hidden states,
it is difficult to handle misaligned features beyond the receptive field of
simple modules. Thus, we propose 2 modules to supplement the RNN architecture
for video deblurring. First, we design Ping-Pong RNN~(PPRNN) that acts on
updating the hidden states by referring to the features from the current and
the previous time steps alternately. PPRNN gathers relevant information from
the both features in an iterative and balanced manner by utilizing its
recurrent architecture. Second, we use a Selective Non-Local Attention~(SNLA)
module to additionally refine the hidden state by aligning it with the
positional information from the input frame feature. The attention score is
scaled by the relevance to the input feature to focus on the necessary
information. By paying attention to hidden states with both modules, which have
strong synergy, our PAHS framework improves the representation powers of RNN
structures and achieves state-of-the-art deblurring performance on standard
benchmarks and real-world videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Program Representations for Food Images and Cooking Recipes. (arXiv:2203.16071v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16071">
<div class="article-summary-box-inner">
<span><p>In this paper, we are interested in modeling a how-to instructional
procedure, such as a cooking recipe, with a meaningful and rich high-level
representation. Specifically, we propose to represent cooking recipes and food
images as cooking programs. Programs provide a structured representation of the
task, capturing cooking semantics and sequential relationships of actions in
the form of a graph. This allows them to be easily manipulated by users and
executed by agents. To this end, we build a model that is trained to learn a
joint embedding between recipes and food images via self-supervision and
jointly generate a program from this embedding as a sequence. To validate our
idea, we crowdsource programs for cooking recipes and show that: (a) projecting
the image-recipe embeddings into programs leads to better cross-modal retrieval
results; (b) generating programs from images leads to better recognition
results compared to predicting raw cooking instructions; and (c) we can
generate food images by manipulating programs via optimizing the latent code of
a GAN. Code, data, and models are available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Anchor-free Universal Lesion Detection in CT-scans. (arXiv:2203.16074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16074">
<div class="article-summary-box-inner">
<span><p>Existing universal lesion detection (ULD) methods utilize compute-intensive
anchor-based architectures which rely on predefined anchor boxes, resulting in
unsatisfactory detection performance, especially in small and mid-sized
lesions. Further, these default fixed anchor-sizes and ratios do not generalize
well to different datasets. Therefore, we propose a robust one-stage
anchor-free lesion detection network that can perform well across varying
lesions sizes by exploiting the fact that the box predictions can be sorted for
relevance based on their center rather than their overlap with the object.
Furthermore, we demonstrate that the ULD can be improved by explicitly
providing it the domain-specific information in the form of multi-intensity
images generated using multiple HU windows, followed by self-attention based
feature-fusion and backbone initialization using weights learned via
self-supervision over CT-scans. We obtain comparable results to the
state-of-the-art methods, achieving an overall sensitivity of 86.05% on the
DeepLesion dataset, which comprises of approximately 32K CT-scans with lesions
annotated across various body organs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STRPM: A Spatiotemporal Residual Predictive Model for High-Resolution Video Prediction. (arXiv:2203.16084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16084">
<div class="article-summary-box-inner">
<span><p>Although many video prediction methods have obtained good performance in
low-resolution (64$\sim$128) videos, predictive models for high-resolution
(512$\sim$4K) videos have not been fully explored yet, which are more
meaningful due to the increasing demand for high-quality videos. Compared with
low-resolution videos, high-resolution videos contain richer appearance
(spatial) information and more complex motion (temporal) information. In this
paper, we propose a Spatiotemporal Residual Predictive Model (STRPM) for
high-resolution video prediction. On the one hand, we propose a Spatiotemporal
Encoding-Decoding Scheme to preserve more spatiotemporal information for
high-resolution videos. In this way, the appearance details for each frame can
be greatly preserved. On the other hand, we design a Residual Predictive Memory
(RPM) which focuses on modeling the spatiotemporal residual features (STRF)
between previous and future frames instead of the whole frame, which can
greatly help capture the complex motion information in high-resolution videos.
In addition, the proposed RPM can supervise the spatial encoder and temporal
encoder to extract different features in the spatial domain and the temporal
domain, respectively. Moreover, the proposed model is trained using generative
adversarial networks (GANs) with a learned perceptual loss (LP-loss) to improve
the perceptual quality of the predictions. Experimental results show that STRPM
can generate more satisfactory results compared with various existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Omni-DETR: Omni-Supervised Object Detection with Transformers. (arXiv:2203.16089v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16089">
<div class="article-summary-box-inner">
<span><p>We consider the problem of omni-supervised object detection, which can use
unlabeled, fully labeled and weakly labeled annotations, such as image tags,
counts, points, etc., for object detection. This is enabled by a unified
architecture, Omni-DETR, based on the recent progress on student-teacher
framework and end-to-end transformer based object detection. Under this unified
architecture, different types of weak labels can be leveraged to generate
accurate pseudo labels, by a bipartite matching based filtering mechanism, for
the model to learn. In the experiments, Omni-DETR has achieved state-of-the-art
results on multiple datasets and settings. And we have found that weak
annotations can help to improve detection performance and a mixture of them can
achieve a better trade-off between annotation cost and accuracy than the
standard complete annotation. These findings could encourage larger object
detection datasets with mixture annotations. The code is available at
https://github.com/amazon-research/omni-detr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Tracking via Ensemble of Local Trackers. (arXiv:2203.16092v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16092">
<div class="article-summary-box-inner">
<span><p>The crux of long-term tracking lies in the difficulty of tracking the target
with discontinuous moving caused by out-of-view or occlusion. Existing
long-term tracking methods follow two typical strategies. The first strategy
employs a local tracker to perform smooth tracking and uses another re-detector
to detect the target when the target is lost. While it can exploit the temporal
context like historical appearances and locations of the target, a potential
limitation of such strategy is that the local tracker tends to misidentify a
nearby distractor as the target instead of activating the re-detector when the
real target is out of view. The other long-term tracking strategy tracks the
target in the entire image globally instead of local tracking based on the
previous tracking results. Unfortunately, such global tracking strategy cannot
leverage the temporal context effectively. In this work, we combine the
advantages of both strategies: tracking the target in a global view while
exploiting the temporal context. Specifically, we perform global tracking via
ensemble of local trackers spreading the full image. The smooth moving of the
target can be handled steadily by one local tracker. When the local tracker
accidentally loses the target due to suddenly discontinuous moving, another
local tracker close to the target is then activated and can readily take over
the tracking to locate the target. While the activated local tracker performs
tracking locally by leveraging the temporal context, the ensemble of local
trackers renders our model the global view for tracking. Extensive experiments
on six datasets demonstrate that our method performs favorably against
state-of-the-art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contribution of the Temperature of the Objects to the Problem of Thermal Imaging Focusing. (arXiv:2203.16106v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16106">
<div class="article-summary-box-inner">
<span><p>When focusing an image, depth of field, aperture and distance from the camera
to the object, must be taking into account, both, in visible and in infrared
spectrum. Our experiments reveal that in addition, the focusing problem in
thermal spectrum is also hardly dependent of the temperature of the object
itself (and/or the scene).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preliminary experiments on thermal emissivity adjustment for face images. (arXiv:2203.16107v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16107">
<div class="article-summary-box-inner">
<span><p>In this paper we summarize several applications based on thermal imaging. We
emphasize the importance of emissivity adjustment for a proper temperature
measurement. A new set of face images acquired at different emissivity values
with steps of 0.01 is also presented and will be distributed for free for
research purposes. Among the utilities, we can mention: a) the possibility to
apply corrections once an image is acquired with a wrong emissivity value and
it is not possible to acquire a new one; b) privacy protection in thermal
images, which can be obtained with a low emissivity factor, which is still
suitable for several applications, but hides the identity of a user; c) image
processing for improving temperature detection in scenes containing objects of
different emissivity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIT: A Bionic and Non-Linear Neuron for Spiking Neural Network. (arXiv:2203.16117v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16117">
<div class="article-summary-box-inner">
<span><p>Spiking Neural Networks (SNNs) have piqued researchers' interest because of
their capacity to process temporal information and low power consumption.
However, current state-of-the-art methods limited their biological plausibility
and performance because their neurons are generally built on the simple
Leaky-Integrate-and-Fire (LIF) model. Due to the high level of dynamic
complexity, modern neuron models have seldom been implemented in SNN practice.
In this study, we adopt the Phase Plane Analysis (PPA) technique, a technique
often utilized in neurodynamics field, to integrate a recent neuron model,
namely, the Izhikevich neuron. Based on the findings in the advancement of
neuroscience, the Izhikevich neuron model can be biologically plausible while
maintaining comparable computational cost with LIF neurons. By utilizing the
adopted PPA, we have accomplished putting neurons built with the modified
Izhikevich model into SNN practice, dubbed as the Standardized Izhikevich Tonic
(SIT) neuron. For performance, we evaluate the suggested technique for image
classification tasks in self-built LIF-and-SIT-consisted SNNs, named Hybrid
Neural Network (HNN) on static MNIST, Fashion-MNIST, CIFAR-10 datasets and
neuromorphic N-MNIST, CIFAR10-DVS, and DVS128 Gesture datasets. The
experimental results indicate that the suggested method achieves comparable
accuracy while exhibiting more biologically realistic behaviors on nearly all
test datasets, demonstrating the efficiency of this novel strategy in bridging
the gap between neurodynamics and SNN practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensor Data Validation and Driving Safety in Autonomous Driving Systems. (arXiv:2203.16130v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16130">
<div class="article-summary-box-inner">
<span><p>Autonomous driving technology has drawn a lot of attention due to its fast
development and extremely high commercial values. The recent technological leap
of autonomous driving can be primarily attributed to the progress in the
environment perception. Good environment perception provides accurate
high-level environment information which is essential for autonomous vehicles
to make safe and precise driving decisions and strategies. Moreover, such
progress in accurate environment perception would not be possible without deep
learning models and advanced onboard sensors, such as optical sensors (LiDARs
and cameras), radars, GPS. However, the advanced sensors and deep learning
models are prone to recently invented attack methods. For example, LiDARs and
cameras can be compromised by optical attacks, and deep learning models can be
attacked by adversarial examples. The attacks on advanced sensors and deep
learning models can largely impact the accuracy of the environment perception,
posing great threats to the safety and security of autonomous vehicles. In this
thesis, we study the detection methods against the attacks on onboard sensors
and the linkage between attacked deep learning models and driving safety for
autonomous vehicles. To detect the attacks, redundant data sources can be
exploited, since information distortions caused by attacks in victim sensor
data result in inconsistency with the information from other redundant sources.
To study the linkage between attacked deep learning models and driving
safety...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tampered VAE for Improved Satellite Image Time Series Classification. (arXiv:2203.16149v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16149">
<div class="article-summary-box-inner">
<span><p>The unprecedented availability of spatial and temporal high-resolution
satellite image time series (SITS) for crop type mapping is believed to
necessitate deep learning architectures to accommodate challenges arising from
both dimensions. Recent state-of-the-art deep learning models have shown
promising results by stacking spatial and temporal encoders. However, we
present a Pyramid Time-Series Transformer (PTST) that operates solely on the
temporal dimension, i.e., neglecting the spatial dimension, can produce
superior results with a drastic reduction in GPU memory consumption and easy
extensibility. Furthermore, we augment it to perform semi-supervised learning
by proposing a classification-friendly VAE framework that introduces clustering
mechanisms into latent space and can promote linear separability therein.
Consequently, a few principal axes of the latent space can explain the majority
of variance in raw data. Meanwhile, the VAE framework with proposed tweaks can
maintain competitive classification performance as its purely discriminative
counterpart when only $40\%$ of labelled data is used. We hope the proposed
framework can serve as a baseline for crop classification with SITS for its
modularity and simplicity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recommendation of Compatible Outfits Conditioned on Style. (arXiv:2203.16161v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16161">
<div class="article-summary-box-inner">
<span><p>Recommendation in the fashion domain has seen a recent surge in research in
various areas, for example, shop-the-look, context-aware outfit creation,
personalizing outfit creation, etc. The majority of state of the art approaches
in the domain of outfit recommendation pursue to improve compatibility among
items so as to produce high quality outfits. Some recent works have realized
that style is an important factor in fashion and have incorporated it in
compatibility learning and outfit generation. These methods often depend on the
availability of fine-grained product categories or the presence of rich item
attributes (e.g., long-skirt, mini-skirt, etc.). In this work, we aim to
generate outfits conditional on styles or themes as one would dress in real
life, operating under the practical assumption that each item is mapped to a
high level category as driven by the taxonomy of an online portal, like
outdoor, formal etc and an image. We use a novel style encoder network that
renders outfit styles in a smooth latent space. We present an extensive
analysis of different aspects of our method and demonstrate its superiority
over existing state of the art baselines through rigorous experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rabbit, toad, and the Moon: Can machine categorize them into one class?. (arXiv:2203.16163v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16163">
<div class="article-summary-box-inner">
<span><p>Recent machine learning algorithms such as neural networks can classify
objects and actions in video frames with high accuracy. Here, I discuss a
classification of objects based on basal dynamic patterns referencing one
tradition, the link between rabbit, toad, and the Moon, which can be seen in
several cultures. In order for them to be classified into one class, a basic
pattern of behavior (cyclic appearance and disappearance) works as a feature
point. A static character such as the shape and time scale of the behavior are
not essential for this classification. In cognitive semantics, image schemas
are introduced to describe basal patterns of events. If learning of these image
schemas is attained, a machine may be able to categorize rabbit, toad, and the
Moon as the same class. For learning, video frames that show boundary boxes or
segmentation may be helpful. Although this discussion is preliminary and many
tasks remain to be solved, the classification based on basal behaviors can be
an important topic for cognitive processes and computer science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLOAT: Factorized Learning of Object Attributes for Improved Multi-object Multi-part Scene Parsing. (arXiv:2203.16168v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16168">
<div class="article-summary-box-inner">
<span><p>Multi-object multi-part scene parsing is a challenging task which requires
detecting multiple object classes in a scene and segmenting the semantic parts
within each object. In this paper, we propose FLOAT, a factorized label space
framework for scalable multi-object multi-part parsing. Our framework involves
independent dense prediction of object category and part attributes which
increases scalability and reduces task complexity compared to the monolithic
label space counterpart. In addition, we propose an inference-time 'zoom'
refinement technique which significantly improves segmentation quality,
especially for smaller objects/parts. Compared to state of the art, FLOAT
obtains an absolute improvement of 2.0% for mean IOU (mIOU) and 4.8% for
segmentation quality IOU (sqIOU) on the Pascal-Part-58 dataset. For the larger
Pascal-Part-108 dataset, the improvements are 2.1% for mIOU and 3.9% for sqIOU.
We incorporate previously excluded part attributes and other minor parts of the
Pascal-Part dataset to create the most comprehensive and challenging version
which we dub Pascal-Part-201. FLOAT obtains improvements of 8.6% for mIOU and
7.5% for sqIOU on the new dataset, demonstrating its parsing effectiveness
across a challenging diversity of objects and parts. The code and datasets are
available at floatseg.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Distillation from the Last Mini-Batch for Consistency Regularization. (arXiv:2203.16172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16172">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) shows a bright promise as a powerful
regularization strategy to boost generalization ability by leveraging learned
sample-level soft targets. Yet, employing a complex pre-trained teacher network
or an ensemble of peer students in existing KD is both time-consuming and
computationally costly. Various self KD methods have been proposed to achieve
higher distillation efficiency. However, they either require extra network
architecture modification or are difficult to parallelize. To cope with these
challenges, we propose an efficient and reliable self-distillation framework,
named Self-Distillation from Last Mini-Batch (DLB). Specifically, we rearrange
the sequential sampling by constraining half of each mini-batch coinciding with
the previous iteration. Meanwhile, the rest half will coincide with the
upcoming iteration. Afterwards, the former half mini-batch distills on-the-fly
soft targets generated in the previous iteration. Our proposed mechanism guides
the training stability and consistency, resulting in robustness to label noise.
Moreover, our method is easy to implement, without taking up extra run-time
memory or requiring model structure modification. Experimental results on three
classification benchmarks illustrate that our approach can consistently
outperform state-of-the-art self-distillation approaches with different network
architectures. Additionally, our method shows strong compatibility with
augmentation strategies by gaining additional performance improvement. The code
is available at https://github.com/Meta-knowledge-Lab/DLB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlowFormer: A Transformer Architecture for Optical Flow. (arXiv:2203.16194v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16194">
<div class="article-summary-box-inner">
<span><p>We introduce Optical Flow TransFormer (FlowFormer), a transformer-based
neural network architecture for learning optical flow. FlowFormer tokenizes the
4D cost volume built from an image pair, encodes the cost tokens into a cost
memory with alternate-group transformer (AGT) layers in a novel latent space,
and decodes the cost memory via a recurrent transformer decoder with dynamic
positional cost queries. On the Sintel benchmark clean pass, FlowFormer
achieves 1.178 average end-ponit-error (AEPE), a 15.1% error reduction from the
best published result (1.388). Besides, FlowFormer also achieves strong
generalization performance. Without being trained on Sintel, FlowFormer
achieves 1.00 AEPE on the Sintel training set clean pass, outperforming the
best published result (1.29) by 22.4%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Road to Online Adaptation for Semantic Image Segmentation. (arXiv:2203.16195v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16195">
<div class="article-summary-box-inner">
<span><p>We propose a new problem formulation and a corresponding evaluation framework
to advance research on unsupervised domain adaptation for semantic image
segmentation. The overall goal is fostering the development of adaptive
learning systems that will continuously learn, without supervision, in
ever-changing environments. Typical protocols that study adaptation algorithms
for segmentation models are limited to few domains, adaptation happens offline,
and human intervention is generally required, at least to annotate data for
hyper-parameter tuning. We argue that such constraints are incompatible with
algorithms that can continuously adapt to different real-world situations. To
address this, we propose a protocol where models need to learn online, from
sequences of temporally correlated images, requiring continuous, frame-by-frame
adaptation. We accompany this new protocol with a variety of baselines to
tackle the proposed formulation, as well as an extensive analysis of their
behaviors, which can serve as a starting point for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-Temporal Parallel Transformer for Arm-Hand Dynamic Estimation. (arXiv:2203.16202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16202">
<div class="article-summary-box-inner">
<span><p>We propose an approach to estimate arm and hand dynamics from monocular video
by utilizing the relationship between arm and hand. Although monocular full
human motion capture technologies have made great progress in recent years,
recovering accurate and plausible arm twists and hand gestures from in-the-wild
videos still remains a challenge. To solve this problem, our solution is
proposed based on the fact that arm poses and hand gestures are highly
correlated in most real situations. To fully exploit arm-hand correlation as
well as inter-frame information, we carefully design a Spatial-Temporal
Parallel Arm-Hand Motion Transformer (PAHMT) to predict the arm and hand
dynamics simultaneously. We also introduce new losses to encourage the
estimations to be smooth and accurate. Besides, we collect a motion capture
dataset including 200K frames of hand gestures and use this data to train our
model. By integrating a 2D hand pose estimation model and a 3D human pose
estimation model, the proposed method can produce plausible arm and hand
dynamics from monocular video. Extensive evaluations demonstrate that the
proposed method has advantages over previous state-of-the-art approaches and
shows robustness under various challenging scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fair Contrastive Learning for Facial Attribute Classification. (arXiv:2203.16209v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16209">
<div class="article-summary-box-inner">
<span><p>Learning visual representation of high quality is essential for image
classification. Recently, a series of contrastive representation learning
methods have achieved preeminent success. Particularly, SupCon outperformed the
dominant methods based on cross-entropy loss in representation learning.
However, we notice that there could be potential ethical risks in supervised
contrastive learning. In this paper, we for the first time analyze unfairness
caused by supervised contrastive learning and propose a new Fair Supervised
Contrastive Loss (FSCL) for fair visual representation learning. Inheriting the
philosophy of supervised contrastive learning, it encourages representation of
the same class to be closer to each other than that of different classes, while
ensuring fairness by penalizing the inclusion of sensitive attribute
information in representation. In addition, we introduce a group-wise
normalization to diminish the disparities of intra-group compactness and
inter-class separability between demographic groups that arouse unfair
classification. Through extensive experiments on CelebA and UTK Face, we
validate that the proposed method significantly outperforms SupCon and existing
state-of-the-art methods in terms of the trade-off between top-1 accuracy and
fairness. Moreover, our method is robust to the intensity of data bias and
effectively works in incomplete supervised settings. Our code is available at
https://github.com/sungho-CoolG/FSCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning of Global Objective for Network Flow in Multi-Object Tracking. (arXiv:2203.16210v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16210">
<div class="article-summary-box-inner">
<span><p>This paper concerns the problem of multi-object tracking based on the
min-cost flow (MCF) formulation, which is conventionally studied as an instance
of linear program. Given its computationally tractable inference, the success
of MCF tracking largely relies on the learned cost function of underlying
linear program. Most previous studies focus on learning the cost function by
only taking into account two frames during training, therefore the learned cost
function is sub-optimal for MCF where a multi-frame data association must be
considered during inference. In order to address this problem, in this paper we
propose a novel differentiable framework that ties training and inference
together during learning by solving a bi-level optimization problem, where the
lower-level solves a linear program and the upper-level contains a loss
function that incorporates global tracking result. By back-propagating the loss
through differentiable layers via gradient descent, the globally parameterized
cost function is explicitly learned and regularized. With this approach, we are
able to learn a better objective for global MCF tracking. As a result, we
achieve competitive performances compared to the current state-of-the-art
methods on the popular multi-object tracking benchmarks such as MOT16, MOT17
and MOT20.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acknowledging the Unknown for Multi-label Learning with Single Positive Labels. (arXiv:2203.16219v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16219">
<div class="article-summary-box-inner">
<span><p>Due to the difficulty of collecting exhaustive multi-label annotations,
multi-label training data often contains partial labels. We consider an extreme
of this problem, called single positive multi-label learning (SPML), where each
multi-label training image has only one positive label. Traditionally, all
unannotated labels are assumed as negative labels in SPML, which would
introduce false negative labels and make model training be dominated by assumed
negative labels. In this work, we choose to treat all unannotated labels from a
different perspective, \textit{i.e.} acknowledging they are unknown. Hence, we
propose entropy-maximization (EM) loss to maximize the entropy of predicted
probabilities for all unannotated labels. Considering the positive-negative
label imbalance of unannotated labels, we propose asymmetric pseudo-labeling
(APL) with asymmetric-tolerance strategies and a self-paced procedure to
provide more precise supervision. Experiments show that our method
significantly improves performance and achieves state-of-the-art results on all
four benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection. (arXiv:2203.16220v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16220">
<div class="article-summary-box-inner">
<span><p>This study addresses the issue of fusing infrared and visible images that
appear differently for object detection. Aiming at generating an image of high
visual quality, previous approaches discover commons underlying the two
modalities and fuse upon the common space either by iterative optimization or
deep networks. These approaches neglect that modality differences implying the
complementary information are extremely important for both fusion and
subsequent detection task. This paper proposes a bilevel optimization
formulation for the joint problem of fusion and detection, and then unrolls to
a target-aware Dual Adversarial Learning (TarDAL) network for fusion and a
commonly used detection network. The fusion network with one generator and dual
discriminators seeks commons while learning from differences, which preserves
structural information of targets from the infrared and textural details from
the visible. Furthermore, we build a synchronized imaging system with
calibrated infrared and optical sensors, and collect currently the most
comprehensive benchmark covering a wide range of scenarios. Extensive
experiments on several public datasets and our benchmark demonstrate that our
method outputs not only visually appealing fusion but also higher detection mAP
than the state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End to End Lip Synchronization with a Temporal AutoEncoder. (arXiv:2203.16224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16224">
<div class="article-summary-box-inner">
<span><p>We study the problem of syncing the lip movement in a video with the audio
stream. Our solution finds an optimal alignment using a dual-domain recurrent
neural network that is trained on synthetic data we generate by dropping and
duplicating video frames. Once the alignment is found, we modify the video in
order to sync the two sources. Our method is shown to greatly outperform the
literature methods on a variety of existing and new benchmarks. As an
application, we demonstrate our ability to robustly align text-to-speech
generated audio with an existing video stream. Our code and samples are
available at
https://github.com/itsyoavshalev/End-to-End-Lip-Synchronization-with-a-Temporal-AutoEncoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biclustering Algorithms Based on Metaheuristics: A Review. (arXiv:2203.16241v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16241">
<div class="article-summary-box-inner">
<span><p>Biclustering is an unsupervised machine learning technique that
simultaneously clusters rows and columns in a data matrix. Biclustering has
emerged as an important approach and plays an essential role in various
applications such as bioinformatics, text mining, and pattern recognition.
However, finding significant biclusters is an NP-hard problem that can be
formulated as an optimization problem. Therefore, different metaheuristics have
been applied to biclustering problems because of their exploratory capability
of solving complex optimization problems in reasonable computation time.
Although various surveys on biclustering have been proposed, there is a lack of
a comprehensive survey on the biclustering problem using metaheuristics. This
chapter will present a survey of metaheuristics approaches to address the
biclustering problem. The review focuses on the underlying optimization methods
and their main search components: representation, objective function, and
variation operators. A specific discussion on single versus multi-objective
approaches is presented. Finally, some emerging research directions are
presented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CycDA: Unsupervised Cycle Domain Adaptation from Image to Video. (arXiv:2203.16244v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16244">
<div class="article-summary-box-inner">
<span><p>Although action recognition has achieved impressive results over recent
years, both collection and annotation of video training data are still
time-consuming and cost intensive. Therefore, image-to-video adaptation has
been proposed to exploit labeling-free web image source for adapting on
unlabeled target videos. This poses two major challenges: (1) spatial domain
shift between web images and video frames; (2) modality gap between image and
video data. To address these challenges, we propose Cycle Domain Adaptation
(CycDA), a cycle-based approach for unsupervised image-to-video domain
adaptation by leveraging the joint spatial information in images and videos on
the one hand and, on the other hand, training an independent spatio-temporal
model to bridge the modality gap. We alternate between the spatial and
spatio-temporal learning with knowledge transfer between the two in each cycle.
We evaluate our approach on benchmark datasets for image-to-video as well as
for mixed-source domain adaptation achieving state-of-the-art results and
demonstrating the benefits of our cyclic adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstaFormer: Instance-Aware Image-to-Image Translation with Transformer. (arXiv:2203.16248v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16248">
<div class="article-summary-box-inner">
<span><p>We present a novel Transformer-based network architecture for instance-aware
image-to-image translation, dubbed InstaFormer, to effectively integrate
global- and instance-level information. By considering extracted content
features from an image as tokens, our networks discover global consensus of
content features by considering context information through a self-attention
module in Transformers. By augmenting such tokens with an instance-level
feature extracted from the content feature with respect to bounding box
information, our framework is capable of learning an interaction between object
instances and the global image, thus boosting the instance-awareness. We
replace layer normalization (LayerNorm) in standard Transformers with adaptive
instance normalization (AdaIN) to enable a multi-modal translation with style
codes. In addition, to improve the instance-awareness and translation quality
at object regions, we present an instance-level content contrastive loss
defined between input and translated image. We conduct experiments to
demonstrate the effectiveness of our InstaFormer over the latest methods and
provide extensive ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PP-YOLOE: An evolved version of YOLO. (arXiv:2203.16250v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16250">
<div class="article-summary-box-inner">
<span><p>In this report, we present PP-YOLOE, an industrial state-of-the-art object
detector with high performance and friendly deployment. We optimize on the
basis of the previous PP-YOLOv2, using anchor-free paradigm, more powerful
backbone and neck equipped with CSPRepResStage, ET-head and dynamic label
assignment algorithm TAL. We provide s/m/l/x models for different practice
scenarios. As a result, PP-YOLOE-l achieves 51.4 mAP on COCO test-dev and 78.1
FPS on Tesla V100, yielding a remarkable improvement of (+1.9 AP, +13.35% speed
up) and (+1.3 AP, +24.96% speed up), compared to the previous state-of-the-art
industrial models PP-YOLOv2 and YOLOX respectively. Further, PP-YOLOE inference
speed achieves 149.2 FPS with TensorRT and FP16-precision. We also conduct
extensive experiments to verify the effectiveness of our designs. Source code
and pre-trained models are available at
https://github.com/PaddlePaddle/PaddleDetection .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data. (arXiv:2203.16258v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16258">
<div class="article-summary-box-inner">
<span><p>Segmenting or detecting objects in sparse Lidar point clouds are two
important tasks in autonomous driving to allow a vehicle to act safely in its
3D environment. The best performing methods in 3D semantic segmentation or
object detection rely on a large amount of annotated data. Yet annotating 3D
Lidar data for these tasks is tedious and costly. In this context, we propose a
self-supervised pre-training method for 3D perception models that is tailored
to autonomous driving data. Specifically, we leverage the availability of
synchronized and calibrated image and Lidar sensors in autonomous driving
setups for distilling self-supervised pre-trained image representations into 3D
models. Hence, our method does not require any point cloud nor image
annotations. The key ingredient of our method is the use of superpixels which
are used to pool 3D point features and 2D pixel features in visually similar
regions. We then train a 3D network on the self-supervised task of matching
these pooled point features with the corresponding pooled image pixel features.
The advantages of contrasting regions obtained by superpixels are that: (1)
grouping together pixels and points of visually coherent regions leads to a
more meaningful contrastive task that produces features well adapted to 3D
semantic segmentation and 3D object detection; (2) all the different regions
have the same weight in the contrastive loss regardless of the number of 3D
points sampled in these regions; (3) it mitigates the noise produced by
incorrect matching of points and pixels due to occlusions between the different
sensors. Extensive experiments on autonomous driving datasets demonstrate the
ability of our image-to-Lidar distillation strategy to produce 3D
representations that transfer well on semantic segmentation and object
detection tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeqTR: A Simple yet Universal Network for Visual Grounding. (arXiv:2203.16265v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16265">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a simple yet universal network termed SeqTR for
visual grounding tasks, e.g., phrase localization, referring expression
comprehension (REC) and segmentation (RES). The canonical paradigms for visual
grounding often require substantial expertise in designing network
architectures and loss functions, making them hard to generalize across tasks.
To simplify and unify the modeling, we cast visual grounding as a point
prediction problem conditioned on image and text inputs, where either the
bounding box or binary mask is represented as a sequence of discrete coordinate
tokens. Under this paradigm, visual grounding tasks are unified in our SeqTR
network without task-specific branches or heads, e.g., the convolutional mask
decoder for RES, which greatly reduces the complexity of multi-task modeling.
In addition, SeqTR also shares the same optimization objective for all tasks
with a simple cross-entropy loss, further reducing the complexity of deploying
hand-crafted loss functions. Experiments on five benchmark datasets demonstrate
that the proposed SeqTR outperforms (or is on par with) the existing
state-of-the-arts, proving that a simple yet universal approach for visual
grounding is indeed feasible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Multi-scale Fusion of 2D and 3D Features for Multi-object Tracking. (arXiv:2203.16268v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16268">
<div class="article-summary-box-inner">
<span><p>Multiple object tracking (MOT) is a significant task in achieving autonomous
driving. Traditional works attempt to complete this task, either based on point
clouds (PC) collected by LiDAR, or based on images captured from cameras.
However, relying on one single sensor is not robust enough, because it might
fail during the tracking process. On the other hand, feature fusion from
multiple modalities contributes to the improvement of accuracy. As a result,
new techniques based on different sensors integrating features from multiple
modalities are being developed. Texture information from RGB cameras and 3D
structure information from Lidar have respective advantages under different
circumstances. However, it's not easy to achieve effective feature fusion
because of completely distinct information modalities. Previous fusion methods
usually fuse the top-level features after the backbones extract the features
from different modalities. In this paper, we first introduce PointNet++ to
obtain multi-scale deep representations of point cloud to make it adaptive to
our proposed Interactive Feature Fusion between multi-scale features of images
and point clouds. Specifically, through multi-scale interactive query and
fusion between pixel-level and point-level features, our method, can obtain
more distinguishing features to improve the performance of multiple object
tracking. Besides, we explore the effectiveness of pre-training on each single
modality and fine-tuning on the fusion-based model. The experimental results
demonstrate that our method can achieve good performance on the KITTI benchmark
and outperform other approaches without using multi-scale feature fusion.
Moreover, the ablation studies indicates the effectiveness of multi-scale
feature fusion and pre-training on single modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Vertebral Fracture Diagnosis. (arXiv:2203.16273v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16273">
<div class="article-summary-box-inner">
<span><p>Do black-box neural network models learn clinically relevant features for
fracture diagnosis? The answer not only establishes reliability quenches
scientific curiosity but also leads to explainable and verbose findings that
can assist the radiologists in the final and increase trust. This work
identifies the concepts networks use for vertebral fracture diagnosis in CT
images. This is achieved by associating concepts to neurons highly correlated
with a specific diagnosis in the dataset. The concepts are either associated
with neurons by radiologists pre-hoc or are visualized during a specific
prediction and left for the user's interpretation. We evaluate which concepts
lead to correct diagnosis and which concepts lead to false positives. The
proposed frameworks and analysis pave the way for reliable and explainable
vertebral fracture diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDSDF: Hybrid Directional and Signed Distance Functions for Fast Inverse Rendering. (arXiv:2203.16284v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16284">
<div class="article-summary-box-inner">
<span><p>Implicit neural representations of 3D shapes form strong priors that are
useful for various applications, such as single and multiple view 3D
reconstruction. A downside of existing neural representations is that they
require multiple network evaluations for rendering, which leads to high
computational costs. This limitation forms a bottleneck particularly in the
context of inverse problems, such as image-based 3D reconstruction. To address
this issue, in this paper (i) we propose a novel hybrid 3D object
representation based on a signed distance function (SDF) that we augment with a
directional distance function (DDF), so that we can predict distances to the
object surface from any point on a sphere enclosing the object. Moreover, (ii)
using the proposed hybrid representation we address the multi-view consistency
problem common in existing DDF representations. We evaluate our novel hybrid
representation on the task of single-view depth reconstruction and show that
our method is several times faster compared to competing methods, while at the
same time achieving better reconstruction accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region of Interest focused MRI to Synthetic CT Translation using Regression and Classification Multi-task Network. (arXiv:2203.16288v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16288">
<div class="article-summary-box-inner">
<span><p>In this work, we present a method for synthetic CT (sCT) generation from
zero-echo-time (ZTE) MRI aimed at structural and quantitative accuracies of the
image, with a particular focus on the accurate bone density value prediction.
We propose a loss function that favors a spatially sparse region in the image.
We harness the ability of a multi-task network to produce correlated outputs as
a framework to enable localisation of region of interest (RoI) via
classification, emphasize regression of values within RoI and still retain the
overall accuracy via global regression. The network is optimized by a composite
loss function that combines a dedicated loss from each task. We demonstrate how
the multi-task network with RoI focused loss offers an advantage over other
configurations of the network to achieve higher accuracy of performance. This
is relevant to sCT where failure to accurately estimate high Hounsfield Unit
values of bone could lead to impaired accuracy in clinical applications. We
compare the dose calculation maps from the proposed sCT and the real CT in a
radiation therapy treatment planning setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AmsterTime: A Visual Place Recognition Benchmark Dataset for Severe Domain Shift. (arXiv:2203.16291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16291">
<div class="article-summary-box-inner">
<span><p>We introduce AmsterTime: a challenging dataset to benchmark visual place
recognition (VPR) in presence of a severe domain shift. AmsterTime offers a
collection of 2,500 well-curated images matching the same scene from a street
view matched to historical archival image data from Amsterdam city. The image
pairs capture the same place with different cameras, viewpoints, and
appearances. Unlike existing benchmark datasets, AmsterTime is directly
crowdsourced in a GIS navigation platform (Mapillary). We evaluate various
baselines, including non-learning, supervised and self-supervised methods,
pre-trained on different relevant datasets, for both verification and retrieval
tasks. Our result credits the best accuracy to the ResNet-101 model pre-trained
on the Landmarks dataset for both verification and retrieval tasks by 84% and
24%, respectively. Additionally, a subset of Amsterdam landmarks is collected
for feature evaluation in a classification task. Classification labels are
further used to extract the visual explanations using Grad-CAM for inspection
of the learned similar visuals in a deep metric learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forecasting from LiDAR via Future Object Detection. (arXiv:2203.16297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16297">
<div class="article-summary-box-inner">
<span><p>Object detection and forecasting are fundamental components of embodied
perception. These two problems, however, are largely studied in isolation by
the community. In this paper, we propose an end-to-end approach for detection
and motion forecasting based on raw sensor measurement as opposed to ground
truth tracks. Instead of predicting the current frame locations and forecasting
forward in time, we directly predict future object locations and backcast to
determine where each trajectory began. Our approach not only improves overall
accuracy compared to other modular or end-to-end baselines, it also prompts us
to rethink the role of explicit tracking for embodied perception. Additionally,
by linking future and current locations in a many-to-one manner, our approach
is able to reason about multiple futures, a capability that was previously
considered difficult for end-to-end approaches. We conduct extensive
experiments on the popular nuScenes dataset and demonstrate the empirical
effectiveness of our approach. In addition, we investigate the appropriateness
of reusing standard forecasting metrics for an end-to-end setup, and find a
number of limitations which allow us to build simple baselines to game these
metrics. We address this issue with a novel set of joint forecasting and
detection metrics that extend the commonly used AP metrics from the detection
community to measuring forecasting accuracy. Our code is available on
\href{https://github.com/neeharperi/FutureDet}{GitHub}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PEGG-Net: Background Agnostic Pixel-Wise Efficient Grasp Generation Under Closed-Loop Conditions. (arXiv:2203.16301v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16301">
<div class="article-summary-box-inner">
<span><p>Performing closed-loop grasping at close proximity to an object requires a
large field of view. However, such images will inevitably bring large amounts
of unnecessary background information, especially when the camera is far away
from the target object at the initial stage, resulting in performance
degradation of the grasping network. To address this problem, we design a novel
PEGG-Net, a real-time, pixel-wise, robotic grasp generation network. The
proposed lightweight network is inherently able to learn to remove background
noise that can reduce grasping accuracy. Our proposed PEGG-Net achieves
improved state-of-the-art performance on both Cornell dataset (98.9%) and
Jacquard dataset (93.8%). In the real-world tests, PEGG-Net can support
closed-loop grasping at up to 50Hz using an image size of 480x480 in dynamic
environments. The trained model also generalizes to previously unseen objects
with complex geometrical shapes, household objects and workshop tools and
achieved an overall grasp success rate of 91.2% in our real-world grasping
experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection. (arXiv:2203.16317v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16317">
<div class="article-summary-box-inner">
<span><p>In this paper, we delve into two key techniques in Semi-Supervised Object
Detection (SSOD), namely pseudo labeling and consistency training. We observe
that these two techniques currently neglect some important properties of object
detection, hindering efficient learning on unlabeled data. Specifically, for
pseudo labeling, existing works only focus on the classification score yet fail
to guarantee the localization precision of pseudo boxes; For consistency
training, the widely adopted random-resize training only considers the
label-level consistency but misses the feature-level one, which also plays an
important role in ensuring the scale invariance. To address the problems
incurred by noisy pseudo boxes, we design Noisy Pseudo box Learning (NPL) that
includes Prediction-guided Label Assignment (PLA) and Positive-proposal
Consistency Voting (PCV). PLA relies on model predictions to assign labels and
makes it robust to even coarse pseudo boxes; while PCV leverages the regression
consistency of positive proposals to reflect the localization quality of pseudo
boxes. Furthermore, in consistency training, we propose Multi-view
Scale-invariant Learning (MSL) that includes mechanisms of both label- and
feature-level consistency, where feature consistency is achieved by aligning
shifted feature pyramids between two images with identical content but varied
scales. On COCO benchmark, our method, termed PSEudo labeling and COnsistency
training (PseCo), outperforms the SOTA (Soft Teacher) by 2.0, 1.8, 2.0 points
under 1%, 5%, and 10% labelling ratios, respectively. It also significantly
improves the learning efficiency for SSOD, e.g., PseCo halves the training time
of the SOTA approach but achieves even better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Robot Active Mapping via Neural Bipartite Graph Matching. (arXiv:2203.16319v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16319">
<div class="article-summary-box-inner">
<span><p>We study the problem of multi-robot active mapping, which aims for complete
scene map construction in minimum time steps. The key to this problem lies in
the goal position estimation to enable more efficient robot movements. Previous
approaches either choose the frontier as the goal position via a myopic
solution that hinders the time efficiency, or maximize the long-term value via
reinforcement learning to directly regress the goal position, but does not
guarantee the complete map construction. In this paper, we propose a novel
algorithm, namely NeuralCoMapping, which takes advantage of both approaches. We
reduce the problem to bipartite graph matching, which establishes the node
correspondences between two graphs, denoting robots and frontiers. We introduce
a multiplex graph neural network (mGNN) that learns the neural distance to fill
the affinity matrix for more effective graph matching. We optimize the mGNN
with a differentiable linear assignment layer by maximizing the long-term
values that favor time efficiency and map completeness via reinforcement
learning. We compare our algorithm with several state-of-the-art multi-robot
active mapping approaches and adapted reinforcement-learning baselines.
Experimental results demonstrate the superior performance and exceptional
generalization ability of our algorithm on various indoor scenes and unseen
number of robots, when only trained with 9 indoor scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification. (arXiv:2203.16325v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16325">
<div class="article-summary-box-inner">
<span><p>Existing deep learning based methods effectively prompt the performance of
aerial scene classification. However, due to the large amount of parameters and
computational cost, it is rather difficult to apply these methods to multiple
real-time remote sensing applications such as on-board data preception on
drones and satellites. In this paper, we address this task by developing a
light-weight ConvNet named multi-stage duplex fusion network (MSDF-Net). The
key idea is to use parameters as little as possible while obtaining as strong
as possible scene representation capability. To this end, a residual-dense
duplex fusion strategy is developed to enhance the feature propagation while
re-using parameters as much as possible, and is realized by our duplex fusion
block (DFblock). Specifically, our MSDF-Net consists of multi-stage structures
with DFblock. Moreover, duplex semantic aggregation (DSA) module is developed
to mine the remote sensing scene information from extracted convolutional
features, which also contains two parallel branches for semantic description.
Extensive experiments are conducted on three widely-used aerial scene
classification benchmarks, and reflect that our MSDF-Net can achieve a
competitive performance against the recent state-of-art while reducing up to
80% parameter numbers. Particularly, an accuracy of 92.96% is achieved on AID
with only 0.49M parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smooth Robust Tensor Completion for Background/Foreground Separation with Missing Pixels: Novel Algorithm with Convergence Guarantee. (arXiv:2203.16328v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16328">
<div class="article-summary-box-inner">
<span><p>The objective of this study is to address the problem of
background/foreground separation with missing pixels by combining the video
acquisition, video recovery, background/foreground separation into a single
framework. To achieve this, a smooth robust tensor completion (SRTC) model is
proposed to recover the data and decompose it into the static background and
smooth foreground, respectively. Specifically, the static background is modeled
by the low-rank tucker decomposition and the smooth foreground (moving objects)
is modeled by the spatiotemporal continuity, which is enforced by the total
variation regularization. An efficient algorithm based on tensor proximal
alternating minimization (tenPAM) is implemented to solve the proposed model
with global convergence guarantee under very mild conditions. Extensive
experiments on real data demonstrate that the proposed method significantly
outperforms the state-of-the-art approaches for background/foreground
separation with missing pixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-efficient Fine-tuning for Vision Transformers. (arXiv:2203.16329v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16329">
<div class="article-summary-box-inner">
<span><p>In computer vision, it has achieved great success in adapting large-scale
pretrained vision models (e.g., Vision Transformer) to downstream tasks via
fine-tuning. Common approaches for fine-tuning either update all model
parameters or leverage linear probes. In this paper, we aim to study
parameter-efficient fine-tuning strategies for Vision Transformers on vision
tasks. We formulate efficient fine-tuning as a subspace training problem and
perform a comprehensive benchmarking over different efficient fine-tuning
methods. We conduct an empirical study on each efficient fine-tuning method
focusing on its performance alongside parameter cost. Furthermore, we also
propose a parameter-efficient fine-tuning framework, which first selects
submodules by measuring local intrinsic dimensions and then projects them into
subspace for further decomposition via a novel Kronecker Adaptation method. We
analyze and compare our method with a diverse set of baseline fine-tuning
methods (including state-of-the-art methods for pretrained language models).
Our method performs the best in terms of the tradeoff between accuracy and
parameter efficiency across three commonly used image classification datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On handwriting pressure normalization for interoperability of different acquisition stylus. (arXiv:2203.16337v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16337">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a pressure characterization and normalization
procedure for online handwritten acquisition. Normalization process has been
tested in biometric recognition experiments (identification and verification)
using online signature database MCYT, which consists of the signatures from 330
users. The goal is to analyze the real mismatch scenarios where users are
enrolled with one stylus and then, later on, they produce some testing samples
using a different stylus model with different pressure response. Experimental
results show: 1) a saturation behavior in pressure signal 2) different dynamic
ranges in the different stylus studied 3) improved biometric recognition
accuracy by means of pressure signal normalization as well as a performance
degradation in mismatched conditions 4) interoperability between different
stylus can be obtained by means of pressure normalization. Normalization
produces an improvement in signature identification rates higher than 7%
(absolute value) when compared with mismatched scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stack operation of tensor networks. (arXiv:2203.16338v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16338">
<div class="article-summary-box-inner">
<span><p>The tensor network, as a facterization of tensors, aims at performing the
operations that are common for normal tensors, such as addition, contraction
and stacking. However, due to its non-unique network structure, only the tensor
network contraction is so far well defined. In this paper, we propose a
mathematically rigorous definition for the tensor network stack approach, that
compress a large amount of tensor networks into a single one without changing
their structures and configurations. We illustrate the main ideas with the
matrix product states based machine learning as an example. Our results are
compared with the for loop and the efficient coding method on both CPU and GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Learned Lossless JPEG Recompression with Multi-Level Cross-Channel Entropy Model in the DCT Domain. (arXiv:2203.16357v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16357">
<div class="article-summary-box-inner">
<span><p>JPEG is a popular image compression method widely used by individuals, data
center, cloud storage and network filesystems. However, most recent progress on
image compression mainly focuses on uncompressed images while ignoring
trillions of already-existing JPEG images. To compress these JPEG images
adequately and restore them back to JPEG format losslessly when needed, we
propose a deep learning based JPEG recompression method that operates on DCT
domain and propose a Multi-Level Cross-Channel Entropy Model to compress the
most informative Y component. Experiments show that our method achieves
state-of-the-art performance compared with traditional JPEG recompression
methods including Lepton, JPEG XL and CMIX. To the best of our knowledge, this
is the first learned compression method that losslessly transcodes JPEG images
to more storage-saving bitstreams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CardioID: Mitigating the Effects of Irregular Cardiac Signals for Biometric Identification. (arXiv:2203.16381v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16381">
<div class="article-summary-box-inner">
<span><p>Cardiac patterns are being used to obtain hard-to-forge biometric signatures
and have led to high accuracy in state-of-the-art (SoA) identification
applications. However, this performance is obtained under controlled scenarios
where cardiac signals maintain a relatively uniform pattern, facilitating the
identification process. In this work, we analyze cardiac signals collected in
more realistic (uncontrolled) scenarios and show that their high signal
variability (i.e., irregularity) makes it harder to obtain stable and distinct
user features. Furthermore, SoA usually fails to identify specific groups of
users, rendering existing identification methods futile in uncontrolled
scenarios. To solve these problems, we propose a framework with three novel
properties. First, we design an adaptive method that achieves stable and
distinct features by tailoring the filtering spectrum to each user. Second, we
show that users can have multiple cardiac morphologies, offering us a much
bigger pool of cardiac signals and users compared to SoA. Third, we overcome
other distortion effects present in authentication applications with a
multi-cluster approach and the Mahalanobis distance. Our evaluation shows that
the average balanced accuracy (BAC) of SoA drops from above 90% in controlled
scenarios to 75% in uncontrolled ones, while our method maintains an average
BAC above 90% in uncontrolled scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On learning adaptive acquisition policies for undersampled multi-coil MRI reconstruction. (arXiv:2203.16392v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16392">
<div class="article-summary-box-inner">
<span><p>Most current approaches to undersampled multi-coil MRI reconstruction focus
on learning the reconstruction model for a fixed, equidistant acquisition
trajectory. In this paper, we study the problem of joint learning of the
reconstruction model together with acquisition policies. To this end, we extend
the End-to-End Variational Network with learnable acquisition policies that can
adapt to different data points. We validate our model on a coil-compressed
version of the large scale undersampled multi-coil fastMRI dataset using two
undersampling factors: $4\times$ and $8\times$. Our experiments show on-par
performance with the learnable non-adaptive and handcrafted equidistant
strategies at $4\times$, and an observed improvement of more than $2\%$ in SSIM
at $8\times$ acceleration, suggesting that potentially-adaptive $k$-space
acquisition trajectories can improve reconstructed image quality for larger
acceleration factors. However, and perhaps surprisingly, our best performing
policies learn to be explicitly non-adaptive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Motion Style Transfer for Interactive Character Control. (arXiv:2203.16393v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16393">
<div class="article-summary-box-inner">
<span><p>Motion style transfer is highly desired for motion generation systems for
gaming. Compared to its offline counterpart, the research on online motion
style transfer under interactive control is limited. In this work, we propose
an end-to-end neural network that can generate motions with different styles
and transfer motion styles in real-time under user control. Our approach
eliminates the use of handcrafted phase features, and could be easily trained
and directly deployed in game systems. In the experiment part, we evaluate our
approach from three aspects that are essential for industrial game design:
accuracy, flexibility, and variety, and our model performs a satisfying result.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognition of polar lows in Sentinel-1 SAR images with deep learning. (arXiv:2203.16401v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16401">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the possibility of detecting polar lows in C-band
SAR images by means of deep learning. Specifically, we introduce a novel
dataset consisting of Sentinel-1 images labeled as positive; representing a
maritime mesocyclone, or negative; representing a normal sea state. The dataset
is constructed using the ERA5 dataset as baseline and it consists of 2004
annotated images. To our knowledge, this is the first dataset of its kind to be
publicly released. The dataset is used to train a deep learning model to
classify the labeled images. Evaluated on an independent test set, the model
yields an F-1 score of 0.95, indicating that polar lows can be consistently
detected from SAR images. Interpretability techniques applied to the deep
learning model reveal that atmospheric fronts and cyclonic eyes are key
features in the classification. Moreover, experimental results show that the
model is accurate even if: (i) such features are significantly cropped due to
the limited swath width of the SAR, (ii) the features are partly covered by sea
ice and (iii) land is covering significant parts of the images. By evaluating
the model performance on multiple input image resolutions (pixel sizes of 500m,
1km and 2km), it is found that higher resolution yield the best performance.
This emphasises the potential of using high resolution sensors like SAR for
detecting polar lows, as compared to conventionally used sensors such as
scatterometers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Vision Transformers: Attention-Based Modelling applied to Cortical Analysis. (arXiv:2203.16414v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16414">
<div class="article-summary-box-inner">
<span><p>The extension of convolutional neural networks (CNNs) to non-Euclidean
geometries has led to multiple frameworks for studying manifolds. Many of those
methods have shown design limitations resulting in poor modelling of long-range
associations, as the generalisation of convolutions to irregular surfaces is
non-trivial. Motivated by the success of attention-modelling in computer
vision, we translate convolution-free vision transformer approaches to surface
data, to introduce a domain-agnostic architecture to study any surface data
projected onto a spherical manifold. Here, surface patching is achieved by
representing spherical data as a sequence of triangular patches, extracted from
a subdivided icosphere. A transformer model encodes the sequence of patches via
successive multi-head self-attention layers while preserving the sequence
resolution. We validate the performance of the proposed Surface Vision
Transformer (SiT) on the task of phenotype regression from cortical surface
metrics derived from the Developing Human Connectome Project (dHCP).
Experiments show that the SiT generally outperforms surface CNNs, while
performing comparably on registered and unregistered data. Analysis of
transformer attention maps offers strong potential to characterise subtle
cognitive developmental patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The impact of using voxel-level segmentation metrics on evaluating multifocal prostate cancer localisation. (arXiv:2203.16415v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16415">
<div class="article-summary-box-inner">
<span><p>Dice similarity coefficient (DSC) and Hausdorff distance (HD) are widely used
for evaluating medical image segmentation. They have also been criticised, when
reported alone, for their unclear or even misleading clinical interpretation.
DSCs may also differ substantially from HDs, due to boundary smoothness or
multiple regions of interest (ROIs) within a subject. More importantly, either
metric can also have a nonlinear, non-monotonic relationship with outcomes
based on Type 1 and 2 errors, designed for specific clinical decisions that use
the resulting segmentation. Whilst cases causing disagreement between these
metrics are not difficult to postulate. This work first proposes a new
asymmetric detection metric, adapting those used in object detection, for
planning prostate cancer procedures. The lesion-level metrics is then compared
with the voxel-level DSC and HD, whereas a 3D UNet is used for segmenting
lesions from multiparametric MR (mpMR) images. Based on experimental results we
report pairwise agreement and correlation 1) between DSC and HD, and 2) between
voxel-level DSC and recall-controlled precision at lesion-level, with Cohen's
[0.49, 0.61] and Pearson's [0.66, 0.76] (p-values}&lt;0.001) at varying cut-offs.
However, the differences in false-positives and false-negatives, between the
actual errors and the perceived counterparts if DSC is used, can be as high as
152 and 154, respectively, out of the 357 test set lesions. We therefore
carefully conclude that, despite of the significant correlations, voxel-level
metrics such as DSC can misrepresent lesion-level detection accuracy for
evaluating localisation of multifocal prostate cancer and should be interpreted
with caution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPD: Single-view 3D Openable Part Detection. (arXiv:2203.16421v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16421">
<div class="article-summary-box-inner">
<span><p>We address the task of predicting what parts of an object can open and how
they move when they do so. The input is a single image of an object, and as
output we detect what parts of the object can open, and the motion parameters
describing the articulation of each openable part. To tackle this task, we
create two datasets of 3D objects: OPDSynth based on existing synthetic
objects, and OPDReal based on RGBD reconstructions of real objects. We then
design OPDRCNN, a neural architecture that detects openable parts and predicts
their motion parameters. Our experiments show that this is a challenging task
especially when considering generalization across object categories, and the
limited amount of information in a single image. Our architecture outperforms
baselines and prior work especially for RGB image inputs. Short video summary
at https://www.youtube.com/watch?v=P85iCaD0rfc
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balanced MSE for Imbalanced Visual Regression. (arXiv:2203.16427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16427">
<div class="article-summary-box-inner">
<span><p>Data imbalance exists ubiquitously in real-world visual regressions, e.g.,
age estimation and pose estimation, hurting the model's generalizability and
fairness. Thus, imbalanced regression gains increasing research attention
recently. Compared to imbalanced classification, imbalanced regression focuses
on continuous labels, which can be boundless and high-dimensional and hence
more challenging. In this work, we identify that the widely used Mean Square
Error (MSE) loss function can be ineffective in imbalanced regression. We
revisit MSE from a statistical view and propose a novel loss function, Balanced
MSE, to accommodate the imbalanced training label distribution. We further
design multiple implementations of Balanced MSE to tackle different real-world
scenarios, particularly including the one that requires no prior knowledge
about the training label distribution. Moreover, to the best of our knowledge,
Balanced MSE is the first general solution to high-dimensional imbalanced
regression. Extensive experiments on both synthetic and three real-world
benchmarks demonstrate the effectiveness of Balanced MSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TubeDETR: Spatio-Temporal Video Grounding with Transformers. (arXiv:2203.16434v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16434">
<div class="article-summary-box-inner">
<span><p>We consider the problem of localizing a spatio-temporal tube in a video
corresponding to a given text query. This is a challenging task that requires
the joint and efficient modeling of temporal, spatial and multi-modal
interactions. To address this task, we propose TubeDETR, a transformer-based
architecture inspired by the recent success of such models for text-conditioned
object detection. Our model notably includes: (i) an efficient video and text
encoder that models spatial multi-modal interactions over sparsely sampled
frames and (ii) a space-time decoder that jointly performs spatio-temporal
localization. We demonstrate the advantage of our proposed components through
an extensive ablation study. We also evaluate our full approach on the
spatio-temporal video grounding task and demonstrate improvements over the
state of the art on the challenging VidSTG and HC-STVG benchmarks. Code and
trained models are publicly available at
https://antoyang.github.io/tubedetr.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16475">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have been widely used for decision making,
prompting a surge of interest in interpreting how these complex models work.
Recent literature on DNN interpretation has revolved around already-trained
models; however, much less research focuses on interpreting how the models
evolve as they are trained. Interpreting model evolution is crucial to monitor
network training and can aid proactive decisions about necessary interventions.
In this work, we present ConceptEvo, a general interpretation framework for
DNNs that reveals the inception and evolution of detected concepts during
training. Through a large-scale human evaluation with 260 participants and
quantitative experiments, we show that ConceptEvo discovers evolution across
different models that are meaningful to humans, helpful for early-training
intervention decisions, and crucial to the prediction for a given class.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RFNet-4D: Joint Object Reconstruction and Flow Estimation from 4D Point Clouds. (arXiv:2203.16482v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16482">
<div class="article-summary-box-inner">
<span><p>Object reconstruction from 3D point clouds has achieved impressive progress
in the computer vision and computer graphics research field. However,
reconstruction from time-varying point clouds (a.k.a. 4D point clouds) is
generally overlooked. In this paper, we propose a new network architecture,
namely RFNet-4D, that jointly reconstructs objects and their motion flows from
4D point clouds. The key insight is that simultaneously performing both tasks
via learning spatial and temporal features from a sequence of point clouds can
leverage individual tasks and lead to improved overall performance. The
proposed network can be trained using both supervised and unsupervised
learning. To prove this ability, we design a temporal vector field learning
module using an unsupervised learning approach for flow estimation, leveraged
by supervised learning of spatial structures for object reconstruction.
Extensive experiments and analyses on benchmark dataset validated the
effectiveness and efficiency of our method. As shown in experimental results,
our method achieves state-of-the-art performance on both flow estimation and
object reconstruction while performing much faster than existing methods in
both training and inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Foveation-based Deep Video Compression without Motion Search. (arXiv:2203.16490v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16490">
<div class="article-summary-box-inner">
<span><p>The requirements of much larger file sizes, different storage formats, and
immersive viewing conditions of VR pose significant challenges to the goals of
acquiring, transmitting, compressing, and displaying high-quality VR content.
At the same time, the great potential of deep learning to advance progress on
the video compression problem has driven a significant research effort. Because
of the high bandwidth requirements of VR, there has also been significant
interest in the use of space-variant, foveated compression protocols. We have
integrated these techniques to create an end-to-end deep learning video
compression framework. A feature of our new compression model is that it
dispenses with the need for expensive search-based motion prediction
computations. This is accomplished by exploiting statistical regularities
inherent in video motion expressed by displaced frame differences. Foveation
protocols are desirable since only a small portion of a video viewed in VR may
be visible as a user gazes in any given direction. Moreover, even within a
current field of view (FOV), the resolution of retinal neurons rapidly
decreases with distance (eccentricity) from the projected point of gaze. In our
learning based approach, we implement foveation by introducing a Foveation
Generator Unit (FGU) that generates foveation masks which direct the allocation
of bits, significantly increasing compression efficiency while making it
possible to retain an impression of little to no additional visual loss given
an appropriate viewing geometry. Our experiment results reveal that our new
compression model, which we call the Foveated MOtionless VIdeo Codec (Foveated
MOVI-Codec), is able to efficiently compress videos without computing motion,
while outperforming foveated version of both H.264 and H.265 on the widely used
UVG dataset and on the HEVC Standard Class B Test Sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast, Accurate and Memory-Efficient Partial Permutation Synchronization. (arXiv:2203.16505v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16505">
<div class="article-summary-box-inner">
<span><p>Previous partial permutation synchronization (PPS) algorithms, which are
commonly used for multi-object matching, often involve computation-intensive
and memory-demanding matrix operations. These operations become intractable for
large scale structure-from-motion datasets. For pure permutation
synchronization, the recent Cycle-Edge Message Passing (CEMP) framework
suggests a memory-efficient and fast solution. Here we overcome the restriction
of CEMP to compact groups and propose an improved algorithm, CEMP-Partial, for
estimating the corruption levels of the observed partial permutations. It
allows us to subsequently implement a nonconvex weighted projected power method
without the need of spectral initialization. The resulting new PPS algorithm,
MatchFAME (Fast, Accurate and Memory-Efficient Matching), only involves sparse
matrix operations, and thus enjoys lower time and space complexities in
comparison to previous PPS algorithms. We prove that under adversarial
corruption, though without additive noise and with certain assumptions,
CEMP-Partial is able to exactly classify corrupted and clean partial
permutations. We demonstrate the state-of-the-art accuracy, speed and memory
efficiency of our method on both synthetic and real datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Improved Lightweight YOLOv5 Model Based on Attention Mechanism for Face Mask Detection. (arXiv:2203.16506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16506">
<div class="article-summary-box-inner">
<span><p>Coronavirus 2019 has brought severe challenges to social stability and public
health worldwide. One effective way of curbing the epidemic is to require
people to wear masks in public places and monitor mask-wearing states by
utilizing suitable automatic detectors. However, existing deep learning based
models struggle to simultaneously achieve the requirements of both high
precision and real-time performance. To solve this problem, we propose an
improved lightweight face mask detector based on YOLOv5, which can achieve an
excellent balance of precision and speed. Firstly, a novel backbone
ShuffleCANet that combines ShuffleNetV2 network with Coordinate Attention
mechanism is proposed as the backbone. Then we use BiFPN as the feature fusion
neck. Furthermore, we replace the loss function of localization with -CIoU to
obtain higher-quality anchors. Some valuable strategies such as data
augmentation, adaptive image scaling, and anchor cluster operation are also
utilized. Experimental results show the performance and effectiveness of the
proposed model. On the basis of the original YOLOv5 model, our work increases
the inference speed by 28.3% while still improving the precision by 0.58% on
the AIZOO face mask dataset. It achieves a mean average precision of 95.2%,
which is 4.4% higher than the baseline and is also more accurate compared with
other existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaMixer: A Fast-Converging Query-Based Object Detector. (arXiv:2203.16507v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16507">
<div class="article-summary-box-inner">
<span><p>Traditional object detectors employ the dense paradigm of scanning over
locations and scales in an image. The recent query-based object detectors break
this convention by decoding image features with a set of learnable queries.
However, this paradigm still suffers from slow convergence, limited
performance, and design complexity of extra networks between backbone and
decoder. In this paper, we find that the key to these issues is the
adaptability of decoders for casting queries to varying objects. Accordingly,
we propose a fast-converging query-based detector, named AdaMixer, by improving
the adaptability of query-based decoding processes in two aspects. First, each
query adaptively samples features over space and scales based on estimated
offsets, which allows AdaMixer to efficiently attend to the coherent regions of
objects. Then, we dynamically decode these sampled features with an adaptive
MLP-Mixer under the guidance of each query. Thanks to these two critical
designs, AdaMixer enjoys architectural simplicity without requiring dense
attentional encoders or explicit pyramid networks. On the challenging MS COCO
benchmark, AdaMixer with ResNet-50 as the backbone, with 12 training epochs,
reaches up to 45.0 AP on the validation set along with 27.9 APs in detecting
small objects. With the longer training scheme, AdaMixer with ResNeXt-101-DCN
and Swin-S reaches 49.5 and 51.3 AP. Our work sheds light on a simple,
accurate, and fast converging architecture for query-based object detectors.
The code is made available at https://github.com/MCG-NJU/AdaMixer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptDet: Expand Your Detector Vocabulary with Uncurated Images. (arXiv:2203.16513v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16513">
<div class="article-summary-box-inner">
<span><p>The goal of this work is to establish a scalable pipeline for expanding an
object detector towards novel/unseen categories, using zero manual annotations.
To achieve that, we make the following four contributions: (i) in pursuit of
generalisation, we propose a two-stage open-vocabulary object detector that
categorises each box proposal by a classifier generated from the text encoder
of a pre-trained visual-language model; (ii) To pair the visual latent space
(from RPN box proposal) with that of the pre-trained text encoder, we propose
the idea of regional prompt learning to optimise a couple of learnable prompt
vectors, converting the textual embedding space to fit those visually
object-centric images; (iii) To scale up the learning procedure towards
detecting a wider spectrum of objects, we exploit the available online
resource, iteratively updating the prompts, and later self-training the
proposed detector with pseudo labels generated on a large corpus of noisy,
uncurated web images. The self-trained detector, termed as PromptDet,
significantly improves the detection performance on categories for which manual
annotations are unavailable or hard to obtain, e.g. rare categories. Finally,
(iv) to validate the necessity of our proposed components, we conduct extensive
experiments on the challenging LVIS and MS-COCO dataset, showing superior
performance over existing approaches with fewer additional training images and
zero manual annotations whatsoever. Project page with code:
https://fcjian.github.io/promptdet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Light-Weight Near-Field Photometric Stereo. (arXiv:2203.16515v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16515">
<div class="article-summary-box-inner">
<span><p>We introduce the first end-to-end learning-based solution to near-field
Photometric Stereo (PS), where the light sources are close to the object of
interest. This setup is especially useful for reconstructing large immobile
objects. Our method is fast, producing a mesh from 52 512$\times$384 resolution
images in about 1 second on a commodity GPU, thus potentially unlocking several
AR/VR applications. Existing approaches rely on optimization coupled with a
far-field PS network operating on pixels or small patches. Using optimization
makes these approaches slow and memory intensive (requiring 17GB GPU and 27GB
of CPU memory) while using only pixels or patches makes them highly susceptible
to noise and calibration errors. To address these issues, we develop a
recursive multi-resolution scheme to estimate surface normal and depth maps of
the whole image at each step. The predicted depth map at each scale is then
used to estimate `per-pixel lighting' for the next scale. This design makes our
approach almost 45$\times$ faster and 2$^{\circ}$ more accurate (11.3$^{\circ}$
vs. 13.3$^{\circ}$ Mean Angular Error) than the state-of-the-art near-field PS
reconstruction technique, which uses iterative optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unseen Classes at a Later Time? No Problem. (arXiv:2203.16517v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16517">
<div class="article-summary-box-inner">
<span><p>Recent progress towards learning from limited supervision has encouraged
efforts towards designing models that can recognize novel classes at test time
(generalized zero-shot learning or GZSL). GZSL approaches assume knowledge of
all classes, with or without labeled data, beforehand. However, practical
scenarios demand models that are adaptable and can handle dynamic addition of
new seen and unseen classes on the fly (that is continual generalized zero-shot
learning or CGZSL). One solution is to sequentially retrain and reuse
conventional GZSL methods, however, such an approach suffers from catastrophic
forgetting leading to suboptimal generalization performance. A few recent
efforts towards tackling CGZSL have been limited by difference in settings,
practicality, data splits and protocols followed-inhibiting fair comparison and
a clear direction forward. Motivated from these observations, in this work, we
firstly consolidate the different CGZSL setting variants and propose a new
Online-CGZSL setting which is more practical and flexible. Secondly, we
introduce a unified feature-generative framework for CGZSL that leverages
bi-directional incremental alignment to dynamically adapt to addition of new
classes, with or without labeled data, that arrive over time in any of these
CGZSL settings. Our comprehensive experiments and analysis on five benchmark
datasets and comparison with baselines show that our approach consistently
outperforms existing methods, especially on the more practical Online setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Transformers for Grounded Situation Recognition. (arXiv:2203.16518v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16518">
<div class="article-summary-box-inner">
<span><p>Grounded situation recognition is the task of predicting the main activity,
entities playing certain roles within the activity, and bounding-box groundings
of the entities in the given image. To effectively deal with this challenging
task, we introduce a novel approach where the two processes for activity
classification and entity estimation are interactive and complementary. To
implement this idea, we propose Collaborative Glance-Gaze TransFormer
(CoFormer) that consists of two modules: Glance transformer for activity
classification and Gaze transformer for entity estimation. Glance transformer
predicts the main activity with the help of Gaze transformer that analyzes
entities and their relations, while Gaze transformer estimates the grounded
entities by focusing only on the entities relevant to the activity predicted by
Glance transformer. Our CoFormer achieves the state of the art in all
evaluation metrics on the SWiG dataset. Training code and model weights are
available at https://github.com/jhcho99/CoFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoordGAN: Self-Supervised Dense Correspondences Emerge from GANs. (arXiv:2203.16521v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16521">
<div class="article-summary-box-inner">
<span><p>Recent advances show that Generative Adversarial Networks (GANs) can
synthesize images with smooth variations along semantically meaningful latent
directions, such as pose, expression, layout, etc. While this indicates that
GANs implicitly learn pixel-level correspondences across images, few studies
explored how to extract them explicitly. In this work, we introduce Coordinate
GAN (CoordGAN), a structure-texture disentangled GAN that learns a dense
correspondence map for each generated image. We represent the correspondence
maps of different images as warped coordinate frames transformed from a
canonical coordinate frame, i.e., the correspondence map, which describes the
structure (e.g., the shape of a face), is controlled via a transformation.
Hence, finding correspondences boils down to locating the same coordinate in
different correspondence maps. In CoordGAN, we sample a transformation to
represent the structure of a synthesized instance, while an independent texture
branch is responsible for rendering appearance details orthogonal to the
structure. Our approach can also extract dense correspondence maps for real
images by adding an encoder on top of the generator. We quantitatively
demonstrate the quality of the learned dense correspondences through
segmentation mask transfer on multiple datasets. We also show that the proposed
generator achieves better structure and texture disentanglement compared to
existing approaches. Project page: https://jitengmu.github.io/CoordGAN/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Plain Vision Transformer Backbones for Object Detection. (arXiv:2203.16527v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16527">
<div class="article-summary-box-inner">
<span><p>We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone
network for object detection. This design enables the original ViT architecture
to be fine-tuned for object detection without needing to redesign a
hierarchical backbone for pre-training. With minimal adaptations for
fine-tuning, our plain-backbone detector can achieve competitive results.
Surprisingly, we observe: (i) it is sufficient to build a simple feature
pyramid from a single-scale feature map (without the common FPN design) and
(ii) it is sufficient to use window attention (without shifting) aided with
very few cross-window propagation blocks. With plain ViT backbones pre-trained
as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the
previous leading methods that were all based on hierarchical backbones,
reaching up to 61.3 box AP on the COCO dataset using only ImageNet-1K
pre-training. We hope our study will draw attention to research on
plain-backbone detectors. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L^3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors. (arXiv:2203.16528v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16528">
<div class="article-summary-box-inner">
<span><p>In this research, we propose a tiny image segmentation model, L^3U-net, that
works on low-resource edge devices in real-time. We introduce a data folding
technique that reduces inference latency by leveraging the parallel
convolutional layer processing capability of the CNN accelerators. We also
deploy the proposed model to such a device, MAX78000, and the results show that
L^3U-net achieves more than 90% accuracy over two different segmentation
datasets with 10 fps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic Surface Representation via Neural Homeomorphism. (arXiv:2203.16529v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16529">
<div class="article-summary-box-inner">
<span><p>While neural representations for static 3D shapes are widely studied,
representations for deformable surfaces are limited to be template-dependent or
lack efficiency. We introduce Canonical Deformation Coordinate Space (CaDeX), a
unified representation of both shape and nonrigid motion. Our key insight is
the factorization of the deformation between frames by continuous bijective
canonical maps (homeomorphisms) and their inverses that go through a learned
canonical shape. Our novel deformation representation and its implementation
are simple, efficient, and guarantee cycle consistency, topology preservation,
and, if needed, volume conservation. Our modelling of the learned canonical
shapes provides a flexible and stable space for shape prior learning. We
demonstrate state-of-the-art performance in modelling a wide range of
deformable geometries: human bodies, animal bodies, and articulated objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Instance-Specific Adaptation for Cross-Domain Segmentation. (arXiv:2203.16530v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16530">
<div class="article-summary-box-inner">
<span><p>We propose a test-time adaptation method for cross-domain image segmentation.
Our method is simple: Given a new unseen instance at test time, we adapt a
pre-trained model by conducting instance-specific BatchNorm (statistics)
calibration. Our approach has two core components. First, we replace the
manually designed BatchNorm calibration rule with a learnable module. Second,
we leverage strong data augmentation to simulate random domain shifts for
learning the calibration rule. In contrast to existing domain adaptation
methods, our method does not require accessing the target domain data at
training time or conducting computationally expensive test-time model
training/optimization. Equipping our method with models trained by standard
recipes achieves significant improvement, comparing favorably with several
state-of-the-art domain generalization and one-shot unsupervised domain
adaptation approaches. Combining our method with the domain generalization
methods further improves performance, reaching a new state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding 3D Object Articulation in Internet Videos. (arXiv:2203.16531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16531">
<div class="article-summary-box-inner">
<span><p>We propose to investigate detecting and characterizing the 3D planar
articulation of objects from ordinary videos. While seemingly easy for humans,
this problem poses many challenges for computers. We propose to approach this
problem by combining a top-down detection system that finds planes that can be
articulated along with an optimization approach that solves for a 3D plane that
can explain a sequence of observed articulations. We show that this system can
be trained on a combination of videos and 3D scan datasets. When tested on a
dataset of challenging Internet videos and the Charades dataset, our approach
obtains strong performance. Project site:
https://jasonqsy.github.io/Articulation3D
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Pre-training for Person Re-identification with Noisy Labels. (arXiv:2203.16533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16533">
<div class="article-summary-box-inner">
<span><p>This paper aims to address the problem of pre-training for person
re-identification (Re-ID) with noisy labels. To setup the pre-training task, we
apply a simple online multi-object tracking system on raw videos of an existing
unlabeled Re-ID dataset "LUPerson" nd build the Noisy Labeled variant called
"LUPerson-NL". Since theses ID labels automatically derived from tracklets
inevitably contain noises, we develop a large-scale Pre-training framework
utilizing Noisy Labels (PNL), which consists of three learning modules:
supervised Re-ID learning, prototype-based contrastive learning, and
label-guided contrastive learning. In principle, joint learning of these three
modules not only clusters similar examples to one prototype, but also rectifies
noisy labels based on the prototype assignment. We demonstrate that learning
directly from raw videos is a promising alternative for pre-training, which
utilizes spatial and temporal correlations as weak supervision. This simple
pre-training task provides a scalable way to learn SOTA Re-ID representations
from scratch on "LUPerson-NL" without bells and whistles. For example, by
applying on the same supervised Re-ID method MGN, our pre-trained model
improves the mAP over the unsupervised pre-training counterpart by 5.7%, 2.2%,
2.3% on CUHK03, DukeMTMC, and MSMT17 respectively. Under the small-scale or
few-shot setting, the performance gain is even more significant, suggesting a
better transferability of the learned representation. Code is available at
https://github.com/DengpanFu/LUPerson-NL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Least Squares Normalized Cross Correlation. (arXiv:1810.04320v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1810.04320">
<div class="article-summary-box-inner">
<span><p>Direct methods are widely used for alignment of models to images, due to
their accuracy, since they minimize errors in the domain of measurement noise.
They have leveraged least squares minimizations, for simple, efficient,
variational optimization, since the seminal 1981 work of Lucas &amp; Kanade, and
normalized cross correlation (NCC), for robustness to intensity variations,
since at least 1972. Despite the complementary benefits of these two well known
methods, they have not been effectively combined to address local variations in
intensity. Many ad-hoc NCC frameworks, sub-optimal least squares methods and
image transformation approaches have thus been proposed instead, each with
their own limitations. This work shows that a least squares optimization of NCC
without approximation is not only possible, but straightforward and efficient.
A robust, locally normalized formulation is introduced to mitigate local
intensity variations and partial occlusions. Finally, sparse features with
oriented patches are proposed for further efficiency. The resulting framework
is simple to implement, computationally efficient and robust to local intensity
variations. It is evaluated on the image alignment problem, showing
improvements in both convergence rate and computation time over existing
lighting invariant methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Sensor Periocular Biometrics in a Global Pandemic: Comparative Benchmark and Novel Multialgorithmic Approach. (arXiv:1902.08123v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1902.08123">
<div class="article-summary-box-inner">
<span><p>The massive availability of cameras results in a wide variability of imaging
conditions, producing large intra-class variations and a significant
performance drop if heterogeneous images are compared for person recognition.
However, as biometrics is deployed, it is common to replace damaged or obsolete
hardware, or to exchange information between heterogeneous applications.
Variations in spectral bands can also occur. For example, surveillance face
images (typically acquired in the visible spectrum, VIS) may need to be
compared against a legacy iris database (typically acquired in near-infrared,
NIR). Here, we propose a multialgorithmic approach to cope with periocular
images from different sensors. With face masks in the front line against
COVID-19, periocular recognition is regaining popularity since it is the only
face region that remains visible. We integrate different comparators with a
fusion scheme based on linear logistic regression, in which scores are
represented by log-likelihood ratios. This allows easy interpretation of scores
and the use of Bayes thresholds for optimal decision-making since scores from
different comparators are in the same probabilistic range. We evaluate our
approach in the context of the Cross-Eyed Competition, whose aim was to compare
recognition approaches when NIR and VIS periocular images are matched. Our
approach achieves EER=0.2% and FRR of just 0.47% at FAR=0.01%, representing the
best overall approach of the competition. Experiments are also reported with a
database of VIS images from different smartphones. We also discuss the impact
of template size and computation times, with the most computationally heavy
comparator playing an important role in the results. Lastly, the proposed
method is shown to outperform other popular fusion approaches, such as the
average of scores, SVMs or Random Forest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Anticipation Tasks: Uncertainty-aware Anticipation of Sparse Surgical Instrument Usage for Context-aware Assistance. (arXiv:2007.00548v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00548">
<div class="article-summary-box-inner">
<span><p>Intra-operative anticipation of instrument usage is a necessary component for
context-aware assistance in surgery, e.g. for instrument preparation or
semi-automation of robotic tasks. However, the sparsity of instrument
occurrences in long videos poses a challenge. Current approaches are limited as
they assume knowledge on the timing of future actions or require dense temporal
segmentations during training and inference. We propose a novel learning task
for anticipation of instrument usage in laparoscopic videos that overcomes
these limitations. During training, only sparse instrument annotations are
required and inference is done solely on image data. We train a probabilistic
model to address the uncertainty associated with future events. Our approach
outperforms several baselines and is competitive to a variant using richer
annotations. We demonstrate the model's ability to quantify task-relevant
uncertainties. To the best of our knowledge, we are the first to propose a
method for anticipating instruments in surgery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Knowledge Distillation via Full Kernel Matrix Transfer. (arXiv:2009.14416v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14416">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation is an effective way for model compression in deep
learning. Given a large model (i.e., teacher model), it aims to improve the
performance of a compact model (i.e., student model) by transferring the
information from the teacher. Various information for distillation has been
studied. Recently, a number of works propose to transfer the pairwise
similarity between examples to distill relative information. However, most of
efforts are devoted to developing different similarity measurements, while only
a small matrix consisting of examples within a mini-batch is transferred at
each iteration that can be inefficient for optimizing the pairwise similarity
over the whole data set. In this work, we aim to transfer the full similarity
matrix effectively. The main challenge is from the size of the full matrix that
is quadratic to the number of examples. To address the challenge, we decompose
the original full matrix with Nystr{\"{o}}m method. By selecting appropriate
landmark points, our theoretical analysis indicates that the loss for transfer
can be further simplified. Concretely, we find that the difference between the
original full kernel matrices between teacher and student can be well bounded
by that of the corresponding partial matrices, which only consists of
similarities between original examples and landmark points. Compared with the
full matrix, the size of the partial matrix is linear in the number of
examples, which improves the efficiency of optimization significantly. The
empirical study on benchmark data sets demonstrates the effectiveness of the
proposed algorithm. Code is available at \url{https://github.com/idstcv/KDA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fooling the primate brain with minimal, targeted image manipulation. (arXiv:2011.05623v3 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.05623">
<div class="article-summary-box-inner">
<span><p>Artificial neural networks (ANNs) are considered the current best models of
biological vision. ANNs are the best predictors of neural activity in the
ventral stream; moreover, recent work has demonstrated that ANN models fitted
to neuronal activity can guide the synthesis of images that drive pre-specified
response patterns in small neuronal populations. Despite the success in
predicting and steering firing activity, these results have not been connected
with perceptual or behavioral changes. Here we propose an array of methods for
creating minimal, targeted image perturbations that lead to changes in both
neuronal activity and perception as reflected in behavior. We generated
'deceptive images' of human faces, monkey faces, and noise patterns so that
they are perceived as a different, pre-specified target category, and measured
both monkey neuronal responses and human behavior to these images. We found
several effective methods for changing primate visual categorization that
required much smaller image change compared to untargeted noise. Our work
shares the same goal with adversarial attack, namely the manipulation of images
with minimal, targeted noise that leads ANN models to misclassify the images.
Our results represent a valuable step in quantifying and characterizing the
differences in perturbation robustness of biological and artificial vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TCLR: Temporal Contrastive Learning for Video Representation. (arXiv:2101.07974v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07974">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has nearly closed the gap between supervised and
self-supervised learning of image representations, and has also been explored
for videos. However, prior work on contrastive learning for video data has not
explored the effect of explicitly encouraging the features to be distinct
across the temporal dimension. We develop a new temporal contrastive learning
framework consisting of two novel losses to improve upon existing contrastive
self-supervised video representation learning methods. The local-local temporal
contrastive loss adds the task of discriminating between non-overlapping clips
from the same video, whereas the global-local temporal contrastive aims to
discriminate between timesteps of the feature map of an input clip in order to
increase the temporal diversity of the learned features. Our proposed temporal
contrastive learning framework achieves significant improvement over the
state-of-the-art results in various downstream video understanding tasks such
as action recognition, limited-label action classification, and
nearest-neighbor video retrieval on multiple video datasets and backbones. We
also demonstrate significant improvement in fine-grained action classification
for visually similar classes. With the commonly used 3D ResNet-18 architecture
with UCF101 pretraining, we achieve 82.4\% (+5.1\% increase over the previous
best) top-1 accuracy on UCF101 and 52.9\% (+5.4\% increase) on HMDB51 action
classification, and 56.2\% (+11.7\% increase) Top-1 Recall on UCF101 nearest
neighbor video retrieval. Code released at github.com/DAVEISHAN/TCLR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10407">
<div class="article-summary-box-inner">
<span><p>The ability to quickly learn from a small quantity oftraining data widens the
range of machine learning applications. In this paper, we propose a
data-efficient image captioning model, VisualGPT, which leverages the
linguistic knowledge from a large pretrained language model(LM). A crucial
challenge is to balance between the use of visual information in the image and
prior linguistic knowledge acquired from pretraining. We designed a novel
self-resurrecting encoder-decoder attention mechanism to quickly adapt the
pretrained LM as the language decoder ona small amount of in-domain training
data. The proposed self-resurrecting activation unit produces sparse
activations but has reduced susceptibility to zero gradients. We train the
proposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual
Captions training data. Under these conditions, we outperform the best baseline
model by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual
Captions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,
a medical report generation dataset. To the best of our knowledge, this is the
first work that improves data efficiency of image captioning by utilizing LM
pretrained on unimodal data. Our code is available at:
https://github.com/Vision-CAIR/VisualGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning ABCs: Approximate Bijective Correspondence for isolating factors of variation with weak supervision. (arXiv:2103.03240v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03240">
<div class="article-summary-box-inner">
<span><p>Representational learning forms the backbone of most deep learning
applications, and the value of a learned representation is intimately tied to
its information content regarding different factors of variation. Finding good
representations depends on the nature of supervision and the learning
algorithm. We propose a novel algorithm that utilizes a weak form of
supervision where the data is partitioned into sets according to certain
inactive (common) factors of variation which are invariant across elements of
each set. Our key insight is that by seeking correspondence between elements of
different sets, we learn strong representations that exclude the inactive
factors of variation and isolate the active factors that vary within all sets.
As a consequence of focusing on the active factors, our method can leverage a
mix of set-supervised and wholly unsupervised data, which can even belong to a
different domain. We tackle the challenging problem of synthetic-to-real object
pose transfer, without pose annotations on anything, by isolating pose
information which generalizes to the category level and across the
synthetic/real domain gap. The method can also boost performance in supervised
settings, by strengthening intermediate representations, as well as operate in
practically attainable scenarios with set-supervised natural images, where
quantity is limited and nuisance factors of variation are more plentiful.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The hidden label-marginal biases of segmentation losses. (arXiv:2104.08717v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08717">
<div class="article-summary-box-inner">
<span><p>Most segmentation losses are arguably variants of the Cross-Entropy (CE) or
Dice losses. In the abundant segmentation literature, there is no clear
consensus as to which of these losses is a better choice, with varying
performances for each across different benchmarks and applications. In this
work, we develop a theoretical analysis that links these two types of losses,
exposing their advantages and weaknesses. First, we provide a
constrained-optimization perspective showing that CE and Dice share a much
deeper connection than previously thought: They both decompose into
label-marginal penalties and closely related ground-truth matching penalties.
Then, we provide bound relationships and an information-theoretic analysis,
which uncover hidden label-marginal biases: Dice has an intrinsic bias towards
specific extremely imbalanced solutions, whereas CE implicitly encourages the
ground-truth region proportions. Our theoretical results explain the wide
experimental evidence in the medical-imaging literature, whereby Dice losses
bring improvements for imbalanced segmentation. It also explains why CE
dominates natural-image problems with diverse class proportions, in which case
Dice might have difficulty adapting to different label-marginal distributions.
Based on our theoretical analysis, we propose a principled and simple solution,
which enables to control explicitly the label-marginal bias. Our loss
integrates CE with explicit ${\cal L}_1$ regularization, which encourages label
marginals to match target class proportions, thereby mitigating class imbalance
but without losing generality. Comprehensive experiments and ablation studies
over different losses and applications validate our theoretical analysis, as
well as the effectiveness of our explicit label-marginal regularizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition. (arXiv:2105.01883v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01883">
<div class="article-summary-box-inner">
<span><p>We propose RepMLP, a multi-layer-perceptron-style neural network building
block for image recognition, which is composed of a series of fully-connected
(FC) layers. Compared to convolutional layers, FC layers are more efficient,
better at modeling the long-range dependencies and positional patterns, but
worse at capturing the local structures, hence usually less favored for image
recognition. We propose a structural re-parameterization technique that adds
local prior into an FC to make it powerful for image recognition. Specifically,
we construct convolutional layers inside a RepMLP during training and merge
them into the FC for inference. On CIFAR, a simple pure-MLP model shows
performance very close to CNN. By inserting RepMLP in traditional CNN, we
improve ResNets by 1.8% accuracy on ImageNet, 2.9% for face recognition, and
2.3% mIoU on Cityscapes with lower FLOPs. Our intriguing findings highlight
that combining the global representational capacity and positional perception
of FC with the local prior of convolution can improve the performance of neural
network with faster speed on both the tasks with translation invariance (e.g.,
semantic segmentation) and those with aligned images and positional patterns
(e.g., face recognition). The code and models are available at
https://github.com/DingXiaoH/RepMLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Person Extreme Motion Prediction. (arXiv:2105.08825v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08825">
<div class="article-summary-box-inner">
<span><p>Human motion prediction aims to forecast future poses given a sequence of
past 3D skeletons. While this problem has recently received increasing
attention, it has mostly been tackled for single humans in isolation. In this
paper, we explore this problem when dealing with humans performing
collaborative tasks, we seek to predict the future motion of two interacted
persons given two sequences of their past skeletons. We propose a novel cross
interaction attention mechanism that exploits historical information of both
persons, and learns to predict cross dependencies between the two pose
sequences. Since no dataset to train such interactive situations is available,
we collected ExPI (Extreme Pose Interaction), a new lab-based person
interaction dataset of professional dancers performing Lindy-hop dancing
actions, which contains 115 sequences with 30K frames annotated with 3D body
poses and shapes. We thoroughly evaluate our cross interaction network on ExPI
and show that both in short- and long-term predictions, it consistently
outperforms state-of-the-art methods for single-person motion prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FairCal: Fairness Calibration for Face Verification. (arXiv:2106.03761v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03761">
<div class="article-summary-box-inner">
<span><p>Despite being widely used, face recognition models suffer from bias: the
probability of a false positive (incorrect face match) strongly depends on
sensitive attributes such as the ethnicity of the face. As a result, these
models can disproportionately and negatively impact minority groups,
particularly when used by law enforcement. The majority of bias reduction
methods have several drawbacks: they use an end-to-end retraining approach, may
not be feasible due to privacy issues, and often reduce accuracy. An
alternative approach is post-processing methods that build fairer decision
classifiers using the features of pre-trained models, thus avoiding the cost of
retraining. However, they still have drawbacks: they reduce accuracy (AGENDA,
PASS, FTC), or require retuning for different false positive rates (FSN). In
this work, we introduce the Fairness Calibration (FairCal) method, a
post-training approach that simultaneously: (i) increases model accuracy
(improving the state-of-the-art), (ii) produces fairly-calibrated
probabilities, (iii) significantly reduces the gap in the false positive rates,
(iv) does not require knowledge of the sensitive attribute, and (v) does not
require retraining, training an additional model, or retuning. We apply it to
the task of Face Verification, and obtain state-of-the-art results with all the
above advantages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral Unsupervised Domain Adaptation for Visual Recognition. (arXiv:2106.06112v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06112">
<div class="article-summary-box-inner">
<span><p>Though unsupervised domain adaptation (UDA) has achieved very impressive
progress recently, it remains a great challenge due to missing target
annotations and the rich discrepancy between source and target distributions.
We propose Spectral UDA (SUDA), an effective and efficient UDA technique that
works in the spectral space and can generalize across different visual
recognition tasks. SUDA addresses the UDA challenges from two perspectives.
First, it introduces a spectrum transformer (ST) that mitigates inter-domain
discrepancies by enhancing domain-invariant spectra while suppressing
domain-variant spectra of source and target samples simultaneously. Second, it
introduces multi-view spectral learning that learns useful unsupervised
representations by maximizing mutual information among multiple ST-generated
spectral views of each target sample. Extensive experiments show that SUDA
achieves superior accuracy consistently across different visual tasks in object
detection, semantic segmentation and image classification. Additionally, SUDA
also works with the transformer-based network and achieves state-of-the-art
performance on object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Randomized Smoothing with Variance Reduced Classifiers. (arXiv:2106.06946v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06946">
<div class="article-summary-box-inner">
<span><p>Randomized Smoothing (RS) is a promising method for obtaining robustness
certificates by evaluating a base model under noise. In this work, we: (i)
theoretically motivate why ensembles are a particularly suitable choice as base
models for RS, and (ii) empirically confirm this choice, obtaining
state-of-the-art results in multiple settings. The key insight of our work is
that the reduced variance of ensembles over the perturbations introduced in RS
leads to significantly more consistent classifications for a given input. This,
in turn, leads to substantially increased certifiable radii for samples close
to the decision boundary. Additionally, we introduce key optimizations which
enable an up to 55-fold decrease in sample complexity of RS for predetermined
radii, thus drastically reducing its computational overhead. Experimentally, we
show that ensembles of only 3 to 10 classifiers consistently improve on their
strongest constituting model with respect to their average certified radius
(ACR) by 5% to 21% on both CIFAR10 and ImageNet, achieving a new
state-of-the-art ACR of 0.86 and 1.11, respectively. We release all code and
models required to reproduce our results at
https://github.com/eth-sri/smoothing-ensembles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-domain Few-shot Learning with Task-specific Adapters. (arXiv:2107.00358v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00358">
<div class="article-summary-box-inner">
<span><p>In this paper, we look at the problem of cross-domain few-shot classification
that aims to learn a classifier from previously unseen classes and domains with
few labeled samples. Recent approaches broadly solve this problem by
parameterizing their few-shot classifiers with task-agnostic and task-specific
weights where the former is typically learned on a large training set and the
latter is dynamically predicted through an auxiliary network conditioned on a
small support set. In this work, we focus on the estimation of the latter, and
propose to learn task-specific weights from scratch directly on a small support
set, in contrast to dynamically estimating them. In particular, through
systematic analysis, we show that task-specific weights through parametric
adapters in matrix form with residual connections to multiple intermediate
layers of a backbone network significantly improves the performance of the
state-of-the-art models in the Meta-Dataset benchmark with minor additional
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Level Semantics Conditioned 3D Point Cloud Segmentation. (arXiv:2107.00430v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00430">
<div class="article-summary-box-inner">
<span><p>In this work, a language-level Semantics Conditioned framework for 3D Point
cloud segmentation, called SeCondPoint, is proposed, where language-level
semantics are introduced to condition the modeling of point feature
distribution as well as the pseudo-feature generation, and a
feature-geometry-based mixup approach is further proposed to facilitate the
distribution learning. To our knowledge, this is the first attempt in
literature to introduce language-level semantics to the 3D point cloud
segmentation task. Since a large number of point features could be generated
from the learned distribution thanks to the semantics conditioned modeling, any
existing segmentation network could be embedded into the proposed framework to
boost its performance. In addition, the proposed framework has the inherent
advantage of dealing with novel classes, which seems an impossible feat for the
current segmentation networks. Extensive experimental results on two public
datasets demonstrate that three typical segmentation networks could achieve
significant improvements over their original performances after enhancement by
the proposed framework in the conventional 3D segmentation task. Two benchmarks
are also introduced for a newly introduced zero-shot 3D segmentation task, and
the results also validate the proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust outlier detection by de-biasing VAE likelihoods. (arXiv:2108.08760v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08760">
<div class="article-summary-box-inner">
<span><p>Deep networks often make confident, yet, incorrect, predictions when tested
with outlier data that is far removed from their training distributions.
Likelihoods computed by deep generative models (DGMs) are a candidate metric
for outlier detection with unlabeled data. Yet, previous studies have shown
that DGM likelihoods are unreliable and can be easily biased by simple
transformations to input data. Here, we examine outlier detection with
variational autoencoders (VAEs), among the simplest of DGMs. We propose novel
analytical and algorithmic approaches to ameliorate key biases with VAE
likelihoods. Our bias corrections are sample-specific, computationally
inexpensive, and readily computed for various decoder visible distributions.
Next, we show that a well-known image pre-processing technique -- contrast
stretching -- extends the effectiveness of bias correction to further improve
outlier detection. Our approach achieves state-of-the-art accuracies with nine
grayscale and natural image datasets, and demonstrates significant advantages
-- both with speed and performance -- over four recent, competing approaches.
In summary, lightweight remedies suffice to achieve robust outlier detection
with VAEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Hard-case Mining across Pyramid Levels for Object Detection. (arXiv:2109.07217v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07217">
<div class="article-summary-box-inner">
<span><p>In object detection, multi-level prediction (e.g., FPN) and reweighting
skills (e.g., focal loss) have drastically improved one-stage detector
performance. However, the synergy between these two techniques is not fully
explored in a unified framework. We find that, during training, the one-stage
detector's optimization is not only restricted to the static hard-case mining
loss (gradient drift) but also suffered from the diverse positive samples'
proportions split by different pyramid levels (level discrepancy). Under this
concern, we propose Hierarchical Progressive Focus (HPF) consisting of two key
designs: 1) progressive focus, a more flexible hard-case mining setting
calculated adaptive to the convergence progress, 2) hierarchical sampling,
automatically generating a set of progressive focus for level-specific target
optimization. Based on focal loss with ATSS-R50, our approach achieves 40.5 AP,
surpassing the state-of-the-art QFL (Quality Focal Loss, 39.9 AP) and VFL
(Varifocal Loss, 40.1 AP). Our best model achieves 55.1 AP on COCO test-dev,
obtaining excellent results with only a typical training setting. Moreover, as
a plug-and-play scheme, HPF can cooperate well with recent advances, providing
a stable performance improvement on nine mainstream detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translating Images into Maps. (arXiv:2110.00966v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00966">
<div class="article-summary-box-inner">
<span><p>We approach instantaneous mapping, converting images to a top-down view of
the world, as a translation problem. We show how a novel form of transformer
network can be used to map from images and video directly to an overhead map or
bird's-eye-view (BEV) of the world, in a single end-to-end network. We assume a
1-1 correspondence between a vertical scanline in the image, and rays passing
through the camera location in an overhead map. This lets us formulate map
generation from an image as a set of sequence-to-sequence translations. Posing
the problem as translation allows the network to use the context of the image
when interpreting the role of each pixel. This constrained formulation, based
upon a strong physical grounding of the problem, leads to a restricted
transformer network that is convolutional in the horizontal direction only. The
structure allows us to make efficient use of data when training, and obtains
state-of-the-art results for instantaneous mapping of three large-scale
datasets, including a 15% and 30% relative gain against existing best
performing methods on the nuScenes and Argoverse datasets, respectively. We
make our code available on
https://github.com/avishkarsaha/translating-images-into-maps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06537">
<div class="article-summary-box-inner">
<span><p>The conventional wisdom behind learning deep classification models is to
focus on bad-classified examples and ignore well-classified examples that are
far from the decision boundary. For instance, when training with cross-entropy
loss, examples with higher likelihoods (i.e., well-classified examples)
contribute smaller gradients in back-propagation. However, we theoretically
show that this common practice hinders representation learning, energy
optimization, and margin growth. To counteract this deficiency, we propose to
reward well-classified examples with additive bonuses to revive their
contribution to the learning process. This counterexample theoretically
addresses these three issues. We empirically support this claim by directly
verifying the theoretical results or significant performance improvement with
our counterexample on diverse tasks, including image classification, graph
classification, and machine translation. Furthermore, this paper shows that we
can deal with complex scenarios, such as imbalanced classification, OOD
detection, and applications under adversarial attacks because our idea can
solve these three issues. Code is available at:
https://github.com/lancopku/well-classified-examples-are-underestimated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EditVAE: Unsupervised Part-Aware Controllable 3D Point Cloud Shape Generation. (arXiv:2110.06679v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06679">
<div class="article-summary-box-inner">
<span><p>This paper tackles the problem of parts-aware point cloud generation. Unlike
existing works which require the point cloud to be segmented into parts a
priori, our parts-aware editing and generation are performed in an unsupervised
manner. We achieve this with a simple modification of the Variational
Auto-Encoder which yields a joint model of the point cloud itself along with a
schematic representation of it as a combination of shape primitives. In
particular, we introduce a latent representation of the point cloud which can
be decomposed into a disentangled representation for each part of the shape.
These parts are in turn disentangled into both a shape primitive and a point
cloud representation, along with a standardising transformation to a canonical
coordinate system. The dependencies between our standardising transformations
preserve the spatial dependencies between the parts in a manner that allows
meaningful parts-aware point cloud generation and shape editing. In addition to
the flexibility afforded by our disentangled representation, the inductive bias
introduced by our joint modeling approach yields state-of-the-art experimental
results on the ShapeNet dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Projective Manifold Gradient Layer for Deep Rotation Regression. (arXiv:2110.11657v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11657">
<div class="article-summary-box-inner">
<span><p>Regressing rotations on SO(3) manifold using deep neural networks is an
important yet unsolved problem. The gap between the Euclidean network output
space and the non-Euclidean SO(3) manifold imposes a severe challenge for
neural network learning in both forward and backward passes. While several
works have proposed different regression-friendly rotation representations,
very few works have been devoted to improving the gradient backpropagating in
the backward pass. In this paper, we propose a manifold-aware gradient that
directly backpropagates into deep network weights. Leveraging Riemannian
optimization to construct a novel projective gradient, our proposed regularized
projective manifold gradient (RPMG) method helps networks achieve new
state-of-the-art performance in a variety of rotation estimation tasks. Our
proposed gradient layer can also be applied to other smooth manifolds such as
the unit sphere. Our project page is at https://jychen18.github.io/RPMG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advances in Neural Rendering. (arXiv:2111.05849v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05849">
<div class="article-summary-box-inner">
<span><p>Synthesizing photo-realistic images and videos is at the heart of computer
graphics and has been the focus of decades of research. Traditionally,
synthetic images of a scene are generated using rendering algorithms such as
rasterization or ray tracing, which take specifically defined representations
of geometry and material properties as input. Collectively, these inputs define
the actual scene and what is rendered, and are referred to as the scene
representation (where a scene consists of one or more objects). Example scene
representations are triangle meshes with accompanied textures (e.g., created by
an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g.,
from a CT scan), or implicit surface functions (e.g., truncated signed distance
fields). The reconstruction of such a scene representation from observations
using differentiable rendering losses is known as inverse graphics or inverse
rendering. Neural rendering is closely related, and combines ideas from
classical computer graphics and machine learning to create algorithms for
synthesizing images from real-world observations. Neural rendering is a leap
forward towards the goal of synthesizing photo-realistic image and video
content. In recent years, we have seen immense progress in this field through
hundreds of publications that show different ways to inject learnable
components into the rendering pipeline. This state-of-the-art report on
advances in neural rendering focuses on methods that combine classical
rendering principles with learned 3D scene representations, often now referred
to as neural scene representations. A key advantage of these methods is that
they are 3D-consistent by design, enabling applications such as novel viewpoint
synthesis of a captured scene. In addition to methods that handle static
scenes, we cover neural scene representations for modeling non-rigidly
deforming objects...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling equivariance for arbitrary Lie groups. (arXiv:2111.08251v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08251">
<div class="article-summary-box-inner">
<span><p>Although provably robust to translational perturbations, convolutional neural
networks (CNNs) are known to suffer from extreme performance degradation when
presented at test time with more general geometric transformations of inputs.
Recently, this limitation has motivated a shift in focus from CNNs to Capsule
Networks (CapsNets). However, CapsNets suffer from admitting relatively few
theoretical guarantees of invariance. We introduce a rigourous mathematical
framework to permit invariance to any Lie group of warps, exclusively using
convolutions (over Lie groups), without the need for capsules. Previous work on
group convolutions has been hampered by strong assumptions about the group,
which precludes the application of such techniques to common warps in computer
vision such as affine and homographic. Our framework enables the implementation
of group convolutions over any finite-dimensional Lie group. We empirically
validate our approach on the benchmark affine-invariant classification task,
where we achieve 30% improvement in accuracy against conventional CNNs while
outperforming most CapsNets. As further illustration of the generality of our
framework, we train a homography-convolutional model which achieves superior
robustness on a homography-perturbed dataset, where CapsNet results degrade.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HARA: A Hierarchical Approach for Robust Rotation Averaging. (arXiv:2111.08831v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08831">
<div class="article-summary-box-inner">
<span><p>We propose a novel hierarchical approach for multiple rotation averaging,
dubbed HARA. Our method incrementally initializes the rotation graph based on a
hierarchy of triplet support. The key idea is to build a spanning tree by
prioritizing the edges with many strong triplet supports and gradually adding
those with weaker and fewer supports. This reduces the risk of adding outliers
in the spanning tree. As a result, we obtain a robust initial solution that
enables us to filter outliers prior to nonlinear optimization. With minimal
modification, our approach can also integrate the knowledge of the number of
valid 2D-2D correspondences. We perform extensive evaluations on both synthetic
and real datasets, demonstrating state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cerberus Transformer: Joint Semantic, Affordance and Attribute Parsing. (arXiv:2111.12608v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12608">
<div class="article-summary-box-inner">
<span><p>Multi-task indoor scene understanding is widely considered as an intriguing
formulation, as the affinity of different tasks may lead to improved
performance. In this paper, we tackle the new problem of joint semantic,
affordance and attribute parsing. However, successfully resolving it requires a
model to capture long-range dependency, learn from weakly aligned data and
properly balance sub-tasks during training. To this end, we propose an
attention-based architecture named Cerberus and a tailored training framework.
Our method effectively addresses the aforementioned challenges and achieves
state-of-the-art performance on all three tasks. Moreover, an in-depth analysis
shows concept affinity consistent with human cognition, which inspires us to
explore the possibility of weakly supervised learning. Surprisingly, Cerberus
achieves strong results using only 0.1%-1% annotation. Visualizations further
confirm that this success is credited to common attention maps across tasks.
Code and models can be accessed at https://github.com/OPEN-AIR-SUN/Cerberus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Amortized Prompt: Guide CLIP to Domain Transfer Learning. (arXiv:2111.12853v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12853">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) is a problematic Domain Transfer Learning problem
aiming to learn a generalizable model to unseen domains. Recent massive
pre-trained models such as CLIP and GPT-3, i.e. foundation models (FMs), are
robust to many distribution shifts and therefore should lead to substantial
improvements in DG. In this work, we study generic ways to adopt CLIP for DG
problems in image classification. We evaluate Test-Time Adaptation (TTA) and
full DG learning settings on several standard benchmarks. We propose AP
(Amortized Prompt) as a novel prompt strategy for domain inference in the form
of prompt generation. Moreover, we show that combining domain prompt inference
with CLIP enables the model to outperform strong DG baselines and other prompt
strategies. Since AP generate prompts to automatically adapt to the target
domain, it can be seen as a TTA method. Therefore, we also conduct a fair
comparison with SOTA TTA methods. The results demonstrate AP can outperform all
baselines with a significant margin. Then, we further analyze the properties of
AP with insightful ablation experiments. We hope the simplicity and success of
our approach emphasize the importance of and lead to broader adoption and
analysis of foundation models in the field of TTA and DG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Multiple Dense Prediction Tasks from Partially Annotated Data. (arXiv:2111.14893v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14893">
<div class="article-summary-box-inner">
<span><p>Despite the recent advances in multi-task learning of dense prediction
problems, most methods rely on expensive labelled datasets. In this paper, we
present a label efficient approach and look at jointly learning of multiple
dense prediction tasks on partially annotated data (i.e. not all the task
labels are available for each image), which we call multi-task
partially-supervised learning. We propose a multi-task training procedure that
successfully leverages task relations to supervise its multi-task learning when
data is partially annotated. In particular, we learn to map each task pair to a
joint pairwise task-space which enables sharing information between them in a
computationally efficient way through another network conditioned on task
pairs, and avoids learning trivial cross-task relations by retaining high-level
information about the input image. We rigorously demonstrate that our proposed
method effectively exploits the images with unlabelled tasks and outperforms
existing semi-supervised learning approaches and related methods on three
standard benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ISNAS-DIP: Image-Specific Neural Architecture Search for Deep Image Prior. (arXiv:2111.15362v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15362">
<div class="article-summary-box-inner">
<span><p>Recent works show that convolutional neural network (CNN) architectures have
a spectral bias towards lower frequencies, which has been leveraged for various
image restoration tasks in the Deep Image Prior (DIP) framework. The benefit of
the inductive bias the network imposes in the DIP framework depends on the
architecture. Therefore, researchers have studied how to automate the search to
determine the best-performing model. However, common neural architecture search
(NAS) techniques are resource and time-intensive. Moreover, best-performing
models are determined for a whole dataset of images instead of for each image
independently, which would be prohibitively expensive. In this work, we first
show that optimal neural architectures in the DIP framework are
image-dependent. Leveraging this insight, we then propose an image-specific NAS
strategy for the DIP framework that requires substantially less training than
typical NAS approaches, effectively enabling image-specific NAS. We justify the
proposed strategy's effectiveness by (1) demonstrating its performance on a NAS
Dataset for DIP that includes 522 models from a particular search space (2)
conducting extensive experiments on image denoising, inpainting, and
super-resolution tasks. Our experiments show that image-specific metrics can
reduce the search space to a small cohort of models, of which the best model
outperforms current NAS approaches for image restoration. Codes and datasets
are available at https://github.com/ozgurkara99/ISNAS-DIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15430">
<div class="article-summary-box-inner">
<span><p>In spite of the dominant performances of deep neural networks, recent works
have shown that they are poorly calibrated, resulting in over-confident
predictions. Miscalibration can be exacerbated by overfitting due to the
minimization of the cross-entropy during training, as it promotes the predicted
softmax probabilities to match the one-hot label assignments. This yields a
pre-softmax activation of the correct class that is significantly larger than
the remaining activations. Recent evidence from the literature suggests that
loss functions that embed implicit or explicit maximization of the entropy of
predictions yield state-of-the-art calibration performances. We provide a
unifying constrained-optimization perspective of current state-of-the-art
calibration losses. Specifically, these losses could be viewed as
approximations of a linear penalty (or a Lagrangian) imposing equality
constraints on logit distances. This points to an important limitation of such
underlying equality constraints, whose ensuing gradients constantly push
towards a non-informative solution, which might prevent from reaching the best
compromise between the discriminative performance and calibration of the model
during gradient-based optimization. Following our observations, we propose a
simple and flexible generalization based on inequality constraints, which
imposes a controllable margin on logit distances. Comprehensive experiments on
a variety of image classification, semantic segmentation and NLP benchmarks
demonstrate that our method sets novel state-of-the-art results on these tasks
in terms of network calibration, without affecting the discriminative
performance. The code is available at https://github.com/by-liu/MbLS .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ST-MFNet: A Spatio-Temporal Multi-Flow Network for Frame Interpolation. (arXiv:2111.15483v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15483">
<div class="article-summary-box-inner">
<span><p>Video frame interpolation (VFI) is currently a very active research topic,
with applications spanning computer vision, post production and video encoding.
VFI can be extremely challenging, particularly in sequences containing large
motions, occlusions or dynamic textures, where existing approaches fail to
offer perceptually robust interpolation performance. In this context, we
present a novel deep learning based VFI method, ST-MFNet, based on a
Spatio-Temporal Multi-Flow architecture. ST-MFNet employs a new multi-scale
multi-flow predictor to estimate many-to-one intermediate flows, which are
combined with conventional one-to-one optical flows to capture both large and
complex motions. In order to enhance interpolation performance for various
textures, a 3D CNN is also employed to model the content dynamics over an
extended temporal window. Moreover, ST-MFNet has been trained within an ST-GAN
framework, which was originally developed for texture synthesis, with the aim
of further improving perceptual interpolation quality. Our approach has been
comprehensively evaluated -- compared with fourteen state-of-the-art VFI
algorithms -- clearly demonstrating that ST-MFNet consistently outperforms
these benchmarks on varied and representative test datasets, with significant
gains up to 1.09dB in PSNR for cases including large motions and dynamic
textures. Project page: https://danielism97.github.io/ST-MFNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Emotion Director: Speech-preserving semantic control of facial expressions in "in-the-wild" videos. (arXiv:2112.00585v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00585">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel deep learning method for photo-realistic
manipulation of the emotional state of actors in "in-the-wild" videos. The
proposed method is based on a parametric 3D face representation of the actor in
the input scene that offers a reliable disentanglement of the facial identity
from the head pose and facial expressions. It then uses a novel deep domain
translation framework that alters the facial expressions in a consistent and
plausible manner, taking into account their dynamics. Finally, the altered
facial expressions are used to photo-realistically manipulate the facial region
in the input scene based on an especially-designed neural face renderer. To the
best of our knowledge, our method is the first to be capable of controlling the
actor's facial expressions by even using as a sole input the semantic labels of
the manipulated emotions, while at the same time preserving the speech-related
lip movements. We conduct extensive qualitative and quantitative evaluations
and comparisons, which demonstrate the effectiveness of our approach and the
especially promising results that we obtain. Our method opens a plethora of new
possibilities for useful applications of neural rendering technologies, ranging
from movie post-production and video games to photo-realistic affective
avatars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras. (arXiv:2112.01524v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01524">
<div class="article-summary-box-inner">
<span><p>We present an approach for 3D global human mesh recovery from monocular
videos recorded with dynamic cameras. Our approach is robust to severe and
long-term occlusions and tracks human bodies even when they go outside the
camera's field of view. To achieve this, we first propose a deep generative
motion infiller, which autoregressively infills the body motions of occluded
humans based on visible motions. Additionally, in contrast to prior work, our
approach reconstructs human meshes in consistent global coordinates even with
dynamic cameras. Since the joint reconstruction of human motions and camera
poses is underconstrained, we propose a global trajectory predictor that
generates global human trajectories based on local body movements. Using the
predicted trajectories as anchors, we present a global optimization framework
that refines the predicted trajectories and optimizes the camera poses to match
the video evidence such as 2D keypoints. Experiments on challenging indoor and
in-the-wild datasets with dynamic cameras demonstrate that the proposed
approach outperforms prior methods significantly in terms of motion infilling
and global mesh recovery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MViTv2: Improved Multiscale Vision Transformers for Classification and Detection. (arXiv:2112.01526v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01526">
<div class="article-summary-box-inner">
<span><p>In this paper, we study Multiscale Vision Transformers (MViTv2) as a unified
architecture for image and video classification, as well as object detection.
We present an improved version of MViT that incorporates decomposed relative
positional embeddings and residual pooling connections. We instantiate this
architecture in five sizes and evaluate it for ImageNet classification, COCO
detection and Kinetics video recognition where it outperforms prior work. We
further compare MViTv2s' pooling attention to window attention mechanisms where
it outperforms the latter in accuracy/compute. Without bells-and-whistles,
MViTv2 has state-of-the-art performance in 3 domains: 88.8% accuracy on
ImageNet classification, 58.7 boxAP on COCO object detection as well as 86.1%
on Kinetics-400 video classification. Code and models are available at
https://github.com/facebookresearch/mvit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forward Compatible Training for Large-Scale Embedding Retrieval Systems. (arXiv:2112.02805v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02805">
<div class="article-summary-box-inner">
<span><p>In visual retrieval systems, updating the embedding model requires
recomputing features for every piece of data. This expensive process is
referred to as backfilling. Recently, the idea of backward compatible training
(BCT) was proposed. To avoid the cost of backfilling, BCT modifies training of
the new model to make its representations compatible with those of the old
model. However, BCT can significantly hinder the performance of the new model.
In this work, we propose a new learning paradigm for representation learning:
forward compatible training (FCT). In FCT, when the old model is trained, we
also prepare for a future unknown version of the model. We propose learning
side-information, an auxiliary feature for each sample which facilitates future
updates of the model. To develop a powerful and flexible framework for model
compatibility, we combine side-information with a forward transformation from
old to new embeddings. Training of the new model is not modified, hence, its
accuracy is not degraded. We demonstrate significant retrieval accuracy
improvement compared to BCT for various datasets: ImageNet-1k (+18.1%),
Places-365 (+5.4%), and VGG-Face2 (+8.3%). FCT obtains model compatibility when
the new and old models are trained across different datasets, losses, and
architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PTTR: Relational 3D Point Cloud Object Tracking with Transformer. (arXiv:2112.02857v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02857">
<div class="article-summary-box-inner">
<span><p>In a point cloud sequence, 3D object tracking aims to predict the location
and orientation of an object in the current search point cloud given a template
point cloud. Motivated by the success of transformers, we propose Point
Tracking TRansformer (PTTR), which efficiently predicts high-quality 3D
tracking results in a coarse-to-fine manner with the help of transformer
operations. PTTR consists of three novel designs. 1) Instead of random
sampling, we design Relation-Aware Sampling to preserve relevant points to
given templates during subsampling. 2) Furthermore, we propose a Point Relation
Transformer (PRT) consisting of a self-attention and a cross-attention module.
The global self-attention operation captures long-range dependencies to enhance
encoded point features for the search area and the template, respectively.
Subsequently, we generate the coarse tracking results by matching the two sets
of point features via cross-attention. 3) Based on the coarse tracking results,
we employ a novel Prediction Refinement Module to obtain the final refined
prediction. In addition, we create a large-scale point cloud single object
tracking benchmark based on the Waymo Open Dataset. Extensive experiments show
that PTTR achieves superior point cloud tracking in both accuracy and
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation. (arXiv:2112.04177v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04177">
<div class="article-summary-box-inner">
<span><p>For online video instance segmentation (VIS), fully utilizing the information
from previous frames in an efficient manner is essential for real-time
applications. Most previous methods follow a two-stage approach requiring
additional computations such as RPN and RoIAlign, and do not fully exploit the
available information in the video for all subtasks in VIS. In this paper, we
propose a novel single-stage framework for online VIS built based on the grid
structured feature representation. The grid-based features allow us to employ
fully convolutional networks for real-time processing, and also to easily reuse
and share features within different components. We also introduce cooperatively
operating modules that aggregate information from available frames, in order to
enrich the features for all subtasks in VIS. Our design fully takes advantage
of previous information in a grid form for all tasks in VIS in an efficient
way, and we achieved the new state-of-the-art accuracy (38.6 AP and 36.9 AP)
and speed (40.0 FPS) on YouTube-VIS 2019 and 2021 datasets among online VIS
methods. The code is available at https://github.com/SuHoHan95/VISOLO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering. (arXiv:2112.04312v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04312">
<div class="article-summary-box-inner">
<span><p>In this work we develop a generalizable and efficient Neural Radiance Field
(NeRF) pipeline for high-fidelity free-viewpoint human body synthesis under
settings with sparse camera views. Though existing NeRF-based methods can
synthesize rather realistic details for human body, they tend to produce poor
results when the input has self-occlusion, especially for unseen humans under
sparse views. Moreover, these methods often require a large number of sampling
points for rendering, which leads to low efficiency and limits their real-world
applicability. To address these challenges, we propose a Geometry-guided
Progressive NeRF (GP-NeRF). In particular, to better tackle self-occlusion, we
devise a geometry-guided multi-view feature integration approach that utilizes
the estimated geometry prior to integrate the incomplete information from input
views and construct a complete geometry volume for the target human body.
Meanwhile, for achieving higher rendering efficiency, we introduce a
progressive rendering pipeline through geometry guidance, which leverages the
geometric feature volume and the predicted density values to progressively
reduce the number of sampling points and speed up the rendering process.
Experiments on the ZJU-MoCap and THUman datasets show that our method
outperforms the state-of-the-arts significantly across multiple generalization
settings, while the time cost is reduced &gt; 70% via applying our efficient
progressive rendering pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLAVA: A Foundational Language And Vision Alignment Model. (arXiv:2112.04482v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04482">
<div class="article-summary-box-inner">
<span><p>State-of-the-art vision and vision-and-language models rely on large-scale
visio-linguistic pretraining for obtaining good performance on a variety of
downstream tasks. Generally, such models are often either cross-modal
(contrastive) or multi-modal (with earlier fusion) but not both; and they often
only target specific modalities or tasks. A promising direction would be to use
a single holistic universal model, as a "foundation", that targets all
modalities at once -- a true vision and language foundation model should be
good at vision tasks, language tasks, and cross- and multi-modal vision and
language tasks. We introduce FLAVA as such a model and demonstrate impressive
performance on a wide range of 35 tasks spanning these target modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIST: Unpaired Neural Implicit Shape Translation Network. (arXiv:2112.05381v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05381">
<div class="article-summary-box-inner">
<span><p>We introduce UNIST, the first deep neural implicit model for general-purpose,
unpaired shape-to-shape translation, in both 2D and 3D domains. Our model is
built on autoencoding implicit fields, rather than point clouds which
represents the state of the art. Furthermore, our translation network is
trained to perform the task over a latent grid representation which combines
the merits of both latent-space processing and position awareness, to not only
enable drastic shape transforms but also well preserve spatial features and
fine local details for natural shape translations. With the same network
architecture and only dictated by the input domain pairs, our model can learn
both style-preserving content alteration and content-preserving style transfer.
We demonstrate the generality and quality of the translation results, and
compare them to well-known baselines. Code is available at
https://qiminchen.github.io/unist/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PartGlot: Learning Shape Part Segmentation from Language Reference Games. (arXiv:2112.06390v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06390">
<div class="article-summary-box-inner">
<span><p>We introduce PartGlot, a neural framework and associated architectures for
learning semantic part segmentation of 3D shape geometry, based solely on part
referential language. We exploit the fact that linguistic descriptions of a
shape can provide priors on the shape's parts -- as natural language has
evolved to reflect human perception of the compositional structure of objects,
essential to their recognition and use. For training, we use the paired
geometry / language data collected in the ShapeGlot work for their reference
game, where a speaker creates an utterance to differentiate a target shape from
two distractors and the listener has to find the target based on this
utterance. Our network is designed to solve this target discrimination problem,
carefully incorporating a Transformer-based attention module so that the output
attention can precisely highlight the semantic part or parts described in the
language. Furthermore, the network operates without any direct supervision on
the 3D geometry itself. Surprisingly, we further demonstrate that the learned
part information is generalizable to shape classes unseen during training. Our
approach opens the possibility of learning 3D shape parts from language alone,
without the need for large-scale part geometry annotations, thus facilitating
annotation acquisition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I M Avatar: Implicit Morphable Head Avatars from Videos. (arXiv:2112.07471v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07471">
<div class="article-summary-box-inner">
<span><p>Traditional 3D morphable face models (3DMMs) provide fine-grained control
over expression but cannot easily capture geometric and appearance details.
Neural volumetric representations approach photorealism but are hard to animate
and do not generalize well to unseen expressions. To tackle this problem, we
propose IMavatar (Implicit Morphable avatar), a novel method for learning
implicit head avatars from monocular videos. Inspired by the fine-grained
control mechanisms afforded by conventional 3DMMs, we represent the expression-
and pose- related deformations via learned blendshapes and skinning fields.
These attributes are pose-independent and can be used to morph the canonical
geometry and texture fields given novel expression and pose parameters. We
employ ray marching and iterative root-finding to locate the canonical surface
intersection for each pixel. A key contribution is our novel analytical
gradient formulation that enables end-to-end training of IMavatars from videos.
We show quantitatively and qualitatively that our method improves geometry and
covers a more complete expression space compared to state-of-the-art methods.
Code, video, and data can be found at
https://ait.ethz.ch/projects/2022/IMavatar/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A-ViT: Adaptive Tokens for Efficient Vision Transformer. (arXiv:2112.07658v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07658">
<div class="article-summary-box-inner">
<span><p>We introduce A-ViT, a method that adaptively adjusts the inference cost of
vision transformer (ViT) for images of different complexity. A-ViT achieves
this by automatically reducing the number of tokens in vision transformers that
are processed in the network as inference proceeds. We reformulate Adaptive
Computation Time (ACT) for this task, extending halting to discard redundant
spatial tokens. The appealing architectural properties of vision transformers
enables our adaptive token reduction mechanism to speed up inference without
modifying the network architecture or inference hardware. We demonstrate that
A-ViT requires no extra parameters or sub-network for halting, as we base the
learning of adaptive halting on the original network parameters. We further
introduce distributional prior regularization that stabilizes training compared
to prior ACT approaches. On the image classification task (ImageNet1K), we show
that our proposed A-ViT yields high efficacy in filtering informative spatial
features and cutting down on the overall compute. The proposed method improves
the throughput of DeiT-Tiny by 62% and DeiT-Small by 38% with only 0.3%
accuracy drop, outperforming prior art by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09081">
<div class="article-summary-box-inner">
<span><p>We present a visual localization system that learns to estimate camera poses
in the real world with the help of synthetic data. Despite significant progress
in recent years, most learning-based approaches to visual localization target
at a single domain and require a dense database of geo-tagged images to
function well. To mitigate the data scarcity issue and improve the scalability
of the neural localization models, we introduce TOPO-DataGen, a versatile
synthetic data generation tool that traverses smoothly between the real and
virtual world, hinged on the geographic camera viewpoint. New large-scale
sim-to-real benchmark datasets are proposed to showcase and evaluate the
utility of the said synthetic data. Our experiments reveal that synthetic data
generically enhances the neural network performance on real data. Furthermore,
we introduce CrossLoc, a cross-modal visual representation learning approach to
pose estimation that makes full use of the scene coordinate ground truth via
self-supervision. Without any extra data, CrossLoc significantly outperforms
the state-of-the-art methods and achieves substantially higher real-data sample
efficiency. Our code and datasets are all available at
https://crossloc.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Semantic Segmentation via Alternative Self-Dual Teaching. (arXiv:2112.09459v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09459">
<div class="article-summary-box-inner">
<span><p>Current weakly supervised semantic segmentation (WSSS) frameworks usually
contain the separated mask-refinement model and the main semantic region mining
model. These approaches would contain redundant feature extraction backbones
and biased learning objectives, making them computational complex yet
sub-optimal to addressing the WSSS task. To solve this problem, this paper
establishes a compact learning framework that embeds the classification and
mask-refinement components into a unified deep model. With the shared feature
extraction backbone, our model is able to facilitate knowledge sharing between
the two components while preserving a low computational complexity. To
encourage high-quality knowledge interaction, we propose a novel alternative
self-dual teaching (ASDT) mechanism. Unlike the conventional distillation
strategy, the knowledge of the two teacher branches in our model is
alternatively distilled to the student branch by a Pulse Width Modulation
(PWM), which generates PW wave-like selection signal to guide the knowledge
distillation process. In this way, the student branch can help prevent the
model from falling into local minimum solutions caused by the imperfect
knowledge provided of either teacher branch. Comprehensive experiments on the
PASCAL VOC 2012 and COCO-Stuff 10K demonstrate the effectiveness of the
proposed alternative self-dual teaching mechanism as well as the new
state-of-the-art performance of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Segmentation Using Text and Image Prompts. (arXiv:2112.10003v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10003">
<div class="article-summary-box-inner">
<span><p>Image segmentation is usually addressed by training a model for a fixed set
of object classes. Incorporating additional classes or more complex queries
later is expensive as it requires re-training the model on a dataset that
encompasses these expressions. Here we propose a system that can generate image
segmentations based on arbitrary prompts at test time. A prompt can be either a
text or an image. This approach enables us to create a unified model (trained
once) for three common segmentation tasks, which come with distinct challenges:
referring expression segmentation, zero-shot segmentation and one-shot
segmentation. We build upon the CLIP model as a backbone which we extend with a
transformer-based decoder that enables dense prediction. After training on an
extended version of the PhraseCut dataset, our system generates a binary
segmentation map for an image based on a free-text prompt or on an additional
image expressing the query. We analyze different variants of the latter
image-based prompts in detail. This novel hybrid input allows for dynamic
adaptation not only to the three segmentation tasks mentioned above, but to any
binary segmentation task where a text or image query can be formulated.
Finally, we find our system to adapt well to generalized queries involving
affordances or properties. Code is available at
https://eckerlab.org/code/clipseg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topology Preserving Local Road Network Estimation from Single Onboard Camera Image. (arXiv:2112.10155v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10155">
<div class="article-summary-box-inner">
<span><p>Knowledge of the road network topology is crucial for autonomous planning and
navigation. Yet, recovering such topology from a single image has only been
explored in part. Furthermore, it needs to refer to the ground plane, where
also the driving actions are taken. This paper aims at extracting the local
road network topology, directly in the bird's-eye-view (BEV), all in a complex
urban setting. The only input consists of a single onboard, forward looking
camera image. We represent the road topology using a set of directed lane
curves and their interactions, which are captured using their intersection
points. To better capture topology, we introduce the concept of \emph{minimal
cycles} and their covers. A minimal cycle is the smallest cycle formed by the
directed curve segments (between two intersections). The cover is a set of
curves whose segments are involved in forming a minimal cycle. We first show
that the covers suffice to uniquely represent the road topology. The covers are
then used to supervise deep neural networks, along with the lane curve
supervision. These learn to predict the road topology from a single input
image. The results on the NuScenes and Argoverse benchmarks are significantly
better than those obtained with baselines. Code:
https://github.com/ybarancan/TopologicalLaneGraph
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality. (arXiv:2112.11081v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11081">
<div class="article-summary-box-inner">
<span><p>Compared to convolutional layers, fully-connected (FC) layers are better at
modeling the long-range dependencies but worse at capturing the local patterns,
hence usually less favored for image recognition. In this paper, we propose a
methodology, Locality Injection, to incorporate local priors into an FC layer
via merging the trained parameters of a parallel conv kernel into the FC
kernel. Locality Injection can be viewed as a novel Structural
Re-parameterization method since it equivalently converts the structures via
transforming the parameters. Based on that, we propose a multi-layer-perceptron
(MLP) block named RepMLP Block, which uses three FC layers to extract features,
and a novel architecture named RepMLPNet. The hierarchical design distinguishes
RepMLPNet from the other concurrently proposed vision MLPs. As it produces
feature maps of different levels, it qualifies as a backbone model for
downstream tasks like semantic segmentation. Our results reveal that 1)
Locality Injection is a general methodology for MLP models; 2) RepMLPNet has
favorable accuracy-efficiency trade-off compared to the other MLPs; 3)
RepMLPNet is the first MLP that seamlessly transfer to Cityscapes semantic
segmentation. The code and models are available at
https://github.com/DingXiaoH/RepMLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. (arXiv:2112.11427v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11427">
<div class="article-summary-box-inner">
<span><p>We introduce a high resolution, 3D-consistent image and shape generation
technique which we call StyleSDF. Our method is trained on single-view RGB data
only, and stands on the shoulders of StyleGAN2 for image generation, while
solving two main challenges in 3D-aware GANs: 1) high-resolution,
view-consistent generation of the RGB images, and 2) detailed 3D shape. We
achieve this by merging a SDF-based 3D representation with a style-based 2D
generator. Our 3D implicit network renders low-resolution feature maps, from
which the style-based network generates view-consistent, 1024x1024 images.
Notably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to
consistent volume rendering. Our method shows higher quality results compared
to state of the art in terms of visual and geometric quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sketch-based Facial Synthesis: A New Challenge. (arXiv:2112.15439v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15439">
<div class="article-summary-box-inner">
<span><p>This paper aims to conduct a comprehensive study on the facial sketch
synthesis (FSS) problem. However, due to the high costs in obtaining hand-drawn
sketch datasets, there lacks a complete benchmark for assessing the development
of FSS algorithms over the last decade. As such, we first introduce a
high-quality dataset for FSS, named FS2K, which consists of 2,104 image-sketch
pairs spanning three types of sketch styles, image backgrounds, lighting
conditions, skin colors, and facial attributes. FS2K differs from previous FSS
datasets in difficulty, diversity, and scalability and should thus facilitate
the progress of FSS research. Second, we present the largest-scale FSS study by
reviewing 139 classical methods, including 24 handcrafted feature-based facial
sketch synthesis approaches, 37 general neural-style transfer methods, 43 deep
image-to-image translation methods, and 35 image-to-sketch approaches. Besides,
we elaborate comprehensive experiments on the existing 19 cutting-edge models.
Third, we present a simple baseline for FSS, named FSGAN. With only two
straightforward components, i.e., facial-aware masking and style-vector
expansion, FSGAN surpasses the performance of all previous state-of-the-art
models on the proposed FS2K dataset by a large margin. Finally, we conclude
with lessons learned over the past years and point out several unsolved
challenges. Our open-source code is available at
https://github.com/DengPingFan/FSGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POCO: Point Convolution for Surface Reconstruction. (arXiv:2201.01831v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01831">
<div class="article-summary-box-inner">
<span><p>Implicit neural networks have been successfully used for surface
reconstruction from point clouds. However, many of them face scalability issues
as they encode the isosurface function of a whole object or scene into a single
latent vector. To overcome this limitation, a few approaches infer latent
vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to
answer occupancy queries. In doing so, they loose the direct connection with
the input points sampled on the surface of objects, and they attach information
uniformly in space rather than where it matters the most, i.e., near the
surface. Besides, relying on fixed patch sizes may require discretization
tuning. To address these issues, we propose to use point cloud convolutions and
compute latent vectors at each input point. We then perform a learning-based
interpolation on nearest neighbors using inferred weights. Experiments on both
object and scene datasets show that our approach significantly outperforms
other methods on most classical metrics, producing finer details and better
reconstructing thinner volumes. The code is available at
https://github.com/valeoai/POCO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04584">
<div class="article-summary-box-inner">
<span><p>Automatic segmentation of lung lesions associated with COVID-19 in CT images
requires large amount of annotated volumes. Annotations mandate expert
knowledge and are time-intensive to obtain through fully manual segmentation
methods. Additionally, lung lesions have large inter-patient variations, with
some pathologies having similar visual appearance as healthy lung tissues. This
poses a challenge when applying existing semi-automatic interactive
segmentation techniques for data labelling. To address these challenges, we
propose an efficient convolutional neural networks (CNNs) that can be learned
online while the annotator provides scribble-based interaction. To accelerate
learning from only the samples labelled through user-interactions, a
patch-based approach is used for training the network. Moreover, we use
weighted cross-entropy loss to address the class imbalance that may result from
user-interactions. During online inference, the learned network is applied to
the whole input volume using a fully convolutional approach. We compare our
proposed method with state-of-the-art using synthetic scribbles and show that
it outperforms existing methods on the task of annotating lung lesions
associated with COVID-19, achieving 16% higher Dice score while reducing
execution time by 3$\times$ and requiring 9000 lesser scribbles-based labelled
voxels. Due to the online learning aspect, our approach adapts quickly to user
input, resulting in high quality segmentation labels. Source code for ECONet is
available at: https://github.com/masadcv/ECONet-MONAILabel.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonlinear Unknown Input Observability and Unknown Input Reconstruction: The General Analytical Solution. (arXiv:2201.07610v2 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07610">
<div class="article-summary-box-inner">
<span><p>Observability is a fundamental structural property of any dynamic system and
describes the possibility of reconstructing the state that characterizes the
system from observing its inputs and outputs. Despite the huge effort made to
study this property and to introduce analytical criteria able to check whether
a dynamic system satisfies this property or not, there is no general analytical
criterion to automatically check the state observability when the dynamics are
also driven by unknown inputs. Here, we introduce the general analytical
solution of this fundamental problem, often called the unknown input
observability problem. This paper provides the general analytical solution of
this problem, namely, it provides the systematic procedure, based on automatic
computation (differentiation and matrix rank determination), that allows us to
automatically check the state observability even in the presence of unknown
inputs. A first solution of this problem was presented in the second part of
the book: "Observability: A New Theory Based on the Group of Invariance" [45].
The solution presented by this paper completes the previous solution in [45].
In particular, the new solution exhaustively accounts for the systems that do
not belong to the category of the systems that are canonic with respect to
their unknown inputs. The new solution is also provided in the form of a new
algorithm. A further novelty with respect to the algorithm provided in [45]
consists of a new convergence criterion that holds in all the cases (the
convergence criterion of the algorithm provided in [45] can fail in some
cases). Finally, we also provide the answer to the problem of unknown input
reconstruction which is intimately related to the problem of state
observability. We illustrate the implementation of the new algorithm by
studying a nonlinear system in the framework of visual-inertial sensor fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Importance of Textlines in Historical Document Classification. (arXiv:2201.09575v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09575">
<div class="article-summary-box-inner">
<span><p>This paper describes a system prepared at Brno University of Technology for
ICDAR 2021 Competition on Historical Document Classification, experiments
leading to its design, and the main findings. The solved tasks include script
and font classification, document origin localization, and dating. We combined
patch-level and line-level approaches, where the line-level system utilizes an
existing, publicly available page layout analysis engine. In both systems,
neural networks provide local predictions which are combined into page-level
decisions, and the results of both systems are fused using linear or log-linear
interpolation. We propose loss functions suitable for weakly supervised
classification problem where multiple possible labels are provided, and we
propose loss functions suitable for interval regression in the dating task. The
line-level system significantly improves results in script and font
classification and in the dating task. The full system achieved 98.48 %, 88.84
%, and 79.69 % accuracy in the font, script, and location classification tasks
respectively. In the dating task, our system achieved a mean absolute error of
21.91 years. Our system achieved the best results in all tasks and became the
overall winner of the competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-based Video Reconstruction via Potential-assisted Spiking Neural Network. (arXiv:2201.10943v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10943">
<div class="article-summary-box-inner">
<span><p>Neuromorphic vision sensor is a new bio-inspired imaging paradigm that
reports asynchronous, continuously per-pixel brightness changes called `events'
with high temporal resolution and high dynamic range. So far, the event-based
image reconstruction methods are based on artificial neural networks (ANN) or
hand-crafted spatiotemporal smoothing techniques. In this paper, we first
implement the image reconstruction work via fully spiking neural network (SNN)
architecture. As the bio-inspired neural networks, SNNs operating with
asynchronous binary spikes distributed over time, can potentially lead to
greater computational efficiency on event-driven hardware. We propose a novel
Event-based Video reconstruction framework based on a fully Spiking Neural
Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and
Membrane Potential (MP) neuron. We find that the spiking neurons have the
potential to store useful temporal information (memory) to complete such
time-dependent tasks. Furthermore, to better utilize the temporal information,
we propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane
potential of spiking neuron. The proposed neuron is referred as Adaptive
Membrane Potential (AMP) neuron, which adaptively updates the membrane
potential according to the input spikes. The experimental results demonstrate
that our models achieve comparable performance to ANN-based models on IJRR,
MVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are
19.36$\times$ and 7.75$\times$ more computationally efficient than their ANN
architectures, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. (arXiv:2201.12329v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12329">
<div class="article-summary-box-inner">
<span><p>We present in this paper a novel query formulation using dynamic anchor boxes
for DETR (DEtection TRansformer) and offer a deeper understanding of the role
of queries in DETR. This new formulation directly uses box coordinates as
queries in Transformer decoders and dynamically updates them layer-by-layer.
Using box coordinates not only helps using explicit positional priors to
improve the query-to-feature similarity and eliminate the slow training
convergence issue in DETR, but also allows us to modulate the positional
attention map using the box width and height information. Such a design makes
it clear that queries in DETR can be implemented as performing soft ROI pooling
layer-by-layer in a cascade manner. As a result, it leads to the best
performance on MS-COCO benchmark among the DETR-like detection models under the
same setting, e.g., AP 45.7\% using ResNet50-DC5 as backbone trained in 50
epochs. We also conducted extensive experiments to confirm our analysis and
verify the effectiveness of our methods. Code is available at
\url{https://github.com/SlongLiu/DAB-DETR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crafting Better Contrastive Views for Siamese Representation Learning. (arXiv:2202.03278v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03278">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised contrastive learning methods greatly benefit from the
Siamese structure that aims at minimizing distances between positive pairs. For
high performance Siamese representation learning, one of the keys is to design
good contrastive pairs. Most previous works simply apply random sampling to
make different crops of the same image, which overlooks the semantic
information that may degrade the quality of views. In this work, we propose
ContrastiveCrop, which could effectively generate better crops for Siamese
representation learning. Firstly, a semantic-aware object localization strategy
is proposed within the training process in a fully unsupervised manner. This
guides us to generate contrastive views which could avoid most false positives
(i.e., object vs. background). Moreover, we empirically find that views with
similar appearances are trivial for the Siamese model training. Thus, a
center-suppressed sampling is further designed to enlarge the variance of
crops. Remarkably, our method takes a careful consideration of positive pairs
for contrastive learning with negligible extra training overhead. As a
plug-and-play and framework-agnostic module, ContrastiveCrop consistently
improves SimCLR, MoCo, BYOL, SimSiam by 0.4% ~ 2.0% classification accuracy on
CIFAR-10, CIFAR-100, Tiny ImageNet and STL-10. Superior results are also
achieved on downstream detection and segmentation tasks when pre-trained on
ImageNet-1K.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DDL-MVS: Depth Discontinuity Learning for MVS Networks. (arXiv:2203.01391v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01391">
<div class="article-summary-box-inner">
<span><p>Traditional MVS methods have good accuracy but struggle with completeness,
while recently developed learning-based multi-view stereo (MVS) techniques have
improved completeness except accuracy being compromised. We propose depth
discontinuity learning for MVS methods, which further improves accuracy while
retaining the completeness of the reconstruction. Our idea is to jointly
estimate the depth and boundary maps where the boundary maps are explicitly
used for further refinement of the depth maps. We validate our idea and
demonstrate that our strategies can be easily integrated into the existing
learning-based MVS pipeline where the reconstruction depends on high-quality
depth map estimation. Extensive experiments on various datasets show that our
method improves reconstruction quality compared to baseline. Experiments also
demonstrate that the presented model and strategies have good generalization
capabilities. The source code will be available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Facial Paralysis Estimation with Facial Action Units. (arXiv:2203.01800v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01800">
<div class="article-summary-box-inner">
<span><p>Facial palsy is unilateral facial nerve weakness or paralysis of rapid onset
with unknown causes. Automatically estimating facial palsy severeness can be
helpful for the diagnosis and treatment of people suffering from it across the
world. In this work, we develop and experiment with a novel model for
estimating facial palsy severity. For this, an effective Facial Action Units
(AU) detection technique is incorporated into our model, where AUs refer to a
unique set of facial muscle movements used to describe almost every
anatomically possible facial expression. In this paper, we propose a novel
Adaptive Local-Global Relational Network (ALGRNet) for facial AU detection and
use it to classify facial paralysis severity. ALGRNet mainly consists of three
main novel structures: (i) an adaptive region learning module that learns the
adaptive muscle regions based on the detected landmarks; (ii) a skip-BiLSTM
that models the latent relationships among local AUs; and (iii) a feature
fusion&amp;refining module that investigates the complementary between the local
and global face. Quantitative results on two AU benchmarks, i.e., BP4D and
DISFA, demonstrate our ALGRNet can achieve promising AU detection accuracy. We
further demonstrate the effectiveness of its application to facial paralysis
estimation by migrating ALGRNet to a facial paralysis dataset collected and
annotated by medical professionals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoNIC Solution. (arXiv:2203.03415v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03415">
<div class="article-summary-box-inner">
<span><p>Nuclei segmentation and classification have been a challenge in digital
pathology due to the specific domain characteristics. First, annotating a
large-scale dataset is quite consuming. It requires specific domain knowledge
and large efforts. Second, some nuclei are clustered together and hard to
segment from each other. Third, the classes are often extremely unbalanced. As
in Lizard, the number of epithelial nuclei is around 67 times larger than the
number of eosinophil nuclei. Fourth, the nuclei often exhibit high inter-class
similarity and intra-class variability. Connective nuclei may look very
different from each other while some of them share a similar shape with the
epithelial ones. Last but not least, pathological patches may have very
different color distributions among different datasets. Thus, a large-scale
generally annotated dataset and a specially-designed algorithm are needed to
solve this problem. The CoNIC challenge aims to promote the automatic
segmentation and classification task and requires researchers to develop
algorithms that perform segmentation, classification, and counting of 6
different types of nuclei with the large-scale annotated dataset: Lizard. Due
to the 60-minute time limit, the algorithm has to be simple and quick. In this
paper, we briefly describe the final method we used in the CoNIC challenge. Our
algorithm is based on Hover-Net and we added several modifications to it to
improve its performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04963">
<div class="article-summary-box-inner">
<span><p>Learned image compression has achieved great success due to its excellent
modeling capacity, but seldom further considers the Rate-Distortion
Optimization (RDO) of each input image. To explore this potential in the
learned codec, we make the first attempt to build a neural data-dependent
transform and introduce a continuous online mode decision mechanism to jointly
optimize the coding efficiency for each individual image. Specifically, apart
from the image content stream, we employ an additional model stream to generate
the transform parameters at the decoder side. The presence of a model stream
enables our model to learn more abstract neural-syntax, which helps cluster the
latent representations of images more compactly. Beyond the transform stage, we
also adopt neural-syntax based post-processing for the scenarios that require
higher quality reconstructions regardless of extra decoding overhead. Moreover,
the involvement of the model stream further makes it possible to optimize both
the representation and the decoder in an online way, i.e. RDO at the testing
time. It is equivalent to a continuous online mode decision, like coding modes
in the traditional codecs, to improve the coding efficiency based on the
individual input image. The experimental results show the effectiveness of the
proposed neural-syntax design and the continuous online mode decision
mechanism, demonstrating the superiority of our method in coding efficiency
compared to the latest conventional standard Versatile Video Coding (VVC) and
other state-of-the-art learning-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation. (arXiv:2203.06386v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06386">
<div class="article-summary-box-inner">
<span><p>The recent large-scale vision-language pre-training (VLP) of dual-stream
architectures (e.g., CLIP) with a tremendous amount of image-text pair data,
has shown its superiority on various multimodal alignment tasks. Despite its
success, the resulting models are not capable of multimodal generative tasks
due to the weak text encoder. To tackle this problem, we propose to augment the
dual-stream VLP model with a textual pre-trained language model (PLM) via
vision-language knowledge distillation (VLKD), enabling the capability for
multimodal generation. VLKD is pretty data- and computation-efficient compared
to the pre-training from scratch. Experimental results show that the resulting
model has strong zero-shot performance on multimodal generation tasks, such as
open-ended visual question answering and image captioning. For example, it
achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous
state-of-the-art zero-shot model with $7\times$ fewer parameters. Furthermore,
the original textual language understanding and generation ability of the PLM
is maintained after VLKD, which makes our model versatile for both multimodal
and unimodal tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation. (arXiv:2203.06558v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06558">
<div class="article-summary-box-inner">
<span><p>Training a generalizable 3D part segmentation network is quite challenging
but of great importance in real-world applications. To tackle this problem,
some works design task-specific solutions by translating human understanding of
the task to machine's learning process, which faces the risk of missing the
optimal strategy since machines do not necessarily understand in the exact
human way. Others try to use conventional task-agnostic approaches designed for
domain generalization problems with no task prior knowledge considered. To
solve the above issues, we propose AutoGPart, a generic method enabling
training generalizable 3D part segmentation networks with the task prior
considered. AutoGPart builds a supervision space with geometric prior knowledge
encoded, and lets the machine to search for the optimal supervisions from the
space for a specific segmentation task automatically. Extensive experiments on
three generalizable 3D part segmentation tasks are conducted to demonstrate the
effectiveness and versatility of AutoGPart. We demonstrate that the performance
of segmentation networks using simple backbones can be significantly improved
when trained with supervisions searched by our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06717">
<div class="article-summary-box-inner">
<span><p>We revisit large kernel design in modern convolutional neural networks
(CNNs). Inspired by recent advances in vision transformers (ViTs), in this
paper, we demonstrate that using a few large convolutional kernels instead of a
stack of small kernels could be a more powerful paradigm. We suggested five
guidelines, e.g., applying re-parameterized large depth-wise convolutions, to
design efficient high-performance large-kernel CNNs. Following the guidelines,
we propose RepLKNet, a pure CNN architecture whose kernel size is as large as
31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the
performance gap between CNNs and ViTs, e.g., achieving comparable or superior
results than Swin Transformer on ImageNet and a few typical downstream tasks,
with lower latency. RepLKNet also shows nice scalability to big data and large
models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K,
which is very competitive among the state-of-the-arts with similar model sizes.
Our study further reveals that, in contrast to small-kernel CNNs, large-kernel
CNNs have much larger effective receptive fields and higher shape bias rather
than texture bias. Code &amp; models at
https://github.com/megvii-research/RepLKNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Autofocusing using Tiny Transformer Networks for Digital Holographic Microscopy. (arXiv:2203.07772v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07772">
<div class="article-summary-box-inner">
<span><p>The numerical wavefront backpropagation principle of digital holography
confers unique extended focus capabilities, without mechanical displacements
along z-axis. However, the determination of the correct focusing distance is a
non-trivial and time consuming issue. A deep learning (DL) solution is proposed
to cast the autofocusing as a regression problem and tested over both
experimental and simulated holograms. Single wavelength digital holograms were
recorded by a Digital Holographic Microscope (DHM) with a 10$\mathrm{x}$
microscope objective from a patterned target moving in 3D over an axial range
of 92 $\mu$m. Tiny DL models are proposed and compared such as a tiny Vision
Transformer (TViT), tiny VGG16 (TVGG) and a tiny Swin-Transfomer (TSwinT). The
experiments show that the predicted focusing distance $Z_R^{\mathrm{Pred}}$ is
accurately inferred with an accuracy of 1.2 $\mu$m in average in comparison
with the DHM depth of field of 15 $\mu$m. Numerical simulations show that all
tiny models give the $Z_R^{\mathrm{Pred}}$ with an error below 0.3 $\mu$m. Such
a prospect would significantly improve the current capabilities of computer
vision position sensing in applications such as 3D microscopy for life sciences
or micro-robotics. Moreover, all models reach state of the art inference time
on CPU, less than 25 ms per inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation. (arXiv:2203.09418v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09418">
<div class="article-summary-box-inner">
<span><p>Establishing correspondences from image to 3D has been a key task of 6DoF
object pose estimation for a long time. To predict pose more accurately, deeply
learned dense maps replaced sparse templates. Dense methods also improved pose
estimation in the presence of occlusion. More recently researchers have shown
improvements by learning object fragments as segmentation. In this work, we
present a discrete descriptor, which can represent the object surface densely.
By incorporating a hierarchical binary grouping, we can encode the object
surface very efficiently. Moreover, we propose a coarse to fine training
strategy, which enables fine-grained correspondence prediction. Finally, by
matching predicted codes with object surface and using a PnP solver, we
estimate the 6DoF pose. Results on the public LM-O and YCB-V datasets show
major improvement over the state of the art w.r.t. ADD(-S) metric, even
surpassing RGB-D based methods in some cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imaging-based histological features are predictive of MET alterations in Non-Small Cell Lung Cancer. (arXiv:2203.10062v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10062">
<div class="article-summary-box-inner">
<span><p>MET is a proto-oncogene whose somatic activation in non-small cell lung
cancer leads to increased cell growth and tumor progression. The two major
classes of MET alterations are gene amplification and exon 14 deletion, both of
which are therapeutic targets and detectable using existing molecular assays.
However, existing tests are limited by their consumption of valuable tissue,
cost and complexity that prevent widespread use. MET alterations could have an
effect on cell morphology, and quantifying these associations could open new
avenues for research and development of morphology-based screening tools. Using
H&amp;E-stained whole slide images (WSIs), we investigated the association of
distinct cell-morphological features with MET amplifications and MET exon 14
deletions. We found that cell shape, color, grayscale intensity and
texture-based features from both tumor infiltrating lymphocytes and tumor cells
distinguished MET wild-type from MET amplified or MET exon 14 deletion cases.
The association of individual cell features with MET alterations suggested a
predictive model could distinguish MET wild-type from MET amplification or MET
exon 14 deletion. We therefore developed an L1-penalized logistic regression
model, achieving a mean Area Under the Receiver Operating Characteristic Curve
(ROC-AUC) of 0.77 +/- 0.05sd in cross-validation and 0.77 on an independent
holdout test set. A sparse set of 43 features differentiated these classes,
which included features similar to what was found in the univariate analysis as
well as the percent of tumor cells in the tissue. Our study demonstrates that
MET alterations result in a detectable morphological signal in tumor cells and
lymphocytes. These results suggest that development of low-cost predictive
models based on H&amp;E-stained WSIs may improve screening for MET altered tumors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Nighttime Aerial Tracking. (arXiv:2203.10541v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10541">
<div class="article-summary-box-inner">
<span><p>Previous advances in object tracking mostly reported on favorable
illumination circumstances while neglecting performance at nighttime, which
significantly impeded the development of related aerial robot applications.
This work instead develops a novel unsupervised domain adaptation framework for
nighttime aerial tracking (named UDAT). Specifically, a unique object discovery
approach is provided to generate training patches from raw nighttime tracking
videos. To tackle the domain discrepancy, we employ a Transformer-based
bridging layer post to the feature extractor to align image features from both
domains. With a Transformer day/night feature discriminator, the daytime
tracking model is adversarially trained to track at night. Moreover, we
construct a pioneering benchmark namely NAT2021 for unsupervised domain
adaptive nighttime tracking, which comprises a test set of 180 manually
annotated tracking sequences and a train set of over 276k unlabelled nighttime
tracking frames. Exhaustive experiments demonstrate the robustness and domain
adaptability of the proposed framework in nighttime aerial tracking. The code
and benchmark are available at https://github.com/vision4robotics/UDAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization. (arXiv:2203.11471v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11471">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute
human pose estimation with calibrated camera. Accurate and generalizable
absolute 3D human pose estimation from monocular 2D pose input is an ill-posed
problem. To address this challenge, we convert the input from pixel space to 3D
normalized rays. This conversion makes our approach robust to camera intrinsic
parameter changes. To deal with the in-the-wild camera extrinsic parameter
variations, Ray3D explicitly takes the camera extrinsic parameters as an input
and jointly models the distribution between the 3D pose rays and camera
extrinsic parameters. This novel network design is the key to the outstanding
generalizability of Ray3D approach. To have a comprehensive understanding of
how the camera intrinsic and extrinsic parameter variations affect the accuracy
of absolute 3D key-point localization, we conduct in-depth systematic
experiments on three single person 3D benchmarks as well as one synthetic
benchmark. These experiments demonstrate that our method significantly
outperforms existing state-of-the-art models. Our code and the synthetic
dataset are available at https://github.com/YxZhxn/Ray3D .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11480">
<div class="article-summary-box-inner">
<span><p>Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSD-KD: A Self-supervised Diverse Knowledge Distillation Method for Lightweight Skin Lesion Classification Using Dermoscopic Images. (arXiv:2203.11490v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11490">
<div class="article-summary-box-inner">
<span><p>Skin cancer is one of the most common types of malignancy, affecting a large
population and causing a heavy economic burden worldwide. Over the last few
years, computer-aided diagnosis has been rapidly developed and make great
progress in healthcare and medical practices due to the advances in artificial
intelligence. However, most studies in skin cancer detection keep pursuing high
prediction accuracies without considering the limitation of computing resources
on portable devices. In this case, knowledge distillation (KD) has been proven
as an efficient tool to help improve the adaptability of lightweight models
under limited resources, meanwhile keeping a high-level representation
capability. To bridge the gap, this study specifically proposes a novel method,
termed SSD-KD, that unifies diverse knowledge into a generic KD framework for
skin diseases classification. Our method models an intra-instance relational
feature representation and integrates it with existing KD research. A dual
relational knowledge distillation architecture is self-supervisedly trained
while the weighted softened outputs are also exploited to enable the student
model to capture richer knowledge from the teacher model. To demonstrate the
effectiveness of our method, we conduct experiments on ISIC 2019, a large-scale
open-accessed benchmark of skin diseases dermoscopic images. Experiments show
that our distilled lightweight model can achieve an accuracy as high as 85% for
the classification tasks of 8 different skin diseases with minimal parameters
and computing requirements. Ablation studies confirm the effectiveness of our
intra- and inter-instance relational knowledge integration strategy. Compared
with state-of-the-art knowledge distillation techniques, the proposed method
demonstrates improved performances for multi-diseases classification on the
large-scale dermoscopy database.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Emotion Recognition using Visual-audio-linguistic information: A Technical Report for ABAW3. (arXiv:2203.13031v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13031">
<div class="article-summary-box-inner">
<span><p>We propose a cross-modal co-attention model for continuous emotion
recognition using visual-audio-linguistic information. The model consists of
four blocks. The visual, audio, and linguistic blocks are used to learn the
spatial-temporal features of the multi-modal input. A co-attention block is
designed to fuse the learned features with the multi-head co-attention
mechanism. The visual encoding from the visual block is concatenated with the
attention feature to emphasize the visual information. To make full use of the
data and alleviate over-fitting, cross-validation is carried out on the
training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. The achieved CCC on the
test set is $0.520$ for valence and $0.602$ for arousal, which significantly
outperforms the baseline method with the corresponding CCC of 0.180 and 0.170
for valence and arousal, respectively. The code is available at
https://github.com/sucv/ABAW3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Monocular Depth Estimation Provide Better Pre-training than Classification for Semantic Segmentation?. (arXiv:2203.13987v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13987">
<div class="article-summary-box-inner">
<span><p>Training a deep neural network for semantic segmentation is labor-intensive,
so it is common to pre-train it for a different task, and then fine-tune it
with a small annotated dataset. State-of-the-art methods use image
classification for pre-training, which introduces uncontrolled biases. We test
the hypothesis that depth estimation from unlabeled videos may provide better
pre-training. Despite the absence of any semantic information, we argue that
estimating scene geometry is closer to the task of semantic segmentation than
classifying whole images into semantic classes. Since analytical validation is
intractable, we test the hypothesis empirically by introducing a pre-training
scheme that yields an improvement of 5.7% mIoU and 4.1% pixel accuracy over
classification-based pre-training. While annotation is not needed for
pre-training, it is needed for testing the hypothesis. We use the KITTI
(outdoor) and NYU-V2 (indoor) benchmarks to that end, and provide an extensive
discussion of the benefits and limitations of the proposed scheme in relation
to existing unsupervised, self-supervised, and semi-supervised pre-training
protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Deep Implicit Functions for 3D Shapes with Dynamic Code Clouds. (arXiv:2203.14048v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14048">
<div class="article-summary-box-inner">
<span><p>Deep Implicit Function (DIF) has gained popularity as an efficient 3D shape
representation. To capture geometry details, current methods usually learn DIF
using local latent codes, which discretize the space into a regular 3D grid (or
octree) and store local codes in grid points (or octree nodes). Given a query
point, the local feature is computed by interpolating its neighboring local
codes with their positions. However, the local codes are constrained at
discrete and regular positions like grid points, which makes the code positions
difficult to be optimized and limits their representation ability. To solve
this problem, we propose to learn DIF with Dynamic Code Cloud, named DCC-DIF.
Our method explicitly associates local codes with learnable position vectors,
and the position vectors are continuous and can be dynamically optimized, which
improves the representation ability. In addition, we propose a novel code
position loss to optimize the code positions, which heuristically guides more
local codes to be distributed around complex geometric details. In contrast to
previous methods, our DCC-DIF represents 3D shapes more efficiently with a
small amount of local codes, and improves the reconstruction quality.
Experiments demonstrate that DCC-DIF achieves better performance over previous
methods. Code and data are available at https://github.com/lity20/DCCDIF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14128">
<div class="article-summary-box-inner">
<span><p>In the last two years, millions of lives have been lost due to COVID-19.
Despite the vaccination programmes for a year, hospitalization rates and deaths
are still high due to the new variants of COVID-19. Stringent guidelines and
COVID-19 screening measures such as temperature check and mask check at all
public places are helping reduce the spread of COVID-19. Visual inspections to
ensure these screening measures can be taxing and erroneous. Automated
inspection ensures an effective and accurate screening. Traditional approaches
involve identification of faces and masks from visual camera images followed by
extraction of temperature values from thermal imaging cameras. Use of visual
imaging as a primary modality limits these applications only for good-lighting
conditions. The use of thermal imaging alone for these screening measures makes
the system invariant to illumination. However, lack of open source datasets is
an issue to develop such systems. In this paper, we discuss our work on using
machine learning over thermal video streams for face and mask detection and
subsequent temperature screening in a passive non-invasive way that enables an
effective automated COVID-19 screening method in public places. We open source
our NTIC dataset that was used for training our models and was collected at 8
different locations. Our results show that the use of thermal imaging is as
effective as visual imaging in the presence of high illumination. This
performance stays the same for thermal images even under low-lighting
conditions, whereas the performance with visual trained classifiers show more
than 50% degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Stream Multi-Level Alignment for Vision-Language Pretraining. (arXiv:2203.14395v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14395">
<div class="article-summary-box-inner">
<span><p>Recent progress in large-scale vision-language pre-training has shown the
importance of aligning the visual and text modalities for downstream
vision-language tasks. Many methods use a dual-stream architecture that fuses
visual tokens and language tokens after representation learning, which aligns
only at a global level and cannot extract finer-scale semantics. In contrast,
we propose a single stream model that aligns the modalities at multiple levels:
i) instance level, ii) fine-grained patch level, iii) conceptual semantic
level. We achieve this using two novel tasks: symmetric cross-modality
reconstruction and a pseudo-labeled key word prediction. In the former part, we
mask the input tokens from one of the modalities and use the cross-modal
information to reconstruct the masked token, thus improving fine-grained
alignment between the two modalities. In the latter part, we parse the caption
to select a few key words and feed it together with the momentum encoder pseudo
signal to self-supervise the visual encoder, enforcing it to learn rich
semantic concepts that are essential for grounding a textual token to an image
region. We demonstrate top performance on a set of Vision-Language downstream
tasks such as zero-shot/fine-tuned image/text retrieval, referring expression,
and VQA. We also demonstrate how the proposed models can align the modalities
at multiple levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ObjectFormer for Image Manipulation Detection and Localization. (arXiv:2203.14681v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14681">
<div class="article-summary-box-inner">
<span><p>Recent advances in image editing techniques have posed serious challenges to
the trustworthiness of multimedia data, which drives the research of image
tampering detection. In this paper, we propose ObjectFormer to detect and
localize image manipulations. To capture subtle manipulation traces that are no
longer visible in the RGB domain, we extract high-frequency features of the
images and combine them with RGB features as multimodal patch embeddings.
Additionally, we use a set of learnable object prototypes as mid-level
representations to model the object-level consistencies among different
regions, which are further used to refine patch embeddings to capture the
patch-level consistencies. We conduct extensive experiments on various datasets
and the results verify the effectiveness of the proposed method, outperforming
state-of-the-art tampering detection and localization methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WSEBP: A Novel Width-depth Synchronous Extension-based Basis Pursuit Algorithm for Multi-Layer Convolutional Sparse Coding. (arXiv:2203.14856v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14856">
<div class="article-summary-box-inner">
<span><p>The pursuit algorithms integrated in multi-layer convolutional sparse coding
(ML-CSC) can interpret the convolutional neural networks (CNNs). However, many
current state-of-art (SOTA) pursuit algorithms require multiple iterations to
optimize the solution of ML-CSC, which limits their applications to deeper CNNs
due to high computational cost and large number of resources for getting very
tiny gain of performance. In this study, we focus on the 0th iteration in
pursuit algorithm by introducing an effective initialization strategy for each
layer, by which the solution for ML-CSC can be improved. Specifically, we first
propose a novel width-depth synchronous extension-based basis pursuit (WSEBP)
algorithm which solves the ML-CSC problem without the limitation of the number
of iterations compared to the SOTA algorithms and maximizes the performance by
an effective initialization in each layer. Then, we propose a simple and
unified ML-CSC-based classification network (ML-CSC-Net) which consists of an
ML-CSC-based feature encoder and a fully-connected layer to validate the
performance of WSEBP on image classification task. The experimental results
show that our proposed WSEBP outperforms SOTA algorithms in terms of accuracy
and consumption resources. In addition, the WSEBP integrated in CNNs can
improve the performance of deeper CNNs and make them interpretable. Finally,
taking VGG as an example, we propose WSEBP-VGG13 to enhance the performance of
VGG13, which achieves competitive results on four public datasets, i.e., 87.79%
vs. 86.83% on Cifar-10 dataset, 58.01% vs. 54.60% on Cifar-100 dataset, 91.52%
vs. 89.58% on COVID-19 dataset, and 99.88% vs. 99.78% on Crack dataset,
respectively. The results show the effectiveness of the proposed WSEBP, the
improved performance of ML-CSC with WSEBP, and interpretation of the CNNs or
deeper CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affine Medical Image Registration with Coarse-to-Fine Vision Transformer. (arXiv:2203.15216v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15216">
<div class="article-summary-box-inner">
<span><p>Affine registration is indispensable in a comprehensive medical image
registration pipeline. However, only a few studies focus on fast and robust
affine registration algorithms. Most of these studies utilize convolutional
neural networks (CNNs) to learn joint affine and non-parametric registration,
while the standalone performance of the affine subnetwork is less explored.
Moreover, existing CNN-based affine registration approaches focus either on the
local misalignment or the global orientation and position of the input to
predict the affine transformation matrix, which are sensitive to spatial
initialization and exhibit limited generalizability apart from the training
dataset. In this paper, we present a fast and robust learning-based algorithm,
Coarse-to-Fine Vision Transformer (C2FViT), for 3D affine medical image
registration. Our method naturally leverages the global connectivity and
locality of the convolutional vision transformer and the multi-resolution
strategy to learn the global affine registration. We evaluate our method on 3D
brain atlas registration and template-matching normalization. Comprehensive
results demonstrate that our method is superior to the existing CNNs-based
affine registration methods in terms of registration accuracy, robustness and
generalizability while preserving the runtime advantage of the learning-based
methods. The source code is available at https://github.com/cwmok/C2FViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few Could Be Better Than All: Feature Sampling and Grouping for Scene Text Detection. (arXiv:2203.15221v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15221">
<div class="article-summary-box-inner">
<span><p>Recently, transformer-based methods have achieved promising progresses in
object detection, as they can eliminate the post-processes like NMS and enrich
the deep representations. However, these methods cannot well cope with scene
text due to its extreme variance of scales and aspect ratios. In this paper, we
present a simple yet effective transformer-based architecture for scene text
detection. Different from previous approaches that learn robust deep
representations of scene text in a holistic manner, our method performs scene
text detection based on a few representative features, which avoids the
disturbance by background and reduces the computational cost. Specifically, we
first select a few representative features at all scales that are highly
relevant to foreground text. Then, we adopt a transformer for modeling the
relationship of the sampled features, which effectively divides them into
reasonable groups. As each feature group corresponds to a text instance, its
bounding box can be easily obtained without any post-processing operation.
Using the basic feature pyramid network for feature extraction, our method
consistently achieves state-of-the-art results on several popular datasets for
scene text detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Image Transformers using Learnable Memory. (arXiv:2203.15243v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15243">
<div class="article-summary-box-inner">
<span><p>In this paper we propose augmenting Vision Transformer models with learnable
memory tokens. Our approach allows the model to adapt to new tasks, using few
parameters, while optionally preserving its capabilities on previously learned
tasks. At each layer we introduce a set of learnable embedding vectors that
provide contextual information useful for specific datasets. We call these
"memory tokens". We show that augmenting a model with just a handful of such
tokens per layer significantly improves accuracy when compared to conventional
head-only fine-tuning, and performs only slightly below the significantly more
expensive full fine-tuning. We then propose an attention-masking approach that
enables extension to new downstream tasks, with a computation reuse. In this
setup in addition to being parameters efficient, models can execute both old
and new tasks as a part of single inference at a small incremental cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAT: Mask-Aware Transformer for Large Hole Image Inpainting. (arXiv:2203.15270v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15270">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown the importance of modeling long-range interactions
in the inpainting problem. To achieve this goal, existing approaches exploit
either standalone attention techniques or transformers, but usually under a low
resolution in consideration of computational cost. In this paper, we present a
novel transformer-based model for large hole inpainting, which unifies the
merits of transformers and convolutions to efficiently process high-resolution
images. We carefully design each component of our framework to guarantee the
high fidelity and diversity of recovered images. Specifically, we customize an
inpainting-oriented transformer block, where the attention module aggregates
non-local information only from partial valid tokens, indicated by a dynamic
mask. Extensive experiments demonstrate the state-of-the-art performance of the
new model on multiple benchmark datasets. Code is released at
https://github.com/fenglinglwb/MAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIOD: Single Instance Annotated Per Category Per Image for Object Detection. (arXiv:2203.15353v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15353">
<div class="article-summary-box-inner">
<span><p>Object detection under imperfect data receives great attention recently.
Weakly supervised object detection (WSOD) suffers from severe localization
issues due to the lack of instance-level annotation, while semi-supervised
object detection (SSOD) remains challenging led by the inter-image discrepancy
between labeled and unlabeled data. In this study, we propose the Single
Instance annotated Object Detection (SIOD), requiring only one instance
annotation for each existing category in an image. Degraded from inter-task
(WSOD) or inter-image (SSOD) discrepancies to the intra-image discrepancy, SIOD
provides more reliable and rich prior knowledge for mining the rest of
unlabeled instances and trades off the annotation cost and performance. Under
the SIOD setting, we propose a simple yet effective framework, termed
Dual-Mining (DMiner), which consists of a Similarity-based Pseudo Label
Generating module (SPLG) and a Pixel-level Group Contrastive Learning module
(PGCL). SPLG firstly mines latent instances from feature representation space
to alleviate the annotation missing problem. To avoid being misled by
inaccurate pseudo labels, we propose PGCL to boost the tolerance to false
pseudo labels. Extensive experiments on MS COCO verify the feasibility of the
SIOD setting and the superiority of the proposed method, which obtains
consistent and significant improvements compared to baseline methods and
achieves comparable results with fully supervised object detection (FSOD)
methods with only 40% instances annotated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries. (arXiv:2203.15355v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15355">
<div class="article-summary-box-inner">
<span><p>Learning under a continuously changing data distribution with incorrect
labels is a desirable real-world problem yet challenging. A large body of
continual learning (CL) methods, however, assumes data streams with clean
labels, and online learning scenarios under noisy data streams are yet
underexplored. We consider a more practical CL task setup of an online learning
from blurry data stream with corrupted labels, where existing CL methods
struggle. To address the task, we first argue the importance of both diversity
and purity of examples in the episodic memory of continual learning models. To
balance diversity and purity in the episodic memory, we propose a novel
strategy to manage and use the memory by a unified approach of label noise
aware diverse sampling and robust learning with semi-supervised learning. Our
empirical validations on four real-world or synthetic noise datasets (CIFAR10
and 100, mini-WebVision, and Food-101N) exhibit that our method significantly
outperforms prior arts in this realistic and challenging continual learning
scenario. Code and data splits are available in
https://github.com/clovaai/puridiver.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransductGAN: a Transductive Adversarial Model for Novelty Detection. (arXiv:2203.15406v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15406">
<div class="article-summary-box-inner">
<span><p>Novelty detection, a widely studied problem in machine learning, is the
problem of detecting a novel class of data that has not been previously
observed. A common setting for novelty detection is inductive whereby only
examples of the negative class are available during training time. Transductive
novelty detection on the other hand has only witnessed a recent surge in
interest, it not only makes use of the negative class during training but also
incorporates the (unlabeled) test set to detect novel examples. Several studies
have emerged under the transductive setting umbrella that have demonstrated its
advantage over its inductive counterpart. Depending on the assumptions about
the data, these methods go by different names (e.g. transductive novelty
detection, semi-supervised novelty detection, positive-unlabeled learning,
out-of-distribution detection). With the use of generative adversarial networks
(GAN), a segment of those studies have adopted a transductive setup in order to
learn how to generate examples of the novel class. In this study, we propose
TransductGAN, a transductive generative adversarial network that attempts to
learn how to generate image examples from both the novel and negative classes
by using a mixture of two Gaussians in the latent space. It achieves that by
incorporating an adversarial autoencoder with a GAN network, the ability to
generate examples of novel data points offers not only a visual representation
of novelties, but also overcomes the hurdle faced by many inductive methods of
how to tune the model hyperparameters at the decision rule level. Our model has
shown superior performance over state-of-the-art inductive and transductive
methods. Our study is fully reproducible with the code available publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSOP: A Multi-Stage One Shot Object Pose Estimation Framework. (arXiv:2203.15533v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15533">
<div class="article-summary-box-inner">
<span><p>We present a novel one-shot method for object detection and 6 DoF pose
estimation, that does not require training on target objects. At test time, it
takes as input a target image and a textured 3D query model. The core idea is
to represent a 3D model with a number of 2D templates rendered from different
viewpoints. This enables CNN-based direct dense feature extraction and
matching. The object is first localized in 2D, then its approximate viewpoint
is estimated, followed by dense 2D-3D correspondence prediction. The final pose
is computed with PnP. We evaluate the method on LineMOD, Occlusion, Homebrewed,
YCB-V and TLESS datasets and report very competitive performance in comparison
to the state-of-the-art methods trained on synthetic data, even though our
method is not trained on the object models used for testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15547">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks need the construction of informative features,
which are determined by channel-wise and spatial-wise information at the
network's layers. In this research, we focus on bringing in a novel solution
that uses sophisticated optimization for enhancing both the spatial and channel
components inside each layer's receptive field. Capsule Networks were used to
understand the spatial association between features in the feature map.
Standalone capsule networks have shown good results on comparatively simple
datasets than on complex datasets as a result of the inordinate amount of
feature information. Thus, to tackle this issue, we have proposed ME-CapsNet by
introducing deeper convolutional layers to extract important features before
passing through modules of capsule layers strategically to improve the
performance of the network significantly. The deeper convolutional layer
includes blocks of Squeeze-Excitation networks which uses a soft-pooling
approach for progressively reducing the spatial size thereby dynamically
recalibrating the channels by reconstructing their interdependencies without
much loss of important feature information. Extensive experimentation was done
using commonly used datasets demonstrating the efficiency of the proposed
ME-CapsNet, which clearly outperforms various research works by achieving
higher accuracy with minimal model complexity in complex datasets.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-31 23:08:48.855383232 UTC">2022-03-31 23:08:48 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>