<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-06T01:30:00Z">05-06</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset for N-ary Relation Extraction of Drug Combinations. (arXiv:2205.02289v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02289">
<div class="article-summary-box-inner">
<span><p>Combination therapies have become the standard of care for diseases such as
cancer, tuberculosis, malaria and HIV. However, the combinatorial set of
available multi-drug treatments creates a challenge in identifying effective
combination therapies available in a situation. To assist medical professionals
in identifying beneficial drug-combinations, we construct an expert-annotated
dataset for extracting information about the efficacy of drug combinations from
the scientific literature. Beyond its practical utility, the dataset also
presents a unique NLP challenge, as the first relation extraction dataset
consisting of variable-length relations. Furthermore, the relations in this
dataset predominantly require language understanding beyond the sentence level,
adding to the challenge of this task. We provide a promising baseline model and
identify clear areas for further improvement. We release our dataset, code, and
baseline models publicly to encourage the NLP community to participate in this
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance. (arXiv:2205.02293v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02293">
<div class="article-summary-box-inner">
<span><p>Human-translated text displays distinct features from naturally written text
in the same language. This phenomena, known as translationese, has been argued
to confound the machine translation (MT) evaluation. Yet, we find that existing
work on translationese neglects some important factors and the conclusions are
mostly correlational but not causal. In this work, we collect CausalMT, a
dataset where the MT training data are also labeled with the human translation
directions. We inspect two critical factors, the train-test direction match
(whether the human translation directions in the training and test sets are
aligned), and data-model direction match (whether the model learns in the same
direction as the human translation direction in the dataset). We show that
these two factors have a large causal effect on the MT performance, in addition
to the test-model direction mismatch highlighted by existing work on the impact
of translationese. In light of our findings, we provide a set of suggestions
for MT training and evaluation. Our code and data are at
https://github.com/EdisonNi-hku/CausalMT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust and Semantically Organised Latent Representations for Unsupervised Text Style Transfer. (arXiv:2205.02309v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02309">
<div class="article-summary-box-inner">
<span><p>Recent studies show that auto-encoder based approaches successfully perform
language generation, smooth sentence interpolation, and style transfer over
unseen attributes using unlabelled datasets in a zero-shot manner. The latent
space geometry of such models is organised well enough to perform on datasets
where the style is "coarse-grained" i.e. a small fraction of words alone in a
sentence are enough to determine the overall style label. A recent study uses a
discrete token-based perturbation approach to map "similar" sentences
("similar" defined by low Levenshtein distance/ high word overlap) close by in
latent space. This definition of "similarity" does not look into the underlying
nuances of the constituent words while mapping latent space neighbourhoods and
therefore fails to recognise sentences with different style-based semantics
while mapping latent neighbourhoods. We introduce EPAAEs (Embedding Perturbed
Adversarial AutoEncoders) which completes this perturbation model, by adding a
finely adjustable noise component on the continuous embeddings space. We
empirically show that this (a) produces a better organised latent space that
clusters stylistically similar sentences together, (b) performs best on a
diverse set of text style transfer tasks than similar denoising-inspired
baselines, and (c) is capable of fine-grained control of Style Transfer
strength. We also extend the text style transfer tasks to NLI datasets and show
that these more complex definitions of style are learned best by EPAAE. To the
best of our knowledge, extending style transfer to NLI tasks has not been
explored before.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models in the Loop: Incorporating Prompting into Weak Supervision. (arXiv:2205.02318v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02318">
<div class="article-summary-box-inner">
<span><p>We propose a new strategy for applying large pre-trained language models to
novel tasks when labeled training data is limited. Rather than apply the model
in a typical zero-shot or few-shot fashion, we treat the model as the basis for
labeling functions in a weak supervision framework. To create a classifier, we
first prompt the model to answer multiple distinct queries about an example and
define how the possible responses should be mapped to votes for labels and
abstentions. We then denoise these noisy label sources using the Snorkel system
and train an end classifier with the resulting training data. Our experimental
evaluation shows that prompting large language models within a weak supervision
framework can provide significant gains in accuracy. On the WRENCH weak
supervision benchmark, this approach can significantly improve over zero-shot
performance, an average 19.5% reduction in errors. We also find that this
approach produces classifiers with comparable or superior accuracy to those
trained from hand-engineered rules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation of Russian Language Models with Reduction of Vocabulary. (arXiv:2205.02340v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02340">
<div class="article-summary-box-inner">
<span><p>Today, transformer language models serve as a core component for majority of
natural language processing tasks. Industrial application of such models
requires minimization of computation time and memory footprint. Knowledge
distillation is one of approaches to address this goal. Existing methods in
this field are mainly focused on reducing the number of layers or dimension of
embeddings/hidden representations. Alternative option is to reduce the number
of tokens in vocabulary and therefore the embeddings matrix of the student
model. The main problem with vocabulary minimization is mismatch between input
sequences and output class distributions of a teacher and a student models. As
a result, it is impossible to directly apply KL-based knowledge distillation.
We propose two simple yet effective alignment techniques to make knowledge
distillation to the students with reduced vocabulary. Evaluation of distilled
models on a number of common benchmarks for Russian such as Russian SuperGLUE,
SberQuAD, RuSentiment, ParaPhaser, Collection-3 demonstrated that our
techniques allow to achieve compression from $17\times$ to $49\times$, while
maintaining quality of $1.7\times$ compressed student with the full-sized
vocabulary, but reduced number of Transformer layers only. We make our code and
distilled models available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02355">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have contributed significantly to relation
extraction by demonstrating remarkable few-shot learning abilities. However,
prompt tuning methods for relation extraction may still fail to generalize to
those rare or hard patterns. Note that the previous parametric learning
paradigm can be viewed as memorization regarding training data as a book and
inference as the close-book test. Those long-tailed or hard patterns can hardly
be memorized in parameters given few-shot instances. To this end, we regard RE
as an open-book examination and propose a new semiparametric paradigm of
retrieval-enhanced prompt tuning for relation extraction. We construct an
open-book datastore for retrieval regarding prompt-based instance
representations and corresponding relation labels as memorized key-value pairs.
During inference, the model can infer relations by linearly interpolating the
base output of PLM with the non-parametric nearest neighbor distribution over
the datastore. In this way, our model not only infers relation through
knowledge stored in the weights during training but also assists
decision-making by unwinding and querying examples in the open-book datastore.
Extensive experiments on benchmark datasets show that our method can achieve
state-of-the-art in both standard supervised and few-shot settings. Code are
available in https://github.com/zjunlp/PromptKG/tree/main/research/RetrievalRE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02357">
<div class="article-summary-box-inner">
<span><p>Multimodal Knowledge Graphs (MKGs), which organize visual-text factual
knowledge, have recently been successfully applied to tasks such as information
retrieval, question answering, and recommendation system. Since most MKGs are
far from complete, extensive knowledge graph completion studies have been
proposed focusing on the multimodal entity, relation extraction and link
prediction. However, different tasks and modalities require changes to the
model architecture, and not all images/objects are relevant to text input,
which hinders the applicability to diverse real-world scenarios. In this paper,
we propose a hybrid transformer with multi-level fusion to address those
issues. Specifically, we leverage a hybrid transformer architecture with
unified input-output for diverse multimodal knowledge graph completion tasks.
Moreover, we propose multi-level fusion, which integrates visual and text
representation via coarse-grained prefix-guided interaction and fine-grained
correlation-aware fusion modules. We conduct extensive experiments to validate
that our MKGformer can obtain SOTA performance on four datasets of multimodal
link prediction, multimodal RE, and multimodal NER. Code is available in
https://github.com/zjunlp/MKGformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language. (arXiv:2205.02364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02364">
<div class="article-summary-box-inner">
<span><p>This research developed a Kencorpus Swahili Question Answering Dataset
KenSwQuAD from raw data of Swahili language, which is a low resource language
predominantly spoken in Eastern African and also has speakers in other parts of
the world. Question Answering datasets are important for machine comprehension
of natural language processing tasks such as internet search and dialog
systems. However, before such machine learning systems can perform these tasks,
they need training data such as the gold standard Question Answering (QA) set
that is developed in this research. The research engaged annotators to
formulate question answer pairs from Swahili texts that had been collected by
the Kencorpus project, a Kenyan languages corpus that collected data from three
Kenyan languages. The total Swahili data collection had 2,585 texts, out of
which we annotated 1,445 story texts with at least 5 QA pairs each, resulting
into a final dataset of 7,526 QA pairs. A quality assurance set of 12.5% of the
annotated texts was subjected to re-evaluation by different annotators who
confirmed that the QA pairs were all correctly annotated. A proof of concept on
applying the set to machine learning on the question answering task confirmed
that the dataset can be used for such practical tasks. The research therefore
developed KenSwQuAD, a question-answer dataset for Swahili that is useful to
the natural language processing community who need training and gold standard
sets for their machine learning applications. The research also contributed to
the resourcing of the Swahili language which is important for communication
around the globe. Updating this set and providing similar sets for other low
resource languages is an important research area that is worthy of further
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PREME: Preference-based Meeting Exploration through an Interactive Questionnaire. (arXiv:2205.02370v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02370">
<div class="article-summary-box-inner">
<span><p>The recent increase in the volume of online meetings necessitates automated
tools for managing and organizing the material, especially when an attendee has
missed the discussion and needs assistance in quickly exploring it. In this
work, we propose a novel end-to-end framework for generating interactive
questionnaires for preference-based meeting exploration. As a result, users are
supplied with a list of suggested questions reflecting their preferences. Since
the task is new, we introduce an automatic evaluation strategy. Namely, it
measures how much the generated questions via questionnaire are answerable to
ensure factual correctness and covers the source meeting for the depth of
possible exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021. (arXiv:2205.02388v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02388">
<div class="article-summary-box-inner">
<span><p>Human intelligence has the remarkable ability to quickly adapt to new tasks
and environments. Starting from a very young age, humans acquire new skills and
learn how to solve new tasks either by imitating the behavior of others or by
following provided natural language instructions. To facilitate research in
this direction, we propose \emph{IGLU: Interactive Grounded Language
Understanding in a Collaborative Environment}.
</p>
<p>The primary goal of the competition is to approach the problem of how to
build interactive agents that learn to solve a task while provided with
grounded natural language instructions in a collaborative environment.
Understanding the complexity of the challenge, we split it into sub-tasks to
make it feasible for participants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Conversational Agents against Imperceptible Toxicity Triggers. (arXiv:2205.02392v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02392">
<div class="article-summary-box-inner">
<span><p>Warning: this paper contains content that maybe offensive or upsetting.
Recent research in Natural Language Processing (NLP) has advanced the
development of various toxicity detection models with the intention of
identifying and mitigating toxic language from existing systems. Despite the
abundance of research in this area, less attention has been given to
adversarial attacks that force the system to generate toxic language and the
defense against them. Existing work to generate such attacks is either based on
human-generated attacks which is costly and not scalable or, in case of
automatic attacks, the attack vector does not conform to human-like language,
which can be detected using a language model loss. In this work, we propose
attacks against conversational agents that are imperceptible, i.e., they fit
the conversation in terms of coherency, relevancy, and fluency, while they are
effective and scalable, i.e., they can automatically trigger the system into
generating toxic language. We then propose a defense mechanism against such
attacks which not only mitigates the attack but also attempts to maintain the
conversational flow. Through automatic and human evaluations, we show that our
defense is effective at avoiding toxic language generation even against
imperceptible toxicity triggers while the generated language fits the
conversation in terms of coherency and relevancy. Lastly, we establish the
generalizability of such a defense mechanism on language generation models
beyond conversational agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimising Equal Opportunity Fairness in Model Training. (arXiv:2205.02393v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02393">
<div class="article-summary-box-inner">
<span><p>Real-world datasets often encode stereotypes and societal biases. Such biases
can be implicitly captured by trained models, leading to biased predictions and
exacerbating existing societal preconceptions. Existing debiasing methods, such
as adversarial training and removing protected information from
representations, have been shown to reduce bias. However, a disconnect between
fairness criteria and training objectives makes it difficult to reason
theoretically about the effectiveness of different techniques. In this work, we
propose two novel training objectives which directly optimise for the
widely-used criterion of {\it equal opportunity}, and show that they are
effective in reducing bias while maintaining high performance over two
classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dangling-Aware Entity Alignment with Mixed High-Order Proximities. (arXiv:2205.02406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02406">
<div class="article-summary-box-inner">
<span><p>We study dangling-aware entity alignment in knowledge graphs (KGs), which is
an underexplored but important problem. As different KGs are naturally
constructed by different sets of entities, a KG commonly contains some dangling
entities that cannot find counterparts in other KGs. Therefore, dangling-aware
entity alignment is more realistic than the conventional entity alignment where
prior studies simply ignore dangling entities. We propose a framework using
mixed high-order proximities on dangling-aware entity alignment. Our framework
utilizes both the local high-order proximity in a nearest neighbor subgraph and
the global high-order proximity in an embedding space for both dangling
detection and entity alignment. Extensive experiments with two evaluation
settings shows that our framework more precisely detects dangling entities, and
better aligns matchable entities. Further investigations demonstrate that our
framework can mitigate the hubness problem on dangling-aware entity alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational Representation Learning in Visually-Rich Documents. (arXiv:2205.02411v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02411">
<div class="article-summary-box-inner">
<span><p>Relational understanding is critical for a number of visually-rich documents
(VRDs) understanding tasks. Through multi-modal pre-training, recent studies
provide comprehensive contextual representations and exploit them as prior
knowledge for downstream tasks. In spite of their impressive results, we
observe that the widespread relational hints (e.g., relation of key/value
fields on receipts) built upon contextual knowledge are not excavated yet. To
mitigate this gap, we propose DocReL, a Document Relational Representation
Learning framework. The major challenge of DocReL roots in the variety of
relations. From the simplest pairwise relation to the complex global structure,
it is infeasible to conduct supervised training due to the definition of
relation varies and even conflicts in different tasks. To deal with the
unpredictable definition of relations, we propose a novel contrastive learning
task named Relational Consistency Modeling (RCM), which harnesses the fact that
existing relations should be consistent in differently augmented positive
views. RCM provides relational representations which are more compatible to the
urgent need of downstream tasks, even without any knowledge about the exact
definition of relation. DocReL achieves better performance on a wide variety of
VRD relational understanding tasks, including table structure recognition, key
information extraction and reading order detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Contrastive Learning for Speech Translation. (arXiv:2205.02444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02444">
<div class="article-summary-box-inner">
<span><p>How can we learn unified representations for spoken utterances and their
written text? Learning similar representations for semantically similar speech
and text is important for speech translation. To this end, we propose ConST, a
cross-modal contrastive learning method for end-to-end speech-to-text
translation. We evaluate ConST and a variety of previous baselines on a popular
benchmark MuST-C. Experiments show that the proposed ConST consistently
outperforms the previous methods on, and achieves an average BLEU of 29.4. The
analysis further verifies that ConST indeed closes the representation gap of
different modalities -- its learned representation improves the accuracy of
cross-modal speech-text retrieval from 4% to 88%. Code and models are available
at https://github.com/ReneeYe/ConST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assistive Recipe Editing through Critiquing. (arXiv:2205.02454v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02454">
<div class="article-summary-box-inner">
<span><p>There has recently been growing interest in the automatic generation of
cooking recipes that satisfy some form of dietary restrictions, thanks in part
to the availability of online recipe data. Prior studies have used pre-trained
language models, or relied on small paired recipe data (e.g., a recipe paired
with a similar one that satisfies a dietary constraint). However, pre-trained
language models generate inconsistent or incoherent recipes, and paired
datasets are not available at scale. We address these deficiencies with
RecipeCrit, a hierarchical denoising auto-encoder that edits recipes given
ingredient-level critiques. The model is trained for recipe completion to learn
semantic relationships within recipes. Our work's main innovation is our
unsupervised critiquing module that allows users to edit recipes by interacting
with the predicted ingredients; the system iteratively rewrites recipes to
satisfy users' feedback. Experiments on the Recipe1M recipe dataset show that
our model can more effectively edit recipes compared to strong
language-modeling baselines, creating recipes that satisfy user constraints and
are more correct, serendipitous, coherent, and relevant as measured by human
judges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COGMEN: COntextualized GNN based Multimodal Emotion recognitioN. (arXiv:2205.02455v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02455">
<div class="article-summary-box-inner">
<span><p>Emotions are an inherent part of human interactions, and consequently, it is
imperative to develop AI systems that understand and recognize human emotions.
During a conversation involving various people, a person's emotions are
influenced by the other speaker's utterances and their own emotional state over
the utterances. In this paper, we propose COntextualized Graph Neural Network
based Multimodal Emotion recognitioN (COGMEN) system that leverages local
information (i.e., inter/intra dependency between speakers) and global
information (context). The proposed model uses Graph Neural Network (GNN) based
architecture to model the complex dependencies (local and global information)
in a conversation. Our model gives state-of-the-art (SOTA) results on IEMOCAP
and MOSEI datasets, and detailed ablation experiments show the importance of
modeling information at both levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog. (arXiv:2205.02471v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02471">
<div class="article-summary-box-inner">
<span><p>A typical end-to-end task-oriented dialog system transfers context into
dialog state, and upon which generates a response, which usually faces the
problem of error propagation from both previously generated inaccurate dialog
states and responses, especially in low-resource scenarios. To alleviate these
issues, we propose BORT, a back and denoising reconstruction approach for
end-to-end task-oriented dialog system. Squarely, to improve the accuracy of
dialog states, back reconstruction is used to reconstruct the original input
context from the generated dialog states since inaccurate dialog states cannot
recover the corresponding input context. To enhance the denoising capability of
the model to reduce the impact of error propagation, denoising reconstruction
is used to reconstruct the corrupted dialog state and response. Extensive
experiments conducted on MultiWOZ 2.0 and CamRest676 show the effectiveness of
BORT. Furthermore, BORT demonstrates its advanced capabilities in the zero-shot
domain and low-resource scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker Recognition in the Wild. (arXiv:2205.02475v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02475">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a pipeline to find the number of speakers, as well
as audios belonging to each of these now identified speakers in a source of
audio data where number of speakers or speaker labels are not known a priori.
We used this approach as a part of our Data Preparation pipeline for Speech
Recognition in Indic Languages
(https://github.com/Open-Speech-EkStep/vakyansh-wav2vec2-experimentation). To
understand and evaluate the accuracy of our proposed pipeline, we introduce two
metrics: Cluster Purity, and Cluster Uniqueness. Cluster Purity quantifies how
"pure" a cluster is. Cluster Uniqueness, on the other hand, quantifies what
percentage of clusters belong only to a single dominant speaker. We discuss
more on these metrics in section \ref{sec:metrics}. Since we develop this
utility to aid us in identifying data based on speaker IDs before training an
Automatic Speech Recognition (ASR) model, and since most of this data takes
considerable effort to scrape, we also conclude that 98\% of data gets mapped
to the top 80\% of clusters (computed by removing any clusters with less than a
fixed number of utterances -- we do this to get rid of some very small clusters
and use this threshold as 30), in the test set chosen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastRE: Towards Fast Relation Extraction with Convolutional Encoder and Improved Cascade Binary Tagging Framework. (arXiv:2205.02490v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02490">
<div class="article-summary-box-inner">
<span><p>Recent work for extracting relations from texts has achieved excellent
performance. However, most existing methods pay less attention to the
efficiency, making it still challenging to quickly extract relations from
massive or streaming text data in realistic scenarios. The main efficiency
bottleneck is that these methods use a Transformer-based pre-trained language
model for encoding, which heavily affects the training speed and inference
speed. To address this issue, we propose a fast relation extraction model
(FastRE) based on convolutional encoder and improved cascade binary tagging
framework. Compared to previous work, FastRE employs several innovations to
improve efficiency while also keeping promising performance. Concretely, FastRE
adopts a novel convolutional encoder architecture combined with dilated
convolution, gated unit and residual connection, which significantly reduces
the computation cost of training and inference, while maintaining the
satisfactory performance. Moreover, to improve the cascade binary tagging
framework, FastRE first introduces a type-relation mapping mechanism to
accelerate tagging efficiency and alleviate relation redundancy, and then
utilizes a position-dependent adaptive thresholding strategy to obtain higher
tagging accuracy and better model generalization. Experimental results
demonstrate that FastRE is well balanced between efficiency and performance,
and achieves 3-10x training speed, 7-15x inference speed faster, and 1/100
parameters compared to the state-of-the-art models, while the performance is
still competitive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration. (arXiv:2205.02517v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02517">
<div class="article-summary-box-inner">
<span><p>The cross-entropy objective has proved to be an all-purpose training
objective for autoregressive language models (LMs). However, without
considering the penalization of problematic tokens, LMs trained using
cross-entropy exhibit text degeneration. To address this, unlikelihood training
has been proposed to force unlikely tokens to be assigned a low probability by
a LM. But unlikelihood does not consider the relationship between the label
tokens and the unlikely token candidates, thus showing marginal improvements in
degeneration. We propose a new contrastive token learning objective that
inherits the advantages of cross-entropy and unlikelihood training and avoids
their limitations. The key idea is to force a LM to generate high probabilities
for label tokens at each step while low probabilities of negative candidates.
Comprehensive experiments on language modeling and open-domain dialogue
generation tasks show that the proposed contrastive token objective yields less
repetitive texts, with a higher generation quality than unlikelihood training,
achieving the new state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theories of "Gender" in NLP Bias Research. (arXiv:2205.02526v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02526">
<div class="article-summary-box-inner">
<span><p>The rise of concern around Natural Language Processing (NLP) technologies
containing and perpetuating social biases has led to a rich and rapidly growing
area of research. Gender bias is one of the central biases being analyzed, but
to date there is no comprehensive analysis of how "gender" is theorized in the
field. We survey nearly 200 articles concerning gender bias in NLP to discover
how the field conceptualizes gender both explicitly (e.g. through definitions
of terms) and implicitly (e.g. through how gender is operationalized in
practice). In order to get a better idea of emerging trajectories of thought,
we split these articles into two sections by time.
</p>
<p>We find that the majority of the articles do not make their theorization of
gender explicit, even if they clearly define "bias." Almost none use a model of
gender that is intersectional or inclusive of nonbinary genders; and many
conflate sex characteristics, social gender, and linguistic gender in ways that
disregard the existence and experience of trans, nonbinary, and intersex
people. There is an increase between the two time-sections in statements
acknowledging that gender is a complicated reality, however, very few articles
manage to put this acknowledgment into practice. In addition to analyzing these
findings, we provide specific recommendations to facilitate interdisciplinary
work, and to incorporate theory and methodology from Gender Studies. Our hope
is that this will produce more inclusive gender bias research in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing the Welsh Text Summarisation Dataset and Baseline Systems. (arXiv:2205.02545v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02545">
<div class="article-summary-box-inner">
<span><p>Welsh is an official language in Wales and is spoken by an estimated 884,300
people (29.2% of the population of Wales). Despite this status and estimated
increase in speaker numbers since the last (2011) census, Welsh remains a
minority language undergoing revitalization and promotion by Welsh Government
and relevant stakeholders. As part of the effort to increase the availability
of Welsh digital technology, this paper introduces the first Welsh
summarisation dataset, which we provide freely for research purposes to help
advance the work on Welsh text summarization. The dataset was created by Welsh
speakers by manually summarising Welsh Wikipedia articles. In addition, the
paper discusses the implementation and evaluation of different summarisation
systems for Welsh. The summarization systems and results will serve as
benchmarks for the development of summarises in other minority language
contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">User-Driven Research of Medical Note Generation Software. (arXiv:2205.02549v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02549">
<div class="article-summary-box-inner">
<span><p>A growing body of work uses Natural Language Processing (NLP) methods to
automatically generate medical notes from audio recordings of doctor-patient
consultations. However, there are very few studies on how such systems could be
used in clinical practice, how clinicians would adjust to using them, or how
system design should be influenced by such considerations. In this paper, we
present three rounds of user studies, carried out in the context of developing
a medical note generation system. We present, analyse and discuss the
participating clinicians' impressions and views of how the system ought to be
adapted to be of value to them. Next, we describe a three-week test run of the
system in a live telehealth clinical practice, major findings from which
include (i) the emergence of five different note-taking behaviours; (ii) the
importance of the system generating notes in real time during the consultation;
and (iii) the identification of a number of clinical use cases that could prove
challenging for automatic note generation systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LUNA: Learning Slot-Turn Alignment for Dialogue State Tracking. (arXiv:2205.02550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02550">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking (DST) aims to predict the current dialogue state
given the dialogue history. Existing methods generally exploit the utterances
of all dialogue turns to assign value for each slot. This could lead to
suboptimal results due to the information introduced from irrelevant utterances
in the dialogue history, which may be useless and can even cause confusion. To
address this problem, we propose LUNA, a sLot-tUrN Alignment enhanced approach.
It first explicitly aligns each slot with its most relevant utterance, then
further predicts the corresponding value based on this aligned utterance
instead of all dialogue utterances. Furthermore, we design a slot ranking
auxiliary task to learn the temporal correlation among slots which could
facilitate the alignment. Comprehensive experiments are conducted on
multi-domain task-oriented dialogue datasets, i.e., MultiWOZ 2.0, MultiWOZ 2.1,
and MultiWOZ 2.2. The results show that LUNA achieves new state-of-the-art
results on these datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Size Does Not Fit All: The Case for Personalised Word Complexity Models. (arXiv:2205.02564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02564">
<div class="article-summary-box-inner">
<span><p>Complex Word Identification (CWI) aims to detect words within a text that a
reader may find difficult to understand. It has been shown that CWI systems can
improve text simplification, readability prediction and vocabulary acquisition
modelling. However, the difficulty of a word is a highly idiosyncratic notion
that depends on a reader's first language, proficiency and reading experience.
In this paper, we show that personal models are best when predicting word
complexity for individual readers. We use a novel active learning framework
that allows models to be tailored to individuals and release a dataset of
complexity annotations and models as a benchmark for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Multi-Domain Corpora Learning for Open-Domain Response Generation. (arXiv:2205.02570v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02570">
<div class="article-summary-box-inner">
<span><p>Open-domain conversational systems are assumed to generate equally good
responses on multiple domains. Previous work achieved good performance on the
single corpus, but training and evaluating on multiple corpora from different
domains are less studied. This paper explores methods of generating relevant
responses for each of multiple multi-domain corpora. We first examine
interleaved learning which intermingles multiple corpora as the baseline. We
then investigate two multi-domain learning methods, labeled learning and
multi-task labeled learning, which encode each corpus through a unique corpus
embedding. Furthermore, we propose Domain-specific Frequency (DF), a novel
word-level importance weight that measures the relative importance of a word
for a specific corpus compared to other corpora. Based on DF, we propose
weighted learning, a method that integrates DF to the loss function. We also
adopt DF as a new evaluation metric. Extensive experiments show that our
methods gain significant improvements on both automatic and human evaluation.
We share our code and data for reproducibility
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">METGEN: A Module-Based Entailment Tree Generation Framework for Answer Explanation. (arXiv:2205.02593v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02593">
<div class="article-summary-box-inner">
<span><p>Knowing the reasoning chains from knowledge to the predicted answers can help
construct an explainable question answering (QA) system. Advances on QA
explanation propose to explain the answers with entailment trees composed of
multiple entailment steps. While current work proposes to generate entailment
trees with end-to-end generative models, the steps in the generated trees are
not constrained and could be unreliable. In this paper, we propose METGEN, a
Module-based Entailment Tree GENeration framework that has multiple modules and
a reasoning controller. Given a question and several supporting knowledge,
METGEN can iteratively generate the entailment tree by conducting single-step
entailment with separate modules and selecting the reasoning flow with the
controller. As each module is guided to perform a specific type of entailment
reasoning, the steps generated by METGEN are more reliable and valid.
Experiment results on the standard benchmark show that METGEN can outperform
previous state-of-the-art models with only 9% of the parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Inference with Self-Attention for Veracity Assessment of Pandemic Claims. (arXiv:2205.02596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02596">
<div class="article-summary-box-inner">
<span><p>We present a comprehensive work on automated veracity assessment from dataset
creation to developing novel methods based on Natural Language Inference (NLI),
focusing on misinformation related to the COVID-19 pandemic. We first describe
the construction of the novel PANACEA dataset consisting of heterogeneous
claims on COVID-19 and their respective information sources. The dataset
construction includes work on retrieval techniques and similarity measurements
to ensure a unique set of claims. We then propose novel techniques for
automated veracity assessment based on Natural Language Inference including
graph convolutional networks and attention based approaches. We have carried
out experiments on evidence retrieval and veracity assessment on the dataset
using the proposed techniques and found them competitive with SOTA methods, and
provided a detailed discussion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Global and Local Hierarchies for Hierarchical Text Classification. (arXiv:2205.02613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02613">
<div class="article-summary-box-inner">
<span><p>Hierarchical text classification aims to leverage label hierarchy in
multi-label text classification. Existing methods encode label hierarchy in a
global view, where label hierarchy is treated as the static hierarchical
structure containing all labels. Since global hierarchy is static and
irrelevant to text samples, it makes these methods hard to exploit hierarchical
information. Contrary to global hierarchy, local hierarchy as the structured
target labels hierarchy corresponding to each text sample is dynamic and
relevant to text samples, which is ignored in previous methods. To exploit
global and local hierarchies, we propose Hierarchy-guided BERT with Global and
Local hierarchies (HBGL), which utilizes the large-scale parameters and prior
language knowledge of BERT to model both global and local hierarchies.
Moreover, HBGL avoids the intentional fusion of semantic and hierarchical
modules by directly modeling semantic and hierarchical information with BERT.
Compared with the state-of-the-art method HGCLR, our method achieves
significant improvement on three benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WDV: A Broad Data Verbalisation Dataset Built from Wikidata. (arXiv:2205.02627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02627">
<div class="article-summary-box-inner">
<span><p>Data verbalisation is a task of great importance in the current field of
natural language processing, as there is great benefit in the transformation of
our abundant structured and semi-structured data into human-readable formats.
Verbalising Knowledge Graph (KG) data focuses on converting interconnected
triple-based claims, formed of subject, predicate, and object, into text.
Although KG verbalisation datasets exist for some KGs, there are still gaps in
their fitness for use in many scenarios. This is especially true for Wikidata,
where available datasets either loosely couple claim sets with textual
information or heavily focus on predicates around biographies, cities, and
countries. To address these gaps, we propose WDV, a large KG claim
verbalisation dataset built from Wikidata, with a tight coupling between
triples and text, covering a wide variety of entities and predicates. We also
evaluate the quality of our verbalisations through a reusable workflow for
measuring human-centred fluency and adequacy scores. Our data and code are
openly available in the hopes of furthering research towards KG verbalisation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient yet Competitive Speech Translation: FBK@IWSLT2022. (arXiv:2205.02629v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02629">
<div class="article-summary-box-inner">
<span><p>The primary goal of this FBK's systems submission to the IWSLT 2022 offline
and simultaneous speech translation tasks is to reduce model training costs
without sacrificing translation quality. As such, we first question the need of
ASR pre-training, showing that it is not essential to achieve competitive
results. Second, we focus on data filtering, showing that a simple method that
looks at the ratio between source and target characters yields a quality
improvement of 1 BLEU. Third, we compare different methods to reduce the
detrimental effect of the audio segmentation mismatch between training data
manually segmented at sentence level and inference data that is automatically
segmented. Towards the same goal of training cost reduction, we participate in
the simultaneous task with the same model trained for offline ST. The
effectiveness of our lightweight training strategy is shown by the high score
obtained on the MuST-C en-de corpus (26.7 BLEU) and is confirmed in
high-resource data conditions by a 1.6 BLEU improvement on the IWSLT2020 test
set over last year's winning system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Can See: Plugging Visual Controls in Text Generation. (arXiv:2205.02655v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02655">
<div class="article-summary-box-inner">
<span><p>Generative language models (LMs) such as GPT-2/3 can be prompted to generate
text with remarkable quality. While they are designed for text-prompted
generation, it remains an open question how the generation process could be
guided by modalities beyond text such as images. In this work, we propose a
training-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),
for plugging in visual controls in the generation process and enabling LMs to
perform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC
is a simple yet efficient plug-and-play framework, which directly combines an
off-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)
for image-grounded text generation. During decoding, MAGIC influences the
generation of the LM by introducing a CLIP-induced score, called magic score,
which regularizes the generated result to be semantically related to a given
image while being coherent to the previously generated context. Notably, the
proposed decoding scheme does not involve any gradient update operation,
therefore being computationally efficient. On the challenging task of zero-shot
image captioning, MAGIC outperforms the state-of-the-art method by notable
margins with a nearly 27 times decoding speedup. MAGIC is a flexible framework
and is theoretically compatible with any text generation tasks that incorporate
image grounding. In the experiments, we showcase that it is also capable of
performing visually grounded story generation given both an image and a text
prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RaFoLa: A Rationale-Annotated Corpus for Detecting Indicators of Forced Labour. (arXiv:2205.02684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02684">
<div class="article-summary-box-inner">
<span><p>Forced labour is the most common type of modern slavery, and it is
increasingly gaining the attention of the research and social community. Recent
studies suggest that artificial intelligence (AI) holds immense potential for
augmenting anti-slavery action. However, AI tools need to be developed
transparently in cooperation with different stakeholders. Such tools are
contingent on the availability and access to domain-specific data, which are
scarce due to the near-invisible nature of forced labour. To the best of our
knowledge, this paper presents the first openly accessible English corpus
annotated for multi-class and multi-label forced labour detection. The corpus
consists of 989 news articles retrieved from specialised data sources and
annotated according to risk indicators defined by the International Labour
Organization (ILO). Each news article was annotated for two aspects: (1)
indicators of forced labour as classification labels and (2) snippets of the
text that justify labelling decisions. We hope that our data set can help
promote research on explainability for multi-class and multi-label text
classification. In this work, we explain our process for collecting the data
underpinning the proposed corpus, describe our annotation guidelines and
present some statistical analysis of its content. Finally, we summarise the
results of baseline experiments based on different variants of the
Bidirectional Encoder Representation from Transformer (BERT) model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Language Variation Acoustically with Few Resources. (arXiv:2205.02694v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02694">
<div class="article-summary-box-inner">
<span><p>Deep acoustic models represent linguistic information based on massive
amounts of data. Unfortunately, for regional languages and dialects such
resources are mostly not available. However, deep acoustic models might have
learned linguistic information that transfers to low-resource languages. In
this study, we evaluate whether this is the case through the task of
distinguishing low-resource (Dutch) regional varieties. By extracting
embeddings from the hidden layers of various wav2vec 2.0 models (including new
models which are pre-trained and/or fine-tuned on Dutch) and using dynamic time
warping, we compute pairwise pronunciation differences averaged over 10 words
for over 100 individual dialects from four (regional) languages. We then
cluster the resulting difference matrix in four groups and compare these to a
gold standard, and a partitioning on the basis of comparing phonetic
transcriptions. Our results show that acoustic models outperform the
(traditional) transcription-based approach without requiring phonetic
transcriptions, with the best performance achieved by the multilingual XLSR-53
model fine-tuned on Dutch. On the basis of only six seconds of speech, the
resulting clustering closely matches the gold standard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit N-grams Induced by Recurrence. (arXiv:2205.02724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02724">
<div class="article-summary-box-inner">
<span><p>Although self-attention based models such as Transformers have achieved
remarkable successes on natural language processing (NLP) tasks, recent studies
reveal that they have limitations on modeling sequential transformations (Hahn,
2020), which may prompt re-examinations of recurrent neural networks (RNNs)
that demonstrated impressive results on handling sequential data. Despite many
prior attempts to interpret RNNs, their internal mechanisms have not been fully
understood, and the question on how exactly they capture sequential features
remains largely unclear. In this work, we present a study that shows there
actually exist some explainable components that reside within the hidden
states, which are reminiscent of the classical n-grams features. We evaluated
such extracted explainable features from trained RNNs on downstream sentiment
analysis tasks and found they could be used to model interesting linguistic
phenomena such as negation and intensification. Furthermore, we examined the
efficacy of using such n-gram components alone as encoders on tasks such as
sentiment analysis and language modeling, revealing they could be playing
important roles in contributing to the overall performance of RNNs. We hope our
findings could add interpretability to RNN architectures, and also provide
inspirations for proposing new architectures for sequential data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CATs are Fuzzy PETs: A Corpus and Analysis of Potentially Euphemistic Terms. (arXiv:2205.02728v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02728">
<div class="article-summary-box-inner">
<span><p>Euphemisms have not received much attention in natural language processing,
despite being an important element of polite and figurative language.
Euphemisms prove to be a difficult topic, not only because they are subject to
language change, but also because humans may not agree on what is a euphemism
and what is not. Nevertheless, the first step to tackling the issue is to
collect and analyze examples of euphemisms. We present a corpus of potentially
euphemistic terms (PETs) along with example texts from the GloWbE corpus.
Additionally, we present a subcorpus of texts where these PETs are not being
used euphemistically, which may be useful for future applications. We also
discuss the results of multiple analyses run on the corpus. Firstly, we find
that sentiment analysis on the euphemistic texts supports that PETs generally
decrease negative and offensive sentiment. Secondly, we observe cases of
disagreement in an annotation task, where humans are asked to label PETs as
euphemistic or not in a subset of our corpus text examples. We attribute the
disagreement to a variety of potential reasons, including if the PET was a
commonly accepted term (CAT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diversifying Neural Dialogue Generation via Negative Distillation. (arXiv:2205.02795v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02795">
<div class="article-summary-box-inner">
<span><p>Generative dialogue models suffer badly from the generic response problem,
limiting their applications to a few toy scenarios. Recently, an interesting
approach, namely negative training, has been proposed to alleviate this problem
by reminding the model not to generate high-frequency responses during
training. However, its performance is hindered by two issues, ignoring
low-frequency but generic responses and bringing low-frequency but meaningless
responses. In this paper, we propose a novel negative training paradigm, called
negative distillation, to keep the model away from the undesirable generic
responses while avoiding the above problems. First, we introduce a negative
teacher model that can produce query-wise generic responses, and then the
student model is required to maximize the distance with multi-level negative
knowledge. Empirical results show that our method outperforms previous negative
training methods significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Cloze By Date: What LMs Know About Unseen Entities. (arXiv:2205.02832v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02832">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are typically trained once on a large-scale corpus and
used for years without being updated. However, in a dynamic world, new entities
constantly arise. We propose a framework to analyze what LMs can infer about
new entities that did not exist when the LMs were pretrained. We derive a
dataset of entities indexed by their origination date and paired with their
English Wikipedia articles, from which we can find sentences about each entity.
We evaluate LMs' perplexity on masked spans within these sentences. We show
that models more informed about the entities, such as those with access to a
textual definition of them, achieve lower perplexity on this benchmark. Our
experimental results demonstrate that making inferences about new entities
remains difficult for LMs. Given its wide coverage on entity knowledge and
temporal indexing, our dataset can be used to evaluate LMs and techniques
designed to modify or extend their knowledge. Our automatic data collection
pipeline can be easily used to continually update our benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SueNes: A Weakly Supervised Approach to Evaluating Single-Document Summarization via Negative Sampling. (arXiv:2005.06377v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.06377">
<div class="article-summary-box-inner">
<span><p>Canonical automatic summary evaluation metrics, such as ROUGE, focus on
lexical similarity which cannot well capture semantics nor linguistic quality
and require a reference summary which is costly to obtain. Recently, there have
been a growing number of efforts to alleviate either or both of the two
drawbacks. In this paper, we present a proof-of-concept study to a weakly
supervised summary evaluation approach without the presence of reference
summaries. Massive data in existing summarization datasets are transformed for
training by pairing documents with corrupted reference summaries. In
cross-domain tests, our strategy outperforms baselines with promising
improvements, and show a great advantage in gauging linguistic qualities over
all metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Emotion Detection. (arXiv:2106.06017v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06017">
<div class="article-summary-box-inner">
<span><p>Emotion detection can provide us with a window into understanding human
behavior. Due to the complex dynamics of human emotions, however, constructing
annotated datasets to train automated models can be expensive. Thus, we explore
the efficacy of cross-lingual approaches that would use data from a source
language to build models for emotion detection in a target language. We compare
three approaches, namely: i) using inherently multilingual models; ii)
translating training data into the target language; and iii) using an
automatically tagged parallel corpus. In our study, we consider English as the
source language with Arabic and Spanish as target languages. We study the
effectiveness of different classification models such as BERT and SVMs trained
with different features. Our BERT-based monolingual models that are trained on
target language data surpass state-of-the-art (SOTA) by 4% and 5% absolute
Jaccard score for Arabic and Spanish respectively. Next, we show that using
cross-lingual approaches with English data alone, we can achieve more than 90%
and 80% relative effectiveness of the Arabic and Spanish BERT models
respectively. Lastly, we use LIME to analyze the challenges of training
cross-lingual models for different language pairs
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15078">
<div class="article-summary-box-inner">
<span><p>Neural text generation models are typically trained by maximizing
log-likelihood with the sequence cross entropy (CE) loss, which encourages an
\emph{exact} token-by-token match between a target sequence with a generated
sequence. Such training objective is sub-optimal when the target sequence is
not perfect, e.g., when the target sequence is corrupted with noises, or when
only weak sequence supervision is available. To address the challenge, we
propose a novel Edit-Invariant Sequence Loss (EISL), which computes the
matching loss of a target n-gram with all n-grams in the generated sequence.
EISL is designed to be robust to various noises and edits in the target
sequences. Moreover, the EISL computation is essentially an approximate
convolution operation with target n-grams as kernels, which is easy to
implement and efficient to compute with existing libraries. To demonstrate the
effectiveness of EISL, we conduct experiments on a wide range of tasks,
including machine translation with noisy target sequences, unsupervised text
style transfer with only weak training signals, and non-autoregressive
generation with non-predefined generation order. Experimental results show our
method significantly outperforms the common CE loss and other strong baselines
on all the tasks. EISL has a simple API which can be used as a drop-in
replacement of the CE loss: https://anonymous.4open.science/r/EISLLoss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuSiQue: Multihop Questions via Single-hop Question Composition. (arXiv:2108.00573v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00573">
<div class="article-summary-box-inner">
<span><p>Multihop reasoning remains an elusive goal as existing multihop benchmarks
are known to be largely solvable via shortcuts. Can we create a question
answering (QA) dataset that, by construction, \emph{requires} proper multihop
reasoning? To this end, we introduce a bottom-up approach that systematically
selects composable pairs of single-hop questions that are connected, i.e.,
where one reasoning step critically relies on information from another. This
bottom-up methodology lets us explore a vast space of questions and add
stringent filters as well as other mechanisms targeting connected reasoning. It
provides fine-grained control over the construction process and the properties
of the resulting $k$-hop questions. We use this methodology to create
MuSiQue-Ans, a new multihop QA dataset with 25K 2-4 hop questions. Relative to
existing datasets, MuSiQue-Ans is more difficult overall (3x increase in
human-machine gap), and harder to cheat via disconnected reasoning (e.g., a
single-hop model has a 30 point drop in F1). We further add unanswerable
contrast questions to produce a more stringent dataset, MuSiQue-Full. We hope
our datasets will help the NLP community develop models that perform genuine
multihop reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13161">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have contributed significantly to
natural language processing by demonstrating remarkable abilities as few-shot
learners. However, their effectiveness depends mainly on scaling the model
parameters and prompt design, hindering their implementation in most real-world
applications. This study proposes a novel pluggable, extensible, and efficient
approach named DifferentiAble pRompT (DART), which can convert small language
models into better few-shot learners without any prompt engineering. The main
principle behind this approach involves reformulating potential natural
language processing tasks into the task of a pre-trained language model and
differentially optimizing the prompt template as well as the target label with
backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any
pre-trained language models; (ii) Extended to widespread classification tasks.
A comprehensive evaluation of standard NLP tasks demonstrates that the proposed
approach achieves a better few-shot performance. Code is available in
https://github.com/zjunlp/DART.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs. (arXiv:2109.04223v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04223">
<div class="article-summary-box-inner">
<span><p>Incorporating factual knowledge into pre-trained language models (PLM) such
as BERT is an emerging trend in recent NLP studies. However, most of the
existing methods combine the external knowledge integration module with a
modified pre-training loss and re-implement the pre-training process on the
large-scale corpus. Re-pretraining these models is usually resource-consuming,
and difficult to adapt to another domain with a different knowledge graph (KG).
Besides, those works either cannot embed knowledge context dynamically
according to textual context or struggle with the knowledge ambiguity issue. In
this paper, we propose a novel knowledge-aware language model framework based
on fine-tuning process, which equips PLM with a unified knowledge-enhanced text
graph that contains both text and multi-relational sub-graphs extracted from
KG. We design a hierarchical relational-graph-based message passing mechanism,
which can allow the representations of injected KG and text to mutually update
each other and can dynamically select ambiguous mentioned entities that share
the same text. Our empirical results show that our model can efficiently
incorporate world knowledge from KGs into existing language models such as
BERT, and achieve significant improvement on the machine reading comprehension
(MRC) task compared with other knowledge-enhanced models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures. (arXiv:2110.07518v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07518">
<div class="article-summary-box-inner">
<span><p>Current open-domain conversational models can easily be made to talk in
inadequate ways. Online learning from conversational feedback given by the
conversation partner is a promising avenue for a model to improve and adapt, so
as to generate fewer of these safety failures. However, current
state-of-the-art models tend to react to feedback with defensive or oblivious
responses. This makes for an unpleasant experience and may discourage
conversation partners from giving feedback in the future. This work proposes
SaFeRDialogues, a task and dataset of graceful responses to conversational
feedback about safety failures. We collect a dataset of 10k dialogues
demonstrating safety failures, feedback signaling them, and a response
acknowledging the feedback. We show how fine-tuning on this dataset results in
conversations that human raters deem considerably more likely to lead to a
civil conversation, without sacrificing engagingness or general conversational
ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02080">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) such as GPT-3 have the surprising ability to do
in-context learning, where the model learns to do a downstream task simply by
conditioning on a prompt consisting of input-output examples. The LM learns
from these examples without being explicitly pretrained to learn. Thus, it is
unclear what enables in-context learning. In this paper, we study how
in-context learning can emerge when pretraining documents have long-range
coherence. Here, the LM must infer a latent document-level concept to generate
coherent next tokens during pretraining. At test time, in-context learning
occurs when the LM also infers a shared latent concept between examples in a
prompt. We prove when this occurs despite a distribution mismatch between
prompts and pretraining data in a setting where the pretraining distribution is
a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs
capable of in-context learning, we generate a small-scale synthetic dataset
(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond
the theory, experiments on GINC exhibit large-scale real-world phenomena
including improved in-context performance with model scaling (despite the same
pretraining loss), sensitivity to example order, and instances where zero-shot
is better than few-shot in-context learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Adaptability in Pre-trained Language Models with 500 Tasks. (arXiv:2112.03204v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03204">
<div class="article-summary-box-inner">
<span><p>When a neural language model (LM) is adapted to perform a new task, what
aspects of the task predict the eventual performance of the model? In NLP,
systematic features of LM generalization to individual examples are well
characterized, but systematic aspects of LM adaptability to new tasks are not
nearly as well understood. We present a large-scale empirical study of the
features and limits of LM adaptability using a new benchmark, TaskBench500,
built from 500 procedurally generated sequence modeling tasks. These tasks
combine core aspects of language processing, including lexical semantics,
sequence processing, memorization, logical reasoning, and world knowledge.
Using TaskBench500, we evaluate three facets of adaptability, finding that: (1)
adaptation procedures differ dramatically in their ability to memorize small
datasets; (2) within a subset of task types, adaptation procedures exhibit
compositional adaptability to complex tasks; and (3) failure to match training
label distributions is explained by mismatches in the intrinsic difficulty of
predicting individual labels. Our experiments show that adaptability to new
tasks, like generalization to new examples, can be systematically described and
understood, and we conclude with a discussion of additional aspects of
adaptability that could be studied using the new benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention. (arXiv:2112.03254v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03254">
<div class="article-summary-box-inner">
<span><p>Most of today's AI systems focus on using self-attention mechanisms and
transformer architectures on large amounts of diverse data to achieve
impressive performance gains. In this paper, we propose to augment the
transformer architecture with an external attention mechanism to bring external
knowledge and context to bear. By integrating external information into the
prediction process, we hope to reduce the need for ever-larger models and
increase the democratization of AI systems. We find that the proposed external
attention mechanism can significantly improve the performance of existing AI
systems, allowing practitioners to easily customize foundation AI models to
many diverse downstream applications. In particular, we focus on the task of
Commonsense Reasoning, demonstrating that the proposed external attention
mechanism can augment existing transformer models and significantly improve the
model's reasoning capabilities. The proposed system, Knowledgeable External
Attention for commonsense Reasoning (KEAR), reaches human parity on the open
CommonsenseQA research benchmark with an accuracy of 89.4\% in comparison to
the human accuracy of 88.9\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Compositional Generalization with Latent Structure and Data Augmentation. (arXiv:2112.07610v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07610">
<div class="article-summary-box-inner">
<span><p>Generic unstructured neural networks have been shown to struggle on
out-of-distribution compositional generalization. Compositional data
augmentation via example recombination has transferred some prior knowledge
about compositionality to such black-box neural models for several semantic
parsing tasks, but this often required task-specific engineering or provided
limited gains.
</p>
<p>We present a more powerful data recombination method using a model called
Compositional Structure Learner (CSL). CSL is a generative model with a
quasi-synchronous context-free grammar backbone, which we induce from the
training data. We sample recombined examples from CSL and add them to the
fine-tuning data of a pre-trained sequence-to-sequence model (T5). This
procedure effectively transfers most of CSL's compositional bias to T5 for
diagnostic tasks, and results in a model even stronger than a T5-CSL ensemble
on two real world compositional generalization tasks. This results in new
state-of-the-art performance for these challenging semantic parsing tasks
requiring generalization to both natural language variation and novel
compositions of elements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textless Speech-to-Speech Translation on Real Data. (arXiv:2112.08352v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08352">
<div class="article-summary-box-inner">
<span><p>We present a textless speech-to-speech translation (S2ST) system that can
translate speech from one language into another language and can be built
without the need of any text data. Different from existing work in the
literature, we tackle the challenge in modeling multi-speaker target speech and
train the systems with real-world S2ST data. The key to our approach is a
self-supervised unit-based speech normalization technique, which finetunes a
pre-trained speech encoder with paired audios from multiple speakers and a
single reference speaker to reduce the variations due to accents, while
preserving the lexical content. With only 10 minutes of paired data for speech
normalization, we obtain on average 3.2 BLEU gain when training the S2ST model
on the VoxPopuli S2ST dataset, compared to a baseline trained on un-normalized
speech target. We also incorporate automatically mined S2ST data and show an
additional 2.0 BLEU gain. To our knowledge, we are the first to establish a
textless S2ST technique that can be trained with real-world data and works for
multiple language pairs. Audio samples are available at
https://facebookresearch.github.io/speech_translation/textless_s2st_real_data/index.html .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KAT: A Knowledge Augmented Transformer for Vision-and-Language. (arXiv:2112.08614v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08614">
<div class="article-summary-box-inner">
<span><p>The primary focus of recent work with largescale transformers has been on
optimizing the amount of information packed into the model's parameters. In
this work, we ask a different question: Can multimodal transformers leverage
explicit knowledge in their reasoning? Existing, primarily unimodal, methods
have explored approaches under the paradigm of knowledge retrieval followed by
answer prediction, but leave open questions about the quality and relevance of
the retrieved knowledge used, and how the reasoning processes over implicit and
explicit knowledge should be integrated. To address these challenges, we
propose a novel model - Knowledge Augmented Transformer (KAT) - which achieves
a strong state-of-the-art result (+6 points absolute) on the open-domain
multimodal task of OK-VQA. Our approach integrates implicit and explicit
knowledge in an end to end encoder-decoder architecture, while still jointly
reasoning over both knowledge sources during answer generation. An additional
benefit of explicit knowledge integration is seen in improved interpretability
of model predictions in our analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DREAM: Improving Situational QA by First Elaborating the Situation. (arXiv:2112.08656v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08656">
<div class="article-summary-box-inner">
<span><p>When people answer questions about a specific situation, e.g., "I cheated on
my mid-term exam last week. Was that wrong?", cognitive science suggests that
they form a mental picture of that situation before answering. While we do not
know how language models (LMs) answer such questions, we conjecture that they
may answer more accurately if they are also provided with additional details
about the question situation, elaborating the "scene". To test this conjecture,
we train a new model, DREAM, to answer questions that elaborate the scenes that
situated questions are about, and then provide those elaborations as additional
context to a question-answering (QA) model. We find that DREAM is able to
create better scene elaborations (more accurate, useful, and consistent) than a
representative state-of-the-art, zero-shot model (Macaw). We also find that
using the scene elaborations as additional context improves the answer accuracy
of a downstream QA system, including beyond that obtainable by simply further
finetuning the QA system on DREAM's training data. These results suggest that
adding focused elaborations about a situation can improve a system's reasoning
about it, and may serve as an effective way of injecting new scenario based
knowledge into QA models. Finally, our approach is dataset-neutral; we observe
improved QA performance across different models, with even bigger gains on
models with fewer parameters. We make our dataset and model publicly available
at https://github.com/allenai/dream.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reframing Human-AI Collaboration for Generating Free-Text Explanations. (arXiv:2112.08674v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08674">
<div class="article-summary-box-inner">
<span><p>Large language models are increasingly capable of generating fluent-appearing
text with relatively little task-specific supervision. But can these models
accurately explain classification decisions? We consider the task of generating
free-text explanations using human-written examples in a few-shot manner. We
find that (1) authoring higher quality prompts results in higher quality
generations; and (2) surprisingly, in a head-to-head comparison, crowdworkers
often prefer explanations generated by GPT-3 to crowdsourced explanations in
existing datasets. Our human studies also show, however, that while models
often produce factual, grammatical, and sufficient explanations, they have room
to improve along axes such as providing novel information and supporting the
label. We create a pipeline that combines GPT-3 with a supervised filter that
incorporates binary acceptability judgments from humans in the loop. Despite
the intrinsic subjectivity of acceptability judgments, we demonstrate that
acceptability is partially correlated with various fine-grained attributes of
explanations. Our approach is able to consistently filter GPT-3-generated
explanations deemed acceptable by humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DOCmT5: Document-Level Pretraining of Multilingual Language Models. (arXiv:2112.08709v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08709">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce DOCmT5, a multilingual sequence-to-sequence
language model pretrained with large scale parallel documents. While previous
approaches have focused on leveraging sentence-level parallel data, we try to
build a general-purpose pretrained model that can understand and generate long
documents. We propose a simple and effective pretraining objective - Document
reordering Machine Translation (DrMT), in which the input documents that are
shuffled and masked need to be translated. DrMT brings consistent improvements
over strong baselines on a variety of document-level generation tasks,
including over 12 BLEU points for seen-language-pair document-level MT, over 7
BLEU points for unseen-language-pair document-level MT and over 3 ROUGE-1
points for seen-language-pair cross-lingual summarization. We achieve
state-of-the-art (SOTA) on WMT20 De-En and IWSLT15 Zh-En document translation
tasks. We also conduct extensive analysis on various factors for document
pretraining, including (1) The effects of pretraining data quality and (2) The
effects of combining mono-lingual and cross-lingual pretraining. We plan to
make our model checkpoints publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v4 [q-bio.BM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11147">
<div class="article-summary-box-inner">
<span><p>Self-supervised protein language models have proved their effectiveness in
learning the proteins representations. With the increasing computational power,
current protein language models pre-trained with millions of diverse sequences
can advance the parameter scale from million-level to billion-level and achieve
remarkable improvement. However, those prevailing approaches rarely consider
incorporating knowledge graphs (KGs), which can provide rich structured
knowledge facts for better protein representations. We argue that informative
biology knowledge in KGs can enhance protein representation with external
knowledge. In this work, we propose OntoProtein, the first general framework
that makes use of structure in GO (Gene Ontology) into protein pre-training
models. We construct a novel large-scale knowledge graph that consists of GO
and its related proteins, and gene annotation texts or protein sequences
describe all nodes in the graph. We propose novel contrastive learning with
knowledge-aware negative sampling to jointly optimize the knowledge graph and
protein embedding during pre-training. Experimental results show that
OntoProtein can surpass state-of-the-art methods with pre-trained protein
language models in TAPE benchmark and yield better performance compared with
baselines in protein-protein interaction and protein function prediction. Code
and datasets are available in https://github.com/zjunlp/OntoProtein.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FAMIE: A Fast Active Learning Framework for Multilingual Information Extraction. (arXiv:2202.08316v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08316">
<div class="article-summary-box-inner">
<span><p>This paper presents FAMIE, a comprehensive and efficient active learning (AL)
toolkit for multilingual information extraction. FAMIE is designed to address a
fundamental problem in existing AL frameworks where annotators need to wait for
a long time between annotation batches due to the time-consuming nature of
model training and data selection at each AL iteration. This hinders the
engagement, productivity, and efficiency of annotators. Based on the idea of
using a small proxy network for fast data selection, we introduce a novel
knowledge distillation mechanism to synchronize the proxy network with the main
large model (i.e., BERT-based) to ensure the appropriateness of the selected
annotation examples for the main model. Our AL framework can support multiple
languages. The experiments demonstrate the advantages of FAMIE in terms of
competitive performance and time efficiency for sequence labeling with AL. We
publicly release our code (\url{https://github.com/nlp-uoregon/famie}) and demo
website (\url{<a href="http://nlp.uoregon.edu:9000/">this http URL</a>}). A demo video for FAMIE is
provided at: \url{https://youtu.be/I2i8n_jAyrY}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13403">
<div class="article-summary-box-inner">
<span><p>Large datasets as required for deep learning of lip reading do not exist in
many languages. In this paper we present the dataset GLips (German Lips)
consisting of 250,000 publicly available videos of the faces of speakers of the
Hessian Parliament, which was processed for word-level lip reading using an
automatic pipeline. The format is similar to that of the English language LRW
(Lip Reading in the Wild) dataset, with each video encoding one word of
interest in a context of 1.16 seconds duration, which yields compatibility for
studying transfer learning between both datasets. By training a deep neural
network, we investigate whether lip reading has language-independent features,
so that datasets of different languages can be used to improve lip reading
models. We demonstrate learning from scratch and show that transfer learning
from LRW to GLips and vice versa improves learning speed and performance, in
particular for the validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17247">
<div class="article-summary-box-inner">
<span><p>Breakthroughs in transformer-based models have revolutionized not only the
NLP field, but also vision and multimodal systems. However, although
visualization and interpretability tools have become available for NLP models,
internal mechanisms of vision and multimodal transformers remain largely
opaque. With the success of these transformers, it is increasingly critical to
understand their inner workings, as unraveling these black-boxes will lead to
more capable and trustworthy models. To contribute to this quest, we propose
VL-InterpreT, which provides novel interactive visualizations for interpreting
the attentions and hidden representations in multimodal transformers.
VL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety
of statistics in attention heads throughout all layers for both vision and
language components, (2) visualizes cross-modal and intra-modal attentions
through easily readable heatmaps, and (3) plots the hidden representations of
vision and language tokens as they pass through the transformer layers. In this
paper, we demonstrate the functionalities of VL-InterpreT through the analysis
of KD-VLP, an end-to-end pretraining vision-language multimodal
transformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and
WebQA, two visual question answering benchmarks. Furthermore, we also present a
few interesting findings about multimodal transformer behaviors that were
learned through our tool.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation. (arXiv:2204.03958v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03958">
<div class="article-summary-box-inner">
<span><p>This paper introduces a model for incomplete utterance restoration (IUR).
Different from prior studies that only work on extraction or abstraction
datasets, we design a simple but effective model, working for both scenarios of
IUR. Our design simulates the nature of IUR, where omitted tokens from the
context contribute to restoration. From this, we construct a Picker that
identifies the omitted tokens. To support the picker, we design two label
creation methods (soft and hard labels), which can work in cases of no
annotation of the omitted tokens. The restoration is done by using a Generator
with the help of the Picker on joint learning. Promising results on four
benchmark datasets in extraction and abstraction scenarios show that our model
is better than the pretrained T5 and non-generative language model methods in
both rich and limited training data settings. The code will be also available
(https://github.com/shumpei19/jointiur).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tweet Emotion Dynamics: Emotion Word Usage in Tweets from US and Canada. (arXiv:2204.04862v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04862">
<div class="article-summary-box-inner">
<span><p>Over the last decade, Twitter has emerged as one of the most influential
forums for social, political, and health discourse. In this paper, we introduce
a massive dataset of more than 45 million geo-located tweets posted between
2015 and 2021 from US and Canada (TUSC), especially curated for natural
language analysis. We also introduce Tweet Emotion Dynamics (TED) -- metrics to
capture patterns of emotions associated with tweets over time. We use TED and
TUSC to explore the use of emotion-associated words across US and Canada;
across 2019 (pre-pandemic), 2020 (the year the pandemic hit), and 2021 (the
second year of the pandemic); and across individual tweeters. We show that
Canadian tweets tend to have higher valence, lower arousal, and higher
dominance than the US tweets. Further, we show that the COVID-19 pandemic had a
marked impact on the emotional signature of tweets posted in 2020, when
compared to the adjoining years. Finally, we determine metrics of TED for
170,000 tweeters to benchmark characteristics of TED metrics at an aggregate
level. TUSC and the metrics for TED will enable a wide variety of research on
studying how we use language to express ourselves, persuade, communicate, and
influence, with particularly promising applications in public health, affective
science, social science, and psychology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text Classification. (arXiv:2204.04952v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04952">
<div class="article-summary-box-inner">
<span><p>Text classification struggles to generalize to unseen classes with very few
labeled text instances per class. In such a few-shot learning (FSL) setting,
metric-based meta-learning approaches have shown promising results. Previous
studies mainly aim to derive a prototype representation for each class.
However, they neglect that it is challenging-yet-unnecessary to construct a
compact representation which expresses the entire meaning for each class. They
also ignore the importance to capture the inter-dependency between query and
the support set for few-shot text classification. To deal with these issues, we
propose a meta-learning based method MGIMN which performs instance-wise
comparison followed by aggregation to generate class-wise matching vectors
instead of prototype learning. The key of instance-wise comparison is the
interactive matching within the class-specific context and episode-specific
context. Extensive experiments demonstrate that the proposed method
significantly outperforms the existing state-of-the-art approaches, under both
the standard FSL and generalized FSL settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ML_LTU at SemEval-2022 Task 4: T5 Towards Identifying Patronizing and Condescending Language. (arXiv:2204.07432v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07432">
<div class="article-summary-box-inner">
<span><p>This paper describes the system used by the Machine Learning Group of LTU in
subtask 1 of the SemEval-2022 Task 4: Patronizing and Condescending Language
(PCL) Detection. Our system consists of finetuning a pretrained
Text-to-Text-Transfer Transformer (T5) and innovatively reducing its
out-of-class predictions. The main contributions of this paper are 1) the
description of the implementation details of the T5 model we used, 2) analysis
of the successes &amp; struggles of the model in this task, and 3) ablation studies
beyond the official submission to ascertain the relative importance of data
split. Our model achieves an F1 score of 0.5452 on the official test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection using generative-based and mutation-based data augmentation. (arXiv:2204.08198v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08198">
<div class="article-summary-box-inner">
<span><p>Sarcasm is a term that refers to the use of words to mock, irritate, or amuse
someone. It is commonly used on social media. The metaphorical and creative
nature of sarcasm presents a significant difficulty for sentiment analysis
systems based on affective computing. The methodology and results of our team,
UTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in
this paper. We put different models, and data augmentation approaches to the
test and report on which one works best. The tests begin with traditional
machine learning models and progress to transformer-based and attention-based
models. We employed data augmentation based on data mutation and data
generation. Using RoBERTa and mutation-based data augmentation, our best
approach achieved an F1-sarcastic of 0.38 in the competition's evaluation
phase. After the competition, we fixed our model's flaws and achieved an
F1-sarcastic of 0.414.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations. (arXiv:2204.09781v2 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09781">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has been severely impacting global society since
December 2019. Massive research has been undertaken to understand the
characteristics of the virus and design vaccines and drugs. The related
findings have been reported in biomedical literature at a rate of about 10,000
articles on COVID-19 per month. Such rapid growth significantly challenges
manual curation and interpretation. For instance, LitCovid is a literature
database of COVID-19-related articles in PubMed, which has accumulated more
than 200,000 articles with millions of accesses each month by users worldwide.
One primary curation task is to assign up to eight topics (e.g., Diagnosis and
Treatment) to the articles in LitCovid. Despite the continuing advances in
biomedical text mining methods, few have been dedicated to topic annotations in
COVID-19 literature. To close the gap, we organized the BioCreative LitCovid
track to call for a community effort to tackle automated topic annotation for
COVID-19 literature. The BioCreative LitCovid dataset, consisting of over
30,000 articles with manually reviewed topics, was created for training and
testing. It is one of the largest multilabel classification datasets in
biomedical scientific literature. 19 teams worldwide participated and made 80
submissions in total. Most teams used hybrid systems based on transformers. The
highest performing submissions achieved 0.8875, 0.9181, and 0.9394 for macro
F1-score, micro F1-score, and instance-based F1-score, respectively. The level
of participation and results demonstrate a successful track and help close the
gap between dataset curation and method development. The dataset is publicly
available via https://ftp.ncbi.nlm.nih.gov/pub/lu/LitCovid/biocreative/ for
benchmarking and further development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue. (arXiv:2204.13021v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13021">
<div class="article-summary-box-inner">
<span><p>We present NLU++, a novel dataset for natural language understanding (NLU) in
task-oriented dialogue (ToD) systems, with the aim to provide a much more
challenging evaluation environment for dialogue NLU models, up to date with the
current application and industry requirements. NLU++ is divided into two
domains (BANKING and HOTELS) and brings several crucial improvements over
current commonly used NLU datasets. 1) NLU++ provides fine-grained domain
ontologies with a large set of challenging multi-intent sentences, introducing
and validating the idea of intent modules that can be combined into complex
intents that convey complex user goals, combined with finer-grained and thus
more challenging slot sets. 2) The ontology is divided into domain-specific and
generic (i.e., domain-universal) intent modules that overlap across domains,
promoting cross-domain reusability of annotated examples. 3) The dataset design
has been inspired by the problems observed in industrial ToD systems, and 4) it
has been collected, filtered and carefully annotated by dialogue NLU experts,
yielding high-quality annotated data. Finally, we benchmark a series of current
state-of-the-art NLU models on NLU++; the results demonstrate the challenging
nature of the dataset, especially in low-data regimes, the validity of `intent
modularisation', and call for further research on ToD NLU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIE: a Parameter and Inference Efficient Solution for Large Scale Knowledge Graph Embedding Reasoning. (arXiv:2204.13957v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13957">
<div class="article-summary-box-inner">
<span><p>Knowledge graph (KG) embedding methods which map entities and relations to
unique embeddings in the KG have shown promising results on many reasoning
tasks. However, the same embedding dimension for both dense entities and sparse
entities will cause either over parameterization (sparse entities) or under
fitting (dense entities). Normally, a large dimension is set to get better
performance. Meanwhile, the inference time grows log-linearly with the number
of entities for all entities are traversed and compared. Both the parameter and
inference become challenges when working with huge amounts of entities. Thus,
we propose PIE, a \textbf{p}arameter and \textbf{i}nference \textbf{e}fficient
solution. Inspired from tensor decomposition methods, we find that decompose
entity embedding matrix into low rank matrices can reduce more than half of the
parameters while maintaining comparable performance. To accelerate model
inference, we propose a self-supervised auxiliary task, which can be seen as
fine-grained entity typing. By randomly masking and recovering entities'
connected relations, the task learns the co-occurrence of entity and relations.
Utilizing the fine grained typing, we can filter unrelated entities during
inference and get targets with possibly sub-linear time requirement.
Experiments on link prediction benchmarks demonstrate the proposed key
capabilities. Moreover, we prove effectiveness of the proposed solution on the
Open Graph Benchmark large scale challenge dataset WikiKG90Mv2 and achieve the
state of the art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPT: Open Pre-trained Transformer Language Models. (arXiv:2205.01068v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01068">
<div class="article-summary-box-inner">
<span><p>Large language models, which are often trained for hundreds of thousands of
compute days, have shown remarkable capabilities for zero- and few-shot
learning. Given their computational cost, these models are difficult to
replicate without significant capital. For the few that are available through
APIs, no access is granted to the full model weights, making them difficult to
study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only
pre-trained transformers ranging from 125M to 175B parameters, which we aim to
fully and responsibly share with interested researchers. We show that OPT-175B
is comparable to GPT-3, while requiring only 1/7th the carbon footprint to
develop. We are also releasing our logbook detailing the infrastructure
challenges we faced, along with code for experimenting with all of the released
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scientific Explanation and Natural Language: A Unified Epistemological-Linguistic Perspective for Explainable AI. (arXiv:2205.01809v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01809">
<div class="article-summary-box-inner">
<span><p>A fundamental research goal for Explainable AI (XAI) is to build models that
are capable of reasoning through the generation of natural language
explanations. However, the methodologies to design and evaluate
explanation-based inference models are still poorly informed by theoretical
accounts on the nature of explanation. As an attempt to provide an
epistemologically grounded characterisation for XAI, this paper focuses on the
scientific domain, aiming to bridge the gap between theory and practice on the
notion of a scientific explanation. Specifically, the paper combines a detailed
survey of the modern accounts of scientific explanation in Philosophy of
Science with a systematic analysis of corpora of natural language explanations,
clarifying the nature and function of explanatory arguments from both a
top-down (categorical) and a bottom-up (corpus-based) perspective. Through a
mixture of quantitative and qualitative methodologies, the presented study
allows deriving the following main conclusions: (1) Explanations cannot be
entirely characterised in terms of inductive or deductive arguments as their
main function is to perform unification; (2) An explanation must cite causes
and mechanisms that are responsible for the occurrence of the event to be
explained; (3) While natural language explanations possess an intrinsic
causal-mechanistic nature, they are not limited to causes and mechanisms, also
accounting for pragmatic elements such as definitions, properties and taxonomic
relations; (4) Patterns of unification naturally emerge in corpora of
explanations even if not intentionally modelled; (5) Unification is realised
through a process of abstraction, whose function is to provide the inference
substrate for subsuming the event to be explained under recurring patterns and
high-level regularities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">i-Code: An Integrative and Composable Multimodal Learning Framework. (arXiv:2205.01818v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01818">
<div class="article-summary-box-inner">
<span><p>Human intelligence is multimodal; we integrate visual, linguistic, and
acoustic signals to maintain a holistic worldview. Most current pretraining
methods, however, are limited to one or two modalities. We present i-Code, a
self-supervised pretraining framework where users may flexibly combine the
modalities of vision, speech, and language into unified and general-purpose
vector representations. In this framework, data from each modality are first
given to pretrained single-modality encoders. The encoder outputs are then
integrated with a multimodal fusion network, which uses novel attention
mechanisms and other architectural innovations to effectively combine
information from the different modalities. The entire system is pretrained
end-to-end with new objectives including masked modality unit modeling and
cross-modality contrastive learning. Unlike previous research using only video
for pretraining, the i-Code framework can dynamically process single, dual, and
triple-modality data during training and inference, flexibly projecting
different combinations of modalities into a single representation space.
Experimental results demonstrate how i-Code can outperform state-of-the-art
techniques on five video understanding tasks and the GLUE NLP benchmark,
improving by as much as 11% and demonstrating the power of integrative
multimodal pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P^3 Ranker: Mitigating the Gaps between Pre-training and Ranking Fine-tuning with Prompt-based Learning and Pre-finetuning. (arXiv:2205.01886v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01886">
<div class="article-summary-box-inner">
<span><p>Compared to other language tasks, applying pre-trained language models (PLMs)
for search ranking often requires more nuances and training signals. In this
paper, we identify and study the two mismatches between pre-training and
ranking fine-tuning: the training schema gap regarding the differences in
training objectives and model architectures, and the task knowledge gap
considering the discrepancy between the knowledge needed in ranking and that
learned during pre-training. To mitigate these gaps, we propose Pre-trained,
Prompt-learned and Pre-finetuned Neural Ranker (P^3 Ranker). P^3 Ranker
leverages prompt-based learning to convert the ranking task into a pre-training
like schema and uses pre-finetuning to initialize the model on intermediate
supervised tasks. Experiments on MS MARCO and Robust04 show the superior
performances of P^3 Ranker in few-shot ranking. Analyses reveal that P^3 Ranker
is able to better accustom to the ranking task through prompt-based learning
and retrieve necessary ranking-oriented knowledge gleaned in pre-finetuning,
resulting in data-efficient PLM adaptation. Our code is available at
https://github.com/NEUIR/P3Ranker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning to Social Norms and Values in Interactive Narratives. (arXiv:2205.01975v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01975">
<div class="article-summary-box-inner">
<span><p>We focus on creating agents that act in alignment with socially beneficial
norms and values in interactive narratives or text-based games -- environments
wherein an agent perceives and interacts with a world through natural language.
Such interactive agents are often trained via reinforcement learning to
optimize task performance, even when such rewards may lead to agent behaviors
that violate societal norms -- causing harm either to the agent itself or other
entities in the environment. Social value alignment refers to creating agents
whose behaviors conform to expected moral and social norms for a given context
and group of people -- in our case, it means agents that behave in a manner
that is less harmful and more beneficial for themselves and others.
</p>
<p>We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25
annotated interactive narratives containing thousands of morally salient
scenarios covering everything from theft and bodily harm to altruism. We
introduce the GALAD (Game-value ALignment through Action Distillation) agent
that uses the social commonsense knowledge present in specially trained
language models to contextually restrict its action space to only those actions
that are aligned with socially beneficial values. An experimental study shows
that the GALAD agent makes decisions efficiently enough to improve
state-of-the-art task performance by 4% while reducing the frequency of
socially harmful behaviors by 25% compared to strong contemporary value
alignment approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models. (arXiv:2205.02023v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02023">
<div class="article-summary-box-inner">
<span><p>The success of multilingual pre-trained models is underpinned by their
ability to learn representations shared by multiple languages even in absence
of any explicit supervision. However, it remains unclear how these models learn
to generalise across languages. In this work, we conjecture that multilingual
pre-trained models can derive language-universal abstractions about grammar. In
particular, we investigate whether morphosyntactic information is encoded in
the same subset of neurons in different languages. We conduct the first
large-scale empirical study over 43 languages and 14 morphosyntactic categories
with a state-of-the-art neuron-level probe. Our findings show that the
cross-lingual overlap between neurons is significant, but its extent may vary
across categories and depends on language proximity and pre-training data size.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">P3IV: Probabilistic Procedure Planning from Instructional Videos with Weak Supervision. (arXiv:2205.02300v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02300">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the problem of procedure planning in instructional
videos. Here, an agent must produce a plausible sequence of actions that can
transform the environment from a given start to a desired goal state. When
learning procedure planning from instructional videos, most recent work
leverages intermediate visual observations as supervision, which requires
expensive annotation efforts to localize precisely all the instructional steps
in training videos. In contrast, we remove the need for expensive temporal
video annotations and propose a weakly supervised approach by learning from
natural language instructions. Our model is based on a transformer equipped
with a memory module, which maps the start and goal observations to a sequence
of plausible actions. Furthermore, we augment our model with a probabilistic
generative module to capture the uncertainty inherent to procedure planning, an
aspect largely overlooked by previous work. We evaluate our model on three
datasets and show our weaklysupervised approach outperforms previous fully
supervised state-of-the-art models on multiple metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking. (arXiv:2205.02301v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02301">
<div class="article-summary-box-inner">
<span><p>Estimating human motion from video is an active research area due to its many
potential applications. Most state-of-the-art methods predict human shape and
posture estimates for individual images and do not leverage the temporal
information available in video. Many "in the wild" sequences of human motion
are captured by a moving camera, which adds the complication of conflated
camera and human motion to the estimation. We therefore present BodySLAM, a
monocular SLAM system that jointly estimates the position, shape, and posture
of human bodies, as well as the camera trajectory. We also introduce a novel
human motion model to constrain sequential body postures and observe the scale
of the scene. Through a series of experiments on video sequences of human
motion captured by a moving monocular camera, we demonstrate that BodySLAM
improves estimates of all human body parameters and camera poses when compared
to estimating these separately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ShoeRinsics: Shoeprint Prediction for Forensics with Intrinsic Decomposition. (arXiv:2205.02361v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02361">
<div class="article-summary-box-inner">
<span><p>Shoe tread impressions are one of the most common types of evidence left at
crime scenes. However, the utility of such evidence is limited by the lack of
databases of footwear impression patterns that cover the huge and growing
number of distinct shoe models. We propose to address this gap by leveraging
shoe tread photographs collected by online retailers. The core challenge is to
predict the impression pattern from the shoe photograph since ground-truth
impressions or 3D shapes of tread patterns are not available. We develop a
model that performs intrinsic image decomposition (predicting depth, normal,
albedo, and lighting) from a single tread photo. Our approach, which we term
ShoeRinsics, combines domain adaptation and re-rendering losses in order to
leverage a mix of fully supervised synthetic data and unsupervised retail image
data. To validate model performance, we also collected a set of paired
shoe-sole images and corresponding prints, and define a benchmarking protocol
to quantify the accuracy of predicted impressions. On this benchmark,
ShoeRinsics outperforms existing methods for depth prediction and
synthetic-to-real domain adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Bayesian Detect to Track System for Robust Visual Object Tracking and Semi-Supervised Model Learning. (arXiv:2205.02371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02371">
<div class="article-summary-box-inner">
<span><p>Object tracking is one of the fundamental problems in visual recognition
tasks and has achieved significant improvements in recent years. The
achievements often come with the price of enormous hardware consumption and
expensive labor effort for consecutive labeling. A missing ingredient for
robust tracking is achieving performance with minimal modification on network
structure and semi-supervised learning intermittent labeled frames. In this
paper, we ad-dress these problems in a Bayesian tracking and detection
framework parameterized by neural network outputs. In our framework, the
tracking and detection process is formulated in a probabilistic way as
multi-objects dynamics and network detection uncertainties. With our
formulation, we propose a particle filter-based approximate sampling algorithm
for tracking object state estimation. Based on our particle filter inference
algorithm, a semi-supervised learn-ing algorithm is utilized for learning
tracking network on intermittent labeled frames by variational inference. In
our experiments, we provide both mAP and probability-based detection
measurements for comparison between our algorithm with non-Bayesian solutions.
We also train a semi-supervised tracking network on M2Cai16-Tool-Locations
Dataset and compare our results with supervised learning on fully labeled
frames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compressive Ptychography using Deep Image and Generative Priors. (arXiv:2205.02397v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02397">
<div class="article-summary-box-inner">
<span><p>Ptychography is a well-established coherent diffraction imaging technique
that enables non-invasive imaging of samples at a nanometer scale. It has been
extensively used in various areas such as the defense industry or materials
science. One major limitation of ptychography is the long data acquisition time
due to mechanical scanning of the sample; therefore, approaches to reduce the
scan points are highly desired. However, reconstructions with less number of
scan points lead to imaging artifacts and significant distortions, hindering a
quantitative evaluation of the results. To address this bottleneck, we propose
a generative model combining deep image priors with deep generative priors. The
self-training approach optimizes the deep generative neural network to create a
solution for a given dataset. We complement our approach with a prior acquired
from a previously trained discriminator network to avoid a possible divergence
from the desired output caused by the noise in the measurements. We also
suggest using the total variation as a complementary before combat artifacts
due to measurement noise. We analyze our approach with numerical experiments
through different probe overlap percentages and varying noise levels. We also
demonstrate improved reconstruction accuracy compared to the state-of-the-art
method and discuss the advantages and disadvantages of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spot-adaptive Knowledge Distillation. (arXiv:2205.02399v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02399">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) has become a well established paradigm for
compressing deep neural networks. The typical way of conducting knowledge
distillation is to train the student network under the supervision of the
teacher network to harness the knowledge at one or multiple spots (i.e.,
layers) in the teacher network. The distillation spots, once specified, will
not change for all the training samples, throughout the whole distillation
process. In this work, we argue that distillation spots should be adaptive to
training samples and distillation epochs. We thus propose a new distillation
strategy, termed spot-adaptive KD (SAKD), to adaptively determine the
distillation spots in the teacher network per sample, at every training
iteration during the whole distillation period. As SAKD actually focuses on
"where to distill" instead of "what to distill" that is widely investigated by
most existing works, it can be seamlessly integrated into existing distillation
methods to further improve their performance. Extensive experiments with 10
state-of-the-art distillers are conducted to demonstrate the effectiveness of
SAKD for improving their distillation performance, under both homogeneous and
heterogeneous distillation settings. Code is available at
https://github.com/zju-vipa/spot-adaptive-pytorch
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Reconstruction from Point Clouds: A Survey and a Benchmark. (arXiv:2205.02413v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02413">
<div class="article-summary-box-inner">
<span><p>Reconstruction of a continuous surface of two-dimensional manifold from its
raw, discrete point cloud observation is a long-standing problem. The problem
is technically ill-posed, and becomes more difficult considering that various
sensing imperfections would appear in the point clouds obtained by practical
depth scanning. In literature, a rich set of methods has been proposed, and
reviews of existing methods are also provided. However, existing reviews are
short of thorough investigations on a common benchmark. The present paper aims
to review and benchmark existing methods in the new era of deep learning
surface reconstruction. To this end, we contribute a large-scale benchmarking
dataset consisting of both synthetic and real-scanned data; the benchmark
includes object- and scene-level surfaces and takes into account various
sensing imperfections that are commonly encountered in practical depth
scanning. We conduct thorough empirical studies by comparing existing methods
on the constructed benchmark, and pay special attention on robustness of
existing methods against various scanning imperfections; we also study how
different methods generalize in terms of reconstructing complex surface shapes.
Our studies help identify the best conditions under which different methods
work, and suggest some empirical findings. For example, while deep learning
methods are increasingly popular, our systematic studies suggest that,
surprisingly, a few classical methods perform even better in terms of both
robustness and generalization; our studies also suggest that the practical
challenges of misalignment of point sets from multi-view scanning, missing of
surface points, and point outliers remain unsolved by all the existing surface
reconstruction methods. We expect that the benchmark and our studies would be
valuable both for practitioners and as a guidance for new innovations in future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Real-time Traffic Sign and Traffic Light Detection on Embedded Systems. (arXiv:2205.02421v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02421">
<div class="article-summary-box-inner">
<span><p>Recent work done on traffic sign and traffic light detection focus on
improving detection accuracy in complex scenarios, yet many fail to deliver
real-time performance, specifically with limited computational resources. In
this work, we propose a simple deep learning based end-to-end detection
framework, which effectively tackles challenges inherent to traffic sign and
traffic light detection such as small size, large number of classes and complex
road scenarios. We optimize the detection models using TensorRT and integrate
with Robot Operating System to deploy on an Nvidia Jetson AGX Xavier as our
embedded device. The overall system achieves a high inference speed of 63
frames per second, demonstrating the capability of our system to perform in
real-time. Furthermore, we introduce CeyRo, which is the first ever large-scale
traffic sign and traffic light detection dataset for the Sri Lankan context.
Our dataset consists of 7984 total images with 10176 traffic sign and traffic
light instances covering 70 traffic sign and 5 traffic light classes. The
images have a high resolution of 1920 x 1080 and capture a wide range of
challenging road scenarios with different weather and lighting conditions. Our
work is publicly available at https://github.com/oshadajay/CeyRo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text to artistic image generation. (arXiv:2205.02439v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02439">
<div class="article-summary-box-inner">
<span><p>Painting is one of the ways for people to express their ideas, but what if
people with disabilities in hands want to paint? To tackle this challenge, we
create an end-to-end solution that can generate artistic images from text
descriptions. However, due to the lack of datasets with paired text description
and artistic images, it is hard to directly train an algorithm which can create
art based on text input. To address this issue, we split our task into three
steps: (1) Generate a realistic image from a text description by using Dynamic
Memory Generative Adversarial Network (<a href="/abs/1904.01310">arXiv:1904.01310</a>), (2) Classify the
image as a genre that exists in the WikiArt dataset using Resnet (arXiv:
<a href="/abs/1512.03385">1512.03385</a>), (3) Select a style that is compatible with the genre and transfer
it to the generated image by using neural artistic stylization network
(<a href="/abs/1705.06830">arXiv:1705.06830</a>).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Declaration-based Prompt Tuning for Visual Question Answering. (arXiv:2205.02456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02456">
<div class="article-summary-box-inner">
<span><p>In recent years, the pre-training-then-fine-tuning paradigm has yielded
immense success on a wide spectrum of cross-modal tasks, such as visual
question answering (VQA), in which a visual-language (VL) model is first
optimized via self-supervised task objectives, e.g., masked language modeling
(MLM) and image-text matching (ITM), and then fine-tuned to adapt to downstream
task (e.g., VQA) via a brand-new objective function, e.g., answer prediction.
The inconsistency of the objective forms not only severely limits the
generalization of pre-trained VL models to downstream tasks, but also requires
a large amount of labeled data for fine-tuning. To alleviate the problem, we
propose an innovative VL fine-tuning paradigm (named Declaration-based Prompt
Tuning, abbreviated as DPT), which jointly optimizes the objectives of
pre-training and fine-tuning of VQA model, boosting the effective adaptation of
pre-trained VL models to the downstream task. Specifically, DPT reformulates
the objective form of VQA task via (1) textual adaptation, which converts the
given questions into declarative sentence-form for prompt-tuning, and (2) task
adaptation, which optimizes the objective function of VQA problem in the manner
of pre-training phase. Experimental results on GQA dataset show that DPT
outperforms the fine-tuned counterpart by a large margin regarding accuracy in
both fully-supervised (2.68%) and zero-shot/few-shot (over 31%) settings. All
the data and codes will be available to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMINR: Multi-frame-to-Multi-frame Inference with Noise Resistance for Precipitation Nowcasting with Radar. (arXiv:2205.02457v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02457">
<div class="article-summary-box-inner">
<span><p>Precipitation nowcasting based on radar echo maps is essential in
meteorological research. Recently, Convolutional RNNs based methods dominate
this field, but they cannot be solved by parallel computation resulting in
longer inference time. FCN based methods adopt a multi-frame-to-single-frame
inference (MSI) strategy to avoid this problem. They feedback into the model
again to predict the next time step to get multi-frame nowcasting results in
the prediction phase, which will lead to the accumulation of prediction errors.
In addition, precipitation noise is a crucial factor contributing to high
prediction errors because of its unpredictability. To address this problem, we
propose a novel Multi-frame-to-Multi-frame Inference (MMI) model with Noise
Resistance (NR) named MMINR. It avoids error accumulation and resists
precipitation noise\'s negative effect in parallel computation. NR contains a
Noise Dropout Module (NDM) and a Semantic Restore Module (SRM). NDM
deliberately dropout noise simple yet efficient, and SRM supplements semantic
information of features to alleviate the problem of semantic information
mistakenly lost by NDM. Experimental results demonstrate that MMINR can attain
competitive scores compared with other SOTAs. The ablation experiments show
that the proposed NDM and SRM can solve the aforementioned problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Correspondences with All-pairs Correlations for Multi-view Depth Estimation. (arXiv:2205.02481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02481">
<div class="article-summary-box-inner">
<span><p>Multi-view depth estimation plays a critical role in reconstructing and
understanding the 3D world. Recent learning-based methods have made significant
progress in it. However, multi-view depth estimation is fundamentally a
correspondence-based optimization problem, but previous learning-based methods
mainly rely on predefined depth hypotheses to build correspondence as the cost
volume and implicitly regularize it to fit depth prediction, deviating from the
essence of iterative optimization based on stereo correspondence. Thus, they
suffer unsatisfactory precision and generalization capability. In this paper,
we are the first to explore more general image correlations to establish
correspondences dynamically for depth estimation. We design a novel iterative
multi-view depth estimation framework mimicking the optimization process, which
consists of 1) a correlation volume construction module that models the pixel
similarity between a reference image and source images as all-to-all
correlations; 2) a flow-based depth initialization module that estimates the
depth from the 2D optical flow; 3) a novel correlation-guided depth refinement
module that reprojects points in different views to effectively fetch relevant
correlations for further fusion and integrate the fused correlation for
iterative depth update. Without predefined depth hypotheses, the fused
correlations establish multi-view correspondence in an efficient way and guide
the depth refinement heuristically. We conduct sufficient experiments on
ScanNet, DeMoN, ETH3D, and 7Scenes to demonstrate the superiority of our method
on multi-view depth estimation and its best generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are GAN-based Morphs Threatening Face Recognition?. (arXiv:2205.02496v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02496">
<div class="article-summary-box-inner">
<span><p>Morphing attacks are a threat to biometric systems where the biometric
reference in an identity document can be altered. This form of attack presents
an important issue in applications relying on identity documents such as border
security or access control. Research in generation of face morphs and their
detection is developing rapidly, however very few datasets with morphing
attacks and open-source detection toolkits are publicly available. This paper
bridges this gap by providing two datasets and the corresponding code for four
types of morphing attacks: two that rely on facial landmarks based on OpenCV
and FaceMorpher, and two that use StyleGAN 2 to generate synthetic morphs. We
also conduct extensive experiments to assess the vulnerability of four
state-of-the-art face recognition systems, including FaceNet, VGG-Face,
ArcFace, and ISV. Surprisingly, the experiments demonstrate that, although
visually more appealing, morphs based on StyleGAN 2 do not pose a significant
threat to the state to face recognition systems, as these morphs were
outmatched by the simple morphs that are based facial landmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">View-labels Are Indispensable: A Multifacet Complementarity Study of Multi-view Clustering. (arXiv:2205.02507v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02507">
<div class="article-summary-box-inner">
<span><p>Consistency and complementarity are two key ingredients for boosting
multi-view clustering (MVC). Recently with the introduction of popular
contrastive learning, the consistency learning of views has been further
enhanced in MVC, leading to promising performance. However, by contrast, the
complementarity has not received sufficient attention except just in the
feature facet, where the Hilbert Schmidt Independence Criterion (HSIC) term or
the independent encoder-decoder network is usually adopted to capture
view-specific information. This motivates us to reconsider the complementarity
learning of views comprehensively from multiple facets including the feature-,
view-label- and contrast- facets, while maintaining the view consistency. We
empirically find that all the facets contribute to the complementarity
learning, especially the view-label facet, which is usually neglected by
existing methods. Based on this, we develop a novel \underline{M}ultifacet
\underline{C}omplementarity learning framework for
\underline{M}ulti-\underline{V}iew \underline{C}lustering (MCMVC), which fuses
multifacet complementarity information, especially explicitly embedding the
view-label information. To our best knowledge, it is the first time to use
view-labels explicitly to guide the complementarity learning of views. Compared
with the SOTA baseline, MCMVC achieves remarkable improvements, e.g., by
average margins over $5.00\%$ and $7.00\%$ respectively in complete and
incomplete MVC settings on Caltech101-20 in terms of three evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Picture is Worth a Thousand Words: A New Wallet Recovery Process. (arXiv:2205.02511v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02511">
<div class="article-summary-box-inner">
<span><p>We introduce a new wallet recovery process. Our solution associates 1) visual
passwords: a photograph of a secretly picked object (Chabanne et al., 2013)
with 2) ImageNet classifiers transforming images into binary vectors and, 3)
obfuscated fuzzy matching (Galbraith and Zobernig, 2019) for the storage of
visual passwords/retrieval of wallet seeds. Our experiments show that the
replacement of long seed phrases by a photograph is possible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLOPose: Transformer-based Multi-Object 6D Pose Estimation using Keypoint Regression. (arXiv:2205.02536v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02536">
<div class="article-summary-box-inner">
<span><p>6D object pose estimation is a crucial prerequisite for autonomous robot
manipulation applications. The state-of-the-art models for pose estimation are
convolutional neural network (CNN)-based. Lately, Transformers, an architecture
originally proposed for natural language processing, is achieving
state-of-the-art results in many computer vision tasks as well. Equipped with
the multi-head self-attention mechanism, Transformers enable simple
single-stage end-to-end architectures for learning object detection and 6D
object pose estimation jointly. In this work, we propose YOLOPose (short form
for You Only Look Once Pose estimation), a Transformer-based multi-object 6D
pose estimation method based on keypoint regression. In contrast to the
standard heatmaps for predicting keypoints in an image, we directly regress the
keypoints. Additionally, we employ a learnable orientation estimation module to
predict the orientation from the keypoints. Along with a separate translation
estimation module, our model is end-to-end differentiable. Our method is
suitable for real-time applications and achieves results comparable to
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parametric Reshaping of Portraits in Videos. (arXiv:2205.02538v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02538">
<div class="article-summary-box-inner">
<span><p>Sharing short personalized videos to various social media networks has become
quite popular in recent years. This raises the need for digital retouching of
portraits in videos. However, applying portrait image editing directly on
portrait video frames cannot generate smooth and stable video sequences. To
this end, we present a robust and easy-to-use parametric method to reshape the
portrait in a video to produce smooth retouched results. Given an input
portrait video, our method consists of two main stages: stabilized face
reconstruction, and continuous video reshaping. In the first stage, we start by
estimating face rigid pose transformations across video frames. Then we jointly
optimize multiple frames to reconstruct an accurate face identity, followed by
recovering face expressions over the entire video. In the second stage, we
first reshape the reconstructed 3D face using a parametric reshaping model
reflecting the weight change of the face, and then utilize the reshaped 3D face
to guide the warping of video frames. We develop a novel signed distance
function based dense mapping method for the warping between face contours
before and after reshaping, resulting in stable warped video frames with
minimum distortions. In addition, we use the 3D structure of the face to
correct the dense mapping to achieve temporal consistency. We generate the
final result by minimizing the background distortion through optimizing a
content-aware warping mesh. Extensive experiments show that our method is able
to create visually pleasing results by adjusting a simple reshaping parameter,
which facilitates portrait video editing for social media and visual effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Controllable Motion Transition for Characters. (arXiv:2205.02540v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02540">
<div class="article-summary-box-inner">
<span><p>Real-time in-between motion generation is universally required in games and
highly desirable in existing animation pipelines. Its core challenge lies in
the need to satisfy three critical conditions simultaneously: quality,
controllability and speed, which renders any methods that need offline
computation (or post-processing) or cannot incorporate (often unpredictable)
user control undesirable. To this end, we propose a new real-time transition
method to address the aforementioned challenges. Our approach consists of two
key components: motion manifold and conditional transitioning. The former
learns the important low-level motion features and their dynamics; while the
latter synthesizes transitions conditioned on a target frame and the desired
transition duration. We first learn a motion manifold that explicitly models
the intrinsic transition stochasticity in human motions via a multi-modal
mapping mechanism. Then, during generation, we design a transition model which
is essentially a sampling strategy to sample from the learned manifold, based
on the target frame and the aimed transition duration. We validate our method
on different datasets in tasks where no post-processing or offline computation
is allowed. Through exhaustive evaluation and comparison, we show that our
method is able to generate high-quality motions measured under multiple
metrics. Our method is also robust under various target frames (with extreme
cases).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OCR Synthetic Benchmark Dataset for Indic Languages. (arXiv:2205.02543v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02543">
<div class="article-summary-box-inner">
<span><p>We present the largest publicly available synthetic OCR benchmark dataset for
Indic languages. The collection contains a total of 90k images and their ground
truth for 23 Indic languages. OCR model validation in Indic languages require a
good amount of diverse data to be processed in order to create a robust and
reliable model. Generating such a huge amount of data would be difficult
otherwise but with synthetic data, it becomes far easier. It can be of great
importance to fields like Computer Vision or Image Processing where once an
initial synthetic data is developed, model creation becomes easier. Generating
synthetic data comes with the flexibility to adjust its nature and environment
as and when required in order to improve the performance of the model. Accuracy
for labeled real-time data is sometimes quite expensive while accuracy for
synthetic data can be easily achieved with a good score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biologically inspired deep residual networks for computer vision applications. (arXiv:2205.02551v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02551">
<div class="article-summary-box-inner">
<span><p>Deep neural network has been ensured as a key technology in the field of many
challenging and vigorously researched computer vision tasks. Furthermore,
classical ResNet is thought to be a state-of-the-art convolutional neural
network (CNN) and was observed to capture features which can have good
generalization ability. In this work, we propose a biologically inspired deep
residual neural network where the hexagonal convolutions are introduced along
the skip connections. The performance of different ResNet variants using square
and hexagonal convolution are evaluated with the competitive training strategy
mentioned by [1]. We show that the proposed approach advances the baseline
image classification accuracy of vanilla ResNet architectures on CIFAR-10 and
the same was observed over multiple subsets of the ImageNet 2012 dataset. We
observed an average improvement by 1.35% and 0.48% on baseline top-1 accuracies
for ImageNet 2012 and CIFAR-10, respectively. The proposed biologically
inspired deep residual networks were observed to have improved generalized
performance and this could be a potential research direction to improve the
discriminative ability of state-of-the-art image classification networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DropTrack -- automatic droplet tracking using deep learning for microfluidic applications. (arXiv:2205.02568v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02568">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are rapidly emerging as data analysis tools, often
outperforming the conventional techniques used in complex microfluidic systems.
One fundamental analysis frequently desired in microfluidic experiments is
counting and tracking the droplets. Specifically, droplet tracking in dense
emulsions is challenging as droplets move in tightly packed configurations.
Sometimes the individual droplets in these dense clusters are hard to resolve,
even for a human observer. Here, two deep learning-based cutting-edge
algorithms for object detection (YOLO) and object tracking (DeepSORT) are
combined into a single image analysis tool, DropTrack, to track droplets in
microfluidic experiments. DropTrack analyzes input videos, extracts droplets'
trajectories, and infers other observables of interest, such as droplet
numbers. Training an object detector network for droplet recognition with
manually annotated images is a labor-intensive task and a persistent
bottleneck. This work partly resolves this problem by training object detector
networks (YOLOv5) with hybrid datasets containing real and synthetic images. We
present an analysis of a double emulsion experiment as a case study to measure
DropTrack's performance. For our test case, the YOLO networks trained with 60%
synthetic images show similar performance in droplet counting as with the one
trained using 100% real images, meanwhile saving the image annotation work by
60%. DropTrack's performance is measured in terms of mean average precision
(mAP), mean square error in counting the droplets, and inference speed. The
fastest configuration of DropTrack runs inference at about 30 frames per
second, well within the standards for real-time image analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intra and Cross-spectrum Iris Presentation Attack Detection in the NIR and Visible Domains Using Attention-based and Pixel-wise Supervised Learning. (arXiv:2205.02573v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02573">
<div class="article-summary-box-inner">
<span><p>Iris Presentation Attack Detection (PAD) is essential to secure iris
recognition systems. Recent iris PAD solutions achieved good performance by
leveraging deep learning techniques. However, most results were reported under
intra-database scenarios and it is unclear if such solutions can generalize
well across databases and capture spectra. These PAD methods run the risk of
overfitting because of the binary label supervision during the network
training, which serves global information learning but weakens the capture of
local discriminative features. This chapter presents a novel attention-based
deep pixel-wise binary supervision (A-PBS) method. A-PBS utilizes pixel-wise
supervision to capture the fine-grained pixel/patch-level cues and attention
mechanism to guide the network to automatically find regions where most
contribute to an accurate PAD decision. Extensive experiments are performed on
six NIR and one visible-light iris databases to show the effectiveness and
robustness of proposed A-PBS methods. We additionally conduct extensive
experiments under intra-/cross-database and intra-/cross-spectrum for detailed
analysis. The results of our experiments indicates the generalizability of the
A-PBS iris PAD approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Holistic Approach to Measure Sample-level Adversarial Vulnerability and its Utility in Building Trustworthy Systems. (arXiv:2205.02604v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02604">
<div class="article-summary-box-inner">
<span><p>Adversarial attack perturbs an image with an imperceptible noise, leading to
incorrect model prediction. Recently, a few works showed inherent bias
associated with such attack (robustness bias), where certain subgroups in a
dataset (e.g. based on class, gender, etc.) are less robust than others. This
bias not only persists even after adversarial training, but often results in
severe performance discrepancies across these subgroups. Existing works
characterize the subgroup's robustness bias by only checking individual
sample's proximity to the decision boundary. In this work, we argue that this
measure alone is not sufficient and validate our argument via extensive
experimental analysis. It has been observed that adversarial attacks often
corrupt the high-frequency components of the input image. We, therefore,
propose a holistic approach for quantifying adversarial vulnerability of a
sample by combining these different perspectives, i.e., degree of model's
reliance on high-frequency features and the (conventional) sample-distance to
the decision boundary. We demonstrate that by reliably estimating adversarial
vulnerability at the sample level using the proposed holistic metric, it is
possible to develop a trustworthy system where humans can be alerted about the
incoming samples that are highly likely to be misclassified at test time. This
is achieved with better precision when our holistic metric is used over
individual measures. To further corroborate the utility of the proposed
holistic approach, we perform knowledge distillation in a limited-sample
setting. We observe that the student network trained with the subset of samples
selected using our combined metric performs better than both the competing
baselines, viz., where samples are selected randomly or based on their
distances to the decision boundary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GANimator: Neural Motion Synthesis from a Single Sequence. (arXiv:2205.02625v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02625">
<div class="article-summary-box-inner">
<span><p>We present GANimator, a generative model that learns to synthesize novel
motions from a single, short motion sequence. GANimator generates motions that
resemble the core elements of the original motion, while simultaneously
synthesizing novel and diverse movements. Existing data-driven techniques for
motion synthesis require a large motion dataset which contains the desired and
specific skeletal structure. By contrast, GANimator only requires training on a
single motion sequence, enabling novel motion synthesis for a variety of
skeletal structures e.g., bipeds, quadropeds, hexapeds, and more. Our framework
contains a series of generative and adversarial neural networks, each
responsible for generating motions in a specific frame rate. The framework
progressively learns to synthesize motion from random noise, enabling
hierarchical control over the generated motion content across varying levels of
detail. We show a number of applications, including crowd simulation, key-frame
editing, style transfer, and interactive control, which all learn from a single
input sequence. Code and data for this paper are at
https://peizhuoli.github.io/ganimator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImPosIng: Implicit Pose Encoding for Efficient Camera Pose Estimation. (arXiv:2205.02638v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02638">
<div class="article-summary-box-inner">
<span><p>We propose a novel learning-based formulation for camera pose estimation that
can perform relocalization accurately and in real-time in city-scale
environments. Camera pose estimation algorithms determine the position and
orientation from which an image has been captured, using a set of
geo-referenced images or 3D scene representation. Our new localization
paradigm, named Implicit Pose Encoding (ImPosing), embeds images and camera
poses into a common latent representation with 2 separate neural networks, such
that we can compute a similarity score for each image-pose pair. By evaluating
candidates through the latent space in a hierarchical manner, the camera
position and orientation are not directly regressed but incrementally refined.
Compared to the representation used in structure-based relocalization methods,
our implicit map is memory bounded and can be properly explored to improve
localization performances against learning-based regression approaches. In this
paper, we describe how to effectively optimize our learned modules, how to
combine them to achieve real-time localization, and demonstrate results on
diverse large scale scenarios that significantly outperform prior work in
accuracy and computational efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models Can See: Plugging Visual Controls in Text Generation. (arXiv:2205.02655v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02655">
<div class="article-summary-box-inner">
<span><p>Generative language models (LMs) such as GPT-2/3 can be prompted to generate
text with remarkable quality. While they are designed for text-prompted
generation, it remains an open question how the generation process could be
guided by modalities beyond text such as images. In this work, we propose a
training-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),
for plugging in visual controls in the generation process and enabling LMs to
perform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC
is a simple yet efficient plug-and-play framework, which directly combines an
off-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)
for image-grounded text generation. During decoding, MAGIC influences the
generation of the LM by introducing a CLIP-induced score, called magic score,
which regularizes the generated result to be semantically related to a given
image while being coherent to the previously generated context. Notably, the
proposed decoding scheme does not involve any gradient update operation,
therefore being computationally efficient. On the challenging task of zero-shot
image captioning, MAGIC outperforms the state-of-the-art method by notable
margins with a nearly 27 times decoding speedup. MAGIC is a flexible framework
and is theoretically compatible with any text generation tasks that incorporate
image grounding. In the experiments, we showcase that it is also capable of
performing visually grounded story generation given both an image and a text
prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Detection on Technical Drawings for the Digitization of Brown-field Processes. (arXiv:2205.02659v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02659">
<div class="article-summary-box-inner">
<span><p>This paper addresses the issue of autonomously detecting text on technical
drawings. The detection of text on technical drawings is a critical step
towards autonomous production machines, especially for brown-field processes,
where no closed CAD-CAM solutions are available yet. Automating the process of
reading and detecting text on technical drawings reduces the effort for
handling inefficient media interruptions due to paper-based processes, which
are often todays quasi-standard in brown-field processes. However, there are no
reliable methods available yet to solve the issue of automatically detecting
text on technical drawings. The unreliable detection of the contents on
technical drawings using classical detection and object character recognition
(OCR) tools is mainly due to the limited number of technical drawings and the
captcha-like structure of the contents. Text is often combined with unknown
symbols and interruptions by lines. Additionally, due to intellectual property
rights and technical know-how issues, there are no out-of-the box training
datasets available in the literature to train such models. This paper combines
a domain knowledge-based generator to generate realistic technical drawings
with a state-of-the-art object detection model to solve the issue of detecting
text on technical drawings. The generator yields artificial technical drawings
in a large variety and can be considered as a data augmentation generator.
These artificial drawings are used for training, while the model is tested on
real data. The authors show that artificially generated data of technical
drawings improve the detection quality with an increasing number of drawings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What is Right for Me is Not Yet Right for You: A Dataset for Grounding Relative Directions via Multi-Task Learning. (arXiv:2205.02671v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02671">
<div class="article-summary-box-inner">
<span><p>Understanding spatial relations is essential for intelligent agents to act
and communicate in the physical world. Relative directions are spatial
relations that describe the relative positions of target objects with regard to
the intrinsic orientation of reference objects. Grounding relative directions
is more difficult than grounding absolute directions because it not only
requires a model to detect objects in the image and to identify spatial
relation based on this information, but it also needs to recognize the
orientation of objects and integrate this information into the reasoning
process. We investigate the challenging problem of grounding relative
directions with end-to-end neural networks. To this end, we provide GRiD-3D, a
novel dataset that features relative directions and complements existing visual
question answering (VQA) datasets, such as CLEVR, that involve only absolute
directions. We also provide baselines for the dataset with two established
end-to-end VQA models. Experimental evaluations show that answering questions
on relative directions is feasible when questions in the dataset simulate the
necessary subtasks for grounding relative directions. We discover that those
subtasks are learned in an order that reflects the steps of an intuitive
pipeline for processing relative directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hardware System Implementation for Human Detection using HOG and SVM Algorithm. (arXiv:2205.02689v1 [cs.AR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02689">
<div class="article-summary-box-inner">
<span><p>Human detection is a popular issue and has been widely used in many
applications. However, including complexities in computation, leading to the
human detection system implemented hardly in real-time applications. This paper
presents the architecture of hardware, a human detection system that was
simulated in the ModelSim tool. As a co-processor, this system was built to
off-load to Central Processor Unit (CPU) and speed up the computation timing.
The 130x66 RGB pixels of static input image attracted features and classify by
using the Histogram of Oriented Gradient (HOG) algorithm and Support Vector
Machine (SVM) algorithm, respectively. As a result, the accuracy rate of this
system reaches 84.35 percent. And the timing for detection decreases to 0.757
ms at 50MHz frequency (54 times faster when this system was implemented in
software by using the Matlab tool).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Batch Artifact Scanning Protocol: A new method using computed tomography (CT) to rapidly create three-dimensional models of objects from large collections en masse. (arXiv:2205.02691v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02691">
<div class="article-summary-box-inner">
<span><p>Within anthropology, the use of three-dimensional (3D) imaging has become
increasingly standard and widespread since it broadens the available avenues
for addressing a wide range of key issues. The ease with which 3D models can be
shared has had major impacts for research, cultural heritage, education,
science communication, and public engagement, as well as contributing to the
preservation of the physical specimens and archiving collections in widely
accessible data bases. Current scanning protocols have the ability to create
the required research quality 3D models; however, they tend to be time and
labor intensive and not practical when working with large collections. Here we
describe a streamlined, Batch Artifact Scanning Protocol we have developed to
rapidly create 3D models using a medical CT scanner. Though this method can be
used on a variety of material types, we use a large collection of
experimentally broken ungulate limb bones. Using the Batch Artifact Scanning
Protocol, we were able to efficiently create 3D models of 2,474 bone fragments
at a rate of less than $3$ minutes per specimen, as opposed to an average of 50
minutes per specimen using structured light scanning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gait Recognition in the Wild: A Benchmark. (arXiv:2205.02692v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02692">
<div class="article-summary-box-inner">
<span><p>Gait benchmarks empower the research community to train and evaluate
high-performance gait recognition systems. Even though growing efforts have
been devoted to cross-view recognition, academia is restricted by current
existing databases captured in the controlled environment. In this paper, we
contribute a new benchmark for Gait REcognition in the Wild (GREW). The GREW
dataset is constructed from natural videos, which contains hundreds of cameras
and thousands of hours streams in open systems. With tremendous manual
annotations, the GREW consists of 26K identities and 128K sequences with rich
attributes for unconstrained gait recognition. Moreover, we add a distractor
set of over 233K sequences, making it more suitable for real-world
applications. Compared with prevailing predefined cross-view datasets, the GREW
has diverse and practical view variations, as well as more natural challenging
factors. To the best of our knowledge, this is the first large-scale dataset
for gait recognition in the wild. Equipped with this benchmark, we dissect the
unconstrained gait recognition problem. Representative appearance-based and
model-based methods are explored, and comprehensive baselines are established.
Experimental results show (1) The proposed GREW benchmark is necessary for
training and evaluating gait recognizer in the wild. (2) For state-of-the-art
gait recognition approaches, there is a lot of room for improvement. (3) The
GREW benchmark can be used as effective pre-training for controlled gait
recognition. Benchmark website is https://www.grew-benchmark.org/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Different Deep Metric Learning Losses Lead to Similar Learned Features?. (arXiv:2205.02698v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02698">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown that many deep metric learning loss functions
perform very similarly under the same experimental conditions. One potential
reason for this unexpected result is that all losses let the network focus on
similar image regions or properties. In this paper, we investigate this by
conducting a two-step analysis to extract and compare the learned visual
features of the same model architecture trained with different loss functions:
First, we compare the learned features on the pixel level by correlating
saliency maps of the same input images. Second, we compare the clustering of
embeddings for several image properties, e.g. object color or illumination. To
provide independent control over these properties, photo-realistic 3D car
renders similar to images in the Cars196 dataset are generated. In our
analysis, we compare 14 pretrained models from a recent study and find that,
even though all models perform similarly, different loss functions can guide
the model to learn different features. We especially find differences between
classification and ranking based losses. Our analysis also shows that some
seemingly irrelevant properties can have significant influence on the resulting
embedding. We encourage researchers from the deep metric learning community to
use our methods to get insights into the features learned by their proposed
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid CNN Based Attention with Category Prior for User Image Behavior Modeling. (arXiv:2205.02711v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02711">
<div class="article-summary-box-inner">
<span><p>User historical behaviors are proved useful for Click Through Rate (CTR)
prediction in online advertising system. In Meituan, one of the largest
e-commerce platform in China, an item is typically displayed with its image and
whether a user clicks the item or not is usually influenced by its image, which
implies that user's image behaviors are helpful for understanding user's visual
preference and improving the accuracy of CTR prediction. Existing user image
behavior models typically use a two-stage architecture, which extracts visual
embeddings of images through off-the-shelf Convolutional Neural Networks (CNNs)
in the first stage, and then jointly trains a CTR model with those visual
embeddings and non-visual features. We find that the two-stage architecture is
sub-optimal for CTR prediction. Meanwhile, precisely labeled categories in
online ad systems contain abundant visual prior information, which can enhance
the modeling of user image behaviors. However, off-the-shelf CNNs without
category prior may extract category unrelated features, limiting CNN's
expression ability. To address the two issues, we propose a hybrid CNN based
attention module, unifying user's image behaviors and category prior, for CTR
prediction. Our approach achieves significant improvements in both online and
offline experiments on a billion scale real serving dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Rendering in a Room: Amodal 3D Understanding and Free-Viewpoint Rendering for the Closed Scene Composed of Pre-Captured Objects. (arXiv:2205.02714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02714">
<div class="article-summary-box-inner">
<span><p>We, as human beings, can understand and picture a familiar scene from
arbitrary viewpoints given a single image, whereas this is still a grand
challenge for computers. We hereby present a novel solution to mimic such human
perception capability based on a new paradigm of amodal 3D scene understanding
with neural rendering for a closed scene. Specifically, we first learn the
prior knowledge of the objects in a closed scene via an offline stage, which
facilitates an online stage to understand the room with unseen furniture
arrangement. During the online stage, given a panoramic image of the scene in
different layouts, we utilize a holistic neural-rendering-based optimization
framework to efficiently estimate the correct 3D scene layout and deliver
realistic free-viewpoint rendering. In order to handle the domain gap between
the offline and online stage, our method exploits compositional neural
rendering techniques for data augmentation in the offline training. The
experiments on both synthetic and real datasets demonstrate that our two-stage
design achieves robust 3D scene understanding and outperforms competing methods
by a large margin, and we also show that our realistic free-viewpoint rendering
enables various applications, including scene touring and editing. Code and
data are available on the project webpage:
https://zju3dv.github.io/nr_in_a_room/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BasicTAD: an Astounding RGB-Only Baseline for Temporal Action Detection. (arXiv:2205.02717v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02717">
<div class="article-summary-box-inner">
<span><p>Temporal action detection (TAD) is extensively studied in the video
understanding community by following the object detection pipelines in images.
However, complex designs are not uncommon in TAD, such as two-stream feature
extraction, multi-stage training, complex temporal modeling, and global context
fusion. In this paper, we do not aim to introduce any novel technique for TAD.
Instead, we study a simple, straightforward, yet must-known baseline given the
current status of complex design and low efficiency in TAD. In our simple
baseline (BasicTAD), we decompose the TAD pipeline into several essential
components: data sampling, backbone design, neck construction, and detection
head. We empirically investigate the existing techniques in each component for
this baseline and, more importantly, perform end-to-end training over the
entire pipeline thanks to the simplicity in design. Our BasicTAD yields an
astounding RGB-Only baseline very close to the state-of-the-art methods with
two-stream inputs. In addition, we further improve the BasicTAD by preserving
more temporal and spatial information in network representation (termed as
BasicTAD Plus). Empirical results demonstrate that our BasicTAD Plus is very
efficient and significantly outperforms the previous methods on the datasets of
THUMOS14 and FineAction. Our approach can serve as a strong baseline for TAD.
The code will be released at https://github.com/MCG-NJU/BasicTAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Koopman pose predictions for temporally consistent human walking estimations. (arXiv:2205.02737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02737">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of tracking the human lower body as an initial step
toward an automatic motion assessment system for clinical mobility evaluation,
using a multimodal system that combines Inertial Measurement Unit (IMU) data,
RGB images, and point cloud depth measurements. This system applies the factor
graph representation to an optimization problem that provides 3-D skeleton
joint estimations. In this paper, we focus on improving the temporal
consistency of the estimated human trajectories to greatly extend the range of
operability of the depth sensor. More specifically, we introduce a new factor
graph factor based on Koopman theory that embeds the nonlinear dynamics of
several lower-limb movement activities. This factor performs a two-step
process: first, a custom activity recognition module based on spatial temporal
graph convolutional networks recognizes the walking activity; then, a Koopman
pose prediction of the subsequent skeleton is used as an a priori estimation to
drive the optimization problem toward more consistent results. We tested the
performance of this module on datasets composed of multiple clinical lowerlimb
mobility tests, and we show that our approach reduces outliers on the skeleton
form by almost 1 m, while preserving natural walking trajectories at depths up
to more than 10 m.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Activity Detection in Long Surgical Videos using Spatio-Temporal Models. (arXiv:2205.02805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02805">
<div class="article-summary-box-inner">
<span><p>Automatic activity detection is an important component for developing
technologies that enable next generation surgical devices and workflow
monitoring systems. In many application, the videos of interest are long and
include several activities; hence, the deep models designed for such purposes
consist of a backbone and a temporal sequence modeling architecture. In this
paper, we investigate both the state-of-the-art activity recognition and
temporal models to find the architectures that yield the highest performance.
We first benchmark these models on a large-scale activity recognition dataset
in the operating room with over 800 full-length surgical videos. However, since
most other medical applications lack such a large dataset, we further evaluate
our models on the Cholec80 surgical phase segmentation dataset, consisting of
only 40 training videos. For backbone architectures, we investigate both 3D
ConvNets and most recent transformer-based models; for temporal modeling, we
include temporal ConvNets, RNNs, and transformer models for a comprehensive and
thorough study. We show that even in the case of limited labeled data, we can
outperform the existing work by benefiting from models pre-trained on other
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Octree Graph Networks for Learning Adaptive Volumetric Shape Representations. (arXiv:2205.02825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02825">
<div class="article-summary-box-inner">
<span><p>We present an adaptive deep representation of volumetric fields of 3D shapes
and an efficient approach to learn this deep representation for high-quality 3D
shape reconstruction and auto-encoding. Our method encodes the volumetric field
of a 3D shape with an adaptive feature volume organized by an octree and
applies a compact multilayer perceptron network for mapping the features to the
field value at each 3D position. An encoder-decoder network is designed to
learn the adaptive feature volume based on the graph convolutions over the dual
graph of octree nodes. The core of our network is a new graph convolution
operator defined over a regular grid of features fused from irregular
neighboring octree nodes at different levels, which not only reduces the
computational and memory cost of the convolutions over irregular neighboring
octree nodes, but also improves the performance of feature learning. Our method
effectively encodes shape details, enables fast 3D shape reconstruction, and
exhibits good generality for modeling 3D shapes out of training categories. We
evaluate our method on a set of reconstruction tasks of 3D shapes and scenes
and validate its superiority over other existing approaches. Our code, data,
and trained models are available at https://wang-ps.github.io/dualocnn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually plausible human-object interaction capture from wearable sensors. (arXiv:2205.02830v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02830">
<div class="article-summary-box-inner">
<span><p>In everyday lives, humans naturally modify the surrounding environment
through interactions, e.g., moving a chair to sit on it. To reproduce such
interactions in virtual spaces (e.g., metaverse), we need to be able to capture
and model them, including changes in the scene geometry, ideally from
ego-centric input alone (head camera and body-worn inertial sensors). This is
an extremely hard problem, especially since the object/scene might not be
visible from the head camera (e.g., a human not looking at a chair while
sitting down, or not looking at the door handle while opening a door). In this
paper, we present HOPS, the first method to capture interactions such as
dragging objects and opening doors from ego-centric data alone. Central to our
method is reasoning about human-object interactions, allowing to track objects
even when they are not visible from the head camera. HOPS localizes and
registers both the human and the dynamic object in a pre-scanned static scene.
HOPS is an important first step towards advanced AR/VR applications based on
immersive virtual universes, and can provide human-centric training data to
teach machines to interact with their surroundings. The supplementary video,
data, and code will be available on our project page at
<a href="http://virtualhumans.mpi-inf.mpg.de/hops/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-view Transformers for real-time Map-view Semantic Segmentation. (arXiv:2205.02833v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02833">
<div class="article-summary-box-inner">
<span><p>We present cross-view transformers, an efficient attention-based model for
map-view semantic segmentation from multiple cameras. Our architecture
implicitly learns a mapping from individual camera views into a canonical
map-view representation using a camera-aware cross-view attention mechanism.
Each camera uses positional embeddings that depend on its intrinsic and
extrinsic calibration. These embeddings allow a transformer to learn the
mapping across different views without ever explicitly modeling it
geometrically. The architecture consists of a convolutional image encoder for
each view and cross-view transformer layers to infer a map-view semantic
segmentation. Our model is simple, easily parallelizable, and runs in
real-time. The presented architecture performs at state-of-the-art on the
nuScenes dataset, with 4x faster inference speeds. Code is available at
https://github.com/bradyz/cross_view_transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fixing Malfunctional Objects With Learned Physical Simulation and Functional Prediction. (arXiv:2205.02834v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02834">
<div class="article-summary-box-inner">
<span><p>This paper studies the problem of fixing malfunctional 3D objects. While
previous works focus on building passive perception models to learn the
functionality from static 3D objects, we argue that functionality is reckoned
with respect to the physical interactions between the object and the user.
Given a malfunctional object, humans can perform mental simulations to reason
about its functionality and figure out how to fix it. Inspired by this, we
propose FixIt, a dataset that contains about 5k poorly-designed 3D physical
objects paired with choices to fix them. To mimic humans' mental simulation
process, we present FixNet, a novel framework that seamlessly incorporates
perception and physical dynamics. Specifically, FixNet consists of a perception
module to extract the structured representation from the 3D point cloud, a
physical dynamics prediction module to simulate the results of interactions on
3D objects, and a functionality prediction module to evaluate the functionality
and choose the correct fix. Experimental results show that our framework
outperforms baseline models by a large margin, and can generalize well to
objects with similar interaction types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics. (arXiv:2205.02835v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02835">
<div class="article-summary-box-inner">
<span><p>Differentiable physics has recently been shown as a powerful tool for solving
soft-body manipulation tasks. However, the differentiable physics solver often
gets stuck when the initial contact points of the end effectors are sub-optimal
or when performing multi-stage tasks that require contact point switching,
which often leads to local minima. To address this challenge, we propose a
contact point discovery approach (CPDeform) that guides the stand-alone
differentiable physics solver to deform various soft-body plasticines. The key
idea of our approach is to integrate optimal transport-based contact points
discovery into the differentiable physics solver to overcome the local minima
from initial contact points or contact switching. On single-stage tasks, our
method can automatically find suitable initial contact points based on
transport priorities. On complex multi-stage tasks, we can iteratively switch
the contact points of end-effectors based on transport priorities. To evaluate
the effectiveness of our method, we introduce PlasticineLab-M that extends the
existing differentiable physics benchmark PlasticineLab to seven new
challenging multi-stage soft-body manipulation tasks. Extensive experimental
results suggest that: 1) on multi-stage tasks that are infeasible for the
vanilla differentiable physics solver, our approach discovers contact points
that efficiently guide the solver to completion; 2) on tasks where the vanilla
solver performs sub-optimally or near-optimally, our contact point discovery
method performs better than or on par with the manipulation performance
obtained with handcrafted contact points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural 3D Scene Reconstruction with the Manhattan-world Assumption. (arXiv:2205.02836v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02836">
<div class="article-summary-box-inner">
<span><p>This paper addresses the challenge of reconstructing 3D indoor scenes from
multi-view images. Many previous works have shown impressive reconstruction
results on textured objects, but they still have difficulty in handling
low-textured planar regions, which are common in indoor scenes. An approach to
solving this issue is to incorporate planer constraints into the depth map
estimation in multi-view stereo-based methods, but the per-view plane
estimation and depth optimization lack both efficiency and multi-view
consistency. In this work, we show that the planar constraints can be
conveniently integrated into the recent implicit neural representation-based
reconstruction methods. Specifically, we use an MLP network to represent the
signed distance function as the scene geometry. Based on the Manhattan-world
assumption, planar constraints are employed to regularize the geometry in floor
and wall regions predicted by a 2D semantic segmentation network. To resolve
the inaccurate segmentation, we encode the semantics of 3D points with another
MLP and design a novel loss that jointly optimizes the scene geometry and
semantics in 3D space. Experiments on ScanNet and 7-Scenes datasets show that
the proposed method outperforms previous methods by a large margin on 3D
reconstruction quality. The code is available at
https://zju3dv.github.io/manhattan_sdf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BlobGAN: Spatially Disentangled Scene Representations. (arXiv:2205.02837v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02837">
<div class="article-summary-box-inner">
<span><p>We propose an unsupervised, mid-level representation for a generative model
of scenes. The representation is mid-level in that it is neither per-pixel nor
per-image; rather, scenes are modeled as a collection of spatial, depth-ordered
"blobs" of features. Blobs are differentiably placed onto a feature grid that
is decoded into an image by a generative adversarial network. Due to the
spatial uniformity of blobs and the locality inherent to convolution, our
network learns to associate different blobs with different entities in a scene
and to arrange these blobs to capture scene layout. We demonstrate this
emergent behavior by showing that, despite training without any supervision,
our method enables applications such as easy manipulation of objects within a
scene (e.g., moving, removing, and restyling furniture), creation of feasible
scenes given constraints (e.g., plausible rooms with drawers at a particular
location), and parsing of real-world images into constituent parts. On a
challenging multi-category dataset of indoor scenes, BlobGAN outperforms
StyleGAN2 in image quality as measured by FID. See our project page for video
results and interactive demo: <a href="http://www.dave.ml/blobgan">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Dual Formulation of Boosting Algorithms. (arXiv:0901.3590v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/0901.3590">
<div class="article-summary-box-inner">
<span><p>We study boosting algorithms from a new perspective. We show that the
Lagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with
generalized hinge loss are all entropy maximization problems. By looking at the
dual problems of these boosting algorithms, we show that the success of
boosting algorithms can be understood in terms of maintaining a better margin
distribution by maximizing margins and at the same time controlling the margin
variance.We also theoretically prove that, approximately, AdaBoost maximizes
the average margin, instead of the minimum margin. The duality formulation also
enables us to develop column generation based optimization algorithms, which
are totally corrective. We show that they exhibit almost identical
classification results to that of standard stage-wise additive boosting
algorithms but with much faster convergence rates. Therefore fewer weak
classifiers are needed to build the ensemble using our proposed optimization
technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Domain Adaptation with Online Relation Regularization for Unsupervised Person Re-ID. (arXiv:2003.06650v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06650">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) aims at adapting the model trained on a
labeled source-domain dataset to an unlabeled target-domain dataset. The task
of UDA on open-set person re-identification (re-ID) is even more challenging as
the identities (classes) do not have overlap between the two domains. One major
research direction was based on domain translation, which, however, has fallen
out of favor in recent years due to inferior performance compared to
pseudo-label-based methods. We argue that the domain translation has great
potential on exploiting the valuable source-domain data but existing methods
did not provide proper regularization on the translation process. Specifically,
previous methods only focus on maintaining the identities of the translated
images while ignoring the inter-sample relations during translation. To tackle
the challenges, we propose an end-to-end structured domain adaptation framework
with an online relation-consistency regularization term. During training, the
person feature encoder is optimized to model inter-sample relations on-the-fly
for supervising relation-consistency domain translation, which in turn,
improves the encoder with informative translated images. The encoder can be
further improved with pseudo labels, where the source-to-target translated
images with ground-truth identities and target-domain images with pseudo
identities are jointly used for training. In the experiments, our proposed
framework is shown to achieve state-of-the-art performance on multiple UDA
tasks of person re-ID. With the synthetic-to-real translated images from our
structured domain-translation network, we achieved second place in the Visual
Domain Adaptation Challenge (VisDA) in 2020.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MACU-Net for Semantic Segmentation of Fine-Resolution Remotely Sensed Images. (arXiv:2007.13083v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.13083">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of remotely sensed images plays an important role in
land resource management, yield estimation, and economic assessment. U-Net, a
deep encoder-decoder architecture, has been used frequently for image
segmentation with high accuracy. In this Letter, we incorporate multi-scale
features generated by different layers of U-Net and design a multi-scale skip
connected and asymmetric-convolution-based U-Net (MACU-Net), for segmentation
using fine-resolution remotely sensed images. Our design has the following
advantages: (1) The multi-scale skip connections combine and realign semantic
features contained in both low-level and high-level feature maps; (2) the
asymmetric convolution block strengthens the feature representation and feature
extraction capability of a standard convolution layer. Experiments conducted on
two remotely sensed datasets captured by different satellite sensors
demonstrate that the proposed MACU-Net transcends the U-Net, U-NetPPL, U-Net
3+, amongst other benchmark approaches. Code is available at
https://github.com/lironui/MACU-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pneumonia Detection on Chest X-ray using Radiomic Features and Contrastive Learning. (arXiv:2101.04269v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04269">
<div class="article-summary-box-inner">
<span><p>Chest X-ray becomes one of the most common medical diagnoses due to its
noninvasiveness. The number of chest X-ray images has skyrocketed, but reading
chest X-rays still have been manually performed by radiologists, which creates
huge burnouts and delays. Traditionally, radiomics, as a subfield of radiology
that can extract a large number of quantitative features from medical images,
demonstrates its potential to facilitate medical imaging diagnosis before the
deep learning era. With the rise of deep learning, the explainability of deep
neural networks on chest X-ray diagnosis remains opaque. In this study, we
proposed a novel framework that leverages radiomics features and contrastive
learning to detect pneumonia in chest X-ray. Experiments on the RSNA Pneumonia
Detection Challenge dataset show that our model achieves superior results to
several state-of-the-art models (&gt; 10% in F1-score) and increases the model's
interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Augmented Contrastive Learning for Abnormality Classification and Localization in Chest X-rays with Radiomics using a Feedback Loop. (arXiv:2104.04968v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04968">
<div class="article-summary-box-inner">
<span><p>Building a highly accurate predictive model for classification and
localization of abnormalities in chest X-rays usually requires a large number
of manually annotated labels and pixel regions (bounding boxes) of
abnormalities. However, it is expensive to acquire such annotations, especially
the bounding boxes. Recently, contrastive learning has shown strong promise in
leveraging unlabeled natural images to produce highly generalizable and
discriminative features. However, extending its power to the medical image
domain is under-explored and highly non-trivial, since medical images are much
less amendable to data augmentations. In contrast, their prior knowledge, as
well as radiomic features, is often crucial. To bridge this gap, we propose an
end-to-end semi-supervised knowledge-augmented contrastive learning framework,
that simultaneously performs disease classification and localization tasks. The
key knob of our framework is a unique positive sampling approach tailored for
the medical images, by seamlessly integrating radiomic features as a knowledge
augmentation. Specifically, we first apply an image encoder to classify the
chest X-rays and to generate the image features. We next leverage Grad-CAM to
highlight the crucial (abnormal) regions for chest X-rays (even when
unannotated), from which we extract radiomic features. The radiomic features
are then passed through another dedicated encoder to act as the positive sample
for the image features generated from the same chest X-ray. In this way, our
framework constitutes a feedback loop for image and radiomic modality features
to mutually reinforce each other. Their contrasting yields knowledge-augmented
representations that are both robust and interpretable. Extensive experiments
on the NIH Chest X-ray dataset demonstrate that our approach outperforms
existing baselines in both classification and localization tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Green View Index and Green View Index best path using Google Street View and deep learning. (arXiv:2104.12627v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12627">
<div class="article-summary-box-inner">
<span><p>As an important part of the urban landscape research, analysing and studying
streetscape can increase the understanding of the cities' infrastructure, which
contributes to better planning and design of the urban living environment. In
this paper, we used Google Street View to obtain street view images of Osaka
City. The semantic segmentation model is adopted to segment the Osaka City
street view images and analyse the Green View Index (GVI). Based on the GVI
value, we take advantage of adjacency matrix and Floyd algorithm is used to
calculate Green View Index best path, solving the limitations of ArcGIS
software. Our analysis not only allows the calculation of specific routes for
the GVI best paths, but also realize the visualization and integration of
neighbourhood landscape. By summarising all the data, a more specific and
objective analysis of the landscape in the study area can be carried out, and
based on this, the available natural resources can be maximized for a better
life.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmented 2D-TAN: A Two-stage Approach for Human-centric Spatio-Temporal Video Grounding. (arXiv:2106.10634v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10634">
<div class="article-summary-box-inner">
<span><p>We propose an effective two-stage approach to tackle the problem of
language-based Human-centric Spatio-Temporal Video Grounding (HC-STVG) task. In
the first stage, we propose an Augmented 2D Temporal Adjacent Network
(Augmented 2D-TAN) to temporally ground the target moment corresponding to the
given description. Primarily, we improve the original 2D-TAN from two aspects:
First, a temporal context-aware Bi-LSTM Aggregation Module is developed to
aggregate clip-level representations, replacing the original max-pooling.
Second, we propose to employ Random Concatenation Augmentation (RCA) mechanism
during the training phase. In the second stage, we use pretrained MDETR model
to generate per-frame bounding boxes via language query, and design a set of
hand-crafted rules to select the best matching bounding box outputted by MDETR
for each frame within the grounded moment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08621">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop face.evoLVe -- a comprehensive library that
collects and implements a wide range of popular deep learning-based methods for
face recognition. First of all, face.evoLVe is composed of key components that
cover the full process of face analytics, including face alignment, data
processing, various backbones, losses, and alternatives with bags of tricks for
improving performance. Later, face.evoLVe supports multi-GPU training on top of
different deep learning platforms, such as PyTorch and PaddlePaddle, which
facilitates researchers to work on both large-scale datasets with millions of
images and low-shot counterparts with limited well-annotated data. More
importantly, along with face.evoLVe, images before &amp; after alignment in the
common benchmark datasets are released with source codes and trained models
provided. All these efforts lower the technical burdens in reproducing the
existing methods for comparison, while users of our library could focus on
developing advanced approaches more efficiently. Last but not least,
face.evoLVe is well designed and vibrantly evolving, so that new face
recognition approaches can be easily plugged into our framework. Note that we
have used face.evoLVe to participate in a number of face recognition
competitions and secured the first place. The version that supports PyTorch is
publicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the
PaddlePaddle version is available at
https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.
Face.evoLVe has been widely used for face analytics, receiving 2.4K stars and
622 forks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware. (arXiv:2108.02671v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02671">
<div class="article-summary-box-inner">
<span><p>Real-world perception systems in many cases build on hardware with limited
resources to adhere to cost and power limitations of their carrying system.
Deploying deep neural networks on resource-constrained hardware became possible
with model compression techniques, as well as efficient and hardware-aware
architecture design. However, model adaptation is additionally required due to
the diverse operation environments. In this work, we address the problem of
training deep neural networks on resource-constrained hardware in the context
of visual domain adaptation. We select the task of monocular depth estimation
where our goal is to transform a pre-trained model to the target's domain data.
While the source domain includes labels, we assume an unlabelled target domain,
as it happens in real-world applications. Then, we present an adversarial
learning approach that is adapted for training on the device with limited
resources. Since visual domain adaptation, i.e. neural network training, has
not been previously explored for resource-constrained hardware, we present the
first feasibility study for image-based depth estimation. Our experiments show
that visual domain adaptation is relevant only for efficient network
architectures and training sets at the order of a few hundred samples. Models
and code are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Neighborhood Deep Fusion Network for Point Cloud Analysis. (arXiv:2108.09228v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09228">
<div class="article-summary-box-inner">
<span><p>Recently, deep neural networks have made remarkable achievements in 3D point
cloud classification. However, existing classification methods are mainly
implemented on idealized point clouds and suffer heavy degradation of
per-formance on non-idealized scenarios. To handle this prob-lem, a feature
representation learning method, named Dual-Neighborhood Deep Fusion Network
(DNDFN), is proposed to serve as an improved point cloud encoder for the task
of non-idealized point cloud classification. DNDFN utilizes a trainable
neighborhood learning method called TN-Learning to capture the global key
neighborhood. Then, the global neighborhood is fused with the local
neighbor-hood to help the network achieve more powerful reasoning ability.
Besides, an Information Transfer Convolution (IT-Conv) is proposed for DNDFN to
learn the edge infor-mation between point-pairs and benefits the feature
transfer procedure. The transmission of information in IT-Conv is similar to
the propagation of information in the graph which makes DNDFN closer to the
human reasoning mode. Extensive experiments on existing benchmarks especially
non-idealized datasets verify the effectiveness of DNDFN and DNDFN achieves the
state of the arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovery-and-Selection: Towards Optimal Multiple Instance Learning for Weakly Supervised Object Detection. (arXiv:2110.09060v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09060">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object detection (WSOD) is a challenging task that requires
simultaneously learn object classifiers and estimate object locations under the
supervision of image category labels. A major line of WSOD methods roots in
multiple instance learning which regards images as bags of instances and
selects positive instances from each bag to learn the detector. However, a
grand challenge emerges when the detector inclines to converge to
discriminative parts of objects rather than the whole objects. In this paper,
under the hypothesis that optimal solutions are included in local minima, we
propose a discovery-and-selection approach fused with multiple instance
learning (DS-MIL), which finds rich local minima and select optimal solution
from multiple local minima. To implement DS-MIL, an attention module is
proposed so that more context information can be captured by feature maps and
more valuable proposals can be collected during training. With proposal
candidates, a selection module is proposed to select informative instances for
object detector. Experimental results on commonly used benchmarks show that our
proposed DS-MIL approach can consistently improve the baselines, reporting
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleAlign: Analysis and Applications of Aligned StyleGAN Models. (arXiv:2110.11323v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11323">
<div class="article-summary-box-inner">
<span><p>In this paper, we perform an in-depth study of the properties and
applications of aligned generative models. We refer to two models as aligned if
they share the same architecture, and one of them (the child) is obtained from
the other (the parent) via fine-tuning to another domain, a common practice in
transfer learning. Several works already utilize some basic properties of
aligned StyleGAN models to perform image-to-image translation. Here, we perform
the first detailed exploration of model alignment, also focusing on StyleGAN.
First, we empirically analyze aligned models and provide answers to important
questions regarding their nature. In particular, we find that the child model's
latent spaces are semantically aligned with those of the parent, inheriting
incredibly rich semantics, even for distant data domains such as human faces
and churches. Second, equipped with this better understanding, we leverage
aligned models to solve a diverse set of tasks. In addition to image
translation, we demonstrate fully automatic cross-domain image morphing. We
further show that zero-shot vision tasks may be performed in the child domain,
while relying exclusively on supervision in the parent domain. We demonstrate
qualitatively and quantitatively that our approach yields state-of-the-art
results, while requiring only simple fine-tuning and inversion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Rich Features for Gait Recognition by Integrating Skeletons and Silhouettes. (arXiv:2110.13408v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13408">
<div class="article-summary-box-inner">
<span><p>Gait recognition captures gait patterns from the walking sequence of an
individual for identification. Most existing gait recognition methods learn
features from silhouettes or skeletons for the robustness to clothing,
carrying, and other exterior factors. The combination of the two data
modalities, however, is not fully exploited. Previous multimodal gait
recognition methods mainly employ the skeleton to assist the local feature
extraction where the intrinsic discrimination of the skeleton data is ignored.
This paper proposes a simple yet effective Bimodal Fusion (BiFusion) network
which mines discriminative gait patterns in skeletons and integrates with
silhouette representations to learn rich features for identification.
Particularly, the inherent hierarchical semantics of body joints in a skeleton
is leveraged to design a novel Multi-Scale Gait Graph (MSGG) network for the
feature extraction of skeletons. Extensive experiments on CASIA-B and OUMVLP
demonstrate both the superiority of the proposed MSGG network in modeling
skeletons and the effectiveness of the bimodal fusion for gait recognition.
Under the most challenging condition of walking in different clothes on
CASIA-B, our method achieves the rank-1 accuracy of 92.1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Perceptual Quality of 2D Animation Interpolation. (arXiv:2111.12792v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12792">
<div class="article-summary-box-inner">
<span><p>Traditional 2D animation is labor-intensive, often requiring animators to
manually draw twelve illustrations per second of movement. While automatic
frame interpolation may ease this burden, the artistic effects inherent to 2D
animation make video synthesis particularly challenging compared to in the
photorealistic domain. Lower framerates result in larger displacements and
occlusions, discrete perceptual elements (e.g. lines and solid-color regions)
pose difficulties for texture-oriented convolutional networks, and exaggerated
nonlinear movements hinder training data collection. Previous work tried
addressing these issues, but used unscalable methods and focused on
pixel-perfect performance. In contrast, we build a scalable system more
appropriately centered on perceptual quality for this artistic domain. Firstly,
we propose a lightweight architecture with a simple yet effective
occlusion-inpainting technique to improve convergence on perceptual metrics
with fewer trainable parameters. Secondly, we design a novel auxiliary module
that leverages the Euclidean distance transform to improve the preservation of
key line and region structures. Thirdly, we automatically double the existing
manually-collected dataset for this task by quantitatively filtering out
movement nonlinearities, allowing us to improve model generalization. Finally,
we establish LPIPS and chamfer distance as strongly preferable to PSNR and SSIM
through a user study, validating our system's emphasis on perceptual quality in
the 2D animation domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion. (arXiv:2111.14690v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14690">
<div class="article-summary-box-inner">
<span><p>A typical pipeline for multi-object tracking (MOT) is to use a detector for
object localization, and following re-identification (re-ID) for object
association. This pipeline is partially motivated by recent progress in both
object detection and re-ID, and partially motivated by biases in existing
tracking datasets, where most objects tend to have distinguishing appearance
and re-ID models are sufficient for establishing associations. In response to
such bias, we would like to re-emphasize that methods for multi-object tracking
should also work when object appearance is not sufficiently discriminative. To
this end, we propose a large-scale dataset for multi-human tracking, where
humans have similar appearance, diverse motion and extreme articulation. As the
dataset contains mostly group dancing videos, we name it "DanceTrack". We
expect DanceTrack to provide a better platform to develop more MOT algorithms
that rely less on visual discrimination and depend more on motion analysis. We
benchmark several state-of-the-art trackers on our dataset and observe a
significant performance drop on DanceTrack when compared against existing
benchmarks. The dataset, project code and competition server are released at:
\url{https://github.com/DanceTrack}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Large Vessel Occlusions using Deep Learning by Deforming Vessel Tree Segmentations. (arXiv:2112.01797v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01797">
<div class="article-summary-box-inner">
<span><p>Computed Tomography Angiography is a key modality providing insights into the
cerebrovascular vessel tree that are crucial for the diagnosis and treatment of
ischemic strokes, in particular in cases of large vessel occlusions (LVO).
Thus, the clinical workflow greatly benefits from an automated detection of
patients suffering from LVOs. This work uses convolutional neural networks for
case-level classification trained with elastic deformation of the vessel tree
segmentation masks to artificially augment training data. Using only masks as
the input to our model uniquely allows us to apply such deformations much more
aggressively than one could with conventional image volumes while retaining
sample realism. The neural network classifies the presence of an LVO and the
affected hemisphere. In a 5-fold cross validated ablation study, we demonstrate
that the use of the suggested augmentation enables us to train robust models
even from few data sets. Training the EfficientNetB1 architecture on 100 data
sets, the proposed augmentation scheme was able to raise the ROC AUC to 0.85
from a baseline value of 0.56 using no augmentation. The best performance was
achieved using a 3D-DenseNet yielding an AUC of 0.87. The augmentation had
positive impact in classification of the affected hemisphere as well, where the
3D-DenseNet reached an AUC of 0.93 on both sides.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smooth-Swap: A Simple Enhancement for Face-Swapping with Smoothness. (arXiv:2112.05907v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05907">
<div class="article-summary-box-inner">
<span><p>Face-swapping models have been drawing attention for their compelling
generation quality, but their complex architectures and loss functions often
require careful tuning for successful training. We propose a new face-swapping
model called `Smooth-Swap', which excludes complex handcrafted designs and
allows fast and stable training. The main idea of Smooth-Swap is to build
smooth identity embedding that can provide stable gradients for identity
change. Unlike the one used in previous models trained for a purely
discriminative task, the proposed embedding is trained with a supervised
contrastive loss promoting a smoother space. With improved smoothness,
Smooth-Swap suffices to be composed of a generic U-Net-based generator and
three basic loss functions, a far simpler design compared with the previous
models. Extensive experiments on face-swapping benchmarks (FFHQ,
FaceForensics++) and face images in the wild show that our model is also
quantitatively and qualitatively comparable or even superior to the existing
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detailed Facial Geometry Recovery from Multi-View Images by Learning an Implicit Function. (arXiv:2201.01016v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01016">
<div class="article-summary-box-inner">
<span><p>Recovering detailed facial geometry from a set of calibrated multi-view
images is valuable for its wide range of applications. Traditional multi-view
stereo (MVS) methods adopt an optimization-based scheme to regularize the
matching cost. Recently, learning-based methods integrate all these into an
end-to-end neural network and show superiority of efficiency. In this paper, we
propose a novel architecture to recover extremely detailed 3D faces within
dozens of seconds. Unlike previous learning-based methods that regularize the
cost volume via 3D CNN, we propose to learn an implicit function for regressing
the matching cost. By fitting a 3D morphable model from multi-view images, the
features of multiple images are extracted and aggregated in the mesh-attached
UV space, which makes the implicit function more effective in recovering
detailed facial shape. Our method outperforms SOTA learning-based MVS in
accuracy by a large margin on the FaceScape dataset. The code and data are
released in https://github.com/zhuhao-nju/mvfr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negative Evidence Matters in Interpretable Histology Image Classification. (arXiv:2201.02445v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02445">
<div class="article-summary-box-inner">
<span><p>Using only global image-class labels, weakly-supervised learning methods,
such as class activation mapping, allow training CNNs to jointly classify an
image, and locate regions of interest associated with the predicted class.
However, without any guidance at the pixel level, such methods may yield
inaccurate regions. This problem is known to be more challenging with histology
images than with natural ones, since objects are less salient, structures have
more variations, and foreground and background regions have stronger
similarities. Therefore, computer vision methods for visual interpretation of
CNNs may not directly apply. In this paper, a simple yet efficient method based
on a composite loss is proposed to learn information from the fully negative
samples (i.e., samples without positive regions), and thereby reduce false
positives/negatives. Our new loss function contains two complementary terms:
the first exploits positive evidence collected from the CNN classifier, while
the second leverages the fully negative samples from training data. In
particular, a pre-trained CNN is equipped with a decoder that allows refining
the regions of interest. The CNN is exploited to collect both positive and
negative evidence at the pixel level to train the decoder. Our method called
NEGEV benefits from the fully negative samples that naturally occur in the
data, without any additional supervision signals beyond image-class labels.
Extensive experiments show that our proposed method can substantial outperform
related state-of-art methods on GlaS (public benchmark for colon cancer), and
Camelyon16 (patch-based benchmark for breast cancer using three different
backbones). Our results highlight the benefits of using both positive and
negative evidence, the first obtained from a classifier, and the other
naturally available in datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WPPNets and WPPFlows: The Power of Wasserstein Patch Priors for Superresolution. (arXiv:2201.08157v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08157">
<div class="article-summary-box-inner">
<span><p>Exploiting image patches instead of whole images have proved to be a powerful
approach to tackle various problems in image processing. Recently, Wasserstein
patch priors (WPP), which are based on the comparison of the patch
distributions of the unknown image and a reference image, were successfully
used as data-driven regularizers in the variational formulation of
superresolution. However, for each input image, this approach requires the
solution of a non-convex minimization problem which is computationally costly.
In this paper, we propose to learn two kinds of neural networks in an
unsupervised way based on WPP loss functions. First, we show how convolutional
neural networks (CNNs) can be incorporated. Once the network, called WPPNet, is
learned, it can very efficiently applied to any input image. Second, we
incorporate conditional normalizing flows to provide a tool for uncertainty
quantification. Numerical examples demonstrate the very good performance of
WPPNets for superresolution in various image classes even if the forward
operator is known only approximately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets. (arXiv:2202.00273v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00273">
<div class="article-summary-box-inner">
<span><p>Computer graphics has experienced a recent surge of data-centric approaches
for photorealistic and controllable content creation. StyleGAN in particular
sets new standards for generative modeling regarding image quality and
controllability. However, StyleGAN's performance severely degrades on large
unstructured datasets such as ImageNet. StyleGAN was designed for
controllability; hence, prior works suspect its restrictive design to be
unsuitable for diverse datasets. In contrast, we find the main limiting factor
to be the current training strategy. Following the recently introduced
Projected GAN paradigm, we leverage powerful neural network priors and a
progressive growing strategy to successfully train the latest StyleGAN3
generator on ImageNet. Our final model, StyleGAN-XL, sets a new
state-of-the-art on large-scale image synthesis and is the first to generate
images at a resolution of $1024^2$ at such a dataset scale. We demonstrate that
this model can invert and edit images beyond the narrow domain of portraits or
specific object classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13403">
<div class="article-summary-box-inner">
<span><p>Large datasets as required for deep learning of lip reading do not exist in
many languages. In this paper we present the dataset GLips (German Lips)
consisting of 250,000 publicly available videos of the faces of speakers of the
Hessian Parliament, which was processed for word-level lip reading using an
automatic pipeline. The format is similar to that of the English language LRW
(Lip Reading in the Wild) dataset, with each video encoding one word of
interest in a context of 1.16 seconds duration, which yields compatibility for
studying transfer learning between both datasets. By training a deep neural
network, we investigate whether lip reading has language-independent features,
so that datasets of different languages can be used to improve lip reading
models. We demonstrate learning from scratch and show that transfer learning
from LRW to GLips and vice versa improves learning speed and performance, in
particular for the validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object discovery and representation networks. (arXiv:2203.08777v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08777">
<div class="article-summary-box-inner">
<span><p>The promise of self-supervised learning (SSL) is to leverage large amounts of
unlabeled data to solve complex tasks. While there has been excellent progress
with simple, image-level learning, recent methods have shown the advantage of
including knowledge of image structure. However, by introducing hand-crafted
image segmentations to define regions of interest, or specialized augmentation
strategies, these methods sacrifice the simplicity and generality that makes
SSL so powerful. Instead, we propose a self-supervised learning paradigm that
discovers this image structure by itself. Our method, Odin, couples object
discovery and representation networks to discover meaningful image
segmentations without any supervision. The resulting learning paradigm is
simpler, less brittle, and more general, and achieves state-of-the-art transfer
learning results for object detection and instance segmentation on COCO, and
semantic segmentation on PASCAL and Cityscapes, while strongly surpassing
supervised pre-training for video segmentation on DAVIS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization. (arXiv:2203.13167v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13167">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the continual learning of Vision Transformers
(ViT) for the challenging exemplar-free scenario, with special focus on how to
efficiently distill the knowledge of its crucial self-attention mechanism
(SAM). Our work takes an initial step towards a surgical investigation of SAM
for designing coherent continual learning methods in ViTs. We first carry out
an evaluation of established continual learning regularization techniques. We
then examine the effect of regularization when applied to two key enablers of
SAM: (a) the contextualized embedding layers, for their ability to capture
well-scaled representations with respect to the values, and (b) the prescaled
attention maps, for carrying value-independent global contextual information.
We depict the perks of each distilling strategy on two image recognition
benchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall
accuracy, (b) helps enhance the rigidity by maintaining competitive
performances. Furthermore, we identify the limitation imposed by the symmetric
nature of regularization losses. To alleviate this, we propose an asymmetric
variant and apply it to the pooled output distillation (POD) loss adapted for
ViTs. Our experiments confirm that introducing asymmetry to POD boosts its
plasticity while retaining stability across (a) and (b). Moreover, we
acknowledge low forgetting measures for all the compared methods, indicating
that ViTs might be naturally inclined continual learner
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17247">
<div class="article-summary-box-inner">
<span><p>Breakthroughs in transformer-based models have revolutionized not only the
NLP field, but also vision and multimodal systems. However, although
visualization and interpretability tools have become available for NLP models,
internal mechanisms of vision and multimodal transformers remain largely
opaque. With the success of these transformers, it is increasingly critical to
understand their inner workings, as unraveling these black-boxes will lead to
more capable and trustworthy models. To contribute to this quest, we propose
VL-InterpreT, which provides novel interactive visualizations for interpreting
the attentions and hidden representations in multimodal transformers.
VL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety
of statistics in attention heads throughout all layers for both vision and
language components, (2) visualizes cross-modal and intra-modal attentions
through easily readable heatmaps, and (3) plots the hidden representations of
vision and language tokens as they pass through the transformer layers. In this
paper, we demonstrate the functionalities of VL-InterpreT through the analysis
of KD-VLP, an end-to-end pretraining vision-language multimodal
transformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and
WebQA, two visual question answering benchmarks. Furthermore, we also present a
few interesting findings about multimodal transformer behaviors that were
learned through our tool.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGRR-Net: Multi-level Graph Relational Reasoning Network for Facial Action Units Detection. (arXiv:2204.01349v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01349">
<div class="article-summary-box-inner">
<span><p>The Facial Action Coding System (FACS) encodes the action units (AUs) in
facial images, which has attracted extensive research attention due to its wide
use in facial expression analysis. Many methods that perform well on automatic
facial action unit (AU) detection primarily focus on modeling various types of
AU relations between corresponding local muscle areas, or simply mining global
attention-aware facial features, however, neglect the dynamic interactions
among local-global features. We argue that encoding AU features just from one
perspective may not capture the rich contextual information between regional
and global face features, as well as the detailed variability across AUs,
because of the diversity in expression and individual characteristics. In this
paper, we propose a novel Multi-level Graph Relational Reasoning Network
(termed MGRR-Net) for facial AU detection. Each layer of MGRR-Net performs a
multi-level (i.e., region-level, pixel-wise and channel-wise level) feature
learning. While the region-level feature learning from local face patches
features via graph neural network can encode the correlation across different
AUs, the pixel-wise and channel-wise feature learning via graph attention
network can enhance the discrimination ability of AU features from global face
features. The fused features from the three levels lead to improved AU
discriminative ability. Extensive experiments on DISFA and BP4D AU datasets
show that the proposed approach achieves superior performance than the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Quasi-AutoRegression: Forecasting the visual popularity of new fashion products. (arXiv:2204.04014v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04014">
<div class="article-summary-box-inner">
<span><p>Estimating the preferences of consumers is of utmost importance for the
fashion industry as appropriately leveraging this information can be beneficial
in terms of profit. Trend detection in fashion is a challenging task due to the
fast pace of change in the fashion industry. Moreover, forecasting the visual
popularity of new garment designs is even more demanding due to lack of
historical data. To this end, we propose MuQAR, a Multimodal
Quasi-AutoRegressive deep learning architecture that combines two modules: (1)
a multi-modal multi-layer perceptron processing categorical, visual and textual
features of the product and (2) a quasi-autoregressive neural network modelling
the "target" time series of the product's attributes along with the "exogenous"
time series of all other attributes. We utilize computer vision, image
classification and image captioning, for automatically extracting visual
features and textual descriptions from the images of new products. Product
design in fashion is initially expressed visually and these features represent
the products' unique characteristics without interfering with the creative
process of its designers by requiring additional inputs (e.g manually written
texts). We employ the product's target attributes time series as a proxy of
temporal popularity patterns, mitigating the lack of historical data, while
exogenous time series help capture trends among interrelated attributes. We
perform an extensive ablation analysis on two large scale image fashion
datasets, Mallzee and SHIFT15m to assess the adequacy of MuQAR and also use the
Amazon Reviews: Home and Kitchen dataset to assess generalisability to other
domains. A comparative study on the VISUELLE dataset, shows that MuQAR is
capable of competing and surpassing the domain's current state of the art by
4.65% and 4.8% in terms of WAPE and MAE respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XMP-Font: Self-Supervised Cross-Modality Pre-training for Few-Shot Font Generation. (arXiv:2204.05084v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05084">
<div class="article-summary-box-inner">
<span><p>Generating a new font library is a very labor-intensive and time-consuming
job for glyph-rich scripts. Few-shot font generation is thus required, as it
requires only a few glyph references without fine-tuning during test. Existing
methods follow the style-content disentanglement paradigm and expect novel
fonts to be produced by combining the style codes of the reference glyphs and
the content representations of the source. However, these few-shot font
generation methods either fail to capture content-independent style
representations, or employ localized component-wise style representations,
which is insufficient to model many Chinese font styles that involve
hyper-component features such as inter-component spacing and
"connected-stroke". To resolve these drawbacks and make the style
representations more reliable, we propose a self-supervised cross-modality
pre-training strategy and a cross-modality transformer-based encoder that is
conditioned jointly on the glyph image and the corresponding stroke labels. The
cross-modality encoder is pre-trained in a self-supervised manner to allow
effective capture of cross- and intra-modality correlations, which facilitates
the content-style disentanglement and modeling style representations of all
scales (stroke-level, component-level and character-level). The pre-trained
encoder is then applied to the downstream font generation task without
fine-tuning. Experimental comparisons of our method with state-of-the-art
methods demonstrate our method successfully transfers styles of all scales. In
addition, it only requires one reference glyph and achieves the lowest rate of
bad cases in the few-shot font generation task 28% lower than the second best
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid ISTA: Unfolding ISTA With Convergence Guarantees Using Free-Form Deep Neural Networks. (arXiv:2204.11640v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11640">
<div class="article-summary-box-inner">
<span><p>It is promising to solve linear inverse problems by unfolding iterative
algorithms (e.g., iterative shrinkage thresholding algorithm (ISTA)) as deep
neural networks (DNNs) with learnable parameters. However, existing ISTA-based
unfolded algorithms restrict the network architectures for iterative updates
with the partial weight coupling structure to guarantee convergence. In this
paper, we propose hybrid ISTA to unfold ISTA with both pre-computed and learned
parameters by incorporating free-form DNNs (i.e., DNNs with arbitrary feasible
and reasonable network architectures), while ensuring theoretical convergence.
We first develop HCISTA to improve the efficiency and flexibility of classical
ISTA (with pre-computed parameters) without compromising the convergence rate
in theory. Furthermore, the DNN-based hybrid algorithm is generalized to
popular variants of learned ISTA, dubbed HLISTA, to enable a free architecture
of learned parameters with a guarantee of linear convergence. To our best
knowledge, this paper is the first to provide a convergence-provable framework
that enables free-form DNNs in ISTA-based unfolded algorithms. This framework
is general to endow arbitrary DNNs for solving linear inverse problems with
convergence guarantees. Extensive experiments demonstrate that hybrid ISTA can
reduce the reconstruction error with an improved convergence rate in the tasks
of sparse recovery and compressive sensing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Logit Adjustment. (arXiv:2204.11822v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11822">
<div class="article-summary-box-inner">
<span><p>Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses
challenges in recognizing novel classes in the test phase. The development of
generative models enables current GZSL techniques to probe further into the
semantic-visual link, culminating in a two-stage form that includes a generator
and a classifier. However, existing generation-based methods focus on enhancing
the generator's effect while neglecting the improvement of the classifier. In
this paper, we first analyze of two properties of the generated pseudo unseen
samples: bias and homogeneity. Then, we perform variational Bayesian inference
to back-derive the evaluation metrics, which reflects the balance of the seen
and unseen classes. As a consequence of our derivation, the aforementioned two
properties are incorporated into the classifier training as seen-unseen priors
via logit adjustment. The Zero-Shot Logit Adjustment further puts
semantic-based classifiers into effect in generation-based GZSL. Our
experiments demonstrate that the proposed technique achieves state-of-the-art
when combined with the basic generator, and it can improve various generative
Zero-Shot Learning frameworks. Our codes are available on
https://github.com/cdb342/IJCAI-2022-ZLA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Reasoning with Spatial-temporal Representation Learning: A Prospective Study. (arXiv:2204.12037v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12037">
<div class="article-summary-box-inner">
<span><p>Spatial-temporal representation learning is ubiquitous in various real-world
applications, including visual comprehension, video understanding, multi-modal
analysis, human-computer interaction, and urban computing. Due to the emergence
of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal
data in big data era, the lack of interpretability, robustness, and
out-of-distribution generalization are becoming the challenges of the existing
visual models. The majority of the existing methods tend to fit the original
data/variable distributions and ignore the essential causal relations behind
the multi-modal knowledge, which lacks an unified guidance and analysis about
why modern spatial-temporal representation learning methods are easily collapse
into data bias and have limited generalization and cognitive abilities.
Inspired by the strong inference ability of human-level agents, recent years
have therefore witnessed great effort in developing causal reasoning paradigms
to realize robust representation and model learning with good cognitive
ability. In this paper, we conduct a comprehensive review of existing causal
reasoning methods for spatial-temporal representation learning, covering
fundamental theories, models, and datasets. The limitations of current methods
and datasets are also discussed. Moreover, we propose some primary challenges,
opportunities, and future research directions for benchmarking causal reasoning
algorithms in spatial-temporal representation learning. This paper aims to
provide a comprehensive overview of this emerging field, attract attention,
encourage discussions, bring to the forefront the urgency of developing novel
causal reasoning methods, publicly available benchmarks, and consensus-building
standards for reliable spatial-temporal representation learning and related
real-world applications more efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cost-Aware Evaluation and Model Scaling for LiDAR-Based 3D Object Detection. (arXiv:2205.01142v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01142">
<div class="article-summary-box-inner">
<span><p>Considerable research efforts have been devoted to LiDAR-based 3D object
detection and its empirical performance has been significantly improved. While
the progress has been encouraging, we observe an overlooked issue: it is not
yet common practice to compare different 3D detectors under the same cost,
e.g., inference latency. This makes it difficult to quantify the true
performance gain brought by recently proposed architecture designs. The goal of
this work is to conduct a cost-aware evaluation of LiDAR-based 3D object
detectors. Specifically, we focus on SECOND, a simple grid-based one-stage
detector, and analyze its performance under different costs by scaling its
original architecture. Then we compare the family of scaled SECOND with recent
3D detection methods, such as Voxel R-CNN and PV-RCNN++. The results are
surprising. We find that, if allowed to use the same latency, SECOND can match
the performance of PV-RCNN++, the current state-of-the-art method on the Waymo
Open Dataset. Scaled SECOND also easily outperforms many recent 3D detection
methods published during the past year. We recommend future research control
the inference cost in their empirical comparison and include the family of
scaled SECOND as a strong baseline when presenting novel 3D detection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The scope for AI-augmented interpretation of building blueprints in commercial and industrial property insurance. (arXiv:2205.01671v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01671">
<div class="article-summary-box-inner">
<span><p>This report, commissioned by the WTW research network, investigates the use
of AI in property risk assessment. It (i) reviews existing work on risk
assessment in commercial and industrial properties and automated information
extraction from building blueprints; and (ii) presents an exploratory 'proof-of
concept-solution' exploring the feasibility of using machine learning for the
automated extraction of information from building blueprints to support
insurance risk assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">i-Code: An Integrative and Composable Multimodal Learning Framework. (arXiv:2205.01818v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01818">
<div class="article-summary-box-inner">
<span><p>Human intelligence is multimodal; we integrate visual, linguistic, and
acoustic signals to maintain a holistic worldview. Most current pretraining
methods, however, are limited to one or two modalities. We present i-Code, a
self-supervised pretraining framework where users may flexibly combine the
modalities of vision, speech, and language into unified and general-purpose
vector representations. In this framework, data from each modality are first
given to pretrained single-modality encoders. The encoder outputs are then
integrated with a multimodal fusion network, which uses novel attention
mechanisms and other architectural innovations to effectively combine
information from the different modalities. The entire system is pretrained
end-to-end with new objectives including masked modality unit modeling and
cross-modality contrastive learning. Unlike previous research using only video
for pretraining, the i-Code framework can dynamically process single, dual, and
triple-modality data during training and inference, flexibly projecting
different combinations of modalities into a single representation space.
Experimental results demonstrate how i-Code can outperform state-of-the-art
techniques on five video understanding tasks and the GLUE NLP benchmark,
improving by as much as 11% and demonstrating the power of integrative
multimodal pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANUBIS: Review and Benchmark Skeleton-Based Action Recognition Methods with a New Dataset. (arXiv:2205.02071v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02071">
<div class="article-summary-box-inner">
<span><p>Skeleton-based action recognition, as a subarea of action recognition, is
swiftly accumulating attention and popularity. The task is to recognize actions
performed by human articulation points. Compared with other data modalities, 3D
human skeleton representations have extensive unique desirable characteristics,
including succinctness, robustness, racial-impartiality, and many more. We aim
to provide a roadmap for new and existing researchers a on the landscapes of
skeleton-based action recognition for new and existing researchers. To this
end, we present a review in the form of a taxonomy on existing works of
skeleton-based action recognition. We partition them into four major
categories: (1) datasets; (2) extracting spatial features; (3) capturing
temporal patterns; (4) improving signal quality. For each method, we provide
concise yet informatively-sufficient descriptions. To promote more fair and
comprehensive evaluation on existing approaches of skeleton-based action
recognition, we collect ANUBIS, a large-scale human skeleton dataset. Compared
with previously collected dataset, ANUBIS are advantageous in the following
four aspects: (1) employing more recently released sensors; (2) containing
novel back view; (3) encouraging high enthusiasm of subjects; (4) including
actions of the COVID pandemic era. Using ANUBIS, we comparably benchmark
performance of current skeleton-based action recognizers. At the end of this
paper, we outlook future development of skeleton-based action recognition by
listing several new technical problems. We believe they are valuable to solve
in order to commercialize skeleton-based action recognition in the near future.
The dataset of ANUBIS is available at:
<a href="http://hcc-workshop.anu.edu.au/webs/anu101/home.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Extrapolation in Space and Time. (arXiv:2205.02084v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02084">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis (NVS) and video prediction (VP) are typically considered
disjoint tasks in computer vision. However, they can both be seen as ways to
observe the spatial-temporal world: NVS aims to synthesize a scene from a new
point of view, while VP aims to see a scene from a new point of time. These two
tasks provide complementary signals to obtain a scene representation, as
viewpoint changes from spatial observations inform depth, and temporal
observations inform the motion of cameras and individual objects. Inspired by
these observations, we propose to study the problem of Video Extrapolation in
Space and Time (VEST). We propose a model that leverages the self-supervision
and the complementary cues from both tasks, while existing methods can only
solve one of them. Experiments show that our method achieves performance better
than or comparable to several state-of-the-art NVS and VP methods on indoor and
outdoor real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuroevolutionary Multi-objective approaches to Trajectory Prediction in Autonomous Vehicles. (arXiv:2205.02105v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02105">
<div class="article-summary-box-inner">
<span><p>The incentive for using Evolutionary Algorithms (EAs) for the automated
optimization and training of deep neural networks (DNNs), a process referred to
as neuroevolution, has gained momentum in recent years. The configuration and
training of these networks can be posed as optimization problems. Indeed, most
of the recent works on neuroevolution have focused their attention on
single-objective optimization. Moreover, from the little research that has been
done at the intersection of neuroevolution and evolutionary multi-objective
optimization (EMO), all the research that has been carried out has focused
predominantly on the use of one type of DNN: convolutional neural networks
(CNNs), using well-established standard benchmark problems such as MNIST. In
this work, we make a leap in the understanding of these two areas
(neuroevolution and EMO), regarded in this work as neuroevolutionary
multi-objective, by using and studying a rich DNN composed of a CNN and
Long-short Term Memory network. Moreover, we use a robust and challenging
vehicle trajectory prediction problem. By using the well-known Non-dominated
Sorting Genetic Algorithm-II, we study the effects of five different
objectives, tested in categories of three, allowing us to show how these
objectives have either a positive or detrimental effect in neuroevolution for
trajectory prediction in autonomous vehicles.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-07 23:08:12.841639797 UTC">2022-05-07 23:08:12 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>