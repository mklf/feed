<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-10T01:30:00Z">05-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it. (arXiv:2205.03472v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03472">
<div class="article-summary-box-inner">
<span><p>Understanding longer narratives or participating in conversations requires
tracking of discourse entities that have been mentioned. Indefinite noun
phrases (NPs), such as 'a dog', frequently introduce discourse entities but
this behavior is modulated by sentential operators such as negation. For
example, 'a dog' in 'Arthur doesn't own a dog' does not introduce a discourse
entity due to the presence of negation. In this work, we adapt the
psycholinguistic assessment of language models paradigm to higher-level
linguistic phenomena and introduce an English evaluation suite that targets the
knowledge of the interactions between sentential operators and indefinite NPs.
We use this evaluation suite for a fine-grained investigation of the entity
tracking abilities of the Transformer-based models GPT-2 and GPT-3. We find
that while the models are to a certain extent sensitive to the interactions we
investigate, they are all challenged by the presence of multiple NPs and their
behavior is not systematic, which suggests that even models at the scale of
GPT-3 do not fully acquire basic entity tracking abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Intent Classification in the Legal Domain. (arXiv:2205.03509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03509">
<div class="article-summary-box-inner">
<span><p>A law practitioner has to go through a lot of long legal case proceedings. To
understand the motivation behind the actions of different parties/individuals
in a legal case, it is essential that the parts of the document that express an
intent corresponding to the case be clearly understood. In this paper, we
introduce a dataset of 93 legal documents, belonging to the case categories of
either Murder, Land Dispute, Robbery, or Corruption, where phrases expressing
intent same as the category of the document are annotated. Also, we annotate
fine-grained intents for each such phrase to enable a deeper understanding of
the case for a reader. Finally, we analyze the performance of several
transformer-based models in automating the process of extracting intent phrases
(both at a coarse and a fine-grained level), and classifying a document into
one of the possible 4 categories, and observe that, our dataset is challenging,
especially in the case of fine-grained intent classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CORWA: A Citation-Oriented Related Work Annotation Dataset. (arXiv:2205.03512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03512">
<div class="article-summary-box-inner">
<span><p>Academic research is an exploratory activity to discover new solutions to
problems. By this nature, academic research works perform literature reviews to
distinguish their novelties from prior work. In natural language processing,
this literature review is usually conducted under the "Related Work" section.
The task of related work generation aims to automatically generate the related
work section given the rest of the research paper and a list of papers to cite.
Prior work on this task has focused on the sentence as the basic unit of
generation, neglecting the fact that related work sections consist of variable
length text fragments derived from different information sources. As a first
step toward a linguistically-motivated related work generation framework, we
present a Citation Oriented Related Work Annotation (CORWA) dataset that labels
different types of citation text fragments from different information sources.
We train a strong baseline model that automatically tags the CORWA labels on
massive unlabeled related work section texts. We further suggest a novel
framework for human-in-the-loop, iterative, abstractive related work
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction. (arXiv:2205.03521v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03521">
<div class="article-summary-box-inner">
<span><p>Multimodal named entity recognition and relation extraction (MNER and MRE) is
a fundamental and crucial branch in information extraction. However, existing
approaches for MNER and MRE usually suffer from error sensitivity when
irrelevant object images incorporated in texts. To deal with these issues, we
propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for
visual-enhanced entity and relation extraction, aiming to achieve more
effective and robust performance. Specifically, we regard visual representation
as pluggable visual prefix to guide the textual representation for error
insensitive forecasting decision. We further propose a dynamic gated
aggregation strategy to achieve hierarchical multi-scaled visual features as
visual prefix for fusion. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our method, and achieve state-of-the-art
performance. Code is available in https://github.com/zjunlp/HVPNeT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attract me to Buy: Advertisement Copywriting Generation with Multimodal Multi-structured Information. (arXiv:2205.03534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03534">
<div class="article-summary-box-inner">
<span><p>Recently, online shopping has gradually become a common way of shopping for
people all over the world. Wonderful merchandise advertisements often attract
more people to buy. These advertisements properly integrate multimodal
multi-structured information of commodities, such as visual spatial information
and fine-grained structure information. However, traditional multimodal text
generation focuses on the conventional description of what existed and
happened, which does not match the requirement of advertisement copywriting in
the real world. Because advertisement copywriting has a vivid language style
and higher requirements of faithfulness. Unfortunately, there is a lack of
reusable evaluation frameworks and a scarcity of datasets. Therefore, we
present a dataset, E-MMAD (e-commercial multimodal multi-structured
advertisement copywriting), which requires, and supports much more detailed
information in text generation. Noticeably, it is one of the largest video
captioning datasets in this field. Accordingly, we propose a baseline method
and faithfulness evaluation metric on the strength of structured information
reasoning to solve the demand in reality on this dataset. It surpasses the
previous methods by a large margin on all metrics. The dataset and method are
coming soon on \url{https://e-mmad.github.io/e-mmad.net/index.html}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CogIntAc: Modeling the Relationships between Intention, Emotion and Action in Interactive Process from Cognitive Perspective. (arXiv:2205.03540v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03540">
<div class="article-summary-box-inner">
<span><p>Intention, emotion and action are important psychological factors in human
activities, which play an important role in the interaction between
individuals. How to model the interaction process between individuals by
analyzing the relationship of their intentions, emotions, and actions at the
cognitive level is challenging. In this paper, we propose a novel cognitive
framework of individual interaction. The core of the framework is that
individuals achieve interaction through external action driven by their inner
intention. Based on this idea, the interactions between individuals can be
constructed by establishing relationships between the intention, emotion and
action. Furthermore, we conduct analysis on the interaction between individuals
and give a reasonable explanation for the predicting results. To verify the
effectiveness of the framework, we reconstruct a dataset and propose three
tasks as well as the corresponding baseline models, including action abduction,
emotion prediction and action generation. The novel framework shows an
interesting perspective on mimicking the mental state of human beings in
cognitive science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SubGraph Networks based Entity Alignment for Cross-lingual Knowledge Graph. (arXiv:2205.03557v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03557">
<div class="article-summary-box-inner">
<span><p>Entity alignment is the task of finding entities representing the same
real-world object in two knowledge graphs(KGs). Cross-lingual knowledge graph
entity alignment aims to discover the cross-lingual links in the multi-language
KGs, which is of great significance to the NLP applications and multi-language
KGs fusion. In the task of aligning cross-language knowledge graphs, the
structures of the two graphs are very similar, and the equivalent entities
often have the same subgraph structure characteristics. The traditional GCN
method neglects to obtain structural features through representative parts of
the original graph and the use of adjacency matrix is not enough to effectively
represent the structural features of the graph. In this paper, we introduce the
subgraph network (SGN) method into the GCN-based cross-lingual KG entity
alignment method. In the method, we extracted the first-order subgraphs of the
KGs to expand the structural features of the original graph to enhance the
representation ability of the entity embedding and improve the alignment
accuracy. Experiments show that the proposed method outperforms the
state-of-the-art GCN-based method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Number Entity Recognition. (arXiv:2205.03559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03559">
<div class="article-summary-box-inner">
<span><p>Numbers are essential components of text, like any other word tokens, from
which natural language processing (NLP) models are built and deployed. Though
numbers are typically not accounted for distinctly in most NLP tasks, there is
still an underlying amount of numeracy already exhibited by NLP models. In this
work, we attempt to tap this potential of state-of-the-art NLP models and
transfer their ability to boost performance in related tasks. Our proposed
classification of numbers into entities helps NLP models perform well on
several tasks, including a handcrafted Fill-In-The-Blank (FITB) task and on
question answering using joint embeddings, outperforming the BERT and RoBERTa
baseline classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Disentangled Textual Representations via Statistical Measures of Similarity. (arXiv:2205.03589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03589">
<div class="article-summary-box-inner">
<span><p>When working with textual data, a natural application of disentangled
representations is fair classification where the goal is to make predictions
without being biased (or influenced) by sensitive attributes that may be
present in the data (e.g., age, gender or race). Dominant approaches to
disentangle a sensitive attribute from textual representations rely on learning
simultaneously a penalization term that involves either an adversarial loss
(e.g., a discriminator) or an information measure (e.g., mutual information).
However, these methods require the training of a deep neural network with
several parameter updates for each update of the representation model. As a
matter of fact, the resulting nested optimization loop is both time consuming,
adding complexity to the optimization dynamic, and requires a fine
hyperparameter selection (e.g., learning rates, architecture). In this work, we
introduce a family of regularizers for learning disentangled representations
that do not require training. These regularizers are based on statistical
measures of similarity between the conditional probability distributions with
respect to the sensitive attributes. Our novel regularizers do not require
additional training, are faster and do not involve additional tuning while
achieving better results both when combined with pretrained and randomly
initialized text encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Computationally Feasible Deep Active Learning. (arXiv:2205.03598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03598">
<div class="article-summary-box-inner">
<span><p>Active learning (AL) is a prominent technique for reducing the annotation
effort required for training machine learning models. Deep learning offers a
solution for several essential obstacles to deploying AL in practice but
introduces many others. One of such problems is the excessive computational
resources required to train an acquisition model and estimate its uncertainty
on instances in the unlabeled pool. We propose two techniques that tackle this
issue for text classification and tagging tasks, offering a substantial
reduction of AL iteration duration and the computational overhead introduced by
deep acquisition models in AL. We also demonstrate that our algorithm that
leverages pseudo-labeling and distilled models overcomes one of the essential
obstacles revealed previously in the literature. Namely, it was shown that due
to differences between an acquisition model used to select instances during AL
and a successor model trained on the labeled data, the benefits of AL can
diminish. We show that our algorithm, despite using a smaller and faster
acquisition model, is capable of training a more expressive successor model
with higher performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniMorph 4.0: Universal Morphology. (arXiv:2205.03608v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03608">
<div class="article-summary-box-inner">
<span><p>The Universal Morphology (UniMorph) project is a collaborative effort
providing broad-coverage instantiated normalized morphological inflection
tables for hundreds of diverse world languages. The project comprises two major
thrusts: a language-independent feature schema for rich morphological
annotation and a type-level resource of annotated data in diverse languages
realizing that schema. This paper presents the expansions and improvements made
on several fronts over the last couple of years (since McCarthy et al. (2020)).
Collaborative efforts by numerous linguists have added 67 new languages,
including 30 endangered languages. We have implemented several improvements to
the extraction pipeline to tackle some issues, e.g. missing gender and macron
information. We have also amended the schema to use a hierarchical structure
that is needed for morphological phenomena like multiple-argument agreement and
case stacking, while adding some missing morphological features to make the
schema more inclusive. In light of the last UniMorph release, we also augmented
the database with morpheme segmentation for 16 languages. Lastly, this new
release makes a push towards inclusion of derivational morphology in UniMorph
by enriching the data and annotation schema with instances representing
derivational processes from MorphyNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding. (arXiv:2205.03656v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03656">
<div class="article-summary-box-inner">
<span><p>Although spoken language understanding (SLU) has achieved great success in
high-resource languages, such as English, it remains challenging in
low-resource languages mainly due to the lack of high quality training data.
The recent multilingual code-switching approach samples some words in an input
utterance and replaces them by expressions in some other languages of the same
meaning. The multilingual code-switching approach achieves better alignments of
representations across languages in zero-shot cross-lingual SLU. Surprisingly,
all existing multilingual code-switching methods disregard the inherent
semantic structure in SLU, i.e., most utterances contain one or more slots, and
each slot consists of one or more words. In this paper, we propose to exploit
the "utterance-slot-word" structure of SLU and systematically model this
structure by a multi-level contrastive learning framework at the utterance,
slot, and word levels. We develop novel code-switching schemes to generate hard
negative examples for contrastive learning at all levels. Furthermore, we
develop a label-aware joint model to leverage label semantics for cross-lingual
knowledge transfer. Our experimental results show that our proposed methods
significantly improve the performance compared with the strong baselines on two
zero-shot cross-lingual SLU benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vector Representations of Idioms in Conversational Systems. (arXiv:2205.03666v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03666">
<div class="article-summary-box-inner">
<span><p>We demonstrate, in this study, that an open-domain conversational system
trained on idioms or figurative language generates more fitting responses to
prompts containing idioms. Idioms are part of everyday speech in many
languages, across many cultures, but they pose a great challenge for many
Natural Language Processing (NLP) systems that involve tasks such as
Information Retrieval (IR) and Machine Translation (MT), besides conversational
AI. We utilize the Potential Idiomatic Expression (PIE)-English idioms corpus
for the two tasks that we investigate: classification and conversation
generation. We achieve state-of-the-art (SoTA) result of 98% macro F1 score on
the classification task by using the SoTA T5 model. We experiment with three
instances of the SoTA dialogue model, Dialogue Generative Pre-trained
Transformer (DialoGPT), for conversation generation. Their performances are
evaluated using the automatic metric perplexity and human evaluation. The
results show that the model trained on the idiom corpus generates more fitting
responses to prompts containing idioms 71.9% of the time, compared to a similar
model not trained on the idioms corpus. We contribute the model checkpoint/demo
and code on the HuggingFace hub for public access.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathetic Response Generation with State Management. (arXiv:2205.03676v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03676">
<div class="article-summary-box-inner">
<span><p>The goal of empathetic response generation is to enhance the ability of
dialogue systems to perceive and express emotions in conversations. Current
approaches to this task mainly focus on improving the response generation model
by recognizing the emotion of the user or predicting a target emotion to guide
the generation of responses. Such models only exploit partial information (the
user's emotion or the target emotion used as a guiding signal) and do not
consider multiple information together. In addition to the emotional style of
the response, the intent of the response is also very important for empathetic
responding. Thus, we propose a novel empathetic response generation model that
can consider multiple state information including emotions and intents
simultaneously. Specifically, we introduce a state management method to
dynamically update the dialogue states, in which the user's emotion is first
recognized, then the target emotion and intent are obtained via predefined
shift patterns with the user's emotion as input. The obtained information is
used to control the response generation. Experimental results show that
dynamically managing different information can help the model generate more
empathetic responses compared with several baselines under both automatic and
human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Retrieval May Not Lead to Better Question Answering. (arXiv:2205.03685v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03685">
<div class="article-summary-box-inner">
<span><p>Considerable progress has been made recently in open-domain question
answering (QA) problems, which require Information Retrieval (IR) and Reading
Comprehension (RC). A popular approach to improve the system's performance is
to improve the quality of the retrieved context from the IR stage. In this work
we show that for StrategyQA, a challenging open-domain QA dataset that requires
multi-hop reasoning, this common approach is surprisingly ineffective --
improving the quality of the retrieved context hardly improves the system's
performance. We further analyze the system's behavior to identify potential
reasons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Progression-Aware Autonomous Dialogue Agent. (arXiv:2205.03692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03692">
<div class="article-summary-box-inner">
<span><p>Recent advances in large-scale language modeling and generation have enabled
the creation of dialogue agents that exhibit human-like responses in a wide
range of conversational scenarios spanning a diverse set of tasks, from general
chit-chat to focused goal-oriented discourse. While these agents excel at
generating high-quality responses that are relevant to prior context, they
suffer from a lack of awareness of the overall direction in which the
conversation is headed, and the likelihood of task success inherent therein.
Thus, we propose a framework in which dialogue agents can evaluate the
progression of a conversation toward or away from desired outcomes, and use
this signal to inform planning for subsequent responses. Our framework is
composed of three key elements: (1) the notion of a "global" dialogue state
(GDS) space, (2) a task-specific progression function (PF) computed in terms of
a conversation's trajectory through this space, and (3) a planning mechanism
based on dialogue rollouts by which an agent may use progression signals to
select its next response.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AKI-BERT: a Pre-trained Clinical Language Model for Early Prediction of Acute Kidney Injury. (arXiv:2205.03695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03695">
<div class="article-summary-box-inner">
<span><p>Acute kidney injury (AKI) is a common clinical syndrome characterized by a
sudden episode of kidney failure or kidney damage within a few hours or a few
days. Accurate early prediction of AKI for patients in ICU who are more likely
than others to have AKI can enable timely interventions, and reduce the
complications of AKI. Much of the clinical information relevant to AKI is
captured in clinical notes that are largely unstructured text and requires
advanced natural language processing (NLP) for useful information extraction.
On the other hand, pre-trained contextual language models such as Bidirectional
Encoder Representations from Transformers (BERT) have improved performances for
many NLP tasks in general domain recently. However, few have explored BERT on
disease-specific medical domain tasks such as AKI early prediction. In this
paper, we try to apply BERT to specific diseases and present an AKI
domain-specific pre-trained language model based on BERT (AKI-BERT) that could
be used to mine the clinical notes for early prediction of AKI. AKI-BERT is a
BERT model pre-trained on the clinical notes of patients having risks for AKI.
Our experiments on Medical Information Mart for Intensive Care III (MIMIC-III)
dataset demonstrate that AKI-BERT can yield performance improvements for early
AKI prediction, thus expanding the utility of the BERT model from general
clinical domain to disease-specific domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. (arXiv:2205.03720v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03720">
<div class="article-summary-box-inner">
<span><p>The massive amount of trainable parameters in the pre-trained language models
(PLMs) makes them hard to be deployed to multiple downstream tasks. To address
this issue, parameter-efficient transfer learning methods have been proposed to
tune only a few parameters during fine-tuning while freezing the rest. This
paper looks at existing methods along this line through the \textit{kernel
lens}. Motivated by the connection between self-attention in transformer-based
PLMs and kernel learning, we propose \textit{kernel-wise adapters}, namely
\textit{Kernel-mix}, that utilize the kernel structure in self-attention to
guide the assignment of the tunable parameters. These adapters use guidelines
found in classical kernel learning and enable separate parameter tuning for
each attention head. Our empirical results, over a diverse set of natural
language generation and understanding tasks, show that our proposed adapters
can attain or improve the strong performance of existing baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DxFormer: A Decoupled Automatic Diagnostic System Based on Decoder-Encoder Transformer with Dense Symptom Representations. (arXiv:2205.03755v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03755">
<div class="article-summary-box-inner">
<span><p>Diagnosis-oriented dialogue system queries the patient's health condition and
makes predictions about possible diseases through continuous interaction with
the patient. A few studies use reinforcement learning (RL) to learn the optimal
policy from the joint action space of symptoms and diseases. However, existing
RL (or Non-RL) methods cannot achieve sufficiently good prediction accuracy,
still far from its upper limit. To address the problem, we propose a decoupled
automatic diagnostic framework DxFormer, which divides the diagnosis process
into two steps: symptom inquiry and disease diagnosis, where the transition
from symptom inquiry to disease diagnosis is explicitly determined by the
stopping criteria. In DxFormer, we treat each symptom as a token, and formalize
the symptom inquiry and disease diagnosis to a language generation model and a
sequence classification model respectively. We use the inverted version of
Transformer, i.e., the decoder-encoder structure, to learn the representation
of symptoms by jointly optimizing the reinforce reward and cross entropy loss.
Extensive experiments on three public real-world datasets prove that our
proposed model can effectively learn doctors' clinical experience and achieve
the state-of-the-art results in terms of symptom recall and diagnostic
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scheduled Multi-task Learning for Neural Chat Translation. (arXiv:2205.03766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03766">
<div class="article-summary-box-inner">
<span><p>Neural Chat Translation (NCT) aims to translate conversational text into
different languages. Existing methods mainly focus on modeling the bilingual
dialogue characteristics (e.g., coherence) to improve chat translation via
multi-task learning on small-scale chat translation data. Although the NCT
models have achieved impressive success, it is still far from satisfactory due
to insufficient chat translation data and simple joint training manners. To
address the above issues, we propose a scheduled multi-task learning framework
for NCT. Specifically, we devise a three-stage training framework to
incorporate the large-scale in-domain chat translation data into training by
adding a second pre-training stage between the original pre-training and
fine-tuning stages. Further, we investigate where and how to schedule the
dialogue-related auxiliary tasks in multiple training stages to effectively
enhance the main chat translation task. Extensive experiments in four language
directions (English-Chinese and English-German) verify the effectiveness and
superiority of the proposed approach. Additionally, we have made the
large-scale in-domain paired bilingual dialogue dataset publicly available to
the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Aware Abbreviation Expansion Using Large Language Models. (arXiv:2205.03767v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03767">
<div class="article-summary-box-inner">
<span><p>Motivated by the need for accelerating text entry in augmentative and
alternative communication (AAC) for people with severe motor impairments, we
propose a paradigm in which phrases are abbreviated aggressively as primarily
word-initial letters. Our approach is to expand the abbreviations into
full-phrase options by leveraging conversation context with the power of
pretrained large language models (LLMs). Through zero-shot, few-shot, and
fine-tuning experiments on four public conversation datasets, we show that for
replies to the initial turn of a dialog, an LLM with 64B parameters is able to
exactly expand over 70 of phrases with abbreviation length up to 10, leading to
an effective keystroke saving rate of up to about 77 on these exact expansions.
Including a small amount of context in the form of a single conversation turn
more than doubles abbreviation expansion accuracies compared to having no
context, an effect that is more pronounced for longer phrases. Additionally,
the robustness of models against typo noise can be enhanced through fine-tuning
on noisy data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Math-KG: Construction and Applications of Mathematical Knowledge Graph. (arXiv:2205.03772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03772">
<div class="article-summary-box-inner">
<span><p>Recently, the explosion of online education platforms makes a success in
encouraging us to easily access online education resources. However, most of
them ignore the integration of massive unstructured information, which
inevitably brings the problem of \textit{information overload} and
\textit{knowledge trek}. In this paper, we proposed a mathematical knowledge
graph named Math-KG, which automatically constructed by the pipeline method
with the natural language processing technology to integrate the resources of
the mathematics. It is built from the corpora of Baidu Baike, Wikipedia. We
implement a simple application system to validate the proposed Math-KG can make
contributions on a series of scenes, including faults analysis and semantic
search. The system is publicly available at GitHub
\footnote{\url{https://github.com/wjn1996/Mathematical-Knowledge-Entity-Recognition}.}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis. (arXiv:2205.03784v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03784">
<div class="article-summary-box-inner">
<span><p>Recent literature focuses on utilizing the entity information in the
sentence-level relation extraction (RE), but this risks leaking superficial and
spurious clues of relations. As a result, RE still suffers from unintended
entity bias, i.e., the spurious correlation between entity mentions (names) and
relations. Entity bias can mislead the RE models to extract the relations that
do not exist in the text. To combat this issue, some previous work masks the
entity mentions to prevent the RE models from overfitting entity mentions.
However, this strategy degrades the RE performance because it loses the
semantic information of entities. In this paper, we propose the CORE
(Counterfactual Analysis based Relation Extraction) debiasing method that
guides the RE models to focus on the main effects of textual context without
losing the entity information. We first construct a causal graph for RE, which
models the dependencies between variables in RE models. Then, we propose to
conduct counterfactual analysis on our causal graph to distill and mitigate the
entity bias, that captures the causal effects of specific entity mentions in
each instance. Note that our CORE method is model-agnostic to debias existing
RE systems during inference without changing their training processes.
Extensive experimental results demonstrate that our CORE yields significant
gains on both effectiveness and generalization for RE. The source code is
provided at: https://github.com/vanoracai/CoRE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRAPHCACHE: Message Passing as Caching for Sentence-Level Relation Extraction. (arXiv:2205.03786v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03786">
<div class="article-summary-box-inner">
<span><p>Entity types and textual context are essential properties for sentence-level
relation extraction (RE). Existing work only encodes these properties within
individual instances, which limits the performance of RE given the insufficient
features in a single sentence. In contrast, we model these properties from the
whole dataset and use the dataset-level information to enrich the semantics of
every instance. We propose the GRAPHCACHE (Graph Neural Network as Caching)
module, that propagates the features across sentences to learn better
representations for RE. GRAPHCACHE aggregates the features from sentences in
the whole dataset to learn global representations of properties, and use them
to augment the local features within individual sentences. The global property
features act as dataset-level prior knowledge for RE, and a complement to the
sentence-level features. Inspired by the classical caching technique in
computer systems, we develop GRAPHCACHE to update the property representations
in an online manner. Overall, GRAPHCACHE yields significant effectiveness gains
on RE and enables efficient message passing across all sentences in the
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Domain Targeted Sentiment Analysis. (arXiv:2205.03804v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03804">
<div class="article-summary-box-inner">
<span><p>Targeted Sentiment Analysis (TSA) is a central task for generating insights
from consumer reviews. Such content is extremely diverse, with sites like
Amazon or Yelp containing reviews on products and businesses from many
different domains. A real-world TSA system should gracefully handle that
diversity. This can be achieved by a multi-domain model -- one that is robust
to the domain of the analyzed texts, and performs well on various domains. To
address this scenario, we present a multi-domain TSA system based on augmenting
a given training set with diverse weak labels from assorted domains. These are
obtained through self-training on the Yelp reviews corpus. Extensive
experiments with our approach on three evaluation datasets across different
domains demonstrate the effectiveness of our solution. We further analyze how
restrictions imposed on the available labeled data affect the performance, and
compare the proposed method to the costly alternative of manually gathering
diverse TSA labeled data. Our results and analysis show that our approach is a
promising step towards a practical domain-robust TSA system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence. (arXiv:2205.03815v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03815">
<div class="article-summary-box-inner">
<span><p>The logical negation property (LNP), which implies generating different
predictions for semantically opposite inputs, is an important property that a
trustworthy language model must satisfy. However, much recent evidence shows
that large-size pre-trained language models (PLMs) do not satisfy this
property. In this paper, we perform experiments using probing tasks to assess
PLM's LNP understanding. Unlike previous studies that only examined negation
expressions, we expand the boundary of the investigation to lexical semantics.
Through experiments, we observe that PLMs violate the LNP frequently. To
alleviate the issue, we propose a novel intermediate training task, names
meaning-matching, designed to directly learn a meaning-text correspondence,
instead of relying on the distributional hypothesis. Through multiple
experiments, we find that the task enables PLMs to learn lexical semantic
information. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm
that it is a safe intermediate task that guarantees a similar or better
performance of downstream tasks. Finally, we observe that our proposed approach
outperforms our previous counterparts despite its time and resource efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Use of BERT for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation. (arXiv:2205.03835v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03835">
<div class="article-summary-box-inner">
<span><p>In recent years, pre-trained models have become dominant in most natural
language processing (NLP) tasks. However, in the area of Automated Essay
Scoring (AES), pre-trained models such as BERT have not been properly used to
outperform other deep learning models such as LSTM. In this paper, we introduce
a novel multi-scale essay representation for BERT that can be jointly learned.
We also employ multiple losses and transfer learning from out-of-domain essays
to further improve the performance. Experiment results show that our approach
derives much benefit from joint learning of multi-scale essay representation
and obtains almost the state-of-the-art result among all deep learning models
in the ASAP task. Our multi-scale essay representation also generalizes well to
CommonLit Readability Prize data set, which suggests that the novel text
representation proposed in this paper may be a new and effective choice for
long-text tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assigning Species Information to Corresponding Genes by a Sequence Labeling Framework. (arXiv:2205.03853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03853">
<div class="article-summary-box-inner">
<span><p>The automatic assignment of species information to the corresponding genes in
a research article is a critically important step in the gene normalization
task, whereby a gene mention is normalized and linked to a database record or
identifier by a text-mining algorithm. Existing methods typically rely on
heuristic rules based on gene and species co-occurrence in the article, but
their accuracy is suboptimal. We therefore developed a high-performance method,
using a novel deep learning-based framework, to classify whether there is a
relation between a gene and a species. Instead of the traditional binary
classification framework in which all possible pairs of genes and species in
the same article are evaluated, we treat the problem as a sequence-labeling
task such that only a fraction of the pairs needs to be considered. Our
benchmarking results show that our approach obtains significantly higher
performance compared to that of the rule-based baseline method for the species
assignment task (from 65.8% to 81.3% in accuracy). The source code and data for
species assignment are freely available at
https://github.com/ncbi/SpeciesAssignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's the Same Old Story! Enriching Event-Centric Knowledge Graphs by Narrative Aspects. (arXiv:2205.03876v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03876">
<div class="article-summary-box-inner">
<span><p>Our lives are ruled by events of varying importance ranging from simple
everyday occurrences to incidents of societal dimension. And a lot of effort is
taken to exchange information and discuss about such events: generally
speaking, stringent narratives are formed to reduce complexity. But when
considering complex events like the current conflict between Russia and Ukraine
it is easy to see that those events cannot be grasped by objective facts alone,
like the start of the conflict or respective troop sizes. There are different
viewpoints and assessments to consider, a different understanding of the roles
taken by individual participants, etc. So how can such subjective and
viewpoint-dependent information be effectively represented together with all
objective information? Recently event-centric knowledge graphs have been
proposed for objective event representation in the otherwise primarily
entity-centric domain of knowledge graphs. In this paper we introduce a novel
and lightweight structure for event-centric knowledge graphs, which for the
first time allows for queries incorporating viewpoint-dependent and narrative
aspects. Our experiments prove the effective incorporation of subjective
attributions for event participants and show the benefits of specifically
tailored indexes for narrative query processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MASALA: Modelling and Analysing the Semantics of Adpositions in Linguistic Annotation of Hindi. (arXiv:2205.03955v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03955">
<div class="article-summary-box-inner">
<span><p>We present a completed, publicly available corpus of annotated semantic
relations of adpositions and case markers in Hindi. We used the multilingual
SNACS annotation scheme, which has been applied to a variety of typologically
diverse languages. Building on past work examining linguistic problems in SNACS
annotation, we use language models to attempt automatic labelling of SNACS
supersenses in Hindi and achieve results competitive with past work on English.
We look towards upstream applications in semantic role labelling and extension
to related languages such as Gujarati.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chart Question Answering: State of the Art and Future Directions. (arXiv:2205.03966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03966">
<div class="article-summary-box-inner">
<span><p>Information visualizations such as bar charts and line charts are very common
for analyzing data and discovering critical insights. Often people analyze
charts to answer questions that they have in mind. Answering such questions can
be challenging as they often require a significant amount of perceptual and
cognitive effort. Chart Question Answering (CQA) systems typically take a chart
and a natural language question as input and automatically generate the answer
to facilitate visual data analysis. Over the last few years, there has been a
growing body of literature on the task of CQA. In this survey, we
systematically review the current state-of-the-art research focusing on the
problem of chart question answering. We provide a taxonomy by identifying
several important dimensions of the problem domain including possible inputs
and outputs of the task and discuss the advantages and limitations of proposed
solutions. We then summarize various evaluation techniques used in the surveyed
papers. Finally, we outline the open challenges and future research
opportunities related to chart question answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning. (arXiv:2205.03972v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03972">
<div class="article-summary-box-inner">
<span><p>Controlled table-to-text generation seeks to generate natural language
descriptions for highlighted subparts of a table. Previous SOTA systems still
employ a sequence-to-sequence generation method, which merely captures the
table as a linear structure and is brittle when table layouts change. We seek
to go beyond this paradigm by (1) effectively expressing the relations of
content pieces in the table, and (2) making our model robust to
content-invariant structural transformations. Accordingly, we propose an
equivariance learning framework, which encodes tables with a structure-aware
self-attention mechanism. This prunes the full self-attention structure into an
order-invariant graph attention that captures the connected graph structure of
cells belonging to the same row or column, and it differentiates between
relevant cells and irrelevant cells from the structural perspective. Our
framework also modifies the positional encoding mechanism to preserve the
relative position of tokens in the same cell but enforce position invariance
among different cells. Our technology is free to be plugged into existing
table-to-text generation models, and has improved T5-based models to offer
better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo,
we preserve promising performance, while previous SOTA systems, even with
transformation-based data augmentation, have seen significant performance
drops. Our code is available at https://github.com/luka-group/Lattice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Structured Span Selector. (arXiv:2205.03977v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03977">
<div class="article-summary-box-inner">
<span><p>Many natural language processing tasks, e.g., coreference resolution and
semantic role labeling, require selecting text spans and making decisions about
them. A typical approach to such tasks is to score all possible spans and
greedily select spans for task-specific downstream processing. This approach,
however, does not incorporate any inductive bias about what sort of spans ought
to be selected, e.g., that selected spans tend to be syntactic constituents. In
this paper, we propose a novel grammar-based structured span selection model
which learns to make use of the partial span-level annotation provided for such
problems. Compared to previous approaches, our approach gets rid of the
heuristic greedy span selection scheme, allowing us to model the downstream
task on an optimal set of spans. We evaluate our model on two popular span
prediction tasks: coreference resolution and semantic role labeling; and show
improvements on both.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACM -- Attribute Conditioning for Abstractive Multi Document Summarization. (arXiv:2205.03978v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03978">
<div class="article-summary-box-inner">
<span><p>Abstractive multi document summarization has evolved as a task through the
basic sequence to sequence approaches to transformer and graph based
techniques. Each of these approaches has primarily focused on the issues of
multi document information synthesis and attention based approaches to extract
salient information. A challenge that arises with multi document summarization
which is not prevalent in single document summarization is the need to
effectively summarize multiple documents that might have conflicting polarity,
sentiment or subjective information about a given topic. In this paper we
propose ACM, attribute conditioned multi document summarization,a model that
incorporates attribute conditioning modules in order to decouple conflicting
information by conditioning for a certain attribute in the output summary. This
approach shows strong gains in ROUGE score over baseline multi document
summarization approaches and shows gains in fluency, informativeness and
reduction in repetitiveness as shown through a human annotation analysis study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Machine Translation Systems for the Next Thousand Languages. (arXiv:2205.03983v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03983">
<div class="article-summary-box-inner">
<span><p>In this paper we share findings from our effort to build practical machine
translation (MT) systems capable of translating across over one thousand
languages. We describe results in three research domains: (i) Building clean,
web-mined datasets for 1500+ languages by leveraging semi-supervised
pre-training for language identification and developing data-driven filtering
techniques; (ii) Developing practical MT models for under-served languages by
leveraging massively multilingual models trained with supervised parallel data
for over 100 high-resource languages and monolingual datasets for an additional
1000+ languages; and (iii) Studying the limitations of evaluation metrics for
these languages and conducting qualitative analysis of the outputs from our MT
models, highlighting several frequent error modes of these types of models. We
hope that our work provides useful insights to practitioners working towards
building MT systems for currently understudied languages, and highlights
research directions that can complement the weaknesses of massively
multilingual models in data-sparse settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation with Paraphrase Generation and Entity Extraction for Multimodal Dialogue System. (arXiv:2205.04006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04006">
<div class="article-summary-box-inner">
<span><p>Contextually aware intelligent agents are often required to understand the
users and their surroundings in real-time. Our goal is to build Artificial
Intelligence (AI) systems that can assist children in their learning process.
Within such complex frameworks, Spoken Dialogue Systems (SDS) are crucial
building blocks to handle efficient task-oriented communication with children
in game-based learning settings. We are working towards a multimodal dialogue
system for younger kids learning basic math concepts. Our focus is on improving
the Natural Language Understanding (NLU) module of the task-oriented SDS
pipeline with limited datasets. This work explores the potential benefits of
data augmentation with paraphrase generation for the NLU models trained on
small task-specific datasets. We also investigate the effects of extracting
entities for conceivably further data expansion. We have shown that
paraphrasing with model-in-the-loop (MITL) strategies using small seed data is
a promising approach yielding improved performance results for the Intent
Recognition task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving negation detection with negation-focused pre-training. (arXiv:2205.04012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04012">
<div class="article-summary-box-inner">
<span><p>Negation is a common linguistic feature that is crucial in many language
understanding tasks, yet it remains a hard problem due to diversity in its
expression in different types of text. Recent work has shown that
state-of-the-art NLP models underperform on samples containing negation in
various tasks, and that negation detection models do not transfer well across
domains. We propose a new negation-focused pre-training strategy, involving
targeted data augmentation and negation masking, to better incorporate negation
information into language models. Extensive experiments on common benchmarks
show that our proposed approach improves negation detection performance and
generalizability over the strong baseline NegBERT (Khandewal and Sawant, 2020).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCoA-MT: A Dataset and Benchmark for Contrastive Controlled MT with Application to Formality. (arXiv:2205.04022v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04022">
<div class="article-summary-box-inner">
<span><p>The machine translation (MT) task is typically formulated as that of
returning a single translation for an input segment. However, in many cases,
multiple different translations are valid and the appropriate translation may
depend on the intended target audience, characteristics of the speaker, or even
the relationship between speakers. Specific problems arise when dealing with
honorifics, particularly translating from English into languages with formality
markers. For example, the sentence "Are you sure?" can be translated in German
as "Sind Sie sich sicher?" (formal register) or "Bist du dir sicher?"
(informal). Using wrong or inconsistent tone may be perceived as inappropriate
or jarring for users of certain cultures and demographics. This work addresses
the problem of learning to control target language attributes, in this case
formality, from a small amount of labeled contrastive data. We introduce an
annotated dataset (CoCoA-MT) and an associated evaluation metric for training
and evaluating formality-controlled MT models for six diverse target languages.
We show that we can train formality-controlled models by fine-tuning on labeled
contrastive data, achieving high accuracy (82% in-domain and 73% out-of-domain)
while maintaining overall quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProQA: Structural Prompt-based Pre-training for Unified Question Answering. (arXiv:2205.04040v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04040">
<div class="article-summary-box-inner">
<span><p>Question Answering (QA) is a longstanding challenge in natural language
processing. Existing QA works mostly focus on specific question types,
knowledge domains, or reasoning skills. The specialty in QA research hinders
systems from modeling commonalities between tasks and generalization for wider
applications. To address this issue, we present ProQA, a unified QA paradigm
that solves various tasks through a single model. ProQA takes a unified
structural prompt as the bridge and improves the QA-centric ability by
structural prompt-based pre-training. Through a structurally designed
prompt-based input schema, ProQA concurrently models the knowledge
generalization for all QA tasks while keeping the knowledge customization for
every specific QA task. Furthermore, ProQA is pre-trained with structural
prompt-formatted large-scale synthesized corpus, which empowers the model with
the commonly-required QA ability. Experimental results on 11 QA benchmarks
demonstrate that ProQA consistently boosts performance on both full data
fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore,
ProQA exhibits strong ability in both continual learning and transfer learning
by taking the advantages of the structural prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Information Constraints for Monte-Carlo Objectives. (arXiv:2012.00708v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.00708">
<div class="article-summary-box-inner">
<span><p>A common failure mode of density models trained as variational autoencoders
is to model the data without relying on their latent variables, rendering these
variables useless. Two contributing factors, the underspecification of the
model and the looseness of the variational lower bound, have been studied
separately in the literature. We weave these two strands of research together,
specifically the tighter bounds of Monte-Carlo objectives and constraints on
the mutual information between the observable and the latent variables.
Estimating the mutual information as the average Kullback-Leibler divergence
between the easily available variational posterior $q(z|x)$ and the prior does
not work with Monte-Carlo objectives because $q(z|x)$ is no longer a direct
approximation to the model's true posterior $p(z|x)$. Hence, we construct
estimators of the Kullback-Leibler divergence of the true posterior from the
prior by recycling samples used in the objective, with which we train models of
continuous and discrete latents at much improved rate-distortion and no
posterior collapse. While alleviated, the tradeoff between modelling the data
and using the latents still remains, and we urge for evaluating inference
methods across a range of mutual information values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Stance Detection for Mis- and Disinformation Identification. (arXiv:2103.00242v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00242">
<div class="article-summary-box-inner">
<span><p>Understanding attitudes expressed in texts, also known as stance detection,
plays an important role in systems for detecting false information online, be
it misinformation (unintentionally false) or disinformation (intentionally
false information). Stance detection has been framed in different ways,
including (a) as a component of fact-checking, rumour detection, and detecting
previously fact-checked claims, or (b) as a task in its own right. While there
have been prior efforts to contrast stance detection with other related tasks
such as argumentation mining and sentiment analysis, there is no existing
survey on examining the relationship between stance detection and mis- and
disinformation detection. Here, we aim to bridge this gap by reviewing and
analysing existing work in this area, with mis- and disinformation in focus,
and discussing lessons learnt and future challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation. (arXiv:2103.11878v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11878">
<div class="article-summary-box-inner">
<span><p>Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT
evaluation. They can neither distinguish document-level improvements in
translation quality from sentence-level ones, nor identify the discourse
phenomena that cause context-agnostic translations. This paper introduces a
novel automatic metric BlonDe to widen the scope of automatic MT evaluation
from sentence to document level. BlonDe takes discourse coherence into
consideration by categorizing discourse-related spans and calculating the
similarity-based F1 measure of categorized spans. We conduct extensive
comparisons on a newly constructed dataset BWB. The experimental results show
that BlonD possesses better selectivity and interpretability at the
document-level, and is more sensitive to document-level nuances. In a
large-scale human study, BlonD also achieves significantly higher Pearson's r
correlation with human judgments compared to previous metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of Context in Detecting Previously Fact-Checked Claims. (arXiv:2104.07423v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07423">
<div class="article-summary-box-inner">
<span><p>Recent years have seen the proliferation of disinformation and fake news
online. Traditional approaches to mitigate these issues is to use manual or
automatic fact-checking. Recently, another approach has emerged: checking
whether the input claim has previously been fact-checked, which can be done
automatically, and thus fast, while also offering credibility and
explainability, thanks to the human fact-checking and explanations in the
associated fact-checking article. Here, we focus on claims made in a political
debate and we study the impact of modeling the context of the claim: both on
the source side, i.e., in the debate, as well as on the target side, i.e., in
the fact-checking explanation document. We do this by modeling the local
context, the global context, as well as by means of co-reference resolution,
and multi-hop reasoning over the sentences of the document describing the
fact-checked claim. The experimental results show that each of these represents
a valuable information source, but that modeling the source-side context is
most important, and can yield 10+ points of absolute improvement over a
state-of-the-art model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15078">
<div class="article-summary-box-inner">
<span><p>Neural text generation models are typically trained by maximizing
log-likelihood with the sequence cross entropy (CE) loss, which encourages an
exact token-by-token match between a target sequence with a generated sequence.
Such training objective is sub-optimal when the target sequence is not perfect,
e.g., when the target sequence is corrupted with noises, or when only weak
sequence supervision is available. To address the challenge, we propose a novel
Edit-Invariant Sequence Loss (EISL), which computes the matching loss of a
target n-gram with all n-grams in the generated sequence. EISL is designed to
be robust to various noises and edits in the target sequences. Moreover, the
EISL computation is essentially an approximate convolution operation with
target n-grams as kernels, which is easy to implement and efficient to compute
with existing libraries. To demonstrate the effectiveness of EISL, we conduct
experiments on a wide range of tasks, including machine translation with noisy
target sequences, unsupervised text style transfer with only weak training
signals, and non-autoregressive generation with non-predefined generation
order. Experimental results show our method significantly outperforms the
common CE loss and other strong baselines on all the tasks. EISL has a simple
API that can be used as a drop-in replacement of the CE loss:
https://github.com/guangyliu/EISL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paraphrasing via Ranking Many Candidates. (arXiv:2107.09274v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09274">
<div class="article-summary-box-inner">
<span><p>We present a simple and effective way to generate a variety of paraphrases
and find a good quality paraphrase among them. As in previous studies, it is
difficult to ensure that one generation method always generates the best
paraphrase in various domains. Therefore, we focus on finding the best
candidate from multiple candidates, rather than assuming that there is only one
combination of generative models and decoding options. Our approach shows that
it is easy to apply in various domains and has sufficiently good performance
compared to previous methods. In addition, our approach can be used for data
augmentation that extends the downstream corpus, showing that it can help
improve performance in English and Korean datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10314">
<div class="article-summary-box-inner">
<span><p>We present small-text, a simple and modular active learning library, which
offers pool-based active learning for single- and multi-label text
classification in Python. It comes with various pre-implemented
state-of-the-art query strategies, including some that can leverage the GPU.
Clearly defined interfaces allow the combination of a multitude of classifiers,
query strategies, and stopping criteria, thereby facilitating a quick mix and
match, and enabling a rapid development of both active learning experiments and
applications. To make various classifiers accessible in a consistent way, it
integrates several well-known existing machine learning libraries, namely,
scikit-learn, PyTorch, and huggingface transformers, where the latter
integrations are available as optionally installable extensions, making the
availability of a GPU competely optional. The library is available under the
MIT License at https://github.com/webis-de/small-text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning. (arXiv:2108.00356v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00356">
<div class="article-summary-box-inner">
<span><p>Masked language models (MLMs) are pre-trained with a denoising objective that
is in a mismatch with the objective of downstream fine-tuning. We propose
pragmatic masking and surrogate fine-tuning as two complementing strategies
that exploit social cues to drive pre-trained representations toward a broad
set of concepts useful for a wide class of social meaning tasks. We test our
models on $15$ different Twitter datasets for social meaning detection. Our
methods achieve $2.34\%$ $F_1$ over a competitive baseline, while outperforming
domain-specific language models pre-trained on large datasets. Our methods also
excel in few-shot learning: with only $5\%$ of training data (severely
few-shot), our methods enable an impressive $68.54\%$ average $F_1$. The
methods are also language agnostic, as we show in a zero-shot setting involving
six datasets from three different languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeliData: A dataset for deliberation in multi-party problem solving. (arXiv:2108.05271v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05271">
<div class="article-summary-box-inner">
<span><p>Dialogue systems research is traditionally focused on dialogues between two
interlocutors, largely ignoring group conversations. Moreover, most previous
research is focused either on task-oriented dialogue (e.g.\ restaurant
bookings) or user engagement (chatbots), while research on systems for
collaborative dialogues is an under-explored area. To this end, we introduce
the first publicly available dataset containing collaborative conversations on
solving a cognitive task, consisting of 500 group dialogues and 14k utterances.
Furthermore, we propose a novel annotation schema that captures deliberation
cues and release 50 dialogues annotated with it. Finally, we demonstrate the
usefulness of the annotated data in training classifiers to predict the
constructiveness of a conversation. The data collection platform, dataset and
annotated corpus are publicly available at https://delibot.xyz
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00122">
<div class="article-summary-box-inner">
<span><p>The sheer volume of financial statements makes it difficult for humans to
access and analyze a business's financials. Robust numerical reasoning likewise
faces unique challenges in this domain. In this work, we focus on answering
deep questions over financial data, aiming to automate the analysis of a large
corpus of financial documents. In contrast to existing tasks on general domain,
the finance domain includes complex numerical reasoning and understanding of
heterogeneous representations. To facilitate analytical progress, we propose a
new large-scale dataset, FinQA, with Question-Answering pairs over Financial
reports, written by financial experts. We also annotate the gold reasoning
programs to ensure full explainability. We further introduce baselines and
conduct comprehensive experiments in our dataset. The results demonstrate that
popular, large, pre-trained models fall far short of expert humans in acquiring
finance knowledge and in complex multi-step numerical reasoning on that
knowledge. Our dataset -- the first of its kind -- should therefore enable
significant, new community research into complex application domains. The
dataset and code are publicly available\url{https://github.com/czyssrs/FinQA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cartography Active Learning. (arXiv:2109.04282v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04282">
<div class="article-summary-box-inner">
<span><p>We propose Cartography Active Learning (CAL), a novel Active Learning (AL)
algorithm that exploits the behavior of the model on individual instances
during training as a proxy to find the most informative instances for labeling.
CAL is inspired by data maps, which were recently proposed to derive insights
into dataset quality (Swayamdipta et al., 2020). We compare our method on
popular text classification tasks to commonly used AL strategies, which instead
rely on post-training behavior. We demonstrate that CAL is competitive to other
common AL methods, showing that training dynamics derived from small seed data
can be successfully used for AL. We provide insights into our new AL method by
analyzing batch-level statistics utilizing the data maps. Our results further
show that CAL results in a more data-efficient learning strategy, achieving
comparable or better results with considerably less training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TruthfulQA: Measuring How Models Mimic Human Falsehoods. (arXiv:2109.07958v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07958">
<div class="article-summary-box-inner">
<span><p>We propose a benchmark to measure whether a language model is truthful in
generating answers to questions. The benchmark comprises 817 questions that
span 38 categories, including health, law, finance and politics. We crafted
questions that some humans would answer falsely due to a false belief or
misconception. To perform well, models must avoid generating false answers
learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a
T5-based model. The best model was truthful on 58% of questions, while human
performance was 94%. Models generated many false answers that mimic popular
misconceptions and have the potential to deceive humans. The largest models
were generally the least truthful. This contrasts with other NLP tasks, where
performance improves with model size. However, this result is expected if false
answers are learned from the training distribution. We suggest that scaling up
models alone is less promising for improving truthfulness than fine-tuning
using training objectives other than imitation of text from the web.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyLex: Incorporating Dynamic Lexicons into BERT for Sequence Labeling. (arXiv:2109.08818v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08818">
<div class="article-summary-box-inner">
<span><p>Incorporating lexical knowledge into deep learning models has been proved to
be very effective for sequence labeling tasks. However, previous works commonly
have difficulty dealing with large-scale dynamic lexicons which often cause
excessive matching noise and problems of frequent updates. In this paper, we
propose DyLex, a plug-in lexicon incorporation approach for BERT based sequence
labeling tasks. Instead of leveraging embeddings of words in the lexicon as in
conventional methods, we adopt word-agnostic tag embeddings to avoid
re-training the representation while updating the lexicon. Moreover, we employ
an effective supervised lexical knowledge denoising method to smooth out
matching noise. Finally, we introduce a col-wise attention based knowledge
fusion mechanism to guarantee the pluggability of the proposed framework.
Experiments on ten datasets of three tasks show that the proposed framework
achieves new SOTA, even with very large scale lexicons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Model Supervised by Understanding Map. (arXiv:2110.06043v10 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06043">
<div class="article-summary-box-inner">
<span><p>Inspired by the notion of Center of Mass in physics, an extension called
Semantic Center of Mass (SCOM) is proposed, and used to discover the abstract
"topic" of a document. The notion is under a framework model called
Understanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM
is to let both the document content and a semantic network -- specifically,
Understanding Map -- play a role, in interpreting the meaning of a document.
Based on different justifications, three possible methods are devised to
discover the SCOM of a document. Some experiments on artificial documents and
Understanding Maps are conducted to test their outcomes. In addition, its
ability of vectorization of documents and capturing sequential information are
tested. We also compared UM-S-TM with probabilistic topic models like Latent
Dirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Socially Aware Bias Measurements for Hindi Language Representations. (arXiv:2110.07871v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07871">
<div class="article-summary-box-inner">
<span><p>Language representations are efficient tools used across NLP applications,
but they are strife with encoded societal biases. These biases are studied
extensively, but with a primary focus on English language representations and
biases common in the context of Western society. In this work, we investigate
biases present in Hindi language representations with focuses on caste and
religion-associated biases. We demonstrate how biases are unique to specific
language representations based on the history and culture of the region they
are widely spoken in, and how the same societal bias (such as binary
gender-associated biases) is encoded by different words and text spans across
languages. The discoveries of our work highlight the necessity of culture
awareness and linguistic artifacts when modeling language representations, in
order to better understand the encoded biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine-in-the-Loop Rewriting for Creative Image Captioning. (arXiv:2111.04193v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04193">
<div class="article-summary-box-inner">
<span><p>Machine-in-the-loop writing aims to enable humans to collaborate with models
to complete their writing tasks more effectively. Prior work has found that
providing humans a machine-written draft or sentence-level continuations has
limited success since the generated text tends to deviate from humans'
intention. To allow the user to retain control over the content, we train a
rewriting model that, when prompted, modifies specified spans of text within
the user's original draft to introduce descriptive and figurative elements
locally in the text. We evaluate the model on its ability to collaborate with
humans on the task of creative image captioning. On a user study through Amazon
Mechanical Turk, our model is rated to be more helpful than a baseline
infilling language model. In addition, third-party evaluation shows that users
write more descriptive and figurative captions when collaborating with our
model compared to completing the task alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is "My Favorite New Movie" My Favorite Movie? Probing the Understanding of Recursive Noun Phrases. (arXiv:2112.08326v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08326">
<div class="article-summary-box-inner">
<span><p>Recursive noun phrases (NPs) have interesting semantic properties. For
example, "my favorite new movie" is not necessarily my favorite movie, whereas
"my new favorite movie" is. This is common sense to humans, yet it is unknown
whether language models have such knowledge. We introduce the Recursive Noun
Phrase Challenge (RNPC), a dataset of three textual inference tasks involving
textual entailment and event plausibility comparison, precisely targeting the
understanding of recursive NPs. When evaluated on RNPC, state-of-the-art
Transformer models only perform around chance. Still, we show that such
knowledge is learnable with appropriate data. We further probe the models for
relevant linguistic features that can be learned from our tasks, including
modifier semantic category and modifier scope. Finally, models trained on RNPC
achieve strong zero-shot performance on an extrinsic Harm Detection evaluation
task, showing the usefulness of the understanding of recursive NPs in
downstream applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocAMR: Multi-Sentence AMR Representation and Evaluation. (arXiv:2112.08513v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08513">
<div class="article-summary-box-inner">
<span><p>Despite extensive research on parsing of English sentences into Abstraction
Meaning Representation (AMR) graphs, which are compared to gold graphs via the
Smatch metric, full-document parsing into a unified graph representation lacks
well-defined representation and evaluation. Taking advantage of a
super-sentential level of coreference annotation from previous work, we
introduce a simple algorithm for deriving a unified graph representation,
avoiding the pitfalls of information loss from over-merging and lack of
coherence from under-merging. Next, we describe improvements to the Smatch
metric to make it tractable for comparing document-level graphs, and use it to
re-evaluate the best published document-level AMR parser. We also present a
pipeline approach combining the top performing AMR parser and coreference
resolution systems, providing a strong baseline for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning To Retrieve Prompts for In-Context Learning. (arXiv:2112.08633v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08633">
<div class="article-summary-box-inner">
<span><p>In-context learning is a recent paradigm in natural language understanding,
where a large pre-trained language model (LM) observes a test instance and a
few training examples as its input, and directly decodes the output without any
update to its parameters. However, performance has been shown to strongly
depend on the selected training examples (termed prompt). In this work, we
propose an efficient method for retrieving prompts for in-context learning
using annotated data and a LM. Given an input-output pair, we estimate the
probability of the output given the input and a candidate training example as
the prompt, and label training examples as positive or negative based on this
probability. We then train an efficient dense retriever from this data, which
is used to retrieve training examples as prompts at test time. We evaluate our
approach on three sequence-to-sequence tasks where language utterances are
mapped to meaning representations, and find that it substantially outperforms
prior work and multiple baselines across the board.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PhysNLU: A Language Resource for Evaluating Natural Language Understanding and Explanation Coherence in Physics. (arXiv:2201.04275v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04275">
<div class="article-summary-box-inner">
<span><p>In order for language models to aid physics research, they must first encode
representations of mathematical and natural language discourse which lead to
coherent explanations, with correct ordering and relevance of statements. We
present a collection of datasets developed to evaluate the performance of
language models in this regard, which measure capabilities with respect to
sentence ordering, position, section prediction, and discourse coherence.
Analysis of the data reveals equations and sub-disciplines which are most
common in physics discourse, as well as the sentence-level frequency of
equations and expressions. We present baselines that demonstrate how
contemporary language models are challenged by coherence related tasks in
physics, even when trained on mathematical natural language objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on the Overlapping Problem of Open-Domain Dialogue Datasets. (arXiv:2201.06219v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06219">
<div class="article-summary-box-inner">
<span><p>Open-domain dialogue systems aim to converse with humans through text, and
dialogue research has heavily relied on benchmark datasets. In this work, we
observe the overlapping problem in DailyDialog and OpenSubtitles, two popular
open-domain dialogue benchmark datasets. Our systematic analysis then shows
that such overlapping can be exploited to obtain fake state-of-the-art
performance. Finally, we address this issue by cleaning these datasets and
setting up a proper data processing procedure for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing. (arXiv:2202.08712v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08712">
<div class="article-summary-box-inner">
<span><p>To date, there are no effective treatments for most neurodegenerative
diseases. Knowledge graphs can provide comprehensive and semantic
representation for heterogeneous data, and have been successfully leveraged in
many biomedical applications including drug repurposing. Our objective is to
construct a knowledge graph from literature to study relations between
Alzheimer's disease (AD) and chemicals, drugs and dietary supplements in order
to identify opportunities to prevent or delay neurodegenerative progression. We
collected biomedical annotations and extracted their relations using SemRep via
SemMedDB. We used both a BERT-based classifier and rule-based methods during
data preprocessing to exclude noise while preserving most AD-related semantic
triples. The 1,672,110 filtered triples were used to train with knowledge graph
completion algorithms (i.e., TransE, DistMult, and ComplEx) to predict
candidates that might be helpful for AD treatment or prevention. Among three
knowledge graph completion models, TransE outperformed the other two (MR =
13.45, Hits@1 = 0.306). We leveraged the time-slicing technique to further
evaluate the prediction results. We found supporting evidence for most highly
ranked candidates predicted by our model which indicates that our approach can
inform reliable new knowledge. This paper shows that our graph mining model can
predict reliable new relationships between AD and other entities (i.e., dietary
supplements, chemicals, and drugs). The knowledge graph constructed can
facilitate data-driven knowledge discoveries and the generation of novel
hypotheses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SkillNet-NLU: A Sparsely Activated Model for General-Purpose Natural Language Understanding. (arXiv:2203.03312v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03312">
<div class="article-summary-box-inner">
<span><p>Prevailing deep models are single-purpose and overspecialize at individual
tasks. However, when being extended to new tasks, they typically forget
previously learned skills and learn from scratch. We address this issue by
introducing SkillNet-NLU, a general-purpose model that stitches together
existing skills to learn new tasks more effectively. The key feature of our
approach is that it is sparsely activated guided by predefined skills.
Different from traditional dense models that always activate all the model
parameters, SkillNet-NLU only activates parts of the model parameters whose
skills are relevant to the target task. When learning for a new task, our
approach precisely activates required skills and also provides an option to add
new skills. We evaluate on natural language understandings tasks and have the
following findings. First, with only one model checkpoint, SkillNet-NLU
performs better than task-specific fine-tuning and two multi-task learning
baselines (i.e., dense model and Mixture-of-Experts model) on six tasks.
Second, sparsely activated pre-training further improves the overall
performance. Third, SkillNet-NLU significantly outperforms baseline systems
when being extended to new tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slangvolution: A Causal Analysis of Semantic Change and Frequency Dynamics in Slang. (arXiv:2203.04651v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04651">
<div class="article-summary-box-inner">
<span><p>Languages are continuously undergoing changes, and the mechanisms that
underlie these changes are still a matter of debate. In this work, we approach
language evolution through the lens of causality in order to model not only how
various distributional factors associate with language change, but how they
causally affect it. In particular, we study slang, which is an informal
language that is typically restricted to a specific group or social setting. We
analyze the semantic change and frequency shift of slang words and compare them
to those of standard, nonslang words. With causal discovery and causal
inference techniques, we measure the effect that word type (slang/nonslang) has
on both semantic change and frequency shift, as well as its relationship to
frequency, polysemy and part of speech. Our analysis provides some new insights
in the study of language change, e.g., we show that slang words undergo less
semantic change but tend to have larger frequency shifts over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Dependency Tree Into Self-attention for Sentence Representation. (arXiv:2203.05918v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05918">
<div class="article-summary-box-inner">
<span><p>Recent progress on parse tree encoder for sentence representation learning is
notable. However, these works mainly encode tree structures recursively, which
is not conducive to parallelization. On the other hand, these works rarely take
into account the labels of arcs in dependency trees. To address both issues, we
propose Dependency-Transformer, which applies a relation-attention mechanism
that works in concert with the self-attention mechanism. This mechanism aims to
encode the dependency and the spatial positional relations between nodes in the
dependency tree of sentences. By a score-based method, we successfully inject
the syntax information without affecting Transformer's parallelizability. Our
model outperforms or is comparable to the state-of-the-art methods on four
tasks for sentence representation and has obvious advantages in computational
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decay No More: A Persistent Twitter Dataset for Learning Social Meaning. (arXiv:2204.04611v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04611">
<div class="article-summary-box-inner">
<span><p>With the proliferation of social media, many studies resort to social media
to construct datasets for developing social meaning understanding systems. For
the popular case of Twitter, most researchers distribute tweet IDs without the
actual text contents due to the data distribution policy of the platform. One
issue is that the posts become increasingly inaccessible over time, which leads
to unfair comparisons and a temporal bias in social media research. To
alleviate this challenge of data decay, we leverage a paraphrase model to
propose a new persistent English Twitter dataset for social meaning (PTSM).
PTSM consists of $17$ social meaning datasets in $10$ categories of tasks. We
experiment with two SOTA pre-trained language models and show that our PTSM can
substitute the actual tweets with paraphrases with marginal performance loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Classify Open Intent via Soft Labeling and Manifold Mixup. (arXiv:2204.07804v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07804">
<div class="article-summary-box-inner">
<span><p>Open intent classification is a practical yet challenging task in dialogue
systems. Its objective is to accurately classify samples of known intents while
at the same time detecting those of open (unknown) intents. Existing methods
usually use outlier detection algorithms combined with K-class classifier to
detect open intents, where K represents the class number of known intents.
Different from them, in this paper, we consider another way without using
outlier detection algorithms. Specifically, we directly train a (K+1)-class
classifier for open intent classification, where the (K+1)-th class represents
open intents. To address the challenge that training a (K+1)-class classifier
with training samples of only K classes, we propose a deep model based on Soft
Labeling and Manifold Mixup (SLMM). In our method, soft labeling is used to
reshape the label distribution of the known intent samples, aiming at reducing
model's overconfident on known intents. Manifold mixup is used to generate
pseudo samples for open intents, aiming at well optimizing the decision
boundary of open intents. Experiments on four benchmark datasets demonstrate
that our method outperforms previous methods and achieves state-of-the-art
performance. All the code and data of this work can be obtained at
https://github.com/zifengcheng/SLMM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Structure based Query Graph Prediction for Question Answering over Knowledge Graph. (arXiv:2204.10194v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10194">
<div class="article-summary-box-inner">
<span><p>Building query graphs from natural language questions is an important step in
complex question answering over knowledge graph (Complex KGQA). In general, a
question can be correctly answered if its query graph is built correctly and
the right answer is then retrieved by issuing the query graph against the KG.
Therefore, this paper focuses on query graph generation from natural language
questions. Existing approaches for query graph generation ignore the semantic
structure of a question, resulting in a large number of noisy query graph
candidates that undermine prediction accuracies. In this paper, we define six
semantic structures from common questions in KGQA and develop a novel
Structure-BERT to predict the semantic structure of a question. By doing so, we
can first filter out noisy candidate query graphs by the predicted semantic
structures, and then rank the remaining candidates with a BERT-based ranking
model. Extensive experiments on two popular benchmarks MetaQA and
WebQuestionsSP (WSP) demonstrate the effectiveness of our method as compared to
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model. (arXiv:2204.13509v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13509">
<div class="article-summary-box-inner">
<span><p>Many recent studies on large-scale language models have reported successful
in-context zero- and few-shot learning ability. However, the in-depth analysis
of when in-context learning occurs is still lacking. For example, it is unknown
how in-context learning performance changes as the training corpus varies.
Here, we investigate the effects of the source and size of the pretraining
corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From
our in-depth investigation, we introduce the following observations: (1)
in-context learning performance heavily depends on the corpus domain source,
and the size of the pretraining corpus does not necessarily determine the
emergence of in-context learning, (2) in-context learning ability can emerge
when a language model is trained on a combination of multiple corpora, even
when each corpus does not result in in-context learning on its own, (3)
pretraining with a corpus related to a downstream task does not always
guarantee the competitive in-context learning performance of the downstream
task, especially in the few-shot setting, and (4) the relationship between
language modeling (measured in perplexity) and in-context learning does not
always correlate: e.g., low perplexity does not always imply high in-context
few-shot learning performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Life is not Always Depressing: Exploring the Happy Moments of People Diagnosed with Depression. (arXiv:2204.13569v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13569">
<div class="article-summary-box-inner">
<span><p>In this work, we explore the relationship between depression and
manifestations of happiness in social media. While the majority of works
surrounding depression focus on symptoms, psychological research shows that
there is a strong link between seeking happiness and being diagnosed with
depression. We make use of Positive-Unlabeled learning paradigm to
automatically extract happy moments from social media posts of both controls
and users diagnosed with depression, and qualitatively analyze them with
linguistic tools such as LIWC and keyness information. We show that the life of
depressed individuals is not always bleak, with positive events related to
friends and family being more noteworthy to their lives compared to the more
mundane happy events reported by control users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neutral Utterances are Also Causes: Enhancing Conversational Causal Emotion Entailment with Social Commonsense Knowledge. (arXiv:2205.00759v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00759">
<div class="article-summary-box-inner">
<span><p>Conversational Causal Emotion Entailment aims to detect causal utterances for
a non-neutral targeted utterance from a conversation. In this work, we build
conversations as graphs to overcome implicit contextual modelling of the
original entailment style. Following the previous work, we further introduce
the emotion information into graphs. Emotion information can markedly promote
the detection of causal utterances whose emotion is the same as the targeted
utterance. However, it is still hard to detect causal utterances with different
emotions, especially neutral ones. The reason is that models are limited in
reasoning causal clues and passing them between utterances. To alleviate this
problem, we introduce social commonsense knowledge (CSK) and propose a
Knowledge Enhanced Conversation graph (KEC). KEC propagates the CSK between two
utterances. As not all CSK is emotionally suitable for utterances, we therefore
propose a sentiment-realized knowledge selecting strategy to filter CSK. To
process KEC, we further construct the Knowledge Enhanced Directed Acyclic Graph
networks. Experimental results show that our method outperforms baselines and
infers more causes with different emotions from the targeted utterance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models. (arXiv:2205.02023v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02023">
<div class="article-summary-box-inner">
<span><p>The success of multilingual pre-trained models is underpinned by their
ability to learn representations shared by multiple languages even in absence
of any explicit supervision. However, it remains unclear how these models learn
to generalise across languages. In this work, we conjecture that multilingual
pre-trained models can derive language-universal abstractions about grammar. In
particular, we investigate whether morphosyntactic information is encoded in
the same subset of neurons in different languages. We conduct the first
large-scale empirical study over 43 languages and 14 morphosyntactic categories
with a state-of-the-art neuron-level probe. Our findings show that the
cross-lingual overlap between neurons is significant, but its extent may vary
across categories and depends on language proximity and pre-training data size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Few-Shot Fine-Tuning for Opinion Summarization. (arXiv:2205.02170v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02170">
<div class="article-summary-box-inner">
<span><p>Abstractive summarization models are typically pre-trained on large amounts
of generic texts, then fine-tuned on tens or hundreds of thousands of annotated
samples. However, in opinion summarization, large annotated datasets of reviews
paired with reference summaries are not available and would be expensive to
create. This calls for fine-tuning methods robust to overfitting on small
datasets. In addition, generically pre-trained models are often not accustomed
to the specifics of customer reviews and, after fine-tuning, yield summaries
with disfluencies and semantic mistakes. To address these problems, we utilize
an efficient few-shot method based on adapters which, as we show, can easily
store in-domain knowledge. Instead of fine-tuning the entire model, we add
adapters and pre-train them in a task-specific way on a large corpus of
unannotated customer reviews, using held-out reviews as pseudo summaries. Then,
fine-tune the adapters on the small available human-annotated dataset. We show
that this self-supervised adapter pre-training improves summary quality over
standard fine-tuning by 2.0 and 1.3 ROUGE-L points on the Amazon and Yelp
datasets, respectively. Finally, for summary personalization, we condition on
aspect keyword queries, automatically created from generic datasets. In the
same vein, we pre-train the adapters in a query-based manner on customer
reviews and then fine-tune them on annotated datasets. This results in
better-organized summary content reflected in improved coherence and fewer
redundancies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collective Relevance Labeling for Passage Retrieval. (arXiv:2205.03273v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03273">
<div class="article-summary-box-inner">
<span><p>Deep learning for Information Retrieval (IR) requires a large amount of
high-quality query-document relevance labels, but such labels are inherently
sparse. Label smoothing redistributes some observed probability mass over
unobserved instances, often uniformly, uninformed of the true distribution. In
contrast, we propose knowledge distillation for informed labeling, without
incurring high computation overheads at evaluation time. Our contribution is
designing a simple but efficient teacher model which utilizes collective
knowledge, to outperform state-of-the-arts distilled from a more complex
teacher model. Specifically, we train up to x8 faster than the state-of-the-art
teacher, while distilling the rankings better. Our code is publicly available
at https://github.com/jihyukkim-nlp/CollectiveKD
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A High-Resolution Chest CT-Scan Image Dataset for COVID-19 Diagnosis and Differentiation. (arXiv:2205.03408v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03408">
<div class="article-summary-box-inner">
<span><p>During the COVID-19 pandemic, computed tomography (CT) is a good way to
diagnose COVID-19 patients. HRCT (High-Resolution Computed Tomography) is a
form of computed tomography that uses advanced methods to improve image
resolution. Publicly accessible COVID-19 CT image datasets are very difficult
to come by due to privacy concerns, which impedes the study and development of
AI-powered COVID-19 diagnostic algorithms based on CT images. To address this
problem, we have introduced HRCTv1-COVID-19, a new COVID-19 high resolution
chest CT Scan image dataset that includes not only COVID-19 cases of Ground
Glass Opacity (GGO), Crazy Paving, and Air Space Consolidation, but also CT
images of cases with negative COVID-19. The HRCTv1-COVID-19 dataset, which
includes slice-level, and patient-level labels, has the potential to aid
COVID-19 research, especially for diagnosis and differentiation using
artificial intelligence algorithms, machine learning and deep learning methods.
This dataset is accessible through web at: <a href="http://databiox.com">this http URL</a> and includes
181,106 chest HRCT images from 395 patients with four labels: GGO, Crazy
Paving, Air Space Consolidation and Negative.
</p>
<p>Keywords- Dataset, COVID-19, CT-Scan, Computed Tomography, Medical Imaging,
Chest Image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VFHQ: A High-Quality Dataset and Benchmark for Video Face Super-Resolution. (arXiv:2205.03409v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03409">
<div class="article-summary-box-inner">
<span><p>Most of the existing video face super-resolution (VFSR) methods are trained
and evaluated on VoxCeleb1, which is designed specifically for speaker
identification and the frames in this dataset are of low quality. As a
consequence, the VFSR models trained on this dataset can not output
visual-pleasing results. In this paper, we develop an automatic and scalable
pipeline to collect a high-quality video face dataset (VFHQ), which contains
over $16,000$ high-fidelity clips of diverse interview scenarios. To verify the
necessity of VFHQ, we further conduct experiments and demonstrate that VFSR
models trained on our VFHQ dataset can generate results with sharper edges and
finer textures than those trained on VoxCeleb1. In addition, we show that the
temporal information plays a pivotal role in eliminating video consistency
issues as well as further improving visual performance. Based on VFHQ, by
analyzing the benchmarking study of several state-of-the-art algorithms under
bicubic and blind settings. See our project page:
https://liangbinxie.github.io/projects/vfhq
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers. (arXiv:2205.03436v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03436">
<div class="article-summary-box-inner">
<span><p>Self-attention based models such as vision transformers (ViTs) have emerged
as a very competitive architecture alternative to convolutional neural networks
(CNNs) in computer vision. Despite increasingly stronger variants with
ever-higher recognition accuracies, due to the quadratic complexity of
self-attention, existing ViTs are typically demanding in computation and model
size. Although several successful design choices (e.g., the convolutions and
hierarchical multi-stage structure) of prior CNNs have been reintroduced into
recent ViTs, they are still not sufficient to meet the limited resource
requirements of mobile devices. This motivates a very recent attempt to develop
light ViTs based on the state-of-the-art MobileNet-v2, but still leaves a
performance gap behind. In this work, pushing further along this under-studied
direction we introduce EdgeViTs, a new family of light-weight ViTs that, for
the first time, enable attention-based vision models to compete with the best
light-weight CNNs in the tradeoff between accuracy and on-device efficiency.
This is realized by introducing a highly cost-effective local-global-local
(LGL) information exchange bottleneck based on optimal integration of
self-attention and convolutions. For device-dedicated evaluation, rather than
relying on inaccurate proxies like the number of FLOPs or parameters, we adopt
a practical approach of focusing directly on on-device latency and, for the
first time, energy efficiency. Specifically, we show that our models are
Pareto-optimal when both accuracy-latency and accuracy-energy trade-offs are
considered, achieving strict dominance over other ViTs in almost all cases and
competing with the most efficient CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Multi-modal 2D/3D Registration via Local Descriptors Learning. (arXiv:2205.03439v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03439">
<div class="article-summary-box-inner">
<span><p>Multi-modal registration is a required step for many image-guided procedures,
especially ultrasound-guided interventions that require anatomical context.
While a number of such registration algorithms are already available, they all
require a good initialization to succeed due to the challenging appearance of
ultrasound images and the arbitrary coordinate system they are acquired in. In
this paper, we present a novel approach to solve the problem of registration of
an ultrasound sweep to a pre-operative image. We learn dense keypoint
descriptors from which we then estimate the registration. We show that our
method overcomes the challenges inherent to registration tasks with freehand
ultrasound sweeps, namely, the multi-modality and multidimensionality of the
data in addition to lack of precise ground truth and low amounts of training
examples. We derive a registration method that is fast, generic, fully
automatic, does not require any initialization and can naturally generate
visualizations aiding interpretability and explainability. Our approach is
evaluated on a clinical dataset of paired MR volumes and ultrasound sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LatentKeypointGAN: Controlling Images via Latent Keypoints -- Extended Abstract. (arXiv:2205.03448v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03448">
<div class="article-summary-box-inner">
<span><p>Generative adversarial networks (GANs) can now generate photo-realistic
images. However, how to best control the image content remains an open
challenge. We introduce LatentKeypointGAN, a two-stage GAN internally
conditioned on a set of keypoints and associated appearance embeddings
providing control of the position and style of the generated objects and their
respective parts. A major difficulty that we address is disentangling the image
into spatial and appearance factors with little domain knowledge and
supervision signals. We demonstrate in a user study and quantitative
experiments that LatentKeypointGAN provides an interpretable latent space that
can be used to re-arrange the generated images by re-positioning and exchanging
keypoint embeddings, such as generating portraits by combining the eyes, and
mouth from different images. Notably, our method does not require labels as it
is self-supervised and thereby applies to diverse application domains, such as
editing portraits, indoor rooms, and full-body human poses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Analysis of Non-Blind Deblurring Methods for Noisy Blurred Images. (arXiv:2205.03464v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03464">
<div class="article-summary-box-inner">
<span><p>Image blurring refers to the degradation of an image wherein the image's
overall sharpness decreases. Image blurring is caused by several factors.
Additionally, during the image acquisition process, noise may get added to the
image. Such a noisy and blurred image can be represented as the image resulting
from the convolution of the original image with the associated point spread
function, along with additive noise. However, the blurred image often contains
inadequate information to uniquely determine the plausible original image.
Based on the availability of blurring information, image deblurring methods can
be classified as blind and non-blind. In non-blind image deblurring, some prior
information is known regarding the corresponding point spread function and the
added noise. The objective of this study is to determine the effectiveness of
non-blind image deblurring methods with respect to the identification and
elimination of noise present in blurred images. In this study, three non-blind
image deblurring methods, namely Wiener deconvolution, Lucy-Richardson
deconvolution, and regularized deconvolution were comparatively analyzed for
noisy images featuring salt-and-pepper noise. Two types of blurring effects
were simulated, namely motion blurring and Gaussian blurring. The said three
non-blind deblurring methods were applied under two scenarios: direct
deblurring of noisy blurred images and deblurring of images after denoising
through the application of the adaptive median filter. The obtained results
were then compared for each scenario to determine the best approach for
deblurring noisy images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVIMO2: An Event Camera Dataset for Motion Segmentation, Optical Flow, Structure from Motion, and Visual Inertial Odometry in Indoor Scenes with Monocular or Stereo Algorithms. (arXiv:2205.03467v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03467">
<div class="article-summary-box-inner">
<span><p>A new event camera dataset, EVIMO2, is introduced that improves on the
popular EVIMO dataset by providing more data, from better cameras, in more
complex scenarios. As with its predecessor, EVIMO2 provides labels in the form
of per-pixel ground truth depth and segmentation as well as camera and object
poses. All sequences use data from physical cameras and many sequences feature
multiple independently moving objects. Typically, such labeled data is
unavailable in physical event camera datasets. Thus, EVIMO2 will serve as a
challenging benchmark for existing algorithms and rich training set for the
development of new algorithms. In particular, EVIMO2 is suited for supporting
research in motion and object segmentation, optical flow, structure from
motion, and visual (inertial) odometry in both monocular or stereo
configurations.
</p>
<p>EVIMO2 consists of 41 minutes of data from three 640$\times$480 event
cameras, one 2080$\times$1552 classical color camera, inertial measurements
from two six axis inertial measurement units, and millimeter accurate object
poses from a Vicon motion capture system. The dataset's 173 sequences are
arranged into three categories. 3.75 minutes of independently moving household
objects, 22.55 minutes of static scenes, and 14.85 minutes of basic motions in
shallow scenes. Some sequences were recorded in low-light conditions where
conventional cameras fail. Depth and segmentation are provided at 60 Hz for the
event cameras and 30 Hz for the classical camera. The masks can be regenerated
using open-source code up to rates as high as 200 Hz.
</p>
<p>This technical report briefly describes EVIMO2. The full documentation is
available online. Videos of individual sequences can be sampled on the download
page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Norm-Scaling for Out-of-Distribution Detection. (arXiv:2205.03493v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03493">
<div class="article-summary-box-inner">
<span><p>Out-of-Distribution (OoD) inputs are examples that do not belong to the true
underlying distribution of the dataset. Research has shown that deep neural
nets make confident mispredictions on OoD inputs. Therefore, it is critical to
identify OoD inputs for safe and reliable deployment of deep neural nets. Often
a threshold is applied on a similarity score to detect OoD inputs. One such
similarity is angular similarity which is the dot product of latent
representation with the mean class representation. Angular similarity encodes
uncertainty, for example, if the angular similarity is less, it is less certain
that the input belongs to that class. However, we observe that, different
classes have different distributions of angular similarity. Therefore, applying
a single threshold for all classes is not ideal since the same similarity score
represents different uncertainties for different classes. In this paper, we
propose norm-scaling which normalizes the logits separately for each class.
This ensures that a single value consistently represents similar uncertainty
for various classes. We show that norm-scaling, when used with maximum softmax
probability detector, achieves 9.78% improvement in AUROC, 5.99% improvement in
AUPR and 33.19% reduction in FPR95 metrics over previous state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Deep Unrolled Reconstruction Using Regularization by Denoising. (arXiv:2205.03519v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03519">
<div class="article-summary-box-inner">
<span><p>Deep learning methods have been successfully used in various computer vision
tasks. Inspired by that success, deep learning has been explored in magnetic
resonance imaging (MRI) reconstruction. In particular, integrating deep
learning and model-based optimization methods has shown considerable
advantages. However, a large amount of labeled training data is typically
needed for high reconstruction quality, which is challenging for some MRI
applications. In this paper, we propose a novel reconstruction method, named
DURED-Net, that enables interpretable unsupervised learning for MR image
reconstruction by combining an unsupervised denoising network and a
plug-and-play method. We aim to boost the reconstruction performance of
unsupervised learning by adding an explicit prior that utilizes imaging
physics. Specifically, the leverage of a denoising network for MRI
reconstruction is achieved using Regularization by Denoising (RED). Experiment
results demonstrate that the proposed method requires a reduced amount of
training data to achieve high reconstruction quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction. (arXiv:2205.03521v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03521">
<div class="article-summary-box-inner">
<span><p>Multimodal named entity recognition and relation extraction (MNER and MRE) is
a fundamental and crucial branch in information extraction. However, existing
approaches for MNER and MRE usually suffer from error sensitivity when
irrelevant object images incorporated in texts. To deal with these issues, we
propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for
visual-enhanced entity and relation extraction, aiming to achieve more
effective and robust performance. Specifically, we regard visual representation
as pluggable visual prefix to guide the textual representation for error
insensitive forecasting decision. We further propose a dynamic gated
aggregation strategy to achieve hierarchical multi-scaled visual features as
visual prefix for fusion. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our method, and achieve state-of-the-art
performance. Code is available in https://github.com/zjunlp/HVPNeT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Terrain Mapping and Landing Site Detection for Autonomous UAVs. (arXiv:2205.03522v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03522">
<div class="article-summary-box-inner">
<span><p>The next generation of Mars rotorcrafts requires on-board autonomous hazard
avoidance landing. To this end, this work proposes a system that performs
continuous multi-resolution height map reconstruction and safe landing spot
detection. Structure-from-Motion measurements are aggregated in a pyramid
structure using a novel Optimal Mixture of Gaussians formulation that provides
a comprehensive uncertainty model. Our multiresolution pyramid is built more
efficiently and accurately than past work by decoupling pyramid filling from
the measurement updates of different resolutions. To detect the safest landing
location, after an optimized hazard segmentation, we use a mean shift algorithm
on multiple distance transform peaks to account for terrain roughness and
uncertainty. The benefits of our contributions are evaluated on real and
synthetic flight data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution. (arXiv:2205.03524v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03524">
<div class="article-summary-box-inner">
<span><p>Due to the sophisticated imaging process, an identical scene captured by
different cameras could exhibit distinct imaging patterns, introducing distinct
proficiency among the super-resolution (SR) models trained on images from
different devices. In this paper, we investigate a novel and practical task
coded cross-device SR, which strives to adapt a real-world SR model trained on
the paired images captured by one camera to low-resolution (LR) images captured
by arbitrary target devices. The proposed task is highly challenging due to the
absence of paired data from various imaging devices. To address this issue, we
propose an unsupervised domain adaptation mechanism for real-world SR, named
Dual ADversarial Adaptation (DADA), which only requires LR images in the target
domain with available real paired data from a source camera. DADA employs the
Domain-Invariant Attention (DIA) module to establish the basis of target model
training even without HR supervision. Furthermore, the dual framework of DADA
facilitates an Inter-domain Adversarial Adaptation (InterAA) in one branch for
two LR input images from two domains, and an Intra-domain Adversarial
Adaptation (IntraAA) in two branches for an LR input image. InterAA and IntraAA
together improve the model transferability from the source domain to the
target. We empirically conduct experiments under six Real to Real adaptation
settings among three different cameras, and achieve superior performance
compared with existing state-of-the-art approaches. We also evaluate the
proposed DADA to address the adaptation to the video camera, which presents a
promising research topic to promote the wide applications of real-world
super-resolution. Our source code is publicly available at
https://github.com/lonelyhope/DADA.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic segmentation of meniscus based on MAE self-supervision and point-line weak supervision paradigm. (arXiv:2205.03525v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03525">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation based on deep learning is often faced with the
problems of insufficient datasets and long time-consuming labeling. In this
paper, we introduce the self-supervised method MAE(Masked Autoencoders) into
knee joint images to provide a good initial weight for the segmentation model
and improve the adaptability of the model to small datasets. Secondly, we
propose a weakly supervised paradigm for meniscus segmentation based on the
combination of point and line to reduce the time of labeling. Based on the weak
label ,we design a region growing algorithm to generate pseudo-label. Finally
we train the segmentation network based on pseudo-labels with weight transfer
from self-supervision. Sufficient experimental results show that our proposed
method combining self-supervision and weak supervision can almost approach the
performance of purely fully supervised models while greatly reducing the
required labeling time and dataset size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attract me to Buy: Advertisement Copywriting Generation with Multimodal Multi-structured Information. (arXiv:2205.03534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03534">
<div class="article-summary-box-inner">
<span><p>Recently, online shopping has gradually become a common way of shopping for
people all over the world. Wonderful merchandise advertisements often attract
more people to buy. These advertisements properly integrate multimodal
multi-structured information of commodities, such as visual spatial information
and fine-grained structure information. However, traditional multimodal text
generation focuses on the conventional description of what existed and
happened, which does not match the requirement of advertisement copywriting in
the real world. Because advertisement copywriting has a vivid language style
and higher requirements of faithfulness. Unfortunately, there is a lack of
reusable evaluation frameworks and a scarcity of datasets. Therefore, we
present a dataset, E-MMAD (e-commercial multimodal multi-structured
advertisement copywriting), which requires, and supports much more detailed
information in text generation. Noticeably, it is one of the largest video
captioning datasets in this field. Accordingly, we propose a baseline method
and faithfulness evaluation metric on the strength of structured information
reasoning to solve the demand in reality on this dataset. It surpasses the
previous methods by a large margin on all metrics. The dataset and method are
coming soon on \url{https://e-mmad.github.io/e-mmad.net/index.html}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiCo-Net: Regress Globally, Match Locally for Robust 6D Pose Estimation. (arXiv:2205.03536v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03536">
<div class="article-summary-box-inner">
<span><p>The challenges of learning a robust 6D pose function lie in 1) severe
occlusion and 2) systematic noises in depth images. Inspired by the success of
point-pair features, the goal of this paper is to recover the 6D pose of an
object instance segmented from RGB-D images by locally matching pairs of
oriented points between the model and camera space. To this end, we propose a
novel Bi-directional Correspondence Mapping Network (BiCo-Net) to first
generate point clouds guided by a typical pose regression, which can thus
incorporate pose-sensitive information to optimize generation of local
coordinates and their normal vectors. As pose predictions via geometric
computation only rely on one single pair of local oriented points, our BiCo-Net
can achieve robustness against sparse and occluded point clouds. An ensemble of
redundant pose predictions from locally matching and direct pose regression
further refines final pose output against noisy observations. Experimental
results on three popularly benchmarking datasets can verify that our method can
achieve state-of-the-art performance, especially for the more challenging
severe occluded scenes. Source codes are available at
https://github.com/Gorilla-Lab-SCUT/BiCo-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bandits for Structure Perturbation-based Black-box Attacks to Graph Neural Networks with Theoretical Guarantees. (arXiv:2205.03546v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03546">
<div class="article-summary-box-inner">
<span><p>Graph neural networks (GNNs) have achieved state-of-the-art performance in
many graph-based tasks such as node classification and graph classification.
However, many recent works have demonstrated that an attacker can mislead GNN
models by slightly perturbing the graph structure. Existing attacks to GNNs are
either under the less practical threat model where the attacker is assumed to
access the GNN model parameters, or under the practical black-box threat model
but consider perturbing node features that are shown to be not enough
effective. In this paper, we aim to bridge this gap and consider black-box
attacks to GNNs with structure perturbation as well as with theoretical
guarantees. We propose to address this challenge through bandit techniques.
Specifically, we formulate our attack as an online optimization with bandit
feedback. This original problem is essentially NP-hard due to the fact that
perturbing the graph structure is a binary optimization problem. We then
propose an online attack based on bandit optimization which is proven to be
{sublinear} to the query number $T$, i.e., $\mathcal{O}(\sqrt{N}T^{3/4})$ where
$N$ is the number of nodes in the graph. Finally, we evaluate our proposed
attack by conducting experiments over multiple datasets and GNN models. The
experimental results on various citation graphs and image graphs show that our
attack is both effective and efficient. Source code is available
at~\url{https://github.com/Metaoblivion/Bandit_GNN_Attack}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-enabled Detection and Classification of Bacterial Colonies using a Thin Film Transistor (TFT) Image Sensor. (arXiv:2205.03549v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03549">
<div class="article-summary-box-inner">
<span><p>Early detection and identification of pathogenic bacteria such as Escherichia
coli (E. coli) is an essential task for public health. The conventional
culture-based methods for bacterial colony detection usually take &gt;24 hours to
get the final read-out. Here, we demonstrate a bacterial colony-forming-unit
(CFU) detection system exploiting a thin-film-transistor (TFT)-based image
sensor array that saves ~12 hours compared to the Environmental Protection
Agency (EPA)-approved methods. To demonstrate the efficacy of this CFU
detection system, a lensfree imaging modality was built using the TFT image
sensor with a sample field-of-view of ~10 cm^2. Time-lapse images of bacterial
colonies cultured on chromogenic agar plates were automatically collected at
5-minute intervals. Two deep neural networks were used to detect and count the
growing colonies and identify their species. When blindly tested with 265
colonies of E. coli and other coliform bacteria (i.e., Citrobacter and
Klebsiella pneumoniae), our system reached an average CFU detection rate of
97.3% at 9 hours of incubation and an average recovery rate of 91.6% at ~12
hours. This TFT-based sensor can be applied to various microbiological
detection methods. Due to the large scalability, ultra-large field-of-view, and
low cost of the TFT-based image sensors, this platform can be integrated with
each agar plate to be tested and disposed of after the automated CFU count. The
imaging field-of-view of this platform can be cost-effectively increased to
&gt;100 cm^2 to provide a massive throughput for CFU detection using, e.g.,
roll-to-roll manufacturing of TFTs as used in the flexible display industry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Heavy Rain Removal to Detail Restoration: A Faster and Better Network. (arXiv:2205.03553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03553">
<div class="article-summary-box-inner">
<span><p>The dense rain accumulation in heavy rain can significantly wash out images
and thus destroy the background details of images. Although existing deep rain
removal models lead to improved performance for heavy rain removal, we find
that most of them ignore the detail reconstruction accuracy of rain-free
images. In this paper, we propose a dual-stage progressive enhancement network
(DPENet) to achieve effective deraining with structure-accurate rain-free
images. Two main modules are included in our framework, namely a rain streaks
removal network (R$^2$Net) and a detail reconstruction network (DRNet). The
former aims to achieve accurate rain removal, and the latter is designed to
recover the details of rain-free images. We introduce two main strategies
within our networks to achieve trade-off between the effectiveness of deraining
and the detail restoration of rain-free images. Firstly, a dilated dense
residual block (DDRB) within the rain streaks removal network is presented to
aggregate high/low level features of heavy rain. Secondly, an enhanced residual
pixel-wise attention block (ERPAB) within the detail reconstruction network is
designed for context information aggregation. We also propose a comprehensive
loss function to highlight the marginal and regional accuracy of rain-free
images. Extensive experiments on benchmark public datasets show both efficiency
and effectiveness of the proposed method in achieving structure-preserving
rain-free images for heavy rain removal. The source code and pre-trained models
can be found at \url{https://github.com/wybchd/DPENet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Target Active Object Tracking with Monte Carlo Tree Search and Target Motion Modeling. (arXiv:2205.03555v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03555">
<div class="article-summary-box-inner">
<span><p>In this work, we are dedicated to multi-target active object tracking (AOT),
where there are multiple targets as well as multiple cameras in the
environment. The goal is maximize the overall target coverage of all cameras.
Previous work makes a strong assumption that each camera is fixed in a location
and only allowed to rotate, which limits its application. In this work, we
relax the setting by allowing all cameras to both move along the boundary lines
and rotate. In our setting, the action space becomes much larger, which leads
to much higher computational complexity to identify the optimal action. To this
end, we propose to leverage the action selection from multi-agent reinforcement
learning (MARL) network to prune the search tree of Monte Carlo Tree Search
(MCTS) method, so as to find the optimal action more efficiently. Besides, we
model the motion of the targets to predict the future position of the targets,
which makes a better estimation of the future environment state in the MCTS
process. We establish a multi-target 2D environment to simulate the sports
games, and experimental results demonstrate that our method can effectively
improve the target coverage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Fusion Network for Multi-Oriented Object Detection. (arXiv:2205.03562v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03562">
<div class="article-summary-box-inner">
<span><p>In object detection, non-maximum suppression (NMS) methods are extensively
adopted to remove horizontal duplicates of detected dense boxes for generating
final object instances. However, due to the degraded quality of dense detection
boxes and not explicit exploration of the context information, existing NMS
methods via simple intersection-over-union (IoU) metrics tend to underperform
on multi-oriented and long-size objects detection. Distinguishing with general
NMS methods via duplicate removal, we propose a novel graph fusion network,
named GFNet, for multi-oriented object detection. Our GFNet is extensible and
adaptively fuse dense detection boxes to detect more accurate and holistic
multi-oriented object instances. Specifically, we first adopt a locality-aware
clustering algorithm to group dense detection boxes into different clusters. We
will construct an instance sub-graph for the detection boxes belonging to one
cluster. Then, we propose a graph-based fusion network via Graph Convolutional
Network (GCN) to learn to reason and fuse the detection boxes for generating
final instance boxes. Extensive experiments both on public available
multi-oriented text datasets (including MSRA-TD500, ICDAR2015, ICDAR2017-MLT)
and multi-oriented object datasets (DOTA) verify the effectiveness and
robustness of our method against general NMS methods in multi-oriented object
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement. (arXiv:2205.03569v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03569">
<div class="article-summary-box-inner">
<span><p>Compressed video action recognition has recently drawn growing attention,
since it remarkably reduces the storage and computational cost via replacing
raw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,
motion vectors and residuals). However, this task severely suffers from the
coarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB
and motion modalities. To address the two issues above, this paper proposes a
novel framework, namely Attentive Cross-modal Interaction Network with Motion
Enhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for
the RGB modality and the other for the motion modality. Particularly, the
motion stream employs a multi-scale block embedded with a denoising module to
enhance representation learning. The interaction between the two streams is
then strengthened by introducing the Selective Motion Complement (SMC) and
Cross-Modality Augment (CMA) modules, where SMC complements the RGB modality
with spatio-temporally attentive local motion features and CMA further combines
the two modalities with selective feature augmentation. Extensive experiments
on the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the
effectiveness and efficiency of MEACI-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utility-Oriented Underwater Image Quality Assessment Based on Transfer Learning. (arXiv:2205.03574v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03574">
<div class="article-summary-box-inner">
<span><p>The widespread image applications have greatly promoted the vision-based
tasks, in which the Image Quality Assessment (IQA) technique has become an
increasingly significant issue. For user enjoyment in multimedia systems, the
IQA exploits image fidelity and aesthetics to characterize user experience;
while for other tasks such as popular object recognition, there exists a low
correlation between utilities and perceptions. In such cases, the
fidelity-based and aesthetics-based IQA methods cannot be directly applied. To
address this issue, this paper proposes a utility-oriented IQA in object
recognition. In particular, we initialize our research in the scenario of
underwater fish detection, which is a critical task that has not yet been
perfectly addressed. Based on this task, we build an Underwater Image Utility
Database (UIUD) and a learning-based Underwater Image Utility Measure (UIUM).
Inspired by the top-down design of fidelity-based IQA, we exploit the deep
models of object recognition and transfer their features to our UIUM.
Experiments validate that the proposed transfer-learning-based UIUM achieves
promising performance in the recognition task. We envision our research
provides insights to bridge the researches of IQA and computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Chinese License Plate Detection and Recognition with High Efficiency. (arXiv:2205.03582v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03582">
<div class="article-summary-box-inner">
<span><p>Recently, deep learning-based methods have reached an excellent performance
on License Plate (LP) detection and recognition tasks. However, it is still
challenging to build a robust model for Chinese LPs since there are not enough
large and representative datasets. In this work, we propose a new dataset named
Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP
images as a supplement to the existing public benchmarks. The images are mainly
captured with electronic monitoring systems with detailed annotations. To our
knowledge, CRPD is the largest public multi-objective Chinese LP dataset with
annotations of vertices. With CRPD, a unified detection and recognition network
with high efficiency is presented as the baseline. The network is end-to-end
trainable with totally real-time inference efficiency (30 fps with 640p). The
experiments on several public benchmarks demonstrate that our method has
reached competitive performance. The code and dataset will be publicly
available at https://github.com/yxgong0/CRPD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPQE: Structure-and-Perception-Based Quality Evaluation for Image Super-Resolution. (arXiv:2205.03584v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03584">
<div class="article-summary-box-inner">
<span><p>The image Super-Resolution (SR) technique has greatly improved the visual
quality of images by enhancing their resolutions. It also calls for an
efficient SR Image Quality Assessment (SR-IQA) to evaluate those algorithms or
their generated images. In this paper, we focus on the SR-IQA under deep
learning and propose a Structure-and-Perception-based Quality Evaluation
(SPQE). In emerging deep-learning-based SR, a generated high-quality, visually
pleasing image may have different structures from its corresponding low-quality
image. In such case, how to balance the quality scores between no-reference
perceptual quality and referenced structural similarity is a critical issue. To
help ease this problem, we give a theoretical analysis on this tradeoff and
further calculate adaptive weights for the two types of quality scores. We also
propose two deep-learning-based regressors to model the no-reference and
referenced scores. By combining the quality scores and their weights, we
propose a unified SPQE metric for SR-IQA. Experimental results demonstrate that
the proposed method outperforms the state-of-the-arts in different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient VVC Intra Prediction Based on Deep Feature Fusion and Probability Estimation. (arXiv:2205.03587v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03587">
<div class="article-summary-box-inner">
<span><p>The ever-growing multimedia traffic has underscored the importance of
effective multimedia codecs. Among them, the up-to-date lossy video coding
standard, Versatile Video Coding (VVC), has been attracting attentions of video
coding community. However, the gain of VVC is achieved at the cost of
significant encoding complexity, which brings the need to realize fast encoder
with comparable Rate Distortion (RD) performance. In this paper, we propose to
optimize the VVC complexity at intra-frame prediction, with a two-stage
framework of deep feature fusion and probability estimation. At the first
stage, we employ the deep convolutional network to extract the spatialtemporal
neighboring coding features. Then we fuse all reference features obtained by
different convolutional kernels to determine an optimal intra coding depth. At
the second stage, we employ a probability-based model and the spatial-temporal
coherence to select the candidate partition modes within the optimal coding
depth. Finally, these selected depths and partitions are executed whilst
unnecessary computations are excluded. Experimental results on standard
database demonstrate the superiority of proposed method, especially for High
Definition (HD) and Ultra-HD (UHD) video sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Video Coding with GAN Latent Learning. (arXiv:2205.03599v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03599">
<div class="article-summary-box-inner">
<span><p>The introduction of multiple viewpoints inevitably increases the bitrates to
store and transmit video scenes. To reduce the compressed bitrates, researchers
have developed to skip intermediate viewpoints during compression and delivery,
and finally reconstruct them with Side Information (SI). Generally, the depth
maps can be utilized to construct SI; however, it shows inferior performance
with inaccurate reconstruction or high bitrates. In this paper, we propose a
multi-view video coding based on SI of Generative Adversarial Network (GAN). At
the encoder, we construct a spatio-temporal Epipolar Plane Image (EPI) and
further utilize convolutional network to extract the latent code of GAN as SI;
while at the decoder side, we combine the SI and adjacent viewpoints to
reconstruct intermediate views by the generator of GAN. In particular, we set a
joint encoder constraint of reconstruction cost and SI entropy, in order to
achieve an optimal tradeoff between reconstruction quality and bitrate
overhead. Experiments show a significantly improved Rate-Distortion (RD)
performance compared with the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Block-wise Pruning with Auxiliary Gating Structures for Deep Convolutional Neural Networks. (arXiv:2205.03602v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03602">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks are prevailing in deep learning tasks. However,
they suffer from massive cost issues when working on mobile devices. Network
pruning is an effective method of model compression to handle such problems.
This paper presents a novel structured network pruning method with auxiliary
gating structures which assigns importance marks to blocks in backbone network
as a criterion when pruning. Block-wise pruning is then realized by proposed
voting strategy, which is different from prevailing methods who prune a model
in small granularity like channel-wise. We further develop a three-stage
training scheduling for the proposed architecture incorporating knowledge
distillation for better performance. Our experiments demonstrate that our
method can achieve state-of-the-arts compression performance for the
classification tasks. In addition, our approach can integrate synergistically
with other pruning methods by providing pretrained models, thus achieving a
better performance than the unpruned model with over 93\% FLOPs reduced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Regularized Correlation Filter for UAV Object Tracking with adaptive Contextual Learning and Keyfilter Selection. (arXiv:2205.03627v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03627">
<div class="article-summary-box-inner">
<span><p>Recently, correlation filter has been widely applied in unmanned aerial
vehicle (UAV) tracking due to its high frame rates, robustness and low
calculation resources. However, it is fragile because of two inherent defects,
i.e, boundary effect and filter corruption. Some methods by enlarging the
search area can mitigate the boundary effect, yet introducing the undesired
background distractors. Another approaches can alleviate the temporal
degeneration of learned filters by introducing the temporal regularizer, which
depends on the assumption that the filers between consecutive frames should be
coherent. In fact, sometimes the filers at the ($t-1$)th frame is vulnerable to
heavy occlusion from backgrounds, which causes that the assumption does not
hold. To handle them, in this work, we propose a novel $\ell_{1}$
regularization correlation filter with adaptive contextual learning and
keyfilter selection for UAV tracking. Firstly, we adaptively detect the
positions of effective contextual distractors by the aid of the distribution of
local maximum values on the response map of current frame which is generated by
using the previous correlation filter model. Next, we eliminate inconsistent
labels for the tracked target by removing one on each distractor and develop a
new score scheme for each distractor. Then, we can select the keyfilter from
the filters pool by finding the maximal similarity between the target at the
current frame and the target template corresponding to each filter in the
filters pool. Finally, quantitative and qualitative experiments on three
authoritative UAV datasets show that the proposed method is superior to the
state-of-the-art tracking methods based on correlation filter framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Quality Assessment of Compressed Videos: A Subjective and Objective Study. (arXiv:2205.03630v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03630">
<div class="article-summary-box-inner">
<span><p>In the video coding process, the perceived quality of a compressed video is
evaluated by full-reference quality evaluation metrics. However, it is
difficult to obtain reference videos with perfect quality. To solve this
problem, it is critical to design no-reference compressed video quality
assessment algorithms, which assists in measuring the quality of experience on
the server side and resource allocation on the network side. Convolutional
Neural Network (CNN) has shown its advantage in Video Quality Assessment (VQA)
with promising successes in recent years. A large-scale quality database is
very important for learning accurate and powerful compressed video quality
metrics. In this work, a semi-automatic labeling method is adopted to build a
large-scale compressed video quality database, which allows us to label a large
number of compressed videos with manageable human workload. The resulting
Compressed Video quality database with Semi-Automatic Ratings (CVSAR), so far
the largest of compressed video quality database. We train a no-reference
compressed video quality assessment model with a 3D CNN for SpatioTemporal
Feature Extraction and Evaluation (STFEE). Experimental results demonstrate
that the proposed method outperforms state-of-the-art metrics and achieves
promising generalization performance in cross-database tests. The CVSAR
database and STFEE model will be made publicly available to facilitate
reproducible research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison Knowledge Translation for Generalizable Image Classification. (arXiv:2205.03633v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03633">
<div class="article-summary-box-inner">
<span><p>Deep learning has recently achieved remarkable performance in image
classification tasks, which depends heavily on massive annotation. However, the
classification mechanism of existing deep learning models seems to contrast to
humans' recognition mechanism. With only a glance at an image of the object
even unknown type, humans can quickly and precisely find other same category
objects from massive images, which benefits from daily recognition of various
objects. In this paper, we attempt to build a generalizable framework that
emulates the humans' recognition mechanism in the image classification task,
hoping to improve the classification performance on unseen categories with the
support of annotations of other categories. Specifically, we investigate a new
task termed Comparison Knowledge Translation (CKT). Given a set of fully
labeled categories, CKT aims to translate the comparison knowledge learned from
the labeled categories to a set of novel categories. To this end, we put
forward a Comparison Classification Translation Network (CCT-Net), which
comprises a comparison classifier and a matching discriminator. The comparison
classifier is devised to classify whether two images belong to the same
category or not, while the matching discriminator works together in an
adversarial manner to ensure whether classified results match the truth.
Exhaustive experiments show that CCT-Net achieves surprising generalization
ability on unseen categories and SOTA performance on target categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ultra-fast image categorization in vivo and in silico. (arXiv:2205.03635v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03635">
<div class="article-summary-box-inner">
<span><p>Humans are able to robustly categorize images and can, for instance, detect
the presence of an animal in a briefly flashed image in as little as 120 ms.
Initially inspired by neuroscience, deep-learning algorithms literally bloomed
up in the last decade such that the accuracy of machines is at present superior
to humans for visual recognition tasks. However, these artificial networks are
usually trained and evaluated on very specific tasks, for instance on the 1000
separate categories of ImageNet. In that regard, biological visual systems are
more flexible and efficient compared to artificial systems on generic
ecological tasks. In order to deepen this comparison, we re-trained the
standard VGG Convolutional Neural Network (CNN) on two independent tasks which
are ecologically relevant for humans: one task defined as detecting the
presence of an animal and the other as detecting the presence of an artifact.
We show that retraining the network achieves human-like performance level which
is reported in psychophysical tasks. We also compare the accuracy of the
detection on an image-by-image basis. This showed in particular that the two
models perform better when combining their outputs. Indeed, animals (e.g.
lions) tend to be less present in photographs containing artifacts (e.g.
buildings). These re-trained models could reproduce some unexpected behavioral
observations from humans psychophysics such as the robustness to rotations
(e.g. upside-down or slanted image) or to a grayscale transformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrating Label Distribution for Class-Imbalanced Barely-Supervised Knee Segmentation. (arXiv:2205.03644v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03644">
<div class="article-summary-box-inner">
<span><p>Segmentation of 3D knee MR images is important for the assessment of
osteoarthritis. Like other medical data, the volume-wise labeling of knee MR
images is expertise-demanded and time-consuming; hence semi-supervised learning
(SSL), particularly barely-supervised learning, is highly desirable for
training with insufficient labeled data. We observed that the class imbalance
problem is severe in the knee MR images as the cartilages only occupy 6% of
foreground volumes, and the situation becomes worse without sufficient labeled
data. To address the above problem, we present a novel framework for
barely-supervised knee segmentation with noisy and imbalanced labels. Our
framework leverages label distribution to encourage the network to put more
effort into learning cartilage parts. Specifically, we utilize 1.) label
quantity distribution for modifying the objective loss function to a
class-aware weighted form and 2.) label position distribution for constructing
a cropping probability mask to crop more sub-volumes in cartilage areas from
both labeled and unlabeled inputs. In addition, we design dual
uncertainty-aware sampling supervision to enhance the supervision of
low-confident categories for efficient unsupervised learning. Experiments show
that our proposed framework brings significant improvements by incorporating
the unlabeled data and alleviating the problem of class imbalance. More
importantly, our method outperforms the state-of-the-art SSL methods,
demonstrating the potential of our framework for the more challenging SSL
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Velocity Picking Using a Multi-Information Fusion Deep Semantic Segmentation Network. (arXiv:2205.03645v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03645">
<div class="article-summary-box-inner">
<span><p>Velocity picking, a critical step in seismic data processing, has been
studied for decades. Although manual picking can produce accurate normal
moveout (NMO) velocities from the velocity spectra of prestack gathers, it is
time-consuming and becomes infeasible with the emergence of large amount of
seismic data. Numerous automatic velocity picking methods have thus been
developed. In recent years, deep learning (DL) methods have produced good
results on the seismic data with medium and high signal-to-noise ratios (SNR).
Unfortunately, it still lacks a picking method to automatically generate
accurate velocities in the situations of low SNR. In this paper, we propose a
multi-information fusion network (MIFN) to estimate stacking velocity from the
fusion information of velocity spectra and stack gather segments (SGS). In
particular, we transform the velocity picking problem into a semantic
segmentation problem based on the velocity spectrum images. Meanwhile, the
information provided by SGS is used as a prior in the network to assist
segmentation. The experimental results on two field datasets show that the
picking results of MIFN are stable and accurate for the scenarios with medium
and high SNR, and it also performs well in low SNR scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Adversarial Learning for Skeleton-level to Pixel-level Adjustable Vessel Segmentation. (arXiv:2205.03646v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03646">
<div class="article-summary-box-inner">
<span><p>You can have your cake and eat it too. Microvessel segmentation in optical
coherence tomography angiography (OCTA) images remains challenging.
Skeleton-level segmentation shows clear topology but without diameter
information, while pixel-level segmentation shows a clear caliber but low
topology. To close this gap, we propose a novel label adversarial learning
(LAL) for skeleton-level to pixel-level adjustable vessel segmentation. LAL
mainly consists of two designs: a label adversarial loss and an embeddable
adjustment layer. The label adversarial loss establishes an adversarial
relationship between the two label supervisions, while the adjustment layer
adjusts the network parameters to match the different adversarial weights. Such
a design can efficiently capture the variation between the two supervisions,
making the segmentation continuous and tunable. This continuous process allows
us to recommend high-quality vessel segmentation with clear caliber and
topology. Experimental results show that our results outperform manual
annotations of current public datasets and conventional filtering effects.
Furthermore, such a continuous process can also be used to generate an
uncertainty map representing weak vessel boundaries and noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Inter-Class Distance for Semantic Segmentation. (arXiv:2205.03650v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03650">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation is widely adopted in semantic segmentation to reduce
the computation cost.The previous knowledge distillation methods for semantic
segmentation focus on pixel-wise feature alignment and intra-class feature
variation distillation, neglecting to transfer the knowledge of the inter-class
distance in the feature space, which is important for semantic segmentation. To
address this issue, we propose an Inter-class Distance Distillation (IDD)
method to transfer the inter-class distance in the feature space from the
teacher network to the student network. Furthermore, semantic segmentation is a
position-dependent task,thus we exploit a position information distillation
module to help the student network encode more position information. Extensive
experiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show
that our method is helpful to improve the accuracy of semantic segmentation
models and achieves the state-of-the-art performance. E.g. it boosts the
benchmark model("PSPNet+ResNet18") by 7.50% in accuracy on the Cityscapes
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust 3D Object Recognition with Dense-to-Sparse Deep Domain Adaptation. (arXiv:2205.03654v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03654">
<div class="article-summary-box-inner">
<span><p>Three-dimensional (3D) object recognition is crucial for intelligent
autonomous agents such as autonomous vehicles and robots alike to operate
effectively in unstructured environments. Most state-of-art approaches rely on
relatively dense point clouds and performance drops significantly for sparse
point clouds. Unsupervised domain adaption allows to minimise the discrepancy
between dense and sparse point clouds with minimal unlabelled sparse point
clouds, thereby saving additional sparse data collection, annotation and
retraining costs. In this work, we propose a novel method for point cloud based
object recognition with competitive performance with state-of-art methods on
dense and sparse point clouds while being trained only with dense point clouds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arrhythmia Classifier using Binarized Convolutional Neural Network for Resource-Constrained Devices. (arXiv:2205.03661v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03661">
<div class="article-summary-box-inner">
<span><p>Monitoring electrocardiogram signals is of great significance for the
diagnosis of arrhythmias. In recent years, deep learning and convolutional
neural networks have been widely used in the classification of cardiac
arrhythmias. However, the existing neural network applied to ECG signal
detection usually requires a lot of computing resources, which is not friendlyF
to resource-constrained equipment, and it is difficult to realize real-time
monitoring. In this paper, a binarized convolutional neural network suitable
for ECG monitoring is proposed, which is hardware-friendly and more suitable
for use in resource-constrained wearable devices. Targeting the MIT-BIH
arrhythmia database, the classifier based on this network reached an accuracy
of 95.67% in the five-class test. Compared with the proposed baseline
full-precision network with an accuracy of 96.45%, it is only 0.78% lower.
Importantly, it achieves 12.65 times the computing speedup, 24.8 times the
storage compression ratio, and only requires a quarter of the memory overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Playing Tic-Tac-Toe Games with Intelligent Single-pixel Imaging. (arXiv:2205.03663v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03663">
<div class="article-summary-box-inner">
<span><p>Single-pixel imaging (SPI) is a novel optical imaging technique by replacing
a two-dimensional pixelated sensor with a single-pixel detector and pattern
illuminations. SPI have been extensively used for various tasks related to
image acquisition and processing. In this work, a novel non-image-based task of
playing Tic-Tac-Toe games interactively is merged into the framework of SPI. An
optoelectronic artificial intelligent (AI) player with minimal digital
computation can detect the game states, generate optimal moves and display
output results mainly by pattern illumination and single-pixel detection.
Simulated and experimental results demonstrate the feasibility of proposed
scheme and its unbeatable performance against human players.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block Modulating Video Compression: An Ultra Low Complexity Image Compression Encoder for Resource Limited Platforms. (arXiv:2205.03677v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03677">
<div class="article-summary-box-inner">
<span><p>We consider the image and video compression on resource limited platforms. An
ultra low-cost image encoder, named Block Modulating Video Compression (BMVC)
with an encoding complexity ${\cal O}(1)$ is proposed to be implemented on
mobile platforms with low consumption of power and computation resources. We
also develop two types of BMVC decoders, implemented by deep neural networks.
The first BMVC decoder is based on the Plug-and-Play (PnP) algorithm, which is
flexible to different compression ratios. And the second decoder is a memory
efficient end-to-end convolutional neural network, which aims for real-time
decoding. Extensive results on the high definition images and videos
demonstrate the superior performance of the proposed codec and the robustness
against bit quantization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenISP: Neural ISP for Low-Light Machine Cognition. (arXiv:2205.03688v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03688">
<div class="article-summary-box-inner">
<span><p>Object detection in low-light conditions remains a challenging but important
problem with many practical implications. Some recent works show that, in
low-light conditions, object detectors using raw image data are more robust
than detectors using image data processed by a traditional ISP pipeline. To
improve detection performance in low-light conditions, one can fine-tune the
detector to use raw image data or use a dedicated low-light neural pipeline
trained with paired low- and normal-light data to restore and enhance the
image. However, different camera sensors have different spectral sensitivity
and learning-based models using raw images process data in the sensor-specific
color space. Thus, once trained, they do not guarantee generalization to other
camera sensors. We propose to improve generalization to unseen camera sensors
by implementing a minimal neural ISP pipeline for machine cognition, named
GenISP, that explicitly incorporates Color Space Transformation to a
device-independent color space. We also propose a two-stage color processing
implemented by two image-to-parameter modules that take down-sized image as
input and regress global color correction parameters. Moreover, we propose to
train our proposed GenISP under the guidance of a pre-trained object detector
and avoid making assumptions about perceptual quality of the image, but rather
optimize the image representation for machine cognition. At the inference
stage, GenISP can be paired with any object detector. We perform extensive
experiments to compare our method to other low-light image restoration and
enhancement methods in an extrinsic task-based evaluation and validate that
GenISP can generalize to unseen sensors and object detectors. Finally, we
contribute a low-light dataset of 7K raw images annotated with 46K bounding
boxes for task-based benchmarking of future low-light image restoration and
object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keratoconus Classifier for Smartphone-based Corneal Topographer. (arXiv:2205.03702v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03702">
<div class="article-summary-box-inner">
<span><p>Keratoconus is a severe eye disease that leads to deformation of the cornea.
It impacts people aged 10-25 years and is the leading cause of blindness in
that demography. Corneal topography is the gold standard for keratoconus
diagnosis. It is a non-invasive process performed using expensive and bulky
medical devices called corneal topographers. This makes it inaccessible to
large populations, especially in the Global South. Low-cost smartphone-based
corneal topographers, such as SmartKC, have been proposed to make keratoconus
diagnosis accessible. Similar to medical-grade topographers, SmartKC outputs
curvature heatmaps and quantitative metrics that need to be evaluated by
doctors for keratoconus diagnosis. An automatic scheme for evaluation of these
heatmaps and quantitative values can play a crucial role in screening
keratoconus in areas where doctors are not available. In this work, we propose
a dual-head convolutional neural network (CNN) for classifying keratoconus on
the heatmaps generated by SmartKC. Since SmartKC is a new device and only had a
small dataset (114 samples), we developed a 2-stage transfer learning strategy
-- using historical data collected from a medical-grade topographer and a
subset of SmartKC data -- to satisfactorily train our network. This, combined
with our domain-specific data augmentations, achieved a sensitivity of 91.3%
and a specificity of 94.2%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review on Viewpoints and Path-planning for UAV-based 3D Reconstruction. (arXiv:2205.03716v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03716">
<div class="article-summary-box-inner">
<span><p>Unmanned aerial vehicles (UAVs) are widely used platforms to carry data
capturing sensors for various applications. The reason for this success can be
found in many aspects: the high maneuverability of the UAVs, the capability of
performing autonomous data acquisition, flying at different heights, and the
possibility to reach almost any vantage point. The selection of appropriate
viewpoints and planning the optimum trajectories of UAVs is an emerging topic
that aims at increasing the automation, efficiency and reliability of the data
capturing process to achieve a dataset with desired quality. On the other hand,
3D reconstruction using the data captured by UAVs is also attracting attention
in research and industry. This review paper investigates a wide range of
model-free and model-based algorithms for viewpoint and path planning for 3D
reconstruction of large-scale objects. The analyzed approaches are limited to
those that employ a single-UAV as a data capturing platform for outdoor 3D
reconstruction purposes. In addition to discussing the evaluation strategies,
this paper also highlights the innovations and limitations of the investigated
approaches. It concludes with a critical analysis of the existing challenges
and future research perspectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Category-Independent Articulated Object Tracking with Factor Graphs. (arXiv:2205.03721v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03721">
<div class="article-summary-box-inner">
<span><p>Robots deployed in human-centric environments may need to manipulate a
diverse range of articulated objects, such as doors, dishwashers, and cabinets.
Articulated objects often come with unexpected articulation mechanisms that are
inconsistent with categorical priors: for example, a drawer might rotate about
a hinge joint instead of sliding open. We propose a category-independent
framework for predicting the articulation models of unknown objects from
sequences of RGB-D images. The prediction is performed by a two-step process:
first, a visual perception module tracks object part poses from raw images, and
second, a factor graph takes these poses and infers the articulation model
including the current configuration between the parts as a 6D twist. We also
propose a manipulation-oriented metric to evaluate predicted joint twists in
terms of how well a compliant robot controller would be able to manipulate the
articulated object given the predicted twist. We demonstrate that our visual
perception and factor graph modules outperform baselines on simulated data and
show the applicability of our factor graph on real world data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Point Cloud Generation for Class Segmentation Applications. (arXiv:2205.03738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03738">
<div class="article-summary-box-inner">
<span><p>Maintenance of industrial facilities is a growing hazard due to the
cumbersome process needed to identify infrastructure degradation. Digital Twins
have the potential to improve maintenance by monitoring the continuous digital
representation of infrastructure. However, the time needed to map the existing
geometry makes their use prohibitive. We previously developed class
segmentation algorithms to automate digital twinning, however a vast amount of
annotated point clouds is needed. Currently, synthetic data generation for
automated segmentation is non-existent. We used Helios++ to automatically
segment point clouds from 3D models. Our research has the potential to pave the
ground for efficient industrial class segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupled-and-Coupled Networks: Self-Supervised Hyperspectral Image Super-Resolution with Subpixel Fusion. (arXiv:2205.03742v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03742">
<div class="article-summary-box-inner">
<span><p>Enormous efforts have been recently made to super-resolve hyperspectral (HS)
images with the aid of high spatial resolution multispectral (MS) images. Most
prior works usually perform the fusion task by means of multifarious
pixel-level priors. Yet the intrinsic effects of a large distribution gap
between HS-MS data due to differences in the spatial and spectral resolution
are less investigated. The gap might be caused by unknown sensor-specific
properties or highly-mixed spectral information within one pixel (due to low
spatial resolution). To this end, we propose a subpixel-level HS
super-resolution framework by devising a novel decoupled-and-coupled network,
called DC-Net, to progressively fuse HS-MS information from the pixel- to
subpixel-level, from the image- to feature-level. As the name suggests, DC-Net
first decouples the input into common (or cross-sensor) and sensor-specific
components to eliminate the gap between HS-MS images before further fusion, and
then fully blends them by a model-guided coupled spectral unmixing (CSU) net.
More significantly, we append a self-supervised learning module behind the CSU
net by guaranteeing the material consistency to enhance the detailed
appearances of the restored HS product. Extensive experimental results show the
superiority of our method both visually and quantitatively and achieve a
significant improvement in comparison with the state-of-the-arts. Furthermore,
the codes and datasets will be available at
https://sites.google.com/view/danfeng-hong for the sake of reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Rubbing Restoration Using Generative Adversarial Networks. (arXiv:2205.03743v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03743">
<div class="article-summary-box-inner">
<span><p>Rubbing restorations are significant for preserving world cultural history.
In this paper, we propose the RubbingGAN model for restoring incomplete rubbing
characters. Specifically, we collect characters from the Zhang Menglong Bei and
build up the first rubbing restoration dataset. We design the first generative
adversarial network for rubbing restoration. Based on the dataset we collect,
we apply the RubbingGAN to learn the Zhang Menglong Bei font style and restore
the characters. The results of experiments show that RubbingGAN can repair both
slightly and severely incomplete rubbing characters fast and effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Select and Calibrate the Low-confidence: Dual-Channel Consistency based Graph Convolutional Networks. (arXiv:2205.03753v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03753">
<div class="article-summary-box-inner">
<span><p>The Graph Convolutional Networks (GCNs) have achieved excellent results in
node classification tasks, but the model's performance at low label rates is
still unsatisfactory. Previous studies in Semi-Supervised Learning (SSL) for
graph have focused on using network predictions to generate soft pseudo-labels
or instructing message propagation, which inevitably contains the incorrect
prediction due to the over-confident in the predictions. Our proposed
Dual-Channel Consistency based Graph Convolutional Networks (DCC-GCN) uses
dual-channel to extract embeddings from node features and topological
structures, and then achieves reliable low-confidence and high-confidence
samples selection based on dual-channel consistency. We further confirmed that
the low-confidence samples obtained based on dual-channel consistency were low
in accuracy, constraining the model's performance. Unlike previous studies
ignoring low-confidence samples, we calibrate the feature embeddings of the
low-confidence samples by using the neighborhood's high-confidence samples. Our
experiments have shown that the DCC-GCN can more accurately distinguish between
low-confidence and high-confidence samples, and can also significantly improve
the accuracy of low-confidence samples. We conducted extensive experiments on
the benchmark datasets and demonstrated that DCC-GCN is significantly better
than state-of-the-art baselines at different label rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Dynamic Embedding for Video Object Segmentation. (arXiv:2205.03761v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03761">
<div class="article-summary-box-inner">
<span><p>Space-time memory (STM) based video object segmentation (VOS) networks
usually keep increasing memory bank every several frames, which shows excellent
performance. However, 1) the hardware cannot withstand the ever-increasing
memory requirements as the video length increases. 2) Storing lots of
information inevitably introduces lots of noise, which is not conducive to
reading the most important information from the memory bank. In this paper, we
propose a Recurrent Dynamic Embedding (RDE) to build a memory bank of constant
size. Specifically, we explicitly generate and update RDE by the proposed
Spatio-temporal Aggregation Module (SAM), which exploits the cue of historical
information. To avoid error accumulation owing to the recurrent usage of SAM,
we propose an unbiased guidance loss during the training stage, which makes SAM
more robust in long videos. Moreover, the predicted masks in the memory bank
are inaccurate due to the inaccurate network inference, which affects the
segmentation of the query frame. To address this problem, we design a novel
self-correction strategy so that the network can repair the embeddings of masks
with different qualities in the memory bank. Extensive experiments show our
method achieves the best tradeoff between performance and speed. Code is
available at https://github.com/Limingxing00/RDE-VOS-CVPR2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoViST:Learning Robust Metrics for Visual Storytelling. (arXiv:2205.03774v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03774">
<div class="article-summary-box-inner">
<span><p>Visual storytelling (VST) is the task of generating a story paragraph that
describes a given image sequence. Most existing storytelling approaches have
evaluated their models using traditional natural language generation metrics
like BLEU or CIDEr. However, such metrics based on n-gram matching tend to have
poor correlation with human evaluation scores and do not explicitly consider
other criteria necessary for storytelling such as sentence structure or topic
coherence. Moreover, a single score is not enough to assess a story as it does
not inform us about what specific errors were made by the model. In this paper,
we propose 3 evaluation metrics sets that analyses which aspects we would look
for in a good story: 1) visual grounding, 2) coherence, and 3) non-redundancy.
We measure the reliability of our metric sets by analysing its correlation with
human judgement scores on a sample of machine stories obtained from 4
state-of-the-arts models trained on the Visual Storytelling Dataset (VIST). Our
metric sets outperforms other metrics on human correlation, and could be served
as a learning based evaluation metric set that is complementary to existing
rule-based metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SparseTT: Visual Tracking with Sparse Transformers. (arXiv:2205.03776v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03776">
<div class="article-summary-box-inner">
<span><p>Transformers have been successfully applied to the visual tracking task and
significantly promote tracking performance. The self-attention mechanism
designed to model long-range dependencies is the key to the success of
Transformers. However, self-attention lacks focusing on the most relevant
information in the search regions, making it easy to be distracted by
background. In this paper, we relieve this issue with a sparse attention
mechanism by focusing the most relevant information in the search regions,
which enables a much accurate tracking. Furthermore, we introduce a double-head
predictor to boost the accuracy of foreground-background classification and
regression of target bounding boxes, which further improve the tracking
performance. Extensive experiments show that, without bells and whistles, our
method significantly outperforms the state-of-the-art approaches on LaSOT,
GOT-10k, TrackingNet, and UAV123, while running at 40 FPS. Notably, the
training time of our method is reduced by 75% compared to that of TransT. The
source code and models are available at https://github.com/fzh0917/SparseTT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Cycled Generative Adversarial Networks for Real-World Face Super-Resolution. (arXiv:2205.03777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03777">
<div class="article-summary-box-inner">
<span><p>Real-world face super-resolution (SR) is a highly ill-posed image restoration
task. The fully-cycled Cycle-GAN architecture is widely employed to achieve
promising performance on face SR, but prone to produce artifacts upon
challenging cases in real-world scenarios, since joint participation in the
same degradation branch will impact final performance due to huge domain gap
between real-world and synthetic LR ones obtained by generators. To better
exploit the powerful generative capability of GAN for real-world face SR, in
this paper, we establish two independent degradation branches in the forward
and backward cycle-consistent reconstruction processes, respectively, while the
two processes share the same restoration branch. Our Semi-Cycled Generative
Adversarial Networks (SCGAN) is able to alleviate the adverse effects of the
domain gap between the real-world LR face images and the synthetic LR ones, and
to achieve accurate and robust face SR performance by the shared restoration
branch regularized by both the forward and backward cycle-consistent learning
processes. Experiments on two synthetic and two real-world datasets demonstrate
that, our SCGAN outperforms the state-of-the-art methods on recovering the face
structures/details and quantitative metrics for real-world face SR. The code
will be publicly released at https://github.com/HaoHou-98/SCGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-parametric Depth Distribution Modelling based Depth Inference for Multi-view Stereo. (arXiv:2205.03783v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03783">
<div class="article-summary-box-inner">
<span><p>Recent cost volume pyramid based deep neural networks have unlocked the
potential of efficiently leveraging high-resolution images for depth inference
from multi-view stereo. In general, those approaches assume that the depth of
each pixel follows a unimodal distribution. Boundary pixels usually follow a
multi-modal distribution as they represent different depths; Therefore, the
assumption results in an erroneous depth prediction at the coarser level of the
cost volume pyramid and can not be corrected in the refinement levels leading
to wrong depth predictions. In contrast, we propose constructing the cost
volume by non-parametric depth distribution modeling to handle pixels with
unimodal and multi-modal distributions. Our approach outputs multiple depth
hypotheses at the coarser level to avoid errors in the early stage. As we
perform local search around these multiple hypotheses in subsequent levels, our
approach does not maintain the rigid depth spatial ordering and, therefore, we
introduce a sparse cost aggregation network to derive information within each
volume. We evaluate our approach extensively on two benchmark datasets: DTU and
Tanks &amp; Temples. Our experimental results show that our model outperforms
existing methods by a large margin and achieves superior performance on
boundary regions. Code is available at https://github.com/NVlabs/NP-CVP-MVSNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-Class Knowledge Distillation for Face Presentation Attack Detection. (arXiv:2205.03792v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03792">
<div class="article-summary-box-inner">
<span><p>Face presentation attack detection (PAD) has been extensively studied by
research communities to enhance the security of face recognition systems.
Although existing methods have achieved good performance on testing data with
similar distribution as the training data, their performance degrades severely
in application scenarios with data of unseen distributions. In situations where
the training and testing data are drawn from different domains, a typical
approach is to apply domain adaptation techniques to improve face PAD
performance with the help of target domain data. However, it has always been a
non-trivial challenge to collect sufficient data samples in the target domain,
especially for attack samples. This paper introduces a teacher-student
framework to improve the cross-domain performance of face PAD with one-class
domain adaptation. In addition to the source domain data, the framework
utilizes only a few genuine face samples of the target domain. Under this
framework, a teacher network is trained with source domain samples to provide
discriminative feature representations for face PAD. Student networks are
trained to mimic the teacher network and learn similar representations for
genuine face samples of the target domain. In the test phase, the similarity
score between the representations of the teacher and student networks is used
to distinguish attacks from genuine ones. To evaluate the proposed framework
under one-class domain adaptation settings, we devised two new protocols and
conducted extensive experiments. The experimental results show that our method
outperforms baselines under one-class domain adaptation settings and even
state-of-the-art methods with unsupervised domain adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Structured Block-Term Tensor Decomposition For Hyperspectral Unmixing. (arXiv:2205.03798v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03798">
<div class="article-summary-box-inner">
<span><p>The block-term tensor decomposition model with multilinear rank-$(L_r,L_r,1)$
terms (or, the "LL1 tensor decomposition" in short) offers a valuable
alternative for hyperspectral unmixing (HU) under the linear mixture model.
Particularly, the LL1 decomposition ensures the endmember/abundance
identifiability in scenarios where such guarantees are not supported by the
classic matrix factorization (MF) approaches. However, existing LL1-based HU
algorithms use a three-factor parameterization of the tensor (i.e., the
hyperspectral image cube), which leads to a number of challenges including high
per-iteration complexity, slow convergence, and difficulties in incorporating
structural prior information. This work puts forth an LL1 tensor
decomposition-based HU algorithm that uses a constrained two-factor
re-parameterization of the tensor data. As a consequence, a two-block
alternating gradient projection (GP)-based LL1 algorithm is proposed for HU.
With carefully designed projection solvers, the GP algorithm enjoys a
relatively low per-iteration complexity. Like in MF-based HU, the factors under
our parameterization correspond to the endmembers and abundances. Thus, the
proposed framework is natural to incorporate physics-motivated priors that
arise in HU. The proposed algorithm often attains orders-of-magnitude speedup
and substantial HU performance gains compared to the existing three-factor
parameterization-based HU algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Past and Future Motion Guided Network for Audio Visual Event Localization. (arXiv:2205.03802v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03802">
<div class="article-summary-box-inner">
<span><p>In recent years, audio-visual event localization has attracted much
attention. It's purpose is to detect the segment containing audio-visual events
and recognize the event category from untrimmed videos. Existing methods use
audio-guided visual attention to lead the model pay attention to the spatial
area of the ongoing event, devoting to the correlation between audio and visual
information but ignoring the correlation between audio and spatial motion. We
propose a past and future motion extraction (pf-ME) module to mine the visual
motion from videos ,embedded into the past and future motion guided network
(PFAGN), and motion guided audio attention (MGAA) module to achieve focusing on
the information related to interesting events in audio modality through the
past and future visual motion. We choose AVE as the experimental verification
dataset and the experiments show that our method outperforms the
state-of-the-arts in both supervised and weakly-supervised settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Few-shot Image Generation. (arXiv:2205.03805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03805">
<div class="article-summary-box-inner">
<span><p>Modern GANs excel at generating high quality and diverse images. However,
when transferring the pretrained GANs on small target data (e.g., 10-shot), the
generator tends to replicate the training samples.
</p>
<p>Several methods have been proposed to address this few-shot image generation
task, but there is a lack of effort to analyze them under a unified framework.
</p>
<p>As our first contribution, we propose a framework to analyze existing methods
during the adaptation. Our analysis discovers that while some methods have
disproportionate focus on diversity preserving which impede quality
improvement, all methods achieve similar quality after convergence.
</p>
<p>Therefore, the better methods are those that can slow down diversity
degradation. Furthermore, our analysis reveals that there is still plenty of
room to further slow down diversity degradation.
</p>
<p>Informed by our analysis and to slow down the diversity degradation of the
target generator during adaptation, our second contribution proposes to apply
mutual information (MI) maximization to retain the source domain's rich
multi-level diversity information in the target domain generator.
</p>
<p>We propose to perform MI maximization by contrastive loss (CL), leverage the
generator and discriminator as two feature encoders to extract different
multi-level features for computing CL. We refer to our method as Dual
Contrastive Learning (DCL).
</p>
<p>Extensive experiments on several public datasets show that, while leading to
a slower diversity-degrading generator during adaptation, our proposed DCL
brings visually pleasant quality and state-of-the-art quantitative performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Tracking with Cyclic Shifting Window Attention. (arXiv:2205.03806v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03806">
<div class="article-summary-box-inner">
<span><p>Transformer architecture has been showing its great strength in visual object
tracking, for its effective attention mechanism. Existing transformer-based
approaches adopt the pixel-to-pixel attention strategy on flattened image
features and unavoidably ignore the integrity of objects. In this paper, we
propose a new transformer architecture with multi-scale cyclic shifting window
attention for visual object tracking, elevating the attention from pixel to
window level. The cross-window multi-scale attention has the advantage of
aggregating attention at different scales and generates the best fine-scale
match for the target object. Furthermore, the cyclic shifting strategy brings
greater accuracy by expanding the window samples with positional information,
and at the same time saves huge amounts of computational power by removing
redundant calculations. Extensive experiments demonstrate the superior
performance of our method, which also sets the new state-of-the-art records on
five challenging datasets, along with the VOT2020, UAV123, LaSOT, TrackingNet,
and GOT-10k benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fingerprint Template Invertibility: Minutiae vs. Deep Templates. (arXiv:2205.03809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03809">
<div class="article-summary-box-inner">
<span><p>Much of the success of fingerprint recognition is attributed to
minutiae-based fingerprint representation. It was believed that minutiae
templates could not be inverted to obtain a high fidelity fingerprint image,
but this assumption has been shown to be false. The success of deep learning
has resulted in alternative fingerprint representations (embeddings), in the
hope that they might offer better recognition accuracy as well as
non-invertibility of deep network-based templates. We evaluate whether deep
fingerprint templates suffer from the same reconstruction attacks as the
minutiae templates. We show that while a deep template can be inverted to
produce a fingerprint image that could be matched to its source image, deep
templates are more resistant to reconstruction attacks than minutiae templates.
In particular, reconstructed fingerprint images from minutiae templates yield a
TAR of about 100.0% (98.3%) @ FAR of 0.01% for type-I (type-II) attacks using a
state-of-the-art commercial fingerprint matcher, when tested on NIST SD4. The
corresponding attack performance for reconstructed fingerprint images from deep
templates using the same commercial matcher yields a TAR of less than 1% for
both type-I and type-II attacks; however, when the reconstructed images are
matched using the same deep network, they achieve a TAR of 85.95% (68.10%) for
type-I (type-II) attacks. Furthermore, what is missing from previous
fingerprint template inversion studies is an evaluation of the black-box attack
performance, which we perform using 3 different state-of-the-art fingerprint
matchers. We conclude that fingerprint images generated by inverting minutiae
templates are highly susceptible to both white-box and black-box attack
evaluations, while fingerprint images generated by deep templates are resistant
to black-box evaluations and comparatively less susceptible to white-box
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PGADA: Perturbation-Guided Adversarial Alignment for Few-shot Learning Under the Support-Query Shift. (arXiv:2205.03817v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03817">
<div class="article-summary-box-inner">
<span><p>Few-shot learning methods aim to embed the data to a low-dimensional
embedding space and then classify the unseen query data to the seen support
set. While these works assume that the support set and the query set lie in the
same embedding space, a distribution shift usually occurs between the support
set and the query set, i.e., the Support-Query Shift, in the real world. Though
optimal transportation has shown convincing results in aligning different
distributions, we find that the small perturbations in the images would
significantly misguide the optimal transportation and thus degrade the model
performance. To relieve the misalignment, we first propose a novel adversarial
data augmentation method, namely Perturbation-Guided Adversarial Alignment
(PGADA), which generates the hard examples in a self-supervised manner. In
addition, we introduce Regularized Optimal Transportation to derive a smooth
optimal transportation plan. Extensive experiments on three benchmark datasets
manifest that our framework significantly outperforms the eleven
state-of-the-art methods on three datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Homography Estimation with Coplanarity-Aware GAN. (arXiv:2205.03821v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03821">
<div class="article-summary-box-inner">
<span><p>Estimating homography from an image pair is a fundamental problem in image
alignment. Unsupervised learning methods have received increasing attention in
this field due to their promising performance and label-free training. However,
existing methods do not explicitly consider the problem of plane-induced
parallax, which will make the predicted homography compromised on multiple
planes. In this work, we propose a novel method HomoGAN to guide unsupervised
homography estimation to focus on the dominant plane. First, a multi-scale
transformer network is designed to predict homography from the feature pyramids
of input images in a coarse-to-fine fashion. Moreover, we propose an
unsupervised GAN to impose coplanarity constraint on the predicted homography,
which is realized by using a generator to predict a mask of aligned regions,
and then a discriminator to check if two masked feature maps are induced by a
single homography. To validate the effectiveness of HomoGAN and its components,
we conduct extensive experiments on a large-scale dataset, and the results show
that our matching error is 22% lower than the previous SOTA method. Code is
available at https://github.com/megvii-research/HomoGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Geometry-Aware Cross Guidance Network for Stereo Image Inpainting. (arXiv:2205.03825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03825">
<div class="article-summary-box-inner">
<span><p>Currently, single image inpainting has achieved promising results based on
deep convolutional neural networks. However, inpainting on stereo images with
missing regions has not been explored thoroughly, which is also a significant
but different problem. One crucial requirement for stereo image inpainting is
stereo consistency. To achieve it, we propose an Iterative Geometry-Aware Cross
Guidance Network (IGGNet). The IGGNet contains two key ingredients, i.e., a
Geometry-Aware Attention (GAA) module and an Iterative Cross Guidance (ICG)
strategy. The GAA module relies on the epipolar geometry cues and learns the
geometry-aware guidance from one view to another, which is beneficial to make
the corresponding regions in two views consistent. However, learning guidance
from co-existing missing regions is challenging. To address this issue, the ICG
strategy is proposed, which can alternately narrow down the missing regions of
the two views in an iterative manner. Experimental results demonstrate that our
proposed network outperforms the latest stereo image inpainting model and
state-of-the-art single image inpainting models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Automated Binary Pattern Extraction For Finger Vein Identification using Double Optimization Stages-Based Unsupervised Learning Approach. (arXiv:2205.03840v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03840">
<div class="article-summary-box-inner">
<span><p>Today, finger vein identification is gaining popularity as a potential
biometric identification framework solution. Machine learning-based
unsupervised, supervised, and deep learning algorithms have had a significant
influence on finger vein detection and recognition at the moment. Deep
learning, on the other hand, necessitates a large number of training datasets
that must be manually produced and labeled. In this research, we offer a
completely automated unsupervised learning strategy for training dataset
creation. Our method is intended to extract and build a decent binary mask
training dataset completely automated. In this technique, two optimization
steps are devised and employed. The initial stage of optimization is to create
a completely automated unsupervised image clustering based on finger vein image
localization. Worldwide finger vein pattern orientation estimation is employed
in the second optimization to optimize the retrieved finger vein lines.
Finally, the proposed system achieves 99.6 - percent pattern extraction
accuracy, which is significantly higher than other common unsupervised learning
methods like k-means and Fuzzy C-Means (FCM).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Conditioning the Input Noise for Controlled Image Generation with Diffusion Models. (arXiv:2205.03859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03859">
<div class="article-summary-box-inner">
<span><p>Conditional image generation has paved the way for several breakthroughs in
image editing, generating stock photos and 3-D object generation. This
continues to be a significant area of interest with the rise of new
state-of-the-art methods that are based on diffusion models. However, diffusion
models provide very little control over the generated image, which led to
subsequent works exploring techniques like classifier guidance, that provides a
way to trade off diversity with fidelity. In this work, we explore techniques
to condition diffusion models with carefully crafted input noise artifacts.
This allows generation of images conditioned on semantic attributes. This is
different from existing approaches that input Gaussian noise and further
introduce conditioning at the diffusion model's inference step. Our experiments
over several examples and conditional settings show the potential of our
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework. (arXiv:2205.03860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03860">
<div class="article-summary-box-inner">
<span><p>Vision-language pre-training (VLP) relying on large-scale pre-training
datasets has shown premier performance on various downstream tasks. In this
sense, a complete and fair benchmark (i.e., including large-scale pre-training
datasets and a variety of downstream datasets) is essential for VLP. But how to
construct such a benchmark in Chinese remains a critical problem. To this end,
we develop a large-scale Chinese cross-modal benchmark called Zero for AI
researchers to fairly compare VLP models. We release two pre-training datasets
and five fine-tuning datasets for downstream tasks. Furthermore, we propose a
novel pre-training framework of pre-Ranking + Ranking for cross-modal learning.
Specifically, we apply global contrastive pre-ranking to learn the individual
representations of images and Chinese texts, respectively. We then fuse the
representations in a fine-grained ranking manner via an image-text cross
encoder and a text-image cross encoder. To further enhance the capability of
the model, we propose a two-way distillation strategy consisting of
target-guided Distillation and feature-guided Distillation. For simplicity, we
call our model R2D2. We achieve state-of-the-art performance on four public
cross-modal datasets and our five downstream datasets. The datasets, models and
codes will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Learning of Hard Positives for Place Recognition. (arXiv:2205.03871v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03871">
<div class="article-summary-box-inner">
<span><p>Image retrieval methods for place recognition learn global image descriptors
that are used for fetching geo-tagged images at inference time. Recent works
have suggested employing weak and self-supervision for mining hard positives
and hard negatives in order to improve localization accuracy and robustness to
visibility changes (e.g. in illumination or view point). However, generating
hard positives, which is essential for obtaining robustness, is still limited
to hard-coded or global augmentations. In this work we propose an adversarial
method to guide the creation of hard positives for training image retrieval
networks. Our method learns local and global augmentation policies which will
increase the training loss, while the image retrieval network is forced to
learn more powerful features for discriminating increasingly difficult
examples. This approach allows the image retrieval network to generalize beyond
the hard examples presented in the data and learn features that are robust to a
wide range of variations. Our method achieves state-of-the-art recalls on the
Pitts250 and Tokyo 24/7 benchmarks and outperforms recent image retrieval
methods on the rOxford and rParis datasets by a noticeable margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Semi-Supervised Learning for Text Recognition. (arXiv:2205.03873v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03873">
<div class="article-summary-box-inner">
<span><p>Until recently, the number of public real-world text images was insufficient
for training scene text recognizers. Therefore, most modern training methods
rely on synthetic data and operate in a fully supervised manner. Nevertheless,
the amount of public real-world text images has increased significantly lately,
including a great deal of unlabeled data. Leveraging these resources requires
semi-supervised approaches; however, the few existing methods do not account
for vision-language multimodality structure and therefore suboptimal for
state-of-the-art multimodal architectures. To bridge this gap, we present
semi-supervised learning for multimodal text recognizers (SemiMTR) that
leverages unlabeled data at each modality training phase. Notably, our method
refrains from extra training stages and maintains the current three-stage
multimodal training procedure. Our algorithm starts by pretraining the vision
model through a single-stage training that unifies self-supervised learning
with supervised training. More specifically, we extend an existing visual
representation learning algorithm and propose the first contrastive-based
method for scene text recognition. After pretraining the language model on a
text corpus, we fine-tune the entire network via a sequential, character-level,
consistency regularization between weakly and strongly augmented views of text
images. In a novel setup, consistency is enforced on each modality separately.
Extensive experiments validate that our method outperforms the current training
schemes and achieves state-of-the-art results on multiple scene text
recognition benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WKGM: Weight-K-space Generative Model for Parallel Imaging Reconstruction. (arXiv:2205.03883v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03883">
<div class="article-summary-box-inner">
<span><p>Parallel Imaging (PI) is one of the most im-portant and successful
developments in accelerating magnetic resonance imaging (MRI). Recently deep
learning PI has emerged as an effective technique to accelerate MRI.
Nevertheless, most approaches have so far been based image domain. In this
work, we propose to explore the k-space domain via robust generative modeling
for flexible PI reconstruction, coined weight-k-space generative model (WKGM).
Specifically, WKGM is a generalized k-space domain model, where the k-space
weighting technology and high-dimensional space strategy are efficiently
incorporated for score-based generative model training, resulting in good and
robust reconstruction. In addition, WKGM is flexible and thus can
synergistically combine various traditional k-space PI models, generating
learning-based priors to produce high-fidelity reconstructions. Experimental
results on datasets with varying sampling patterns and acceleration factors
demonstrate that WKGM can attain state-of-the-art reconstruction results under
the well-learned k-space generative prior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Adaptation for Recipe Retrieval with Mixup. (arXiv:2205.03891v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03891">
<div class="article-summary-box-inner">
<span><p>Cross-modal recipe retrieval has attracted research attention in recent
years, thanks to the availability of large-scale paired data for training.
Nevertheless, obtaining adequate recipe-image pairs covering the majority of
cuisines for supervised learning is difficult if not impossible. By
transferring knowledge learnt from a data-rich cuisine to a data-scarce
cuisine, domain adaptation sheds light on this practical problem. Nevertheless,
existing works assume recipes in source and target domains are mostly
originated from the same cuisine and written in the same language. This paper
studies unsupervised domain adaptation for image-to-recipe retrieval, where
recipes in source and target domains are in different languages. Moreover, only
recipes are available for training in the target domain. A novel recipe mixup
method is proposed to learn transferable embedding features between the two
domains. Specifically, recipe mixup produces mixed recipes to form an
intermediate domain by discretely exchanging the section(s) between source and
target recipes. To bridge the domain gap, recipe mixup loss is proposed to
enforce the intermediate domain to locate in the shortest geodesic path between
source and target domains in the recipe embedding space. By using Recipe 1M
dataset as source domain (English) and Vireo-FoodTransfer dataset as target
domain (Chinese), empirical experiments verify the effectiveness of recipe
mixup for cross-lingual adaptation in the context of image-to-recipe retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvMAE: Masked Convolution Meets Masked Autoencoders. (arXiv:2205.03892v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03892">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) become widely-adopted architectures for various
vision tasks. Masked auto-encoding for feature pretraining and multi-scale
hybrid convolution-transformer architectures can further unleash the potentials
of ViT, leading to state-of-the-art performances on image classification,
detection and semantic segmentation. In this paper, our ConvMAE framework
demonstrates that multi-scale hybrid convolution-transformer can learn more
discriminative representations via the mask auto-encoding scheme. However,
directly using the original masking strategy leads to the heavy computational
cost and pretraining-finetuning discrepancy. To tackle the issue, we adopt the
masked convolution to prevent information leakage in the convolution blocks. A
simple block-wise masking strategy is proposed to ensure computational
efficiency. We also propose to more directly supervise the multi-scale features
of the encoder to boost multi-scale features. Based on our pretrained ConvMAE
models, ConvMAE-Base improves ImageNet-1K finetuning accuracy by 1.4% compared
with MAE-Base. On object detection, ConvMAE-Base finetuned for only 25 epochs
surpasses MAE-Base fined-tuned for 100 epochs by 2.9% box AP and 2.2% mask AP
respectively. Code and pretrained models are available at
https://github.com/Alpha-VL/ConvMAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preservation of High Frequency Content for Deep Learning-Based Medical Image Classification. (arXiv:2205.03898v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03898">
<div class="article-summary-box-inner">
<span><p>Chest radiographs are used for the diagnosis of multiple critical illnesses
(e.g., Pneumonia, heart failure, lung cancer), for this reason, systems for the
automatic or semi-automatic analysis of these data are of particular interest.
An efficient analysis of large amounts of chest radiographs can aid physicians
and radiologists, ultimately allowing for better medical care of lung-, heart-
and chest-related conditions. We propose a novel Discrete Wavelet Transform
(DWT)-based method for the efficient identification and encoding of visual
information that is typically lost in the down-sampling of high-resolution
radiographs, a common step in computer-aided diagnostic pipelines. Our proposed
approach requires only slight modifications to the input of existing
state-of-the-art Convolutional Neural Networks (CNNs), making it easily
applicable to existing image classification frameworks. We show that the extra
high-frequency components offered by our method increased the classification
performance of several CNNs in benchmarks employing the NIH Chest-8 and
ImageNet-2017 datasets. Based on our results we hypothesize that providing
frequency-specific coefficients allows the CNNs to specialize in the
identification of structures that are particular to a frequency band,
ultimately increasing classification performance, without an increase in
computational load. The implementation of our work is available at
github.com/DeclanMcIntosh/LeGallCuda.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SoftPool++: An Encoder-Decoder Network for Point Cloud Completion. (arXiv:2205.03899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03899">
<div class="article-summary-box-inner">
<span><p>We propose a novel convolutional operator for the task of point cloud
completion. One striking characteristic of our approach is that, conversely to
related work it does not require any max-pooling or voxelization operation.
Instead, the proposed operator used to learn the point cloud embedding in the
encoder extracts permutation-invariant features from the point cloud via a
soft-pooling of feature activations, which are able to preserve fine-grained
geometric details. These features are then passed on to a decoder architecture.
Due to the compression in the encoder, a typical limitation of this type of
architectures is that they tend to lose parts of the input shape structure. We
propose to overcome this limitation by using skip connections specifically
devised for point clouds, where links between corresponding layers in the
encoder and the decoder are established. As part of these connections, we
introduce a transformation matrix that projects the features from the encoder
to the decoder and vice-versa. The quantitative and qualitative results on the
task of object completion from partial scans on the ShapeNet dataset show that
incorporating our approach achieves state-of-the-art performance in shape
completion both at low and high resolutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Discovery and Composition of Object Light Fields. (arXiv:2205.03923v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03923">
<div class="article-summary-box-inner">
<span><p>Neural scene representations, both continuous and discrete, have recently
emerged as a powerful new paradigm for 3D scene understanding. Recent efforts
have tackled unsupervised discovery of object-centric neural scene
representations. However, the high cost of ray-marching, exacerbated by the
fact that each object representation has to be ray-marched separately, leads to
insufficiently sampled radiance fields and thus, noisy renderings, poor
framerates, and high memory and time complexity during training and rendering.
Here, we propose to represent objects in an object-centric, compositional scene
representation as light fields. We propose a novel light field compositor
module that enables reconstructing the global light field from a set of
object-centric light fields. Dubbed Compositional Object Light Fields (COLF),
our method enables unsupervised learning of object-centric neural scene
representations, state-of-the-art reconstruction and novel view synthesis
performance on standard datasets, and rendering and training speeds at orders
of magnitude faster than existing 3D approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Resolution UAV Image Generation for Sorghum Panicle Detection. (arXiv:2205.03947v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03947">
<div class="article-summary-box-inner">
<span><p>The number of panicles (or heads) of Sorghum plants is an important
phenotypic trait for plant development and grain yield estimation. The use of
Unmanned Aerial Vehicles (UAVs) enables the capability of collecting and
analyzing Sorghum images on a large scale. Deep learning can provide methods
for estimating phenotypic traits from UAV images but requires a large amount of
labeled data. The lack of training data due to the labor-intensive ground
truthing of UAV images causes a major bottleneck in developing methods for
Sorghum panicle detection and counting. In this paper, we present an approach
that uses synthetic training images from generative adversarial networks (GANs)
for data augmentation to enhance the performance of Sorghum panicle detection
and counting. Our method can generate synthetic high-resolution UAV RGB images
with panicle labels by using image-to-image translation GANs with a limited
ground truth dataset of real UAV RGB images. The results show the improvements
in panicle detection and counting using our data augmentation approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation. (arXiv:2205.03962v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03962">
<div class="article-summary-box-inner">
<span><p>Virtual facial avatars will play an increasingly important role in immersive
communication, games and the metaverse, and it is therefore critical that they
be inclusive. This requires accurate recovery of the appearance, represented by
albedo, regardless of age, sex, or ethnicity. While significant progress has
been made on estimating 3D facial geometry, albedo estimation has received less
attention. The task is fundamentally ambiguous because the observed color is a
function of albedo and lighting, both of which are unknown. We find that
current methods are biased towards light skin tones due to (1) strongly biased
priors that prefer lighter pigmentation and (2) algorithmic solutions that
disregard the light/albedo ambiguity. To address this, we propose a new
evaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation
and, hence, fairness. Specifically, we create the first facial albedo
evaluation benchmark where subjects are balanced in terms of skin color, and
measure accuracy using the Individual Typology Angle (ITA) metric. We then
address the light/albedo ambiguity by building on a key observation: the image
of the full scene -- as opposed to a cropped image of the face -- contains
important information about lighting that can be used for disambiguation. TRUST
regresses facial albedo by conditioning both on the face region and a global
illumination signal obtained from the scene image. Our experimental results
show significant improvement compared to state-of-the-art methods on albedo
estimation, both in terms of accuracy and fairness. The evaluation benchmark
and code will be made available for research purposes at
https://trust.is.tue.mpg.de.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Private Eye: On the Limits of Textual Screen Peeking via Eyeglass Reflections in Video Conferencing. (arXiv:2205.03971v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03971">
<div class="article-summary-box-inner">
<span><p>Personal video conferencing has become the new norm after COVID-19 caused a
seismic shift from in-person meetings and phone calls to video conferencing for
daily communications and sensitive business. Video leaks participants'
on-screen information because eyeglasses and other reflective objects
unwittingly expose partial screen contents. Using mathematical modeling and
human subjects experiments, this research explores the extent to which emerging
webcams might leak recognizable textual information gleamed from eyeglass
reflections captured by webcams. The primary goal of our work is to measure,
compute, and predict the factors, limits, and thresholds of recognizability as
webcam technology evolves in the future. Our work explores and characterizes
the viable threat models based on optical attacks using multi-frame super
resolution techniques on sequences of video frames. Our experimental results
and models show it is possible to reconstruct and recognize on-screen text with
a height as small as 10 mm with a 720p webcam. We further apply this threat
model to web textual content with varying attacker capabilities to find
thresholds at which text becomes recognizable. Our user study with 20
participants suggests present-day 720p webcams are sufficient for adversaries
to reconstruct textual content on big-font websites. Our models further show
that the evolution toward 4K cameras will tip the threshold of text leakage to
reconstruction of most header texts on popular websites. Our research proposes
near-term mitigations, and justifies the importance of following the principle
of least privilege for long-term defense against this attack. For
privacy-sensitive scenarios, it's further recommended to develop technologies
that blur all objects by default, then only unblur what is absolutely necessary
to facilitate natural-looking conversations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Nonlocal Graph-PDE and Higher-Order Geometric Integration for Image Labeling. (arXiv:2205.03991v1 [math.OC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03991">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel nonlocal partial difference equation (PDE) for
labeling metric data on graphs. The PDE is derived as nonlocal
reparametrization of the assignment flow approach that was introduced in
\textit{J.~Math.~Imaging \&amp; Vision} 58(2), 2017. Due to this parameterization,
solving the PDE numerically is shown to be equivalent to computing the
Riemannian gradient flow with respect to a nonconvex potential. We devise an
entropy-regularized difference-of-convex-functions (DC) decomposition of this
potential and show that the basic geometric Euler scheme for integrating the
assignment flow is equivalent to solving the PDE by an established DC
programming scheme. Moreover, the viewpoint of geometric integration reveals a
basic way to exploit higher-order information of the vector field that drives
the assignment flow, in order to devise a novel accelerated DC programming
scheme. A detailed convergence analysis of both numerical schemes is provided
and illustrated by numerical experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hardware-Robust In-RRAM-Computing for Object Detection. (arXiv:2205.03996v1 [cs.AR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03996">
<div class="article-summary-box-inner">
<span><p>In-memory computing is becoming a popular architecture for deep-learning
hardware accelerators recently due to its highly parallel computing, low power,
and low area cost. However, in-RRAM computing (IRC) suffered from large device
variation and numerous nonideal effects in hardware. Although previous
approaches including these effects in model training successfully improved
variation tolerance, they only considered part of the nonideal effects and
relatively simple classification tasks. This paper proposes a joint hardware
and software optimization strategy to design a hardware-robust IRC macro for
object detection. We lower the cell current by using a low word-line voltage to
enable a complete convolution calculation in one operation that minimizes the
impact of nonlinear addition. We also implement ternary weight mapping and
remove batch normalization for better tolerance against device variation, sense
amplifier variation, and IR drop problem. An extra bias is included to overcome
the limitation of the current sensing range. The proposed approach has been
successfully applied to a complex object detection task with only 3.85\% mAP
drop, whereas a naive design suffers catastrophic failure under these nonideal
effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Row-wise Accelerator for Vision Transformer. (arXiv:2205.03998v1 [cs.AR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03998">
<div class="article-summary-box-inner">
<span><p>Following the success of the natural language processing, the transformer for
vision applications has attracted significant attention in recent years due to
its excellent performance. However, existing deep learning hardware
accelerators for vision cannot execute this structure efficiently due to
significant model architecture differences. As a result, this paper proposes
the hardware accelerator for vision transformers with row-wise scheduling,
which decomposes major operations in vision transformers as a single dot
product primitive for a unified and efficient execution. Furthermore, by
sharing weights in columns, we can reuse the data and reduce the usage of
memory. The implementation with TSMC 40nm CMOS technology only requires 262K
gate count and 149KB SRAM buffer for 403.2 GOPS throughput at 600MHz clock
frequency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Photo-to-Shape Material Transfer for Diverse Structures. (arXiv:2205.04018v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04018">
<div class="article-summary-box-inner">
<span><p>We introduce a method for assigning photorealistic relightable materials to
3D shapes in an automatic manner. Our method takes as input a photo exemplar of
a real object and a 3D object with segmentation, and uses the exemplar to guide
the assignment of materials to the parts of the shape, so that the appearance
of the resulting shape is as similar as possible to the exemplar. To accomplish
this goal, our method combines an image translation neural network with a
material assignment neural network. The image translation network translates
the color from the exemplar to a projection of the 3D shape and the part
segmentation from the projection to the exemplar. Then, the material prediction
network assigns materials from a collection of realistic materials to the
projected parts, based on the translated images and perceptual similarity of
the materials. One key idea of our method is to use the translation network to
establish a correspondence between the exemplar and shape projection, which
allows us to transfer materials between objects with diverse structures.
Another key idea of our method is to use the two pairs of (color, segmentation)
images provided by the image translation to guide the material assignment,
which enables us to ensure the consistency in the assignment. We demonstrate
that our method allows us to assign materials to shapes so that their
appearances better resemble the input exemplars, improving the quality of the
results over the state-of-the-art method, and allowing us to automatically
create thousands of shapes with high-quality photorealistic materials. Code and
data for this paper are available at https://github.com/XiangyuSu611/TMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I Know What You Draw: Learning Grasp Detection Conditioned on a Few Freehand Sketches. (arXiv:2205.04026v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04026">
<div class="article-summary-box-inner">
<span><p>In this paper, we are interested in the problem of generating target grasps
by understanding freehand sketches. The sketch is useful for the persons who
cannot formulate language and the cases where a textual description is not
available on the fly. However, very few works are aware of the usability of
this novel interactive way between humans and robots. To this end, we propose a
method to generate a potential grasp configuration relevant to the
sketch-depicted objects. Due to the inherent ambiguity of sketches with
abstract details, we take the advantage of the graph by incorporating the
structure of the sketch to enhance the representation ability. This
graph-represented sketch is further validated to improve the generalization of
the network, capable of learning the sketch-queried grasp detection by using a
small collection (around 100 samples) of hand-drawn sketches. Additionally, our
model is trained and tested in an end-to-end manner which is easy to be
implemented in real-world applications. Experiments on the multi-object VMRD
and GraspNet-1Billion datasets demonstrate the good generalization of the
proposed method. The physical robot experiments confirm the utility of our
method in object-cluttered scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning 6-DoF Object Poses to Grasp Category-level Objects by Language Instructions. (arXiv:2205.04028v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04028">
<div class="article-summary-box-inner">
<span><p>This paper studies the task of any objects grasping from the known categories
by free-form language instructions. This task demands the technique in computer
vision, natural language processing, and robotics. We bring these disciplines
together on this open challenge, which is essential to human-robot interaction.
Critically, the key challenge lies in inferring the category of objects from
linguistic instructions and accurately estimating the 6-DoF information of
unseen objects from the known classes. In contrast, previous works focus on
inferring the pose of object candidates at the instance level. This
significantly limits its applications in real-world scenarios.In this paper, we
propose a language-guided 6-DoF category-level object localization model to
achieve robotic grasping by comprehending human intention. To this end, we
propose a novel two-stage method. Particularly, the first stage grounds the
target in the RGB image through language description of names, attributes, and
spatial relations of objects. The second stage extracts and segments point
clouds from the cropped depth image and estimates the full 6-DoF object pose at
category-level. Under such a manner, our approach can locate the specific
object by following human instructions, and estimate the full 6-DoF pose of a
category-known but unseen instance which is not utilized for training the
model. Extensive experimental results show that our method is competitive with
the state-of-the-art language-conditioned grasp method. Importantly, we deploy
our approach on a physical robot to validate the usability of our framework in
real-world applications. Please refer to the supplementary for the demo videos
of our robot experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental-DETR: Incremental Few-Shot Object Detection via Self-Supervised Learning. (arXiv:2205.04042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04042">
<div class="article-summary-box-inner">
<span><p>Incremental few-shot object detection aims at detecting novel classes without
forgetting knowledge of the base classes with only a few labeled training data
from the novel classes. Most related prior works are on incremental object
detection that rely on the availability of abundant training samples per novel
class that substantially limits the scalability to real-world setting where
novel data can be scarce. In this paper, we propose the Incremental-DETR that
does incremental few-shot object detection via fine-tuning and self-supervised
learning on the DETR object detector. To alleviate severe over-fitting with few
novel class data, we first fine-tune the class-specific components of DETR with
self-supervision from additional object proposals generated using Selective
Search as pseudo labels. We further introduce a incremental few-shot
fine-tuning strategy with knowledge distillation on the class-specific
components of DETR to encourage the network in detecting novel classes without
catastrophic forgetting. Extensive experiments conducted on standard
incremental object detection and incremental few-shot object detection settings
show that our approach significantly outperforms state-of-the-art methods by a
large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Co-attentional Transformer reconstructs 100x ultra-fast/low-dose whole-body PET from longitudinal images and anatomically guided MRI. (arXiv:2205.04044v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04044">
<div class="article-summary-box-inner">
<span><p>Despite its tremendous value for the diagnosis, treatment monitoring and
surveillance of children with cancer, whole body staging with positron emission
tomography (PET) is time consuming and associated with considerable radiation
exposure. 100x (1% of the standard clinical dosage) ultra-low-dose/ultra-fast
whole-body PET reconstruction has the potential for cancer imaging with
unprecedented speed and improved safety, but it cannot be achieved by the naive
use of machine learning techniques. In this study, we utilize the global
similarity between baseline and follow-up PET and magnetic resonance (MR)
images to develop Masked-LMCTrans, a longitudinal multi-modality co-attentional
CNN-Transformer that provides interaction and joint reasoning between serial
PET/MRs of the same patient. We mask the tumor area in the referenced baseline
PET and reconstruct the follow-up PET scans. In this manner, Masked-LMCTrans
reconstructs 100x almost-zero radio-exposure whole-body PET that was not
possible before. The technique also opens a new pathway for longitudinal
radiology imaging reconstruction, a significantly under-explored area to date.
Our model was trained and tested with Stanford PET/MRI scans of pediatric
lymphoma patients and evaluated externally on PET/MRI images from T\"ubingen
University. The high image quality of the reconstructed 100x whole-body PET
images resulting from the application of Masked-LMCTrans will substantially
advance the development of safer imaging approaches and shorter exam-durations
for pediatric patients, as well as expand the possibilities for frequent
longitudinal monitoring of these patients by PET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Carving out the low surface brightness universe with NoiseChisel. (arXiv:1909.11230v2 [astro-ph.IM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.11230">
<div class="article-summary-box-inner">
<span><p>NoiseChisel is a program to detect very low signal-to-noise ratio (S/N)
features with minimal assumptions on their morphology. It was introduced in
2015 and released within a collection of data analysis programs and libraries
known as GNU Astronomy Utilities (Gnuastro). The 10th stable version of
Gnuastro was released in August 2019 and NoiseChisel has significantly
improved: detecting even fainter signal, enabling better user control over its
inner workings, and many bug fixes. The most important change until version
0.10 is that NoiseChisel's segmentation features have been moved into a new
program called Segment. Another major change is the final growth strategy of
its true detections, for example NoiseChisel is able to detect the outer wings
of M51 down to S/N of 0.25, or 25.97 mag/arcsec2 on a single-exposure SDSS
image (r-band). Segment is also able to detect the localized HII regions as
"clumps" much more successfully. For a detailed list of improvements after
version 0.10, see the most recent manual. Finally, to orchestrate a controlled
analysis, the concept of reproducibility is discussed: this paper itself is
exactly reproducible (commit 751467d).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moderately Supervised Learning: Definition, Framework and Generality. (arXiv:2008.11945v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.11945">
<div class="article-summary-box-inner">
<span><p>Supervised learning (SL) has achieved remarkable success in numerous
artificial intelligence (AI) applications. In the current literature, by
referring to the properties of the ground-truth labels prepared for the
training data set, SL is roughly categorized as fully supervised learning (FSL)
and weakly supervised learning (WSL). FSL concerns the situation where the
training data set is assigned with ideal ground-truth labels, while WSL
concerns the situation where the training data set is assigned with non-ideal
ground-truth labels. However, solutions for various FSL tasks have shown that
the given ground-truth labels are not always learnable, and the target
transformation from the given ground-truth labels to learnable targets can
significantly affect the performance of the final FSL solutions. The roughness
of the FSL category conceals some details that are critical to building the
appropriate solutions for some specific FSL tasks. In this paper, taking into
consideration the properties of the target transformation from the given
ground-truth labels to learnable targets, we firstly categorize FSL into three
narrower sub-types and then focus on the sub-type moderately supervised
learning (MSL) that concerns the situation where the given ground-truth labels
are ideal, but due to the simplicity in annotation of the given ground-truth
labels, careful designs are required to transform the given ground-truth labels
into learnable targets. From the perspectives of the definition, framework and
generality, we comprehensively illustrate MSL to reveal what details are
concealed by the roughness of the FSL category. At the meantime, via presenting
the definition, framework and generality of MSL, this paper as well establishes
a tutorial for AI application engineers to refer to viewing a problem to be
solved from the mathematicians' vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset. (arXiv:2009.02899v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.02899">
<div class="article-summary-box-inner">
<span><p>Post-hoc analysis is a popular category in eXplainable artificial
intelligence (XAI) study. In particular, methods that generate heatmaps have
been used to explain the deep neural network (DNN), a black-box model. Heatmaps
can be appealing due to the intuitive and visual ways to understand them but
assessing their qualities might not be straightforward. Different ways to
assess heatmaps' quality have their own merits and shortcomings. This paper
introduces a synthetic dataset that can be generated adhoc along with the
ground-truth heatmaps for more objective quantitative assessment. Each sample
data is an image of a cell with easily recognized features that are
distinguished from localization ground-truth mask, hence facilitating a more
transparent assessment of different XAI methods. Comparison and recommendations
are made, shortcomings are clarified along with suggestions for future research
directions to handle the finer details of select post-hoc analysis methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResNet-LDDMM: Advancing the LDDMM Framework using Deep Residual Networks. (arXiv:2102.07951v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07951">
<div class="article-summary-box-inner">
<span><p>In deformable registration, the geometric framework - large deformation
diffeomorphic metric mapping or LDDMM, in short - has inspired numerous
techniques for comparing, deforming, averaging and analyzing shapes or images.
Grounded in flows, which are akin to the equations of motion used in fluid
dynamics, LDDMM algorithms solve the flow equation in the space of plausible
deformations, i.e. diffeomorphisms. In this work, we make use of deep residual
neural networks to solve the non-stationary ODE (flow equation) based on a
Euler's discretization scheme. The central idea is to represent time-dependent
velocity fields as fully connected ReLU neural networks (building blocks) and
derive optimal weights by minimizing a regularized loss function. Computing
minimizing paths between deformations, thus between shapes, turns to find
optimal network parameters by back-propagating over the intermediate building
blocks. Geometrically, at each time step, ResNet-LDDMM searches for an optimal
partition of the space into multiple polytopes, and then computes optimal
velocity vectors as affine transformations on each of these polytopes. As a
result, different parts of the shape, even if they are close (such as two
fingers of a hand), can be made to belong to different polytopes, and therefore
be moved in different directions without costing too much energy. Importantly,
we show how diffeomorphic transformations, or more precisely bilipshitz
transformations, are predicted by our algorithm. We illustrate these ideas on
diverse registration problems of 3D shapes under complex topology-preserving
transformations. We thus provide essential foundations for more advanced shape
variability analysis under a novel joint geometric-neural networks
Riemannian-like framework, i.e. ResNet-LDDMM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Contrastive Optimization of Siamese Networks for Place Recognition. (arXiv:2103.06638v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06638">
<div class="article-summary-box-inner">
<span><p>Visual place recognition is a challenging task in computer vision and a key
component of camera-based localization and navigation systems. Recently,
Convolutional Neural Networks (CNNs) achieved high results and good
generalization capabilities. They are usually trained using pairs or triplets
of images labeled as either similar or dissimilar, in a binary fashion. In
practice, the similarity between two images is not binary, but continuous.
Furthermore, training these CNNs is computationally complex and involves costly
pair and triplet mining strategies.
</p>
<p>We propose a Generalized Contrastive loss (GCL) function that relies on image
similarity as a continuous measure, and use it to train a siamese CNN.
Furthermore, we present three techniques for automatic annotation of image
pairs with labels indicating their degree of similarity, and deploy them to
re-annotate the MSLS, TB-Places, and 7Scenes datasets.
</p>
<p>We demonstrate that siamese CNNs trained using the GCL function and the
improved annotations consistently outperform their binary counterparts. Our
models trained on MSLS outperform the state-of-the-art methods, including
NetVLAD, NetVLAD-SARE, AP-GeM and Patch-NetVLAD, and generalize well on the
Pittsburgh30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons
datasets. Furthermore, training a siamese network using the GCL function does
not require complex pair mining. We release the source code at
https://github.com/marialeyvallina/generalized_contrastive_loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Deep Multi-Graph Matching and 3D Geometry Learning from Inhomogeneous 2D Image Collections. (arXiv:2103.17229v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17229">
<div class="article-summary-box-inner">
<span><p>Graph matching aims to establish correspondences between vertices of graphs
such that both the node and edge attributes agree. Various learning-based
methods were recently proposed for finding correspondences between image key
points based on deep graph matching formulations. While these approaches mainly
focus on learning node and edge attributes, they completely ignore the 3D
geometry of the underlying 3D objects depicted in the 2D images. We fill this
gap by proposing a trainable framework that takes advantage of graph neural
networks for learning a deformable 3D geometry model from inhomogeneous image
collections, i.e.,~a set of images that depict different instances of objects
from the same category. Experimentally, we demonstrate that our method
outperforms recent learning-based approaches for graph matching considering
both accuracy and cycle-consistency error, while we in addition obtain the
underlying 3D geometry of the objects depicted in the 2D images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advanced Deep Networks for 3D Mitochondria Instance Segmentation. (arXiv:2104.07961v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07961">
<div class="article-summary-box-inner">
<span><p>Mitochondria instance segmentation from electron microscopy (EM) images has
seen notable progress since the introduction of deep learning methods. In this
paper, we propose two advanced deep networks, named Res-UNet-R and Res-UNet-H,
for 3D mitochondria instance segmentation from Rat and Human samples.
Specifically, we design a simple yet effective anisotropic convolution block
and deploy a multi-scale training strategy, which together boost the
segmentation performance. Moreover, we enhance the generalizability of the
trained models on the test set by adding a denoising operation as
pre-processing. In the Large-scale 3D Mitochondria Instance Segmentation
Challenge at ISBI 2021, our method ranks the 1st place. Code is available at
https://github.com/Limingxing00/MitoEM2021-Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting-GNN: Boosting Algorithm for Graph Networks on Imbalanced Node Classification. (arXiv:2105.11625v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11625">
<div class="article-summary-box-inner">
<span><p>The Graph Neural Network (GNN) has been widely used for graph data
representation. However, the existing researches only consider the ideal
balanced dataset, and the imbalanced dataset is rarely considered. Traditional
methods such as resampling, reweighting, and synthetic samples that deal with
imbalanced datasets are no longer applicable in GNN. This paper proposes an
ensemble model called Boosting-GNN, which uses GNNs as the base classifiers
during boosting. In Boosting-GNN, higher weights are set for the training
samples that are not correctly classified by the previous classifier, thus
achieving higher classification accuracy and better reliability. Besides,
transfer learning is used to reduce computational cost and increase fitting
ability. Experimental results indicate that the proposed Boosting-GNN model
achieves better performance than GCN, GraphSAGE, GAT, SGC, N-GCN, and most
advanced reweighting and resampling methods on synthetic imbalanced datasets,
with an average performance improvement of 4.5%
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey and Taxonomy on Image Dehazing Based on Deep Learning. (arXiv:2106.03323v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03323">
<div class="article-summary-box-inner">
<span><p>With the development of convolutional neural networks, hundreds of deep
learning based dehazing methods have been proposed. In this paper, we provide a
comprehensive survey on supervised, semi-supervised, and unsupervised dehazing.
We first discuss the physical model, datasets, network modules, loss functions,
and evaluation metrics that are commonly used. Then, the main contributions of
various dehazing algorithms are categorized and summarized. Further,
quantitative and qualitative experiments of various baseline methods are
carried out. Finally, the unsolved issues and challenges that can inspire the
future research are pointed out. A collection of useful dehazing materials is
available at https://github.com/Xiaofeng-life/AwesomeDehazing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salvage of Supervision in Weakly Supervised Object Detection. (arXiv:2106.04073v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04073">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object detection~(WSOD) has recently attracted much
attention. However, the lack of bounding-box supervision makes its accuracy
much lower than fully supervised object detection (FSOD), and currently modern
FSOD techniques cannot be applied to WSOD. To bridge the performance and
technical gaps between WSOD and FSOD, this paper proposes a new framework,
Salvage of Supervision (SoS), with the key idea being to harness every
potentially useful supervisory signal in WSOD: the weak image-level labels, the
pseudo-labels, and the power of semi-supervised object detection. This paper
proposes new approaches to utilize these weak and noisy signals effectively,
and shows that each type of supervisory signal brings in notable improvements,
outperforms existing WSOD methods (which mainly use only the weak labels) by
large margins. The proposed SoS-WSOD method also has the ability to freely use
modern FSOD techniques. SoS-WSOD achieves 64.4 $m\text{AP}_{50}$ on VOC2007,
61.9 $m\text{AP}_{50}$ on VOC2012 and 16.6 $m\text{AP}_{50:95}$ on MS-COCO, and
also has fast inference speed. Ablations and visualization further verify the
effectiveness of SoS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-domain Contrastive Learning for Unsupervised Domain Adaptation. (arXiv:2106.05528v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05528">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a fully-labeled source domain to a different unlabeled target domain. Most
existing UDA methods learn domain-invariant feature representations by
minimizing feature distances across domains. In this work, we build upon
contrastive self-supervised learning to align features so as to reduce the
domain discrepancy between training and testing sets. Exploring the same set of
categories shared by both domains, we introduce a simple yet effective
framework CDCL, for domain alignment. In particular, given an anchor image from
one domain, we minimize its distances to cross-domain samples from the same
class relative to those from different categories. Since target labels are
unavailable, we use a clustering-based approach with carefully initialized
centers to produce pseudo labels. In addition, we demonstrate that CDCL is a
general framework and can be adapted to the data-free setting, where the source
data are unavailable during training, with minimal modification. We conduct
experiments on two widely used domain adaptation benchmarks, i.e., Office-31
and VisDA-2017, for image classification tasks, and demonstrate that CDCL
achieves state-of-the-art performance on both datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs. (arXiv:2106.06959v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06959">
<div class="article-summary-box-inner">
<span><p>The discovery of the disentanglement properties of the latent space in GANs
motivated a lot of research to find the semantically meaningful directions on
it. In this paper, we suggest that the disentanglement property is closely
related to the geometry of the latent space. In this regard, we propose an
unsupervised method for finding the semantic-factorizing directions on the
intermediate latent space of GANs based on the local geometry. Intuitively, our
proposed method, called Local Basis, finds the principal variation of the
latent space in the neighborhood of the base latent variable. Experimental
results show that the local principal variation corresponds to the semantic
factorization and traversing along it provides strong robustness to image
traversal. Moreover, we suggest an explanation for the limited success in
finding the global traversal directions in the latent space, especially W-space
of StyleGAN2. We show that W-space is warped globally by comparing the local
geometry, discovered from Local Basis, through the metric on Grassmannian
Manifold. The global warpage implies that the latent space is not well-aligned
globally and therefore the global traversal directions are bound to show
limited success on it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07806">
<div class="article-summary-box-inner">
<span><p>Machine learning is revolutionizing image-based diagnostics in pathology and
radiology. ML models have shown promising results in research settings, but
their lack of interoperability has been a major barrier for clinical
integration and evaluation. The DICOM a standard specifies Information Object
Definitions and Services for the representation and communication of digital
images and related information, including image-derived annotations and
analysis results. However, the complexity of the standard represents an
obstacle for its adoption in the ML community and creates a need for software
libraries and tools that simplify working with data sets in DICOM format. Here
we present the highdicom library, which provides a high-level application
programming interface for the Python programming language that abstracts
low-level details of the standard and enables encoding and decoding of
image-derived information in DICOM format in a few lines of Python code. The
highdicom library ties into the extensive Python ecosystem for image processing
and machine learning. Simultaneously, by simplifying creation and parsing of
DICOM-compliant files, highdicom achieves interoperability with the medical
imaging systems that hold the data used to train and run ML models, and
ultimately communicate and store model outputs for clinical use. We demonstrate
through experiments with slide microscopy and computed tomography imaging,
that, by bridging these two ecosystems, highdicom enables developers to train
and evaluate state-of-the-art ML models in pathology and radiology while
remaining compliant with the DICOM standard and interoperable with clinical
systems at all stages. To promote standardization of ML research and streamline
the ML model development and deployment process, we made the library available
free and open-source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification. (arXiv:2106.07846v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07846">
<div class="article-summary-box-inner">
<span><p>Unsupervised person re-identification (Re-ID) aims to match pedestrian images
from different camera views in unsupervised setting. Existing methods for
unsupervised person Re-ID are usually built upon the pseudo labels from
clustering. However, the quality of clustering depends heavily on the quality
of the learned features, which are overwhelmingly dominated by the colors in
images especially in the unsupervised setting. In this paper, we propose a
Cluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised
person Re-ID, in which cluster structure is leveraged to guide the feature
learning in a properly designed asymmetric contrastive learning framework. To
be specific, we propose a novel cluster-level contrastive loss to help the
siamese network effectively mine the invariance in feature learning with
respect to the cluster structure within and between different data augmentation
views, respectively. Extensive experiments conducted on three benchmark
datasets demonstrate superior performance of our proposal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images. (arXiv:2106.07873v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07873">
<div class="article-summary-box-inner">
<span><p>State-of-the-art (SOTA) Generative Models (GMs) can synthesize
photo-realistic images that are hard for humans to distinguish from genuine
photos. Identifying and understanding manipulated media are crucial to mitigate
the social concerns on the potential misuse of GMs. We propose to perform
reverse engineering of GMs to infer model hyperparameters from the images
generated by these models. We define a novel problem, "model parsing", as
estimating GM network architectures and training loss functions by examining
their generated images - a task seemingly impossible for human beings. To
tackle this problem, we propose a framework with two components: a Fingerprint
Estimation Network (FEN), which estimates a GM fingerprint from a generated
image by training with four constraints to encourage the fingerprint to have
desired properties, and a Parsing Network (PN), which predicts network
architecture and loss functions from the estimated fingerprints. To evaluate
our approach, we collect a fake image dataset with 100K images generated by 116
different GMs. Extensive experiments show encouraging results in parsing the
hyperparameters of the unseen models. Finally, our fingerprint estimation can
be leveraged for deepfake detection and image attribution, as we show by
reporting SOTA results on both the deepfake detection (Celeb-DF) and image
attribution benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking. (arXiv:2107.10433v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10433">
<div class="article-summary-box-inner">
<span><p>Many RGB-T trackers attempt to attain robust feature representation by
utilizing an adaptive weighting scheme (or attention mechanism). Different from
these works, we propose a new dynamic modality-aware filter generation module
(named MFGNet) to boost the message communication between visible and thermal
data by adaptively adjusting the convolutional kernels for various input images
in practical tracking. Given the image pairs as input, we first encode their
features with the backbone network. Then, we concatenate these feature maps and
generate dynamic modality-aware filters with two independent networks. The
visible and thermal filters will be used to conduct a dynamic convolutional
operation on their corresponding input feature maps respectively. Inspired by
residual connection, both the generated visible and thermal feature maps will
be summarized with input feature maps. The augmented feature maps will be fed
into the RoI align module to generate instance-level features for subsequent
classification. To address issues caused by heavy occlusion, fast motion and
out-of-view, we propose to conduct a joint local and global search by
exploiting a new direction-aware target driven attention mechanism. The spatial
and temporal recurrent neural network is used to capture the direction-aware
context for accurate global attention prediction. Extensive experiments on
three large-scale RGB-T tracking benchmark datasets validated the effectiveness
of our proposed algorithm. The source code of this paper is available at
\textcolor{magenta}{\url{https://github.com/wangxiao5791509/MFG_RGBT_Tracking_PyTorch}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v7 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12056">
<div class="article-summary-box-inner">
<span><p>Existing machines are functionally specific tools that were made for easy
prediction and control. Tomorrow's machines may be closer to biological systems
in their mutability, resilience, and autonomy. But first they must be capable
of sequentially learning, and retaining, new information without being exposed
to it arbitrarily often. Past efforts to engineer such systems have sought to
build or regulate artificial neural networks using disjoint sets of weights
that are uniquely sensitive to specific tasks or inputs. This has not yet
enabled continual learning over long sequences of previously unseen data
without corrupting existing knowledge: a problem known as catastrophic
forgetting. In this paper, we introduce a system that can learn sequentially
over previously unseen datasets (ImageNet, CIFAR-100) with little forgetting
over time. This is done by controlling the activity of weights in a
convolutional neural network on the basis of inputs using top-down regulation
generated by a second feed-forward neural network. We find that our method
learns continually under domain transfer with sparse bursts of activity in
weights that are recycled across tasks, rather than by maintaining
task-specific modules. Sparse synaptic bursting is found to balance activity
and suppression such that new functions can be learned without corrupting
extant knowledge, thus mirroring the balance of order and disorder in systems
at the edge of chaos. This behavior emerges during a prior pre-training (or
'meta-learning') phase in which regulated synapses are selectively
disinhibited, or grown, from an initial state of uniform suppression through
prediction error minimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LDC-VAE: A Latent Distribution Consistency Approach to Variational AutoEncoders. (arXiv:2109.10640v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10640">
<div class="article-summary-box-inner">
<span><p>Variational autoencoders (VAEs), as an important aspect of generative models,
have received a lot of research interests and reached many successful
applications. However, it is always a challenge to achieve the consistency
between the learned latent distribution and the prior latent distribution when
optimizing the evidence lower bound (ELBO), and finally leads to an
unsatisfactory performance in data generation. In this paper, we propose a
latent distribution consistency approach to avoid such substantial
inconsistency between the posterior and prior latent distributions in ELBO
optimizing. We name our method as latent distribution consistency VAE
(LDC-VAE). We achieve this purpose by assuming the real posterior distribution
in latent space as a Gibbs form, and approximating it by using our encoder.
However, there is no analytical solution for such Gibbs posterior in
approximation, and traditional approximation ways are time consuming, such as
using the iterative sampling-based MCMC. To address this problem, we use the
Stein Variational Gradient Descent (SVGD) to approximate the Gibbs posterior.
Meanwhile, we use the SVGD to train a sampler net which can obtain efficient
samples from the Gibbs posterior. Comparative studies on the popular image
generation datasets show that our method has achieved comparable or even better
performance than several powerful improvements of VAEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hard-sample Guided Hybrid Contrast Learning for Unsupervised Person Re-Identification. (arXiv:2109.12333v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12333">
<div class="article-summary-box-inner">
<span><p>Unsupervised person re-identification (Re-ID) is a promising and very
challenging research problem in computer vision. Learning robust and
discriminative features with unlabeled data is of central importance to Re-ID.
Recently, more attention has been paid to unsupervised Re-ID algorithms based
on clustered pseudo-label. However, the previous approaches did not fully
exploit information of hard samples, simply using cluster centroid or all
instances for contrastive learning. In this paper, we propose a Hard-sample
Guided Hybrid Contrast Learning (HHCL) approach combining cluster-level loss
with instance-level loss for unsupervised person Re-ID. Our approach applies
cluster centroid contrastive loss to ensure that the network is updated in a
more stable way. Meanwhile, introduction of a hard instance contrastive loss
further mines the discriminative information. Extensive experiments on two
popular large-scale Re-ID benchmarks demonstrate that our HHCL outperforms
previous state-of-the-art methods and significantly improves the performance of
unsupervised person Re-ID. The code of our work is available soon at
https://github.com/bupt-ai-cz/HHCL-ReID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Hand Pose and Shape Estimation from RGB Images for Keypoint-Based Hand Gesture Recognition. (arXiv:2109.13879v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13879">
<div class="article-summary-box-inner">
<span><p>Estimating the 3D pose of a hand from a 2D image is a well-studied problem
and a requirement for several real-life applications such as virtual reality,
augmented reality, and hand gesture recognition. Currently, reasonable
estimations can be computed from single RGB images, especially when a
multi-task learning approach is used to force the system to consider the shape
of the hand when its pose is determined. However, depending on the method used
to represent the hand, the performance can drop considerably in real-life
tasks, suggesting that stable descriptions are required to achieve satisfactory
results. In this paper, we present a keypoint-based end-to-end framework for 3D
hand and pose estimation and successfully apply it to the task of hand gesture
recognition as a study case. Specifically, after a pre-processing step in which
the images are normalized, the proposed pipeline uses a multi-task semantic
feature extractor generating 2D heatmaps and hand silhouettes from RGB images,
a viewpoint encoder to predict the hand and camera view parameters, a stable
hand estimator to produce the 3D hand pose and shape, and a loss function to
guide all of the components jointly during the learning phase. Tests were
performed on a 3D pose and shape estimation benchmark dataset to assess the
proposed framework, which obtained state-of-the-art performance. Our system was
also evaluated on two hand-gesture recognition benchmark datasets and
significantly outperformed other keypoint-based approaches, indicating that it
is an effective solution that is able to generate stable 3D estimates for hand
pose and shape.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupled Adaptation for Cross-Domain Object Detection. (arXiv:2110.02578v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02578">
<div class="article-summary-box-inner">
<span><p>Cross-domain object detection is more challenging than object classification
since multiple objects exist in an image and the location of each object is
unknown in the unlabeled target domain. As a result, when we adapt features of
different objects to enhance the transferability of the detector, the features
of the foreground and the background are easy to be confused, which may hurt
the discriminability of the detector. Besides, previous methods focused on
category adaptation but ignored another important part for object detection,
i.e., the adaptation on bounding box regression. To this end, we propose
D-adapt, namely Decoupled Adaptation, to decouple the adversarial adaptation
and the training of the detector. Besides, we fill the blank of regression
domain adaptation in object detection by introducing a bounding box adaptor.
Experiments show that D-adapt achieves state-of-the-art results on four
cross-domain object detection tasks and yields 17% and 21% relative improvement
on benchmark datasets Clipart1k and Comic2k in particular.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Fusion Prior for Multi-Focus Image Super Resolution Fusion. (arXiv:2110.05706v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05706">
<div class="article-summary-box-inner">
<span><p>This paper unifies the multi-focus images fusion (MFIF) and blind super
resolution (SR) problems as the multi-focus image super resolution fusion
(MFISRF) task, and proposes a novel unified dataset-free unsupervised framework
named deep fusion prior (DFP) to address such MFISRF task. DFP consists of
SKIPnet network, DoubleReblur focus measurement tactic, decision embedding
module and loss functions. In particular, DFP can obtain MFISRF only from two
low-resolution inputs without any extent dataset; SKIPnet implementing
unsupervised learning via deep image prior is an end-to-end generated network
acting as the engine of DFP; DoubleReblur is used to determine the primary
decision map without learning but based on estimated PSF and Gaussian kernels
convolution; decision embedding module optimizes the decision map via learning;
and DFP losses composed of content loss, joint gradient loss and gradient limit
loss can obtain high-quality MFISRF results robustly. Experiments have proved
that our proposed DFP approaches and even outperforms those state-of-art MFIF
and SR method combinations. Additionally, DFP is a general framework, thus its
networks and focus measurement tactics can be continuously updated to further
improve the MFISRF performance. DFP codes are open source and will be available
soon at <a href="http://github.com/GuYuanjie/DeepFusionPrior.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Reducing Aleatoric Uncertainty for Medical Imaging Tasks. (arXiv:2110.11012v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11012">
<div class="article-summary-box-inner">
<span><p>In safety-critical applications like medical diagnosis, certainty associated
with a model's prediction is just as important as its accuracy. Consequently,
uncertainty estimation and reduction play a crucial role. Uncertainty in
predictions can be attributed to noise or randomness in data (aleatoric) and
incorrect model inferences (epistemic). While model uncertainty can be reduced
with more data or bigger models, aleatoric uncertainty is more intricate. This
work proposes a novel approach that interprets data uncertainty estimated from
a self-supervised task as noise inherent to the data and utilizes it to reduce
aleatoric uncertainty in another task related to the same dataset via data
augmentation. The proposed method was evaluated on a benchmark medical imaging
dataset with image reconstruction as the self-supervised task and segmentation
as the image analysis task. Our findings demonstrate the effectiveness of the
proposed approach in significantly reducing the aleatoric uncertainty in the
image segmentation task while achieving better or on-par performance compared
to the standard augmentation techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Magnification Network for Vessel Segmentation in OCTA Images. (arXiv:2110.13428v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13428">
<div class="article-summary-box-inner">
<span><p>Optical coherence tomography angiography (OCTA) is a novel non-invasive
imaging modality that allows micron-level resolution to visualize the retinal
microvasculature. The retinal vessel segmentation in OCTA images is still an
open problem, and especially the thin and dense structure of the capillary
plexus is an important challenge of this problem. In this work, we propose a
novel image magnification network (IMN) for vessel segmentation in OCTA images.
Contrary to the U-Net structure with a down-sampling encoder and up-sampling
decoder, the proposed IMN adopts the design of up-sampling encoding and then
down-sampling decoding. This design is to capture more low-level image details
to reduce the omission of small structures. The experimental results on three
open OCTA datasets show that the proposed IMN with an average dice score of
90.2% achieves the best performance in vessel segmentation of OCTA images.
Besides, we also demonstrate the superior performance of IMN in cross-field
image vessel segmentation and vessel skeleton extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift Estimation. (arXiv:2111.02682v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02682">
<div class="article-summary-box-inner">
<span><p>The recent developments of deep learning models that capture complex temporal
patterns of crop phenology have greatly advanced crop classification from
Satellite Image Time Series (SITS). However, when applied to target regions
spatially different from the training region, these models perform poorly
without any target labels due to the temporal shift of crop phenology between
regions. Although various unsupervised domain adaptation techniques have been
proposed in recent years, no method explicitly learns the temporal shift of
SITS and thus provides only limited benefits for crop classification. To
address this, we propose TimeMatch, which explicitly accounts for the temporal
shift for improved SITS-based domain adaptation. In TimeMatch, we first
estimate the temporal shift from the target to the source region using the
predictions of a source-trained model. Then, we re-train the model for the
target region by an iterative algorithm where the estimated shift is used to
generate accurate target pseudo-labels. Additionally, we introduce an
open-access dataset for cross-region adaptation from SITS in four different
regions in Europe. On our dataset, we demonstrate that TimeMatch outperforms
all competing methods by 11% in average F1-score across five different
adaptation scenarios, setting a new state-of-the-art in cross-region
adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Detection on Chest X-Ray Images: A comparison of CNN architectures and ensembles. (arXiv:2111.09972v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09972">
<div class="article-summary-box-inner">
<span><p>COVID-19 quickly became a global pandemic after only four months of its first
detection. It is crucial to detect this disease as soon as possible to decrease
its spread. The use of chest X-ray (CXR) images became an effective screening
strategy, complementary to the reverse transcription-polymerase chain reaction
(RT-PCR). Convolutional neural networks (CNNs) are often used for automatic
image classification and they can be very useful in CXR diagnostics. In this
paper, 21 different CNN architectures are tested and compared in the task of
identifying COVID-19 in CXR images. They were applied to the COVIDx8B dataset,
a large COVID-19 dataset with 16,352 CXR images coming from patients of at
least 51 countries. Ensembles of CNNs were also employed and they showed better
efficacy than individual instances. The best individual CNN instance results
were achieved by DenseNet169, with an accuracy of 98.15% and an F1 score of
98.12%. These were further increased to 99.25% and 99.24%, respectively,
through an ensemble with five instances of DenseNet169. These results are
higher than those obtained in recent works using the same dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lebanon Solar Rooftop Potential Assessment using Buildings Segmentation from Aerial Images. (arXiv:2111.11397v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11397">
<div class="article-summary-box-inner">
<span><p>Estimating solar rooftop potential at a national level is a fundamental
building block for every country to utilize solar power efficiently. Solar
rooftop potential assessment relies on several features such as building
geometry, location, and surrounding facilities. Hence, national-level
approximations that do not take these factors into deep consideration are often
inaccurate. This paper introduces Lebanon's first comprehensive footprint and
solar rooftop potential maps using deep learning-based instance segmentation to
extract buildings' footprints from satellite images. A photovoltaic panels
placement algorithm that considers the morphology of each roof is proposed. We
show that the average rooftop's solar potential can fulfill the yearly electric
needs of a single-family residence while using only 5% of the roof surface. The
usage of 50% of a residential apartment rooftop area would achieve energy
security for up to 8 households. We also compute the average and total solar
rooftop potential per district to localize regions corresponding to the highest
and lowest solar rooftop potential yield. Factors such as size, ground coverage
ratio and PV_out are carefully investigated for each district. Baalbeck
district yielded the highest total solar rooftop potential despite its low
built-up area. While, Beirut capital city has the highest average solar rooftop
potential due to its extremely populated urban nature. Reported results and
analysis reveal solar rooftop potential urban patterns and provides
policymakers and key stakeholders with tangible insights. Lebanon's total solar
rooftop potential is about 28.1 TWh/year, two times larger than the national
energy consumption in 2019.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset-free Self-supervised Disentangled Learning Method for Adaptive Infrared and Visible Images Super-resolution Fusion. (arXiv:2112.02869v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02869">
<div class="article-summary-box-inner">
<span><p>This study proposes a novel general dataset-free self-supervised learning
framework based-on physical model named self-supervised disentangled learning
(SDL), and proposes a novel method named Deep Retinex fusion (DRF) which
applies SDL framework with generative networks and Retinex theory in infrared
and visible images super-resolution fusion. Meanwhile, a generative dual-path
fusion network ZipperNet and adaptive fusion loss function Retinex loss are
designed for effectively high-quality fusion. The core idea of DRF (based-on
SDL) consists of two parts: one is generating components which are disentangled
from physical model using generative networks; the other is loss functions
which are designed based-on physical relation, and generated components are
combined by loss functions in training phase. Furthermore, in order to verify
the effectiveness of our proposed DRF, qualitative and quantitative comparisons
compared with six state-of-the-art methods are performed on three different
infrared and visible datasets. Our code will be open source available soon at
https://github.com/GuYuanjie/Deep-Retinex-fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">360-DFPE: Leveraging Monocular 360-Layouts for Direct Floor Plan Estimation. (arXiv:2112.06180v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06180">
<div class="article-summary-box-inner">
<span><p>We present 360-DFPE, a sequential floor plan estimation method that directly
takes 360-images as input without relying on active sensors or 3D information.
Our approach leverages a loosely coupled integration between a monocular visual
SLAM solution and a monocular 360-room layout approach, which estimate camera
poses and layout geometries, respectively. Since our task is to sequentially
capture the floor plan using monocular images, the entire scene structure, room
instances, and room shapes are unknown. To tackle these challenges, we first
handle the scale difference between visual odometry and layout geometry via
formulating an entropy minimization process, which enables us to directly align
360-layouts without knowing the entire scene in advance. Second, to
sequentially identify individual rooms, we propose a novel room identification
algorithm that tracks every room along the camera exploration using geometry
information. Lastly, to estimate the final shape of the room, we propose a
shortest path algorithm with an iterative coarse-to-fine strategy, which
improves prior formulations with higher accuracy and faster run-time. Moreover,
we collect a new floor plan dataset with challenging large-scale scenes,
providing both point clouds and sequential 360-image information. Experimental
results show that our monocular solution achieves favorable performance against
the current state-of-the-art algorithms that rely on active sensors and require
the entire scene reconstruction data in advance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Triangle Attack: A Query-efficient Decision-based Adversarial Attack. (arXiv:2112.06569v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06569">
<div class="article-summary-box-inner">
<span><p>Decision-based attack poses a severe threat to real-world applications since
it regards the target model as a black box and only accesses the hard
prediction label. Great efforts have been made recently to decrease the number
of queries; however, existing decision-based attacks still require thousands of
queries in order to generate good quality adversarial examples. In this work,
we find that a benign sample, the current and the next adversarial examples
could naturally construct a triangle in a subspace for any iterative attacks.
Based on the law of sines, we propose a novel Triangle Attack (TA) to optimize
the perturbation by utilizing the geometric information that the longer side is
always opposite the larger angle in any triangle. However, directly applying
such information on the input image is ineffective because it cannot thoroughly
explore the neighborhood of the input sample in the high dimensional space. To
address this issue, TA optimizes the perturbation in the low frequency space
for effective dimensionality reduction owing to the generality of such
geometric property. Extensive evaluations on the ImageNet dataset demonstrate
that TA achieves a much higher attack success rate within 1,000 queries and
needs a much less number of queries to achieve the same attack success rate
under various perturbation budgets than existing decision-based attacks. With
such high efficiency, we further demonstrate the applicability of TA on
real-world API, i.e., Tencent Cloud API.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving neural implicit surfaces geometry with patch warping. (arXiv:2112.09648v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09648">
<div class="article-summary-box-inner">
<span><p>Neural implicit surfaces have become an important technique for multi-view 3D
reconstruction but their accuracy remains limited. In this paper, we argue that
this comes from the difficulty to learn and render high frequency textures with
neural networks. We thus propose to add to the standard neural rendering
optimization a direct photo-consistency term across the different views.
Intuitively, we optimize the implicit geometry so that it warps views on each
other in a consistent way. We demonstrate that two elements are key to the
success of such an approach: (i) warping entire patches, using the predicted
occupancy and normals of the 3D points along each ray, and measuring their
similarity with a robust structural similarity (SSIM); (ii) handling visibility
and occlusion in such a way that incorrect warps are not given too much
importance while encouraging a reconstruction as complete as possible. We
evaluate our approach, dubbed NeuralWarp, on the standard DTU and EPFL
benchmarks and show it outperforms state of the art unsupervised implicit
surfaces reconstructions by over 20% on both datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query Adaptive Few-Shot Object Detection with Heterogeneous Graph Convolutional Networks. (arXiv:2112.09791v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09791">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection (FSOD) aims to detect never-seen objects using few
examples. This field sees recent improvement owing to the meta-learning
techniques by learning how to match between the query image and few-shot class
examples, such that the learned model can generalize to few-shot novel classes.
However, currently, most of the meta-learning-based methods perform pairwise
matching between query image regions (usually proposals) and novel classes
separately, therefore failing to take into account multiple relationships among
them. In this paper, we propose a novel FSOD model using heterogeneous graph
convolutional networks. Through efficient message passing among all the
proposal and class nodes with three different types of edges, we could obtain
context-aware proposal features and query-adaptive, multiclass-enhanced
prototype representations for each class, which could help promote the pairwise
matching and improve final FSOD accuracy. Extensive experimental results show
that our proposed model, denoted as QA-FewDet, outperforms the current
state-of-the-art approaches on the PASCAL VOC and MSCOCO FSOD benchmarks under
different shots and evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScanQA: 3D Question Answering for Spatial Scene Understanding. (arXiv:2112.10482v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10482">
<div class="article-summary-box-inner">
<span><p>We propose a new 3D spatial understanding task of 3D Question Answering
(3D-QA). In the 3D-QA task, models receive visual information from the entire
3D scene of the rich RGB-D indoor scan and answer the given textual questions
about the 3D scene. Unlike the 2D-question answering of VQA, the conventional
2D-QA models suffer from problems with spatial understanding of object
alignment and directions and fail the object identification from the textual
questions in 3D-QA. We propose a baseline model for 3D-QA, named ScanQA model,
where the model learns a fused descriptor from 3D object proposals and encoded
sentence embeddings. This learned descriptor correlates the language
expressions with the underlying geometric features of the 3D scan and
facilitates the regression of 3D bounding boxes to determine described objects
in textual questions and outputs correct answers. We collected human-edited
question-answer pairs with free-form answers that are grounded to 3D objects in
each 3D scene. Our new ScanQA dataset contains over 40K question-answer pairs
from the 800 indoor scenes drawn from the ScanNet dataset. To the best of our
knowledge, the proposed 3D-QA task is the first large-scale effort to perform
object-grounded question-answering in 3D environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Can Machine Vision Do for Lymphatic Histopathology Image Analysis: A Comprehensive Review. (arXiv:2201.08550v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08550">
<div class="article-summary-box-inner">
<span><p>In the past ten years, the computing power of machine vision (MV) has been
continuously improved, and image analysis algorithms have developed rapidly. At
the same time, histopathological slices can be stored as digital images.
Therefore, MV algorithms can provide doctors with diagnostic references. In
particular, the continuous improvement of deep learning algorithms has further
improved the accuracy of MV in disease detection and diagnosis. This paper
reviews the applications of image processing technology based on MV in lymphoma
histopathological images in recent years, including segmentation,
classification and detection. Finally, the current methods are analyzed, some
more potential methods are proposed, and further prospects are made.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Video Coding: Instill Static-Dynamic Clues into Structured Bitstream for AI Tasks. (arXiv:2201.10162v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10162">
<div class="article-summary-box-inner">
<span><p>Traditional media coding schemes typically encode image/video into a
semantic-unknown binary stream, which fails to directly support downstream
intelligent tasks at the bitstream level. Semantically Structured Image Coding
(SSIC) framework makes the first attempt to enable decoding-free or
partial-decoding image intelligent task analysis via a Semantically Structured
Bitstream (SSB). However, the SSIC only considers image coding and its
generated SSB only contains the static object information. In this paper, we
extend the idea of semantically structured coding from video coding perspective
and propose an advanced Semantically Structured Video Coding (SSVC) framework
to support heterogeneous intelligent applications. Video signals contain more
rich dynamic motion information and exist more redundancy due to the similarity
between adjacent frames. Thus, we present a reformulation of semantically
structured bitstream (SSB) in SSVC which contains both static object
characteristics and dynamic motion clues. Specifically, we introduce optical
flow to encode continuous motion information and reduce cross-frame redundancy
via a predictive coding architecture, then the optical flow and residual
information are reorganized into SSB, which enables the proposed SSVC could
better adaptively support video-based downstream intelligent applications.
Extensive experiments demonstrate that the proposed SSVC framework could
directly support multiple intelligent tasks just depending on a partially
decoded bitstream. This avoids the full bitstream decompression and thus
significantly saves bitrate/bandwidth consumption for intelligent analytics. We
verify this point on the tasks of image object detection, pose estimation,
video action recognition, video object segmentation, etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OctAttention: Octree-Based Large-Scale Contexts Model for Point Cloud Compression. (arXiv:2202.06028v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06028">
<div class="article-summary-box-inner">
<span><p>In point cloud compression, sufficient contexts are significant for modeling
the point cloud distribution. However, the contexts gathered by the previous
voxel-based methods decrease when handling sparse point clouds. To address this
problem, we propose a multiple-contexts deep learning framework called
OctAttention employing the octree structure, a memory-efficient representation
for point clouds. Our approach encodes octree symbol sequences in a lossless
way by gathering the information of sibling and ancestor nodes. Expressly, we
first represent point clouds with octree to reduce spatial redundancy, which is
robust for point clouds with different resolutions. We then design a
conditional entropy model with a large receptive field that models the sibling
and ancestor contexts to exploit the strong dependency among the neighboring
nodes and employ an attention mechanism to emphasize the correlated nodes in
the context. Furthermore, we introduce a mask operation during training and
testing to make a trade-off between encoding time and performance. Compared to
the previous state-of-the-art works, our approach obtains a 10%-35% BD-Rate
gain on the LiDAR benchmark (e.g. SemanticKITTI) and object point cloud dataset
(e.g. MPEG 8i, MVUB), and saves 95% coding time compared to the voxel-based
baseline. The code is available at https://github.com/zb12138/OctAttention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALGAN: Anomaly Detection by Generating Pseudo Anomalous Data via Latent Variables. (arXiv:2202.10281v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10281">
<div class="article-summary-box-inner">
<span><p>In many anomaly detection tasks, where anomalous data rarely appear and are
difficult to collect, training using only normal data is important. Although it
is possible to manually create anomalous data using prior knowledge, they may
be subject to user bias. In this paper, we propose an Anomalous Latent variable
Generative Adversarial Network (ALGAN) in which the GAN generator produces
pseudo-anomalous data as well as fake-normal data, whereas the discriminator is
trained to distinguish between normal and pseudo-anomalous data. This differs
from the standard GAN discriminator, which specializes in classifying two
similar classes. The training dataset contains only normal data; the latent
variables are introduced in anomalous states and are input into the generator
to produce diverse pseudo-anomalous data. We compared the performance of ALGAN
with other existing methods on the MVTec-AD, Magnetic Tile Defects, and
COIL-100 datasets. The experimental results showed that ALGAN exhibited an
AUROC comparable to those of state-of-the-art methods while achieving a much
faster prediction time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LF-VIO: A Visual-Inertial-Odometry Framework for Large Field-of-View Cameras with Negative Plane. (arXiv:2202.12613v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12613">
<div class="article-summary-box-inner">
<span><p>Visual-inertial-odometry has attracted extensive attention in the field of
autonomous driving and robotics. The size of Field of View (FoV) plays an
important role in Visual-Odometry (VO) and Visual-Inertial-Odometry (VIO), as a
large FoV enables to perceive a wide range of surrounding scene elements and
features. However, when the field of the camera reaches the negative half
plane, one cannot simply use [u,v,1]^T to represent the image feature points
anymore. To tackle this issue, we propose LF-VIO, a real-time VIO framework for
cameras with extremely large FoV. We leverage a three-dimensional vector with
unit length to represent feature points, and design a series of algorithms to
overcome this challenge. To address the scarcity of panoramic visual odometry
datasets with ground-truth location and pose, we present the PALVIO dataset,
collected with a Panoramic Annular Lens (PAL) system with an entire FoV of
360x(40-120) degrees and an IMU sensor. With a comprehensive variety of
experiments, the proposed LF-VIO is verified on both the established PALVIO
benchmark and a public fisheye camera dataset with a FoV of 360x(0-93.5)
degrees. LF-VIO outperforms state-of-the-art visual-inertial-odometry methods.
Our dataset and code are made publicly available at
https://github.com/flysoaryun/LF-VIO
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Query-based Paradigm for Point Cloud Understanding. (arXiv:2203.01252v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01252">
<div class="article-summary-box-inner">
<span><p>3D point cloud understanding is an important component in autonomous driving
and robotics. In this paper, we present a novel Embedding-Querying paradigm
(EQ- Paradigm) for 3D understanding tasks including detection, segmentation,
and classification. EQ-Paradigm is a unified paradigm that enables the
combination of any existing 3D backbone architectures with different task
heads. Under the EQ-Paradigm, the input is firstly encoded in the embedding
stage with an arbitrary feature extraction architecture, which is independent
of tasks and heads. Then, the querying stage enables the encoded features to be
applicable for diverse task heads. This is achieved by introducing an
intermediate representation, i.e., Q-representation, in the querying stage to
serve as a bridge between the embedding stage and task heads. We design a novel
Q- Net as the querying stage network. Extensive experimental results on various
3D tasks, including object detection, semantic segmentation and shape
classification, show that EQ-Paradigm in tandem with Q-Net is a general and
effective pipeline, which enables a flexible collaboration of backbones and
heads, and further boosts the performance of the state-of-the-art methods.
Codes and models are available at
https://github.com/dvlab-research/DeepVision3D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep-ASPECTS: A Segmentation-Assisted Model for Stroke Severity Measurement. (arXiv:2203.03622v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03622">
<div class="article-summary-box-inner">
<span><p>A stroke occurs when an artery in the brain ruptures and bleeds or when the
blood supply to the brain is cut off. Blood and oxygen cannot reach the brain's
tissues due to the rupture or obstruction resulting in tissue death. The Middle
cerebral artery (MCA) is the largest cerebral artery and the most commonly
damaged vessel in stroke. The quick onset of a focused neurological deficit
caused by interruption of blood flow in the territory supplied by the MCA is
known as an MCA stroke. Alberta stroke programme early CT score (ASPECTS) is
used to estimate the extent of early ischemic changes in patients with MCA
stroke. This study proposes a deep learning-based method to score the CT scan
for ASPECTS. Our work has three highlights. First, we propose a novel method
for medical image segmentation for stroke detection. Second, we show the
effectiveness of AI solution for fully-automated ASPECT scoring with reduced
diagnosis time for a given non-contrast CT (NCCT) Scan. Our algorithms show a
dice similarity coefficient of 0.64 for the MCA anatomy segmentation and 0.72
for the infarcts segmentation. Lastly, we show that our model's performance is
inline with inter-reader variability between radiologists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAMI-AD: An Activity Detector Exploiting Part-attention and Motion Information in Surveillance Videos. (arXiv:2203.03796v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03796">
<div class="article-summary-box-inner">
<span><p>Activity detection in surveillance videos is a challenging task caused by
small objects, complex activity categories, its untrimmed nature, etc. Existing
methods are generally limited in performance due to inaccurate proposals, poor
classifiers or inadequate post-processing method. In this work, we propose a
comprehensive and effective activity detection system in untrimmed surveillance
videos for person-centered and vehicle-centered activities. It consists of four
modules, i.e., object localizer, proposal filter, activity classifier and
activity refiner. For person-centered activities, a novel part-attention
mechanism is proposed to explore detailed features in different body parts. As
for vehicle-centered activities, we propose a localization masking method to
jointly encode motion and foreground attention features. We conduct experiments
on the large-scale activity detection datasets VIRAT, and achieve the best
results for both groups of activities. Furthermore, our team won the 1st place
in the TRECVID 2021 ActEV challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance. (arXiv:2203.08368v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08368">
<div class="article-summary-box-inner">
<span><p>The exponentially large discrete search space in mixed-precision quantization
(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous
works usually resort to iterative search methods on the training set, which
consume hundreds or even thousands of GPU-hours. In this study, we reveal that
some unique learnable parameters in quantization, namely the scale factors in
the quantizer, can serve as importance indicators of a layer, reflecting the
contribution of that layer to the final accuracy at certain bit-widths. These
importance indicators naturally perceive the numerical transformation during
quantization-aware training, which can precisely and correctly provide
quantization sensitivity metrics of layers. However, a deep network always
contains hundreds of such indicators, and training them one by one would lead
to an excessive time cost. To overcome this issue, we propose a joint training
scheme that can obtain all indicators at once. It considerably speeds up the
indicators training process by parallelizing the original sequential training
processes. With these learned importance indicators, we formulate the MPQ
search problem as a one-time integer linear programming (ILP) problem. That
avoids the iterative search and significantly reduces search time without
limiting the bit-width search space. For example, MPQ search on ResNet18 with
our indicators takes only 0.06 seconds. Also, extensive experiments show our
approach can achieve SOTA accuracy on ImageNet for far-ranging models with
various constraints (e.g., BitOps, compress rate).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Flow: Cross-layer Graph Flow Distillation for Dual Efficient Medical Image Segmentation. (arXiv:2203.08667v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08667">
<div class="article-summary-box-inner">
<span><p>With the development of deep convolutional neural networks, medical image
segmentation has achieved a series of breakthroughs in recent years. However,
the higher-performance convolutional neural networks always mean numerous
parameters and high computation costs, which will hinder the applications in
clinical scenarios. Meanwhile, the scarceness of large-scale annotated medical
image datasets further impedes the application of high-performance networks. To
tackle these problems, we propose Graph Flow, a comprehensive knowledge
distillation framework, for both network-efficiency and annotation-efficiency
medical image segmentation. Specifically, our core Graph Flow Distillation
transfer the essence of cross-layer variations from a well-trained cumbersome
teacher network to a non-trained compact student network. In addition, an
unsupervised Paraphraser Module is designed to purify the knowledge of the
teacher network, which is also beneficial for the stabilization of training
procedure. Furthermore, we build a unified distillation framework by
integrating the adversarial distillation and the vanilla logits distillation,
which can further refine the final predictions of the compact network.
Extensive experiments conducted on Gastric Cancer Segmentation Dataset and
Synapse Multi-organ Segmentation Dataset demonstrate the prominent ability of
our method which achieves state-of-the-art performance on these
different-modality and multi-category medical image datasets. Moreover, we
demonstrate the effectiveness of our Graph Flow through a new semi-supervised
paradigm for dual efficient medical image segmentation. Our code will be
available at Graph Flow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transframer: Arbitrary Frame Prediction with Generative Models. (arXiv:2203.09494v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09494">
<div class="article-summary-box-inner">
<span><p>We present a general-purpose framework for image modelling and vision tasks
based on probabilistic frame prediction. Our approach unifies a broad range of
tasks, from image segmentation, to novel view synthesis and video
interpolation. We pair this framework with an architecture we term Transframer,
which uses U-Net and Transformer components to condition on annotated context
frames, and outputs sequences of sparse, compressed image features. Transframer
is the state-of-the-art on a variety of video generation benchmarks, is
competitive with the strongest models on few-shot view synthesis, and can
generate coherent 30 second videos from a single image without any explicit
geometric information. A single generalist Transframer simultaneously produces
promising results on 8 tasks, including semantic segmentation, image
classification and optical flow prediction with no task-specific architectural
components, demonstrating that multi-task computer vision can be tackled using
probabilistic image models. Our approach can in principle be applied to a wide
range of applications that require learning the conditional structure of
annotated image-formatted data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-space and Image Domain Collaborative Energy based Model for Parallel MRI Reconstruction. (arXiv:2203.10776v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10776">
<div class="article-summary-box-inner">
<span><p>Decreasing magnetic resonance (MR) image acquisition times can potentially
make MR examinations more accessible. Prior arts including the deep learning
models have been devoted to solving the problem of long MRI imaging time.
Recently, deep generative models have exhibited great potentials in algorithm
robustness and usage flexibility. Nevertheless, no existing such schemes that
can be learned or employed directly to the k-space measurement. Furthermore,
how do the deep generative models work well in hybrid domain is also worth to
be investigated. In this work, by taking advantage of the deep en-ergy-based
models, we propose a k-space and image domain collaborative generative model to
comprehensively estimate the MR data from under-sampled measurement.
Experimental comparisons with the state-of-the-arts demonstrated that the
proposed hybrid method has less error in reconstruction and is more stable
under different acceleration factors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RD-Optimized Trit-Plane Coding of Deep Compressed Image Latent Tensors. (arXiv:2203.13467v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13467">
<div class="article-summary-box-inner">
<span><p>DPICT is the first learning-based image codec supporting fine granular
scalability. In this paper, we describe how to implement two key components of
DPICT efficiently: trit-plane slicing and rate-distortion-optimized
(RD-optimized) coding. In DPICT, we transform an image into a latent tensor,
represent the tensor in ternary digits (trits), and encode the trits in the
decreasing order of significance. For entropy encoding, it is necessary to
compute the probability of each trit, which demands high time complexity in
both the encoder and the decoder. To reduce the complexity, we develop a
parallel computing scheme for the probabilities, which is described in detail
with pseudo-codes. Moreover, we compare the trit-plane slicing in DPICT with
the alternative bit-plane slicing. Experimental results show that the time
complexity is reduced significantly by the parallel computing and that the
trit-plane slicing provides better RD performances than the bit-plane slicing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expanding Low-Density Latent Regions for Open-Set Object Detection. (arXiv:2203.14911v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14911">
<div class="article-summary-box-inner">
<span><p>Modern object detectors have achieved impressive progress under the close-set
setup. However, open-set object detection (OSOD) remains challenging since
objects of unknown categories are often misclassified to existing known
classes. In this work, we propose to identify unknown objects by separating
high/low-density regions in the latent space, based on the consensus that
unknown objects are usually distributed in low-density latent regions. As
traditional threshold-based methods only maintain limited low-density regions,
which cannot cover all unknown objects, we present a novel Open-set Detector
(OpenDet) with expanded low-density regions. To this aim, we equip OpenDet with
two learners, Contrastive Feature Learner (CFL) and Unknown Probability Learner
(UPL). CFL performs instance-level contrastive learning to encourage compact
features of known classes, leaving more low-density regions for unknown
classes; UPL optimizes unknown probability based on the uncertainty of
predictions, which further divides more low-density regions around the cluster
of known classes. Thus, unknown objects in low-density regions can be easily
identified with the learned unknown probability. Extensive experiments
demonstrate that our method can significantly improve the OSOD performance,
e.g., OpenDet reduces the Absolute Open-Set Errors by 25%-35% on six OSOD
benchmarks. Code is available at: https://github.com/csuhan/opendet2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SepViT: Separable Vision Transformer. (arXiv:2203.15380v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15380">
<div class="article-summary-box-inner">
<span><p>Vision Transformers have witnessed prevailing success in a series of vision
tasks. However, they often require enormous amount of computations to achieve
high performance, which is burdensome to deploy on resource-constrained
devices. To address these issues, we draw lessons from depthwise separable
convolution and imitate its ideology to design the Separable Vision
Transformer, abbreviated as SepViT. SepViT helps to carry out the information
interaction within and among the windows via a depthwise separable
self-attention. The novel window token embedding and grouped self-attention are
employed to model the attention relationship among windows with negligible
computational cost and capture a long-range visual dependencies of multiple
windows, respectively. Extensive experiments on various benchmark tasks
demonstrate SepViT can achieve state-of-the-art results in terms of trade-off
between accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy
on ImageNet-1K classification while decreasing the latency by 40%, compared to
the ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream
vision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic
segmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box
AP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TopTemp: Parsing Precipitate Structure from Temper Topology. (arXiv:2204.00629v2 [cond-mat.mtrl-sci] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00629">
<div class="article-summary-box-inner">
<span><p>Technological advances are in part enabled by the development of novel
manufacturing processes that give rise to new materials or material property
improvements. Development and evaluation of new manufacturing methodologies is
labor-, time-, and resource-intensive expensive due to complex, poorly defined
relationships between advanced manufacturing process parameters and the
resulting microstructures. In this work, we present a topological
representation of temper (heat-treatment) dependent material micro-structure,
as captured by scanning electron microscopy, called TopTemp. We show that this
topological representation is able to support temper classification of
microstructures in a data limited setting, generalizes well to previously
unseen samples, is robust to image perturbations, and captures domain
interpretable features. The presented work outperforms conventional deep
learning baselines and is a first step towards improving understanding of
process parameters and resulting material properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Different Losses for Deep Learning Image Colorization. (arXiv:2204.02980v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02980">
<div class="article-summary-box-inner">
<span><p>Image colorization aims to add color information to a grayscale image in a
realistic way. Recent methods mostly rely on deep learning strategies. While
learning to automatically colorize an image, one can define well-suited
objective functions related to the desired color output. Some of them are based
on a specific type of error between the predicted image and ground truth one,
while other losses rely on the comparison of perceptual properties. But, is the
choice of the objective function that crucial, i.e., does it play an important
role in the results? In this chapter, we aim to answer this question by
analyzing the impact of the loss function on the estimated colorization
results. To that goal, we review the different losses and evaluation metrics
that are used in the literature. We then train a baseline network with several
of the reviewed objective functions: classic L1 and L2 losses, as well as more
complex combinations such as Wasserstein GAN and VGG-based LPIPS loss.
Quantitative results show that the models trained with VGG-based LPIPS provide
overall slightly better results for most evaluation metrics. Qualitative
results exhibit more vivid colors when with Wasserstein GAN plus the L2 loss or
again with the VGG-based LPIPS. Finally, the convenience of quantitative user
studies is also discussed to overcome the difficulty of properly assessing on
colorized images, notably for the case of old archive photographs where no
ground truth is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Category-Aware Transformer Network for Better Human-Object Interaction Detection. (arXiv:2204.04911v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04911">
<div class="article-summary-box-inner">
<span><p>Human-Object Interactions (HOI) detection, which aims to localize a human and
a relevant object while recognizing their interaction, is crucial for
understanding a still image. Recently, transformer-based models have
significantly advanced the progress of HOI detection. However, the capability
of these models has not been fully explored since the Object Query of the model
is always simply initialized as just zeros, which would affect the performance.
In this paper, we try to study the issue of promoting transformer-based HOI
detectors by initializing the Object Query with category-aware semantic
information. To this end, we innovatively propose the Category-Aware
Transformer Network (CATN). Specifically, the Object Query would be initialized
via category priors represented by an external object detection model to yield
better performance. Moreover, such category priors can be further used for
enhancing the representation ability of features via the attention mechanism.
We have firstly verified our idea via the Oracle experiment by initializing the
Object Query with the groundtruth category information. And then extensive
experiments have been conducted to show that a HOI detection model equipped
with our idea outperforms the baseline by a large margin to achieve a new
state-of-the-art result.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Atmospheric Turbulence Removal with Complex-Valued Convolutional Neural Network. (arXiv:2204.06989v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06989">
<div class="article-summary-box-inner">
<span><p>Atmospheric turbulence distorts visual imagery and is always problematic for
information interpretation by both human and machine. Most well-developed
approaches to remove atmospheric turbulence distortion are model-based.
However, these methods require high computation and large memory making
real-time operation infeasible. Deep learning-based approaches have hence
gained more attention but currently work efficiently only on static scenes.
This paper presents a novel learning-based framework offering short temporal
spanning to support dynamic scenes. We exploit complex-valued convolutions as
phase information, altered by atmospheric turbulence, is captured better than
using ordinary real-valued convolutions. Two concatenated modules are proposed.
The first module aims to remove geometric distortions and, if enough memory,
the second module is applied to refine micro details of the videos.
Experimental results show that our proposed framework efficiently mitigates the
atmospheric turbulence distortion and significantly outperforms existing
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OMG: Observe Multiple Granularities for Natural Language-Based Vehicle Retrieval. (arXiv:2204.08209v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08209">
<div class="article-summary-box-inner">
<span><p>Retrieving tracked-vehicles by natural language descriptions plays a critical
role in smart city construction. It aims to find the best match for the given
texts from a set of tracked vehicles in surveillance videos. Existing works
generally solve it by a dual-stream framework, which consists of a text
encoder, a visual encoder and a cross-modal loss function. Although some
progress has been made, they failed to fully exploit the information at various
levels of granularity. To tackle this issue, we propose a novel framework for
the natural language-based vehicle retrieval task, OMG, which Observes Multiple
Granularities with respect to visual representation, textual representation and
objective functions. For the visual representation, target features, context
features and motion features are encoded separately. For the textual
representation, one global embedding, three local embeddings and a color-type
prompt embedding are extracted to represent various granularities of semantic
features. Finally, the overall framework is optimized by a cross-modal
multi-granularity contrastive loss function. Experiments demonstrate the
effectiveness of our method. Our OMG significantly outperforms all previous
methods and ranks the 9th on the 6th AI City Challenge Track2. The codes are
available at https://github.com/dyhBUPT/OMG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infographics Wizard: Flexible Infographics Authoring and Design Exploration. (arXiv:2204.09904v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09904">
<div class="article-summary-box-inner">
<span><p>Infographics are an aesthetic visual representation of information following
specific design principles of human perception. Designing infographics can be a
tedious process for non-experts and time-consuming, even for professional
designers. With the help of designers, we propose a semi-automated infographic
framework for general structured and flow-based infographic design generation.
For novice designers, our framework automatically creates and ranks infographic
designs for a user-provided text with no requirement for design input. However,
expert designers can still provide custom design inputs to customize the
infographics. We will also contribute an individual visual group (VG) designs
dataset (in SVG), along with a 1k complete infographic image dataset with
segmented VGs in this work. Evaluation results confirm that by using our
framework, designers from all expertise levels can generate generic infographic
designs faster than existing methods while maintaining the same quality as
hand-designed infographics templates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alleviating Representational Shift for Continual Fine-tuning. (arXiv:2204.10535v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10535">
<div class="article-summary-box-inner">
<span><p>We study a practical setting of continual learning: fine-tuning on a
pre-trained model continually. Previous work has found that, when training on
new tasks, the features (penultimate layer representations) of previous data
will change, called representational shift. Besides the shift of features, we
reveal that the intermediate layers' representational shift (IRS) also matters
since it disrupts batch normalization, which is another crucial cause of
catastrophic forgetting. Motivated by this, we propose ConFiT, a fine-tuning
method incorporating two components, cross-convolution batch normalization
(Xconv BN) and hierarchical fine-tuning. Xconv BN maintains pre-convolution
running means instead of post-convolution, and recovers post-convolution ones
before testing, which corrects the inaccurate estimates of means under IRS.
Hierarchical fine-tuning leverages a multi-stage strategy to fine-tune the
pre-trained network, preventing massive changes in Conv layers and thus
alleviating IRS. Experimental results on four datasets show that our method
remarkably outperforms several state-of-the-art methods with lower storage
overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Reasoning Meets Visual Representation Learning: A Prospective Study. (arXiv:2204.12037v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12037">
<div class="article-summary-box-inner">
<span><p>Visual representation learning is ubiquitous in various real-world
applications, including visual comprehension, video understanding, multi-modal
analysis, human-computer interaction, and urban computing. Due to the emergence
of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal
data in big data era, the lack of interpretability, robustness, and
out-of-distribution generalization are becoming the challenges of the existing
visual models. The majority of the existing methods tend to fit the original
data/variable distributions and ignore the essential causal relations behind
the multi-modal knowledge, which lacks an unified guidance and analysis about
why modern visual representation learning methods are easily collapse into data
bias and have limited generalization and cognitive abilities. Inspired by the
strong inference ability of human-level agents, recent years have therefore
witnessed great effort in developing causal reasoning paradigms to realize
robust representation and model learning with good cognitive ability. In this
paper, we conduct a comprehensive review of existing causal reasoning methods
for visual representation learning, covering fundamental theories, models, and
datasets. The limitations of current methods and datasets are also discussed.
Moreover, we propose some prospective challenges, opportunities, and future
research directions for benchmarking causal reasoning algorithms in visual
representation learning. This paper aims to provide a comprehensive overview of
this emerging field, attract attention, encourage discussions, bring to the
forefront the urgency of developing novel causal reasoning methods, publicly
available benchmarks, and consensus-building standards for reliable visual
representation learning and related real-world applications more efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Transferability for Domain Adaptive Detection Transformers. (arXiv:2204.14195v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14195">
<div class="article-summary-box-inner">
<span><p>DETR-style detectors stand out amongst in-domain scenarios, but their
properties in domain shift settings are under-explored. This paper aims to
build a simple but effective baseline with a DETR-style detector on domain
shift settings based on two findings. For one, mitigating the domain shift on
the backbone and the decoder output features excels in getting favorable
results. For another, advanced domain alignment methods in both parts further
enhance the performance. Thus, we propose the Object-Aware Alignment (OAA)
module and the Optimal Transport based Alignment (OTA) module to achieve
comprehensive domain alignment on the outputs of the backbone and the detector.
The OAA module aligns the foreground regions identified by pseudo-labels in the
backbone outputs, leading to domain-invariant based features. The OTA module
utilizes sliced Wasserstein distance to maximize the retention of location
information while minimizing the domain gap in the decoder outputs. We
implement the findings and the alignment modules into our adaptation method,
and it benchmarks the DETR-style detector on the domain shift settings.
Experiments on various domain adaptive scenarios validate the effectiveness of
our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look Closer to Supervise Better: One-Shot Font Generation via Component-Based Discriminator. (arXiv:2205.00146v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00146">
<div class="article-summary-box-inner">
<span><p>Automatic font generation remains a challenging research issue due to the
large amounts of characters with complicated structures. Typically, only a few
samples can serve as the style/content reference (termed few-shot learning),
which further increases the difficulty to preserve local style patterns or
detailed glyph structures. We investigate the drawbacks of previous studies and
find that a coarse-grained discriminator is insufficient for supervising a font
generator. To this end, we propose a novel Component-Aware Module (CAM), which
supervises the generator to decouple content and style at a more fine-grained
level, i.e., the component level. Different from previous studies struggling to
increase the complexity of generators, we aim to perform more effective
supervision for a relatively simple generator to achieve its full potential,
which is a brand new perspective for font generation. The whole framework
achieves remarkable results by coupling component-level supervision with
adversarial learning, hence we call it Component-Guided GAN, shortly CG-GAN.
Extensive experiments show that our approach outperforms state-of-the-art
one-shot font generation methods. Furthermore, it can be applied to handwritten
word synthesis and scene text image editing, suggesting the generalization of
our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution Neural Network. (arXiv:2205.01550v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01550">
<div class="article-summary-box-inner">
<span><p>Point clouds have the characteristics of disorder, unstructured and
sparseness.Aiming at the problem of the non-structural nature of point clouds,
thanks to the excellent performance of convolutional neural networks in image
processing, one of the solutions is to extract features from point clouds based
on two-dimensional convolutional neural networks. The three-dimensional
information carried in the point cloud can be converted to two-dimensional, and
then processed by a two-dimensional convolutional neural network, and finally
back-projected to three-dimensional.In the process of projecting 3D information
to 2D and back-projection, certain information loss will inevitably be caused
to the point cloud and category inconsistency will be introduced in the
back-projection stage;Another solution is the voxel-based point cloud
segmentation method, which divides the point cloud into small grids one by
one.However, the point cloud is sparse, and the direct use of 3D convolutional
neural network inevitably wastes computing resources. In this paper, we propose
a feature extraction module based on multi-scale ultra-sparse convolution and a
feature selection module based on channel attention, and build a point cloud
segmentation network framework based on this.By introducing multi-scale sparse
convolution, network could capture richer feature information based on
convolution kernels of different sizes, improving the segmentation result of
point cloud segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Clustering Based Pseudo-labeling Strategy for Multi-modal Aerial View Object Classification. (arXiv:2205.01920v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01920">
<div class="article-summary-box-inner">
<span><p>Multi-modal aerial view object classification (MAVOC) in Automatic target
recognition (ATR), although an important and challenging problem, has been
under studied. This paper firstly finds that fine-grained data, class imbalance
and various shooting conditions preclude the representational ability of
general image classification. Moreover, the MAVOC dataset has scene aggregation
characteristics. By exploiting these properties, we propose Scene Clustering
Based Pseudo-labeling Strategy (SCP-Label), a simple yet effective method to
employ in post-processing. The SCP-Label brings greater accuracy by assigning
the same label to objects within the same scene while also mitigating bias and
confusion with model ensembles. Its performance surpasses the official baseline
by a large margin of +20.57% Accuracy on Track 1 (SAR), and +31.86% Accuracy on
Track 2 (SAR+EO), demonstrating the potential of SCP-Label as post-processing.
Finally, we win the championship both on Track1 and Track2 in the CVPR 2022
Perception Beyond the Visible Spectrum (PBVS) Workshop MAVOC Challenge. Our
code is available at https://github.com/HowieChangchn/SCP-Label.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANUBIS: Skeleton Action Recognition Dataset, Review, and Benchmark. (arXiv:2205.02071v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02071">
<div class="article-summary-box-inner">
<span><p>Skeleton-based action recognition, as a subarea of action recognition, is
swiftly accumulating attention and popularity. The task is to recognize actions
performed by human articulation points. Compared with other data modalities, 3D
human skeleton representations have extensive unique desirable characteristics,
including succinctness, robustness, racial-impartiality, and many more. We aim
to provide a roadmap for new and existing researchers a on the landscapes of
skeleton-based action recognition for new and existing researchers. To this
end, we present a review in the form of a taxonomy on existing works of
skeleton-based action recognition. We partition them into four major
categories: (1) datasets; (2) extracting spatial features; (3) capturing
temporal patterns; (4) improving signal quality. For each method, we provide
concise yet informatively-sufficient descriptions. To promote more fair and
comprehensive evaluation on existing approaches of skeleton-based action
recognition, we collect ANUBIS, a large-scale human skeleton dataset. Compared
with previously collected dataset, ANUBIS are advantageous in the following
four aspects: (1) employing more recently released sensors; (2) containing
novel back view; (3) encouraging high enthusiasm of subjects; (4) including
actions of the COVID pandemic era. Using ANUBIS, we comparably benchmark
performance of current skeleton-based action recognizers. At the end of this
paper, we outlook future development of skeleton-based action recognition by
listing several new technical problems. We believe they are valuable to solve
in order to commercialize skeleton-based action recognition in the near future.
The dataset of ANUBIS is available at:
<a href="http://hcc-workshop.anu.edu.au/webs/anu101/home.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnrealNAS: Can We Search Neural Architectures with Unreal Data?. (arXiv:2205.02162v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02162">
<div class="article-summary-box-inner">
<span><p>Neural architecture search (NAS) has shown great success in the automatic
design of deep neural networks (DNNs). However, the best way to use data to
search network architectures is still unclear and under exploration. Previous
work has analyzed the necessity of having ground-truth labels in NAS and
inspired broad interest. In this work, we take a further step to question
whether real data is necessary for NAS to be effective. The answer to this
question is important for applications with limited amount of accessible data,
and can help people improve NAS by leveraging the extra flexibility of data
generation. To explore if NAS needs real data, we construct three types of
unreal datasets using: 1) randomly labeled real images; 2) generated images and
labels; and 3) generated Gaussian noise with random labels. These datasets
facilitate to analyze the generalization and expressivity of the searched
architectures. We study the performance of architectures searched on these
constructed datasets using popular differentiable NAS methods. Extensive
experiments on CIFAR, ImageNet and CheXpert show that the searched
architectures can achieve promising results compared with those derived from
the conventional NAS pipeline with real labeled data, suggesting the
feasibility of performing NAS with unreal data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth Image. (arXiv:1909.01106v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.01106">
<div class="article-summary-box-inner">
<span><p>We propose a novel model for 3D semantic completion from a single depth
image, based on a single encoder and three separate generators used to
reconstruct different geometric and semantic representations of the original
and completed scene, all sharing the same latent space. To transfer information
between the geometric and semantic branches of the network, we introduce paths
between them concatenating features at corresponding network layers. Motivated
by the limited amount of training samples from real scenes, an interesting
attribute of our architecture is the capacity to supplement the existing
dataset by generating a new training dataset with high quality, realistic
scenes that even includes occlusion and real noise. We build the new dataset by
sampling the features directly from latent space which generates a pair of
partial volumetric surface and completed volumetric semantic surface. Moreover,
we utilize multiple discriminators to increase the accuracy and realism of the
reconstructions. We demonstrate the benefits of our approach on standard
benchmarks for the two most common completion tasks: semantic 3D scene
completion and 3D object completion.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-10 23:09:04.137751045 UTC">2022-05-10 23:09:04 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>