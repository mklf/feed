<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-14T01:30:00Z">04-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05999">
<div class="article-summary-box-inner">
<span><p>Code is seldom written in a single left-to-right pass and is instead
repeatedly edited and refined. We introduce InCoder, a unified generative model
that can perform program synthesis (via left-to-right generation) as well as
editing (via infilling). InCoder is trained to generate code files from a large
corpus of permissively licensed code, where regions of code have been randomly
masked and moved to the end of each file, allowing code infilling with
bidirectional context. Our model is the first generative model that is able to
directly perform zero-shot code infilling, which we evaluate on challenging
tasks such as type inference, comment generation, and variable re-naming. We
find that the ability to condition on bidirectional context substantially
improves performance on these tasks, while still performing comparably on
standard program synthesis benchmarks in comparison to left-to-right only
models pretrained at similar scale. The InCoder models and code are publicly
released. https://sites.google.com/view/incoder-code-models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CUNI-KIT System for Simultaneous Speech Translation Task at IWSLT 2022. (arXiv:2204.06028v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06028">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe our submission to the Simultaneous Speech
Translation at IWSLT 2022. We explore strategies to utilize an offline model in
a simultaneous setting without the need to modify the original model. In our
experiments, we show that our onlinization algorithm is almost on par with the
offline setting while being 3x faster than offline in terms of latency on the
test set. We make our system publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L3Cube-MahaNER: A Marathi Named Entity Recognition Dataset and BERT models. (arXiv:2204.06029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06029">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) is a basic NLP task and finds major
applications in conversational and search systems. It helps us identify key
entities in a sentence used for the downstream application. NER or similar slot
filling systems for popular languages have been heavily used in commercial
applications. In this work, we focus on Marathi, an Indian language, spoken
prominently by the people of Maharashtra state. Marathi is a low resource
language and still lacks useful NER resources. We present L3Cube-MahaNER, the
first major gold standard named entity recognition dataset in Marathi. We also
describe the manual annotation guidelines followed during the process. In the
end, we benchmark the dataset on different CNN, LSTM, and Transformer based
models like mBERT, XLM-RoBERTa, IndicBERT, MahaBERT, etc. The MahaBERT provides
the best performance among all the models. The data and models are available at
https://github.com/l3cube-pune/MarathiNLP .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review on Language Models as Knowledge Bases. (arXiv:2204.06031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06031">
<div class="article-summary-box-inner">
<span><p>Recently, there has been a surge of interest in the NLP community on the use
of pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have
shown that LMs trained on a sufficiently large (web) corpus will encode a
significant amount of knowledge implicitly in its parameters. The resulting LM
can be probed for different kinds of knowledge and thus acting as a KB. This
has a major advantage over traditional KBs in that this method requires no
human supervision. In this paper, we present a set of aspects that we deem a LM
should have to fully act as a KB, and review the recent literature with respect
to those aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Trolls Under Bridges: Preliminary Work on a Motif Detector. (arXiv:2204.06085v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06085">
<div class="article-summary-box-inner">
<span><p>Motifs are distinctive recurring elements found in folklore that have
significance as communicative devices in news, literature, press releases, and
propaganda. Motifs concisely imply a large constellation of culturally-relevant
information, and their broad usage suggests their cognitive importance as
touchstones of cultural knowledge, making their detection a worthy step toward
culturally-aware natural language processing tasks. Until now, folklorists and
others interested in motifs have only extracted motifs from narratives
manually. We present a preliminary report on the development of a system for
automatically detecting motifs. We briefly describe an annotation effort to
produce data for training motif detection, which is on-going. We describe our
in-progress architecture in detail, which aims to capture, in part, how people
determine whether or not a motif candidate is being used in a motific way. This
description includes a test of an off-the-shelf metaphor detector as a feature
for motif detection, which achieves a F1 of 0.35 on motifs and a macro-average
F1 of 0.21 across four categories which we assign to motif candidates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASQA: Factoid Questions Meet Long-Form Answers. (arXiv:2204.06092v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06092">
<div class="article-summary-box-inner">
<span><p>An abundance of datasets and availability of reliable evaluation metrics have
resulted in strong progress in factoid question answering (QA). This progress,
however, does not easily transfer to the task of long-form QA, where the goal
is to answer questions that require in-depth explanations. The hurdles include
(i) a lack of high-quality data, and (ii) the absence of a well-defined notion
of the answer's quality. In this work, we address these problems by (i)
releasing a novel dataset and a task that we call ASQA (Answer Summaries for
Questions which are Ambiguous); and (ii) proposing a reliable metric for
measuring performance on ASQA. Our task focuses on factoid questions that are
ambiguous, that is, have different correct answers depending on interpretation.
Answers to ambiguous questions should synthesize factual information from
multiple sources into a long-form summary that resolves the ambiguity. In
contrast to existing long-form QA tasks (such as ELI5), ASQA admits a clear
notion of correctness: a user faced with a good summary should be able to
answer different interpretations of the original ambiguous question. We use
this notion of correctness to define an automated metric of performance for
ASQA. Our analysis demonstrates an agreement between this metric and human
judgments, and reveals a considerable gap between human performance and strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impossible Triangle: What's Next for Pre-trained Language Models?. (arXiv:2204.06130v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06130">
<div class="article-summary-box-inner">
<span><p>Recent development of large-scale pre-trained language models (PLM) have
significantly improved the capability of models in various NLP tasks, in terms
of performance after task-specific fine-tuning and zero-shot / few-shot
learning. However, many of such models come with a dauntingly huge size that
few institutions can afford to pre-train, fine-tune or even deploy, while
moderate-sized models usually lack strong generalized few-shot learning
capabilities. In this paper, we first elaborate the current obstacles of using
PLM models in terms of the Impossible Triangle: 1) moderate model size, 2)
state-of-the-art few-shot learning capability, and 3) state-of-the-art
fine-tuning capability. We argue that all existing PLM models lack one or more
properties from the Impossible Triangle. To remedy these missing properties of
PLMs, various techniques have been proposed, such as knowledge distillation,
data augmentation and prompt learning, which inevitably brings additional work
to the application of PLMs in real scenarios. We then offer insights into
future research directions of PLMs to achieve the Impossible Triangle, and
break down the task into several key phases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIT at SemEval-2022 Task 2: Pre-trained Language Model for Idioms Detection. (arXiv:2204.06145v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06145">
<div class="article-summary-box-inner">
<span><p>The same multi-word expressions may have different meanings in different
sentences. They can be mainly divided into two categories, which are literal
meaning and idiomatic meaning. Non-contextual-based methods perform poorly on
this problem, and we need contextual embedding to understand the idiomatic
meaning of multi-word expressions correctly. We use a pre-trained language
model, which can provide a context-aware sentence embedding, to detect whether
multi-word expression in the sentence is idiomatic usage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Cluster-Based k-Nearest-Neighbor Machine Translation. (arXiv:2204.06175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06175">
<div class="article-summary-box-inner">
<span><p>k-Nearest-Neighbor Machine Translation (kNN-MT) has been recently proposed as
a non-parametric solution for domain adaptation in neural machine translation
(NMT). It aims to alleviate the performance degradation of advanced MT systems
in translating out-of-domain sentences by coordinating with an additional
token-level feature-based retrieval module constructed from in-domain data.
Previous studies have already demonstrated that non-parametric NMT is even
superior to models fine-tuned on out-of-domain data. In spite of this success,
kNN retrieval is at the expense of high latency, in particular for large
datastores. To make it practical, in this paper, we explore a more efficient
kNN-MT and propose to use clustering to improve the retrieval efficiency.
Concretely, we first propose a cluster-based Compact Network for feature
reduction in a contrastive learning manner to compress context features into
90+% lower dimensional vectors. We then suggest a cluster-based pruning
solution to filter out 10%-40% redundant nodes in large datastores while
retaining translation quality. Our proposed methods achieve better or
comparable performance while reducing up to 57% inference latency against the
advanced non-parametric MT model on several machine translation benchmarks.
Experimental results indicate that the proposed methods maintain the most
useful information of the original datastore and the Compact Network shows good
generalization on unseen domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Universality-Individuality Integration Model for Dialog Act Classification. (arXiv:2204.06185v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06185">
<div class="article-summary-box-inner">
<span><p>Dialog Act (DA) reveals the general intent of the speaker utterance in a
conversation. Accurately predicting DAs can greatly facilitate the development
of dialog agents. Although researchers have done extensive research on dialog
act classification, the feature information of classification has not been
fully considered. This paper suggests that word cues, part-of-speech cues and
statistical cues can complement each other to improve the basis for
recognition. In addition, the different types of the three lead to the
diversity of their distribution forms, which hinders the mining of feature
information. To solve this problem, we propose a novel model based on
universality and individuality strategies, called Universality-Individuality
Integration Model (UIIM). UIIM not only deepens the connection between the
clues by learning universality, but also utilizes the learning of individuality
to capture the characteristics of the clues themselves. Experiments were made
over two most popular benchmark data sets SwDA and MRDA for dialogue act
classification, and the results show that extracting the universalities and
individualities between cues can more fully excavate the hidden information in
the utterance, and improve the accuracy of automatic dialogue act recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing for Constituency Structure in Neural Language Models. (arXiv:2204.06201v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06201">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate to which extent contextual neural language
models (LMs) implicitly learn syntactic structure. More concretely, we focus on
constituent structure as represented in the Penn Treebank (PTB). Using standard
probing techniques based on diagnostic classifiers, we assess the accuracy of
representing constituents of different categories within the neuron activations
of a LM such as RoBERTa. In order to make sure that our probe focuses on
syntactic knowledge and not on implicit semantic generalizations, we also
experiment on a PTB version that is obtained by randomly replacing constituents
with each other while keeping syntactic structure, i.e., a semantically
ill-formed but syntactically well-formed version of the PTB. We find that 4
pretrained transfomer LMs obtain high performance on our probing tasks even on
manipulated data, suggesting that semantic and syntactic knowledge in their
representations can be separated and that constituency information is in fact
learned by the LM. Moreover, we show that a complete constituency tree can be
linearly separated from LM representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Question Rewriting Help Conversational Question Answering?. (arXiv:2204.06239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06239">
<div class="article-summary-box-inner">
<span><p>Question rewriting (QR) is a subtask of conversational question answering
(CQA) aiming to ease the challenges of understanding dependencies among
dialogue history by reformulating questions in a self-contained form. Despite
seeming plausible, little evidence is available to justify QR as a mitigation
method for CQA. To verify the effectiveness of QR in CQA, we investigate a
reinforcement learning approach that integrates QR and CQA tasks and does not
require corresponding QR datasets for targeted CQA. We find, however, that the
RL method is on par with the end-to-end baseline. We provide an analysis of the
failure and describe the difficulty of exploiting QR for CQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experimental Standards for Deep Learning Research: A Natural Language Processing Perspective. (arXiv:2204.06251v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06251">
<div class="article-summary-box-inner">
<span><p>The field of Deep Learning (DL) has undergone explosive growth during the
last decade, with a substantial impact on Natural Language Processing (NLP) as
well. Yet, as with other fields employing DL techniques, there has been a lack
of common experimental standards compared to more established disciplines.
Starting from fundamental scientific principles, we distill ongoing discussions
on experimental standards in DL into a single, widely-applicable methodology.
Following these best practices is crucial to strengthening experimental
evidence, improve reproducibility and enable scientific progress. These
standards are further collected in a public repository to help them
transparently adapt to future needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Matters in Language Conditioned Robotic Imitation Learning. (arXiv:2204.06252v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06252">
<div class="article-summary-box-inner">
<span><p>A long-standing goal in robotics is to build robots that can perform a wide
range of daily tasks from perceptions obtained with their onboard sensors and
specified only via natural language. While recently substantial advances have
been achieved in language-driven robotics by leveraging end-to-end learning
from pixels, there is no clear and well-understood process for making various
design choices due to the underlying variation in setups. In this paper, we
conduct an extensive study of the most critical challenges in learning language
conditioned policies from offline free-form imitation datasets. We further
identify architectural and algorithmic techniques that improve performance,
such as a hierarchical decomposition of the robot control learning, a
multimodal transformer encoder, discrete latent plans and a self-supervised
contrastive loss that aligns video and language representations. By combining
the results of our investigation with our improved model components, we are
able to present a novel approach that significantly outperforms the state of
the art on the challenging language conditioned long-horizon robot manipulation
CALVIN benchmark. We have open-sourced our implementation to facilitate future
research in learning to perform many complex manipulation skills in a row
specified with natural language. Codebase and trained models available at
<a href="http://hulc.cs.uni-freiburg.de">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-critical Sequence Training for Automatic Speech Recognition. (arXiv:2204.06260v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06260">
<div class="article-summary-box-inner">
<span><p>Although automatic speech recognition (ASR) task has gained remarkable
success by sequence-to-sequence models, there are two main mismatches between
its training and testing that might lead to performance degradation: 1) The
typically used cross-entropy criterion aims to maximize log-likelihood of the
training data, while the performance is evaluated by word error rate (WER), not
log-likelihood; 2) The teacher-forcing method leads to the dependence on ground
truth during training, which means that model has never been exposed to its own
prediction before testing. In this paper, we propose an optimization method
called self-critical sequence training (SCST) to make the training procedure
much closer to the testing phase. As a reinforcement learning (RL) based
method, SCST utilizes a customized reward function to associate the training
criterion and WER. Furthermore, it removes the reliance on teacher-forcing and
harmonizes the model with respect to its inference procedure. We conducted
experiments on both clean and noisy speech datasets, and the results show that
the proposed SCST respectively achieves 8.7% and 7.8% relative improvements
over the baseline in terms of WER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TangoBERT: Reducing Inference Cost by using Cascaded Architecture. (arXiv:2204.06271v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06271">
<div class="article-summary-box-inner">
<span><p>The remarkable success of large transformer-based models such as BERT,
RoBERTa and XLNet in many NLP tasks comes with a large increase in monetary and
environmental cost due to their high computational load and energy consumption.
In order to reduce this computational load in inference time, we present
TangoBERT, a cascaded model architecture in which instances are first processed
by an efficient but less accurate first tier model, and only part of those
instances are additionally processed by a less efficient but more accurate
second tier model. The decision of whether to apply the second tier model is
based on a confidence score produced by the first tier model. Our simple method
has several appealing practical advantages compared to standard cascading
approaches based on multi-layered transformer models. First, it enables higher
speedup gains (average lower latency). Second, it takes advantage of batch size
optimization for cascading, which increases the relative inference cost
reductions. We report TangoBERT inference CPU speedup on four text
classification GLUE tasks and on one reading comprehension task. Experimental
results show that TangoBERT outperforms efficient early exit baseline models;
on the the SST-2 task, it achieves an accuracy of 93.9% with a CPU speedup of
8.2x.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding. (arXiv:2204.06283v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06283">
<div class="article-summary-box-inner">
<span><p>In the age of large transformer language models, linguistic evaluation play
an important role in diagnosing models' abilities and limitations on natural
language understanding. However, current evaluation methods show some
significant shortcomings. In particular, they do not provide insight into how
well a language model captures distinct linguistic skills essential for
language understanding and reasoning. Thus they fail to effectively map out the
aspects of language understanding that remain challenging to existing models,
which makes it hard to discover potential limitations in models and datasets.
In this paper, we introduce Curriculum as a new format of NLI benchmark for
evaluation of broad-coverage linguistic phenomena. Curriculum contains a
collection of datasets that covers 36 types of major linguistic phenomena and
an evaluation procedure for diagnosing how well a language model captures
reasoning skills for distinct types of linguistic phenomena. We show that this
linguistic-phenomena-driven benchmark can serve as an effective tool for
diagnosing model behavior and verifying model learning quality. In addition,
Our experiments provide insight into the limitation of existing benchmark
datasets and state-of-the-art models that may encourage future research on
re-designing datasets, model architectures, and learning objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIB-VA at SemEval-2022 Task 5: A Multimodal Architecture for the Detection and Classification of Misogynous Memes. (arXiv:2204.06299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06299">
<div class="article-summary-box-inner">
<span><p>The detection of offensive, hateful content on social media is a challenging
problem that affects many online users on a daily basis. Hateful content is
often used to target a group of people based on ethnicity, gender, religion and
other factors. The hate or contempt toward women has been increasing on social
platforms. Misogynous content detection is especially challenging when textual
and visual modalities are combined to form a single context, e.g., an overlay
text embedded on top of an image, also known as meme. In this paper, we present
a multimodal architecture that combines textual and visual features in order to
detect misogynous meme content. The proposed architecture is evaluated in the
SemEval-2022 Task 5: MAMI - Multimedia Automatic Misogyny Identification
challenge under the team name TIB-VA. Our solution obtained the best result in
the Task-B where the challenge is to classify whether a given document is
misogynous and further identify the main sub-classes of shaming, stereotype,
objectification, and violence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification. (arXiv:2204.06305v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06305">
<div class="article-summary-box-inner">
<span><p>Prompt-based learning (i.e., prompting) is an emerging paradigm for
exploiting knowledge learned by a pretrained language model. In this paper, we
propose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method
to automatically select label mappings for few-shot text classification with
prompting. Our method exploits one-to-many label mappings and a
statistics-based algorithm to select label mappings given a prompt template.
Our experiments demonstrate that AMuLaP achieves competitive performance on the
GLUE benchmark without human effort or external resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Call-sign recognition and understanding for noisy air-traffic transcripts using surveillance information. (arXiv:2204.06309v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06309">
<div class="article-summary-box-inner">
<span><p>Air traffic control (ATC) relies on communication via speech between pilot
and air-traffic controller (ATCO). The call-sign, as unique identifier for each
flight, is used to address a specific pilot by the ATCO. Extracting the
call-sign from the communication is a challenge because of the noisy ATC voice
channel and the additional noise introduced by the receiver. A low
signal-to-noise ratio (SNR) in the speech leads to high word error rate (WER)
transcripts. We propose a new call-sign recognition and understanding (CRU)
system that addresses this issue. The recognizer is trained to identify
call-signs in noisy ATC transcripts and convert them into the standard
International Civil Aviation Organization (ICAO) format. By incorporating
surveillance information, we can multiply the call-sign accuracy (CSA) up to a
factor of four. The introduced data augmentation adds additional performance on
high WER transcripts and allows the adaptation of the model to unseen
airspaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Production federated keyword spotting via distillation, filtering, and joint federated-centralized training. (arXiv:2204.06322v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06322">
<div class="article-summary-box-inner">
<span><p>We trained a keyword spotting model using federated learning on real user
devices and observed significant improvements when the model was deployed for
inference on phones. To compensate for data domains that are missing from
on-device training caches, we employed joint federated-centralized training.
And to learn in the absence of curated labels on-device, we formulated a
confidence filtering strategy based on user-feedback signals for federated
distillation. These techniques created models that significantly improved
quality metrics in offline evaluations and user-experience metrics in live A/B
experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition. (arXiv:2204.06328v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06328">
<div class="article-summary-box-inner">
<span><p>Pre-training with self-supervised models, such as Hidden-unit BERT (HuBERT)
and wav2vec 2.0, has brought significant improvements in automatic speech
recognition (ASR). However, these models usually require an expensive
computational cost to achieve outstanding performance, slowing down the
inference speed. To improve the model efficiency, we propose an early exit
scheme for ASR, namely HuBERT-EE, that allows the model to stop the inference
dynamically. In HuBERT-EE, multiple early exit branches are added at the
intermediate layers, and each branch is used to decide whether a prediction can
be exited early. Experimental results on the LibriSpeech dataset show that
HuBERT-EE can accelerate the inference of a large-scale HuBERT model while
simultaneously balancing the trade-off between the word error rate (WER)
performance and the latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Approach to Train Diverse Types of Language Models for Health Mention Classification of Tweets. (arXiv:2204.06337v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06337">
<div class="article-summary-box-inner">
<span><p>Health mention classification deals with the disease detection in a given
text containing disease words. However, non-health and figurative use of
disease words adds challenges to the task. Recently, adversarial training
acting as a means of regularization has gained popularity in many NLP tasks. In
this paper, we propose a novel approach to train language models for health
mention classification of tweets that involves adversarial training. We
generate adversarial examples by adding perturbation to the representations of
transformer models for tweet examples at various levels using Gaussian noise.
Further, we employ contrastive loss as an additional objective function. We
evaluate the proposed method on the PHM2017 dataset extended version. Results
show that our proposed approach improves the performance of classifier
significantly over the baseline methods. Moreover, our analysis shows that
adding noise at earlier layers improves models' performance whereas adding
noise at intermediate layers deteriorates models' performance. Finally, adding
noise towards the final layers performs better than the middle layers noise
addition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WikiDiverse: A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types. (arXiv:2204.06347v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06347">
<div class="article-summary-box-inner">
<span><p>Multimodal Entity Linking (MEL) which aims at linking mentions with
multimodal contexts to the referent entities from a knowledge base (e.g.,
Wikipedia), is an essential task for many multimodal applications. Although
much attention has been paid to MEL, the shortcomings of existing MEL datasets
including limited contextual topics and entity types, simplified mention
ambiguity, and restricted availability, have caused great obstacles to the
research and application of MEL. In this paper, we present WikiDiverse, a
high-quality human-annotated MEL dataset with diversified contextual topics and
entity types from Wikinews, which uses Wikipedia as the corresponding knowledge
base. A well-tailored annotation procedure is adopted to ensure the quality of
the dataset. Based on WikiDiverse, a sequence of well-designed MEL models with
intra-modality and inter-modality attentions are implemented, which utilize the
visual information of images more adequately than existing MEL models do.
Extensive experimental analyses are conducted to investigate the contributions
of different modalities in terms of MEL, facilitating the future research on
this task. The dataset and baseline models are available at
https://github.com/wangxw5/wikiDiverse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRUSH: Contextually Regularized and User anchored Self-supervised Hate speech Detection. (arXiv:2204.06389v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06389">
<div class="article-summary-box-inner">
<span><p>The last decade has witnessed a surge in the interaction of people through
social networking platforms. While there are several positive aspects of these
social platforms, the proliferation has led them to become the breeding ground
for cyber-bullying and hate speech. Recent advances in NLP have often been used
to mitigate the spread of such hateful content. Since the task of hate speech
detection is usually applicable in the context of social networks, we introduce
CRUSH, a framework for hate speech detection using user-anchored
self-supervision and contextual regularization. Our proposed approach secures ~
1-12% improvement in test set metrics over best performing previous approaches
on two types of tasks and multiple popular english social media datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Markovian Generative Architectures for Efficient Task-Oriented Dialog Systems. (arXiv:2204.06452v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06452">
<div class="article-summary-box-inner">
<span><p>Recently, Transformer based pretrained language models (PLMs), such as GPT2
and T5, have been leveraged to build generative task-oriented dialog (TOD)
systems. A drawback of existing PLM-based models is their non-Markovian
architectures across turns, i.e., the whole history is used as the conditioning
input at each turn, which brings inefficiencies in memory, computation and
learning. In this paper, we propose to revisit Markovian Generative
Architectures (MGA), which have been used in previous LSTM-based TOD systems,
but not studied for PLM-based systems. Experiments on MultiWOZ2.1 show the
efficiency advantages of the proposed Markovian PLM-based systems over their
non-Markovian counterparts, in both supervised and semi-supervised settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer. (arXiv:2204.06457v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06457">
<div class="article-summary-box-inner">
<span><p>Large pre-trained multilingual models such as mBERT and XLM-R enabled
effective cross-lingual zero-shot transfer in many NLP tasks. A cross-lingual
adjustment of these models using a small parallel corpus can potentially
further improve results. This is a more data efficient method compared to
training a machine-translation system or a multi-lingual model from scratch
using only parallel data. In this study, we experiment with zero-shot transfer
of English models to four typologically different languages (Spanish, Russian,
Vietnamese, and Hindi) and three NLP tasks (QA, NLI, and NER). We carry out a
cross-lingual adjustment of an off-the-shelf mBERT model. We confirm prior
finding that this adjustment makes embeddings of semantically similar words
from different languages closer to each other, while keeping unrelated words
apart. However, from the paired-differences histograms introduced in our work
we can see that the adjustment only modestly affects the relative distances
between related and unrelated words. In contrast, fine-tuning of mBERT on
English data (for a specific task such as NER) draws embeddings of both related
and unrelated words closer to each other. The cross-lingual adjustment of mBERT
improves NLI in four languages and NER in two languages, while QA performance
never improves and sometimes degrades. When we fine-tune a cross-lingual
adjusted mBERT for a specific task (e.g., NLI), the cross-lingual adjustment of
mBERT may still improve the separation between related and related words, but
this works consistently only for the XNLI task. Our study contributes to a
better understanding of cross-lingual transfer capabilities of large
multilingual language models and of effectiveness of their cross-lingual
adjustment in various NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Language Model Adaptive Fine-Tuning: A Study on African Languages. (arXiv:2204.06487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06487">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained language models (PLMs) have demonstrated impressive
performance on several downstream tasks on both high resourced and
low-resourced languages. However, there is still a large performance drop for
languages unseen during pre-training, especially African languages. One of the
most effective approaches to adapt to a new language is language adaptive
fine-tuning (LAFT) -- fine-tuning a multilingual PLM on monolingual texts of a
language using the same pre-training objective. However, African languages with
large monolingual texts are few, and adapting to each of them individually
takes large disk space and limits the cross-lingual transfer abilities of the
resulting models because they have been specialized for a single language. In
this paper, we perform multilingual adaptive fine-tuning (MAFT) on 17
most-resourced African languages and three other high-resource languages widely
spoken on the African continent -- English, French, and Arabic to encourage
cross-lingual transfer learning. Additionally, to further specialize the
multilingual PLM, we removed vocabulary tokens from the embedding layer that
corresponds to non-African writing scripts before MAFT, thus reducing the model
size by around 50\%. Our evaluation on two multilingual PLMs (AfriBERTa and
XLM-R) and three NLP tasks (NER, news topic classification, and sentiment
classification) shows that our approach is competitive to applying LAFT on
individual languages while requiring significantly less disk space. Finally, we
show that our adapted PLM also improves the zero-shot cross-lingual transfer
abilities of parameter efficient fine-tuning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Study of Indian English Pronunciation Variabilities relative to Received Pronunciation. (arXiv:2204.06502v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06502">
<div class="article-summary-box-inner">
<span><p>In contrast to British or American English, labeled pronunciation data on the
phonetic level is scarce for Indian English (IE). This has made it challenging
to study pronunciations of Indian English. Moreover, IE has many varieties,
resulting from various native language influences on L2 English. Indian English
has been studied in the past, by a few linguistic works. They report phonetic
rules for such characterisation, however, the extent to which they can be
applied to a diverse large-scale Indian pronunciation data remains
under-examined. We consider a corpus, IndicTIMIT, which is rich in the
diversity of IE varieties and is curated in a nativity balanced manner. It
contains data from 80 speakers corresponding to various regions of India. We
present an approach to validate the phonetic rules of IE along with reporting
unexplored rules derived using a data-driven manner, on this corpus. We also
provide quantitative information regarding which rules are more prominently
observed than the others, attributing to their relevance in IE accordingly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations. (arXiv:2204.06508v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06508">
<div class="article-summary-box-inner">
<span><p>Despite recent improvements in abstractive summarization, most current
approaches generate summaries that are not factually consistent with the source
document, severely restricting their trust and usage in real-world
applications. Recent works have shown promising improvements in factuality
error identification using text or dependency arc entailments; however, they do
not consider the entire semantic graph simultaneously. To this end, we propose
FactGraph, a method that decomposes the document and the summary into
structured meaning representations (MR), which are more suitable for factuality
evaluation. MRs describe core semantic concepts and their relations,
aggregating the main content in both document and summary in a canonical form,
and reducing data sparsity. FactGraph encodes such graphs using a graph encoder
augmented with structure-aware adapters to capture interactions among the
concepts based on the graph connectivity, along with text representations using
an adapter-based text encoder. Experiments on different benchmarks for
evaluating factuality show that FactGraph outperforms previous approaches by up
to 15%. Furthermore, FactGraph improves performance on identifying content
verifiability errors and better captures subsentence-level factual
inconsistencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Training of Language Models using JAX pjit and TPUv4. (arXiv:2204.06514v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06514">
<div class="article-summary-box-inner">
<span><p>Modern large language models require distributed training strategies due to
their size. The challenges of efficiently and robustly training them are met
with rapid developments on both software and hardware frontiers. In this
technical report, we explore challenges and design decisions associated with
developing a scalable training framework, and present a quantitative analysis
of efficiency improvements coming from adopting new software and hardware
solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A pipeline and comparative study of 12 machine learning models for text classification. (arXiv:2204.06518v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06518">
<div class="article-summary-box-inner">
<span><p>Text-based communication is highly favoured as a communication method,
especially in business environments. As a result, it is often abused by sending
malicious messages, e.g., spam emails, to deceive users into relaying personal
information, including online accounts credentials or banking details. For this
reason, many machine learning methods for text classification have been
proposed and incorporated into the services of most email providers. However,
optimising text classification algorithms and finding the right tradeoff on
their aggressiveness is still a major research problem.
</p>
<p>We present an updated survey of 12 machine learning text classifiers applied
to a public spam corpus. A new pipeline is proposed to optimise hyperparameter
selection and improve the models' performance by applying specific methods
(based on natural language processing) in the preprocessing stage.
</p>
<p>Our study aims to provide a new methodology to investigate and optimise the
effect of different feature sizes and hyperparameters in machine learning
classifiers that are widely used in text classification problems. The
classifiers are tested and evaluated on different metrics including F-score
(accuracy), precision, recall, and run time. By analysing all these aspects, we
show how the proposed pipeline can be used to achieve a good accuracy towards
spam filtering on the Enron dataset, a widely used public email corpus.
Statistical tests and explainability techniques are applied to provide a robust
analysis of the proposed pipeline and interpret the classification outcomes of
the 12 machine learning models, also identifying words that drive the
classification results. Our analysis shows that it is possible to identify an
effective machine learning model to classify the Enron dataset with an F-score
of 94%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Event Linking to Wikidata. (arXiv:2204.06535v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06535">
<div class="article-summary-box-inner">
<span><p>We present a task of multilingual linking of events to a knowledge base. We
automatically compile a large-scale dataset for this task, comprising of 1.8M
mentions across 44 languages referring to over 10.9K events from Wikidata. We
propose two variants of the event linking task: 1) multilingual, where event
descriptions are from the same language as the mention, and 2) crosslingual,
where all event descriptions are in English. On the two proposed tasks, we
compare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and
multilingual adaptations of the biencoder and crossencoder architectures from
BLINK (Wu et al., 2020). In our experiments on the two task variants, we find
both biencoder and crossencoder models significantly outperform the BM25+
baseline. Our results also indicate that the crosslingual task is in general
more challenging than the multilingual task. To test the out-of-domain
generalization of the proposed linking systems, we additionally create a
Wikinews-based evaluation set. We present qualitative analysis highlighting
various aspects captured by the proposed dataset, including the need for
temporal reasoning over context and tackling diverse event descriptions across
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Uncertainty Quantification for Machine Translation Evaluation. (arXiv:2204.06546v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06546">
<div class="article-summary-box-inner">
<span><p>Neural-based machine translation (MT) evaluation metrics are progressing
fast. However, these systems are often hard to interpret and might produce
unreliable scores when human references or assessments are noisy or when data
is out-of-domain. Recent work leveraged uncertainty quantification techniques
such as Monte Carlo dropout and deep ensembles to provide confidence intervals,
but these techniques (as we show) are limited in several ways. In this paper we
investigate more powerful and efficient uncertainty predictors for MT
evaluation metrics and their potential to capture aleatoric and epistemic
uncertainty. To this end we train the COMET metric with new heteroscedastic
regression, divergence minimization, and direct uncertainty prediction
objectives. Our experiments show improved results on WMT20 and WMT21 metrics
task datasets and a substantial reduction in computational costs. Moreover,
they demonstrate the ability of our predictors to identify low quality
references and to reveal model uncertainty due to out-of-domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Few-shot Debugging for NLU Test Suites. (arXiv:2204.06555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06555">
<div class="article-summary-box-inner">
<span><p>We study few-shot debugging of transformer based natural language
understanding models, using recently popularized test suites to not just
diagnose but correct a problem. Given a few debugging examples of a certain
phenomenon, and a held-out test set of the same phenomenon, we aim to maximize
accuracy on the phenomenon at a minimal cost of accuracy on the original test
set. We examine several methods that are faster than full epoch retraining. We
introduce a new fast method, which samples a few in-danger examples from the
original training set. Compared to fast methods using parameter distance
constraints or Kullback-Leibler divergence, we achieve superior original
accuracy for comparable debugging accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query Obfuscation Semantic Decomposition. (arXiv:1909.05819v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.05819">
<div class="article-summary-box-inner">
<span><p>We propose a method to protect the privacy of search engine users by
decomposing the queries using semantically \emph{related} and unrelated
\emph{distractor} terms. Instead of a single query, the search engine receives
multiple decomposed query terms. Next, we reconstruct the search results
relevant to the original query term by aggregating the search results retrieved
for the decomposed query terms. We show that the word embeddings learnt using a
distributed representation learning method can be used to find semantically
related and distractor query terms. We derive the relationship between the
\emph{obfuscity} achieved through the proposed query anonymisation method and
the \emph{reconstructability} of the original search results using the
decomposed queries. We analytically study the risk of discovering the search
engine users' information intents under the proposed query obfuscation method,
and empirically evaluate its robustness against clustering-based attacks. Our
experimental results show that the proposed method can accurately reconstruct
the search results for user queries, without compromising the privacy of the
search engine users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Language Model for Task-Oriented Dialogue. (arXiv:2005.00796v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00796">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue is often decomposed into three tasks: understanding
user input, deciding actions, and generating a response. While such
decomposition might suggest a dedicated model for each sub-task, we find a
simple, unified approach leads to state-of-the-art performance on the MultiWOZ
dataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a
single, causal language model trained on all sub-tasks recast as a single
sequence prediction problem. This allows SimpleTOD to fully leverage transfer
learning from pre-trained, open domain, causal language models such as GPT-2.
SimpleTOD improves over the prior state-of-the-art in joint goal accuracy for
dialogue state tracking, and our analysis reveals robustness to noisy
annotations in this setting. SimpleTOD also improves the main metrics used to
evaluate action decisions and response generation in an end-to-end setting:
inform rate by 8.1 points, success rate by 9.7 points, and combined score by
7.2 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memformer: A Memory-Augmented Transformer for Sequence Modeling. (arXiv:2010.06891v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06891">
<div class="article-summary-box-inner">
<span><p>Transformers have reached remarkable success in sequence modeling. However,
these models have efficiency issues as they need to store all the history
token-level representations as memory. We present Memformer, an efficient
neural network for sequence modeling, that utilizes an external dynamic memory
to encode and retrieve past information. Our model achieves linear time
complexity and constant memory space complexity when processing long sequences.
We also propose a new optimization scheme, memory replay back-propagation
(MRBP), which promotes long-range back-propagation through time with a
significantly reduced memory requirement. Experimental results show that
Memformer has achieved comparable performance compared to the baselines by
using 8.1x less memory space and 3.2x faster on inference. Analysis of the
attention pattern shows that our external memory slots can encode and retain
important information through timesteps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla. (arXiv:2101.00204v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00204">
<div class="article-summary-box-inner">
<span><p>In this short paper, we introduce `BanglaBERT', a BERT-based Natural Language
Understanding (NLU) model pretrained in Bangla, a widely spoken yet
low-resource language in the NLP literature. To pretrain BanglaBERT, we collect
27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular
Bangla sites. We introduce two new downstream task datasets on natural language
inference and question answering and benchmark on four diverse NLU tasks
covering text classification, sequence labeling, and span prediction. In the
process, we bring them under the first-ever Bangla Language Understanding
Evaluation (BangLUE) benchmark. BanglaBERT achieves state-of-the-art results
outperforming multilingual and monolingual models. We are making the BanglaBERT
model, the new datasets, and a leaderboard publicly available at
\url{https://github.com/csebuetnlp/banglabert} to advance Bangla NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combating Temporal Drift in Crisis with Adapted Embeddings. (arXiv:2104.08535v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08535">
<div class="article-summary-box-inner">
<span><p>Language usage changes over time, and this can impact the effectiveness of
NLP systems. This work investigates methods for adapting to changing discourse
during crisis events. We explore social media data during crisis, for which
effective, time-sensitive methods are necessary. We experiment with two
separate methods to accommodate changing data: temporal pretraining, which uses
unlabeled data for the target time periods to train better language models, and
a model of embedding shift based on tools for analyzing semantic change. This
shift allows us to counteract temporal drift by normalizing incoming data based
on observed patterns of language change. Simulating scenarios in which we lack
access to incoming labeled data, we demonstrate the effectiveness of these
methods for a wide variety of crises, showing we can improve performance by up
to 8.0 F1 score for relevance classification across datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document Structure aware Relational Graph Convolutional Networks for Ontology Population. (arXiv:2104.12950v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12950">
<div class="article-summary-box-inner">
<span><p>Ontologies comprising of concepts, their attributes, and relationships are
used in many knowledge based AI systems. While there have been efforts towards
populating domain specific ontologies, we examine the role of document
structure in learning ontological relationships between concepts in any
document corpus. Inspired by ideas from hypernym discovery and explainability,
our method performs about 15 points more accurate than a stand-alone R-GCN
model for this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Cross-Lingual Sentence Representation Learning. (arXiv:2105.13856v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13856">
<div class="article-summary-box-inner">
<span><p>Large-scale models for learning fixed-dimensional cross-lingual sentence
representations like LASER (Artetxe and Schwenk, 2019b) lead to significant
improvement in performance on downstream tasks. However, further increases and
modifications based on such large-scale models are usually impractical due to
memory limitations. In this work, we introduce a lightweight dual-transformer
architecture with just 2 layers for generating memory-efficient cross-lingual
sentence representations. We explore different training tasks and observe that
current cross-lingual training tasks leave a lot to be desired for this shallow
architecture. To ameliorate this, we propose a novel cross-lingual language
model, which combines the existing single-word masked language model with the
newly proposed cross-lingual token-level reconstruction task. We further
augment the training task by the introduction of two computationally-lite
sentence-level contrastive learning tasks to enhance the alignment of
cross-lingual sentence representation space, which compensates for the learning
bottleneck of the lightweight transformer for generative tasks. Our comparisons
with competing models on cross-lingual sentence retrieval and multilingual
document classification confirm the effectiveness of the newly proposed
training tasks for a shallow model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BenchIE: A Framework for Multi-Faceted Fact-Based Open Information Extraction Evaluation. (arXiv:2109.06850v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06850">
<div class="article-summary-box-inner">
<span><p>Intrinsic evaluations of OIE systems are carried out either manually -- with
human evaluators judging the correctness of extractions -- or automatically, on
standardized benchmarks. The latter, while much more cost-effective, is less
reliable, primarily because of the incompleteness of the existing OIE
benchmarks: the ground truth extractions do not include all acceptable variants
of the same fact, leading to unreliable assessment of the models' performance.
Moreover, the existing OIE benchmarks are available for English only. In this
work, we introduce BenchIE: a benchmark and evaluation framework for
comprehensive evaluation of OIE systems for English, Chinese, and German. In
contrast to existing OIE benchmarks, BenchIE is fact-based, i.e., it takes into
account informational equivalence of extractions: our gold standard consists of
fact synsets, clusters in which we exhaustively list all acceptable surface
forms of the same fact. Moreover, having in mind common downstream applications
for OIE, we make BenchIE multi-faceted; i.e., we create benchmark variants that
focus on different facets of OIE evaluation, e.g., compactness or minimality of
extractions. We benchmark several state-of-the-art OIE systems using BenchIE
and demonstrate that these systems are significantly less effective than
indicated by existing OIE benchmarks. We make BenchIE (data and evaluation
code) publicly available on https://github.com/gkiril/benchie.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnnIE: An Annotation Platform for Constructing Complete Open Information Extraction Benchmark. (arXiv:2109.07464v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07464">
<div class="article-summary-box-inner">
<span><p>Open Information Extraction (OIE) is the task of extracting facts from
sentences in the form of relations and their corresponding arguments in
schema-free manner. Intrinsic performance of OIE systems is difficult to
measure due to the incompleteness of existing OIE benchmarks: the ground truth
extractions do not group all acceptable surface realizations of the same fact
that can be extracted from a sentence. To measure performance of OIE systems
more realistically, it is necessary to manually annotate complete facts (i.e.,
clusters of all acceptable surface realizations of the same fact) from input
sentences. We propose AnnIE: an interactive annotation platform that
facilitates such challenging annotation tasks and supports creation of complete
fact-oriented OIE evaluation benchmarks. AnnIE is modular and flexible in order
to support different use case scenarios (i.e., benchmarks covering different
types of facts). We use AnnIE to build two complete OIE benchmarks: one with
verb-mediated facts and another with facts encompassing named entities.
Finally, we evaluate several OIE systems on our complete benchmarks created
with AnnIE. Our results suggest that existing incomplete benchmarks are overly
lenient, and that OIE systems are not as robust as previously reported. We
publicly release AnnIE under non-restrictive license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Template-free Prompt Tuning for Few-shot NER. (arXiv:2109.13532v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13532">
<div class="article-summary-box-inner">
<span><p>Prompt-based methods have been successfully applied in sentence-level
few-shot learning tasks, mostly owing to the sophisticated design of templates
and label words. However, when applied to token-level labeling tasks such as
NER, it would be time-consuming to enumerate the template queries over all
potential entity spans. In this work, we propose a more elegant method to
reformulate NER tasks as LM problems without any templates. Specifically, we
discard the template construction process while maintaining the word prediction
paradigm of pre-training models to predict a class-related pivot word (or label
word) at the entity position. Meanwhile, we also explore principled ways to
automatically search for appropriate label words that the pre-trained models
can easily adapt to. While avoiding complicated template-based process, the
proposed LM objective also reduces the gap between different objectives used in
pre-training and fine-tuning, thus it can better benefit the few-shot
performance. Experimental results demonstrate the effectiveness of the proposed
method over bert-tagger and template-based method under few-shot setting.
Moreover, the decoding speed of the proposed method is up to 1930.12 times
faster than the template-based method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation. (arXiv:2110.08547v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08547">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates that multilingual pretraining and multilingual
fine-tuning are both critical for facilitating cross-lingual transfer in
zero-shot translation, where the neural machine translation (NMT) model is
tested on source languages unseen during supervised training. Following this
idea, we present SixT+, a strong many-to-English NMT model that supports 100
source languages but is trained with a parallel dataset in only six source
languages. SixT+ initializes the decoder embedding and the full encoder with
XLM-R large and then trains the encoder and decoder layers with a simple
two-stage training strategy. SixT+ achieves impressive performance on
many-to-English translation. It significantly outperforms CRISS and m2m-100,
two strong multilingual NMT systems, with an average gain of 7.2 and 5.0 BLEU
respectively. Additionally, SixT+ offers a set of model parameters that can be
further fine-tuned to other unsupervised tasks. We demonstrate that adding
SixT+ initialization outperforms state-of-the-art explicitly designed
unsupervised NMT models on Si&lt;-&gt;En and Ne&lt;-&gt;En by over 1.2 average BLEU. When
applied to zero-shot cross-lingual abstractive summarization, it produces an
average performance gain of 12.3 ROUGE-L over mBART-ft. We conduct detailed
analyses to understand the key ingredients of SixT+, including multilinguality
of the auxiliary parallel data, positional disentangled encoder, and the
cross-lingual transferability of its encoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why KDAC? A general activation function for knowledge discovery. (arXiv:2111.13858v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13858">
<div class="article-summary-box-inner">
<span><p>Deep learning oriented named entity recognition (DNER) has gradually become
the paradigm of knowledge discovery, which greatly promotes domain
intelligence. However, the current activation function of DNER fails to treat
gradient vanishing, no negative output or non-differentiable existence, which
may impede knowledge exploration caused by the omission and incomplete
representation of latent semantics. To break through the dilemma, we present a
novel activation function termed KDAC. Detailly, KDAC is an aggregation
function with multiple conversion modes. The backbone of the activation region
is the interaction between exponent and linearity, and the both ends extend
through adaptive linear divergence, which surmounts the obstacle of gradient
vanishing and no negative output. Crucially, the non-differentiable points are
alerted and eliminated by an approximate smoothing algorithm. KDAC has a series
of brilliant properties, including nonlinear, stable near-linear transformation
and derivative, as well as dynamic style, etc. We perform experiments based on
BERT-BiLSTM-CNN-CRF model on six benchmark datasets containing different domain
knowledge, such as Weibo, Clinical, E-commerce, Resume, HAZOP and People's
daily. The evaluation results show that KDAC is advanced and effective, and can
provide more generalized activation to stimulate the performance of DNER. We
hope that KDAC can be exploited as a promising activation function to devote
itself to the construction of knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consensus Graph Representation Learning for Better Grounded Image Captioning. (arXiv:2112.00974v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00974">
<div class="article-summary-box-inner">
<span><p>The contemporary visual captioning models frequently hallucinate objects that
are not actually in a scene, due to the visual misclassification or
over-reliance on priors that resulting in the semantic inconsistency between
the visual information and the target lexical words. The most common way is to
encourage the captioning model to dynamically link generated object words or
phrases to appropriate regions of the image, i.e., the grounded image
captioning (GIC). However, GIC utilizes an auxiliary task (grounding objects)
that has not solved the key issue of object hallucination, i.e., the semantic
inconsistency. In this paper, we take a novel perspective on the issue above -
exploiting the semantic coherency between the visual and language modalities.
Specifically, we propose the Consensus Rraph Representation Learning framework
(CGRL) for GIC that incorporates a consensus representation into the grounded
captioning pipeline. The consensus is learned by aligning the visual graph
(e.g., scene graph) to the language graph that consider both the nodes and
edges in a graph. With the aligned consensus, the captioning model can capture
both the correct linguistic characteristics and visual relevance, and then
grounding appropriate image regions further. We validate the effectiveness of
our model, with a significant decline in object hallucination (-9% CHAIRi) on
the Flickr30k Entities dataset. Besides, our CGRL also evaluated by several
automatic metrics and human evaluation, the results indicate that the proposed
approach can simultaneously improve the performance of image captioning (+2.9
Cider) and grounding (+2.3 F1LOC).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenIE: Generative Information Extraction. (arXiv:2112.08340v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08340">
<div class="article-summary-box-inner">
<span><p>Structured and grounded representation of text is typically formalized by
closed information extraction, the problem of extracting an exhaustive set of
(subject, relation, object) triplets that are consistent with a predefined set
of entities and relations from a knowledge base schema. Most existing works are
pipelines prone to error accumulation, and all approaches are only applicable
to unrealistically small numbers of entities and relations. We introduce GenIE
(generative information extraction), the first end-to-end autoregressive
formulation of closed information extraction. GenIE naturally exploits the
language knowledge from the pre-trained transformer by autoregressively
generating relations and entities in textual form. Thanks to a new bi-level
constrained generation strategy, only triplets consistent with the predefined
knowledge base schema are produced. Our experiments show that GenIE is
state-of-the-art on closed information extraction, generalizes from fewer
training data points than baselines, and scales to a previously unmanageable
number of entities and relations. With this work, closed information extraction
becomes practical in realistic scenarios, providing new opportunities for
downstream tasks. Finally, this work paves the way towards a unified end-to-end
approach to the core tasks of information extraction. Code, data and models
available at https://github.com/epfl-dlab/GenIE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Augmented Language Models for Cause-Effect Relation Classification. (arXiv:2112.08615v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08615">
<div class="article-summary-box-inner">
<span><p>Previous studies have shown the efficacy of knowledge augmentation methods in
pretrained language models. However, these methods behave differently across
domains and downstream tasks. In this work, we investigate the augmentation of
pretrained language models with knowledge graph data in the cause-effect
relation classification and commonsense causal reasoning tasks. After
automatically verbalizing triples in ATOMIC2020, a wide coverage commonsense
reasoning knowledge graph, we continually pretrain BERT and evaluate the
resulting model on cause-effect pair classification and answering commonsense
causal reasoning questions. Our results show that a continually pretrained
language model augmented with commonsense reasoning knowledge outperforms our
baselines on two commonsense causal reasoning benchmarks, COPA and BCOPA-CE,
and a Temporal and Causal Reasoning (TCR) dataset, without additional
improvement in model architecture or using quality-enhanced data for
fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling the Knowledge of Romanian BERTs Using Multiple Teachers. (arXiv:2112.12650v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12650">
<div class="article-summary-box-inner">
<span><p>Running large-scale pre-trained language models in computationally
constrained environments remains a challenging problem yet to be addressed,
while transfer learning from these models has become prevalent in Natural
Language Processing tasks. Several solutions, including knowledge distillation,
network quantization, or network pruning have been previously proposed;
however, these approaches focus mostly on the English language, thus widening
the gap when considering low-resource languages. In this work, we introduce
three light and fast versions of distilled BERT models for the Romanian
language: Distil-BERT-base-ro, Distil-RoBERT-base, and
DistilMulti-BERT-base-ro. The first two models resulted from the individual
distillation of knowledge from two base versions of Romanian BERTs available in
literature, while the last one was obtained by distilling their ensemble. To
our knowledge, this is the first attempt to create publicly available Romanian
distilled BERT models, which were thoroughly evaluated on five tasks:
part-of-speech tagging, named entity recognition, sentiment analysis, semantic
textual similarity, and dialect identification. Our experimental results argue
that the three distilled models offer performance comparable to their teachers,
while being twice as fast on a GPU and ~35% smaller. In addition, we further
test the similarity between the predictions of our students versus their
teachers by measuring their label and probability loyalty, together with
regression loyalty - a new metric introduced in this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v3 [q-bio.BM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11147">
<div class="article-summary-box-inner">
<span><p>Self-supervised protein language models have proved their effectiveness in
learning the proteins representations. With the increasing computational power,
current protein language models pre-trained with millions of diverse sequences
can advance the parameter scale from million-level to billion-level and achieve
remarkable improvement. However, those prevailing approaches rarely consider
incorporating knowledge graphs (KGs), which can provide rich structured
knowledge facts for better protein representations. We argue that informative
biology knowledge in KGs can enhance protein representation with external
knowledge. In this work, we propose OntoProtein, the first general framework
that makes use of structure in GO (Gene Ontology) into protein pre-training
models. We construct a novel large-scale knowledge graph that consists of GO
and its related proteins, and gene annotation texts or protein sequences
describe all nodes in the graph. We propose novel contrastive learning with
knowledge-aware negative sampling to jointly optimize the knowledge graph and
protein embedding during pre-training. Experimental results show that
OntoProtein can surpass state-of-the-art methods with pre-trained protein
language models in TAPE benchmark and yield better performance compared with
baselines in protein-protein interaction and protein function prediction. Code
and datasets are available in https://github.com/zjunlp/OntoProtein.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender stereotypes in the mediated personalization of politics: Empirical evidence from a lexical, syntactic and sentiment analysis. (arXiv:2202.03083v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03083">
<div class="article-summary-box-inner">
<span><p>The media attention to the personal sphere of famous and important
individuals has become a key element of the gender narrative. Here we combine
lexical, syntactic and sentiment analysis to investigate the role of gender in
the personalization of a wide range of political office holders in Italy during
the period 2017-2020. On the basis of a score for words that is introduced to
account for gender unbalance in both representative and news coverage, we show
that the political personalization in Italy is more detrimental for women than
men, with the persistence of entrenched stereotypes including a masculine
connotation of leadership, the resulting women's unsuitability to hold
political functions, and a greater deal of focus on their attractiveness and
body parts. In addition, women politicians are covered with a more negative
tone than their men counterpart when personal details are reported. Further,
the major contribution to the observed gender differences comes from online
news rather than print news, suggesting that the expression of certain
stereotypes may be better conveyed when click baiting and personal targeting
have a major impact.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniSAr: A Unified Structure-Aware Autoregressive Language Model for Text-to-SQL. (arXiv:2203.07781v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07781">
<div class="article-summary-box-inner">
<span><p>Existing text-to-SQL semantic parsers are typically designed for particular
settings such as handling queries that span multiple tables, domains or turns
which makes them ineffective when applied to different settings. We present
UniSAr (Unified Structure-Aware Autoregressive Language Model), which benefits
from directly using an off-the-shelf language model architecture and
demonstrates consistently high performance under different settings.
Specifically, UniSAr extends existing autoregressive language models to
incorporate three non-invasive extensions to make them structure-aware: (1)
adding structure mark to encode database schema, conversation context, and
their relationships; (2) constrained decoding to decode well structured SQL for
a given database schema; and (3) SQL completion to complete potential missing
JOIN relationships in SQL based on database schema. On seven well-known
text-to-SQL datasets covering multi-domain, multi-table and multi-turn, UniSAr
demonstrates highly comparable or better performance to the most advanced
specifically-designed text-to-SQL models. Importantly, our UniSAr is
non-invasive, such that other core model advances in text-to-SQL can also adopt
our extensions to further enhance performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XTREME-S: Evaluating Cross-lingual Speech Representations. (arXiv:2203.10752v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10752">
<div class="article-summary-box-inner">
<span><p>We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual
speech representations in many languages. XTREME-S covers four task families:
speech recognition, classification, speech-to-text translation and retrieval.
Covering 102 languages from 10+ language families, 3 different domains and 4
task families, XTREME-S aims to simplify multilingual speech representation
evaluation, as well as catalyze research in "universal" speech representation
learning. This paper describes the new benchmark and establishes the first
speech-only and speech-text baselines using XLS-R and mSLAM on all downstream
tasks. We motivate the design choices and detail how to use the benchmark.
Datasets and fine-tuning scripts are made easily accessible at
https://hf.co/datasets/google/xtreme_s.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks. (arXiv:2204.04596v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04596">
<div class="article-summary-box-inner">
<span><p>Parameter-efficient tuning aims to distill knowledge for downstream tasks by
optimizing a few introduced parameters while freezing the pretrained language
models (PLMs). Continuous prompt tuning which prepends a few trainable vectors
to the embeddings of input is one of these methods and has drawn much attention
due to its effectiveness and efficiency. This family of methods can be
illustrated as exerting nonlinear transformations of hidden states inside PLMs.
However, a natural question is ignored: can the hidden states be directly used
for classification without changing them? In this paper, we aim to answer this
question by proposing a simple tuning method which only introduces three
trainable vectors. Firstly, we integrate all layers hidden states using the
introduced vectors. And then, we input the integrated hidden state(s) to a
task-specific linear classifier to predict categories. This scheme is similar
to the way ELMo utilises hidden states except that they feed the hidden states
to LSTM-based models. Although our proposed tuning scheme is simple, it
achieves comparable performance with prompt tuning methods like P-tuning and
P-tuning v2, verifying that original hidden states do contain useful
information for classification tasks. Moreover, our method has an advantage
over prompt tuning in terms of time and the number of parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions. (arXiv:2204.05212v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05212">
<div class="article-summary-box-inner">
<span><p>Current QA systems can generate reasonable-sounding yet false answers without
explanation or evidence for the generated answer, which is especially
problematic when humans cannot readily check the model's answers. This presents
a challenge for building trust in machine learning systems. We take inspiration
from real-world situations where difficult questions are answered by
considering opposing sides (see Irving et al., 2018). For multiple-choice QA
examples, we build a dataset of single arguments for both a correct and
incorrect answer option in a debate-style set-up as an initial step in training
models to produce explanations for two candidate answers. We use long contexts
-- humans familiar with the context write convincing explanations for
pre-selected correct and incorrect answers, and we test if those explanations
allow humans who have not read the full context to more accurately determine
the correct answer. We do not find that explanations in our set-up improve
human accuracy, but a baseline condition shows that providing human-selected
text snippets does improve accuracy. We use these findings to suggest ways of
improving the debate set up for future data collection efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposed Meta-Learning for Few-Shot Named Entity Recognition. (arXiv:2204.05751v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05751">
<div class="article-summary-box-inner">
<span><p>Few-shot named entity recognition (NER) systems aim at recognizing
novel-class named entities based on only a few labeled examples. In this paper,
we present a decomposed meta-learning approach which addresses the problem of
few-shot NER by sequentially tackling few-shot span detection and few-shot
entity typing using meta-learning. In particular, we take the few-shot span
detection as a sequence labeling problem and train the span detector by
introducing the model-agnostic meta-learning (MAML) algorithm to find a good
model parameter initialization that could fast adapt to new entity classes. For
few-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced
prototypical networks to find a good embedding space that can better
distinguish text span representations from different entity classes. Extensive
experiments on various benchmarks show that our approach achieves superior
performance over prior methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Annotation of Therapeutic Working Alliance in Psychotherapy. (arXiv:2204.05522v1 [q-bio.NC] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05522">
<div class="article-summary-box-inner">
<span><p>The therapeutic working alliance is an important predictor of the outcome of
the psychotherapy treatment. In practice, the working alliance is estimated
from a set of scoring questionnaires in an inventory that both the patient and
the therapists fill out. In this work, we propose an analytical framework of
directly inferring the therapeutic working alliance from the natural language
within the psychotherapy sessions in a turn-level resolution with deep
embeddings such as the Doc2Vec and SentenceBERT models. The transcript of each
psychotherapy session can be transcribed and generated in real-time from the
session speech recordings, and these embedded dialogues are compared with the
distributed representations of the statements in the working alliance
inventory. We demonstrate, in a real-world dataset with over 950 sessions of
psychotherapy treatments in anxiety, depression, schizophrenia and suicidal
patients, the effectiveness of this method in mapping out trajectories of
patient-therapist alignment and the interpretability that can offer insights in
clinical psychiatry. We believe such a framework can be provide timely feedback
to the therapist regarding the quality of the conversation in interview
sessions.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">AGQA 2.0: An Updated Benchmark for Compositional Spatio-Temporal Reasoning. (arXiv:2204.06105v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06105">
<div class="article-summary-box-inner">
<span><p>Prior benchmarks have analyzed models' answers to questions about videos in
order to measure visual compositional reasoning. Action Genome Question
Answering (AGQA) is one such benchmark. AGQA provides a training/test split
with balanced answer distributions to reduce the effect of linguistic biases.
However, some biases remain in several AGQA categories. We introduce AGQA 2.0,
a version of this benchmark with several improvements, most namely a stricter
balancing procedure. We then report results on the updated benchmark for all
experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-World Instance Segmentation: Exploiting Pseudo Ground Truth From Learned Pairwise Affinity. (arXiv:2204.06107v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06107">
<div class="article-summary-box-inner">
<span><p>Open-world instance segmentation is the task of grouping pixels into object
instances without any pre-determined taxonomy. This is challenging, as
state-of-the-art methods rely on explicit class semantics obtained from large
labeled datasets, and out-of-domain evaluation performance drops significantly.
Here we propose a novel approach for mask proposals, Generic Grouping Networks
(GGNs), constructed without semantic supervision. Our approach combines a local
measure of pixel affinity with instance-level mask supervision, producing a
training regimen designed to make the model as generic as the data diversity
allows. We introduce a method for predicting Pairwise Affinities (PA), a
learned local relationship between pairs of pixels. PA generalizes very well to
unseen categories. From PA we construct a large set of pseudo-ground-truth
instance masks; combined with human-annotated instance masks we train GGNs and
significantly outperform the SOTA on open-world instance segmentation on
various benchmarks including COCO, LVIS, ADE20K, and UVO. Code is available on
project website: https://sites.google.com/view/generic-grouping/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SRMD: Sparse Random Mode Decomposition. (arXiv:2204.06108v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06108">
<div class="article-summary-box-inner">
<span><p>Signal decomposition and multiscale signal analysis provide many useful tools
for time-frequency analysis. We proposed a random feature method for analyzing
time-series data by constructing a sparse approximation to the spectrogram. The
randomization is both in the time window locations and the frequency sampling,
which lowers the overall sampling and computational cost. The sparsification of
the spectrogram leads to a sharp separation between time-frequency clusters
which makes it easier to identify intrinsic modes, and thus leads to a new
data-driven mode decomposition. The applications include signal representation,
outlier removal, and mode decomposition. On the benchmark tests, we show that
our approach outperforms other state-of-the-art decomposition methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Baseline Computation for Attribution Methods Based on Interpolated Inputs. (arXiv:2204.06120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06120">
<div class="article-summary-box-inner">
<span><p>We discuss a way to find a well behaved baseline for attribution methods that
work by feeding a neural network with a sequence of interpolated inputs between
two given inputs. Then, we test it with our novel Riemann-Stieltjes Integrated
Gradient-weighted Class Activation Mapping (RSI-Grad-CAM) attribution method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Text-Conditional Image Generation with CLIP Latents. (arXiv:2204.06125v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06125">
<div class="article-summary-box-inner">
<span><p>Contrastive models like CLIP have been shown to learn robust representations
of images that capture both semantics and style. To leverage these
representations for image generation, we propose a two-stage model: a prior
that generates a CLIP image embedding given a text caption, and a decoder that
generates an image conditioned on the image embedding. We show that explicitly
generating image representations improves image diversity with minimal loss in
photorealism and caption similarity. Our decoders conditioned on image
representations can also produce variations of an image that preserve both its
semantics and style, while varying the non-essential details absent from the
image representation. Moreover, the joint embedding space of CLIP enables
language-guided image manipulations in a zero-shot fashion. We use diffusion
models for the decoder and experiment with both autoregressive and diffusion
models for the prior, finding that the latter are computationally more
efficient and produce higher-quality samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Texture Extraction and Distribution for Controllable Person Image Synthesis. (arXiv:2204.06160v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06160">
<div class="article-summary-box-inner">
<span><p>We deal with the controllable person image synthesis task which aims to
re-render a human from a reference image with explicit control over body pose
and appearance. Observing that person images are highly structured, we propose
to generate desired images by extracting and distributing semantic entities of
reference images. To achieve this goal, a neural texture extraction and
distribution operation based on double attention is described. This operation
first extracts semantic neural textures from reference feature maps. Then, it
distributes the extracted neural textures according to the spatial
distributions learned from target poses. Our model is trained to predict human
images in arbitrary poses, which encourages it to extract disentangled and
expressive neural textures representing the appearance of different semantic
entities. The disentangled representation further enables explicit appearance
control. Neural textures of different reference images can be fused to control
the appearance of the interested areas. Experimental comparisons show the
superiority of the proposed model. Code is available at
https://github.com/RenYurui/Neural-Texture-Extraction-Distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Neural Textures: Generating Talking-Face Videos with Continuously Controllable Expressions. (arXiv:2204.06180v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06180">
<div class="article-summary-box-inner">
<span><p>Recently, talking-face video generation has received considerable attention.
So far most methods generate results with neutral expressions or expressions
that are implicitly determined by neural networks in an uncontrollable way. In
this paper, we propose a method to generate talking-face videos with
continuously controllable expressions in real-time. Our method is based on an
important observation: In contrast to facial geometry of moderate resolution,
most expression information lies in textures. Then we make use of neural
textures to generate high-quality talking face videos and design a novel neural
network that can generate neural textures for image frames (which we called
dynamic neural textures) based on the input expression and continuous intensity
expression coding (CIEC). Our method uses 3DMM as a 3D model to sample the
dynamic neural texture. The 3DMM does not cover the teeth area, so we propose a
teeth submodule to complete the details in teeth. Results and an ablation study
show the effectiveness of our method in generating high-quality talking-face
videos with continuously controllable expressions. We also set up four baseline
methods by combining existing representative methods and compare them with our
method. Experimental results including a user study show that our method has
the best performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViViD++: Vision for Visibility Dataset. (arXiv:2204.06183v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06183">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a dataset capturing diverse visual data formats
that target varying luminance conditions. While RGB cameras provide nourishing
and intuitive information, changes in lighting conditions potentially result in
catastrophic failure for robotic applications based on vision sensors.
Approaches overcoming illumination problems have included developing more
robust algorithms or other types of visual sensors, such as thermal and event
cameras. Despite the alternative sensors' potential, there still are few
datasets with alternative vision sensors. Thus, we provided a dataset recorded
from alternative vision sensors, by handheld or mounted on a car, repeatedly in
the same space but in different conditions. We aim to acquire visible
information from co-aligned alternative vision sensors. Our sensor system
collects data more independently from visible light intensity by measuring the
amount of infrared dissipation, depth by structured reflection, and
instantaneous temporal changes in luminance. We provide these measurements
along with inertial sensors and ground-truth for developing robust visual SLAM
under poor illumination. The full dataset is available at:
https://visibilitydataset.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COAP: Compositional Articulated Occupancy of People. (arXiv:2204.06184v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06184">
<div class="article-summary-box-inner">
<span><p>We present a novel neural implicit representation for articulated human
bodies. Compared to explicit template meshes, neural implicit body
representations provide an efficient mechanism for modeling interactions with
the environment, which is essential for human motion reconstruction and
synthesis in 3D scenes. However, existing neural implicit bodies suffer from
either poor generalization on highly articulated poses or slow inference time.
In this work, we observe that prior knowledge about the human body's shape and
kinematic structure can be leveraged to improve generalization and efficiency.
We decompose the full-body geometry into local body parts and employ a
part-aware encoder-decoder architecture to learn neural articulated occupancy
that models complex deformations locally. Our local shape encoder represents
the body deformation of not only the corresponding body part but also the
neighboring body parts. The decoder incorporates the geometric constraints of
local body shape which significantly improves pose generalization. We
demonstrate that our model is suitable for resolving self-intersections and
collisions with 3D environments. Quantitative and qualitative experiments show
that our method largely outperforms existing solutions in terms of both
efficiency and accuracy. The code and models are available at
https://neuralbodies.github.io/COAP/index.html
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrating Class Weights with Multi-Modal Information for Partial Video Domain Adaptation. (arXiv:2204.06187v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06187">
<div class="article-summary-box-inner">
<span><p>Assuming the source label space subsumes the target one, Partial Video Domain
Adaptation (PVDA) is a more general and practical scenario for cross-domain
video classification problems. The key challenge of PVDA is to mitigate the
negative transfer caused by the source-only outlier classes. To tackle this
challenge, a crucial step is to aggregate target predictions to assign class
weights by up-weighing target classes and down-weighing outlier classes.
However, the incorrect predictions of class weights can mislead the network and
lead to negative transfer. Previous works improve the class weight accuracy by
utilizing temporal features and attention mechanisms, but these methods may
fall short when trying to generate accurate class weight when domain shifts are
significant, as in most real-world scenarios. To deal with these challenges, we
propose the Multi-modality Cluster-calibrated partial Adversarial Network
(MCAN). MCAN enhances video feature extraction with multi-modal features from
multiple temporal scales to form more robust overall features. It utilizes a
novel class weight calibration method to alleviate the negative transfer caused
by incorrect class weights. The calibration method tries to identify and weigh
correct and incorrect predictions using distributional information implied by
unsupervised clustering. Extensive experiments are conducted on prevailing PVDA
benchmarks, and the proposed MCAN achieves significant improvements when
compared to state-of-the-art PVDA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Model with GA based Feature Selection and Context Integration. (arXiv:2204.06189v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06189">
<div class="article-summary-box-inner">
<span><p>Deep learning models have been very successful in computer vision and image
processing applications. Since its inception, Many top-performing methods for
image segmentation are based on deep CNN models. However, deep CNN models fail
to integrate global and local context alongside visual features despite having
complex multi-layer architectures. We propose a novel three-layered deep
learning model that assiminlate or learns independently global and local
contextual information alongside visual features. The novelty of the proposed
model is that One-vs-All binary class-based learners are introduced to learn
Genetic Algorithm (GA) optimized features in the visual layer, followed by the
contextual layer that learns global and local contexts of an image, and finally
the third layer integrates all the information optimally to obtain the final
class label. Stanford Background and CamVid benchmark image parsing datasets
were used for our model evaluation, and our model shows promising results. The
empirical analysis reveals that optimized visual features with global and local
contextual information play a significant role to improve accuracy and produce
stable predictions comparable to state-of-the-art deep CNN models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">5G Features and Standards for Vehicle Data Exploitation. (arXiv:2204.06211v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06211">
<div class="article-summary-box-inner">
<span><p>Cars capture and generate huge volumes of data in real-time about the driving
dynamics, the environment, and the driver and passengers' activities. Due to
the proliferation of cooperative, connected and automated mobility (CCAM), the
value of data from vehicles is getting strategic, not just for the automotive
industry, but also for many diverse stakeholders including small and
medium-sized enterprises (SMEs) and start-ups. 5G can enable car-captured data
to feed innovative applications and services deployed in the cloud ensuring
lower latency and higher throughput than previous cellular technologies. This
paper identifies and discusses the relevance of the main 5G features that can
contribute to a scalable, flexible, reliable and secure data pipeline, pointing
to the standards and technical reports that specify their implementation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defensive Patches for Robust Recognition in the Physical World. (arXiv:2204.06213v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06213">
<div class="article-summary-box-inner">
<span><p>To operate in real-world high-stakes environments, deep learning systems have
to endure noises that have been continuously thwarting their robustness.
Data-end defense, which improves robustness by operations on input data instead
of modifying models, has attracted intensive attention due to its feasibility
in practice. However, previous data-end defenses show low generalization
against diverse noises and weak transferability across multiple models.
Motivated by the fact that robust recognition depends on both local and global
features, we propose a defensive patch generation framework to address these
problems by helping models better exploit these features. For the
generalization against diverse noises, we inject class-specific identifiable
patterns into a confined local patch prior, so that defensive patches could
preserve more recognizable features towards specific classes, leading models
for better recognition under noises. For the transferability across multiple
models, we guide the defensive patches to capture more global feature
correlations within a class, so that they could activate model-shared global
perceptions and transfer better among models. Our defensive patches show great
potentials to improve application robustness in practice by simply sticking
them around target objects. Extensive experiments show that we outperform
others by large margins (improve 20+\% accuracy for both adversarial and
corruption robustness on average in the digital and physical world). Our codes
are available at https://github.com/nlsde-safety-team/DefensivePatch
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-based Deep Learning Architecture with Optimal Integration Layer for Image Parsing. (arXiv:2204.06214v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06214">
<div class="article-summary-box-inner">
<span><p>Deep learning models have been efficient lately on image parsing tasks.
However, deep learning models are not fully capable of exploiting visual and
contextual information simultaneously. The proposed three-layer context-based
deep architecture is capable of integrating context explicitly with visual
information. The novel idea here is to have a visual layer to learn visual
characteristics from binary class-based learners, a contextual layer to learn
context, and then an integration layer to learn from both via genetic
algorithm-based optimal fusion to produce a final decision. The experimental
outcomes when evaluated on benchmark datasets are promising. Further analysis
shows that optimized network weights can improve performance and make stable
predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization. (arXiv:2204.06228v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06228">
<div class="article-summary-box-inner">
<span><p>Due to its high societal impact, deepfake detection is getting active
attention in the computer vision community. Most deepfake detection methods
rely on identity, facial attribute and adversarial perturbation based
spatio-temporal modifications at the whole video or random locations, while
keeping the meaning of the content intact. However, a sophisticated deepfake
may contain only a small segment of video/audio manipulation, through which the
meaning of the content can be, for example, completely inverted from sentiment
perspective. To address this gap, we introduce a content driven audio-visual
deepfake dataset, termed as Localized Audio Visual DeepFake (LAV-DF),
explicitly designed for the task of learning temporal forgery localization.
Specifically, the content driven audio-visual manipulations are performed at
strategic locations in order to change the sentiment polarity of the whole
video. Our baseline method for benchmarking the proposed dataset is a 3DCNN
model, termed as Boundary Aware Temporal Forgery Detection (BA-TFD), which is
guided via contrastive, boundary matching and frame classification loss
functions. Our extensive quantitative analysis demonstrates the strong
performance of the proposed method for both task of temporal forgery
localization and deepfake detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rapid model transfer for medical image segmentation via iterative human-in-the-loop update: from labelled public to unlabelled clinical datasets for multi-organ segmentation in CT. (arXiv:2204.06243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06243">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable success on medical image analysis with deep learning,
it is still under exploration regarding how to rapidly transfer AI models from
one dataset to another for clinical applications. This paper presents a novel
and generic human-in-the-loop scheme for efficiently transferring a
segmentation model from a small-scale labelled dataset to a larger-scale
unlabelled dataset for multi-organ segmentation in CT. To achieve this, we
propose to use an igniter network which can learn from a small-scale labelled
dataset and generate coarse annotations to start the process of human-machine
interaction. Then, we use a sustainer network for our larger-scale dataset, and
iteratively updated it on the new annotated data. Moreover, we propose a
flexible labelling strategy for the annotator to reduce the initial annotation
workload. The model performance and the time cost of annotation in each subject
evaluated on our private dataset are reported and analysed. The results show
that our scheme can not only improve the performance by 19.7% on Dice, but also
expedite the cost time of manual labelling from 13.87 min to 1.51 min per CT
volume during the model transfer, demonstrating the clinical usefulness with
promising potentials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Matters in Language Conditioned Robotic Imitation Learning. (arXiv:2204.06252v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06252">
<div class="article-summary-box-inner">
<span><p>A long-standing goal in robotics is to build robots that can perform a wide
range of daily tasks from perceptions obtained with their onboard sensors and
specified only via natural language. While recently substantial advances have
been achieved in language-driven robotics by leveraging end-to-end learning
from pixels, there is no clear and well-understood process for making various
design choices due to the underlying variation in setups. In this paper, we
conduct an extensive study of the most critical challenges in learning language
conditioned policies from offline free-form imitation datasets. We further
identify architectural and algorithmic techniques that improve performance,
such as a hierarchical decomposition of the robot control learning, a
multimodal transformer encoder, discrete latent plans and a self-supervised
contrastive loss that aligns video and language representations. By combining
the results of our investigation with our improved model components, we are
able to present a novel approach that significantly outperforms the state of
the art on the challenging language conditioned long-horizon robot manipulation
CALVIN benchmark. We have open-sourced our implementation to facilitate future
research in learning to perform many complex manipulation skills in a row
specified with natural language. Codebase and trained models available at
<a href="http://hulc.cs.uni-freiburg.de">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection. (arXiv:2204.06272v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06272">
<div class="article-summary-box-inner">
<span><p>3D visual grounding aims to locate the referred target object in 3D point
cloud scenes according to a free-form language description. Previous methods
mostly follow a two-stage paradigm, i.e., language-irrelevant detection and
cross-modal matching, which is limited by the isolated architecture. In such a
paradigm, the detector needs to sample keypoints from raw point clouds due to
the inherent properties of 3D point clouds (irregular and large-scale), to
generate the corresponding object proposal for each keypoint. However, sparse
proposals may leave out the target in detection, while dense proposals may
confuse the matching model. Moreover, the language-irrelevant detection stage
can only sample a small proportion of keypoints on the target, deteriorating
the target prediction. In this paper, we propose a 3D Single-Stage Referred
Point Progressive Selection (3D-SPS) method, which progressively selects
keypoints with the guidance of language and directly locates the target.
Specifically, we propose a Description-aware Keypoint Sampling (DKS) module to
coarsely focus on the points of language-relevant objects, which are
significant clues for grounding. Besides, we devise a Target-oriented
Progressive Mining (TPM) module to finely concentrate on the points of the
target, which is enabled by progressive intra-modal relation modeling and
inter-modal target mining. 3D-SPS bridges the gap between detection and
matching in the 3D visual grounding task, localizing the target at a single
stage. Experiments demonstrate that 3D-SPS achieves state-of-the-art
performance on both ScanRefer and Nr3D/Sr3D datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing cloudiness in nonwovens. (arXiv:2204.06275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06275">
<div class="article-summary-box-inner">
<span><p>The homogeneity of filter media is important for material selection and
quality control, along with the specific weight (nominal grammage) and the
distribution of the local weight. Cloudiness or formation is a concept used to
describe deviations from homogeneity in filter media. We suggest to derive the
cloudiness index from the power spectrum of the relative local areal weight,
integrated over a selected frequency range. The power spectrum captures the
energy density in a broad spectral range. Moreover, under certain conditions,
the structure of a nonwoven is fully characterized by the areal weight, the
variance of the local areal weight, and the power spectrum. Consequently, the
power spectrum is the parameter that exclusively reflects the cloudiness. Here,
we address questions arising from practical application. The most prominent is
the choice of the spectral band. It certainly depends on the characteristic
"size of the clouds", but is limited by the size and lateral resolution of the
images. We show that the cloudiness index based on the power spectrum of the
relative local areal weight is theoretically well founded and can be robustly
measured from image data. Choosing the spectral band allows to capture the
cloudiness either visually perceived or found to be decisive for product
properties. It is thus well suited to build a technical standard on it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reuse your features: unifying retrieval and feature-metric alignment. (arXiv:2204.06292v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06292">
<div class="article-summary-box-inner">
<span><p>We propose a compact pipeline to unify all the steps of Visual Localization:
image retrieval, candidate re-ranking and initial pose estimation, and camera
pose refinement. Our key assumption is that the deep features used for these
individual tasks share common characteristics, so we should reuse them in all
the procedures of the pipeline. Our DRAN (Deep Retrieval and image Alignment
Network) is able to extract global descriptors for efficient image retrieval,
use intermediate hierarchical features to re-rank the retrieval list and
produce an intial pose guess, which is finally refined by means of a
feature-metric optimization based on learned deep multi-scale dense features.
DRAN is the first single network able to produce the features for the three
steps of visual localization. DRAN achieves a competitive performance in terms
of robustness and accuracy specially in extreme day-night changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Diffusion and VCA-Assisted Image Segmentation of Hyperspectral Images. (arXiv:2204.06298v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06298">
<div class="article-summary-box-inner">
<span><p>Hyperspectral images encode rich structure that can be exploited for material
discrimination by machine learning algorithms. This article introduces the
Active Diffusion and VCA-Assisted Image Segmentation (ADVIS) for active
material discrimination. ADVIS selects high-purity, high-density pixels that
are far in diffusion distance (a data-dependent metric) from other high-purity,
high-density pixels in the hyperspectral image. The ground truth labels of
these pixels are queried and propagated to the rest of the image. The ADVIS
active learning algorithm is shown to strongly outperform its fully
unsupervised clustering algorithm counterpart, suggesting that the
incorporation of a very small number of carefully-selected ground truth labels
can result in substantially superior material discrimination in hyperspectral
images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIB-VA at SemEval-2022 Task 5: A Multimodal Architecture for the Detection and Classification of Misogynous Memes. (arXiv:2204.06299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06299">
<div class="article-summary-box-inner">
<span><p>The detection of offensive, hateful content on social media is a challenging
problem that affects many online users on a daily basis. Hateful content is
often used to target a group of people based on ethnicity, gender, religion and
other factors. The hate or contempt toward women has been increasing on social
platforms. Misogynous content detection is especially challenging when textual
and visual modalities are combined to form a single context, e.g., an overlay
text embedded on top of an image, also known as meme. In this paper, we present
a multimodal architecture that combines textual and visual features in order to
detect misogynous meme content. The proposed architecture is evaluated in the
SemEval-2022 Task 5: MAMI - Multimedia Automatic Misogyny Identification
challenge under the team name TIB-VA. Our solution obtained the best result in
the Task-B where the challenge is to classify whether a given document is
misogynous and further identify the main sub-classes of shaming, stereotype,
objectification, and violence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Consistent Generative Adversarial Networks for 3D-aware Image Synthesis. (arXiv:2204.06307v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06307">
<div class="article-summary-box-inner">
<span><p>3D-aware image synthesis aims to generate images of objects from multiple
views by learning a 3D representation. However, one key challenge remains:
existing approaches lack geometry constraints, hence usually fail to generate
multi-view consistent images. To address this challenge, we propose Multi-View
Consistent Generative Adversarial Networks (MVCGAN) for high-quality 3D-aware
image synthesis with geometry constraints. By leveraging the underlying 3D
geometry information of generated images, i.e., depth and camera transformation
matrix, we explicitly establish stereo correspondence between views to perform
multi-view joint optimization. In particular, we enforce the photometric
consistency between pairs of views and integrate a stereo mixup mechanism into
the training process, encouraging the model to reason about the correct 3D
shape. Besides, we design a two-stage training strategy with feature-level
multi-view joint optimization to improve the image quality. Extensive
experiments on three datasets demonstrate that MVCGAN achieves the
state-of-the-art performance for 3D-aware image synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-based Framework for Automatic Cranial Defect Reconstruction and Implant Modeling. (arXiv:2204.06310v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06310">
<div class="article-summary-box-inner">
<span><p>The goal of this work is to propose a robust, fast, and fully automatic
method for personalized cranial defect reconstruction and implant modeling.
</p>
<p>We propose a two-step deep learning-based method using a modified U-Net
architecture to perform the defect reconstruction, and a dedicated iterative
procedure to improve the implant geometry, followed by automatic generation of
models ready for 3-D printing. We propose a cross-case augmentation based on
imperfect image registration combining cases from different datasets. We
perform ablation studies regarding different augmentation strategies and
compare them to other state-of-the-art methods.
</p>
<p>We evaluate the method on three datasets introduced during the AutoImplant
2021 challenge, organized jointly with the MICCAI conference. We perform the
quantitative evaluation using the Dice and boundary Dice coefficients, and the
Hausdorff distance. The average Dice coefficient, boundary Dice coefficient,
and the 95th percentile of Hausdorff distance are 0.91, 0.94, and 1.53 mm
respectively. We perform an additional qualitative evaluation by 3-D printing
and visualization in mixed reality to confirm the implant's usefulness.
</p>
<p>We propose a complete pipeline that enables one to create the cranial implant
model ready for 3-D printing. The described method is a greatly extended
version of the method that scored 1st place in all AutoImplant 2021 challenge
tasks. We freely release the source code, that together with the open datasets,
makes the results fully reproducible. The automatic reconstruction of cranial
defects may enable manufacturing personalized implants in a significantly
shorter time, possibly allowing one to perform the 3-D printing process
directly during a given intervention. Moreover, we show the usability of the
defect reconstruction in mixed reality that may further reduce the surgery
time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognition of Freely Selected Keypoints on Human Limbs. (arXiv:2204.06326v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06326">
<div class="article-summary-box-inner">
<span><p>Nearly all Human Pose Estimation (HPE) datasets consist of a fixed set of
keypoints. Standard HPE models trained on such datasets can only detect these
keypoints. If more points are desired, they have to be manually annotated and
the model needs to be retrained. Our approach leverages the Vision Transformer
architecture to extend the capability of the model to detect arbitrary
keypoints on the limbs of persons. We propose two different approaches to
encode the desired keypoints. (1) Each keypoint is defined by its position
along the line between the two enclosing keypoints from the fixed set and its
relative distance between this line and the edge of the limb. (2) Keypoints are
defined as coordinates on a norm pose. Both approaches are based on the
TokenPose architecture, while the keypoint tokens that correspond to the fixed
keypoints are replaced with our novel module. Experiments show that our
approaches achieve similar results to TokenPose on the fixed keypoints and are
capable of detecting arbitrary keypoints on the limbs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transparent Shape from Single Polarization Images. (arXiv:2204.06331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06331">
<div class="article-summary-box-inner">
<span><p>This paper presents a data-driven approach for transparent shape from
polarization. Due to the inherent high transmittance, the previous shape from
polarization(SfP) methods based on specular reflection model have difficulty in
estimating transparent shape, and the lack of datasets for transparent SfP also
limits the application of the data-driven approach. Hence, we construct the
transparent SfP dataset which consists of both synthetic and real-world
datasets. To determine the reliability of the physics-based reflection model,
we define the physics-based prior confidence by exploiting the inherent fault
of polarization information, then we propose a multi-branch fusion network to
embed the confidence. Experimental results show that our approach outperforms
other SfP methods. Compared with the previous method, the mean and median
angular error of our approach are reduced from $19.00^\circ$ and $14.91^\circ$
to $16.72^\circ$ and $13.36^\circ$, and the accuracy $11.25^\circ, 22.5^\circ,
30^\circ$ are improved from $38.36\%, 77.36\%, 87.48\%$ to $45.51\%, 78.86\%,
89.98\%$, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Bias in Facial Analysis Systems by Incorporating Label Diversity. (arXiv:2204.06364v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06364">
<div class="article-summary-box-inner">
<span><p>Facial analysis models are increasingly applied in real-world applications
that have significant impact on peoples' lives. However, as previously shown,
models that automatically classify facial attributes might exhibit algorithmic
discrimination behavior with respect to protected groups, potentially posing
negative impacts on individuals and society. It is therefore critical to
develop techniques that can mitigate unintended biases in facial classifiers.
Hence, in this work, we introduce a novel learning method that combines both
subjective human-based labels and objective annotations based on mathematical
definitions of facial traits. Specifically, we generate new objective
annotations from a large-scale human-annotated dataset, each capturing a
different perspective of the analyzed facial trait. We then propose an ensemble
learning method, which combines individual models trained on different types of
annotations. We provide an in-depth analysis of the annotation procedure as
well as the dataset distribution. Moreover, we empirically demonstrate that, by
incorporating label diversity, and without additional synthetic images, our
method successfully mitigates unintended biases, while maintaining significant
accuracy on the downstream task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning based automatic detection of offshore oil slicks using SAR data and contextual information. (arXiv:2204.06371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06371">
<div class="article-summary-box-inner">
<span><p>Ocean surface monitoring, especially oil slick detection, has become
mandatory due to its importance for oil exploration and risk prevention on
ecosystems. For years, the detection task has been performed manually by
photo-interpreters using Synthetic Aperture Radar (SAR) images with the help of
contextual data such as wind. This tedious manual work cannot handle the
increasing amount of data collected by the available sensors and thus requires
automation. Literature reports conventional and semi-automated detection
methods that generally focus either on oil slicks originating from
anthropogenic (spills) or natural (seeps) sources on limited data collections.
As an extension, this paper presents the automation of offshore oil slicks on
an extensive database with both kinds of slicks. It builds upon the slick
annotations of specialized photo-interpreters on Sentinel-1 SAR data for 4
years over 3 exploration and monitoring areas worldwide. All the considered SAR
images and related annotation relate to real oil slick monitoring scenarios.
Further, wind estimation is systematically computed to enrich the data
collection. Paper contributions are the following : (i) a performance
comparison of two deep learning approaches: semantic segmentation using
FC-DenseNet and instance segmentation using Mask-RCNN. (ii) the introduction of
meteorological information (wind speed) is deemed valuable for oil slick
detection in the performance evaluation. The main results of this study show
the effectiveness of slick detection by deep learning approaches, in particular
FC-DenseNet, which captures more than 92% of oil instances in our test set.
Furthermore, a strong correlation between model performances and contextual
information such as slick size and wind speed is demonstrated in the
performance evaluation. This work opens perspectives to design models that can
fuse SAR and wind information to reduce the false alarm rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Receding Neuron Importances for Structured Pruning. (arXiv:2204.06404v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06404">
<div class="article-summary-box-inner">
<span><p>Structured pruning efficiently compresses networks by identifying and
removing unimportant neurons. While this can be elegantly achieved by applying
sparsity-inducing regularisation on BatchNorm parameters, an L1 penalty would
shrink all scaling factors rather than just those of superfluous neurons. To
tackle this issue, we introduce a simple BatchNorm variation with bounded
scaling parameters, based on which we design a novel regularisation term that
suppresses only neurons with low importance. Under our method, the weights of
unnecessary neurons effectively recede, producing a polarised bimodal
distribution of importances. We show that neural networks trained this way can
be pruned to a larger extent and with less deterioration. We one-shot prune VGG
and ResNet architectures at different ratios on CIFAR and ImagenNet datasets.
In the case of VGG-style networks, our method significantly outperforms
existing approaches particularly under a severe pruning regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DMCNet: Diversified Model Combination Network for Understanding Engagement from Video Screengrabs. (arXiv:2204.06454v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06454">
<div class="article-summary-box-inner">
<span><p>Engagement is an essential indicator of the Quality-of-Learning Experience
(QoLE) and plays a major role in developing intelligent educational interfaces.
The number of people learning through Massively Open Online Courses (MOOCs) and
other online resources has been increasing rapidly because they provide us with
the flexibility to learn from anywhere at any time. This provides a good
learning experience for the students. However, such learning interface requires
the ability to recognize the level of engagement of the students for a holistic
learning experience. This is useful for both students and educators alike.
However, understanding engagement is a challenging task, because of its
subjectivity and ability to collect data. In this paper, we propose a variety
of models that have been trained on an open-source dataset of video
screengrabs. Our non-deep learning models are based on the combination of
popular algorithms such as Histogram of Oriented Gradient (HOG), Support Vector
Machine (SVM), Scale Invariant Feature Transform (SIFT) and Speeded Up Robust
Features (SURF). The deep learning methods include Densely Connected
Convolutional Networks (DenseNet-121), Residual Network (ResNet-18) and
MobileNetV1. We show the performance of each models using a variety of metrics
such as the Gini Index, Adjusted F-Measure (AGF), and Area Under receiver
operating characteristic Curve (AUC). We use various dimensionality reduction
techniques such as Principal Component Analysis (PCA) and t-Distributed
Stochastic Neighbor Embedding (t-SNE) to understand the distribution of data in
the feature sub-space. Our work will thereby assist the educators and students
in obtaining a fruitful and efficient online learning experience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WSSS4LUAD: Grand Challenge on Weakly-supervised Tissue Semantic Segmentation for Lung Adenocarcinoma. (arXiv:2204.06455v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06455">
<div class="article-summary-box-inner">
<span><p>Lung cancer is the leading cause of cancer death worldwide, and
adenocarcinoma (LUAD) is the most common subtype. Exploiting the potential
value of the histopathology images can promote precision medicine in oncology.
Tissue segmentation is the basic upstream task of histopathology image
analysis. Existing deep learning models have achieved superior segmentation
performance but require sufficient pixel-level annotations, which is
time-consuming and expensive. To enrich the label resources of LUAD and to
alleviate the annotation efforts, we organize this challenge WSSS4LUAD to call
for the outstanding weakly-supervised semantic segmentation techniques for
histopathology images of LUAD. Participants have to design the algorithm to
segment tumor epithelial, tumor-associated stroma and normal tissue with only
patch-level labels. This challenge includes 10,091 patch-level annotations (the
training set) and over 130 million labeled pixels (the validation and test
sets), from 67 WSIs (47 from GDPH, 20 from TCGA). All the labels were generated
by a pathologist-in-the-loop pipeline with the help of AI models and checked by
the label review board. Among 532 registrations, 28 teams submitted the results
in the test phase with over 1,000 submissions. Finally, the first place team
achieved mIoU of 0.8413 (tumor: 0.8389, stroma: 0.7931, normal: 0.8919).
According to the technical reports of the top-tier teams, CAM is still the most
popular approach in WSSS. Cutmix data augmentation has been widely adopted to
generate more reliable samples. With the success of this challenge, we believe
that WSSS approaches with patch-level annotations can replace the traditional
pixel annotations while reducing the annotation efforts. The entire dataset has
been released to encourage more researches on computational pathology in LUAD
and more novel WSSS techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpoofGAN: Synthetic Fingerprint Spoof Images. (arXiv:2204.06498v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06498">
<div class="article-summary-box-inner">
<span><p>A major limitation to advances in fingerprint spoof detection is the lack of
publicly available, large-scale fingerprint spoof datasets, a problem which has
been compounded by increased concerns surrounding privacy and security of
biometric data. Furthermore, most state-of-the-art spoof detection algorithms
rely on deep networks which perform best in the presence of a large amount of
training data. This work aims to demonstrate the utility of synthetic (both
live and spoof) fingerprints in supplying these algorithms with sufficient data
to improve the performance of fingerprint spoof detection algorithms beyond the
capabilities when training on a limited amount of publicly available real
datasets. First, we provide details of our approach in modifying a
state-of-the-art generative architecture to synthesize high quality live and
spoof fingerprints. Then, we provide quantitative and qualitative analysis to
verify the quality of our synthetic fingerprints in mimicking the distribution
of real data samples. We showcase the utility of our synthetic live and spoof
fingerprints in training a deep network for fingerprint spoof detection, which
dramatically boosts the performance across three different evaluation datasets
compared to an identical model trained on real data alone. Finally, we
demonstrate that only 25% of the original (real) dataset is required to obtain
similar detection performance when augmenting the training dataset with
synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization. (arXiv:2204.06504v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06504">
<div class="article-summary-box-inner">
<span><p>Since 2016, we have witnessed the tremendous growth of artificial
intelligence+visualization (AI+VIS) research. However, existing survey papers
on AI+VIS focus on visual analytics and information visualization, not
scientific visualization (SciVis). In this paper, we survey related deep
learning (DL) works in SciVis, specifically in the direction of DL4SciVis:
designing DL solutions for solving SciVis problems. To stay focused, we
primarily consider works that handle scalar and vector field data but exclude
mesh data. We classify and discuss these works along six dimensions: domain
setting, research task, learning type, network architecture, loss function, and
evaluation metric. The paper concludes with a discussion of the remaining gaps
to fill along the discussed dimensions and the grand challenges we need to
tackle as a community. This state-of-the-art survey guides SciVis researchers
in gaining an overview of this emerging topic and points out future directions
to grow this research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out-of-distribution Detection with Deep Nearest Neighbors. (arXiv:2204.06507v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06507">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection is a critical task for deploying machine
learning models in the open world. Distance-based methods have demonstrated
promise, where testing samples are detected as OOD if they are relatively far
away from in-distribution (ID) data. However, prior methods impose a strong
distributional assumption of the underlying feature space, which may not always
hold. In this paper, we explore the efficacy of non-parametric nearest-neighbor
distance for OOD detection, which has been largely overlooked in the
literature. Unlike prior works, our method does not impose any distributional
assumption, hence providing stronger flexibility and generality. We demonstrate
the effectiveness of nearest-neighbor-based OOD detection on several benchmarks
and establish superior performance. Under the same model trained on
ImageNet-1k, our method substantially reduces the false positive rate
(FPR@TPR95) by 24.77% compared to a strong baseline SSD+, which uses a
parametric approach Mahalanobis distance in detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does depth estimation help object detection?. (arXiv:2204.06512v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06512">
<div class="article-summary-box-inner">
<span><p>Ground-truth depth, when combined with color data, helps improve object
detection accuracy over baseline models that only use color. However, estimated
depth does not always yield improvements. Many factors affect the performance
of object detection when estimated depth is used. In this paper, we
comprehensively investigate these factors with detailed experiments, such as
using ground-truth vs. estimated depth, effects of different state-of-the-art
depth estimation networks, effects of using different indoor and outdoor RGB-D
datasets as training data for depth estimation, and different architectural
choices for integrating depth to the base object detector network. We propose
an early concatenation strategy of depth, which yields higher mAP than previous
works' while using significantly fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A9-Dataset: Multi-Sensor Infrastructure-Based Dataset for Mobility Research. (arXiv:2204.06527v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06527">
<div class="article-summary-box-inner">
<span><p>Data-intensive machine learning based techniques increasingly play a
prominent role in the development of future mobility solutions - from driver
assistance and automation functions in vehicles, to real-time traffic
management systems realized through dedicated infrastructure. The availability
of high quality real-world data is often an important prerequisite for the
development and reliable deployment of such systems in large scale. Towards
this endeavour, we present the A9-Dataset based on roadside sensor
infrastructure from the 3 km long Providentia++ test field near Munich in
Germany. The dataset includes anonymized and precision-timestamped multi-modal
sensor and object data in high resolution, covering a variety of traffic
situations. As part of the first set of data, which we describe in this paper,
we provide camera and LiDAR frames from two overhead gantry bridges on the A9
autobahn with the corresponding objects labeled with 3D bounding boxes. The
first set includes in total more than 1000 sensor frames and 14000 traffic
objects. The dataset is available for download at https://a9-dataset.com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Vector Fields for Surface Representation and Inference. (arXiv:2204.06552v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06552">
<div class="article-summary-box-inner">
<span><p>Neural implicit fields have recently been shown to represent 3D shapes
accurately, opening up various applications in 3D shape analysis. Up to now,
such implicit fields for 3D representation are scalar, encoding the signed
distance or binary volume occupancy and more recently the unsigned distance.
However, the first two can only represent closed shapes, while the unsigned
distance has difficulties in accurate and fast shape inference. In this paper,
we propose a Neural Vector Field for shape representation in order to overcome
the two aforementioned problems. Mapping each point in space to the direction
towards the closest surface, we can represent any type of shape. Similarly the
shape mesh can be reconstructed by applying the marching cubes algorithm, with
proposed small changes, on top of the inferred vector field. We compare the
method on ShapeNet where the proposed new neural implicit field shows superior
accuracy in representing both closed and open shapes outperforming previous
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Video Generation through Global and Local Motion Dynamics. (arXiv:2204.06558v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06558">
<div class="article-summary-box-inner">
<span><p>We present GLASS, a method for Global and Local Action-driven Sequence
Synthesis. GLASS is a generative model that is trained on video sequences in an
unsupervised manner and that can animate an input image at test time. The
method learns to segment frames into foreground-background layers and to
generate transitions of the foregrounds over time through a global and local
action representation. Global actions are explicitly related to 2D shifts,
while local actions are instead related to (both geometric and photometric)
local deformations. GLASS uses a recurrent neural network to transition between
frames and is trained through a reconstruction loss. We also introduce
W-Sprites (Walking Sprites), a novel synthetic dataset with a predefined action
space. We evaluate our method on both W-Sprites and real datasets, and find
that GLASS is able to generate realistic video sequences from a single input
image and to successfully learn a more advanced action space than in prior
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CaCL: Class-aware Codebook Learning for Weakly Supervised Segmentation on Diffuse Image Patterns. (arXiv:2011.00794v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00794">
<div class="article-summary-box-inner">
<span><p>Weakly supervised learning has been rapidly advanced in biomedical image
analysis to achieve pixel-wise labels (segmentation) from image-wise
annotations (classification), as biomedical images naturally contain image-wise
labels in many scenarios. The current weakly supervised learning algorithms
from the computer vision community are largely designed for focal objects
(e.g., dogs and cats). However, such algorithms are not optimized for diffuse
patterns in biomedical imaging (e.g., stains and fluorescence in microscopy
imaging). In this paper, we propose a novel class-aware codebook learning
(CaCL) algorithm to perform weakly supervised learning for diffuse image
patterns. Specifically, the CaCL algorithm is deployed to segment protein
expressed brush border regions from histological images of human duodenum. Our
contribution is three-fold: (1) we approach the weakly supervised segmentation
from a novel codebook learning perspective; (2) the CaCL algorithm segments
diffuse image patterns rather than focal objects; and (3) the proposed
algorithm is implemented in a multi-task framework based on Vector
Quantised-Variational AutoEncoder (VQ-VAE) via joint image reconstruction,
classification, feature embedding, and segmentation. The experimental results
show that our method achieved superior performance compared with baseline
weakly supervised algorithms. The code is available at
https://github.com/ddrrnn123/CaCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Outliers with Foreign Patch Interpolation. (arXiv:2011.04197v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.04197">
<div class="article-summary-box-inner">
<span><p>In medical imaging, outliers can contain hypo/hyper-intensities, minor
deformations, or completely altered anatomy. To detect these irregularities it
is helpful to learn the features present in both normal and abnormal images.
However this is difficult because of the wide range of possible abnormalities
and also the number of ways that normal anatomy can vary naturally. As such, we
leverage the natural variations in normal anatomy to create a range of
synthetic abnormalities. Specifically, the same patch region is extracted from
two independent samples and replaced with an interpolation between both
patches. The interpolation factor, patch size, and patch location are randomly
sampled from uniform distributions. A wide residual encoder decoder is trained
to give a pixel-wise prediction of the patch and its interpolation factor. This
encourages the network to learn what features to expect normally and to
identify where foreign patterns have been introduced. The estimate of the
interpolation factor lends itself nicely to the derivation of an outlier score.
Meanwhile the pixel-wise output allows for pixel- and subject- level
predictions using the same model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FPCC: Fast Point Cloud Clustering based Instance Segmentation for Industrial Bin-picking. (arXiv:2012.14618v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14618">
<div class="article-summary-box-inner">
<span><p>Instance segmentation is an important pre-processing task in numerous
real-world applications, such as robotics, autonomous vehicles, and
human-computer interaction. Compared with the rapid development of deep
learning for two-dimensional (2D) image tasks, deep learning-based instance
segmentation of 3D point cloud still has a lot of room for development. In
particular, distinguishing a large number of occluded objects of the same class
is a highly challenging problem, which is seen in a robotic bin-picking. In a
usual bin-picking scene, many identical objects are stacked together and the
model of the objects is known. Thus, the semantic information can be ignored;
instead, the focus in the bin-picking is put on the segmentation of instances.
Based on this task requirement, we propose a Fast Point Cloud Clustering (FPCC)
for instance segmentation of bin-picking scene. FPCC includes a network named
FPCC-Net and a fast clustering algorithm. FPCC-net has two subnets, one for
inferring the geometric centers for clustering and the other for describing
features of each point. FPCC-Net extracts features of each point and infers
geometric center points of each instance simultaneously. After that, the
proposed clustering algorithm clusters the remaining points to the closest
geometric center in feature embedding space. Experiments show that FPCC also
surpasses the existing works in bin-picking scenes and is more computationally
efficient. Our code and data are available at https://github.com/xyjbaal/FPCC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient Object Detection via Integrity Learning. (arXiv:2101.07663v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07663">
<div class="article-summary-box-inner">
<span><p>Although current salient object detection (SOD) works have achieved
significant progress, they are limited when it comes to the integrity of the
predicted salient regions. We define the concept of integrity at both a micro
and macro level. Specifically, at the micro level, the model should highlight
all parts that belong to a certain salient object. Meanwhile, at the macro
level, the model needs to discover all salient objects in a given image. To
facilitate integrity learning for SOD, we design a novel Integrity Cognition
Network (ICON), which explores three important components for learning strong
integrity features. 1) Unlike existing models, which focus more on feature
discriminability, we introduce a diverse feature aggregation (DFA) component to
aggregate features with various receptive fields (i.e., kernel shape and
context) and increase feature diversity. Such diversity is the foundation for
mining the integral salient objects. 2) Based on the DFA features, we introduce
an integrity channel enhancement (ICE) component with the goal of enhancing
feature channels that highlight the integral salient objects, while suppressing
the other distracting ones. 3) After extracting the enhanced features, the
part-whole verification (PWV) method is employed to determine whether the part
and whole object features have strong agreement. Such part-whole agreements can
further improve the micro-level integrity for each salient object. To
demonstrate the effectiveness of our ICON, comprehensive experiments are
conducted on seven challenging benchmarks. Our ICON outperforms the baseline
methods in terms of a wide range of metrics. Notably, our ICON achieves about
10% relative improvement over the previous best model in terms of average false
negative ratio (FNR), on six datasets. Codes and results are available at:
https://github.com/mczhuge/ICON.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly But Deeply Supervised Occlusion-Reasoned Parametric Road Layouts. (arXiv:2104.06730v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06730">
<div class="article-summary-box-inner">
<span><p>We propose an end-to-end network that takes a single perspective RGB image of
a complex road scene as input, to produce occlusion-reasoned layouts in
perspective space as well as a parametric bird's-eye-view (BEV) space. In
contrast to prior works that require dense supervision such as semantic labels
in perspective view, our method only requires human annotations for parametric
attributes that are cheaper and less ambiguous to obtain. To solve this
challenging task, our design is comprised of modules that incorporate inductive
biases to learn occlusion-reasoning, geometric transformation and semantic
abstraction, where each module may be supervised by appropriately transforming
the parametric annotations. We demonstrate how our design choices and proposed
deep supervision help achieve meaningful representations and accurate
predictions. We validate our approach on two public datasets, KITTI and
NuScenes, to achieve state-of-the-art results with considerably less human
supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Transformer Based Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images. (arXiv:2104.12137v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12137">
<div class="article-summary-box-inner">
<span><p>The fully convolutional network (FCN) with an encoder-decoder architecture
has been the standard paradigm for semantic segmentation. The encoder-decoder
architecture utilizes an encoder to capture multilevel feature maps, which are
incorporated into the final prediction by a decoder. As the context is crucial
for precise segmentation, tremendous effort has been made to extract such
information in an intelligent fashion, including employing dilated/atrous
convolutions or inserting attention modules. However, these endeavors are all
based on the FCN architecture with ResNet or other backbones, which cannot
fully exploit the context from the theoretical concept. By contrast, we
introduce the Swin Transformer as the backbone to extract the context
information and design a novel decoder of densely connected feature aggregation
module (DCFAM) to restore the resolution and produce the segmentation map. The
experimental results on two remotely sensed semantic segmentation datasets
demonstrate the effectiveness of the proposed scheme.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content-Augmented Feature Pyramid Network with Light Linear Spatial Transformers for Object Detection. (arXiv:2105.09464v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09464">
<div class="article-summary-box-inner">
<span><p>As one of the prevalent components, Feature Pyramid Network (FPN) is widely
used in current object detection models for improving multi-scale object
detection performance. However, its feature fusion mode is still in a
misaligned and local manner, thus limiting the representation power. To address
the inherit defects of FPN, a novel architecture termed Content-Augmented
Feature Pyramid Network (CA-FPN) is proposed in this paper. Firstly, a Global
Content Extraction Module (GCEM) is proposed to extract multi-scale context
information. Secondly, lightweight linear spatial Transformer connections are
added in the top-down pathway to augment each feature map with multi-scale
features, where a linearized approximate self-attention function is designed
for reducing model complexity. By means of the self-attention mechanism in
Transformer, there is no longer need to align feature maps during feature
fusion, thus solving the misaligned defect. By setting the query scope to the
entire feature map, the local defect can also be solved. Extensive experiments
on COCO and PASCAL VOC datasets demonstrated that our CA-FPN outperforms other
FPN-based detectors without bells and whistles and is robust in different
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Synchronized Reprojection-based Model for 3D Human Pose Estimation. (arXiv:2106.04274v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04274">
<div class="article-summary-box-inner">
<span><p>3D human pose estimation is still a challenging problem despite the large
amount of work that has been done in this field. Generally, most methods
directly use neural networks and ignore certain constraints (e.g., reprojection
constraints and joint angle and bone length constraints). This paper proposes a
weakly supervised GAN-based model for 3D human pose estimation that considers
3D information along with 2D information simultaneously, in which a
reprojection network is employed to learn the mapping of the distribution from
3D poses to 2D poses. In particular, we train the reprojection network and the
generative adversarial network synchronously. Furthermore, inspired by the
typical kinematic chain space (KCS) matrix, we propose a weighted KCS matrix,
which is added into the discriminator's input to impose joint angle and bone
length constraints. The experimental results on Human3.6M show that our method
outperforms state-of-the-art methods by approximately 24.7\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SVMAC: Unsupervised 3D Human Pose Estimation from a Single Image with Single-view-multi-angle Consistency. (arXiv:2106.05616v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05616">
<div class="article-summary-box-inner">
<span><p>Recovering 3D human pose from 2D joints is still a challenging problem,
especially without any 3D annotation, video information, or multi-view
information. In this paper, we present an unsupervised GAN-based model
consisting of multiple weight-sharing generators to estimate a 3D human pose
from a single image without 3D annotations. In our model, we introduce
single-view-multi-angle consistency (SVMAC) to significantly improve the
estimation performance. With 2D joint locations as input, our model estimates a
3D pose and a camera simultaneously. During training, the estimated 3D pose is
rotated by random angles and the estimated camera projects the rotated 3D poses
back to 2D. The 2D reprojections will be fed into weight-sharing generators to
estimate the corresponding 3D poses and cameras, which are then mixed to impose
SVMAC constraints to self-supervise the training process. The experimental
results show that our method outperforms the state-of-the-art unsupervised
methods on Human 3.6M and MPI-INF-3DHP. Moreover, qualitative results on MPII
and LSP show that our method can generalize well to unknown data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts. (arXiv:2109.04379v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04379">
<div class="article-summary-box-inner">
<span><p>Preserving maximal information is one of principles of designing
self-supervised learning methodologies. To reach this goal, contrastive
learning adopts an implicit way which is contrasting image pairs. However, we
believe it is not fully optimal to simply use the contrastive estimation for
preservation. Moreover, it is necessary and complemental to introduce an
explicit solution to preserve more information. From this perspective, we
introduce Preservational Learning to reconstruct diverse image contexts in
order to preserve more information in learned representations. Together with
the contrastive loss, we present Preservational Contrastive Representation
Learning (PCRL) for learning self-supervised medical representations. PCRL
provides very competitive results under the pretraining-finetuning protocol,
outperforming both self-supervised and supervised counterparts in 5
classification/segmentation tasks substantially.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNetFormer: An UNet-like Transformer for Efficient Semantic Segmentation of Remote Sensing Urban Scene Imagery. (arXiv:2109.08937v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08937">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of remotely sensed urban scene images is required in a
wide range of practical applications, such as land cover mapping, urban change
detection, environmental protection, and economic assessment. Driven by rapid
developments in deep learning technologies, the convolutional neural network
(CNN) has dominated semantic segmentation for many years. CNN adopts
hierarchical feature representation, demonstrating strong capabilities for
local information extraction. However, the local property of the convolution
layer limits the network from capturing global context. Recently, as a hot
topic in the domain of computer vision, Transformer has demonstrated its great
potential in global information modelling, boosting many vision-related tasks
such as image classification, object detection, and particularly semantic
segmentation. In this paper, we propose an UNet-like Transformer (UNetFormer)
for real-time urban scene segmentation. The novel UNetFormer adopts a hybrid
structure with a CNN-based encoder and a Transformer-based decoder, learning
global-local context with high computational efficiency. Extensive experiments
reveal that the proposed UNetFormer not only runs faster during the inference
stage but also produces higher accuracy compared with state-of-the-art
lightweight models. Specifically, the proposed UNetFormer achieved a 67.8% mIoU
on the UAVid test set and a 52.4% mIoU on the LoveDA dataset, while the
inference speed can achieve up to 322.4 FPS speed with the input in the shape
of 512x512 on an NVIDIA GTX 3090 GPU. The source code will be freely available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PC$^2$-PU: Patch Correlation and Point Correlation for Effective Point Cloud Upsampling. (arXiv:2109.09337v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09337">
<div class="article-summary-box-inner">
<span><p>Point cloud upsampling is to densify a sparse point set acquired from 3D
sensors, providing a denser representation for the underlying surface. Existing
methods divide the input points into small patches and upsample each patch
separately, however, ignoring the global spatial consistency between patches.
In this paper, we present a novel method PC$^2$-PU, which explores
patch-to-patch and point-to-point correlations for more effective and robust
point cloud upsampling. Specifically, our network has two appealing designs:
(i) We take adjacent patches as supplementary inputs to compensate the loss
structure information within a single patch and introduce a Patch Correlation
Module to capture the difference and similarity between patches. (ii) After
augmenting each patch's geometry, we further introduce a Point Correlation
Module to reveal the relationship of points inside each patch to maintain the
local spatial consistency. Extensive experiments on both synthetic and real
scanned datasets demonstrate that our method surpasses previous upsampling
methods, particularly with the noisy inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomy and Perception for Space Mining. (arXiv:2109.12109v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12109">
<div class="article-summary-box-inner">
<span><p>Future Moon bases will likely be constructed using resources mined from the
surface of the Moon. The difficulty of maintaining a human workforce on the
Moon and communications lag with Earth means that mining will need to be
conducted using collaborative robots with a high degree of autonomy. In this
paper, we describe our solution for Phase 2 of the NASA Space Robotics
Challenge, which provided a simulated lunar environment in which teams were
tasked to develop software systems to achieve autonomous collaborative robots
for mining on the Moon. Our 3rd place and innovation award winning solution
shows how machine learning-enabled vision could alleviate major challenges
posed by the lunar environment towards autonomous space mining, chiefly the
lack of satellite positioning systems, hazardous terrain, and delicate robot
interactions. A robust multi-robot coordinator was also developed to achieve
long-term operation and effective collaboration between robots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays. (arXiv:2109.14187v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14187">
<div class="article-summary-box-inner">
<span><p>Deep learning has shown recent success in classifying anomalies in chest
x-rays, but datasets are still small compared to natural image datasets.
Supervision of abnormality localization has been shown to improve trained
models, partially compensating for dataset sizes. However, explicitly labeling
these anomalies requires an expert and is very time-consuming. We propose a
potentially scalable method for collecting implicit localization data using an
eye tracker to capture gaze locations and a microphone to capture a dictation
of a report, imitating the setup of a reading room. The resulting REFLACX
(Reports and Eye-Tracking Data for Localization of Abnormalities in Chest
X-rays) dataset was labeled across five radiologists and contains 3,032
synchronized sets of eye-tracking data and timestamped report transcriptions
for 2,616 chest x-rays from the MIMIC-CXR dataset. We also provide auxiliary
annotations, including bounding boxes around lungs and heart and validation
labels consisting of ellipses localizing abnormalities and image-level labels.
Furthermore, a small subset of the data contains readings from all
radiologists, allowing for the calculation of inter-rater scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptive Semantic Segmentation via Regional Contrastive Consistency Regularization. (arXiv:2110.05170v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05170">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) for semantic segmentation has been
well-studied in recent years. However, most existing works largely neglect the
local regional consistency across different domains and are less robust to
changes in outdoor environments. In this paper, we propose a novel and fully
end-to-end trainable approach, called regional contrastive consistency
regularization (RCCR) for domain adaptive semantic segmentation. Our core idea
is to pull the similar regional features extracted from the same location of
different images, i.e., the original image and augmented image, to be closer,
and meanwhile push the features from the different locations of the two images
to be separated. We innovatively propose a region-wise contrastive loss with
two sampling strategies to realize effective regional consistency. Besides, we
present momentum projection heads, where the teacher projection head is the
exponential moving average of the student. Finally, a memory bank mechanism is
designed to learn more robust and stable region-wise features under varying
environments. Extensive experiments on two common UDA benchmarks, i.e., GTAV to
Cityscapes and SYNTHIA to Cityscapes, demonstrate that our approach outperforms
the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Set Recognition: a Good Closed-Set Classifier is All You Need?. (arXiv:2110.06207v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06207">
<div class="article-summary-box-inner">
<span><p>The ability to identify whether or not a test sample belongs to one of the
semantic classes in a classifier's training set is critical to practical
deployment of the model. This task is termed open-set recognition (OSR) and has
received significant attention in recent years. In this paper, we first
demonstrate that the ability of a classifier to make the 'none-of-above'
decision is highly correlated with its accuracy on the closed-set classes. We
find that this relationship holds across loss objectives and architectures, and
further demonstrate the trend both on the standard OSR benchmarks as well as on
a large-scale ImageNet evaluation. Second, we use this correlation to boost the
performance of a maximum logit score OSR 'baseline' by improving its closed-set
accuracy, and with this strong baseline achieve state-of-the-art on a number of
OSR benchmarks. Similarly, we boost the performance of the existing
state-of-the-art method by improving its closed-set accuracy, but the resulting
discrepancy with the strong baseline is marginal. Our third contribution is to
present the 'Semantic Shift Benchmark' (SSB), which better respects the task of
detecting semantic novelty, in contrast to other forms of distribution shift
also considered in related sub-fields, such as out-of-distribution detection.
On this new evaluation, we again demonstrate that there is negligible
difference between the strong baseline and the existing state-of-the-art.
Project Page: https://www.robots.ox.ac.uk/~vgg/research/osr/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QU-net++: Image Quality Detection Framework for Segmentation of Medical 3D Image Stacks. (arXiv:2110.14181v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14181">
<div class="article-summary-box-inner">
<span><p>Automated segmentation of pathological regions of interest aids medical image
diagnostics and follow-up care. However, accurate pathological segmentations
require high quality of annotated data that can be both cost and time intensive
to generate. In this work, we propose an automated two-step method that detects
a minimal image subset required to train segmentation models by evaluating the
quality of medical images from 3D image stacks using a U-net++ model. These
images that represent a lack of quality training can then be annotated and used
to fully train a U-net-based segmentation model. The proposed QU-net++ model
detects this lack of quality training based on the disagreement in
segmentations produced from the final two output layers. The proposed model
isolates around 10% of the slices per 3D image stack and can scale across
imaging modalities to segment cysts in OCT images and ground glass opacity
(GGO) in lung CT images with Dice scores in the range 0.56-0.72. Thus, the
proposed method can be applied for cost effective multi-modal pathology
segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks. (arXiv:2111.05955v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05955">
<div class="article-summary-box-inner">
<span><p>Spiking neural networks (SNNs) have become an interesting alternative to
conventional artificial neural networks (ANN) thanks to their temporal
processing capabilities and energy efficient implementations in neuromorphic
hardware. However the challenges involved in training SNNs have limited their
performance in terms of accuracy and thus their applications. Improving
learning algorithms and neural architectures for a more accurate feature
extraction is therefore one of the current priorities in SNN research. In this
paper we present a study on the key components of modern spiking architectures.
We empirically compare different techniques in image classification datasets
taken from the best performing networks. We design a spiking version of the
successful residual network architecture and provide an in-depth study on the
possible implementations of spiking residual connections. Our results provide a
state of the art guide to SNN design, which allows to make informed choices
when trying to build the optimal visual feature extractor. Finally, our network
outperforms previous SNN architectures in CIFAR-10 (94.14%) and CIFAR-100
(74.65%) datasets and matches the state of the art in DVS-CIFAR10 (72.98%),
with less parameters than the previous state of the art and without the need
for ANN-SNN conversion. Code available at
https://github.com/VicenteAlex/Spiking_ResNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Related Work on Image Quality Assessment. (arXiv:2111.06291v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06291">
<div class="article-summary-box-inner">
<span><p>Due to the existence of quality degradations introduced in various stages of
visual signal acquisition, compression, transmission and display, image quality
assessment (IQA) plays a vital role in image-based applications. According to
whether the reference image is complete and available, image quality evaluation
can be divided into three categories: Full-Reference(FR), Reduced-
Reference(RR), and Non- Reference(NR). This article will review the
state-of-the-art image quality assessment algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Lightweight Graph Transformer Network for Human Mesh Reconstruction from 2D Human Pose. (arXiv:2111.12696v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12696">
<div class="article-summary-box-inner">
<span><p>Existing deep learning-based human mesh reconstruction approaches have a
tendency to build larger networks in order to achieve higher accuracy.
Computational complexity and model size are often neglected, despite being key
characteristics for practical use of human mesh reconstruction models (e.g.
virtual try-on systems). In this paper, we present GTRS, a lightweight
pose-based method that can reconstruct human mesh from 2D human pose. We
propose a pose analysis module that uses graph transformers to exploit
structured and implicit joint correlations, and a mesh regression module that
combines the extracted pose feature with the mesh template to reconstruct the
final human mesh. We demonstrate the efficiency and generalization of GTRS by
extensive evaluations on the Human3.6M and 3DPW datasets. In particular, GTRS
achieves better accuracy than the SOTA pose-based method Pose2Mesh while only
using 10.2% of the parameters (Params) and 2.5% of the FLOPs on the challenging
in-the-wild 3DPW dataset. Code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shunted Self-Attention via Multi-Scale Token Aggregation. (arXiv:2111.15193v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15193">
<div class="article-summary-box-inner">
<span><p>Recent Vision Transformer~(ViT) models have demonstrated encouraging results
across various computer vision tasks, thanks to their competence in modeling
long-range dependencies of image patches or tokens via self-attention. These
models, however, usually designate the similar receptive fields of each token
feature within each layer. Such a constraint inevitably limits the ability of
each self-attention layer in capturing multi-scale features, thereby leading to
performance degradation in handling images with multiple objects of different
scales. To address this issue, we propose a novel and generic strategy, termed
shunted self-attention~(SSA), that allows ViTs to model the attentions at
hybrid scales per attention layer. The key idea of SSA is to inject
heterogeneous receptive field sizes into tokens: before computing the
self-attention matrix, it selectively merges tokens to represent larger object
features while keeping certain tokens to preserve fine-grained features. This
novel merging scheme enables the self-attention to learn relationships between
objects with different sizes and simultaneously reduces the token numbers and
the computational cost. Extensive experiments across various tasks demonstrate
the superiority of SSA. Specifically, the SSA-based transformer achieves 84.0\%
Top-1 accuracy and outperforms the state-of-the-art Focal Transformer on
ImageNet with only half of the model size and computation cost, and surpasses
Focal Transformer by 1.3 mAP on COCO and 2.9 mIOU on ADE20K under similar
parameter and computation cost. Code has been released at
https://github.com/OliverRensu/Shunted-Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building extraction with vision transformer. (arXiv:2111.15637v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15637">
<div class="article-summary-box-inner">
<span><p>As an important carrier of human productive activities, the extraction of
buildings is not only essential for urban dynamic monitoring but also necessary
for suburban construction inspection. Nowadays, accurate building extraction
from remote sensing images remains a challenge due to the complex background
and diverse appearances of buildings. The convolutional neural network (CNN)
based building extraction methods, although increased the accuracy
significantly, are criticized for their inability for modelling global
dependencies. Thus, this paper applies the Vision Transformer for building
extraction. However, the actual utilization of the Vision Transformer often
comes with two limitations. First, the Vision Transformer requires more GPU
memory and computational costs compared to CNNs. This limitation is further
magnified when encountering large-sized inputs like fine-resolution remote
sensing images. Second, spatial details are not sufficiently preserved during
the feature extraction of the Vision Transformer, resulting in the inability
for fine-grained building segmentation. To handle these issues, we propose a
novel Vision Transformer (BuildFormer), with a dual-path structure.
Specifically, we design a spatial-detailed context path to encode rich spatial
details and a global context path to capture global dependencies. Besides, we
develop a window-based linear multi-head self-attention to make the complexity
of the multi-head self-attention linear with the window size, which strengthens
the global context extraction by using large windows and greatly improves the
potential of the Vision Transformer in processing large-sized remote sensing
images. The proposed method yields state-of-the-art performance (75.74% IoU) on
the Massachusetts building dataset. Code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Exponentially Tilted Gaussian Prior for Variational Autoencoders. (arXiv:2111.15646v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15646">
<div class="article-summary-box-inner">
<span><p>An important property for deep neural networks is the ability to perform
robust out-of-distribution detection on previously unseen data. This property
is essential for safety purposes when deploying models for real world
applications. Recent studies show that probabilistic generative models can
perform poorly on this task, which is surprising given that they seek to
estimate the likelihood of training data. To alleviate this issue, we propose
the exponentially tilted Gaussian prior distribution for the Variational
Autoencoder (VAE) which pulls points onto the surface of a hyper-sphere in
latent space. This achieves state-of-the art results on the area under the
curve-receiver operator characteristics metric using just the log-likelihood
that the VAE naturally assigns. Because this prior is a simple modification of
the traditional VAE prior, it is faster and easier to implement than
competitive methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consensus Graph Representation Learning for Better Grounded Image Captioning. (arXiv:2112.00974v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00974">
<div class="article-summary-box-inner">
<span><p>The contemporary visual captioning models frequently hallucinate objects that
are not actually in a scene, due to the visual misclassification or
over-reliance on priors that resulting in the semantic inconsistency between
the visual information and the target lexical words. The most common way is to
encourage the captioning model to dynamically link generated object words or
phrases to appropriate regions of the image, i.e., the grounded image
captioning (GIC). However, GIC utilizes an auxiliary task (grounding objects)
that has not solved the key issue of object hallucination, i.e., the semantic
inconsistency. In this paper, we take a novel perspective on the issue above -
exploiting the semantic coherency between the visual and language modalities.
Specifically, we propose the Consensus Rraph Representation Learning framework
(CGRL) for GIC that incorporates a consensus representation into the grounded
captioning pipeline. The consensus is learned by aligning the visual graph
(e.g., scene graph) to the language graph that consider both the nodes and
edges in a graph. With the aligned consensus, the captioning model can capture
both the correct linguistic characteristics and visual relevance, and then
grounding appropriate image regions further. We validate the effectiveness of
our model, with a significant decline in object hallucination (-9% CHAIRi) on
the Flickr30k Entities dataset. Besides, our CGRL also evaluated by several
automatic metrics and human evaluation, the results indicate that the proposed
approach can simultaneously improve the performance of image captioning (+2.9
Cider) and grounding (+2.3 F1LOC).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Detect Every Thing in an Open World. (arXiv:2112.01698v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01698">
<div class="article-summary-box-inner">
<span><p>Many open-world applications require the detection of novel objects, yet
state-of-the-art object detection and instance segmentation networks do not
excel at this task. The key issue lies in their assumption that regions without
any annotations should be suppressed as negatives, which teaches the model to
treat the unannotated objects as background. To address this issue, we propose
a simple yet surprisingly powerful data augmentation and training scheme we
call Learning to Detect Every Thing (LDET). To avoid suppressing hidden
objects, background objects that are visible but unlabeled, we paste annotated
objects on a background image sampled from a small region of the original
image. Since training solely on such synthetically-augmented images suffers
from domain shift, we decouple the training into two parts: 1) training the
region classification and regression head on augmented images, and 2)~training
the mask heads on original images. In this way, a model does not learn to
classify hidden objects as background while generalizing well to real images.
LDET leads to significant improvements on many datasets in the open-world
instance segmentation task, outperforming baselines on cross-category
generalization on COCO, as well as cross-dataset evaluation on UVO and
Cityscapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Modality-Aware Multiple Granularity Pre-Training for RGB-Infrared Person Re-Identification. (arXiv:2112.06147v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06147">
<div class="article-summary-box-inner">
<span><p>RGB-Infrared person re-identification (RGB-IR ReID) aims to associate people
across disjoint RGB and IR camera views. Currently, state-of-the-art
performance of RGB-IR ReID is not as impressive as that of conventional ReID.
Much of that is due to the notorious modality bias training issue brought by
the single-modality ImageNet pre-training, which might yield RGB-biased
representations that severely hinder the cross-modality image retrieval. This
paper makes first attempt to tackle the task from a pre-training perspective.
We propose a self-supervised pre-training solution, named Modality-Aware
Multiple Granularity Learning (MMGL), which directly trains models from scratch
only on multi-modal ReID datasets, but achieving competitive results against
ImageNet pre-training, without using any external data or sophisticated tuning
tricks. First, we develop a simple-but-effective 'permutation recovery' pretext
task that globally maps shuffled RGB-IR images into a shared latent permutation
space, providing modality-invariant global representations for downstream ReID
tasks. Second, we present a part-aware cycle-contrastive (PCC) learning
strategy that utilizes cross-modality cycle-consistency to maximize agreement
between semantically similar RGB-IR image patches. This enables contrastive
learning for the unpaired multi-modal scenarios, further improving the
discriminability of local features without laborious instance augmentation.
Based on these designs, MMGL effectively alleviates the modality bias training
problem. Extensive experiments demonstrate that it learns better
representations (+8.03% Rank-1 accuracy) with faster training speed (converge
only in few hours) and higher data efficiency (&lt;5% data size) than ImageNet
pre-training. The results also suggest it generalizes well to various existing
models, losses and has promising transferability across datasets. The code will
be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Raw High-Definition Radar for Multi-Task Learning. (arXiv:2112.10646v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10646">
<div class="article-summary-box-inner">
<span><p>With their robustness to adverse weather conditions and ability to measure
speeds, radar sensors have been part of the automotive landscape for more than
two decades. Recent progress toward High Definition (HD) Imaging radar has
driven the angular resolution below the degree, thus approaching laser scanning
performance. However, the amount of data a HD radar delivers and the
computational cost to estimate the angular positions remain a challenge. In
this paper, we propose a novel HD radar sensing model, FFT-RadNet, that
eliminates the overhead of computing the range-azimuth-Doppler 3D tensor,
learning instead to recover angles from a range-Doppler spectrum. FFT-RadNet is
trained both to detect vehicles and to segment free driving space. On both
tasks, it competes with the most recent radar-based models while requiring less
compute and memory. Also, we collected and annotated 2-hour worth of raw data
from synchronized automotive-grade sensors (camera, laser, HD radar) in various
environments (city street, highway, countryside road). This unique dataset,
nick-named RADIal for "Radar, Lidar et al.", is available at
https://github.com/valeoai/RADIal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Resolution Image Synthesis with Latent Diffusion Models. (arXiv:2112.10752v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10752">
<div class="article-summary-box-inner">
<span><p>By decomposing the image formation process into a sequential application of
denoising autoencoders, diffusion models (DMs) achieve state-of-the-art
synthesis results on image data and beyond. Additionally, their formulation
allows for a guiding mechanism to control the image generation process without
retraining. However, since these models typically operate directly in pixel
space, optimization of powerful DMs often consumes hundreds of GPU days and
inference is expensive due to sequential evaluations. To enable DM training on
limited computational resources while retaining their quality and flexibility,
we apply them in the latent space of powerful pretrained autoencoders. In
contrast to previous work, training diffusion models on such a representation
allows for the first time to reach a near-optimal point between complexity
reduction and detail preservation, greatly boosting visual fidelity. By
introducing cross-attention layers into the model architecture, we turn
diffusion models into powerful and flexible generators for general conditioning
inputs such as text or bounding boxes and high-resolution synthesis becomes
possible in a convolutional manner. Our latent diffusion models (LDMs) achieve
a new state of the art for image inpainting and highly competitive performance
on various tasks, including unconditional image generation, semantic scene
synthesis, and super-resolution, while significantly reducing computational
requirements compared to pixel-based DMs. Code is available at
https://github.com/CompVis/latent-diffusion .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeMask: Semantically Masked Transformers for Semantic Segmentation. (arXiv:2112.12782v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12782">
<div class="article-summary-box-inner">
<span><p>Finetuning a pretrained backbone in the encoder part of an image transformer
network has been the traditional approach for the semantic segmentation task.
However, such an approach leaves out the semantic context that an image
provides during the encoding stage. This paper argues that incorporating
semantic information of the image into pretrained hierarchical
transformer-based backbones while finetuning improves the performance
considerably. To achieve this, we propose SeMask, a simple and effective
framework that incorporates semantic information into the encoder with the help
of a semantic attention operation. In addition, we use a lightweight semantic
decoder during training to provide supervision to the intermediate semantic
prior maps at every stage. Our experiments demonstrate that incorporating
semantic priors enhances the performance of the established hierarchical
encoders with a slight increase in the number of FLOPs. We provide empirical
proof by integrating SeMask into Swin Transformer and Mix Transformer backbones
as our encoder paired with different decoders. Our framework achieves a new
state-of-the-art of 58.25% mIoU on the ADE20K dataset and improvements of over
3% in the mIoU metric on the Cityscapes dataset. The code and checkpoints are
publicly available at
https://github.com/Picsart-AI-Research/SeMask-Segmentation .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransVPR: Transformer-based place recognition with multi-level attention aggregation. (arXiv:2201.02001v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02001">
<div class="article-summary-box-inner">
<span><p>Visual place recognition is a challenging task for applications such as
autonomous driving navigation and mobile robot localization. Distracting
elements presenting in complex scenes often lead to deviations in the
perception of visual place. To address this problem, it is crucial to integrate
information from only task-relevant regions into image representations. In this
paper, we introduce a novel holistic place recognition model, TransVPR, based
on vision Transformers. It benefits from the desirable property of the
self-attention operation in Transformers which can naturally aggregate
task-relevant features. Attentions from multiple levels of the Transformer,
which focus on different regions of interest, are further combined to generate
a global image representation. In addition, the output tokens from Transformer
layers filtered by the fused attention mask are considered as key-patch
descriptors, which are used to perform spatial matching to re-rank the
candidates retrieved by the global image features. The whole model allows
end-to-end training with a single objective and image-level supervision.
TransVPR achieves state-of-the-art performance on several real-world benchmarks
while maintaining low computational time and storage requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">gDNA: Towards Generative Detailed Neural Avatars. (arXiv:2201.04123v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04123">
<div class="article-summary-box-inner">
<span><p>To make 3D human avatars widely available, we must be able to generate a
variety of 3D virtual humans with varied identities and shapes in arbitrary
poses. This task is challenging due to the diversity of clothed body shapes,
their complex articulations, and the resulting rich, yet stochastic geometric
detail in clothing. Hence, current methods to represent 3D people do not
provide a full generative model of people in clothing. In this paper, we
propose a novel method that learns to generate detailed 3D shapes of people in
a variety of garments with corresponding skinning weights. Specifically, we
devise a multi-subject forward skinning module that is learned from only a few
posed, un-rigged scans per subject. To capture the stochastic nature of
high-frequency details in garments, we leverage an adversarial loss formulation
that encourages the model to capture the underlying statistics. We provide
empirical evidence that this leads to realistic generation of local details
such as wrinkles. We show that our model is able to generate natural human
avatars wearing diverse and detailed clothing. Furthermore, we show that our
method can be used on the task of fitting human models to raw scans,
outperforming the previous state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-linear Motion Estimation for Video Frame Interpolation using Space-time Convolutions. (arXiv:2201.11407v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11407">
<div class="article-summary-box-inner">
<span><p>Video frame interpolation aims to synthesize one or multiple frames between
two consecutive frames in a video. It has a wide range of applications
including slow-motion video generation, frame-rate up-scaling and developing
video codecs. Some older works tackled this problem by assuming per-pixel
linear motion between video frames. However, objects often follow a non-linear
motion pattern in the real domain and some recent methods attempt to model
per-pixel motion by non-linear models (e.g., quadratic). A quadratic model can
also be inaccurate, especially in the case of motion discontinuities over time
(i.e. sudden jerks) and occlusions, where some of the flow information may be
invalid or inaccurate.
</p>
<p>In our paper, we propose to approximate the per-pixel motion using a
space-time convolution network that is able to adaptively select the motion
model to be used. Specifically, we are able to softly switch between a linear
and a quadratic model. Towards this end, we use an end-to-end 3D CNN
encoder-decoder architecture over bidirectional optical flows and occlusion
maps to estimate the non-linear motion model of each pixel. Further, a motion
refinement module is employed to refine the non-linear motion and the
interpolated frames are estimated by a simple warping of the neighboring frames
with the estimated per-pixel motion. Through a set of comprehensive
experiments, we validate the effectiveness of our model and show that our
method outperforms state-of-the-art algorithms on four datasets (Vimeo, DAVIS,
HD and GoPro).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks. (arXiv:2201.11440v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11440">
<div class="article-summary-box-inner">
<span><p>Novel and high-performance medical image classification pipelines are heavily
utilizing ensemble learning strategies. The idea of ensemble learning is to
assemble diverse models or multiple predictions and, thus, boost prediction
performance. However, it is still an open question to what extent as well as
which ensemble learning strategies are beneficial in deep learning based
medical image classification pipelines. In this work, we proposed a
reproducible medical image classification pipeline for analyzing the
performance impact of the following ensemble learning techniques: Augmenting,
Stacking, and Bagging. The pipeline consists of state-of-the-art preprocessing
and image augmentation methods as well as 9 deep convolution neural network
architectures. It was applied on four popular medical imaging datasets with
varying complexity. Furthermore, 12 pooling functions for combining multiple
predictions were analyzed, ranging from simple statistical functions like
unweighted averaging up to more complex learning-based functions like support
vector machines. Our results revealed that Stacking achieved the largest
performance gain of up to 13% F1-score increase. Augmenting showed consistent
improvement capabilities by up to 4% and is also applicable to single model
based pipelines. Cross-validation based Bagging demonstrated significant
performance gain close to Stacking, which resulted in an F1-score increase up
to +11%. Furthermore, we demonstrated that simple statistical pooling functions
are equal or often even better than more complex pooling functions. We
concluded that the integration of ensemble learning techniques is a powerful
method for any medical image classification pipeline to improve robustness and
boost performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate calibration of multi-perspective cameras from a generalization of the hand-eye constraint. (arXiv:2202.00886v5 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00886">
<div class="article-summary-box-inner">
<span><p>Multi-perspective cameras are quickly gaining importance in many applications
such as smart vehicles and virtual or augmented reality. However, a large
system size or absence of overlap in neighbouring fields-of-view often
complicate their calibration. We present a novel solution which relies on the
availability of an external motion capture system. Our core contribution
consists of an extension to the hand-eye calibration problem which jointly
solves multi-eye-to-base problems in closed form. We furthermore demonstrate
its equivalence to the multi-eye-in-hand problem. The practical validity of our
approach is supported by our experiments, indicating that the method is highly
efficient and accurate, and outperforms existing closed-form alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mathematical Cookbook for Snapshot Compressive Imaging. (arXiv:2202.07437v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07437">
<div class="article-summary-box-inner">
<span><p>The author intends to provide you with a beautiful, elegant, user-friendly
cookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the
cookbook is composed of introduction and conventional optimization, using
regularization-based optimization algorithms for SCI. The latest releases are
strongly recommended! For any other questions, suggestions, or comments, feel
free to email the author.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection. (arXiv:2203.02194v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02194">
<div class="article-summary-box-inner">
<span><p>In some scenarios, classifier requires detecting out-of-distribution samples
far from its training data. With desirable characteristics, reconstruction
autoencoder-based methods deal with this problem by using input reconstruction
error as a metric of novelty vs. normality. We formulate the essence of such
approach as a quadruplet domain translation with an intrinsic bias to only
query for a proxy of conditional data uncertainty. Accordingly, an improvement
direction is formalized as maximumly compressing the autoencoder's latent space
while ensuring its reconstructive power for acting as a described domain
translator. From it, strategies are introduced including semantic
reconstruction, data certainty decomposition and normalized L2 distance to
substantially improve original methods, which together establish
state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of
CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method
works without any additional data, hard-to-implement structure, time-consuming
pipeline, and even harming the classification accuracy of known classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from All Vehicles. (arXiv:2203.11934v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11934">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a system to train driving policies from experiences
collected not just from the ego-vehicle, but all vehicles that it observes.
This system uses the behaviors of other agents to create more diverse driving
scenarios without collecting additional data. The main difficulty in learning
from other vehicles is that there is no sensor information. We use a set of
supervisory tasks to learn an intermediate representation that is invariant to
the viewpoint of the controlling vehicle. This not only provides a richer
signal at training time but also allows more complex reasoning during
inference. Learning how all vehicles drive helps predict their behavior at test
time and can avoid collisions. We evaluate this system in closed-loop driving
simulations. Our system outperforms all prior methods on the public CARLA
Leaderboard by a wide margin, improving driving score by 25 and route
completion rate by 24 points. Our method won the 2021 CARLA Autonomous Driving
challenge. Code and data are available at https://github.com/dotchen/LAV.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Microscopy Designs an All Optical Quantitative Phase Microscope. (arXiv:2203.14944v2 [physics.optics] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14944">
<div class="article-summary-box-inner">
<span><p>Ever since the first microscope by Zacharias Janssen in the late 16th
century, scientists have been inventing new types of microscopes for various
tasks. Inventing a novel architecture demands years, if not decades, worth of
scientific experience and creativity. In this work, we introduce Differentiable
Microscopy ($\partial\mu$), a deep learning-based design paradigm, to aid
scientists design new interpretable microscope architectures. Differentiable
microscopy first models a common physics-based optical system however with
trainable optical elements at key locations on the optical path. Using
pre-acquired data, we then train the model end-to-end for a task of interest.
The learnt design proposal can then be simplified by interpreting the learnt
optical elements. As a first demonstration, based on the optical 4-$f$ system,
we present an all-optical quantitative phase microscope (QPM) design that
requires no computational post-reconstruction. A follow-up literature survey
suggested that the learnt architecture is similar to the generalized phase
concept developed two decades ago. We then incorporate the generalized phase
contrast concept to simplify the learning procedure. Furthermore, this physical
optical setup is miniaturized using a diffractive deep neural network (D2NN).
We outperform the existing benchmark for all-optical phase-to-intensity
conversion on multiple datasets, and ours is the first demonstration of its
kind on D2NNs. The proposed differentiable microscopy framework supplements the
creative process of designing new optical systems and would perhaps lead to
unconventional but better optical designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Face Video Compression using Multiple Views. (arXiv:2203.15401v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15401">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep generative models led to the development of neural
face video compression codecs that use an order of magnitude less bandwidth
than engineered codecs. These neural codecs reconstruct the current frame by
warping a source frame and using a generative model to compensate for
imperfections in the warped source frame. Thereby, the warp is encoded and
transmitted using a small number of keypoints rather than a dense flow field,
which leads to massive savings compared to traditional codecs. However, by
relying on a single source frame only, these methods lead to inaccurate
reconstructions (e.g. one side of the head becomes unoccluded when turning the
head and has to be synthesized). Here, we aim to tackle this issue by relying
on multiple source frames (views of the face) and present encouraging results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leverage Your Local and Global Representations: A New Self-Supervised Learning Strategy. (arXiv:2203.17205v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17205">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) methods aim to learn view-invariant
representations by maximizing the similarity between the features extracted
from different crops of the same image regardless of cropping size and content.
In essence, this strategy ignores the fact that two crops may truly contain
different image information, e.g., background and small objects, and thus tends
to restrain the diversity of the learned representations. In this work, we
address this issue by introducing a new self-supervised learning strategy,
LoGo, that explicitly reasons about Local and Global crops. To achieve view
invariance, LoGo encourages similarity between global crops from the same
image, as well as between a global and a local crop. However, to correctly
encode the fact that the content of smaller crops may differ entirely, LoGo
promotes two local crops to have dissimilar representations, while being close
to global crops. Our LoGo strategy can easily be applied to existing SSL
methods. Our extensive experiments on a variety of datasets and using different
self-supervised learning frameworks validate its superiority over existing
approaches. Noticeably, we achieve better results than supervised models on
transfer learning when using only 1/10 of the data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dynamic Correlations in Spatiotemporal Graphs for Motion Prediction. (arXiv:2204.01297v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01297">
<div class="article-summary-box-inner">
<span><p>Human motion prediction is a challenge task due to the dynamic spatiotemporal
graph correlations in different motion sequences. How to efficiently represent
spatiotemporal graph correlations and model dynamic correlation variances
between different motion sequences is a challenge for spatiotemporal graph
representation in motion prediction. In this work, we present Dynamic
SpatioTemporal Graph Convolution (DSTD-GC). The proposed DSTD-GC decomposes
dynamic spatiotemporal graph modeling into a combination of Dynamic Spatial
Graph Convolution (DS-GC) and Dynamic Temporal Graph Convolution (DT-GC). As
human motions are subject to common constraints like body connections and
present dynamic motion patterns from different samples, we present Constrained
Dynamic Correlation Modeling strategy to represent the spatial/temporal graph
as a shared spatial/temporal correlation and a function to extract
temporal-specific /spatial-specific adjustments for each sample. The modeling
strategy represents the spatiotemporal graph with 28.6\% parameters of the
state-of-the-art static decomposition representation while also explicitly
models sample-specific spatiotemporal correlation variances. Moreover, we also
mathematically reformulating spatiotemporal graph convolutions and their
decomposed variants into a unified form and find that DSTD-GC relaxes strict
constraints of other graph convolutions, leading to a stronger representation
capability. Combining DSTD-GC with prior knowledge, we propose a powerful
spatiotemporal graph convolution network called DSTD-GCN which outperforms
state-of-the-art methods on the Human3.6M and CMU Mocap datasets in prediction
accuracy with fewest parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RODD: A Self-Supervised Approach for Robust Out-of-Distribution Detection. (arXiv:2204.02553v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02553">
<div class="article-summary-box-inner">
<span><p>Recent studies have addressed the concern of detecting and rejecting the
out-of-distribution (OOD) samples as a major challenge in the safe deployment
of deep learning (DL) models. It is desired that the DL model should only be
confident about the in-distribution (ID) data which reinforces the driving
principle of the OOD detection. In this paper, we propose a simple yet
effective generalized OOD detection method independent of out-of-distribution
datasets. Our approach relies on self-supervised feature learning of the
training samples, where the embeddings lie on a compact low-dimensional space.
Motivated by the recent studies that show self-supervised adversarial
contrastive learning helps robustify the model, we empirically show that a
pre-trained model with self-supervised contrastive learning yields a better
model for uni-dimensional feature learning in the latent space. The method
proposed in this work referred to as RODD outperforms SOTA detection
performance on an extensive suite of benchmark datasets on OOD detection tasks.
On the CIFAR-100 benchmarks, RODD achieves a 26.97 $\%$ lower false-positive
rate (FPR@95) compared to SOTA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Digital Disguises: Leveraging Face Swaps to Protect Patient Privacy. (arXiv:2204.03559v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03559">
<div class="article-summary-box-inner">
<span><p>With rapid advancements in image generation technology, face swapping for
privacy protection has emerged as an active area of research. The ultimate
benefit is improved access to video datasets, e.g. in healthcare settings.
Recent literature has proposed deep network-based architectures to perform
facial swaps and reported the associated reduction in facial recognition
accuracy. However, there is not much reporting on how well these methods
preserve the types of semantic information needed for the privatized videos to
remain useful for their intended application. Our main contribution is a novel
end-to-end face swapping pipeline for recorded videos of standardized
assessments of autism symptoms in children. Through this design, we are the
first to provide a methodology for assessing the privacy-utility trade-offs for
the face swapping approach to patient privacy protection. Our methodology can
show, for example, that current deep network based face swapping is
bottle-necked by face detection in real world videos, and the extent to which
gaze and expression information is preserved by face swaps relative to baseline
privatization methods such as blurring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PlutoNet: An Efficient Polyp Segmentation Network. (arXiv:2204.03652v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03652">
<div class="article-summary-box-inner">
<span><p>Polyps in the colon can turn into cancerous cells if not removed with early
intervention. Deep learning models are used to minimize the number of polyps
that goes unnoticed by the experts, and to accurately segment the detected
polyps during these interventions. Although these models perform well on these
tasks, they require too many parameters, which can pose a problem with
real-time applications. To address this problem, we propose a novel
segmentation model called PlutoNet which requires only 2,626,337 parameters
while outperforming state-of-the-art models on multiple medical image
segmentation tasks. We use EfficientNetB0 architecture as a backbone and
propose the novel \emph{modified partial decoder}, which is a combination of
partial decoder and full scale connections, which further reduces the number of
parameters required, as well as captures semantic details. We use asymmetric
convolutions to handle varying polyp sizes. Finally, we weight each feature map
to improve segmentation by using a squeeze and excitation block. In addition to
polyp segmentation in colonoscopy, we tested our model on segmentation of
nuclei and surgical instruments to demonstrate its generalizability to
different medical image segmentation tasks. Our model outperformed the
state-of-the-art models with a Dice score of \%92.3 in CVC-ClinicDB dataset and
\%89.3 in EndoScene dataset, a Dice score of \%91.93 on the 2018 Data Science
Bowl Challenge dataset, and a Dice score of \%94.8 on Kvasir-Instrument
dataset. Our experiments and ablation studies show that our model is superior
in terms of accuracy, and it is able generalize well to multiple medical
segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Networks for Image Augmentation in Agriculture: A Systematic Review. (arXiv:2204.04707v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04707">
<div class="article-summary-box-inner">
<span><p>In agricultural image analysis, optimal model performance is keenly pursued
for better fulfilling visual recognition tasks (e.g., image classification,
segmentation, object detection and localization), in the presence of challenges
with biological variability and unstructured environments. Large-scale,
balanced and ground-truthed image datasets, however, are often difficult to
obtain to fuel the development of advanced, high-performance models. As
artificial intelligence through deep learning is impacting analysis and
modeling of agricultural images, data augmentation plays a crucial role in
boosting model performance while reducing manual efforts for data preparation,
by algorithmically expanding training datasets. Beyond traditional data
augmentation techniques, generative adversarial network (GAN) invented in 2014
in the computer vision community, provides a suite of novel approaches that can
learn good data representations and generate highly realistic samples. Since
2017, there has been a growth of research into GANs for image augmentation or
synthesis in agriculture for improved model performance. This paper presents an
overview of the evolution of GAN architectures followed by a systematic review
of their application to agriculture
(https://github.com/Derekabc/GANs-Agriculture), involving various vision tasks
for plant health, weeds, fruits, aquaculture, animal farming, plant phenotyping
as well as postharvest detection of fruit defects. Challenges and opportunities
of GANs are discussed for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Glass Segmentation with RGB-Thermal Image Pairs. (arXiv:2204.05453v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05453">
<div class="article-summary-box-inner">
<span><p>This paper proposes a new glass segmentation method utilizing paired RGB and
thermal images. Due to the large difference between the transmission property
of visible light and that of the thermal energy through the glass where most
glass is transparent to the visible light but opaque to thermal energy, glass
regions of a scene are made more distinguishable with a pair of RGB and thermal
images than solely with an RGB image. To exploit such a unique property, we
propose a neural network architecture that effectively combines an RGB-thermal
image pair with a new multi-modal fusion module based on attention, and
integrate CNN and transformer to extract local features and long-range
dependencies, respectively. As well, we have collected a new dataset containing
5551 RGB-thermal image pairs with ground-truth segmentation annotations. The
qualitative and quantitative evaluations demonstrate the effectiveness of the
proposed approach on fusing RGB and thermal data for glass segmentation. Our
code and data are available at
https://github.com/Dong-Huo/RGB-T-Glass-Segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisCUIT: Visual Auditor for Bias in CNN Image Classifier. (arXiv:2204.05899v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05899">
<div class="article-summary-box-inner">
<span><p>CNN image classifiers are widely used, thanks to their efficiency and
accuracy. However, they can suffer from biases that impede their practical
applications. Most existing bias investigation techniques are either
inapplicable to general image classification tasks or require significant user
efforts in perusing all data subgroups to manually specify which data
attributes to inspect. We present VisCUIT, an interactive visualization system
that reveals how and why a CNN classifier is biased. VisCUIT visually
summarizes the subgroups on which the classifier underperforms and helps users
discover and characterize the cause of the underperformances by revealing image
concepts responsible for activating neurons that contribute to
misclassifications. VisCUIT runs in modern browsers and is open-source,
allowing people to easily access and extend the tool to other model
architectures and datasets. VisCUIT is available at the following public demo
link: https://poloclub.github.io/VisCUIT. A video demo is available at
https://youtu.be/eNDbSyM4R_4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Captioning: a comparative review of where we are and which could be the route. (arXiv:2204.05976v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05976">
<div class="article-summary-box-inner">
<span><p>Video captioning is the process of describing the content of a sequence of
images capturing its semantic relationships and meanings. Dealing with this
task with a single image is arduous, not to mention how difficult it is for a
video (or images sequence). The amount and relevance of the applications of
video captioning are vast, mainly to deal with a significant amount of video
recordings in video surveillance, or assisting people visually impaired, to
mention a few. To analyze where the efforts of our community to solve the video
captioning task are, as well as what route could be better to follow, this
manuscript presents an extensive review of more than 105 papers for the period
of 2016 to 2021. As a result, the most-used datasets and metrics are
identified. Also, the main approaches used and the best ones. We compute a set
of rankings based on several performance metrics to obtain, according to its
performance, the best method with the best result on the video captioning task.
Finally, some insights are concluded about which could be the next steps or
opportunity areas to improve dealing with this complex task.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-14 23:08:09.772480929 UTC">2022-04-14 23:08:09 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>