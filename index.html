<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-14T01:30:00Z">06-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Less Is More: Linear Layers on CLIP Features as Powerful VizWiz Model. (arXiv:2206.05281v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05281">
<div class="article-summary-box-inner">
<span><p>Current architectures for multi-modality tasks such as visual question
answering suffer from their high complexity. As a result, these architectures
are difficult to train and require high computational resources. To address
these problems we present a CLIP-based architecture that does not require any
fine-tuning of the feature extractors. A simple linear classifier is used on
the concatenated features of the image and text encoder. During training an
auxiliary loss is added which operates on the answer types. The resulting
classification is then used as an attention gate on the answer class selection.
On the VizWiz 2022 Visual Question Answering Challenge we achieve 60.15 %
accuracy on Task 1: Predict Answer to a Visual Question and AP score of 83.78 %
on Task 2: Predict Answerability of a Visual Question.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AHD ConvNet for Speech Emotion Classification. (arXiv:2206.05286v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05286">
<div class="article-summary-box-inner">
<span><p>Accomplishments in the field of artificial intelligence are utilized in the
advancement of computing and making of intelligent machines for facilitating
mankind and improving user experience. Emotions are rudimentary for people,
affecting thinking and ordinary exercises like correspondence, learning and
direction. Speech emotion recognition is domain of interest in this regard and
in this work, we propose a novel mel spectrogram learning approach in which our
model uses the datapoints to learn emotions from the given wav form voice notes
in the popular CREMA-D dataset. Our model uses log mel-spectrogram as feature
with number of mels = 64. It took less training time compared to other
approaches used to address the problem of emotion speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-in-Graph Network for Automatic Gene Ontology Description Generation. (arXiv:2206.05311v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05311">
<div class="article-summary-box-inner">
<span><p>Gene Ontology (GO) is the primary gene function knowledge base that enables
computational tasks in biomedicine. The basic element of GO is a term, which
includes a set of genes with the same function. Existing research efforts of GO
mainly focus on predicting gene term associations. Other tasks, such as
generating descriptions of new terms, are rarely pursued. In this paper, we
propose a novel task: GO term description generation. This task aims to
automatically generate a sentence that describes the function of a GO term
belonging to one of the three categories, i.e., molecular function, biological
process, and cellular component. To address this task, we propose a
Graph-in-Graph network that can efficiently leverage the structural information
of GO. The proposed network introduces a two-layer graph: the first layer is a
graph of GO terms where each node is also a graph (gene graph). Such a
Graph-in-Graph network can derive the biological functions of GO terms and
generate proper descriptions. To validate the effectiveness of the proposed
network, we build three large-scale benchmark datasets. By incorporating the
proposed Graph-in-Graph network, the performances of seven different
sequence-to-sequence models can be substantially boosted across all evaluation
metrics, with up to 34.7%, 14.5%, and 39.1% relative improvements in BLEU,
ROUGE-L, and METEOR, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-TOP: Zero-Shot Cross-Schema Task-Oriented Parsing. (arXiv:2206.05352v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05352">
<div class="article-summary-box-inner">
<span><p>Deep learning methods have enabled task-oriented semantic parsing of
increasingly complex utterances. However, a single model is still typically
trained and deployed for each task separately, requiring labeled training data
for each, which makes it challenging to support new tasks, even within a single
business vertical (e.g., food-ordering or travel booking). In this paper we
describe Cross-TOP (Cross-Schema Task-Oriented Parsing), a zero-shot method for
complex semantic parsing in a given vertical. By leveraging the fact that user
requests from the same vertical share lexical and semantic similarities, a
single cross-schema parser is trained to service an arbitrary number of tasks,
seen or unseen, within a vertical. We show that Cross-TOP can achieve high
accuracy on a previously unseen task without requiring any additional training
data, thereby providing a scalable way to bootstrap semantic parsers for new
tasks. As part of this work we release the FoodOrdering dataset, a
task-oriented parsing dataset in the food-ordering vertical, with utterances
and annotations derived from five schemas, each from a different restaurant
menu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why is constrained neural language generation particularly challenging?. (arXiv:2206.05395v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05395">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep neural language models combined with the capacity of
large scale datasets have accelerated the development of natural language
generation systems that produce fluent and coherent texts (to various degrees
of success) in a multitude of tasks and application contexts. However,
controlling the output of these models for desired user and task needs is still
an open challenge. This is crucial not only to customizing the content and
style of the generated language, but also to their safe and reliable deployment
in the real world. We present an extensive survey on the emerging topic of
constrained neural language generation in which we formally define and
categorize the problems of natural language generation by distinguishing
between conditions and constraints (the latter being testable conditions on the
output text instead of the input), present constrained text generation tasks,
and review existing methods and evaluation metrics for constrained text
generation. Our aim is to highlight recent progress and trends in this emerging
field, informing on the most promising directions and limitations towards
advancing the state-of-the-art of constrained neural language generation
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building a Personalized Dialogue System with Prompt-Tuning. (arXiv:2206.05399v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05399">
<div class="article-summary-box-inner">
<span><p>Dialogue systems without consistent responses are not fascinating. In this
study, we build a dialogue system that can respond based on a given character
setting (persona) to bring consistency. Considering the trend of the rapidly
increasing scale of language models, we propose an approach that uses
prompt-tuning, which has low learning costs, on pre-trained large-scale
language models. The results of automatic and manual evaluations in English and
Japanese show that it is possible to build a dialogue system with more natural
and personalized responses using less computational resources than fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Decomposition-Based Approach for Evaluating Inter-Annotator Disagreement in Narrative Analysis. (arXiv:2206.05446v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05446">
<div class="article-summary-box-inner">
<span><p>In this work, we explore sources of inter-annotator disagreement in narrative
analysis, in light of the question of whether or not a narrative plot exists in
the text. For this purpose, we present a method for a conceptual decomposition
of an existing annotation into two separate levels: (1) \textbf{whether} or not
a narrative plot exists in the text, and (2) \textbf{which} plot elements exist
in the text. We apply this method to an existing dataset of sentences annotated
with three different narrative plot elements: \textit{Complication},
\textit{Resolution} and \textit{Success}. We then employ statistical analysis
in order to quantify how much of the inter-annotator disagreement can be
explained by each of the two levels. We further perform a qualitative analysis
of disagreement cases in each level, observing several sources of disagreement,
such as text ambiguity, scheme definition and personal differences between the
annotators. The insights gathered on the dataset may serve to reduce
inter-annotator disagreement in future annotation endeavors. We conclude with a
broader discussion on the potential implications of our approach in studying
and evaluating inter-annotator disagreement in other settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Snippet Generation. (arXiv:2206.05473v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05473">
<div class="article-summary-box-inner">
<span><p>We model product reviews to generate comparative responses consisting of
positive and negative experiences regarding the product. Specifically, we
generate a single-sentence, comparative response from a given positive and a
negative opinion. We contribute the first dataset for this task of Comparative
Snippet Generation from contrasting opinions regarding a product, and a
performance analysis of a pre-trained BERT model to generate such snippets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Adversarial Robustness of NLP Models by Information Bottleneck. (arXiv:2206.05511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05511">
<div class="article-summary-box-inner">
<span><p>Existing studies have demonstrated that adversarial examples can be directly
attributed to the presence of non-robust features, which are highly predictive,
but can be easily manipulated by adversaries to fool NLP models. In this study,
we explore the feasibility of capturing task-specific robust features, while
eliminating the non-robust ones by using the information bottleneck theory.
Through extensive experiments, we show that the models trained with our
information bottleneck-based method are able to achieve a significant
improvement in robust accuracy, exceeding performances of all the previously
reported defense methods while suffering almost no performance drop in clean
accuracy on SST-2, AGNEWS and IMDB datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recognition. (arXiv:2206.05518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05518">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) based models have been shown to generate
powerful representations that can be used to improve the performance of
downstream speech tasks. Several state-of-the-art SSL models are available, and
each of these models optimizes a different loss which gives rise to the
possibility of their features being complementary. This paper proposes using an
ensemble of such SSL representations and models, which exploits the
complementary nature of the features extracted by the various pretrained
models. We hypothesize that this results in a richer feature representation and
shows results for the ASR downstream task. To this end, we use three SSL models
that have shown excellent results on ASR tasks, namely HuBERT, Wav2vec2.0, and
WaveLM. We explore the ensemble of models fine-tuned for the ASR task and the
ensemble of features using the embeddings obtained from the pre-trained models
for a downstream ASR task. We get improved performance over individual models
and pre-trained features using Librispeech(100h) and WSJ dataset for the
downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Gap Between Training and Inference of Bayesian Controllable Language Models. (arXiv:2206.05519v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05519">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have achieved great success on
natural language generation tasks. However, it is difficult to control the
pre-trained language models to generate sentences with the desired attribute
such as topic and sentiment, etc. Recently, Bayesian Controllable Language
Models (BCLMs) have been shown to be efficient in controllable language
generation. Rather than fine-tuning the parameters of pre-trained language
models, BCLMs use external discriminators to guide the generation of
pre-trained language models. However, the mismatch between training and
inference of BCLMs limits the performance of the models. To address the
problem, in this work we propose a "Gemini Discriminator" for controllable
language generation which alleviates the mismatch problem with a small
computational cost. We tested our method on two controllable language
generation tasks: sentiment control and topic control. On both tasks, our
method reached achieved new state-of-the-art results in automatic and human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Continuous Learning Framework for Multi-modal Knowledge Discovery and Pre-training. (arXiv:2206.05555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05555">
<div class="article-summary-box-inner">
<span><p>Multi-modal pre-training and knowledge discovery are two important research
topics in multi-modal machine learning. Nevertheless, none of existing works
make attempts to link knowledge discovery with knowledge guided multi-modal
pre-training. In this paper, we propose to unify them into a continuous
learning framework for mutual improvement. Taking the open-domain uni-modal
datasets of images and texts as input, we maintain a knowledge graph as the
foundation to support these two tasks. For knowledge discovery, a pre-trained
model is used to identify cross-modal links on the graph. For model
pre-training, the knowledge graph is used as the external knowledge to guide
the model updating. These two steps are iteratively performed in our framework
for continuous learning. The experimental results on MS-COCO and Flickr30K with
respect to both knowledge discovery and the pre-trained model validate the
effectiveness of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can the Language of the Collation be Translated into the Language of the Stemma? Using Machine Translation for Witness Localization. (arXiv:2206.05603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05603">
<div class="article-summary-box-inner">
<span><p>Stemmatology is a subfield of philology where one approach to understand the
copy-history of textual variants of a text (witnesses of a tradition) is to
generate an evolutionary tree. Computational methods are partly shared between
the sister discipline of phylogenetics and stemmatology. In 2022, a surveypaper
in nature communications found that Deep Learning (DL), which otherwise has
brought about major improvements in many fields (Krohn et al 2020) has had only
minor successes in phylogenetics and that "it is difficult to conceive of an
end-to-end DL model to directly estimate phylogenetic trees from raw data in
the near future"(Sapoval et al. 2022, p.8). In stemmatology, there is to date
no known DL approach at all. In this paper, we present a new DL approach to
placement of manuscripts on a stemma and demonstrate its potential. This could
be extended to phylogenetics where the universal code of DNA might be an even
better prerequisite for the method using sequence to sequence based neural
networks in order to retrieve tree distances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Pre-trained Language Models with Noise Stability Regularization. (arXiv:2206.05658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05658">
<div class="article-summary-box-inner">
<span><p>The advent of large-scale pre-trained language models has contributed greatly
to the recent progress in natural language processing. Many state-of-the-art
language models are first trained on a large text corpus and then fine-tuned on
downstream tasks. Despite its recent success and wide adoption, fine-tuning a
pre-trained language model often suffers from overfitting, which leads to poor
generalizability due to the extremely high complexity of the model and the
limited training samples from downstream tasks. To address this problem, we
propose a novel and effective fine-tuning framework, named Layerwise Noise
Stability Regularization (LNSR). Specifically, we propose to inject the
standard Gaussian noise or In-manifold noise and regularize hidden
representations of the fine-tuned model. We first provide theoretical analyses
to support the efficacy of our method. We then demonstrate the advantages of
the proposed method over other state-of-the-art algorithms including L2-SP,
Mixout and SMART. While these previous works only verify the effectiveness of
their methods on relatively simple text classification tasks, we also verify
the effectiveness of our method on question answering tasks, where the target
problem is much more difficult and more training examples are available.
Furthermore, extensive experimental results indicate that the proposed
algorithm can not only enhance the in-domain performance of the language models
but also improve the domain generalization performance on out-of-domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding in social media: An approach to building a chit-chat dialogue model. (arXiv:2206.05696v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05696">
<div class="article-summary-box-inner">
<span><p>Building open-domain dialogue systems capable of rich human-like
conversational ability is one of the fundamental challenges in language
generation. However, even with recent advancements in the field, existing
open-domain generative models fail to capture and utilize external knowledge,
leading to repetitive or generic responses to unseen utterances. Current work
on knowledge-grounded dialogue generation primarily focuses on persona
incorporation or searching a fact-based structured knowledge source such as
Wikipedia. Our method takes a broader and simpler approach, which aims to
improve the raw conversation ability of the system by mimicking the human
response behavior through casual interactions found on social media. Utilizing
a joint retriever-generator setup, the model queries a large set of filtered
comment data from Reddit to act as additional context for the seq2seq
generator. Automatic and human evaluations on open-domain dialogue datasets
demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoSe-Co: Text Conditioned Generative CommonSense Contextualizer. (arXiv:2206.05706v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05706">
<div class="article-summary-box-inner">
<span><p>Pre-trained Language Models (PTLMs) have been shown to perform well on
natural language tasks. Many prior works have leveraged structured commonsense
present in the form of entities linked through labeled relations in Knowledge
Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static
module which limits coverage since KGs contain finite knowledge. Generative
methods train PTLMs on KG triples to improve the scale at which knowledge can
be obtained. However, training on symbolic KG entities limits their
applicability in tasks involving natural language text where they ignore
overall context. To mitigate this, we propose a CommonSense Contextualizer
(CoSe-Co) conditioned on sentences as input to make it generically usable in
tasks for generating knowledge relevant to the overall context of input text.
To train CoSe-Co, we propose a novel dataset comprising of sentence and
commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and
contain novel entities not present in the underlying KG. We augment generated
knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading
to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets.
We also demonstrate its applicability in improving performance of a baseline
model for paraphrase generation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The YiTrans End-to-End Speech Translation System for IWSLT 2022 Offline Shared Task. (arXiv:2206.05777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05777">
<div class="article-summary-box-inner">
<span><p>This paper describes the submission of our end-to-end YiTrans speech
translation system for the IWSLT 2022 offline task, which translates from
English audio to German, Chinese, and Japanese. The YiTrans system is built on
large-scale pre-trained encoder-decoder models. More specifically, we first
design a multi-stage pre-training strategy to build a multi-modality model with
a large amount of labeled and unlabeled data. We then fine-tune the
corresponding components of the model for the downstream speech translation
tasks. Moreover, we make various efforts to improve performance, such as data
filtering, data augmentation, speech segmentation, model ensemble, and so on.
Experimental results show that our YiTrans system obtains a significant
improvement than the strong baseline on three translation directions, and it
achieves +5.2 BLEU improvements over last year's optimal end-to-end system on
tst2021 English-German. Our final submissions rank first on English-German and
English-Chinese end-to-end systems in terms of the automatic evaluation metric.
We make our code and models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Intent Classification. (arXiv:2206.05790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05790">
<div class="article-summary-box-inner">
<span><p>Training accurate intent classifiers requires labeled data, which can be
costly to obtain. Data augmentation methods may ameliorate this issue, but the
quality of the generated data varies significantly across techniques. We study
the process of systematically producing pseudo-labeled data given a small seed
set using a wide variety of data augmentation techniques, including mixing
methods together. We find that while certain methods dramatically improve
qualitative and quantitative performance, other methods have minimal or even
negative impact. We also analyze key considerations when implementing data
augmentation methods in production.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-critiquing models for assisting human evaluators. (arXiv:2206.05802v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05802">
<div class="article-summary-box-inner">
<span><p>We fine-tune large language models to write natural language critiques
(natural language critical comments) using behavioral cloning. On a topic-based
summarization task, critiques written by our models help humans find flaws in
summaries that they would have otherwise missed. Our models help find naturally
occurring flaws in both model and human written summaries, and intentional
flaws in summaries written by humans to be deliberately misleading. We study
scaling properties of critiquing with both topic-based summarization and
synthetic tasks. Larger models write more helpful critiques, and on most tasks,
are better at self-critiquing, despite having harder-to-critique outputs.
Larger models can also integrate their own self-critiques as feedback, refining
their own summaries into better ones. Finally, we motivate and introduce a
framework for comparing critiquing ability to generation and discrimination
ability. Our measurements suggest that even large models may still have
relevant knowledge they cannot or do not articulate as critiques. These results
are a proof of concept for using AI-assisted human feedback to scale the
supervision of machine learning systems to tasks that are difficult for humans
to evaluate directly. We release our training datasets, as well as samples from
our critique assistance experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for Simultaneous Speech Translation. (arXiv:2206.05807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05807">
<div class="article-summary-box-inner">
<span><p>Simultaneous speech translation (SimulST) systems aim at generating their
output with the lowest possible latency, which is normally computed in terms of
Average Lagging (AL). In this paper we highlight that, despite its widespread
adoption, AL provides underestimated scores for systems that generate longer
predictions compared to the corresponding references. We also show that this
problem has practical relevance, as recent SimulST systems have indeed a
tendency to over-generate. As a solution, we propose LAAL (Length-Adaptive
Average Lagging), a modified version of the metric that takes into account the
over-generation phenomenon and allows for unbiased evaluation of both
under-/over-generating systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLIPv2: Unifying Localization and Vision-Language Understanding. (arXiv:2206.05836v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05836">
<div class="article-summary-box-inner">
<span><p>We present GLIPv2, a grounded VL understanding model, that serves both
localization tasks (e.g., object detection, instance segmentation) and
Vision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2
elegantly unifies localization pre-training and Vision-Language Pre-training
(VLP) with three pre-training tasks: phrase grounding as a VL reformulation of
the detection task, region-word contrastive learning as a novel region-word
level contrastive learning task, and the masked language modeling. This
unification not only simplifies the previous multi-stage VLP procedure but also
achieves mutual benefits between localization and understanding tasks.
Experimental results show that a single GLIPv2 model (all model weights are
shared) achieves near SoTA performance on various localization and
understanding tasks. The model also shows (1) strong zero-shot and few-shot
adaption performance on open-vocabulary object detection tasks and (2) superior
grounding capability on VL understanding tasks. Code will be released at
https://github.com/microsoft/GLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing the diagrammatic semiotic mode. (arXiv:2001.11224v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.11224">
<div class="article-summary-box-inner">
<span><p>As the use and diversity of diagrams across many disciplines grows, there is
an increasing interest in the diagrams research community concerning how such
diversity might be documented and explained. In this article, we argue that one
way of achieving increased reliability, coverage, and utility for a general
classification of diagrams is to draw on recently developed semiotic principles
developed within the field of multimodality. To this end, we sketch out the
internal details of what may tentatively be termed the diagrammatic semiotic
mode. This provides a natural account of how diagrammatic representations may
integrate natural language, various forms of graphics, diagrammatic elements
such as arrows, lines and other expressive resources into coherent
organisations, while still respecting the crucial diagrammatic contributions of
visual organisation. We illustrate the proposed approach using two recent
diagram corpora and show how a multimodal approach supports the empirical
analysis of diagrammatic representations, especially in identifying
diagrammatic constituents and describing their interrelations in a manner that
may be generalised across diagram types and be used to characterise distinct
kinds of functionality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Linearity of Cross-Lingual Word Embedding Mappings. (arXiv:2004.01079v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.01079">
<div class="article-summary-box-inner">
<span><p>The technique of Cross-Lingual Word Embedding (CLWE) plays a fundamental role
in tackling Natural Language Processing challenges for low-resource languages.
Its dominant approaches assumed that the relationship between embeddings could
be represented by a linear mapping, but there has been no exploration of the
conditions under which this assumption holds. Such a research gap becomes very
critical recently, as it has been evidenced that relaxing mappings to be
non-linear can lead to better performance in some cases. We, for the first
time, present a theoretical analysis that identifies the preservation of
analogies encoded in monolingual word embeddings as a necessary and sufficient
condition for the ground-truth CLWE mapping between those embeddings to be
linear. On a novel cross-lingual analogy dataset that covers five
representative analogy categories for twelve distinct languages, we carry out
experiments which provide direct empirical support for our theoretical claim.
These results offer additional insight into the observations of other
researchers and contribute inspiration for the development of more effective
cross-lingual representation learning strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Embeddings Preserving the Semantic Relationships in WordNet. (arXiv:2004.10863v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.10863">
<div class="article-summary-box-inner">
<span><p>In this paper, we provide a novel way to generate low dimensional vector
embeddings for the noun and verb synsets in WordNet, where the hypernym-hyponym
relationship is preserved in the embeddings. We call this embedding the Sense
Spectrum (and Sense Spectra for embeddings). In order to create suitable labels
for the training of sense spectra, we designed a new similarity measurement for
noun and verb synsets in WordNet. We call this similarity measurement the
Hypernym Intersection Similarity (HIS), since it compares the common and unique
hypernyms between two synsets. Our experiments show that on the noun and verb
pairs of the SimLex-999 dataset, HIS outperforms the three similarity
measurements in WordNet. Moreover, to the best of our knowledge, the sense
spectra provide the first dense synset embeddings that preserve the semantic
relationships in WordNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theoretical Rule-based Knowledge Graph Reasoning by Connectivity Dependency Discovery. (arXiv:2011.06174v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06174">
<div class="article-summary-box-inner">
<span><p>Discovering precise and interpretable rules from knowledge graphs is regarded
as an essential challenge, which can improve the performances of many
downstream tasks and even provide new ways to approach some Natural Language
Processing research topics. In this paper, we present a fundamental theory for
rule-based knowledge graph reasoning, based on which the connectivity
dependencies in the graph are captured via multiple rule types. It is the first
time for some of these rule types in a knowledge graph to be considered. Based
on these rule types, our theory can provide precise interpretations to unknown
triples. Then, we implement our theory by what we call the RuleDict model.
Results show that our RuleDict model not only provides precise rules to
interpret new triples, but also achieves state-of-the-art performances on one
benchmark knowledge graph completion task, and is competitive on other tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VGNMN: Video-grounded Neural Module Network to Video-Grounded Language Tasks. (arXiv:2104.07921v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07921">
<div class="article-summary-box-inner">
<span><p>Neural module networks (NMN) have achieved success in image-grounded tasks
such as Visual Question Answering (VQA) on synthetic images. However, very
limited work on NMN has been studied in the video-grounded dialogue tasks.
These tasks extend the complexity of traditional visual tasks with the
additional visual temporal variance and language cross-turn dependencies.
Motivated by recent NMN approaches on image-grounded tasks, we introduce
Video-grounded Neural Module Network (VGNMN) to model the information retrieval
process in video-grounded language tasks as a pipeline of neural modules. VGNMN
first decomposes all language components in dialogues to explicitly resolve any
entity references and detect corresponding action-based inputs from the
question. The detected entities and actions are used as parameters to
instantiate neural module networks and extract visual cues from the video. Our
experiments show that VGNMN can achieve promising performance on a challenging
video-grounded dialogue benchmark as well as a video QA benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trakhtenbrot's Theorem in Coq: Finite Model Theory through the Constructive Lens. (arXiv:2104.14445v5 [cs.LO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14445">
<div class="article-summary-box-inner">
<span><p>We study finite first-order satisfiability (FSAT) in the constructive setting
of dependent type theory. Employing synthetic accounts of enumerability and
decidability, we give a full classification of FSAT depending on the
first-order signature of non-logical symbols. On the one hand, our development
focuses on Trakhtenbrot's theorem, stating that FSAT is undecidable as soon as
the signature contains an at least binary relation symbol. Our proof proceeds
by a many-one reduction chain starting from the Post correspondence problem. On
the other hand, we establish the decidability of FSAT for monadic first-order
logic, i.e. where the signature only contains at most unary function and
relation symbols, as well as the enumerability of FSAT for arbitrary enumerable
signatures. To showcase an application of Trakhtenbrot's theorem, we continue
our reduction chain with a many-one reduction from FSAT to separation logic.
All our results are mechanised in the framework of a growing Coq library of
synthetic undecidability proofs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clean or Annotate: How to Spend a Limited Data Collection Budget. (arXiv:2110.08355v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08355">
<div class="article-summary-box-inner">
<span><p>Crowdsourcing platforms are often used to collect datasets for training
machine learning models, despite higher levels of inaccurate labeling compared
to expert labeling. There are two common strategies to manage the impact of
such noise. The first involves aggregating redundant annotations, but comes at
the expense of labeling substantially fewer examples. Secondly, prior works
have also considered using the entire annotation budget to label as many
examples as possible and subsequently apply denoising algorithms to implicitly
clean the dataset. We find a middle ground and propose an approach which
reserves a fraction of annotations to explicitly clean up highly probable error
samples to optimize the annotation process. In particular, we allocate a large
portion of the labeling budget to form an initial dataset used to train a
model. This model is then used to identify specific examples that appear most
likely to be incorrect, which we spend the remaining budget to relabel.
Experiments across three model variations and four natural language processing
tasks show our approach outperforms or matches both label aggregation and
advanced denoising methods designed to handle noisy labels when allocated the
same finite annotation budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Transformers with Probabilistic Attention Keys. (arXiv:2110.08678v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08678">
<div class="article-summary-box-inner">
<span><p>Multi-head attention is a driving force behind state-of-the-art transformers,
which achieve remarkable performance across a variety of natural language
processing (NLP) and computer vision tasks. It has been observed that for many
applications, those attention heads learn redundant embedding, and most of them
can be removed without degrading the performance of the model. Inspired by this
observation, we propose Transformer with a Mixture of Gaussian Keys
(Transformer-MGK), a novel transformer architecture that replaces redundant
heads in transformers with a mixture of keys at each head. These mixtures of
keys follow a Gaussian mixture model and allow each attention head to focus on
different parts of the input sequence efficiently. Compared to its conventional
transformer counterpart, Transformer-MGK accelerates training and inference,
has fewer parameters, and requires fewer FLOPs to compute while achieving
comparable or better accuracy across tasks. Transformer-MGK can also be easily
extended to use with linear attention. We empirically demonstrate the advantage
of Transformer-MGK in a range of practical applications, including language
modeling and tasks that involve very long sequences. On the Wikitext-103 and
Long Range Arena benchmark, Transformer-MGKs with 4 heads attain comparable or
better performance to the baseline transformers with 8 heads.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoLLIE: Continual Learning of Language Grounding from Language-Image Embeddings. (arXiv:2111.07993v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07993">
<div class="article-summary-box-inner">
<span><p>This paper presents CoLLIE: a simple, yet effective model for continual
learning of how language is grounded in vision. Given a pre-trained multimodal
embedding model, where language and images are projected in the same semantic
space (in this case CLIP by OpenAI), CoLLIE learns a transformation function
that adjusts the language embeddings when needed to accommodate new language
use. This is done by predicting the difference vector that needs to be applied,
as well as a scaling factor for this vector, so that the adjustment is only
applied when needed. Unlike traditional few-shot learning, the model does not
just learn new classes and labels, but can also generalize to similar language
use and leverage semantic compositionality. We verify the model's performance
on two different tasks of identifying the targets of referring expressions,
where it has to learn new language use. The results show that the model can
efficiently learn and generalize from only a few examples, with little
interference with the model's original zero-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmTract: Investor Emotions and Market Behavior. (arXiv:2112.03868v2 [q-fin.PR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03868">
<div class="article-summary-box-inner">
<span><p>We develop a tool that extracts emotions from social media text data. Our
methodology has three main advantages. First, it is tailored for financial
context; second, it incorporates key aspects of social media data, such as
non-standard phrases, emojis and emoticons; and third, it operates by
sequentially learning a latent representation that includes features such as
word order, word usage, and local context. This tool, along with a user guide
is available at: https://github.com/dvamossy/EmTract. Using EmTract, we explore
the relationship between investor emotions expressed on social media and asset
prices. We document a number of interesting insights. First, we confirm some of
the findings of controlled laboratory experiments relating investor emotions to
asset price movements. Second, we show that investor emotions are predictive of
daily price movements. These impacts are larger when volatility or short
interest are higher, and when institutional ownership or liquidity are lower.
Third, increased investor enthusiasm prior to the IPO contributes to the large
first-day return and long-run underperformance of IPO stocks. To corroborate
our results, we provide a number of robustness checks, including using an
alternative emotion model. Our findings reinforce the intuition that emotions
and market dynamics are closely related, and highlight the importance of
considering investor emotions when assessing a stock's short-term value.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design Challenges for a Multi-Perspective Search Engine. (arXiv:2112.08357v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08357">
<div class="article-summary-box-inner">
<span><p>Many users turn to document retrieval systems (e.g. search engines) to seek
answers to controversial questions. Answering such user queries usually require
identifying responses within web documents, and aggregating the responses based
on their different perspectives.
</p>
<p>Classical document retrieval systems fall short at delivering a set of direct
and diverse responses to the users. Naturally, identifying such responses
within a document is a natural language understanding task. In this paper, we
examine the challenges of synthesizing such language understanding objectives
with document retrieval, and study a new perspective-oriented document
retrieval paradigm. We discuss and assess the inherent natural language
understanding challenges in order to achieve the goal. Following the design
challenges and principles, we demonstrate and evaluate a practical prototype
pipeline system. We use the prototype system to conduct a user survey in order
to assess the utility of our paradigm, as well as understanding the user
information needs for controversial queries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08879">
<div class="article-summary-box-inner">
<span><p>Most language grounding models learn to select the referred object from a
pool of object proposals provided by a pre-trained detector. This object
proposal bottleneck is limiting because an utterance may refer to visual
entities at various levels of granularity, such as the chair, the leg of a
chair, or the tip of the front leg of a chair, which may be missed by the
detector. Recently, MDETR introduced a language grounding model for 2D images
that do not have such a box proposal bottleneck; instead of selecting objects
from a proposal pool, it instead decodes the referenced object boxes directly
from image and language features and achieves big leaps in performance. We
propose a language grounding model for 3D scenes built on MDETR, which we call
BEAUTY-DETR, from bottom-up and top-down DETR. BEAUTY-DETR attends on an
additional object proposal pool computed bottom-up from a pre-trained detector.
Yet it decodes referenced objects without selecting them from the pool. In this
way, it uses powerful object detectors to help ground language without being
restricted by their misses. Second, BEAUTY-DETR augments supervision from
language grounding annotations by configuring object detection annotations as
language prompts to be grounded in images. The proposed model sets a new
state-of-the-art across popular 3D language grounding benchmarks with
significant performance gains over previous 3D approaches (12.6% on SR3D, 11.6%
on NR3D and 6.3% on ScanRefer). It outperforms a straightforward MDETR for the
3D point clouds method we implemented by 6.7% on SR3D, 11.8% on NR3D and 5% on
the ScanRefer benchmark. When applied to language grounding in 2D images, it
performs on par with MDETR. We ablate each of the design choices of the model
and quantify their contribution to performance. Code and checkpoints are
available at https://github.com/nickgkan/beauty_detr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What is Event Knowledge Graph: A Survey. (arXiv:2112.15280v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15280">
<div class="article-summary-box-inner">
<span><p>Besides entity-centric knowledge, usually organized as Knowledge Graph (KG),
events are also an essential kind of knowledge in the world, which trigger the
spring up of event-centric knowledge representation form like Event KG (EKG).
It plays an increasingly important role in many downstream applications, such
as search, question-answering, recommendation, financial quantitative
investments, and text generation. This paper provides a comprehensive survey of
EKG from history, ontology, instance, and application views. Specifically, to
characterize EKG thoroughly, we focus on its history, definition, schema
induction, acquisition, related representative graphs/systems, and
applications. The development processes and trends are studied therein. We
further summarize prospective directions to facilitate future research on EKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solvability of orbit-finite systems of linear equations. (arXiv:2201.09060v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09060">
<div class="article-summary-box-inner">
<span><p>We study orbit-finite systems of linear equations, in the setting of sets
with atoms. Our principal contribution is a decision procedure for solvability
of such systems. The procedure works for every field (and even commutative
ring) under mild effectiveness assumptions, and reduces a given orbit-finite
system to a number of finite ones: exponentially many in general, but
polynomially many when atom dimension of input systems is fixed. Towards
obtaining the procedure we push further the theory of vector spaces generated
by orbit-finite sets, and show that each such vector space admits an
orbit-finite basis. This fundamental property is a key tool in our development,
but should be also of wider interest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing language context confusion for end-to-end code-switching automatic speech recognition. (arXiv:2201.12155v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12155">
<div class="article-summary-box-inner">
<span><p>Code-switching is about dealing with alternative languages in the
communication process. Training end-to-end (E2E) automatic speech recognition
(ASR) systems for code-switching is known to be a challenging problem because
of the lack of data compounded by the increased language context confusion due
to the presence of more than one language. In this paper, we propose a
language-related attention mechanism to reduce multilingual context confusion
for the E2E code-switching ASR model based on the Equivalence Constraint Theory
(EC). The linguistic theory requires that any monolingual fragment that occurs
in the code-switching sentence must occur in one of the monolingual sentences.
It establishes a bridge between monolingual data and code-switching data. By
calculating the respective attention of multiple languages, our method can
efficiently transfer language knowledge from rich monolingual data. We evaluate
our method on ASRU 2019 Mandarin-English code-switching challenge dataset.
Compared with the baseline model, the proposed method achieves 11.37% relative
mix error rate reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuraHealth: An Automated Screening Pipeline to Detect Undiagnosed Cognitive Impairment in Electronic Health Records with Deep Learning and Natural Language Processing. (arXiv:2202.00478v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00478">
<div class="article-summary-box-inner">
<span><p>Dementia related cognitive impairment (CI) affects over 55 million people
worldwide and is growing rapidly at the rate of one new case every 3 seconds.
With a recurring failure of clinical trials, early diagnosis is crucial, but
75% of dementia cases go undiagnosed globally with up to 90% in
low-and-middle-income countries. Current diagnostic methods are notoriously
complex, involving manual review of medical notes, numerous cognitive tests,
expensive brain scans or spinal fluid tests. Information relevant to CI is
often found in the electronic health records (EHRs) and can provide vital clues
for early diagnosis, but a manual review by experts is tedious and error prone.
This project develops a novel state-of-the-art automated screening pipeline for
scalable and high-speed discovery of undetected CI in EHRs. To understand the
linguistic context from complex language structures in EHR, a database of 8,656
sequences was constructed to train attention-based deep learning natural
language processing model to classify sequences. A patient level prediction
model based on logistic regression was developed using the sequence level
classifier. The deep learning system achieved 93% accuracy and AUC = 0.98 to
identify patients who had no earlier diagnosis, dementia-related diagnosis
code, or dementia-related medications in their EHR. These patients would have
otherwise gone undetected or detected too late. The EHR screening pipeline was
deployed in NeuraHealthNLP, a web application for automated and real-time CI
screening by simply uploading EHRs in a browser. NeuraHealthNLP is cheaper,
faster, more accessible, and outperforms current clinical methods including
text-based analytics and machine learning approaches. It makes early diagnosis
viable in regions with scarce health care services but accessible internet or
cellular services.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching Tweets With Applicable Fact-Checks Across Languages. (arXiv:2202.07094v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07094">
<div class="article-summary-box-inner">
<span><p>An important challenge for news fact-checking is the effective dissemination
of existing fact-checks. This in turn brings the need for reliable methods to
detect previously fact-checked claims. In this paper, we focus on automatically
finding existing fact-checks for claims made in social media posts (tweets). We
conduct both classification and retrieval experiments, in monolingual (English
only), multilingual (Spanish, Portuguese), and cross-lingual (Hindi-English)
settings using multilingual transformer models such as XLM-RoBERTa and
multilingual embeddings such as LaBSE and SBERT. We present promising results
for "match" classification (86% average accuracy) in four language pairs. We
also find that a BM25 baseline outperforms or is on par with state-of-the-art
multilingual embedding models for the retrieval task during our monolingual
experiments. We highlight and discuss NLP challenges while addressing this
problem in different languages, and we introduce a novel curated dataset of
fact-checks and corresponding tweets for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PET: An Annotated Dataset for Process Extraction from Natural Language Text. (arXiv:2203.04860v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04860">
<div class="article-summary-box-inner">
<span><p>Process extraction from text is an important task of process discovery, for
which various approaches have been developed in recent years. However, in
contrast to other information extraction tasks, there is a lack of
gold-standard corpora of business process descriptions that are carefully
annotated with all the entities and relationships of interest. Due to this, it
is currently hard to compare the results obtained by extraction approaches in
an objective manner, whereas the lack of annotated texts also prevents the
application of data-driven information extraction methodologies, typical of the
natural language processing field. Therefore, to bridge this gap, we present
the PET dataset, a first corpus of business process descriptions annotated with
activities, gateways, actors, and flow information. We present our new
resource, including a variety of baselines to benchmark the difficulty and
challenges of business process extraction from text. PET can be accessed via
huggingface.co/datasets/patriziobellan/PET
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition. (arXiv:2203.06925v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06925">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition task is one of the core tasks of information
extraction.Word ambiguity and word abbreviation are important reasons for the
low recognition rate of named entities. In this paper, we propose a novel named
entity recognition model WCL-BBCD (Word Contrastive Learning with
BERT-BiLSTM-CRF-DBpedia) incorporating the idea of contrastive learning. The
model first trains the sentence pairs in the text, calculate similarity between
words in sentence pairs by cosine similarity, and fine-tunes the BERT model
used for the named entity recognition task through the similarity, so as to
alleviate word ambiguity. Then, the fine-tuned BERT model is combined with the
BiLSTM-CRF model to perform the named entity recognition task. Finally, the
recognition results are corrected in combination with prior knowledge such as
knowledge graphs, so as to alleviate the recognition caused by word
abbreviations low-rate problem. Experimental results show that our model
outperforms other similar model methods on the CoNLL-2003 English dataset and
OntoNotes V5 English dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Switched and Code Mixed Speech Recognition for Indic languages. (arXiv:2203.16578v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16578">
<div class="article-summary-box-inner">
<span><p>Training multilingual automatic speech recognition (ASR) systems is
challenging because acoustic and lexical information is typically language
specific. Training multilingual system for Indic languages is even more tougher
due to lack of open source datasets and results on different approaches. We
compare the performance of end to end multilingual speech recognition system to
the performance of monolingual models conditioned on language identification
(LID). The decoding information from a multilingual model is used for language
identification and then combined with monolingual models to get an improvement
of 50% WER across languages. We also propose a similar technique to solve the
Code Switched problem and achieve a WER of 21.77 and 28.27 over Hindi-English
and Bengali-English respectively. Our work talks on how transformer based ASR
especially wav2vec 2.0 can be applied in developing multilingual ASR and code
switched ASR for Indic languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Speech Recognition for Indic Languages using Language Model. (arXiv:2203.16595v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16595">
<div class="article-summary-box-inner">
<span><p>We study the effect of applying a language model (LM) on the output of
Automatic Speech Recognition (ASR) systems for Indic languages. We fine-tune
wav2vec $2.0$ models for $18$ Indic languages and adjust the results with
language models trained on text derived from a variety of sources. Our findings
demonstrate that the average Character Error Rate (CER) decreases by over $28$
\% and the average Word Error Rate (WER) decreases by about $36$ \% after
decoding with LM. We show that a large LM may not provide a substantial
improvement as compared to a diverse one. We also demonstrate that high quality
transcriptions can be obtained on domain-specific data without retraining the
ASR model and show results on biomedical domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Word Error Rate a good evaluation metric for Speech Recognition in Indic Languages?. (arXiv:2203.16601v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16601">
<div class="article-summary-box-inner">
<span><p>We propose a new method for the calculation of error rates in Automatic
Speech Recognition (ASR). This new metric is for languages that contain half
characters and where the same character can be written in different forms. We
implement our methodology in Hindi which is one of the main languages from
Indic context and we think this approach is scalable to other similar languages
containing a large character set. We call our metrics Alternate Word Error Rate
(AWER) and Alternate Character Error Rate (ACER).
</p>
<p>We train our ASR models using wav2vec 2.0\cite{baevski2020wav2vec} for Indic
languages. Additionally we use language models to improve our model
performance. Our results show a significant improvement in analyzing the error
rates at word and character level and the interpretability of the ASR system is
improved upto $3$\% in AWER and $7$\% in ACER for Hindi. Our experiments
suggest that in languages which have complex pronunciation, there are multiple
ways of writing words without changing their meaning. In such cases AWER and
ACER will be more useful rather than WER and CER as metrics. Further, we open
source a new benchmarking dataset of 21 hours for Hindi with the new metric
scripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTuit: Understanding Spanish language in Twitter through a native transformer. (arXiv:2204.03465v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03465">
<div class="article-summary-box-inner">
<span><p>The appearance of complex attention-based language models such as BERT,
Roberta or GPT-3 has allowed to address highly complex tasks in a plethora of
scenarios. However, when applied to specific domains, these models encounter
considerable difficulties. This is the case of Social Networks such as Twitter,
an ever-changing stream of information written with informal and complex
language, where each message requires careful evaluation to be understood even
by humans given the important role that context plays. Addressing tasks in this
domain through Natural Language Processing involves severe challenges. When
powerful state-of-the-art multilingual language models are applied to this
scenario, language specific nuances use to get lost in translation. To face
these challenges we present \textbf{BERTuit}, the larger transformer proposed
so far for Spanish language, pre-trained on a massive dataset of 230M Spanish
tweets using RoBERTa optimization. Our motivation is to provide a powerful
resource to better understand Spanish Twitter and to be used on applications
focused on this social network, with special emphasis on solutions devoted to
tackle the spreading of misinformation in this platform. BERTuit is evaluated
on several tasks and compared against M-BERT, XLM-RoBERTa and XLM-T, very
competitive multilingual transformers. The utility of our approach is shown
with applications, in this case: a zero-shot methodology to visualize groups of
hoaxes and profiling authors spreading disinformation.
</p>
<p>Misinformation spreads wildly on platforms such as Twitter in languages other
than English, meaning performance of transformers may suffer when transferred
outside English speaking communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection. (arXiv:2204.05515v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05515">
<div class="article-summary-box-inner">
<span><p>Compared with unimodal data, multimodal data can provide more features to
help the model analyze the sentiment of data. Previous research works rarely
consider token-level feature fusion, and few works explore learning the common
features related to sentiment in multimodal data to help the model fuse
multimodal features. In this paper, we propose a Contrastive Learning and
Multi-Layer Fusion (CLMLF) method for multimodal sentiment detection.
Specifically, we first encode text and image to obtain hidden representations,
and then use a multi-layer fusion module to align and fuse the token-level
features of text and image. In addition to the sentiment analysis task, we also
designed two contrastive learning tasks, label based contrastive learning and
data based contrastive learning tasks, which will help the model learn common
features related to sentiment in multimodal data. Extensive experiments
conducted on three publicly available multimodal datasets demonstrate the
effectiveness of our approach for multimodal sentiment detection compared with
existing methods. The codes are available for use at
https://github.com/Link-Li/CLMLF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection using generative-based and mutation-based data augmentation. (arXiv:2204.08198v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08198">
<div class="article-summary-box-inner">
<span><p>Sarcasm is a term that refers to the use of words to mock, irritate, or amuse
someone. It is commonly used on social media. The metaphorical and creative
nature of sarcasm presents a significant difficulty for sentiment analysis
systems based on affective computing. The methodology and results of our team,
UTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in
this paper. We put different models, and data augmentation approaches to the
test and report on which one works best. The tests begin with traditional
machine learning models and progress to transformer-based and attention-based
models. We employed data augmentation based on data mutation and data
generation. Using RoBERTa and mutation-based data augmentation, our best
approach achieved an F1-sarcastic of 0.38 in the competition's evaluation
phase. After the competition, we fixed our model's flaws and achieved an
F1-sarcastic of 0.414.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction. (arXiv:2205.00498v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00498">
<div class="article-summary-box-inner">
<span><p>Implicit event argument extraction (EAE) aims to identify arguments that
could scatter over the document. Most previous work focuses on learning the
direct relations between arguments and the given trigger, while the implicit
relations with long-range dependency are not well studied. Moreover, recent
neural network based approaches rely on a large amount of labeled data for
training, which is unavailable due to the high labelling cost. In this paper,
we propose a Curriculum learning based Prompt tuning (CUP) approach, which
resolves implicit EAE by four learning stages. The stages are defined according
to the relations with the trigger node in a semantic graph, which well captures
the long-range dependency between arguments and the trigger. In addition, we
integrate a prompt-based encoder-decoder model to elicit related knowledge from
pre-trained language models (PLMs) in each stage, where the prompt templates
are adapted with the learning progress to enhance the reasoning for arguments.
Experimental results on two well-known benchmark datasets show the great
advantages of our proposed approach. In particular, we outperform the
state-of-the-art models in both fully-supervised and low-data scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemAttack: Natural Textual Attacks via Different Semantic Spaces. (arXiv:2205.01287v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01287">
<div class="article-summary-box-inner">
<span><p>Recent studies show that pre-trained language models (LMs) are vulnerable to
textual adversarial attacks. However, existing attack methods either suffer
from low attack success rates or fail to search efficiently in the
exponentially large perturbation space. We propose an efficient and effective
framework SemAttack to generate natural adversarial text by constructing
different semantic perturbation functions. In particular, SemAttack optimizes
the generated perturbations constrained on generic semantic spaces, including
typo space, knowledge space (e.g., WordNet), contextualized semantic space
(e.g., the embedding space of BERT clusterings), or the combination of these
spaces. Thus, the generated adversarial texts are more semantically close to
the original inputs. Extensive experiments reveal that state-of-the-art (SOTA)
large-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are
still vulnerable to SemAttack. We further demonstrate that SemAttack is general
and able to generate natural adversarial texts for different languages (e.g.,
English and Chinese) with high attack success rates. Human evaluations also
confirm that our generated adversarial texts are natural and barely affect
human performance. Our code is publicly available at
https://github.com/AI-secure/SemAttack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Models to Express Their Uncertainty in Words. (arXiv:2205.14334v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14334">
<div class="article-summary-box-inner">
<span><p>We show that a GPT-3 model can learn to express uncertainty about its own
answers in natural language -- without use of model logits. When given a
question, the model generates both an answer and a level of confidence (e.g.
"90% confidence" or "high confidence"). These levels map to probabilities that
are well calibrated. The model also remains moderately calibrated under
distribution shift, and is sensitive to uncertainty in its own answers, rather
than imitating human examples. To our knowledge, this is the first time a model
has been shown to express calibrated uncertainty about its own answers in
natural language. For testing calibration, we introduce the CalibratedMath
suite of tasks. We compare the calibration of uncertainty expressed in words
("verbalized probability") to uncertainty extracted from model logits. Both
kinds of uncertainty are capable of generalizing calibration under distribution
shift. We also provide evidence that GPT-3's ability to generalize calibration
depends on pre-trained latent representations that correlate with epistemic
uncertainty over its answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic-Aware Evaluation and Transformer Methods for Topic-Controllable Summarization. (arXiv:2206.04317v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04317">
<div class="article-summary-box-inner">
<span><p>Topic-controllable summarization is an emerging research area with a wide
range of potential applications. However, existing approaches suffer from
significant limitations. First, there is currently no established evaluation
metric for this task. Furthermore, existing methods built upon recurrent
architectures, which can significantly limit their performance compared to more
recent Transformer-based architectures, while they also require modifications
to the model's architecture for controlling the topic. In this work, we propose
a new topic-oriented evaluation measure to automatically evaluate the generated
summaries based on the topic affinity between the generated summary and the
desired topic. We also conducted a user study that validates the reliability of
this measure. Finally, we propose simple, yet powerful methods for
topic-controllable summarization either incorporating topic embeddings into the
model's architecture or employing control tokens to guide the summary
generation. Experimental results show that control tokens can achieve better
performance compared to more complicated embedding-based approaches while being
at the same time significantly faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SsciBERT: A Pre-trained Language Model for Social Science Texts. (arXiv:2206.04510v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04510">
<div class="article-summary-box-inner">
<span><p>The academic literature of social sciences is the literature that records
human civilization and studies human social problems. With the large-scale
growth of this literature, ways to quickly find existing research on relevant
issues have become an urgent demand for researchers. Previous studies, such as
SciBERT, have shown that pre-training using domain-specific texts can improve
the performance of natural language processing tasks in those fields. However,
there is no pre-trained language model for social sciences, so this paper
proposes a pre-trained model on many abstracts published in the Social Science
Citation Index (SSCI) journals. The models, which are available on Github
(https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT), show excellent
performance on discipline classification and abstract structure-function
recognition tasks with the social sciences literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction. (arXiv:2206.05238v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05238">
<div class="article-summary-box-inner">
<span><p>The most prominent tasks in emotion analysis are to assign emotions to texts
and to understand how emotions manifest in language. An important observation
for natural language processing is that emotions can be communicated implicitly
by referring to events alone, appealing to an empathetic, intersubjective
understanding of events, even without explicitly mentioning an emotion name. In
psychology, the class of emotion theories known as appraisal theories aims at
explaining the link between events and emotions. Appraisals can be formalized
as variables that measure a cognitive evaluation by people living through an
event that they consider relevant. They include the assessment if an event is
novel, if the person considers themselves to be responsible, if it is in line
with the own goals, and many others. Such appraisals explain which emotions are
developed based on an event, e.g., that a novel situation can induce surprise
or one with uncertain consequences could evoke fear. We analyze the suitability
of appraisal theories for emotion analysis in text with the goal of
understanding if appraisal concepts can reliably be reconstructed by
annotators, if they can be predicted by text classifiers, and if appraisal
concepts help to identify emotion categories. To achieve that, we compile a
corpus by asking people to textually describe events that triggered particular
emotions and to disclose their appraisals. Then, we ask readers to reconstruct
emotions and appraisals from the text. This setup allows us to measure if
emotions and appraisals can be recovered purely from text and provides a human
baseline to judge model's performance measures. Our comparison of text
classification methods to human annotators shows that both can reliably detect
emotions and appraisals with similar performance. We further show that
appraisal concepts improve the categorization of emotions in text.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-temporal Concept based Explanation of 3D ConvNets. (arXiv:2206.05275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05275">
<div class="article-summary-box-inner">
<span><p>Recent studies have achieved outstanding success in explaining 2D image
recognition ConvNets. On the other hand, due to the computation cost and
complexity of video data, the explanation of 3D video recognition ConvNets is
relatively less studied. In this paper, we present a 3D ACE (Automatic
Concept-based Explanation) framework for interpreting 3D ConvNets. In our
approach: (1) videos are represented using high-level supervoxels, which is
straightforward for human to understand; and (2) the interpreting framework
estimates a score for each voxel, which reflects its importance in the decision
procedure. Experiments show that our method can discover spatial-temporal
concepts of different importance-levels, and thus can explore the influence of
the concepts on a target task, such as action classification, in-depth. The
codes are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Superresolution and Segmentation of OCT scans using Multi-Stage adversarial Guided Attention Training. (arXiv:2206.05277v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05277">
<div class="article-summary-box-inner">
<span><p>Optical coherence tomography (OCT) is one of the non-invasive and
easy-to-acquire biomarkers (the thickness of the retinal layers, which is
detectable within OCT scans) being investigated to diagnose Alzheimer's disease
(AD). This work aims to segment the OCT images automatically; however, it is a
challenging task due to various issues such as the speckle noise, small target
region, and unfavorable imaging conditions. In our previous work, we have
proposed the multi-stage &amp; multi-discriminatory generative adversarial network
(MultiSDGAN) to translate OCT scans in high-resolution segmentation labels. In
this investigation, we aim to evaluate and compare various combinations of
channel and spatial attention to the MultiSDGAN architecture to extract more
powerful feature maps by capturing rich contextual relationships to improve
segmentation performance. Moreover, we developed and evaluated a guided
mutli-stage attention framework where we incorporated a guided attention
mechanism by forcing an L-1 loss between a specifically designed binary mask
and the generated attention maps. Our ablation study results on the WVU-OCT
data-set in five-fold cross-validation (5-CV) suggest that the proposed
MultiSDGAN with a serial attention module provides the most competitive
performance, and guiding the spatial attention feature maps by binary masks
further improves the performance in our proposed network. Comparing the
baseline model with adding the guided-attention, our results demonstrated
relative improvements of 21.44% and 19.45% on the Dice coefficient and SSIM,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Branch Squeeze-Fusion-Excitation Module for Cross-Modality Registration of Cardiac SPECT and CT. (arXiv:2206.05278v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05278">
<div class="article-summary-box-inner">
<span><p>Single-photon emission computed tomography (SPECT) is a widely applied
imaging approach for diagnosis of coronary artery diseases. Attenuation maps
(u-maps) derived from computed tomography (CT) are utilized for attenuation
correction (AC) to improve diagnostic accuracy of cardiac SPECT. However, SPECT
and CT are obtained sequentially in clinical practice, which potentially
induces misregistration between the two scans. Convolutional neural networks
(CNN) are powerful tools for medical image registration. Previous CNN-based
methods for cross-modality registration either directly concatenated two input
modalities as an early feature fusion or extracted image features using two
separate CNN modules for a late fusion. These methods do not fully extract or
fuse the cross-modality information. Besides, deep-learning-based rigid
registration of cardiac SPECT and CT-derived u-maps has not been investigated
before. In this paper, we propose a Dual-Branch Squeeze-Fusion-Excitation
(DuSFE) module for the registration of cardiac SPECT and CT-derived u-maps.
DuSFE fuses the knowledge from multiple modalities to recalibrate both
channel-wise and spatial features for each modality. DuSFE can be embedded at
multiple convolutional layers to enable feature fusion at different spatial
dimensions. Our studies using clinical data demonstrated that a network
embedded with DuSFE generated substantial lower registration errors and
therefore more accurate AC SPECT images than previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PILC: Practical Image Lossless Compression with an End-to-end GPU Oriented Neural Framework. (arXiv:2206.05279v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05279">
<div class="article-summary-box-inner">
<span><p>Generative model based image lossless compression algorithms have seen a
great success in improving compression ratio. However, the throughput for most
of them is less than 1 MB/s even with the most advanced AI accelerated chips,
preventing them from most real-world applications, which often require 100
MB/s. In this paper, we propose PILC, an end-to-end image lossless compression
framework that achieves 200 MB/s for both compression and decompression with a
single NVIDIA Tesla V100 GPU, 10 times faster than the most efficient one
before. To obtain this result, we first develop an AI codec that combines
auto-regressive model and VQ-VAE which performs well in lightweight setting,
then we design a low complexity entropy coder that works well with our codec.
Experiments show that our framework compresses better than PNG by a margin of
30% in multiple datasets. We believe this is an important step to bring AI
compression forward to commercial use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less Is More: Linear Layers on CLIP Features as Powerful VizWiz Model. (arXiv:2206.05281v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05281">
<div class="article-summary-box-inner">
<span><p>Current architectures for multi-modality tasks such as visual question
answering suffer from their high complexity. As a result, these architectures
are difficult to train and require high computational resources. To address
these problems we present a CLIP-based architecture that does not require any
fine-tuning of the feature extractors. A simple linear classifier is used on
the concatenated features of the image and text encoder. During training an
auxiliary loss is added which operates on the answer types. The resulting
classification is then used as an attention gate on the answer class selection.
On the VizWiz 2022 Visual Question Answering Challenge we achieve 60.15 %
accuracy on Task 1: Predict Answer to a Visual Question and AP score of 83.78 %
on Task 2: Predict Answerability of a Visual Question.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Estimate Shapley Values with Vision Transformers. (arXiv:2206.05282v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05282">
<div class="article-summary-box-inner">
<span><p>Transformers have become a default architecture in computer vision, but
understanding what drives their predictions remains a challenging problem.
Current explanation approaches rely on attention values or input gradients, but
these give a limited understanding of a model's dependencies. Shapley values
offer a theoretically sound alternative, but their computational cost makes
them impractical for large, high-dimensional models. In this work, we aim to
make Shapley values practical for vision transformers (ViTs). To do so, we
first leverage an attention masking approach to evaluate ViTs with partial
information, and we then develop a procedure for generating Shapley value
explanations via a separate, learned explainer model. Our experiments compare
Shapley values to many baseline methods (e.g., attention rollout, GradCAM,
LRP), and we find that our approach provides more accurate explanations than
any existing method for ViTs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Poissonian Blurred Image Deconvolution by Framelet based Local Minimal Prior. (arXiv:2206.05283v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05283">
<div class="article-summary-box-inner">
<span><p>Image production tools do not always create a clear image, noisy and blurry
images are sometimes created. Among these cases, Poissonian noise is one of the
most famous noises that appear in medical images and images taken in astronomy.
Blurred image with Poissonian noise obscures important details that are of
great importance in medicine or astronomy. Therefore, studying and increasing
the quality of images that are affected by this type of noise is always
considered by researchers. In this paper, in the first step, based on framelet
transform, a local minimal prior is introduced, and in the next step, this tool
together with fractional calculation is used for Poissonian blurred image
deconvolution. In the following, the model is generalized to the blind case. To
evaluate the performance of the presented model, several images such as real
images have been investigated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupling Predictions in Distributed Learning for Multi-Center Left Atrial MRI Segmentation. (arXiv:2206.05284v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05284">
<div class="article-summary-box-inner">
<span><p>Distributed learning has shown great potential in medical image analysis. It
allows to use multi-center training data with privacy protection. However, data
distributions in local centers can vary from each other due to different
imaging vendors, and annotation protocols. Such variation degrades the
performance of learning-based methods. To mitigate the influence, two groups of
methods have been proposed for different aims, i.e., the global methods and the
personalized methods. The former are aimed to improve the performance of a
single global model for all test data from unseen centers (known as generic
data); while the latter target multiple models for each center (denoted as
local data). However, little has been researched to achieve both goals
simultaneously. In this work, we propose a new framework of distributed
learning that bridges the gap between two groups, and improves the performance
for both generic and local data. Specifically, our method decouples the
predictions for generic data and local data, via distribution-conditioned
adaptation matrices. Results on multi-center left atrial (LA) MRI segmentation
showed that our method demonstrated superior performance over existing methods
on both generic and local data. Our code is available at
https://github.com/key1589745/decouple_predict
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Labels to Priors in Capsule Endoscopy: A Prior Guided Approach for Improving Generalization with Few Labels. (arXiv:2206.05288v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05288">
<div class="article-summary-box-inner">
<span><p>The lack of generalizability of deep learning approaches for the automated
diagnosis of pathologies in Wireless Capsule Endoscopy (WCE) has prevented any
significant advantages from trickling down to real clinical practices. As a
result, disease management using WCE continues to depend on exhaustive manual
investigations by medical experts. This explains its limited use despite
several advantages. Prior works have considered using higher quality and
quantity of labels as a way of tackling the lack of generalization, however
this is hardly scalable considering pathology diversity not to mention that
labeling large datasets encumbers the medical staff additionally. We propose
using freely available domain knowledge as priors to learn more robust and
generalizable representations. We experimentally show that domain priors can
benefit representations by acting in proxy of labels, thereby significantly
reducing the labeling requirement while still enabling fully unsupervised yet
pathology-aware learning. We use the contrastive objective along with
prior-guided views during pretraining, where the view choices inspire
sensitivity to pathological information. Extensive experiments on three
datasets show that our method performs better than (or closes gap with) the
state-of-the-art in the domain, establishing a new benchmark in pathology
classification and cross-dataset generalization, as well as scaling to unseen
pathology categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localized adversarial artifacts for compressed sensing MRI. (arXiv:2206.05289v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05289">
<div class="article-summary-box-inner">
<span><p>As interest in deep neural networks (DNNs) for image reconstruction tasks
grows, their reliability has been called into question (Antun et al., 2020;
Gottschling et al., 2020). However, recent work has shown that compared to
total variation (TV) minimization, they show similar robustness to adversarial
noise in terms of $\ell^2$-reconstruction error (Genzel et al., 2022). We
consider a different notion of robustness, using the $\ell^\infty$-norm, and
argue that localized reconstruction artifacts are a more relevant defect than
the $\ell^2$-error. We create adversarial perturbations to undersampled MRI
measurements which induce severe localized artifacts in the TV-regularized
reconstruction. The same attack method is not as effective against DNN based
reconstruction. Finally, we show that this phenomenon is inherent to
reconstruction methods for which exact recovery can be guaranteed, as with
compressed sensing reconstructions with $\ell^1$- or TV-minimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProActive: Self-Attentive Temporal Point Process Flows for Activity Sequences. (arXiv:2206.05291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05291">
<div class="article-summary-box-inner">
<span><p>Any human activity can be represented as a temporal sequence of actions
performed to achieve a certain goal. Unlike machine-made time series, these
action sequences are highly disparate as the time taken to finish a similar
action might vary between different persons. Therefore, understanding the
dynamics of these sequences is essential for many downstream tasks such as
activity length prediction, goal prediction, etc. Existing neural approaches
that model an activity sequence are either limited to visual data or are task
specific, i.e., limited to next action or goal prediction. In this paper, we
present ProActive, a neural marked temporal point process (MTPP) framework for
modeling the continuous-time distribution of actions in an activity sequence
while simultaneously addressing three high-impact problems -- next action
prediction, sequence-goal prediction, and end-to-end sequence generation.
Specifically, we utilize a self-attention module with temporal normalizing
flows to model the influence and the inter-arrival times between actions in a
sequence. Moreover, for time-sensitive prediction, we perform an early
detection of sequence goal via a constrained margin-based optimization
procedure. This in-turn allows ProActive to predict the sequence goal using a
limited number of actions. Extensive experiments on sequences derived from
three activity recognition datasets show the significant accuracy boost of
ProActive over the state-of-the-art in terms of action and goal prediction, and
the first-ever application of end-to-end action sequence generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EigenFairing: 3D Model Fairing using Image Coherence. (arXiv:2206.05309v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05309">
<div class="article-summary-box-inner">
<span><p>A surface is often modeled as a triangulated mesh of 3D points and textures
associated with faces of the mesh. The 3D points could be either sampled from
range data or derived from a set of images using a stereo or
Structure-from-Motion algorithm. When the points do not lie at critical points
of maximum curvature or discontinuities of the real surface, faces of the mesh
do not lie close to the modeled surface. This results in textural artifacts,
and the model is not perfectly coherent with a set of actual images -- the ones
that are used to texture-map its mesh. This paper presents a technique for
perfecting the 3D surface model by repositioning its vertices so that it is
coherent with a set of observed images of the object. The textural artifacts
and incoherence with images are due to the non-planarity of a surface patch
being approximated by a planar face, as observed from multiple viewpoints.
Image areas from the viewpoints are used to represent texture for the patch in
Eigenspace. The Eigenspace representation captures variations of texture, which
we seek to minimize. A coherence measure based on the difference between the
face textures reconstructed from Eigenspace and the actual images is used to
reposition the vertices so that the model is improved or faired. We refer to
this technique of model refinement as EigenFairing, by which the model is
faired, both geometrically and texturally, to better approximate the real
surface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Instance Identification in Dynamic Environments. (arXiv:2206.05319v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05319">
<div class="article-summary-box-inner">
<span><p>We study the problem of identifying object instances in a dynamic environment
where people interact with the objects. In such an environment, objects'
appearance changes dynamically by interaction with other entities, occlusion by
hands, background change, etc. This leads to a larger intra-instance variation
of appearance than in static environments. To discover the challenges in this
setting, we newly built a benchmark of more than 1,500 instances built on the
EPIC-KITCHENS dataset which includes natural activities and conducted an
extensive analysis of it. Experimental results suggest that (i) robustness
against instance-specific appearance change (ii) integration of low-level
(e.g., color, texture) and high-level (e.g., object category) features (iii)
foreground feature selection on overlapping objects are required for further
improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory Classifiers: Two-stage Classification for Robustness in Machine Learning. (arXiv:2206.05323v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05323">
<div class="article-summary-box-inner">
<span><p>The performance of machine learning models can significantly degrade under
distribution shifts of the data. We propose a new method for classification
which can improve robustness to distribution shifts, by combining expert
knowledge about the ``high-level" structure of the data with standard
classifiers. Specifically, we introduce two-stage classifiers called
\textit{memory classifiers}. First, these identify prototypical data points --
\textit{memories} -- to cluster the training data. This step is based on
features designed with expert guidance; for instance, for image data they can
be extracted using digital image processing algorithms. Then, within each
cluster, we learn local classifiers based on finer discriminating features, via
standard models like deep neural networks. We establish generalization bounds
for memory classifiers. We illustrate in experiments that they can improve
generalization and robustness to distribution shifts on image datasets. We show
improvements which push beyond standard data augmentation techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Rendering of Neural SDFs through Reparameterization. (arXiv:2206.05344v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05344">
<div class="article-summary-box-inner">
<span><p>We present a method to automatically compute correct gradients with respect
to geometric scene parameters in neural SDF renderers. Recent physically-based
differentiable rendering techniques for meshes have used edge-sampling to
handle discontinuities, particularly at object silhouettes, but SDFs do not
have a simple parametric form amenable to sampling. Instead, our approach
builds on area-sampling techniques and develops a continuous warping function
for SDFs to account for these discontinuities. Our method leverages the
distance to surface encoded in an SDF and uses quadrature on sphere tracer
points to compute this warping function. We further show that this can be done
by subsampling the points to make the method tractable for neural SDFs. Our
differentiable renderer can be used to optimize neural shapes from multi-view
images and produces comparable 3D reconstructions to recent SDF-based inverse
rendering methods, without the need for 2D segmentation masks to guide the
geometry optimization and no volumetric approximations to the geometry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Detection, Recognition, Deep Learning, and the Universal Law of Generalization. (arXiv:2206.05365v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05365">
<div class="article-summary-box-inner">
<span><p>Object detection and recognition are fundamental functions underlying the
success of species. Because the appearance of an object exhibits a large
variability, the brain has to group these different stimuli under the same
object identity, a process of generalization. Does the process of
generalization follow some general principles or is it an ad-hoc
"bag-of-tricks"? The Universal Law of Generalization provided evidence that
generalization follows similar properties across a variety of species and
tasks. Here we test the hypothesis that the internal representations underlying
generalization reflect the natural properties of object detection and
recognition in our environment rather than the specifics of the system solving
these problems. By training a deep-neural-network with images of "clear" and
"camouflaged" animals, we found that with a proper choice of category
prototypes, the generalization functions are monotone decreasing, similar to
the generalization functions of biological systems. Our findings support the
hypothesis of the study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Neural Radiance Fields for Novel View Synthesis with Transformer. (arXiv:2206.05375v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05375">
<div class="article-summary-box-inner">
<span><p>We propose a Transformer-based NeRF (TransNeRF) to learn a generic neural
radiance field conditioned on observed-view images for the novel view synthesis
task. By contrast, existing MLP-based NeRFs are not able to directly receive
observed views with an arbitrary number and require an auxiliary pooling-based
operation to fuse source-view information, resulting in the missing of
complicated relationships between source views and the target rendering view.
Furthermore, current approaches process each 3D point individually and ignore
the local consistency of a radiance field scene representation. These
limitations potentially can reduce their performance in challenging real-world
applications where large differences between source views and a novel rendering
view may exist. To address these challenges, our TransNeRF utilizes the
attention mechanism to naturally decode deep associations of an arbitrary
number of source views into a coordinate-based scene representation. Local
consistency of shape and appearance are considered in the ray-cast space and
the surrounding-view space within a unified Transformer network. Experiments
demonstrate that our TransNeRF, trained on a wide variety of scenes, can
achieve better performance in comparison to state-of-the-art image-based neural
rendering methods in both scene-agnostic and per-scene finetuning scenarios
especially when there is a considerable gap between source views and a
rendering view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast building segmentation from satellite imagery and few local labels. (arXiv:2206.05377v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05377">
<div class="article-summary-box-inner">
<span><p>Innovations in computer vision algorithms for satellite image analysis can
enable us to explore global challenges such as urbanization and land use change
at the planetary level. However, domain shift problems are a common occurrence
when trying to replicate models that drive these analyses to new areas,
particularly in the developing world. If a model is trained with imagery and
labels from one location, then it usually will not generalize well to new
locations where the content of the imagery and data distributions are
different. In this work, we consider the setting in which we have a single
large satellite imagery scene over which we want to solve an applied problem --
building footprint segmentation. Here, we do not necessarily need to worry
about creating a model that generalizes past the borders of our scene but can
instead train a local model. We show that surprisingly few labels are needed to
solve the building segmentation problem with very high-resolution (0.5m/px)
satellite imagery with this setting in mind. Our best model trained with just
527 sparse polygon annotations (an equivalent of 1500 x 1500 densely labeled
pixels) has a recall of 0.87 over held out footprints and a R2 of 0.93 on the
task of counting the number of buildings in 200 x 200-meter windows. We apply
our models over high-resolution imagery in Amman, Jordan in a case study on
urban change detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Benchmark for Compositional Visual Reasoning. (arXiv:2206.05379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05379">
<div class="article-summary-box-inner">
<span><p>A fundamental component of human vision is our ability to parse complex
visual scenes and judge the relations between their constituent objects. AI
benchmarks for visual reasoning have driven rapid progress in recent years with
state-of-the-art systems now reaching human accuracy on some of these
benchmarks. Yet, a major gap remains in terms of the sample efficiency with
which humans and AI systems learn new visual reasoning tasks. Humans'
remarkable efficiency at learning has been at least partially attributed to
their ability to harness compositionality -- such that they can efficiently
take advantage of previously gained knowledge when learning new tasks. Here, we
introduce a novel visual reasoning benchmark, Compositional Visual Relations
(CVR), to drive progress towards the development of more data-efficient
learning algorithms. We take inspiration from fluidic intelligence and
non-verbal reasoning tests and describe a novel method for creating
compositions of abstract rules and associated image datasets at scale. Our
proposed benchmark includes measures of sample efficiency, generalization and
transfer across task rules, as well as the ability to leverage
compositionality. We systematically evaluate modern neural architectures and
find that, surprisingly, convolutional architectures surpass transformer-based
architectures across all performance measures in most data regimes. However,
all computational models are a lot less data efficient compared to humans even
after learning informative visual representations using self-supervision.
Overall, we hope that our challenge will spur interest in the development of
neural architectures that can learn to harness compositionality toward more
efficient learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Self-Supervised Fish Segmentation in Underwater Videos. (arXiv:2206.05390v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05390">
<div class="article-summary-box-inner">
<span><p>Underwater fish segmentation to estimate fish body measurements is still
largely unsolved due to the complex underwater environment. Relying on
fully-supervised segmentation models requires collecting per-pixel labels,
which is time-consuming and prone to overfitting. Self-supervised learning
methods can help avoid the requirement of large annotated training datasets,
however, to be useful in real-world applications, they should achieve good
segmentation quality. In this paper, we introduce a Transformer-based method
that uses self-supervision for high-quality fish segmentation. Our proposed
model is trained on videos -- without any annotations -- to perform fish
segmentation in underwater videos taken in situ in the wild. We show that when
trained on a set of underwater videos from one dataset, the proposed model
surpasses previous CNN-based and Transformer-based self-supervised methods and
achieves performance relatively close to supervised methods on two new unseen
underwater video datasets. This demonstrates the great generalisability of our
model and the fact that it does not need a pre-trained model. In addition, we
show that, due to its dense representation learning, our model is
compute-efficient. We provide quantitative and qualitative results that
demonstrate our model's significant capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey. (arXiv:2206.05394v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05394">
<div class="article-summary-box-inner">
<span><p>Marine ecosystems and their fish habitats are becoming increasingly important
due to their integral role in providing a valuable food source and conservation
outcomes. Due to their remote and difficult to access nature, marine
environments and fish habitats are often monitored using underwater cameras.
These cameras generate a massive volume of digital data, which cannot be
efficiently analysed by current manual processing methods, which involve a
human observer. DL is a cutting-edge AI technology that has demonstrated
unprecedented performance in analysing visual data. Despite its application to
a myriad of domains, its use in underwater fish habitat monitoring remains
under explored. In this paper, we provide a tutorial that covers the key
concepts of DL, which help the reader grasp a high-level understanding of how
DL works. The tutorial also explains a step-by-step procedure on how DL
algorithms should be developed for challenging applications such as underwater
fish monitoring. In addition, we provide a comprehensive survey of key deep
learning techniques for fish habitat monitoring including classification,
counting, localization, and segmentation. Furthermore, we survey publicly
available underwater fish datasets, and compare various DL techniques in the
underwater fish monitoring domains. We also discuss some challenges and
opportunities in the emerging field of deep learning for fish habitat
processing. This paper is written to serve as a tutorial for marine scientists
who would like to grasp a high-level understanding of DL, develop it for their
applications by following our step-by-step tutorial, and see how it is evolving
to facilitate their research efforts. At the same time, it is suitable for
computer scientists who would like to survey state-of-the-art DL-based
methodologies for fish habitat monitoring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E$^2$PN: Efficient SE(3)-Equivariant Point Network. (arXiv:2206.05398v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05398">
<div class="article-summary-box-inner">
<span><p>This paper proposes a new point-cloud convolution structure that learns
SE(3)-equivariant features. Compared with existing SE(3)-equivariant networks,
our design is lightweight, simple, and flexible to be incorporated into general
point-cloud learning networks. We strike a balance between the complexity and
capacity of our model by selecting an unconventional domain for the feature
maps. We further reduce the computational load by properly discretizing
$\mathbb{R}^3$ to fully leverage the rotational symmetry. Moreover, we employ a
permutation layer to recover the full SE(3) group from its quotient space.
Experiments show that our method achieves comparable or superior performance in
various tasks while consuming much less memory and running faster than existing
work. The proposed method can foster the adoption of equivariant feature
learning in various practical applications based on point clouds and inspire
future developments of equivariant feature learning for real-world
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Definition Map Generation Technologies For Autonomous Driving: A Review. (arXiv:2206.05400v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05400">
<div class="article-summary-box-inner">
<span><p>Autonomous driving has been among the most popular and challenging topics in
the past few years. On the road to achieving full autonomy, researchers have
utilized various sensors, such as LiDAR, camera, Inertial Measurement Unit
(IMU), and GPS, and developed intelligent algorithms for autonomous driving
applications such as object detection, object segmentation, obstacle avoidance,
and path planning. High-definition (HD) maps have drawn lots of attention in
recent years. Because of the high precision and informative level of HD maps in
localization, it has immediately become one of the critical components of
autonomous driving. From big organizations like Baidu Apollo, NVIDIA, and
TomTom to individual researchers, researchers have created HD maps for
different scenes and purposes for autonomous driving. It is necessary to review
the state-of-the-art methods for HD map generation. This paper reviews recent
HD map generation technologies that leverage both 2D and 3D map generation.
This review introduces the concept of HD maps and their usefulness in
autonomous driving and gives a detailed overview of HD map generation
techniques. We will also discuss the limitations of the current HD map
generation technologies to motivate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VAC2: Visual Analysis of Combined Causality in Event Sequences. (arXiv:2206.05420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05420">
<div class="article-summary-box-inner">
<span><p>Identifying causality behind complex systems plays a significant role in
different domains, such as decision making, policy implementations, and
management recommendations. However, existing causality studies on temporal
event sequences data mainly focus on individual causal discovery, which is
incapable of exploiting combined causality. To fill the absence of combined
causes discovery on temporal event sequence data,eliminating and recruiting
principles are defined to balance the effectiveness and controllability on
cause combinations. We also leverage the Granger causality algorithm based on
the reactive point processes to describe impelling or inhibiting behavior
patterns among entities. In addition, we design an informative and aesthetic
visual metaphor of "electrocircuit" to encode aggregated causality for ensuring
our causality visualization is non-overlapping and non-intersecting. Diverse
sorting strategies and aggregation layout are also embedded into our
parallel-based, directed and weighted hypergraph for illustrating combined
causality. Our developed combined causality visual analysis system can help
users effectively explore combined causes as well as an individual cause. This
interactive system supports multi-level causality exploration with diverse
ordering strategies and a focus and context technique to help users obtain
different levels of information abstraction. The usefulness and effectiveness
of the system are further evaluated by conducting a pilot user study and two
case studies on event sequence data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Access Control of Semantic Segmentation Models Using Encrypted Feature Maps. (arXiv:2206.05422v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05422">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an access control method with a secret key for
semantic segmentation models for the first time so that unauthorized users
without a secret key cannot benefit from the performance of trained models. The
method enables us not only to provide a high segmentation performance to
authorized users but to also degrade the performance for unauthorized users. We
first point out that, for the application of semantic segmentation,
conventional access control methods which use encrypted images for
classification tasks are not directly applicable due to performance
degradation. Accordingly, in this paper, selected feature maps are encrypted
with a secret key for training and testing models, instead of input images. In
an experiment, the protected models allowed authorized users to obtain almost
the same performance as that of non-protected models but also with robustness
against unauthorized access without a key.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Precise Affordance Annotation for Egocentric Action Video Datasets. (arXiv:2206.05424v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05424">
<div class="article-summary-box-inner">
<span><p>Object affordance is an important concept in human-object interaction,
providing information on action possibilities based on human motor capacity and
objects' physical property thus benefiting tasks such as action anticipation
and robot imitation learning. However, existing datasets often: 1) mix up
affordance with object functionality; 2) confuse affordance with goal-related
action; and 3) ignore human motor capacity. This paper proposes an efficient
annotation scheme to address these issues by combining goal-irrelevant motor
actions and grasp types as affordance labels and introducing the concept of
mechanical action to represent the action possibilities between two objects. We
provide new annotations by applying this scheme to the EPIC-KITCHENS dataset
and test our annotation with tasks such as affordance recognition. We
qualitatively verify that models trained with our annotation can distinguish
affordance and mechanical actions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned reconstruction with convergence guarantees. (arXiv:2206.05431v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05431">
<div class="article-summary-box-inner">
<span><p>In recent years, deep learning has achieved remarkable empirical success for
image reconstruction. This has catalyzed an ongoing quest for precise
characterization of correctness and reliability of data-driven methods in
critical use-cases, for instance in medical imaging. Notwithstanding the
excellent performance and efficacy of deep learning-based methods, concerns
have been raised regarding their stability, or lack thereof, with serious
practical implications. Significant advances have been made in recent years to
unravel the inner workings of data-driven image recovery methods, challenging
their widely perceived black-box nature. In this article, we will specify
relevant notions of convergence for data-driven image reconstruction, which
will form the basis of a survey of learned methods with mathematically rigorous
reconstruction guarantees. An example that is highlighted is the role of ICNN,
offering the possibility to combine the power of deep learning with classical
convex regularization theory for devising methods that are provably convergent.
</p>
<p>This survey article is aimed at both methodological researchers seeking to
advance the frontiers of our understanding of data-driven image reconstruction
methods as well as practitioners, by providing an accessible description of
convergence concepts and by placing some of the existing empirical practices on
a solid mathematical foundation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Luminance-Guided Chrominance Image Enhancement for HEVC Intra Coding. (arXiv:2206.05432v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05432">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a luminance-guided chrominance image enhancement
convolutional neural network for HEVC intra coding. Specifically, we firstly
develop a gated recursive asymmetric-convolution block to restore each degraded
chrominance image, which generates an intermediate output. Then, guided by the
luminance image, the quality of this intermediate output is further improved,
which finally produces the high-quality chrominance image. When our proposed
method is adopted in the compression of color images with HEVC intra coding, it
achieves 28.96% and 16.74% BD-rate gains over HEVC for the U and V images,
respectively, which accordingly demonstrate its superiority.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Projection from Optical Coherence Tomography B-Scan without Retinal Layer Segmentation Supervision. (arXiv:2206.05472v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05472">
<div class="article-summary-box-inner">
<span><p>Projection map (PM) from optical coherence tomography (OCT) B-scan is an
important tool to diagnose retinal diseases, which typically requires retinal
layer segmentation. In this study, we present a novel end-to-end framework to
predict PMs from B-scans. Instead of segmenting retinal layers explicitly, we
represent them implicitly as predicted coordinates. By pixel interpolation on
uniformly sampled coordinates between retinal layers, the corresponding PMs
could be easily obtained with pooling. Notably, all the operators are
differentiable; therefore, this Differentiable Projection Module (DPM) enables
end-to-end training with the ground truth of PMs rather than retinal layer
segmentation. Our framework produces high-quality PMs, significantly
outperforming baselines, including a vanilla CNN without DPM and an
optimization-based DPM without a deep prior. Furthermore, the proposed DPM, as
a novel neural representation of areas/volumes between curves/surfaces, could
be of independent interest for geometric deep learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kaggle Kinship Recognition Challenge: Introduction of Convolution-Free Model to boost conventional. (arXiv:2206.05488v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05488">
<div class="article-summary-box-inner">
<span><p>This work aims to explore a convolution-free base classifier that can be used
to widen the variations of the conventional ensemble classifier. Specifically,
we propose Vision Transformers as base classifiers to combine with CNNs for a
unique ensemble solution in Kaggle kinship recognition. In this paper, we
verify our proposed idea by implementing and optimizing variants of the Vision
Transformer model on top of the existing CNN models. The combined models
achieve better scores than conventional ensemble classifiers based solely on
CNN variants. We demonstrate that highly optimized CNN ensembles publicly
available on the Kaggle Discussion board can easily achieve a significant boost
in ROC score by simply ensemble with variants of the Vision Transformer model
due to low correlation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Evaluation of OCR on Egocentric Data. (arXiv:2206.05496v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05496">
<div class="article-summary-box-inner">
<span><p>In this paper, we evaluate state-of-the-art OCR methods on Egocentric data.
We annotate text in EPIC-KITCHENS images, and demonstrate that existing OCR
methods struggle with rotated text, which is frequently observed on objects
being handled. We introduce a simple rotate-and-merge procedure which can be
applied to pre-trained OCR models that halves the normalized edit distance
error. This suggests that future OCR attempts should incorporate rotation into
model design and training procedures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Causality for Learning Algorithms in Medical Image Analysis. (arXiv:2206.05498v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05498">
<div class="article-summary-box-inner">
<span><p>Medical image analysis is a vibrant research area that offers doctors and
medical practitioners invaluable insight and the ability to accurately diagnose
and monitor disease. Machine learning provides an additional boost for this
area. However, machine learning for medical image analysis is particularly
vulnerable to natural biases like domain shifts that affect algorithmic
performance and robustness. In this paper we analyze machine learning for
medical image analysis within the framework of Technology Readiness Levels and
review how causal analysis methods can fill a gap when creating robust and
adaptable medical image analysis algorithms. We review methods using causality
in medical imaging AI/ML and find that causal analysis has the potential to
mitigate critical problems for clinical translation but that uptake and
clinical downstream research has been limited so far.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Real-world Single Image Deraining: A New Benchmark and Beyond. (arXiv:2206.05514v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05514">
<div class="article-summary-box-inner">
<span><p>Single image deraining (SID) in real scenarios attracts increasing attention
in recent years. Due to the difficulty in obtaining real-world rainy/clean
image pairs, previous real datasets suffer from low-resolution images,
homogeneous rain streaks, limited background variation, and even misalignment
of image pairs, resulting in incomprehensive evaluation of SID methods. To
address these issues, we establish a new high-quality dataset named
RealRain-1k, consisting of $1,120$ high-resolution paired clean and rainy
images with low- and high-density rain streaks, respectively. Images in
RealRain-1k are automatically generated from a large number of real-world rainy
video clips through a simple yet effective rain density-controllable filtering
method, and have good properties of high image resolution, background
diversity, rain streaks variety, and strict spatial alignment. RealRain-1k also
provides abundant rain streak layers as a byproduct, enabling us to build a
large-scale synthetic dataset named SynRain-13k by pasting the rain streak
layers on abundant natural images. Based on them and existing datasets, we
benchmark more than 10 representative SID methods on three tracks: (1) fully
supervised learning on RealRain-1k, (2) domain generalization to real datasets,
and (3) syn-to-real transfer learning. The experimental results (1) show the
difference of representative methods in image restoration performance and model
complexity, (2) validate the significance of the proposed datasets for model
generalization, and (3) provide useful insights on the superiority of learning
from diverse domains and shed lights on the future research on real-world SID.
The datasets will be released at https://github.com/hiker-lw/RealRain-1k
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-Based MR Image Re-parameterization. (arXiv:2206.05516v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05516">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance (MR) image re-parameterization refers to the process of
generating via simulations of an MR image with a new set of MRI scanning
parameters. Different parameter values generate distinct contrast between
different tissues, helping identify pathologic tissue. Typically, more than one
scan is required for diagnosis; however, acquiring repeated scans can be
costly, time-consuming, and difficult for patients. Thus, using MR image
re-parameterization to predict and estimate the contrast in these imaging scans
can be an effective alternative. In this work, we propose a novel deep learning
(DL) based convolutional model for MRI re-parameterization. Based on our
preliminary results, DL-based techniques hold the potential to learn the
non-linearities that govern the re-parameterization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Two-stage Method for Non-extreme Value Salt-and-Pepper Noise Removal. (arXiv:2206.05520v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05520">
<div class="article-summary-box-inner">
<span><p>There are several previous methods based on neural network can have great
performance in denoising salt and pepper noise. However, those methods are
based on a hypothesis that the value of salt and pepper noise is exactly 0 and
255. It is not true in the real world. The result of those methods deviate
sharply when the value is different from 0 and 255. To overcome this weakness,
our method aims at designing a convolutional neural network to detect the noise
pixels in a wider range of value and then a filter is used to modify pixel
value to 0, which is beneficial for further filtering. Additionally, another
convolutional neural network is used to conduct the denoising and restoration
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simplified Un-Supervised Learning Based Approach for Ink Mismatch Detection in Handwritten Hyper-Spectral Document Images. (arXiv:2206.05539v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05539">
<div class="article-summary-box-inner">
<span><p>Hyper-spectral imaging has become the latest trend in the field of optical
imaging systems. Among various other applications, hyper-spectral imaging has
been widely used for analysis of printed and handwritten documents. This paper
proposes an efficient technique for estimating the number of different but
visibly similar inks present in a Hyper spectral Document Image. Our approach
is based on un-supervised learning and does not require any prior knowledge of
the dataset. The algorithm was tested on the iVision HHID dataset and has
achieved comparable results with the state of the algorithms present in the
literature. This work can prove to be effective when employed during the early
stages of forgery detection in Hyper-spectral Document Images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surround-View Cameras based Holistic Visual Perception for Automated Driving. (arXiv:2206.05542v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05542">
<div class="article-summary-box-inner">
<span><p>The formation of eyes led to the big bang of evolution. The dynamics changed
from a primitive organism waiting for the food to come into contact for eating
food being sought after by visual sensors. The human eye is one of the most
sophisticated developments of evolution, but it still has defects. Humans have
evolved a biological perception algorithm capable of driving cars, operating
machinery, piloting aircraft, and navigating ships over millions of years.
Automating these capabilities for computers is critical for various
applications, including self-driving cars, augmented reality, and architectural
surveying. Near-field visual perception in the context of self-driving cars can
perceive the environment in a range of $0-10$ meters and 360{\deg} coverage
around the vehicle. It is a critical decision-making component in the
development of safer automated driving. Recent advances in computer vision and
deep learning, in conjunction with high-quality sensors such as cameras and
LiDARs, have fueled mature visual perception solutions. Until now, far-field
perception has been the primary focus. Another significant issue is the limited
processing power available for developing real-time applications. Because of
this bottleneck, there is frequently a trade-off between performance and
run-time efficiency. We concentrate on the following issues in order to address
them: 1) Developing near-field perception algorithms with high performance and
low computational complexity for various visual perception tasks such as
geometric and semantic tasks using convolutional neural networks. 2) Using
Multi-Task Learning to overcome computational bottlenecks by sharing initial
convolutional layers between tasks and developing optimization strategies that
balance tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Continuous Learning Framework for Multi-modal Knowledge Discovery and Pre-training. (arXiv:2206.05555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05555">
<div class="article-summary-box-inner">
<span><p>Multi-modal pre-training and knowledge discovery are two important research
topics in multi-modal machine learning. Nevertheless, none of existing works
make attempts to link knowledge discovery with knowledge guided multi-modal
pre-training. In this paper, we propose to unify them into a continuous
learning framework for mutual improvement. Taking the open-domain uni-modal
datasets of images and texts as input, we maintain a knowledge graph as the
foundation to support these two tasks. For knowledge discovery, a pre-trained
model is used to identify cross-modal links on the graph. For model
pre-training, the knowledge graph is used as the external knowledge to guide
the model updating. These two steps are iteratively performed in our framework
for continuous learning. The experimental results on MS-COCO and Flickr30K with
respect to both knowledge discovery and the pre-trained model validate the
effectiveness of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MammoDL: Mammographic Breast Density Estimation using Federated Learning. (arXiv:2206.05575v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05575">
<div class="article-summary-box-inner">
<span><p>Assessing breast cancer risk from imaging remains a subjective process, in
which radiologists employ computer aided detection (CAD) systems or qualitative
visual assessment to estimate breast percent density (PD). More advanced
machine learning (ML) models have become the most promising way to quantify
breast cancer risk for early, accurate, and equitable diagnoses, but training
such models in medical research is often restricted to small,
single-institution data. Since patient demographics and imaging characteristics
may vary considerably across imaging sites, models trained on
single-institution data tend not to generalize well. In response to this
problem, MammoDL is proposed, an open-source software tool that leverages UNet
architecture to accurately estimate breast PD and complexity from digital
mammography (DM). With the Open Federated Learning (OpenFL) library, this
solution enables secure training on datasets across multiple institutions.
MammoDL is a leaner, more flexible model than its predecessors, boasting
improved generalization due to federation-enabled training on larger, more
representative datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine learning approaches for COVID-19 detection from chest X-ray imaging: A Systematic Review. (arXiv:2206.05615v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05615">
<div class="article-summary-box-inner">
<span><p>There is a necessity to develop affordable, and reliable diagnostic tools,
which allow containing the COVID-19 spreading. Machine Learning (ML) algorithms
have been proposed to design support decision-making systems to assess chest
X-ray images, which have proven to be useful to detect and evaluate disease
progression. Many research articles are published around this subject, which
makes it difficult to identify the best approaches for future work. This paper
presents a systematic review of ML applied to COVID-19 detection using chest
X-ray images, aiming to offer a baseline for researchers in terms of methods,
architectures, databases, and current limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning with Research Prototypes for Multi-Center MRI-based Detection of Prostate Cancer with Diverse Histopathology. (arXiv:2206.05617v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05617">
<div class="article-summary-box-inner">
<span><p>Early prostate cancer detection and staging from MRI are extremely
challenging tasks for both radiologists and deep learning algorithms, but the
potential to learn from large and diverse datasets remains a promising avenue
to increase their generalization capability both within- and across clinics. To
enable this for prototype-stage algorithms, where the majority of existing
research remains, in this paper we introduce a flexible federated learning
framework for cross-site training, validation, and evaluation of deep prostate
cancer detection algorithms. Our approach utilizes an abstracted representation
of the model architecture and data, which allows unpolished prototype deep
learning models to be trained without modification using the NVFlare federated
learning framework. Our results show increases in prostate cancer detection and
classification accuracy using a specialized neural network model and diverse
prostate biopsy data collected at two University of California research
hospitals, demonstrating the efficacy of our approach in adapting to different
datasets and improving MR-biomarker discovery. We open-source our FLtools
system, which can be easily adapted to other deep learning projects for medical
imaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic PET via Domain Translation of 3D MRI. (arXiv:2206.05618v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05618">
<div class="article-summary-box-inner">
<span><p>Historically, patient datasets have been used to develop and validate various
reconstruction algorithms for PET/MRI and PET/CT. To enable such algorithm
development, without the need for acquiring hundreds of patient exams, in this
paper we demonstrate a deep learning technique to generate synthetic but
realistic whole-body PET sinograms from abundantly-available whole-body MRI.
Specifically, we use a dataset of 56 $^{18}$F-FDG-PET/MRI exams to train a 3D
residual UNet to predict physiologic PET uptake from whole-body T1-weighted
MRI. In training we implemented a balanced loss function to generate realistic
uptake across a large dynamic range and computed losses along tomographic lines
of response to mimic the PET acquisition. The predicted PET images are forward
projected to produce synthetic PET time-of-flight (ToF) sinograms that can be
used with vendor-provided PET reconstruction algorithms, including using
CT-based attenuation correction (CTAC) and MR-based attenuation correction
(MRAC). The resulting synthetic data recapitulates physiologic $^{18}$F-FDG
uptake, e.g. high uptake localized to the brain and bladder, as well as uptake
in liver, kidneys, heart and muscle. To simulate abnormalities with high
uptake, we also insert synthetic lesions. We demonstrate that this synthetic
PET data can be used interchangeably with real PET data for the PET
quantification task of comparing CT and MR-based attenuation correction
methods, achieving $\leq 7.6\%$ error in mean-SUV compared to using real data.
These results together show that the proposed synthetic PET data pipeline can
be reasonably used for development, evaluation, and validation of PET/MRI
reconstruction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Models for Automated Classification of Dog Emotional States from Facial Expressions. (arXiv:2206.05619v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05619">
<div class="article-summary-box-inner">
<span><p>Similarly to humans, facial expressions in animals are closely linked with
emotional states. However, in contrast to the human domain, automated
recognition of emotional states from facial expressions in animals is
underexplored, mainly due to difficulties in data collection and establishment
of ground truth concerning emotional states of non-verbal users. We apply
recent deep learning techniques to classify (positive) anticipation and
(negative) frustration of dogs on a dataset collected in a controlled
experimental setting. We explore the suitability of different backbones (e.g.
ResNet, ViT) under different supervisions to this task, and find that features
of a self-supervised pretrained ViT (DINO-ViT) are superior to the other
alternatives. To the best of our knowledge, this work is the first to address
the task of automatic classification of canine emotions on data acquired in a
controlled experiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review on Plastic Artificial Neural Networks: Exploring the Intersection between Neural Architecture Search and Continual Learning. (arXiv:2206.05625v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05625">
<div class="article-summary-box-inner">
<span><p>Despite the significant advances achieved in Artificial Neural Networks
(ANNs), their design process remains notoriously tedious, depending primarily
on intuition, experience and trial-and-error. This human-dependent process is
often time-consuming and prone to errors. Furthermore, the models are generally
bound to their training contexts, with no considerations of changes to their
surrounding environments. Continual adaptability and automation of neural
networks is of paramount importance to several domains where model
accessibility is limited after deployment (e.g IoT devices, self-driving
vehicles, etc). Additionally, even accessible models require frequent
maintenance post-deployment to overcome issues such as Concept/Data Drift,
which can be cumbersome and restrictive. The current state of the art on
adaptive ANNs is still a premature area of research; nevertheless, Neural
Architecture Search (NAS), a form of AutoML, and Continual Learning (CL) have
recently gained an increasing momentum in the Deep Learning research field,
aiming to provide more robust and adaptive ANN development frameworks. This
study is the first extensive review on the intersection between AutoML and CL,
outlining research directions for the different methods that can facilitate
full automation and lifelong plasticity in ANNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Unsupervised Deep-Learning Method for Bone Age Assessment. (arXiv:2206.05641v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05641">
<div class="article-summary-box-inner">
<span><p>The bone age, reflecting the degree of development of the bones, can be used
to predict the adult height and detect endocrine diseases of children. Both
examinations of radiologists and variability of operators have a significant
impact on bone age assessment. To decrease human intervention , machine
learning algorithms are used to assess the bone age automatically. However,
conventional supervised deep-learning methods need pre-labeled data. In this
paper, based on the convolutional auto-encoder with constraints (CCAE), an
unsupervised deep-learning model proposed in the classification of the
fingerprint, we propose this model for the classification of the bone age and
baptize it BA-CCAE. In the proposed BA-CCAE model, the key regions of the raw
X-ray images of the bone age are encoded, yielding the latent vectors. The
K-means clustering algorithm is used to obtain the final classifications by
grouping the latent vectors of the bone images. A set of experiments on the
Radiological Society of North America pediatric bone age dataset (RSNA) show
that the accuracy of classifications at 48-month intervals is 76.15%. Although
the accuracy now is lower than most of the existing supervised models, the
proposed BA-CCAE model can establish the classification of bone age without any
pre-labeled data, and to the best of our knowledge, the proposed BA-CCAE is one
of the few trails using the unsupervised deep-learning method for the bone age
assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Fast Alternating Minimization Algorithm for Coded Aperture Snapshot Spectral Imaging Based on Sparsity and Deep Image Priors. (arXiv:2206.05647v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05647">
<div class="article-summary-box-inner">
<span><p>Coded aperture snapshot spectral imaging (CASSI) is a technique used to
reconstruct three-dimensional hyperspectral images (HSIs) from one or several
two-dimensional projection measurements. However, fewer projection measurements
or more spectral channels leads to a severly ill-posed problem, in which case
regularization methods have to be applied. In order to significantly improve
the accuracy of reconstruction, this paper proposes a fast alternating
minimization algorithm based on the sparsity and deep image priors (Fama-SDIP)
of natural images. By integrating deep image prior (DIP) into the principle of
compressive sensing (CS) reconstruction, the proposed algorithm can achieve
state-of-the-art results without any training dataset. Extensive experiments
show that Fama-SDIP method significantly outperforms prevailing leading methods
on simulation and real HSI datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indirect-Instant Attention Optimization for Crowd Counting in Dense Scenes. (arXiv:2206.05648v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05648">
<div class="article-summary-box-inner">
<span><p>One of appealing approaches to guiding learnable parameter optimization, such
as feature maps, is global attention, which enlightens network intelligence at
a fraction of the cost. However, its loss calculation process still falls
short: 1)We can only produce one-dimensional 'pseudo labels' for attention,
since the artificial threshold involved in the procedure is not robust; 2) The
attention awaiting loss calculation is necessarily high-dimensional, and
decreasing it by convolution will inevitably introduce additional learnable
parameters, thus confusing the source of the loss. To this end, we devise a
simple but efficient Indirect-Instant Attention Optimization (IIAO) module
based on SoftMax-Attention , which transforms high-dimensional attention map
into a one-dimensional feature map in the mathematical sense for loss
calculation midway through the network, while automatically providing adaptive
multi-scale fusion to feature pyramid module. The special transformation yields
relatively coarse features and, originally, the predictive fallibility of
regions varies by crowd density distribution, so we tailor the Regional
Correlation Loss (RCLoss) to retrieve continuous error-prone regions and smooth
spatial information . Extensive experiments have proven that our approach
surpasses previous SOTA methods in many benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TileGen: Tileable, Controllable Material Generation and Capture. (arXiv:2206.05649v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05649">
<div class="article-summary-box-inner">
<span><p>Recent methods (e.g. MaterialGAN) have used unconditional GANs to generate
per-pixel material maps, or as a prior to reconstruct materials from input
photographs. These models can generate varied random material appearance, but
do not have any mechanism to constrain the generated material to a specific
category or to control the coarse structure of the generated material, such as
the exact brick layout on a brick wall. Furthermore, materials reconstructed
from a single input photo commonly have artifacts and are generally not
tileable, which limits their use in practical content creation pipelines. We
propose TileGen, a generative model for SVBRDFs that is specific to a material
category, always tileable, and optionally conditional on a provided input
structure pattern. TileGen is a variant of StyleGAN whose architecture is
modified to always produce tileable (periodic) material maps. In addition to
the standard "style" latent code, TileGen can optionally take a condition
image, giving a user direct control over the dominant spatial (and optionally
color) features of the material. For example, in brick materials, the user can
specify a brick layout and the brick color, or in leather materials, the
locations of wrinkles and folds. Our inverse rendering approach can find a
material perceptually matching a single target photograph by optimization. This
reconstruction can also be conditional on a user-provided pattern. The
resulting materials are tileable, can be larger than the target image, and are
editable by varying the condition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preprocessing Enhanced Image Compression for Machine Vision. (arXiv:2206.05650v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05650">
<div class="article-summary-box-inner">
<span><p>Recently, more and more images are compressed and sent to the back-end
devices for the machine analysis tasks~(\textit{e.g.,} object detection)
instead of being purely watched by humans. However, most traditional or learned
image codecs are designed to minimize the distortion of the human visual system
without considering the increased demand from machine vision systems. In this
work, we propose a preprocessing enhanced image compression method for machine
vision tasks to address this challenge. Instead of relying on the learned image
codecs for end-to-end optimization, our framework is built upon the traditional
non-differential codecs, which means it is standard compatible and can be
easily deployed in practical applications. Specifically, we propose a neural
preprocessing module before the encoder to maintain the useful semantic
information for the downstream tasks and suppress the irrelevant information
for bitrate saving. Furthermore, our neural preprocessing module is
quantization adaptive and can be used in different compression ratios. More
importantly, to jointly optimize the preprocessing module with the downstream
machine vision tasks, we introduce the proxy network for the traditional
non-differential codecs in the back-propagation stage. We provide extensive
experiments by evaluating our compression method for two representative
downstream tasks with different backbone networks. Experimental results show
our method achieves a better trade-off between the coding bitrate and the
performance of the downstream machine vision tasks by saving about 20% bitrate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STD-NET: Search of Image Steganalytic Deep-learning Architecture via Hierarchical Tensor Decomposition. (arXiv:2206.05651v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05651">
<div class="article-summary-box-inner">
<span><p>Recent studies shows that the majority of existing deep steganalysis models
have a large amount of redundancy, which leads to a huge waste of storage and
computing resources. The existing model compression method cannot flexibly
compress the convolutional layer in residual shortcut block so that a
satisfactory shrinking rate cannot be obtained. In this paper, we propose
STD-NET, an unsupervised deep-learning architecture search approach via
hierarchical tensor decomposition for image steganalysis. Our proposed strategy
will not be restricted by various residual connections, since this strategy
does not change the number of input and output channels of the convolution
block. We propose a normalized distortion threshold to evaluate the sensitivity
of each involved convolutional layer of the base model to guide STD-NET to
compress target network in an efficient and unsupervised approach, and obtain
two network structures of different shapes with low computation cost and
similar performance compared with the original one. Extensive experiments have
confirmed that, on one hand, our model can achieve comparable or even better
detection performance in various steganalytic scenarios due to the great
adaptivity of the obtained network architecture. On the other hand, the
experimental results also demonstrate that our proposed strategy is more
efficient and can remove more redundancy compared with previous steganalytic
network compression methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking. (arXiv:2206.05683v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05683">
<div class="article-summary-box-inner">
<span><p>Animal pose estimation and tracking (APT) is a fundamental task for detecting
and tracking animal keypoints from a sequence of video frames. Previous
animal-related datasets focus either on animal tracking or single-frame animal
pose estimation, and never on both aspects. The lack of APT datasets hinders
the development and evaluation of video-based animal pose estimation and
tracking methods, limiting real-world applications, e.g., understanding animal
behavior in wildlife conservation. To fill this gap, we make the first step and
propose APT-36K, i.e., the first large-scale benchmark for animal pose
estimation and tracking. Specifically, APT-36K consists of 2,400 video clips
collected and filtered from 30 animal species with 15 frames for each video,
resulting in 36,000 frames in total. After manual annotation and careful
double-check, high-quality keypoint and tracking annotations are provided for
all the animal instances. Based on APT-36K, we benchmark several representative
models on the following three tracks: (1) supervised animal pose estimation on
a single frame under intra- and inter-domain transfer learning settings, (2)
inter-species domain generalization test for unseen animals, and (3) animal
pose estimation with animal tracking. Based on the experimental results, we
gain some empirical insights and show that APT-36K provides a valuable animal
pose estimation and tracking benchmark, offering new challenges and
opportunities for future research. The code and dataset will be made publicly
available at https://github.com/pandorgan/APT-36K.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRNet: Decomposition and Reconstruction Network for Remote Physiological Measurement. (arXiv:2206.05687v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05687">
<div class="article-summary-box-inner">
<span><p>Remote photoplethysmography (rPPG) based physiological measurement has great
application values in affective computing, non-contact health monitoring,
telehealth monitoring, etc, which has become increasingly important especially
during the COVID-19 pandemic. Existing methods are generally divided into two
groups. The first focuses on mining the subtle blood volume pulse (BVP) signals
from face videos, but seldom explicitly models the noises that dominate face
video content. They are susceptible to the noises and may suffer from poor
generalization ability in unseen scenarios. The second focuses on modeling
noisy data directly, resulting in suboptimal performance due to the lack of
regularity of these severe random noises. In this paper, we propose a
Decomposition and Reconstruction Network (DRNet) focusing on the modeling of
physiological features rather than noisy data. A novel cycle loss is proposed
to constrain the periodicity of physiological information. Besides, a
plug-and-play Spatial Attention Block (SAB) is proposed to enhance features
along with the spatial location information. Furthermore, an efficient Patch
Cropping (PC) augmentation strategy is proposed to synthesize augmented samples
with different noise and features. Extensive experiments on different public
datasets as well as the cross-database testing demonstrate the effectiveness of
our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PD-DWI: Predicting response to neoadjuvant chemotherapy in invasive breast cancer with Physiologically-Decomposed Diffusion-Weighted MRI machine-learning model. (arXiv:2206.05695v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05695">
<div class="article-summary-box-inner">
<span><p>Early prediction of pathological complete response (pCR) following
neoadjuvant chemotherapy (NAC) for breast cancer plays a critical role in
surgical planning and optimizing treatment strategies. Recently, machine and
deep-learning based methods were suggested for early pCR prediction from
multi-parametric MRI (mp-MRI) data including dynamic contrast-enhanced MRI and
diffusion-weighted MRI (DWI) with moderate success. We introduce PD-DWI, a
physiologically decomposed DWI machine-learning model to predict pCR from DWI
and clinical data. Our model first decomposes the raw DWI data into the various
physiological cues that are influencing the DWI signal and then uses the
decomposed data, in addition to clinical variables, as the input features of a
radiomics-based XGBoost model. We demonstrated the added-value of our PD-DWI
model over conventional machine-learning approaches for pCR prediction from
mp-MRI data using the publicly available Breast Multi-parametric MRI for
prediction of NAC Response (BMMR2) challenge. Our model substantially improves
the area under the curve (AUC), compared to the current best result on the
leaderboard (0.8849 vs. 0.8397) for the challenge test set. PD-DWI has the
potential to improve prediction of pCR following NAC for breast cancer, reduce
overall mp-MRI acquisition times and eliminate the need for contrast-agent
injection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DPCN++: Differentiable Phase Correlation Network for Versatile Pose Registration. (arXiv:2206.05707v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05707">
<div class="article-summary-box-inner">
<span><p>Pose registration is critical in vision and robotics. This paper focuses on
the challenging task of initialization-free pose registration up to 7DoF for
homogeneous and heterogeneous measurements. While recent learning-based methods
show promise using differentiable solvers, they either rely on heuristically
defined correspondences or are prone to local minima. We present a
differentiable phase correlation (DPC) solver that is globally convergent and
correspondence-free. When combined with simple feature extraction networks, our
general framework DPCN++ allows for versatile pose registration with arbitrary
initialization. Specifically, the feature extraction networks first learn dense
feature grids from a pair of homogeneous/heterogeneous measurements. These
feature grids are then transformed into a translation and scale invariant
spectrum representation based on Fourier transform and spherical radial
aggregation, decoupling translation and scale from rotation. Next, the
rotation, scale, and translation are independently and efficiently estimated in
the spectrum step-by-step using the DPC solver. The entire pipeline is
differentiable and trained end-to-end. We evaluate DCPN++ on a wide range of
registration tasks taking different input modalities, including 2D bird's-eye
view images, 3D object and scene measurements, and medical images. Experimental
results demonstrate that DCPN++ outperforms both classical and learning-based
baselines, especially on partially observed and heterogeneous measurements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Narrowing the Gap: Improved Detector Training with Noisy Location Annotations. (arXiv:2206.05708v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05708">
<div class="article-summary-box-inner">
<span><p>Deep learning methods require massive of annotated data for optimizing
parameters. For example, datasets attached with accurate bounding box
annotations are essential for modern object detection tasks. However, labeling
with such pixel-wise accuracy is laborious and time-consuming, and elaborate
labeling procedures are indispensable for reducing man-made noise, involving
annotation review and acceptance testing. In this paper, we focus on the impact
of noisy location annotations on the performance of object detection approaches
and aim to, on the user side, reduce the adverse effect of the noise. First,
noticeable performance degradation is experimentally observed for both
one-stage and two-stage detectors when noise is introduced to the bounding box
annotations. For instance, our synthesized noise results in performance
decrease from 38.9% AP to 33.6% AP for FCOS detector on COCO test split, and
37.8%AP to 33.7%AP for Faster R-CNN. Second, a self-correction technique based
on a Bayesian filter for prediction ensemble is proposed to better exploit the
noisy location annotations following a Teacher-Student learning paradigm.
Experiments for both synthesized and real-world scenarios consistently
demonstrate the effectiveness of our approach, e.g., our method increases the
degraded performance of the FCOS detector from 33.6% AP to 35.6% AP on COCO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-based Spatial Transformer with Memory Replay for Multi-future Pedestrian Trajectory Prediction. (arXiv:2206.05712v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05712">
<div class="article-summary-box-inner">
<span><p>Pedestrian trajectory prediction is an essential and challenging task for a
variety of real-life applications such as autonomous driving and robotic motion
planning. Besides generating a single future path, predicting multiple
plausible future paths is becoming popular in some recent work on trajectory
prediction. However, existing methods typically emphasize spatial interactions
between pedestrians and surrounding areas but ignore the smoothness and
temporal consistency of predictions. Our model aims to forecast multiple paths
based on a historical trajectory by modeling multi-scale graph-based spatial
transformers combined with a trajectory smoothing algorithm named ``Memory
Replay'' utilizing a memory graph. Our method can comprehensively exploit the
spatial information as well as correct the temporally inconsistent trajectories
(e.g., sharp turns). We also propose a new evaluation metric named ``Percentage
of Trajectory Usage'' to evaluate the comprehensiveness of diverse multi-future
predictions. Our extensive experiments show that the proposed model achieves
state-of-the-art performance on multi-future prediction and competitive results
for single-future prediction. Code released at
https://github.com/Jacobieee/ST-MR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crowd Localization from Gaussian Mixture Scoped Knowledge and Scoped Teacher. (arXiv:2206.05717v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05717">
<div class="article-summary-box-inner">
<span><p>Crowd localization is to predict each instance head position in crowd
scenarios. Since the distance of instances being to the camera are variant,
there exists tremendous gaps among scales of instances within an image, which
is called the intrinsic scale shift. The core reason of intrinsic scale shift
being one of the most essential issues in crowd localization is that it is
ubiquitous in crowd scenes and makes scale distribution chaotic.
</p>
<p>To this end, the paper concentrates on access to tackle the chaos of the
scale distribution incurred by intrinsic scale shift. We propose Gaussian
Mixture Scope (GMS) to regularize the chaotic scale distribution. Concretely,
the GMS utilizes a Gaussian mixture distribution to adapt to scale distribution
and decouples the mixture model into sub-normal distributions to regularize the
chaos within the sub-distributions. Then, an alignment is introduced to
regularize the chaos among sub-distributions. However, despite that GMS is
effective in regularizing the data distribution, it amounts to dislodging the
hard samples in training set, which incurs overfitting. We assert that it is
blamed on the block of transferring the latent knowledge exploited by GMS from
data to model. Therefore, a Scoped Teacher playing a role of bridge in
knowledge transform is proposed. What' s more, the consistency regularization
is also introduced to implement knowledge transform. To that effect, the
further constraints are deployed on Scoped Teacher to derive feature
consistence between teacher and student end.
</p>
<p>With proposed GMS and Scoped Teacher implemented on five mainstream datasets
of crowd localization, the extensive experiments demonstrate the superiority of
our work. Moreover, comparing with existing crowd locators, our work achieves
state-of-the-art via F1-meansure comprehensively on five datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Occlusion Of Adding New Category In Objection Detection. (arXiv:2206.05730v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05730">
<div class="article-summary-box-inner">
<span><p>Building instance detection models that are data efficient and can handle
rare object categories is an important challenge in computer vision. But data
collection methods and metrics are lack of research towards real scenarios
application using neural network. Here, we perform a systematic study of the
Object Occlusion data collection and augmentation methods where we imitate
object occlusion relationship in target scenarios. However, we find that the
simple mechanism of object occlusion is good enough and can provide acceptable
accuracy in real scenarios adding new category. We illustate that only adding
15 images of new category in a half million training dataset with hundreds
categories, can give this new category 95% accuracy in unseen test dataset
including thousands of images of this category.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse views. (arXiv:2206.05737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05737">
<div class="article-summary-box-inner">
<span><p>We introduce SparseNeuS, a novel neural rendering based method for the task
of surface reconstruction from multi-view images. This task becomes more
difficult when only sparse images are provided as input, a scenario where
existing neural reconstruction approaches usually produce incomplete or
distorted results. Moreover, their inability of generalizing to unseen new
scenes impedes their application in practice. Contrarily, SparseNeuS can
generalize to new scenes and work well with sparse images (as few as 2 or 3).
SparseNeuS adopts signed distance function (SDF) as the surface representation,
and learns generalizable priors from image features by introducing geometry
encoding volumes for generic surface prediction. Moreover, several strategies
are introduced to effectively leverage sparse views for high-quality
reconstruction, including 1) a multi-level geometry reasoning framework to
recover the surfaces in a coarse-to-fine manner; 2) a multi-scale color
blending scheme for more reliable color prediction; 3) a consistency-aware
fine-tuning scheme to control the inconsistent regions caused by occlusion and
noise. Extensive experiments demonstrate that our approach not only outperforms
the state-of-the-art methods, but also exhibits good efficiency,
generalizability, and flexibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Fake News Detection with Adaptive Unimodal Representation Aggregation. (arXiv:2206.05741v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05741">
<div class="article-summary-box-inner">
<span><p>The development of Internet technology has continuously intensified the
spread and destructive power of rumors and fake news. Previous researches on
multimedia fake news detection include a series of complex feature extraction
and fusion networks to achieve feature alignment between images and texts.
However, what the multimodal features are composed of and how features from
different modalities affect the decision-making process are still open
questions. We present AURA, a multimodal fake news detection network with
Adaptive Unimodal Representation Aggregation. We first extract representations
respectively from image pattern, image semantics and text, and multimodal
representations are generated by sending the semantic and linguistic
representations into an expert network. Then, we perform coarse-level fake news
detection and cross-modal cosistency learning according to the unimodal and
multimodal representations. The classification and consistency scores are
mapped into modality-aware attention scores that readjust the features.
Finally, we aggregation and classify the weighted features for refined fake
news detection. Comprehensive experiments on Weibo and Gossipcop prove that
AURA can successfully beat several state-of-the-art FND schemes, where the
overall prediction accuracy and the recall of fake news is steadily improved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Attack: Universal Adversarial Perturbation on Embodied Vision Navigation. (arXiv:2206.05751v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05751">
<div class="article-summary-box-inner">
<span><p>Embodied agents in vision navigation coupled with deep neural networks have
attracted increasing attention. However, deep neural networks are vulnerable to
malicious adversarial noises, which may potentially cause catastrophic failures
in Embodied Vision Navigation. Among these adversarial noises, universal
adversarial perturbations (UAP), i.e., the image-agnostic perturbation applied
on each frame received by the agent, are more critical for Embodied Vision
Navigation since they are computation-efficient and application-practical
during the attack. However, existing UAP methods do not consider the system
dynamics of Embodied Vision Navigation. For extending UAP in the sequential
decision setting, we formulate the disturbed environment under the universal
noise $\delta$, as a $\delta$-disturbed Markov Decision Process ($\delta$-MDP).
Based on the formulation, we analyze the properties of $\delta$-MDP and propose
two novel Consistent Attack methods for attacking Embodied agents, which first
consider the dynamic of the MDP by estimating the disturbed Q function and the
disturbed distribution. In spite of victim models, our Consistent Attack can
cause a significant drop in the performance for the Goalpoint task in habitat.
Extensive experimental results indicate that there exist potential risks for
applying Embodied Vision Navigation methods to the real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeATrans: Learning Segmentation-Assisted diagnosis model via Transforme. (arXiv:2206.05763v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05763">
<div class="article-summary-box-inner">
<span><p>Clinically, the accurate annotation of lesions/tissues can significantly
facilitate the disease diagnosis. For example, the segmentation of optic
disc/cup (OD/OC) on fundus image would facilitate the glaucoma diagnosis, the
segmentation of skin lesions on dermoscopic images is helpful to the melanoma
diagnosis, etc. With the advancement of deep learning techniques, a wide range
of methods proved the lesions/tissues segmentation can also facilitate the
automated disease diagnosis models. However, existing methods are limited in
the sense that they can only capture static regional correlations in the
images. Inspired by the global and dynamic nature of Vision Transformer, in
this paper, we propose Segmentation-Assisted diagnosis Transformer (SeATrans)
to transfer the segmentation knowledge to the disease diagnosis network.
Specifically, we first propose an asymmetric multi-scale interaction strategy
to correlate each single low-level diagnosis feature with multi-scale
segmentation features. Then, an effective strategy called SeA-block is adopted
to vitalize diagnosis feature via correlated segmentation features. To model
the segmentation-diagnosis interaction, SeA-block first embeds the diagnosis
feature based on the segmentation information via the encoder, and then
transfers the embedding back to the diagnosis feature space by a decoder.
Experimental results demonstrate that SeATrans surpasses a wide range of
state-of-the-art (SOTA) segmentation-assisted diagnosis methods on several
disease diagnosis tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Semantic Consistency Feature Alignment Object Detection Model Based on Mixed-Class Distribution Metrics. (arXiv:2206.05765v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05765">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation is critical in various computer vision tasks,
such as object detection, instance segmentation, etc. They attempt to reduce
domain bias-induced performance degradation while also promoting model
application speed. Previous works in domain adaptation object detection attempt
to align image-level and instance-level shifts to eventually minimize the
domain discrepancy, but they may align single-class features to mixed-class
features in image-level domain adaptation because each image in the object
detection task may be more than one class and object. In order to achieve
single-class with single-class alignment and mixed-class with mixed-class
alignment, we treat the mixed-class of the feature as a new class and propose a
mixed-classes $H-divergence$ for object detection to achieve homogenous feature
alignment and reduce negative transfer. Then, a Semantic Consistency Feature
Alignment Model (SCFAM) based on mixed-classes $H-divergence$ was also
presented. To improve single-class and mixed-class semantic information and
accomplish semantic separation, the SCFAM model proposes Semantic Prediction
Models (SPM) and Semantic Bridging Components (SBC). And the weight of the pix
domain discriminator loss is then changed based on the SPM result to reduce
sample imbalance. Extensive unsupervised domain adaption experiments on widely
used datasets illustrate our proposed approach's robust object detection in
domain bias settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Whole-Slide Image Pyramids for Cancer Prognosis via Dual-Stream Networks. (arXiv:2206.05782v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05782">
<div class="article-summary-box-inner">
<span><p>The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been a
challenging task. Most existing approaches focus solely on single-resolution
images. The multi-resolution schemes, utilizing image pyramids to enhance WSI
visual representations, have not yet been paid enough attention to. In order to
explore a multi-resolution solution for improving cancer prognosis accuracy,
this paper proposes a dual-stream architecture to model WSIs by an image
pyramid strategy. This architecture consists of two sub-streams: one is for
low-resolution WSIs, and the other is especially for high-resolution ones.
Compared to other approaches, our scheme has three highlights: (i) there exists
a one-to-one relation between stream and resolution; (ii) a square pooling
layer is added to align the patches from two resolution streams, largely
reducing computation cost and enabling a natural stream feature fusion; (iii) a
cross-attention-based method is proposed to pool high-resolution patches
spatially under the guidance of low-resolution ones. We validate our scheme on
three publicly-available datasets, a total number of 3,101 WSIs from 1,911
patients. Experimental results verify that (1) hierarchical dual-stream
representation is more effective than single-stream ones for cancer prognosis,
gaining an average C-Index rise of 5.0% and 1.8% on a single low-resolution and
high-resolution stream, respectively; (2) our dual-stream scheme could
outperform current state-of-the-art ones, by a 5.1% average improvement of
C-Index; (3) the cancer diseases with observable survival differences could
have different preferences for model complexity. Our scheme could serve as an
alternative tool for further facilitating WSI prognosis research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Branch Specialization and its Application in Image Decomposition. (arXiv:2206.05810v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05810">
<div class="article-summary-box-inner">
<span><p>Branched neural networks have been used extensively for a variety of tasks.
Branches are sub-parts of the model that perform independent processing
followed by aggregation. It is known that this setting induces a phenomenon
called Branch Specialization, where different branches become experts in
different sub-tasks. Such observations were qualitative by nature. In this
work, we present a methodological analysis of Branch Specialization. We explain
the role of gradient descent in this phenomenon. We show that branched
generative networks naturally decompose animal images to meaningful channels of
fur, whiskers and spots and face images to channels such as different
illumination components and face parts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COLD Fusion: Calibrated and Ordinal Latent Distribution Fusion for Uncertainty-Aware Multimodal Emotion Recognition. (arXiv:2206.05833v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05833">
<div class="article-summary-box-inner">
<span><p>Automatically recognising apparent emotions from face and voice is hard, in
part because of various sources of uncertainty, including in the input data and
the labels used in a machine learning framework. This paper introduces an
uncertainty-aware audiovisual fusion approach that quantifies modality-wise
uncertainty towards emotion prediction. To this end, we propose a novel fusion
framework in which we first learn latent distributions over audiovisual
temporal context vectors separately, and then constrain the variance vectors of
unimodal latent distributions so that they represent the amount of information
each modality provides w.r.t. emotion recognition. In particular, we impose
Calibration and Ordinal Ranking constraints on the variance vectors of
audiovisual latent distributions. When well-calibrated, modality-wise
uncertainty scores indicate how much their corresponding predictions may differ
from the ground truth labels. Well-ranked uncertainty scores allow the ordinal
ranking of different frames across the modalities. To jointly impose both these
constraints, we propose a softmax distributional matching loss. In both
classification and regression settings, we compare our uncertainty-aware fusion
model with standard model-agnostic fusion baselines. Our evaluation on two
emotion recognition corpora, AVEC 2019 CES and IEMOCAP, shows that audiovisual
emotion recognition can considerably benefit from well-calibrated and
well-ranked latent uncertainty measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLIPv2: Unifying Localization and Vision-Language Understanding. (arXiv:2206.05836v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05836">
<div class="article-summary-box-inner">
<span><p>We present GLIPv2, a grounded VL understanding model, that serves both
localization tasks (e.g., object detection, instance segmentation) and
Vision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2
elegantly unifies localization pre-training and Vision-Language Pre-training
(VLP) with three pre-training tasks: phrase grounding as a VL reformulation of
the detection task, region-word contrastive learning as a novel region-word
level contrastive learning task, and the masked language modeling. This
unification not only simplifies the previous multi-stage VLP procedure but also
achieves mutual benefits between localization and understanding tasks.
Experimental results show that a single GLIPv2 model (all model weights are
shared) achieves near SoTA performance on various localization and
understanding tasks. The model also shows (1) strong zero-shot and few-shot
adaption performance on open-vocabulary object detection tasks and (2) superior
grounding capability on VL understanding tasks. Code will be released at
https://github.com/microsoft/GLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralODF: Learning Omnidirectional Distance Fields for 3D Shape Representation. (arXiv:2206.05837v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05837">
<div class="article-summary-box-inner">
<span><p>In visual computing, 3D geometry is represented in many different forms
including meshes, point clouds, voxel grids, level sets, and depth images. Each
representation is suited for different tasks thus making the transformation of
one representation into another (forward map) an important and common problem.
We propose Omnidirectional Distance Fields (ODFs), a new 3D shape
representation that encodes geometry by storing the depth to the object's
surface from any 3D position in any viewing direction. Since rays are the
fundamental unit of an ODF, it can be used to easily transform to and from
common 3D representations like meshes or point clouds. Different from level set
methods that are limited to representing closed surfaces, ODFs are unsigned and
can thus model open surfaces (e.g., garments). We demonstrate that ODFs can be
effectively learned with a neural network (NeuralODF) despite the inherent
discontinuities at occlusion boundaries. We also introduce efficient forward
mapping algorithms for transforming ODFs to and from common 3D representations.
Specifically, we introduce an efficient Jumping Cubes algorithm for generating
meshes from ODFs. Experiments demonstrate that NeuralODF can learn to capture
high-quality shape by overfitting to a single object, and also learn to
generalize on common shape categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficiency Comparison of AI classification algorithms for Image Detection and Recognition in Real-time. (arXiv:2206.05842v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05842">
<div class="article-summary-box-inner">
<span><p>Face detection and identification is the most difficult and often used task
in Artificial Intelligence systems. The goal of this study is to present and
compare the results of several face detection and recognition algorithms used
in the system. This system begins with a training image of a human, then
continues on to the test image, identifying the face, comparing it to the
trained face, and finally classifying it using OpenCV classifiers. This
research will discuss the most effective and successful tactics used in the
system, which are implemented using Python, OpenCV, and Matplotlib. It may also
be used in locations with CCTV, such as public spaces, shopping malls, and ATM
booths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FisheyeEX: Polar Outpainting for Extending the FoV of Fisheye Lens. (arXiv:2206.05844v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05844">
<div class="article-summary-box-inner">
<span><p>Fisheye lens gains increasing applications in computational photography and
assisted driving because of its wide field of view (FoV). However, the fisheye
image generally contains invalid black regions induced by its imaging model. In
this paper, we present a FisheyeEX method that extends the FoV of the fisheye
lens by outpainting the invalid regions, improving the integrity of captured
scenes. Compared with the rectangle and undistorted image, there are two
challenges for fisheye image outpainting: irregular painting regions and
distortion synthesis. Observing the radial symmetry of the fisheye image, we
first propose a polar outpainting strategy to extrapolate the coherent
semantics from the center to the outside region. Such an outpainting manner
considers the distribution pattern of radial distortion and the circle
boundary, boosting a more reasonable completion direction. For the distortion
synthesis, we propose a spiral distortion-aware perception module, in which the
learning path keeps consistent with the distortion prior of the fisheye image.
Subsequently, a scene revision module rearranges the generated pixels with the
estimated distortion to match the fisheye image, thus extending the FoV. In the
experiment, we evaluate the proposed FisheyeEX on three popular outdoor
datasets: Cityscapes, BDD100k, and KITTI, and one real-world fisheye image
dataset. The results demonstrate that our approach significantly outperforms
the state-of-the-art methods, gaining around 27% more content beyond the
original fisheye image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InBiaseD: Inductive Bias Distillation to Improve Generalization and Robustness through Shape-awareness. (arXiv:2206.05846v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05846">
<div class="article-summary-box-inner">
<span><p>Humans rely less on spurious correlations and trivial cues, such as texture,
compared to deep neural networks which lead to better generalization and
robustness. It can be attributed to the prior knowledge or the high-level
cognitive inductive bias present in the brain. Therefore, introducing
meaningful inductive bias to neural networks can help learn more generic and
high-level representations and alleviate some of the shortcomings. We propose
InBiaseD to distill inductive bias and bring shape-awareness to the neural
networks. Our method includes a bias alignment objective that enforces the
networks to learn more generic representations that are less vulnerable to
unintended cues in the data which results in improved generalization
performance. InBiaseD is less susceptible to shortcut learning and also
exhibits lower texture bias. The better representations also aid in improving
robustness to adversarial attacks and we hence plugin InBiaseD seamlessly into
the existing adversarial training schemes to show a better trade-off between
generalization and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Generalized Specialist Approach To Train Quality Resilient Snapshot Ensemble. (arXiv:2206.05853v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05853">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) apply well with food image recognition
due to the ability to learn discriminative visual features. Nevertheless,
recognizing distorted images is challenging for existing CNNs. Hence, the study
modelled a generalized specialist approach to train a quality resilient
ensemble. The approach aids the models in the ensemble framework retain general
skills of recognizing clean images and shallow skills of classifying noisy
images with one deep expertise area on a particular distortion. Subsequently, a
novel data augmentation random quality mixup (RQMixUp) is combined with
snapshot ensembling to train G-Specialist. During each training cycle of
G-Specialist, a model is fine-tuned on the synthetic images generated by
RQMixup, intermixing clean and distorted images of a particular distortion at a
randomly chosen level. Resultantly, each snapshot in the ensemble gained
expertise on several distortion levels, with shallow skills on other quality
distortions. Next, the filter outputs from diverse experts were fused for
higher accuracy. The learning process has no additional cost due to a single
training process to train experts, compatible with a wide range of supervised
CNNs for transfer learning. Finally, the experimental analysis on three
real-world food and a Malaysian food database showed significant improvement
for distorted images with competitive classification performance on pristine
food images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Directed-Evolution Method for Sparsification and Compression of Neural Networks with Application to Object Identification and Segmentation and considerations of optimal quantization using small number of bits. (arXiv:2206.05859v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05859">
<div class="article-summary-box-inner">
<span><p>This work introduces Directed-Evolution (DE) method for sparsification of
neural networks, where the relevance of parameters to the network accuracy is
directly assessed and the parameters that produce the least effect on accuracy
when tentatively zeroed are indeed zeroed. DE method avoids a potentially
combinatorial explosion of all possible candidate sets of parameters to be
zeroed in large networks by mimicking evolution in the natural world. DE uses a
distillation context [5]. In this context, the original network is the teacher
and DE evolves the student neural network to the sparsification goal while
maintaining minimal divergence between teacher and student. After the desired
sparsification level is reached in each layer of the network by DE, a variety
of quantization alternatives are used on the surviving parameters to find the
lowest number of bits for their representation with acceptable loss of
accuracy. A procedure to find optimal distribution of quantization levels in
each sparsified layer is presented. Suitable final lossless encoding of the
surviving quantized parameters is used for the final parameter representation.
DE was used in sample of representative neural networks using MNIST,
FashionMNIST and COCO data sets with progressive larger networks. An 80 classes
YOLOv3 with more than 60 million parameters network trained on COCO dataset
reached 90% sparsification and correctly identifies and segments all objects
identified by the original network with more than 80% confidence using 4bit
parameter quantization. Compression between 40x and 80x. It has not escaped the
authors that techniques from different methods can be nested. Once the best
parameter set for sparsification is identified in a cycle of DE, a decision on
zeroing only a sub-set of those parameters can be made using a combination of
criteria like parameter magnitude and Hessian approximations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TC-SfM: Robust Track-Community-Based Structure-from-Motion. (arXiv:2206.05866v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05866">
<div class="article-summary-box-inner">
<span><p>Structure-from-Motion (SfM) aims to recover 3D scene structures and camera
poses based on the correspondences between input images, and thus the ambiguity
caused by duplicate structures (i.e., different structures with strong visual
resemblance) always results in incorrect camera poses and 3D structures. To
deal with the ambiguity, most existing studies resort to additional constraint
information or implicit inference by analyzing two-view geometries or feature
points. In this paper, we propose to exploit high-level information in the
scene, i.e., the spatial contextual information of local regions, to guide the
reconstruction. Specifically, a novel structure is proposed, namely,
{\textit{track-community}}, in which each community consists of a group of
tracks and represents a local segment in the scene. A community detection
algorithm is used to partition the scene into several segments. Then, the
potential ambiguous segments are detected by analyzing the neighborhood of
tracks and corrected by checking the pose consistency. Finally, we perform
partial reconstruction on each segment and align them with a novel
bidirectional consistency cost function which considers both 3D-3D
correspondences and pairwise relative camera poses. Experimental results
demonstrate that our approach can robustly alleviate reconstruction failure
resulting from visually indistinguishable structures and accurately merge the
partial reconstructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantification and Analysis of Layer-wise and Pixel-wise Information Discarding. (arXiv:1906.04109v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.04109">
<div class="article-summary-box-inner">
<span><p>This paper presents a method to explain how the information of each input
variable is gradually discarded during the forward propagation in a deep neural
network (DNN), which provides new perspectives to explain DNNs. We define two
types of entropy-based metrics, i.e. (1) the discarding of pixel-wise
information used in the forward propagation, and (2) the uncertainty of the
input reconstruction, to measure input information contained by a specific
layer from two perspectives. Unlike previous attribution metrics, the proposed
metrics ensure the fairness of comparisons between different layers of
different DNNs. We can use these metrics to analyze the efficiency of
information processing in DNNs, which exhibits strong connections to the
performance of DNNs. We analyze information discarding in a pixel-wise manner,
which is different from the information bottleneck theory measuring feature
information w.r.t. the sample distribution. Experiments have shown the
effectiveness of our metrics in analyzing classic DNNs and explaining existing
deep-learning techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognizing License Plates in Real-Time. (arXiv:1906.04376v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.04376">
<div class="article-summary-box-inner">
<span><p>License plate detection and recognition (LPDR) is of growing importance for
enabling intelligent transportation and ensuring the security and safety of the
cities. However, LPDR faces a big challenge in a practical environment. The
license plates can have extremely diverse sizes, fonts and colors, and the
plate images are usually of poor quality caused by skewed capturing angles,
uneven lighting, occlusion, and blurring. In applications such as surveillance,
it often requires fast processing. To enable real-time and accurate license
plate recognition, in this work, we propose a set of techniques: 1) a contour
reconstruction method along with edge-detection to quickly detect the candidate
plates; 2) a simple zero-one-alternation scheme to effectively remove the fake
top and bottom borders around plates to facilitate more accurate segmentation
of characters on plates; 3) a set of techniques to augment the training data,
incorporate SIFT features into the CNN network, and exploit transfer learning
to obtain the initial parameters for more effective training; and 4) a
two-phase verification procedure to determine the correct plate at low cost, a
statistical filtering in the plate detection stage to quickly remove unwanted
candidates, and the accurate CR results after the CR process to perform further
plate verification without additional processing. We implement a complete LPDR
system based on our algorithms. The experimental results demonstrate that our
system can accurately recognize license plate in real-time. Additionally, it
works robustly under various levels of illumination and noise, and in the
presence of car movement. Compared to peer schemes, our system is not only
among the most accurate ones but is also the fastest, and can be easily applied
to other scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal, transferable and targeted adversarial attacks. (arXiv:1908.11332v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.11332">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks have been found vulnerable re-cently. A kind of
well-designed inputs, which called adver-sarial examples, can lead the networks
to make incorrectpredictions. Depending on the different scenarios, goalsand
capabilities, the difficulties of the attacks are different.For example, a
targeted attack is more difficult than a non-targeted attack, a universal
attack is more difficult than anon-universal attack, a transferable attack is
more difficultthan a nontransferable one. The question is: Is there existan
attack that can meet all these requirements? In this pa-per, we answer this
question by producing a kind of attacksunder these conditions. We learn a
universal mapping tomap the sources to the adversarial examples. These
exam-ples can fool classification networks to classify all of theminto one
targeted class, and also have strong transferability.Our code is released at:
xxxxx.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detached Error Feedback for Distributed SGD with Random Sparsification. (arXiv:2004.05298v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.05298">
<div class="article-summary-box-inner">
<span><p>The communication bottleneck has been a critical problem in large-scale
distributed deep learning. In this work, we study distributed SGD with random
block-wise sparsification as the gradient compressor, which is ring-allreduce
compatible and highly computation-efficient but leads to inferior performance.
To tackle this important issue, we improve the communication-efficient
distributed SGD from a novel aspect, that is, the trade-off between the
variance and second moment of the gradient. With this motivation, we propose a
new detached error feedback (DEF) algorithm, which shows better convergence
bound than error feedback for non-convex problems. We also propose DEF-A to
accelerate the generalization of DEF at the early stages of the training, which
shows better generalization bounds than DEF. Furthermore, we establish the
connection between communication-efficient distributed SGD and SGD with iterate
averaging (SGD-IA) for the first time. Extensive deep learning experiments show
significant empirical improvement of the proposed methods under various
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-Based Sorghum Head Counting When You Only Look Once. (arXiv:2009.11929v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.11929">
<div class="article-summary-box-inner">
<span><p>Modern trends in digital agriculture have seen a shift towards artificial
intelligence for crop quality assessment and yield estimation. In this work, we
document how a parameter tuned single-shot object detection algorithm can be
used to identify and count sorghum head from aerial drone images. Our approach
involves a novel exploratory analysis that identified key structural elements
of the sorghum images and motivated the selection of parameter-tuned anchor
boxes that contributed significantly to performance. These insights led to the
development of a deep learning model that outperformed the baseline model and
achieved an out-of-sample mean average precision of 0.95.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstructing A Large Scale 3D Face Dataset for Deep 3D Face Identification. (arXiv:2010.08391v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08391">
<div class="article-summary-box-inner">
<span><p>Deep learning methods have brought many breakthroughs to computer vision,
especially in 2D face recognition. However, the bottleneck of deep learning
based 3D face recognition is that it is difficult to collect millions of 3D
faces, whether for industry or academia. In view of this situation, there are
many methods to generate more 3D faces from existing 3D faces through 3D face
data augmentation, which are used to train deep 3D face recognition models.
However, to the best of our knowledge, there is no method to generate 3D faces
from 2D face images for training deep 3D face recognition models. This letter
focuses on the role of reconstructed 3D facial surfaces in 3D face
identification and proposes a framework of 2D-aided deep 3D face
identification. In particular, we propose to reconstruct millions of 3D face
scans from a large scale 2D face database (i.e.VGGFace2), using a deep learning
based 3D face reconstruction method (i.e.ExpNet). Then, we adopt a two-phase
training approach: In the first phase, we use millions of face images to
pre-train the deep convolutional neural network (DCNN), and in the second
phase, we use normal component images (NCI) of reconstructed 3D face scans to
train the DCNN. Extensive experimental results illustrate that the proposed
approach can greatly improve the rank-1 score of 3D face identification on the
FRGC v2.0, the Bosphorus, and the BU-3DFE 3D face databases, compared to the
model trained by 2D face images. Finally, our proposed approach achieves
state-of-the-art rank-1 scores on the FRGC v2.0 (97.6%), Bosphorus (98.4%), and
BU-3DFE (98.8%) databases. The experimental results show that the reconstructed
3D facial surfaces are useful and our 2D-aided deep 3D face identification
framework is meaningful, facing the scarcity of 3D faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPU-Net: Self-Supervised Point Cloud Upsampling by Coarse-to-Fine Reconstruction with Self-Projection Optimization. (arXiv:2012.04439v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04439">
<div class="article-summary-box-inner">
<span><p>The task of point cloud upsampling aims to acquire dense and uniform point
sets from sparse and irregular point sets. Although significant progress has
been made with deep learning models, state-of-the-art methods require
ground-truth dense point sets as the supervision, which makes them limited to
be trained under synthetic paired training data and not suitable to be under
real-scanned sparse data. However, it is expensive and tedious to obtain large
numbers of paired sparse-dense point sets as supervision from real-scanned
sparse data. To address this problem, we propose a self-supervised point cloud
upsampling network, named SPU-Net, to capture the inherent upsampling patterns
of points lying on the underlying object surface. Specifically, we propose a
coarse-to-fine reconstruction framework, which contains two main components:
point feature extraction and point feature expansion, respectively. In the
point feature extraction, we integrate the self-attention module with the graph
convolution network (GCN) to capture context information inside and among local
regions simultaneously. In the point feature expansion, we introduce a
hierarchically learnable folding strategy to generate upsampled point sets with
learnable 2D grids. Moreover, to further optimize the noisy points in the
generated point sets, we propose a novel self-projection optimization
associated with uniform and reconstruction terms as a joint loss to facilitate
the self-supervised point cloud upsampling. We conduct various experiments on
both synthetic and real-scanned datasets, and the results demonstrate that we
achieve comparable performances to state-of-the-art supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient Object Detection via Integrity Learning. (arXiv:2101.07663v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07663">
<div class="article-summary-box-inner">
<span><p>Although current salient object detection (SOD) works have achieved
significant progress, they are limited when it comes to the integrity of the
predicted salient regions. We define the concept of integrity at both a micro
and macro level. Specifically, at the micro level, the model should highlight
all parts that belong to a certain salient object. Meanwhile, at the macro
level, the model needs to discover all salient objects in a given image. To
facilitate integrity learning for SOD, we design a novel Integrity Cognition
Network (ICON), which explores three important components for learning strong
integrity features. 1) Unlike existing models, which focus more on feature
discriminability, we introduce a diverse feature aggregation (DFA) component to
aggregate features with various receptive fields (i.e., kernel shape and
context) and increase feature diversity. Such diversity is the foundation for
mining the integral salient objects. 2) Based on the DFA features, we introduce
an integrity channel enhancement (ICE) component with the goal of enhancing
feature channels that highlight the integral salient objects, while suppressing
the other distracting ones. 3) After extracting the enhanced features, the
part-whole verification (PWV) method is employed to determine whether the part
and whole object features have strong agreement. Such part-whole agreements can
further improve the micro-level integrity for each salient object. To
demonstrate the effectiveness of our ICON, comprehensive experiments are
conducted on seven challenging benchmarks. Our ICON outperforms the baseline
methods in terms of a wide range of metrics. Notably, our ICON achieves about
10% relative improvement over the previous best model in terms of average false
negative ratio (FNR), on six datasets. Codes and results are available at:
https://github.com/mczhuge/ICON.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliminate Deviation with Deviation for Data Augmentation and a General Multi-modal Data Learning Method. (arXiv:2101.08533v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08533">
<div class="article-summary-box-inner">
<span><p>One of the challenges of computer vision is that it needs to adapt to color
deviations in changeable environments. Therefore, minimizing the adverse
effects of color deviation on the prediction is one of the main goals of vision
task. Current solutions focus on using generative models to augment training
data to enhance the invariance of input variation. However, such methods often
introduce new noise, which limits the gain from generated data. To this end,
this paper proposes a strategy eliminate deviation with deviation, which is
named Random Color Dropout (RCD). Our hypothesis is that if there are color
deviation between the query image and the gallery image, the retrieval results
of some examples will be better after ignoring the color information.
Specifically, this strategy balances the weights between color features and
color-independent features in the neural network by dropouting partial color
information in the training data, so as to overcome the effect of color
devitaion. The proposed RCD can be combined with various existing ReID models
without changing the learning strategy, and can be applied to other computer
vision fields, such as object detection. Experiments on several ReID baselines
and three common large-scale datasets such as Market1501, DukeMTMC, and MSMT17
have verified the effectiveness of this method. Experiments on Cross-domain
tests have shown that this strategy is significant eliminating the domain gap.
Furthermore, in order to understand the working mechanism of RCD, we analyzed
the effectiveness of this strategy from the perspective of classification,
which reveals that it may be better to utilize many instead of all of color
information in visual tasks with strong domain variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GANav: Efficient Terrain Segmentation for Robot Navigation in Unstructured Outdoor Environments. (arXiv:2103.04233v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04233">
<div class="article-summary-box-inner">
<span><p>We propose GANav, a novel group-wise attention mechanism to identify safe and
navigable regions in off-road terrains and unstructured environments from RGB
images. Our approach classifies terrains based on their navigability levels
using coarse-grained semantic segmentation. Our novel group-wise attention loss
enables any backbone network to explicitly focus on the different groups'
features with low spatial resolution. Our design leads to efficient inference
while maintaining a high level of accuracy compared to existing SOTA methods.
Our extensive evaluations on the RUGD and RELLIS-3D datasets shows that GANav
achieves an improvement over the SOTA mIoU by 2.25-39.05% on RUGD and
5.17-19.06% on RELLIS-3D. We interface GANav with a deep reinforcement
learning-based navigation algorithm and highlight its benefits in terms of
navigation in real-world unstructured terrains. We integrate our GANav-based
navigation algorithm with ClearPath Jackal and Husky robots, and observe an
increase of 10% in terms of success rate, 2-47% in terms of selecting the
surface with the best navigability and a decrease of 4.6-13.9% in trajectory
roughness. Further, GANav reduces the false positive rate of forbidden regions
by 37.79%. Code, videos, and a full technical report are available at
https://gamma.umd.edu/offroad/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VGNMN: Video-grounded Neural Module Network to Video-Grounded Language Tasks. (arXiv:2104.07921v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07921">
<div class="article-summary-box-inner">
<span><p>Neural module networks (NMN) have achieved success in image-grounded tasks
such as Visual Question Answering (VQA) on synthetic images. However, very
limited work on NMN has been studied in the video-grounded dialogue tasks.
These tasks extend the complexity of traditional visual tasks with the
additional visual temporal variance and language cross-turn dependencies.
Motivated by recent NMN approaches on image-grounded tasks, we introduce
Video-grounded Neural Module Network (VGNMN) to model the information retrieval
process in video-grounded language tasks as a pipeline of neural modules. VGNMN
first decomposes all language components in dialogues to explicitly resolve any
entity references and detect corresponding action-based inputs from the
question. The detected entities and actions are used as parameters to
instantiate neural module networks and extract visual cues from the video. Our
experiments show that VGNMN can achieve promising performance on a challenging
video-grounded dialogue benchmark as well as a video QA benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regularized Deep Linear Discriminant Analysis. (arXiv:2105.07129v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07129">
<div class="article-summary-box-inner">
<span><p>As a non-linear extension of the classic Linear Discriminant Analysis(LDA),
Deep Linear Discriminant Analysis(DLDA) replaces the original Categorical Cross
Entropy(CCE) loss function with eigenvalue-based loss function to make a deep
neural network(DNN) able to learn linearly separable hidden representations. In
this paper, we first point out DLDA focuses on training the cooperative
discriminative ability of all the dimensions in the latent subspace, while put
less emphasis on training the separable capacity of single dimension. To
improve DLDA, a regularization method on within-class scatter matrix is
proposed to strengthen the discriminative ability of each dimension, and also
keep them complement each other. Experiment results on STL-10, CIFAR-10 and
Pediatric Pneumonic Chest X-ray Dataset showed that our proposed regularization
method Regularized Deep Linear Discriminant Analysis(RDLDA) outperformed DLDA
and conventional neural network with CCE as objective. To further improve the
discriminative ability of RDLDA in the local space, an algorithm named Subclass
RDLDA is also proposed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview of Deep Learning Techniques for Epileptic Seizures Detection and Prediction Based on Neuroimaging Modalities: Methods, Challenges, and Future Works. (arXiv:2105.14278v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14278">
<div class="article-summary-box-inner">
<span><p>Since epilepsy happens due to abnormal activity in the brain, seizures can
affect any process your brain handles. Some signs and symptoms of seizures
include confusion, abnormal staring, and rapid, sudden, and uncontrollable hand
movements. Epileptic seizure detection methods involve neurological exams,
blood tests, neuropsychological tests, and neuroimaging modalities. Among
these, neuroimaging modalities have received considerable attention from
specialist physicians. One method to facilitate the accurate and fast diagnosis
of epileptic seizures is to employ computer-aided diagnosis systems (CADS)
based on deep learning (DL) and neuroimaging modalities. This paper has studied
a comprehensive overview of DL methods exploited for epileptic seizures
detection and prediction using neuroimaging modalities. First, DL-based CADS
for the epileptic seizures detection and prediction using neuroimaging
modalities are discussed. Also, descriptions of various datasets, preprocessing
algorithms, and DL models which have been used for epileptic seizures detection
and prediction have been included. Then, research on rehabilitation tools has
been presented, which contains brain-computer interface (BCI), implantable,
cloud computing, internet of things (IoT), hardware implementation of DL
techniques on field-programmable gate array (FPGA), etc. In the discussion
section, a comparison has been carried out between research on epileptic
seizure detection and prediction. The most important challenges in epileptic
seizures detection and prediction using neuroimaging modalities and DL models
have been expressed. In addition, future work proposal in the fields of
datasets, DL, rehabilitation, and hardware models has been proposed. The final
section is dedicated to the conclusion and incorporates the most significant
findings in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy. (arXiv:2106.01100v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01100">
<div class="article-summary-box-inner">
<span><p>During lung radiotherapy, the position of infrared reflective objects on the
chest can be recorded to estimate the tumor location. However, radiotherapy
systems have a latency inherent to robot control limitations that impedes the
radiation delivery precision. Prediction with online learning of recurrent
neural networks (RNN) allows for adaptation to non-stationary respiratory
signals, but classical methods such as RTRL and truncated BPTT are respectively
slow and biased. This study investigates the capabilities of unbiased online
recurrent optimization (UORO) to forecast respiratory motion and enhance safety
in lung radiotherapy.
</p>
<p>We used 9 observation records of the 3D position of 3 external markers on the
chest and abdomen of healthy individuals breathing during intervals from 73s to
222s. The sampling frequency was 10Hz, and the amplitudes of the recorded
trajectories range from 6mm to 40mm in the superior-inferior direction. We
forecast the 3D location of each marker simultaneously with a horizon value
between 0.1s and 2.0s, using an RNN trained with UORO. We compare its
performance with an RNN trained with RTRL, LMS, and offline linear regression.
We provide closed-form expressions for quantities involved in the gradient loss
calculation in UORO, thereby making its implementation efficient. Training and
cross-validation were performed during the first minute of each sequence.
</p>
<p>On average over the horizon values considered and the 9 sequences, UORO
achieves the lowest root-mean-square (RMS) error and maximum error among the
compared algorithms. These errors are respectively equal to 1.3mm and 8.8mm,
and the prediction time per time step was lower than 2.8ms (Dell Intel core
i9-9900K 3.60 GHz). Linear regression has the lowest RMS error for the horizon
values 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,
and UORO for horizon values greater than 0.6s.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Temporal Action Detection with Transformer. (arXiv:2106.10271v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10271">
<div class="article-summary-box-inner">
<span><p>Temporal action detection (TAD) aims to determine the semantic label and the
boundaries of every action instance in an untrimmed video. Previous methods
tackle this task with complex pipelines. In this paper, we propose an
end-to-end temporal action detection Transformer (TadTR) with a simple set
prediction pipeline. Given a small set of learnable embeddings called action
queries, TadTR adaptively extracts temporal context from the video for each
query and directly predicts action instances. To adapt Transformer for TAD, we
propose three improvements to enhance its locality awareness. The core is a
temporal deformable attention module that selectively attends to a sparse set
of key snippets in a video. A segment refinement mechanism and an actionness
regression head are designed to refine the boundaries and confidence of the
predicted instances, respectively. TadTR requires lower computation cost than
previous detectors while preserving remarkable performance. As a self-contained
detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and
HACS Segments (32.09% mAP). Combined with an extra action classifier, it
obtains 36.75% mAP on ActivityNet-1.3. Our code is available at
\url{https://github.com/xlliu7/TadTR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Statistics Guided Label Refurbishment Mechanism: Mitigating Label Noise in Medical Image Classification. (arXiv:2106.12284v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12284">
<div class="article-summary-box-inner">
<span><p>Purpose: Deep neural networks (DNNs) have been widely applied in medical
image classification, benefiting from its powerful mapping capability among
medical images. However, these existing deep learning-based methods depend on
an enormous amount of carefully labeled images. Meanwhile, noise is inevitably
introduced in the labeling process, degrading the performance of models. Hence,
it's significant to devise robust training strategies to mitigate label noise
in the medical image classification tasks. Methods: In this work, we propose a
novel Bayesian statistics guided label refurbishment mechanism (BLRM) for DNNs
to prevent overfitting noisy images. BLRM utilizes maximum a posteriori
probability (MAP) in the Bayesian statistics and the exponentially
time-weighted technique to selectively correct the labels of noisy images. The
training images are purified gradually with the training epochs when BLRM is
activated, further improving classification performance. Results: Comprehensive
experiments on both synthetic noisy images (public OCT &amp; Messidor datasets) and
real-world noisy images (ANIMAL-10N) demonstrate that BLRM refurbishes the
noisy labels selectively, curbing the adverse effects of noisy data. Also, the
anti-noise BLRM integrated with DNNs are effective at different noise ratio and
are independent of backbone DNN architectures. In addition, BLRM is superior to
state-of-the-art comparative methods of anti-noise. Conclusions: These
investigations indicate that the proposed BLRM is well capable of mitigating
label noise in medical image classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imaging dynamics beneath turbid media via parallelized single-photon detection. (arXiv:2107.01422v4 [physics.optics] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01422">
<div class="article-summary-box-inner">
<span><p>Noninvasive optical imaging through dynamic scattering media has numerous
important biomedical applications but still remains a challenging task. While
standard diffuse imaging methods measure optical absorption or fluorescent
emission, it is also well-established that the temporal correlation of
scattered coherent light diffuses through tissue much like optical intensity.
Few works to date, however, have aimed to experimentally measure and process
such temporal correlation data to demonstrate deep-tissue video reconstruction
of decorrelation dynamics. In this work, we utilize a single-photon avalanche
diode (SPAD) array camera to simultaneously monitor the temporal dynamics of
speckle fluctuations at the single-photon level from 12 different phantom
tissue surface locations delivered via a customized fiber bundle array. We then
apply a deep neural network to convert the acquired single-photon measurements
into video of scattering dynamics beneath rapidly decorrelating tissue
phantoms. We demonstrate the ability to reconstruct images of transient
(0.1-0.4s) dynamic events occurring up to 8 mm beneath a decorrelating tissue
phantom with millimeter-scale resolution, and highlight how our model can
flexibly extend to monitor flow speed within buried phantom vessels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MINERVAS: Massive INterior EnviRonments VirtuAl Synthesis. (arXiv:2107.06149v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06149">
<div class="article-summary-box-inner">
<span><p>With the rapid development of data-driven techniques, data has played an
essential role in various computer vision tasks. Many realistic and synthetic
datasets have been proposed to address different problems. However, there are
lots of unresolved challenges: (1) the creation of dataset is usually a tedious
process with manual annotations, (2) most datasets are only designed for a
single specific task, (3) the modification or randomization of the 3D scene is
difficult, and (4) the release of commercial 3D data may encounter copyright
issue. This paper presents MINERVAS, a Massive INterior EnviRonments VirtuAl
Synthesis system, to facilitate the 3D scene modification and the 2D image
synthesis for various vision tasks. In particular, we design a programmable
pipeline with Domain-Specific Language, allowing users to (1) select scenes
from the commercial indoor scene database, (2) synthesize scenes for different
tasks with customized rules, and (3) render various imagery data, such as
visual color, geometric structures, semantic label. Our system eases the
difficulty of customizing massive numbers of scenes for different tasks and
relieves users from manipulating fine-grained scene configurations by providing
user-controllable randomness using multi-level samplers. Most importantly, it
empowers users to access commercial scene databases with millions of indoor
scenes and protects the copyright of core data assets, e.g., 3D CAD models. We
demonstrate the validity and flexibility of our system by using our synthesized
data to improve the performance on different kinds of computer vision tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weighted Intersection over Union (wIoU): A New Evaluation Metric for Image Segmentation. (arXiv:2107.09858v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09858">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel evaluation metric for performance
evaluation of semantic segmentation. In recent years, many studies have tried
to train pixel-level classifiers on large-scale image datasets to perform
accurate semantic segmentation. The goal of semantic segmentation is to assign
a class label of each pixel in the scene. It has various potential applications
in computer vision fields e.g., object detection, classification, scene
understanding and Etc. To validate the proposed wIoU evaluation metric, we
tested state-of-the art methods on public benchmark datasets (e.g., KITTI)
based on the proposed wIoU metric and compared with other conventional
evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few Shots Are All You Need: A Progressive Few Shot Learning Approach for Low Resource Handwritten Text Recognition. (arXiv:2107.10064v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10064">
<div class="article-summary-box-inner">
<span><p>Handwritten text recognition in low resource scenarios, such as manuscripts
with rare alphabets, is a challenging problem. The main difficulty comes from
the very few annotated data and the limited linguistic information (e.g.
dictionaries and language models). Thus, we propose a few-shot learning-based
handwriting recognition approach that significantly reduces the human labor
annotation process, requiring only few images of each alphabet symbol. The
method consists in detecting all the symbols of a given alphabet in a textline
image and decoding the obtained similarity scores to the final sequence of
transcribed symbols. Our model is first pretrained on synthetic line images
generated from any alphabet, even though different from the target domain. A
second training step is then applied to diminish the gap between the source and
target data. Since this retraining would require annotation of thousands of
handwritten symbols together with their bounding boxes, we propose to avoid
such human effort through an unsupervised progressive learning approach that
automatically assigns pseudo-labels to the non-annotated data. The evaluation
on different manuscript datasets show that our model can lead to competitive
results with a significant reduction in human effort. The code will be publicly
available in this repository: \url{https://github.com/dali92002/HTRbyMatching}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content-aware Directed Propagation Network with Pixel Adaptive Kernel Attention. (arXiv:2107.13144v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13144">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have been not only widespread but also
achieved noticeable results on numerous applications including image
classification, restoration, and generation. Although the weight-sharing
property of convolutions makes them widely adopted in various tasks, its
content-agnostic characteristic can also be considered a major drawback. To
solve this problem, in this paper, we propose a novel operation, called pixel
adaptive kernel attention (PAKA). PAKA provides directivity to the filter
weights by multiplying spatially varying attention from learnable features. The
proposed method infers pixel-adaptive attention maps along the channel and
spatial directions separately to address the decomposed model with fewer
parameters. Our method is trainable in an end-to-end manner and applicable to
any CNN-based models. In addition, we propose an improved information
aggregation module with PAKA, called the hierarchical PAKA module (HPM). We
demonstrate the superiority of our HPM by presenting state-of-the-art
performance on semantic segmentation compared to the conventional information
aggregation modules. We validate the proposed method through additional
ablation studies and visualizing the effect of PAKA providing directivity to
the weights of convolutions. We also show the generalizability of the proposed
method by applying it to multi-modal tasks especially color-guided depth map
super-resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-Temporal Sparsity. (arXiv:2108.02297v5 [cs.AR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02297">
<div class="article-summary-box-inner">
<span><p>Long Short-Term Memory (LSTM) recurrent networks are frequently used for
tasks involving time-sequential data such as speech recognition. Unlike
previous LSTM accelerators that either exploit spatial weight sparsity or
temporal activation sparsity, this paper proposes a new accelerator called
"Spartus" that exploits spatio-temporal sparsity to achieve ultra-low latency
inference. Spatial sparsity is induced using a new Column-Balanced Targeted
Dropout (CBTD) structured pruning method, producing structured sparse weight
matrices for a balanced workload. The pruned networks running on Spartus
hardware achieve weight sparsity levels of up to 96% and 94% with negligible
accuracy loss on the TIMIT and the Librispeech datasets. To induce temporal
sparsity in LSTM, we extend the previous DeltaGRU method to the DeltaLSTM
method. Combining spatio-temporal sparsity with CBTD and DeltaLSTM saves on
weight memory access and associated arithmetic operations. The Spartus
architecture is scalable and supports real-time online speech recognition when
implemented on small and large FPGAs. Spartus per-sample latency for a single
DeltaLSTM layer of 1024 neurons averages 1 us. Exploiting spatio-temporal
sparsity on our test LSTM network using the TIMIT dataset leads to 46X speedup
of Spartus over its theoretical hardware performance to achieve 9.4 TOp/s
effective batch-1 throughput and 1.1 TOp/s/W power efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Learning Gaussian Anomaly Detection by Fine-tuning Representations. (arXiv:2108.04116v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04116">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art anomaly detection (AD) methods exploit the powerful
representations yielded by large-scale ImageNet training. However, catastrophic
forgetting prevents the successful fine-tuning of pre-trained representations
on new datasets in the semi-supervised setting, and representations are
therefore commonly fixed. In our work, we propose a new method to overcome
catastrophic forgetting and thus successfully fine-tune pre-trained
representations for AD in the transfer learning setting. Specifically, we
induce a multivariate Gaussian distribution for the normal class based on the
linkage between generative and discriminative modeling, and use the Mahalanobis
distance of normal images to the estimated distribution as training objective.
We additionally propose to use augmentations commonly employed for vicinal risk
minimization in a validation scheme to detect onset of catastrophic forgetting.
Extensive evaluations on the public MVTec dataset reveal that a new state of
the art is achieved by our method in the AD task while simultaneously achieving
anomaly segmentation performance comparable to prior state of the art. Further,
ablation studies demonstrate the importance of the induced Gaussian
distribution as well as the robustness of the proposed fine-tuning scheme with
respect to the choice of augmentations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A MIMO Radar-based Few-Shot Learning Approach for Human-ID. (arXiv:2110.08595v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08595">
<div class="article-summary-box-inner">
<span><p>Radar for deep learning-based human identification has become a research area
of increasing interest. It has been shown that micro-Doppler ($\mu$-D) can
reflect the walking behavior through capturing the periodic limbs'
micro-motions. One of the main aspects is maximizing the number of included
classes while considering the real-time and training dataset size constraints.
In this paper, a multiple-input-multiple-output (MIMO) radar is used to
formulate micro-motion spectrograms of the elevation angular velocity
($\mu$-$\omega$). The effectiveness of concatenating this newly-formulated
spectrogram with the commonly used $\mu$-D is investigated. To accommodate for
non-constrained real walking motion, an adaptive cycle segmentation framework
is utilized and a metric learning network is trained on half gait cycles
($\approx$ 0.5 s). Studies on the effects of various numbers of classes
(5--20), different dataset sizes, and varying observation time windows 1--2 s
are conducted. A non-constrained walking dataset of 22 subjects is collected
with different aspect angles with respect to the radar. The proposed few-shot
learning (FSL) approach achieves a classification error of 11.3 % with only 2
min of training data per subject.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP. (arXiv:2110.11316v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11316">
<div class="article-summary-box-inner">
<span><p>CLIP yielded impressive results on zero-shot transfer learning tasks and is
considered as a foundation model like BERT or GPT3. CLIP vision models that
have a rich representation are pre-trained using the InfoNCE objective and
natural language supervision before they are fine-tuned on particular tasks.
Though CLIP excels at zero-shot transfer learning, it suffers from an
explaining away problem, that is, it focuses on one or few features, while
neglecting other relevant features. This problem is caused by insufficiently
extracting the covariance structure in the original multi-modal data. We
suggest to use modern Hopfield networks to tackle the problem of explaining
away. Their retrieved embeddings have an enriched covariance structure derived
from co-occurrences of features in the stored embeddings. However, modern
Hopfield networks increase the saturation effect of the InfoNCE objective which
hampers learning. We propose to use the InfoLOOB objective to mitigate this
saturation effect. We introduce the novel ``Contrastive Leave One Out Boost''
(CLOOB), which uses modern Hopfield networks for covariance enrichment together
with the InfoLOOB objective. In experiments we compare CLOOB to CLIP after
pre-training on the Conceptual Captions and the YFCC dataset with respect to
their zero-shot transfer learning performance on other datasets. CLOOB
consistently outperforms CLIP at zero-shot transfer learning across all
considered architectures and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Person Re-identification with Multi-Modal Joint Defence. (arXiv:2111.09571v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09571">
<div class="article-summary-box-inner">
<span><p>The Person Re-identification (ReID) system based on metric learning has been
proved to inherit the vulnerability of deep neural networks (DNNs), which are
easy to be fooled by adversarail metric attacks. Existing work mainly relies on
adversarial training for metric defense, and more methods have not been fully
studied. By exploring the impact of attacks on the underlying features, we
propose targeted methods for metric attacks and defence methods. In terms of
metric attack, we use the local color deviation to construct the intra-class
variation of the input to attack color features. In terms of metric defenses,
we propose a joint defense method which includes two parts of proactive defense
and passive defense. Proactive defense helps to enhance the robustness of the
model to color variations and the learning of structure relations across
multiple modalities by constructing different inputs from multimodal images,
and passive defense exploits the invariance of structural features in a
changing pixel space by circuitous scaling to preserve structural features
while eliminating some of the adversarial noise. Extensive experiments
demonstrate that the proposed joint defense compared with the existing
adversarial metric defense methods which not only against multiple attacks at
the same time but also has not significantly reduced the generalization
capacity of the model. The code is available at
https://github.com/finger-monkey/multi-modal_joint_defence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks. (arXiv:2111.10854v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10854">
<div class="article-summary-box-inner">
<span><p>Although Capsule Network is powerful at defining the positional relationship
between features in deep neural networks for visual recognition tasks, it is
computationally expensive and not suitable for running on mobile devices. The
bottleneck is in the computational complexity of the Dynamic Routing mechanism
used between the capsules. On the other hand, XNOR-Net is fast and
computationally efficient, though it suffers from low accuracy due to
information loss in the binarization process. To address the computational
burdens of the Dynamic Routing mechanism, this paper proposes new Fully
Connected (FC) layers by xnorizing the linear projector outside or inside the
Dynamic Routing within the CapsFC layer. Specifically, our proposed FC layers
have two versions, XnODR (Xnorize the Linear Projection Linear Projector
Outside Dynamic Routing) and XnIDR (Xnorize the Linear Projection Linear
Projector Inside Dynamic Routing). To test the generalization of both XnODR and
XnIDR, we insert them into two different networks, MobileNet V2 and ResNet-50.
Our experiments on three datasets, MNIST, CIFAR-10, and MultiMNIST validate
their effectiveness. The results demonstrate that both XnODR and XnIDR help
networks to have high accuracy with lower FLOPs and fewer parameters (e.g.,
95.32\% correctness with 2.99M parameters and 312.04M FLOPs on CIFAR-10).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">U-shape Transformer for Underwater Image Enhancement. (arXiv:2111.11843v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11843">
<div class="article-summary-box-inner">
<span><p>The light absorption and scattering of underwater impurities lead to poor
underwater imaging quality. The existing data-driven based underwater image
enhancement (UIE) techniques suffer from the lack of a large-scale dataset
containing various underwater scenes and high-fidelity reference images.
Besides, the inconsistent attenuation in different color channels and space
areas is not fully considered for boosted enhancement. In this work, we
constructed a large-scale underwater image (LSUI) dataset including 5004 image
pairs, and reported an U-shape Transformer network where the transformer model
is for the first time introduced to the UIE task. The U-shape Transformer is
integrated with a channel-wise multi-scale feature fusion transformer (CMSFFT)
module and a spatial-wise global feature modeling transformer (SGFMT) module,
which reinforce the network's attention to the color channels and space areas
with more serious attenuation. Meanwhile, in order to further improve the
contrast and saturation, a novel loss function combining RGB, LAB and LCH color
spaces is designed following the human vision principle. The extensive
experiments on available datasets validate the state-of-the-art performance of
the reported technique with more than 2dB superiority.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSI: A Pedestrian Behavior Dataset for Socially Intelligent Autonomous Car. (arXiv:2112.02604v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02604">
<div class="article-summary-box-inner">
<span><p>Prediction of pedestrian behavior is critical for fully autonomous vehicles
to drive in busy city streets safely and efficiently. The future autonomous
cars need to fit into mixed conditions with not only technical but also social
capabilities. As more algorithms and datasets have been developed to predict
pedestrian behaviors, these efforts lack the benchmark labels and the
capability to estimate the temporal-dynamic intent changes of the pedestrians,
provide explanations of the interaction scenes, and support algorithms with
social intelligence. This paper proposes and shares another benchmark dataset
called the IUPUI-CSRC Pedestrian Situated Intent (PSI) data with two innovative
labels besides comprehensive computer vision labels. The first novel label is
the dynamic intent changes for the pedestrians to cross in front of the
ego-vehicle, achieved from 24 drivers with diverse backgrounds. The second one
is the text-based explanations of the driver reasoning process when estimating
pedestrian intents and predicting their behaviors during the interaction
period. These innovative labels can enable several computer vision tasks,
including pedestrian intent/behavior prediction, vehicle-pedestrian interaction
segmentation, and video-to-language mapping for explainable algorithms. The
released dataset can fundamentally improve the development of pedestrian
behavior prediction models and develop socially intelligent autonomous cars to
interact with pedestrians efficiently. The dataset has been evaluated with
different tasks and is released to the public to access.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Paced Deep Regression Forests with Consideration of Ranking Fairness. (arXiv:2112.06455v8 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06455">
<div class="article-summary-box-inner">
<span><p>Deep discriminative models (DDMs), e.g. deep regression forests and deep
decision forests, have been extensively studied recently to solve problems such
as facial age estimation, head pose estimation, etc.. Due to a shortage of
well-labeled data that does not have noise and imbalanced distribution
problems, learning DDMs is always challenging. Existing methods usually tackle
these challenges through learning more discriminative features or re-weighting
samples. We argue that learning DDMs gradually, from easy to hard, is more
reasonable, for two reasons. First, this is more consistent with the cognitive
process of human beings. Second, noisy as well as underrepresented examples can
be distinguished by virtue of previously learned knowledge. Thus, we resort to
a gradual learning strategy -- self-paced learning (SPL). Then, a natural
question arises: can SPL lead DDMs to achieve more robust and less biased
solutions? To answer this question, this paper proposes a new SPL method: easy
and underrepresented examples first, for learning DDMs. This tackles the
fundamental ranking and selection problem in SPL from a new perspective:
fairness. Our idea is fundamental and can be easily combined with a variety of
DDMs. Extensive experimental results on three computer vision tasks, i.e.,
facial age estimation, head pose estimation, and gaze estimation, show our new
method gains considerable performance improvement in both accuracy and
fairness. Source code is available at https://github.com/learninginvision/SPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08879">
<div class="article-summary-box-inner">
<span><p>Most language grounding models learn to select the referred object from a
pool of object proposals provided by a pre-trained detector. This object
proposal bottleneck is limiting because an utterance may refer to visual
entities at various levels of granularity, such as the chair, the leg of a
chair, or the tip of the front leg of a chair, which may be missed by the
detector. Recently, MDETR introduced a language grounding model for 2D images
that do not have such a box proposal bottleneck; instead of selecting objects
from a proposal pool, it instead decodes the referenced object boxes directly
from image and language features and achieves big leaps in performance. We
propose a language grounding model for 3D scenes built on MDETR, which we call
BEAUTY-DETR, from bottom-up and top-down DETR. BEAUTY-DETR attends on an
additional object proposal pool computed bottom-up from a pre-trained detector.
Yet it decodes referenced objects without selecting them from the pool. In this
way, it uses powerful object detectors to help ground language without being
restricted by their misses. Second, BEAUTY-DETR augments supervision from
language grounding annotations by configuring object detection annotations as
language prompts to be grounded in images. The proposed model sets a new
state-of-the-art across popular 3D language grounding benchmarks with
significant performance gains over previous 3D approaches (12.6% on SR3D, 11.6%
on NR3D and 6.3% on ScanRefer). It outperforms a straightforward MDETR for the
3D point clouds method we implemented by 6.7% on SR3D, 11.8% on NR3D and 5% on
the ScanRefer benchmark. When applied to language grounding in 2D images, it
performs on par with MDETR. We ablate each of the design choices of the model
and quantify their contribution to performance. Code and checkpoints are
available at https://github.com/nickgkan/beauty_detr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multispectral image fusion by super pixel statistics. (arXiv:2112.11329v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11329">
<div class="article-summary-box-inner">
<span><p>Multispectral image fusion is a fundamental problem of image processing and
remote sensing. This problem is addressed by both classic and deep learning
approaches. This paper is focused on the classic solutions that can work in
real-time systems and introduces a new novel approach to this group of works.
The proposed method carries out multispectral image fusion based on the content
of the fused images. Furthermore, it relies on an analysis of the level of
information of segmented superpixels in the fused inputs. Specifically, the
proposed method addresses the task of visible color RGB to Near-Infrared (NIR)
fusion. The RGB image captures the color of the scene while the NIR channel
captures details and sees beyond haze and clouds. Since each channel senses
different information of the scene, their multispectral fusion is challenging
and interesting. Therefore, the proposed method is designed to produce a fusion
that contains the relevant content of each spectra. The experiments of this
manuscript show that the proposed method is visually informative with respect
to other classic fusion methods. Moreover, it can be run fastly on embedded
devices without heavy computation requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoCoPnet: Exploring Local Motion and Contrast Priors for Infrared Small Target Super-Resolution. (arXiv:2201.01014v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01014">
<div class="article-summary-box-inner">
<span><p>Infrared small target super-resolution (SR) aims to recover reliable and
detailed high-resolution image with highcontrast targets from its
low-resolution counterparts. Since the infrared small target lacks color and
fine structure information, it is significant to exploit the supplementary
information among sequence images to enhance the target. In this paper, we
propose the first infrared small target SR method named local motion and
contrast prior driven deep network (MoCoPnet) to integrate the domain knowledge
of infrared small target into deep network, which can mitigate the intrinsic
feature scarcity of infrared small targets. Specifically, motivated by the
local motion prior in the spatio-temporal dimension, we propose a local
spatiotemporal attention module to perform implicit frame alignment and
incorporate the local spatio-temporal information to enhance the local features
(especially for small targets). Motivated by the local contrast prior in the
spatial dimension, we propose a central difference residual group to
incorporate the central difference convolution into the feature extraction
backbone, which can achieve center-oriented gradient-aware feature extraction
to further improve the target contrast. Extensive experiments have demonstrated
that our method can recover accurate spatial dependency and improve the target
contrast. Comparative results show that MoCoPnet can outperform the
state-of-the-art video SR and single image SR methods in terms of both SR
performance and target enhancement. Based on the SR results, we further
investigate the influence of SR on infrared small target detection and the
experimental results demonstrate that MoCoPnet promotes the detection
performance. The code is available at https://github.com/XinyiYing/MoCoPnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralMLS: Geometry-Aware Control Point Deformation. (arXiv:2201.01873v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01873">
<div class="article-summary-box-inner">
<span><p>We introduce NeuralMLS, a space-based deformation technique, guided by a set
of displaced control points. We leverage the power of neural networks to inject
the underlying shape geometry into the deformation parameters. The goal of our
technique is to enable a realistic and intuitive shape deformation. Our method
is built upon moving least-squares (MLS), since it minimizes a weighted sum of
the given control point displacements. Traditionally, the influence of each
control point on every point in space (i.e., the weighting function) is defined
using inverse distance heuristics. In this work, we opt to learn the weighting
function, by training a neural network on the control points from a single
input shape, and exploit the innate smoothness of neural networks. Our
geometry-aware control point deformation is agnostic to the surface
representation and quality; it can be applied to point clouds or meshes,
including non-manifold and disconnected surface soups. We show that our
technique facilitates intuitive piecewise smooth deformations, which are well
suited for manufactured objects. We show the advantages of our approach
compared to existing surface and space-based deformation techniques, both
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Mix-normalization Method for Generalizable Multi-source Person Re-identification. (arXiv:2201.09846v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09846">
<div class="article-summary-box-inner">
<span><p>Person re-identification (Re-ID) has achieved great success in the supervised
scenario. However, it is difficult to directly transfer the supervised model to
arbitrary unseen domains due to the model overfitting to the seen source
domains. In this paper, we aim to tackle the generalizable multi-source person
Re-ID task (i.e., there are multiple available source domains, and the testing
domain is unseen during training) from the data augmentation perspective, thus
we put forward a novel method, termed MixNorm, which consists of domain-aware
mix-normalization (DMN) and domain-ware center regularization (DCR). Different
from the conventional data augmentation, the proposed domain-aware
mix-normalization to enhance the diversity of features during training from the
normalization view of the neural network, which can effectively alleviate the
model overfitting to the source domains, so as to boost the generalization
capability of the model in the unseen domain. To better learn the
domain-invariant model, we further develop the domain-aware center
regularization to better map the produced diverse features into the same space.
Extensive experiments on multiple benchmark datasets validate the effectiveness
of the proposed method and show that the proposed method can outperform the
state-of-the-art methods. Besides, further analysis also reveals the
superiority of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Moving Vehicle Detection from Audio-Visual Cues. (arXiv:2201.12771v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12771">
<div class="article-summary-box-inner">
<span><p>Robust detection of moving vehicles is a critical task for any autonomously
operating outdoor robot or self-driving vehicle. Most modern approaches for
solving this task rely on training image-based detectors using large-scale
vehicle detection datasets such as nuScenes or the Waymo Open Dataset.
Providing manual annotations is an expensive and laborious exercise that does
not scale well in practice. To tackle this problem, we propose a
self-supervised approach that leverages audio-visual cues to detect moving
vehicles in videos. Our approach employs contrastive learning for localizing
vehicles in images from corresponding pairs of images and recorded audio. In
extensive experiments carried out with a real-world dataset, we demonstrate
that our approach provides accurate detections of moving vehicles and does not
require manual annotations. We furthermore show that our model can be used as a
teacher to supervise an audio-only detection model. This student model is
invariant to illumination changes and thus effectively bridges the domain gap
inherent to models leveraging exclusively vision as the predominant modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COIN++: Neural Compression Across Modalities. (arXiv:2201.12904v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12904">
<div class="article-summary-box-inner">
<span><p>Neural compression algorithms are typically based on autoencoders that
require specialized encoder and decoder architectures for different data
modalities. In this paper, we propose COIN++, a neural compression framework
that seamlessly handles a wide range of data modalities. Our approach is based
on converting data to implicit neural representations, i.e. neural functions
that map coordinates (such as pixel locations) to features (such as RGB
values). Then, instead of storing the weights of the implicit neural
representation directly, we store modulations applied to a meta-learned base
network as a compressed code for the data. We further quantize and entropy code
these modulations, leading to large compression gains while reducing encoding
time by two orders of magnitude compared to baselines. We empirically
demonstrate the effectiveness of our method by compressing various data
modalities, from images and audio to medical and climate data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Network-level Safety Metrics for Overall Traffic Safety Assessment: A Case Study. (arXiv:2201.13229v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13229">
<div class="article-summary-box-inner">
<span><p>Driving safety analysis has recently experienced unprecedented improvements
thanks to technological advances in precise positioning sensors, artificial
intelligence (AI)-based safety features, autonomous driving systems, connected
vehicles, high-throughput computing, and edge computing servers. Particularly,
deep learning (DL) methods empowered volume video processing to extract
safety-related features from massive videos captured by roadside units (RSU).
Safety metrics are commonly used measures to investigate crashes and
near-conflict events. However, these metrics provide limited insight into the
overall network-level traffic management. On the other hand, some safety
assessment efforts are devoted to processing crash reports and identifying
spatial and temporal patterns of crashes that correlate with road geometry,
traffic volume, and weather conditions. This approach relies merely on crash
reports and ignores the rich information of traffic videos that can help
identify the role of safety violations in crashes. To bridge these two
perspectives, we define a new set of network-level safety metrics (NSM) to
assess the overall safety profile of traffic flow by processing imagery taken
by RSU cameras. Our analysis suggests that NSMs show significant statistical
associations with crash rates. This approach is different than simply
generalizing the results of individual crash analyses, since all vehicles
contribute to calculating NSMs, not only the ones involved in crash incidents.
This perspective considers the traffic flow as a complex dynamic system where
actions of some nodes can propagate through the network and influence the crash
risk for other nodes. We also provide a comprehensive review of surrogate
safety metrics (SSM) in the Appendix A.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-Sensor Binarized Fully Convolutional Neural Network with A Pixel Processor Array. (arXiv:2202.00836v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00836">
<div class="article-summary-box-inner">
<span><p>This work presents a method to implement fully convolutional neural networks
(FCNs) on Pixel Processor Array (PPA) sensors, and demonstrates coarse
segmentation and object localisation tasks. We design and train binarized FCN
for both binary weights and activations using batchnorm, group convolution, and
learnable threshold for binarization, producing networks small enough to be
embedded on the focal plane of the PPA, with limited local memory resources,
and using parallel elementary add/subtract, shifting, and bit operations only.
We demonstrate the first implementation of an FCN on a PPA device, performing
three convolution layers entirely in the pixel-level processors. We use this
architecture to demonstrate inference generating heat maps for object
segmentation and localisation at over 280 FPS using the SCAMP-5 PPA vision
chip.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking and Analyzing Point Cloud Classification under Corruptions. (arXiv:2202.03377v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03377">
<div class="article-summary-box-inner">
<span><p>3D perception, especially point cloud classification, has achieved
substantial progress. However, in real-world deployment, point cloud
corruptions are inevitable due to the scene complexity, sensor inaccuracy, and
processing imprecision. In this work, we aim to rigorously benchmark and
analyze point cloud classification under corruptions. To conduct a systematic
investigation, we first provide a taxonomy of common 3D corruptions and
identify the atomic corruptions. Then, we perform a comprehensive evaluation on
a wide range of representative point cloud models to understand their
robustness and generalizability. Our benchmark results show that although point
cloud classification performance improves over time, the state-of-the-art
methods are on the verge of being less robust. Based on the obtained
observations, we propose several effective techniques to enhance point cloud
classifier robustness. We hope our comprehensive benchmark, in-depth analysis,
and proposed techniques could spark future research in robust 3D perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class Distance Weighted Cross-Entropy Loss for Ulcerative Colitis Severity Estimation. (arXiv:2202.05167v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05167">
<div class="article-summary-box-inner">
<span><p>In scoring systems used to measure the endoscopic activity of ulcerative
colitis, such as Mayo endoscopic score or Ulcerative Colitis Endoscopic Index
Severity, levels increase with severity of the disease activity. Such relative
ranking among the scores makes it an ordinal regression problem. On the other
hand, most studies use categorical cross-entropy loss function to train deep
learning models, which is not optimal for the ordinal regression problem. In
this study, we propose a novel loss function, class distance weighted
cross-entropy (CDW-CE), that respects the order of the classes and takes the
distance of the classes into account in calculation of the cost. Experimental
evaluations show that models trained with CDW-CE outperform the models trained
with conventional categorical cross-entropy and other commonly used loss
functions which are designed for the ordinal regression problems. In addition,
the class activation maps of models trained with CDW-CE loss are more
class-discriminative and they are found to be more reasonable by the domain
experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opinions Vary? Diagnosis First!. (arXiv:2202.06505v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06505">
<div class="article-summary-box-inner">
<span><p>With the advancement of deep learning techniques, an increasing number of
methods have been proposed for optic disc and cup (OD/OC) segmentation from the
fundus images. Clinically, OD/OC segmentation is often annotated by multiple
clinical experts to mitigate the personal bias. However, it is hard to train
the automated deep learning models on multiple labels. A common practice to
tackle the issue is majority vote, e.g., taking the average of multiple labels.
However such a strategy ignores the different expertness of medical experts.
Motivated by the observation that OD/OC segmentation is often used for the
glaucoma diagnosis clinically, in this paper, we propose a novel strategy to
fuse the multi-rater OD/OC segmentation labels via the glaucoma diagnosis
performance. Specifically, we assess the expertness of each rater through an
attentive glaucoma diagnosis network. For each rater, its contribution for the
diagnosis will be reflected as an expertness map. To ensure the expertness maps
are general for different glaucoma diagnosis models, we further propose an
Expertness Generator (ExpG) to eliminate the high-frequency components in the
optimization process. Based on the obtained expertness maps, the multi-rater
labels can be fused as a single ground-truth which we dubbed as Diagnosis First
Ground-truth (DiagFirstGT). Experimental results show that by using DiagFirstGT
as ground-truth, OD/OC segmentation networks will predict the masks with
superior glaucoma diagnosis performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Light Fields for Super-Resolution and Disparity Estimation. (arXiv:2202.10603v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10603">
<div class="article-summary-box-inner">
<span><p>Light field (LF) cameras record both intensity and directions of light rays,
and encode 3D scenes into 4D LF images. Recently, many convolutional neural
networks (CNNs) have been proposed for various LF image processing tasks.
However, it is challenging for CNNs to effectively process LF images since the
spatial and angular information are highly inter-twined with varying
disparities. In this paper, we propose a generic mechanism to disentangle these
coupled information for LF image processing. Specifically, we first design a
class of domain-specific convolutions to disentangle LFs from different
dimensions, and then leverage these disentangled features by designing
task-specific modules. Our disentangling mechanism can well incorporate the LF
structure prior and effectively handle 4D LF data. Based on the proposed
mechanism, we develop three networks (i.e., DistgSSR, DistgASR and DistgDisp)
for spatial super-resolution, angular super-resolution and disparity
estimation. Experimental results show that our networks achieve
state-of-the-art performance on all these three tasks, which demonstrates the
effectiveness, efficiency, and generality of our disentangling mechanism.
Project page: https://yingqianwang.github.io/DistgLF/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voice-Face Homogeneity Tells Deepfake. (arXiv:2203.02195v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02195">
<div class="article-summary-box-inner">
<span><p>Detecting forgery videos is highly desirable due to the abuse of deepfake.
Existing detection approaches contribute to exploring the specific artifacts in
deepfake videos and fit well on certain data. However, the growing technique on
these artifacts keeps challenging the robustness of traditional deepfake
detectors. As a result, the development of generalizability of these approaches
has reached a blockage. To address this issue, given the empirical results that
the identities behind voices and faces are often mismatched in deepfake videos,
and the voices and faces have homogeneity to some extent, in this paper, we
propose to perform the deepfake detection from an unexplored voice-face
matching view. To this end, a voice-face matching method is devised to measure
the matching degree of these two. Nevertheless, training on specific deepfake
datasets makes the model overfit certain traits of deepfake algorithms. We
instead, advocate a method that quickly adapts to untapped forgery, with a
pre-training then fine-tuning paradigm. Specifically, we first pre-train the
model on a generic audio-visual dataset, followed by the fine-tuning on
downstream deepfake data. We conduct extensive experiments over three widely
exploited deepfake datasets - DFDC, FakeAVCeleb, and DeepfakeTIMIT. Our method
obtains significant performance gains as compared to other state-of-the-art
competitors. It is also worth noting that our method already achieves
competitive results when fine-tuned on limited deepfake data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Optical Flow, Depth, and Scene Flow without Real-World Labels. (arXiv:2203.15089v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15089">
<div class="article-summary-box-inner">
<span><p>Self-supervised monocular depth estimation enables robots to learn 3D
perception from raw video streams. This scalable approach leverages projective
geometry and ego-motion to learn via view synthesis, assuming the world is
mostly static. Dynamic scenes, which are common in autonomous driving and
human-robot interaction, violate this assumption. Therefore, they require
modeling dynamic objects explicitly, for instance via estimating pixel-wise 3D
motion, i.e. scene flow. However, the simultaneous self-supervised learning of
depth and scene flow is ill-posed, as there are infinitely many combinations
that result in the same 3D point. In this paper we propose DRAFT, a new method
capable of jointly learning depth, optical flow, and scene flow by combining
synthetic data with geometric self-supervision. Building upon the RAFT
architecture, we learn optical flow as an intermediate task to bootstrap depth
and scene flow learning via triangulation. Our algorithm also leverages
temporal and geometric consistency losses across tasks to improve multi-task
learning. Our DRAFT architecture simultaneously establishes a new state of the
art in all three tasks in the self-supervised monocular setting on the standard
KITTI benchmark. Project page: https://sites.google.com/tri.global/draft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EResFD: Rediscovery of the Effectiveness of Standard Convolution for Lightweight Face Detection. (arXiv:2204.01209v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01209">
<div class="article-summary-box-inner">
<span><p>This paper analyses the design choices of face detection architecture that
improve efficiency between computation cost and accuracy. Specifically, we
re-examine the effectiveness of the standard convolutional block as a
lightweight backbone architecture on face detection. Unlike the current
tendency of lightweight architecture design, which heavily utilizes depthwise
separable convolution layers, we show that heavily channel-pruned standard
convolution layer can achieve better accuracy and inference speed when using a
similar parameter size. This observation is supported by the analyses
concerning the characteristics of the target data domain, face. Based on our
observation, we propose to employ ResNet with a highly reduced channel, which
surprisingly allows high efficiency compared to other mobile-friendly networks
(e.g., MobileNet-V1,-V2,-V3). From the extensive experiments, we show that the
proposed backbone can replace that of the state-of-the-art face detector with a
faster inference speed. Also, we further propose a new feature aggregation
method maximizing the detection performance. Our proposed detector EResFD
obtained 80.4% mAP on WIDER FACE Hard subset which only takes 37.7 ms for VGA
image inference in on CPU. Code will be available at
https://github.com/clovaai/EResFD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"This is my unicorn, Fluffy": Personalizing frozen vision-language representations. (arXiv:2204.01694v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01694">
<div class="article-summary-box-inner">
<span><p>Large Vision &amp; Language models pretrained on web-scale data provide
representations that are invaluable for numerous V&amp;L problems. However, it is
unclear how they can be used for reasoning about user-specific visual concepts
in unstructured language. This problem arises in multiple domains, from
personalized image retrieval to personalized interaction with smart devices. We
introduce a new learning setup called Personalized Vision &amp; Language (PerVL)
with two new benchmark datasets for retrieving and segmenting user-specific
"personalized" concepts "in the wild". In PerVL, one should learn personalized
concepts (1) independently of the downstream task (2) allowing a pretrained
model to reason about them with free language, and (3) does not require
personalized negative examples. We propose an architecture for solving PerVL
that operates by extending the input vocabulary of a pretrained model with new
word embeddings for the new personalized concepts. The model can then reason
about them by simply using them in a sentence. We demonstrate that our approach
learns personalized visual concepts from a few examples and can effectively
apply them in image retrieval and semantic segmentation using rich textual
queries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transient motion classification through turbid volumes via parallelized single-photon detection and deep contrastive embedding. (arXiv:2204.01733v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01733">
<div class="article-summary-box-inner">
<span><p>Fast noninvasive probing of spatially varying decorrelating events, such as
cerebral blood flow beneath the human skull, is an essential task in various
scientific and clinical settings. One of the primary optical techniques used is
diffuse correlation spectroscopy (DCS), whose classical implementation uses a
single or few single-photon detectors, resulting in poor spatial localization
accuracy and relatively low temporal resolution. Here, we propose a technique
termed Classifying Rapid decorrelation Events via Parallelized single photon
dEtection (CREPE)}, a new form of DCS that can probe and classify different
decorrelating movements hidden underneath turbid volume with high sensitivity
using parallelized speckle detection from a $32\times32$ pixel SPAD array. We
evaluate our setup by classifying different spatiotemporal-decorrelating
patterns hidden beneath a 5mm tissue-like phantom made with rapidly
decorrelating dynamic scattering media. Twelve multi-mode fibers are used to
collect scattered light from different positions on the surface of the tissue
phantom. To validate our setup, we generate perturbed decorrelation patterns by
both a digital micromirror device (DMD) modulated at multi-kilo-hertz rates, as
well as a vessel phantom containing flowing fluid. Along with a deep
contrastive learning algorithm that outperforms classic unsupervised learning
methods, we demonstrate our approach can accurately detect and classify
different transient decorrelation events (happening in 0.1-0.4s) underneath
turbid scattering media, without any data labeling. This has the potential to
be applied to noninvasively monitor deep tissue motion patterns, for example
identifying normal or abnormal cerebral blood flow events, at multi-Hertz rates
within a compact and static detection probe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Permanence Emerges in a Random Walk along Memory. (arXiv:2204.01784v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01784">
<div class="article-summary-box-inner">
<span><p>This paper proposes a self-supervised objective for learning representations
that localize objects under occlusion - a property known as object permanence.
A central question is the choice of learning signal in cases of total
occlusion. Rather than directly supervising the locations of invisible objects,
we propose a self-supervised objective that requires neither human annotation,
nor assumptions about object dynamics. We show that object permanence can
emerge by optimizing for temporal coherence of memory: we fit a Markov walk
along a space-time graph of memories, where the states in each time step are
non-Markovian features from a sequence encoder. This leads to a memory
representation that stores occluded objects and predicts their motion, to
better localize them. The resulting model outperforms existing approaches on
several datasets of increasing complexity and realism, despite requiring
minimal supervision, and hence being broadly applicable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Frame Self-Supervised Depth with Transformers. (arXiv:2204.07616v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07616">
<div class="article-summary-box-inner">
<span><p>Multi-frame depth estimation improves over single-frame approaches by also
leveraging geometric relationships between images via feature matching, in
addition to learning appearance-based features. In this paper we revisit
feature matching for self-supervised monocular depth estimation, and propose a
novel transformer architecture for cost volume generation. We use
depth-discretized epipolar sampling to select matching candidates, and refine
predictions through a series of self- and cross-attention layers. These layers
sharpen the matching probability between pixel features, improving over
standard similarity metrics prone to ambiguities and local minima. The refined
cost volume is decoded into depth estimates, and the whole pipeline is trained
end-to-end from videos using only a photometric objective. Experiments on the
KITTI and DDAD datasets show that our DepthFormer architecture establishes a
new state of the art in self-supervised monocular depth estimation, and is even
competitive with highly specialized supervised single-frame architectures. We
also show that our learned cross-attention network yields representations
transferable across datasets, increasing the effectiveness of pre-training
strategies. Project page: https://sites.google.com/tri.global/depthformer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimMC: Simple Masked Contrastive Learning of Skeleton Representations for Unsupervised Person Re-Identification. (arXiv:2204.09826v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09826">
<div class="article-summary-box-inner">
<span><p>Recent advances in skeleton-based person re-identification (re-ID) obtain
impressive performance via either hand-crafted skeleton descriptors or skeleton
representation learning with deep learning paradigms. However, they typically
require skeletal pre-modeling and label information for training, which leads
to limited applicability of these methods. In this paper, we focus on
unsupervised skeleton-based person re-ID, and present a generic Simple Masked
Contrastive learning (SimMC) framework to learn effective representations from
unlabeled 3D skeletons for person re-ID. Specifically, to fully exploit
skeleton features within each skeleton sequence, we first devise a masked
prototype contrastive learning (MPC) scheme to cluster the most typical
skeleton features (skeleton prototypes) from different subsequences randomly
masked from raw sequences, and contrast the inherent similarity between
skeleton features and different prototypes to learn discriminative skeleton
representations without using any label. Then, considering that different
subsequences within the same sequence usually enjoy strong correlations due to
the nature of motion continuity, we propose the masked intra-sequence
contrastive learning (MIC) to capture intra-sequence pattern consistency
between subsequences, so as to encourage learning more effective skeleton
representations for person re-ID. Extensive experiments validate that the
proposed SimMC outperforms most state-of-the-art skeleton-based methods. We
further show its scalability and efficiency in enhancing the performance of
existing models. Our codes are available at https://github.com/Kali-Hac/SimMC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Attention Emerges from Recurrent Sparse Reconstruction. (arXiv:2204.10962v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10962">
<div class="article-summary-box-inner">
<span><p>Visual attention helps achieve robust perception under noise, corruption, and
distribution shifts in human vision, which are areas where modern neural
networks still fall short. We present VARS, Visual Attention from Recurrent
Sparse reconstruction, a new attention formulation built on two prominent
features of the human visual attention mechanism: recurrency and sparsity.
Related features are grouped together via recurrent connections between
neurons, with salient objects emerging via sparse regularization. VARS adopts
an attractor network with recurrent connections that converges toward a stable
pattern over time. Network layers are represented as ordinary differential
equations (ODEs), formulating attention as a recurrent attractor network that
equivalently optimizes the sparse reconstruction of input using a dictionary of
"templates" encoding underlying patterns of data. We show that self-attention
is a special case of VARS with a single-step optimization and no sparsity
constraint. VARS can be readily used as a replacement for self-attention in
popular vision transformers, consistently improving their robustness across
various benchmarks. Code is released on GitHub (https://github.com/bfshi/VARS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning. (arXiv:2204.11167v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11167">
<div class="article-summary-box-inner">
<span><p>Reasoning about visual relationships is central to how humans interpret the
visual world. This task remains challenging for current deep learning
algorithms since it requires addressing three key technical problems jointly:
1) identifying object entities and their properties, 2) inferring semantic
relations between pairs of entities, and 3) generalizing to novel
object-relation combinations, i.e., systematic generalization. In this work, we
use vision transformers (ViTs) as our base model for visual reasoning and make
better use of concepts defined as object entities and their relations to
improve the reasoning ability of ViTs. Specifically, we introduce a novel
concept-feature dictionary to allow flexible image feature retrieval at
training time with concept keys. This dictionary enables two new concept-guided
auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a
local task for facilitating semantic object-centric correspondence learning. To
examine the systematic generalization of visual reasoning models, we introduce
systematic splits for the standard HICO and GQA benchmarks. We show the
resulting model, Concept-guided Vision Transformer (or RelViT for short)
significantly outperforms prior approaches on HICO and GQA by 16% and 13% in
the original split, and by 43% and 18% in the systematic split. Our ablation
analyses also reveal our model's compatibility with multiple ViT variants and
robustness to hyper-parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework. (arXiv:2205.03860v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03860">
<div class="article-summary-box-inner">
<span><p>Vision-language pre-training (VLP) on large-scale datasets has shown premier
performance on various downstream tasks. A complete and fair benchmark (i.e.,
including large-scale pre-training datasets and diverse downstream tasks) is
essential for VLP. While there are plenty of benchmarks with English corpus,
building a rich benchmark for VLP with other languages, such as Chinese,
remains a critical problem. To this end, we build a large-scale Chinese
cross-modal benchmark called Zero for the research community to fairly compare
VLP models. We release two pre-training datasets and five fine-tuning datasets
for downstream tasks. Alongside, we propose a novel pre-training framework of
pre-Ranking + Ranking for cross-modal learning. Specifically, we apply global
contrastive pre-ranking to learn the individual representations of images and
texts, respectively. We then fuse the representations in a fine-grained ranking
manner via an image-text cross encoder and a text-image cross encoder. To
further enhance the capability of the model, we propose a two-way distillation
strategy consisting of target-guided Distillation and feature-guided
Distillation. For brevity, we name our model R2D2. We achieve state-of-the-art
performance on four public cross-modal datasets and the proposed five
downstream datasets. When conducting zero-shot tasks on Flickr30k-CN, COCO-CN,
and MUGE, R2D2 pre-trained on a 250 million dataset achieves significant
improvements of 4.7%, 5.4%, and 6.3% in mean recall compared to the
state-of-the-art. The datasets, models, and codes are available at
https://github.com/yuxie11/R2D2
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anatomy-aware Self-supervised Learning for Anomaly Detection in Chest Radiographs. (arXiv:2205.04282v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04282">
<div class="article-summary-box-inner">
<span><p>Large numbers of labeled medical images are essential for the accurate
detection of anomalies, but manual annotation is labor-intensive and
time-consuming. Self-supervised learning (SSL) is a training method to learn
data-specific features without manual annotation. Several SSL-based models have
been employed in medical image anomaly detection. These SSL methods effectively
learn representations in several field-specific images, such as natural and
industrial product images. However, owing to the requirement of medical
expertise, typical SSL-based models are inefficient in medical image anomaly
detection. We present an SSL-based model that enables anatomical
structure-based unsupervised anomaly detection (UAD). The model employs the
anatomy-aware pasting (AnatPaste) augmentation tool. AnatPaste employs a
threshold-based lung segmentation pretext task to create anomalies in normal
chest radiographs, which are used for model pretraining. These anomalies are
similar to real anomalies and help the model recognize them. We evaluate our
model on three opensource chest radiograph datasets. Our model exhibit area
under curves (AUC) of 92.1%, 78.7%, and 81.9%, which are the highest among
existing UAD models. This is the first SSL model to employ anatomical
information as a pretext task. AnatPaste can be applied in various deep
learning models and downstream tasks. It can be employed for other modalities
by fixing appropriate segmentation. Our code is publicly available at:
https://github.com/jun-sato/AnatPaste.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Deep Learning Methods in Medical Imaging Diagnosis: A Survey. (arXiv:2205.04766v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04766">
<div class="article-summary-box-inner">
<span><p>The remarkable success of deep learning has prompted interest in its
application to medical imaging diagnosis. Even though state-of-the-art deep
learning models have achieved human-level accuracy on the classification of
different types of medical data, these models are hardly adopted in clinical
workflows, mainly due to their lack of interpretability. The black-box-ness of
deep learning models has raised the need for devising strategies to explain the
decision process of these models, leading to the creation of the topic of
eXplainable Artificial Intelligence (XAI). In this context, we provide a
thorough survey of XAI applied to medical imaging diagnosis, including visual,
textual, example-based and concept-based explanation methods. Moreover, this
work reviews the existing medical imaging datasets and the existing metrics for
evaluating the quality of the explanations. In addition, we include a
performance comparison among a set of report generation-based methods. Finally,
the major challenges in applying XAI to medical imaging and the future research
directions on the topic are also discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSO-Convolutional Neural Networks with Heterogeneous Learning Rate. (arXiv:2205.10456v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10456">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (ConvNets or CNNs) have been candidly deployed
in the scope of computer vision and related fields. Nevertheless, the dynamics
of training of these neural networks lie still elusive: it is hard and
computationally expensive to train them. A myriad of architectures and training
strategies have been proposed to overcome this challenge and address several
problems in image processing such as speech, image and action recognition as
well as object detection. In this article, we propose a novel Particle Swarm
Optimization (PSO) based training for ConvNets. In such framework, the vector
of weights of each ConvNet is typically cast as the position of a particle in
phase space whereby PSO collaborative dynamics intertwines with Stochastic
Gradient Descent (SGD) in order to boost training performance and
generalization. Our approach goes as follows: i) [regular phase] each ConvNet
is trained independently via SGD; ii) [collaborative phase] ConvNets share
among themselves their current vector of weights (or particle-position) along
with their gradient estimates of the Loss function. Distinct step sizes are
coined by distinct ConvNets. By properly blending ConvNets with large (possibly
random) step-sizes along with more conservative ones, we propose an algorithm
with competitive performance with respect to other PSO-based approaches on
Cifar-10 (accuracy of 98.31%). These accuracy levels are obtained by resorting
to only four ConvNets -- such results are expected to scale with the number of
collaborative ConvNets accordingly. We make our source codes available for
download https://github.com/leonlha/PSO-ConvNet-Dynamics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering. (arXiv:2205.11506v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11506">
<div class="article-summary-box-inner">
<span><p>Federated learning is generally used in tasks where labels are readily
available (e.g., next word prediction). Relaxing this constraint requires
design of unsupervised learning techniques that can support desirable
properties for federated training: robustness to statistical/systems
heterogeneity, scalability with number of participants, and communication
efficiency. Prior work on this topic has focused on directly extending
centralized self-supervised learning techniques, which are not designed to have
the properties listed above. To address this situation, we propose Orchestra, a
novel unsupervised federated learning technique that exploits the federation's
hierarchy to orchestrate a distributed clustering task and enforce a globally
consistent partitioning of clients' data into discriminable clusters. We show
the algorithmic pipeline in Orchestra guarantees good generalization
performance under a linear probe, allowing it to outperform alternative
techniques in a broad range of conditions, including variation in
heterogeneity, number of clients, participation ratio, and local epochs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Model Generalization for Monocular 3D Object Detection. (arXiv:2205.11664v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11664">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection (Mono3D) has achieved tremendous improvements
with emerging large-scale autonomous driving datasets and the rapid development
of deep learning techniques. However, caused by severe domain gaps (e.g., the
field of view (FOV), pixel size, and object size among datasets), Mono3D
detectors have difficulty in generalization, leading to drastic performance
degradation on unseen domains. To solve these issues, we combine the
position-invariant transform and multi-scale training with the pixel-size depth
strategy to construct an effective unified camera-generalized paradigm (CGP).
It fully considers discrepancies in the FOV and pixel size of images captured
by different cameras. Moreover, we further investigate the obstacle in
quantitative metrics when cross-dataset inference through an exhaustive
systematic study. We discern that the size bias of prediction leads to a
colossal failure. Hence, we propose the 2D-3D geometry-consistent object
scaling strategy (GCOS) to bridge the gap via an instance-level augment. Our
method called DGMono3D achieves remarkable performance on all evaluated
datasets and surpasses the SoTA unsupervised domain adaptation scheme even
without utilizing data on the target domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Map-based Features for Efficient Attention-based Vehicle Motion Prediction. (arXiv:2205.13071v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13071">
<div class="article-summary-box-inner">
<span><p>Motion prediction (MP) of multiple agents is a crucial task in arbitrarily
complex environments, from social robots to self-driving cars. Current
approaches tackle this problem using end-to-end networks, where the input data
is usually a rendered top-view of the scene and the past trajectories of all
the agents; leveraging this information is a must to obtain optimal
performance. In that sense, a reliable Autonomous Driving (AD) system must
produce reasonable predictions on time, however, despite many of these
approaches use simple ConvNets and LSTMs, models might not be efficient enough
for real-time applications when using both sources of information (map and
trajectory history). Moreover, the performance of these models highly depends
on the amount of training data, which can be expensive (particularly the
annotated HD maps). In this work, we explore how to achieve competitive
performance on the Argoverse 1.0 Benchmark using efficient attention-based
models, which take as input the past trajectories and map-based features from
minimal map information to ensure efficient and reliable MP. These features
represent interpretable information as the driveable area and plausible goal
points, in opposition to black-box CNN-based methods for map processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Sequential Contexts using Transformer for 3D Hand Pose Estimation. (arXiv:2206.00171v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00171">
<div class="article-summary-box-inner">
<span><p>3D hand pose estimation (HPE) is the process of locating the joints of the
hand in 3D from any visual input. HPE has recently received an increased amount
of attention due to its key role in a variety of human-computer interaction
applications. Recent HPE methods have demonstrated the advantages of employing
videos or multi-view images, allowing for more robust HPE systems. Accordingly,
in this study, we propose a new method to perform Sequential learning with
Transformer for Hand Pose (SeTHPose) estimation. Our SeTHPose pipeline begins
by extracting visual embeddings from individual hand images. We then use a
transformer encoder to learn the sequential context along time or viewing
angles and generate accurate 2D hand joint locations. Then, a graph
convolutional neural network with a U-Net configuration is used to convert the
2D hand joint locations to 3D poses. Our experiments show that SeTHPose
performs well on both hand sequence varieties, temporal and angular. Also,
SeTHPose outperforms other methods in the field to achieve new state-of-the-art
results on two public available sequential datasets, STB and MuViHand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography. (arXiv:2206.02225v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02225">
<div class="article-summary-box-inner">
<span><p>Displacement estimation is a critical step of virtually all Ultrasound
Elastography (USE) techniques. Two main features make this task unique compared
to the general optical flow problem: the high-frequency nature of ultrasound
radio-frequency (RF) data and the governing laws of physics on the displacement
field. Recently, the architecture of the optical flow networks has been
modified to be able to use RF data. Also, semi-supervised and unsupervised
techniques have been employed for USE by considering prior knowledge of
displacement continuity in the form of the first- and second-derivative
regularizers. Despite these attempts, no work has considered the tissue
compression pattern, and displacements in axial and lateral directions have
been assumed to be independent. However, tissue motion pattern is governed by
laws of physics in USE, rendering the axial and the lateral displacements
highly correlated. In this paper, we propose Physically Inspired ConsTraint for
Unsupervised Regularized Elastography (PICTURE), where we impose constraints on
the Poisson's ratio to improve lateral displacement estimates. Experiments on
phantom and in vivo data show that PICTURE substantially improves the quality
of the lateral displacement estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JigsawHSI: a network for Hyperspectral Image classification. (arXiv:2206.02327v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02327">
<div class="article-summary-box-inner">
<span><p>This article describes Jigsaw, a convolutional neural network (CNN) used in
geosciences and based on Inception but tailored for geoscientific analyses.
Introduces JigsawHSI (based on Jigsaw) and uses it on the land-use land-cover
(LULC) classification problem with the Indian Pines, Pavia University and
Salinas hyperspectral image data sets. The network is compared against
HybridSN, a spectral-spatial 3D-CNN followed by 2D-CNN that achieves
state-of-the-art results on the datasets. This short article proves that
JigsawHSI is able to meet or exceed HybridSN's performance in all three cases.
Additionally, the use of jigsaw in geosciences is highlighted, while the code
and toolkit are made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity. (arXiv:2206.03544v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03544">
<div class="article-summary-box-inner">
<span><p>Reconstructing natural videos from fMRI brain recordings is very challenging,
for two main reasons: (i) As fMRI data acquisition is difficult, we only have a
limited amount of supervised samples, which is not enough to cover the huge
space of natural videos; and (ii) The temporal resolution of fMRI recordings is
much lower than the frame rate of natural videos. In this paper, we propose a
self-supervised approach for natural-movie reconstruction. By employing
cycle-consistency over Encoding-Decoding natural videos, we can: (i) exploit
the full framerate of the training videos, and not be limited only to clips
that correspond to fMRI recordings; (ii) exploit massive amounts of external
natural videos which the subjects never saw inside the fMRI machine. These
enable increasing the applicable training data by several orders of magnitude,
introducing natural video priors to the decoding network, as well as temporal
coherence. Our approach significantly outperforms competing methods, since
those train only on the limited supervised data. We further introduce a new and
simple temporal prior of natural videos, which - when folded into our fMRI
decoder further - allows us to reconstruct videos at a higher frame-rate (HFR)
of up to x8 of the original fMRI sample rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior. (arXiv:2206.03858v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03858">
<div class="article-summary-box-inner">
<span><p>Inverse rendering is an ill-posed problem. Previous work has sought to
resolve this by focussing on priors for object or scene shape or appearance. In
this work, we instead focus on a prior for natural illuminations. Current
methods rely on spherical harmonic lighting or other generic representations
and, at best, a simplistic prior on the parameters. We propose a conditional
neural field representation based on a variational auto-decoder with a SIREN
network and, extending Vector Neurons, build equivariance directly into the
network. Using this we develop a rotation-equivariant, high dynamic range (HDR)
neural illumination model that is compact and able to express complex,
high-frequency features of natural environment maps. Training our model on a
curated dataset of 1.6K HDR environment maps of natural scenes, we compare it
against traditional representations, demonstrate its applicability for an
inverse rendering task and show environment map completion from partial
observations. A PyTorch implementation, our dataset and trained models can be
found at jadgardner.github.io/RENI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04046">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) aims at learning generalizable models under
distribution shifts to avoid redundantly overfitting massive training data.
Previous works with complex loss design and gradient constraint have not yet
led to empirical success on large-scale benchmarks. In this work, we reveal the
mixture-of-experts (MoE) model's generalizability on DG by leveraging to
distributively handle multiple aspects of the predictive features across
domains. To this end, we propose Sparse Fusion Mixture-of-Experts (SF-MoE),
which incorporates sparsity and fusion mechanisms into the MoE framework to
keep the model both sparse and predictive. SF-MoE has two dedicated modules: 1)
sparse block and 2) fusion block, which disentangle and aggregate the diverse
learned signals of an object, respectively. Extensive experiments demonstrate
that SF-MoE is a domain-generalizable learner on large-scale benchmarks. It
outperforms state-of-the-art counterparts by more than 2% across 5 large-scale
DG datasets (e.g., DomainNet), with the same or even lower computational costs.
We further reveal the internal mechanism of SF-MoE from distributed
representation perspective (e.g., visual attributes). We hope this framework
could facilitate future research to push generalizable object recognition to
the real world. Code and models are released at
https://github.com/Luodian/SF-MoE-DG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Cues Lead to a Strong Multi-Object Tracker. (arXiv:2206.04656v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04656">
<div class="article-summary-box-inner">
<span><p>For a long time, the most common paradigm in Multi-Object Tracking was
tracking-by-detection (TbD), where objects are first detected and then
associated over video frames. For association, most models resource to motion
and appearance cues. While still relying on these cues, recent approaches based
on, e.g., attention have shown an ever-increasing need for training data and
overall complex frameworks. We claim that 1) strong cues can be obtained from
little amounts of training data if some key design choices are applied, 2)
given these strong cues, standard Hungarian matching-based association is
enough to obtain impressive results. Our main insight is to identify key
components that allow a standard reidentification network to excel at
appearance-based tracking. We extensively analyze its failure cases and show
that a combination of our appearance features with a simple motion model leads
to strong tracking results. Our model achieves state-of-the-art performance on
MOT17 and MOT20 datasets outperforming previous state-of-the-art trackers by up
to 5.4pp in IDF1 and 4.4pp in HOTA. We will release the code and models after
the paper's acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation. (arXiv:2206.04682v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04682">
<div class="article-summary-box-inner">
<span><p>Accurately segmenting temporal frames of cine magnetic resonance imaging
(MRI) is a crucial step in various real-time MRI guided cardiac interventions.
To achieve fast and accurate visual assistance, there are strict requirements
on the maximum latency and minimum throughput of the segmentation framework.
State-of-the-art neural networks on this task are mostly hand-crafted to
satisfy these constraints while achieving high accuracy. On the other hand,
while existing literature have demonstrated the power of neural architecture
search (NAS) in automatically identifying the best neural architectures for
various medical applications, they are mostly guided by accuracy, sometimes
with computation complexity, and the importance of real-time constraints are
overlooked. A major challenge is that such constraints are non-differentiable
and are thus not compatible with the widely used differentiable NAS frameworks.
In this paper, we present a strategy that directly handles real-time
constraints in a differentiable NAS framework named RT-DNAS. Experiments on
extended 2017 MICCAI ACDC dataset show that compared with state-of-the-art
manually and automatically designed architectures, RT-DNAS is able to identify
ones with better accuracy while satisfying the real-time constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-MIA: COVID-19 Detection & Severity Analysis through Medical Imaging. (arXiv:2206.04732v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04732">
<div class="article-summary-box-inner">
<span><p>This paper presents the baseline approach for the organized 2nd Covid-19
Competition, occurring in the framework of the AIMIA Workshop in the European
Conference on Computer Vision (ECCV 2022). It presents the COV19-CT-DB database
which is annotated for COVID-19 detction, consisting of about 7,700 3-D CT
scans. Part of the database consisting of Covid-19 cases is further annotated
in terms of four Covid-19 severity conditions. We have split the database and
the latter part of it in training, validation and test datasets. The former two
datasets are used for training and validation of machine learning models, while
the latter will be used for evaluation of the developed models. The baseline
approach consists of a deep learning approach, based on a CNN-RNN network and
report its performance on the COVID19-CT-DB database.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-14 23:07:43.809820929 UTC">2022-06-14 23:07:43 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>