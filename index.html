<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-08T01:30:00Z">03-08</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting linguistic speech patterns of Japanese fictional characters using subword units. (arXiv:2203.02632v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02632">
<div class="article-summary-box-inner">
<span><p>This study extracted and analyzed the linguistic speech patterns that
characterize Japanese anime or game characters. Conventional morphological
analyzers, such as MeCab, segment words with high performance, but they are
unable to segment broken expressions or utterance endings that are not listed
in the dictionary, which often appears in lines of anime or game characters. To
overcome this challenge, we propose segmenting lines of Japanese anime or game
characters using subword units that were proposed mainly for deep learning, and
extracting frequently occurring strings to obtain expressions that characterize
their utterances. We analyzed the subword units weighted by TF/IDF according to
gender, age, and each anime character and show that they are linguistic speech
patterns that are specific for each feature. Additionally, a classification
experiment shows that the model with subword units outperformed that with the
conventional method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unfreeze with Care: Space-Efficient Fine-Tuning of Semantic Parsing Models. (arXiv:2203.02652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02652">
<div class="article-summary-box-inner">
<span><p>Semantic parsing is a key NLP task that maps natural language to structured
meaning representations. As in many other NLP tasks, SOTA performance in
semantic parsing is now attained by fine-tuning a large pretrained language
model (PLM). While effective, this approach is inefficient in the presence of
multiple downstream tasks, as a new set of values for all parameters of the PLM
needs to be stored for each task separately. Recent work has explored methods
for adapting PLMs to downstream tasks while keeping most (or all) of their
parameters frozen. We examine two such promising techniques, prefix tuning and
bias-term tuning, specifically on semantic parsing. We compare them against
each other on two different semantic parsing datasets, and we also compare them
against full and partial fine-tuning, both in few-shot and conventional data
settings. While prefix tuning is shown to do poorly for semantic parsing tasks
off the shelf, we modify it by adding special token embeddings, which results
in very strong performance without compromising parameter savings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross Language Image Matching for Weakly Supervised Semantic Segmentation. (arXiv:2203.02668v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02668">
<div class="article-summary-box-inner">
<span><p>It has been widely known that CAM (Class Activation Map) usually only
activates discriminative object regions and falsely includes lots of
object-related backgrounds. As only a fixed set of image-level object labels
are available to the WSSS (weakly supervised semantic segmentation) model, it
could be very difficult to suppress those diverse background regions consisting
of open set objects. In this paper, we propose a novel Cross Language Image
Matching (CLIMS) framework, based on the recently introduced Contrastive
Language-Image Pre-training (CLIP) model, for WSSS. The core idea of our
framework is to introduce natural language supervision to activate more
complete object regions and suppress closely-related open background regions.
In particular, we design object, background region and text label matching
losses to guide the model to excite more reasonable object regions for CAM of
each category. In addition, we design a co-occurring background suppression
loss to prevent the model from activating closely-related background regions,
with a predefined set of class-related background text descriptions. These
designs enable the proposed CLIMS to generate a more complete and compact
activation map for the target objects. Extensive experiments on PASCAL VOC2012
dataset show that our CLIMS significantly outperforms the previous
state-of-the-art methods. Code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralDPS: Neural Deterministic Plus Stochastic Model with Multiband Excitation for Noise-Controllable Waveform Generation. (arXiv:2203.02678v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02678">
<div class="article-summary-box-inner">
<span><p>The traditional vocoders have the advantages of high synthesis efficiency,
strong interpretability, and speech editability, while the neural vocoders have
the advantage of high synthesis quality. To combine the advantages of two
vocoders, inspired by the traditional deterministic plus stochastic model, this
paper proposes a novel neural vocoder named NeuralDPS which can retain high
speech quality and acquire high synthesis efficiency and noise controllability.
Firstly, this framework contains four modules: a deterministic source module, a
stochastic source module, a neural V/UV decision module and a neural filter
module. The input required by the vocoder is just the spectral parameter, which
avoids the error caused by estimating additional parameters, such as F0.
Secondly, to solve the problem that different frequency bands may have
different proportions of deterministic components and stochastic components, a
multiband excitation strategy is used to generate a more accurate excitation
signal and reduce the neural filter's burden. Thirdly, a method to control
noise components of speech is proposed. In this way, the signal-to-noise ratio
(SNR) of speech can be adjusted easily. Objective and subjective experimental
results show that our proposed NeuralDPS vocoder can obtain similar performance
with the WaveNet and it generates waveforms at least 280 times faster than the
WaveNet vocoder. It is also 28% faster than WaveGAN's synthesis efficiency on a
single CPU core. We have also verified through experiments that this method can
effectively control the noise components in the predicted speech and adjust the
SNR of speech. Examples of generated speech can be found at
https://hairuo55.github.io/NeuralDPS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just Rank: Rethinking Evaluation with Word and Sentence Similarities. (arXiv:2203.02679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02679">
<div class="article-summary-box-inner">
<span><p>Word and sentence embeddings are useful feature representations in natural
language processing. However, intrinsic evaluation for embeddings lags far
behind, and there has been no significant update since the past decade. Word
and sentence similarity tasks have become the de facto evaluation method. It
leads models to overfit to such evaluations, negatively impacting embedding
models' development. This paper first points out the problems using semantic
similarity as the gold standard for word and sentence embedding evaluations.
Further, we propose a new intrinsic evaluation method called EvalRank, which
shows a much stronger correlation with downstream tasks. Extensive experiments
are conducted based on 60+ models and popular datasets to certify our
judgments. Finally, the practical evaluation toolkit is released for future
benchmarking purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Proof is in the Pudding: Using Automated Theorem Proving to Generate Cooking Recipes. (arXiv:2203.02683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02683">
<div class="article-summary-box-inner">
<span><p>This paper presents FASTFOOD, a rule-based Natural Language Generation
Program for cooking recipes. Recipes are generated by using an Automated
Theorem Proving procedure to select the ingredients and instructions, with
ingredients corresponding to axioms and instructions to implications. FASTFOOD
also contains a temporal optimization module which can rearrange the recipe to
make it more time-efficient for the user, e.g. the recipe specifies to chop the
vegetables while the rice is boiling. The system is described in detail, using
a framework which divides Natural Language Generation into 4 phases: content
production, content selection, content organisation and content realisation. A
comparison is then made with similar existing systems and techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Representation Learning for Continual Relation Extraction. (arXiv:2203.02721v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02721">
<div class="article-summary-box-inner">
<span><p>Continual relation extraction (CRE) aims to continuously train a model on
data with new relations while avoiding forgetting old ones. Some previous work
has proved that storing a few typical samples of old relations and replaying
them when learning new relations can effectively avoid forgetting. However,
these memory-based methods tend to overfit the memory samples and perform
poorly on imbalanced datasets. To solve these challenges, a consistent
representation learning method is proposed, which maintains the stability of
the relation embedding by adopting contrastive learning and knowledge
distillation when replaying memory. Specifically, supervised contrastive
learning based on a memory bank is first used to train each new task so that
the model can effectively learn the relation representation. Then, contrastive
replay is conducted of the samples in memory and makes the model retain the
knowledge of historical relations through memory knowledge distillation to
prevent the catastrophic forgetting of the old task. The proposed method can
better learn consistent representations to alleviate forgetting effectively.
Extensive experiments on FewRel and TACRED datasets show that our method
significantly outperforms state-of-the-art baselines and yield strong
robustness on the imbalanced dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Differential Privacy on Group Disparity Mitigation. (arXiv:2203.02745v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02745">
<div class="article-summary-box-inner">
<span><p>The performance cost of differential privacy has, for some applications, been
shown to be higher for minority groups; fairness, conversely, has been shown to
disproportionally compromise the privacy of members of such groups. Most work
in this area has been restricted to computer vision and risk assessment. In
this paper, we evaluate the impact of differential privacy on fairness across
four tasks, focusing on how attempts to mitigate privacy violations and
between-group performance differences interact: Does privacy inhibit attempts
to ensure fairness? To this end, we train $(\varepsilon,\delta)$-differentially
private models with empirical risk minimization and group distributionally
robust training objectives. Consistent with previous findings, we find that
differential privacy increases between-group performance differences in the
baseline setting; but more interestingly, differential privacy reduces
between-group performance differences in the robust setting. We explain this by
reinterpreting differential privacy as regularization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feeding What You Need by Understanding What You Learned. (arXiv:2203.02753v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02753">
<div class="article-summary-box-inner">
<span><p>Machine Reading Comprehension (MRC) reveals the ability to understand a given
text passage and answer questions based on it. Existing research works in MRC
rely heavily on large-size models and corpus to improve the performance
evaluated by metrics such as Exact Match ($EM$) and $F_1$. However, such a
paradigm lacks sufficient interpretation to model capability and can not
efficiently train a model with a large corpus. In this paper, we argue that a
deep understanding of model capabilities and data properties can help us feed a
model with appropriate training data based on its learning status.
Specifically, we design an MRC capability assessment framework that assesses
model capabilities in an explainable and multi-dimensional manner. Based on it,
we further uncover and disentangle the connections between various data
properties and model performance. Finally, to verify the effectiveness of the
proposed MRC capability assessment framework, we incorporate it into a
curriculum learning pipeline and devise a Capability Boundary Breakthrough
Curriculum (CBBC) strategy, which performs a model capability-based training to
maximize the data value and improve training efficiency. Extensive experiments
demonstrate that our approach significantly improves performance, achieving up
to an 11.22% / 8.71% improvement of $EM$ / $F_1$ on MRC tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation. (arXiv:2203.02764v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02764">
<div class="article-summary-box-inner">
<span><p>Most existing works in vision-and-language navigation (VLN) focus on either
discrete or continuous environments, training agents that cannot generalize
across the two. The fundamental difference between the two setups is that
discrete navigation assumes prior knowledge of the connectivity graph of the
environment, so that the agent can effectively transfer the problem of
navigation with low-level controls to jumping from node to node with high-level
actions by grounding to an image of a navigable direction. To bridge the
discrete-to-continuous gap, we propose a predictor to generate a set of
candidate waypoints during navigation, so that agents designed with high-level
actions can be transferred to and trained in continuous environments. We refine
the connectivity graph of Matterport3D to fit the continuous
Habitat-Matterport3D, and train the waypoints predictor with the refined graphs
to produce accessible waypoints at each time step. Moreover, we demonstrate
that the predicted waypoints can be augmented during training to diversify the
views and paths, and therefore enhance agent's generalization ability. Through
extensive experiments we show that agents navigating in continuous environments
with predicted waypoints perform significantly better than agents using
low-level actions, which reduces the absolute discrete-to-continuous gap by
11.76% Success Weighted by Path Length (SPL) for the Cross-Modal Matching Agent
and 18.24% SPL for the Recurrent VLN-BERT. Our agents, trained with a simple
imitation learning objective, outperform previous methods by a large margin,
achieving new state-of-the-art results on the testing environments of the
R2R-CE and the RxR-CE datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CptGraphSum: Let key clues guide the cross-lingual abstractive summarization. (arXiv:2203.02797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02797">
<div class="article-summary-box-inner">
<span><p>Cross-Lingual Summarization (CLS) is the task to generate a summary in one
language for an article in a different language. Previous studies on CLS mainly
take pipeline methods or train the end-to-end model using the translated
parallel data. However, the quality of generated cross-lingual summaries needs
more further efforts to improve, and the model performance has never been
evaluated on the hand-written CLS dataset. Therefore, we first propose a
clue-guided cross-lingual abstractive summarization method to improve the
quality of cross-lingual summaries, and then construct a novel hand-written CLS
dataset for evaluation. Specifically, we extract keywords, named entities, etc.
of the input article as key clues for summarization and then design a
clue-guided algorithm to transform an article into a graph with less noisy
sentences. One Graph encoder is built to learn sentence semantics and article
structures and one Clue encoder is built to encode and translate key clues,
ensuring the information of important parts are reserved in the generated
summary. These two encoders are connected by one decoder to directly learn
cross-lingual semantics. Experimental results show that our method has stronger
robustness for longer inputs and substantially improves the performance over
the strong baseline, achieving an improvement of 8.55 ROUGE-1
(English-to-Chinese summarization) and 2.13 MoverScore (Chinese-to-English
summarization) scores over the existing SOTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus on the Target's Vocabulary: Masked Label Smoothing for Machine Translation. (arXiv:2203.02889v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02889">
<div class="article-summary-box-inner">
<span><p>Label smoothing and vocabulary sharing are two widely used techniques in
neural machine translation models. However, we argue that simply applying both
techniques can be conflicting and even leads to sub-optimal performance. When
allocating smoothed probability, original label smoothing treats the
source-side words that would never appear in the target language equally to the
real target-side words, which could bias the translation model. To address this
issue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the
soft label probability of source-side words to zero. Simple yet effective, MLS
manages to better integrate label smoothing with vocabulary sharing. Our
extensive experiments show that MLS consistently yields improvement over
original label smoothing on different datasets, including bilingual and
multilingual translation from both translation quality and model's calibration.
Our code is released at https://github.com/PKUnlp-icler/MLS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization. (arXiv:2203.02894v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02894">
<div class="article-summary-box-inner">
<span><p>Multi-document summarization (MDS) has made significant progress in recent
years, in part facilitated by the availability of new, dedicated datasets and
capacious language models. However, a standing limitation of these models is
that they are trained against limited references and with plain
maximum-likelihood objectives. As for many other generative tasks,
reinforcement learning (RL) offers the potential to improve the training of MDS
models; yet, it requires a carefully-designed reward that can ensure
appropriate leverage of both the reference summaries and the input documents.
For this reason, in this paper we propose fine-tuning an MDS baseline with a
reward that balances a reference-based metric such as ROUGE with coverage of
the input documents. To implement the approach, we utilize RELAX (Grathwohl et
al., 2018), a contemporary gradient estimator which is both low-variance and
unbiased, and we fine-tune the baseline in a few-shot style for both stability
and computational efficiency. Experimental results over the Multi-News and WCEP
MDS datasets show significant improvements of up to +0.95 pp average ROUGE
score and +3.17 pp METEOR score over the baseline, and competitive results with
the literature. In addition, they show that the coverage of the input documents
is increased, and evenly across all documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents. (arXiv:2203.02898v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02898">
<div class="article-summary-box-inner">
<span><p>Text semantic matching is a fundamental task that has been widely used in
various scenarios, such as community question answering, information retrieval,
and recommendation. Most state-of-the-art matching models, e.g., BERT, directly
perform text comparison by processing each word uniformly. However, a query
sentence generally comprises content that calls for different levels of
matching granularity. Specifically, keywords represent factual information such
as action, entity, and event that should be strictly matched, while intents
convey abstract concepts and ideas that can be paraphrased into various
expressions. In this work, we propose a simple yet effective training strategy
for text semantic matching in a divide-and-conquer manner by disentangling
keywords from intents. Our approach can be easily combined with pre-trained
language models (PLM) without influencing their inference efficiency, achieving
stable performance improvements against a wide range of PLMs on three
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Network Enhanced Language Models for Efficient Multilingual Text Classification. (arXiv:2203.02912v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02912">
<div class="article-summary-box-inner">
<span><p>Online social media works as a source of various valuable and actionable
information during disasters. These information might be available in multiple
languages due to the nature of user generated content. An effective system to
automatically identify and categorize these actionable information should be
capable to handle multiple languages and under limited supervision. However,
existing works mostly focus on English language only with the assumption that
sufficient labeled data is available. To overcome these challenges, we propose
a multilingual disaster related text classification system which is capable to
work under \{mono, cross and multi\} lingual scenarios and under limited
supervision. Our end-to-end trainable framework combines the versatility of
graph neural networks, by applying over the corpus, with the power of
transformer based large language models, over examples, with the help of
cross-attention between the two. We evaluate our framework over total nine
English, Non-English and monolingual datasets in \{mono, cross and multi\}
lingual classification scenarios. Our framework outperforms state-of-the-art
models in disaster domain and multilingual BERT baseline in terms of Weighted
F$_1$ score. We also show the generalizability of the proposed model under
limited supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doctor Recommendation in Online Health Forums via Expertise Learning. (arXiv:2203.02932v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02932">
<div class="article-summary-box-inner">
<span><p>Huge volumes of patient queries are daily generated on online health forums,
rendering manual doctor allocation a labor-intensive task. To better help
patients, this paper studies a novel task of doctor recommendation to enable
automatic pairing of a patient to a doctor with relevant expertise. While most
prior work in recommendation focuses on modeling target users from their past
behavior, we can only rely on the limited words in a query to infer a patient's
needs for privacy reasons. For doctor modeling, we study the joint effects of
their profiles and previous dialogues with other patients and explore their
interactions via self-learning. The learned doctor embeddings are further
employed to estimate their capabilities of handling a patient query with a
multi-head attention mechanism. For experiments, a large-scale dataset is
collected from Chunyu Yisheng, a Chinese online health forum, where our model
exhibits the state-of-the-art results, outperforming baselines only consider
profiles and past dialogues to characterize a doctor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation. (arXiv:2203.02951v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02951">
<div class="article-summary-box-inner">
<span><p>Token-level adaptive training approaches can alleviate the token imbalance
problem and thus improve neural machine translation, through re-weighting the
losses of different target tokens based on specific statistical metrics (e.g.,
token frequency or mutual information). Given that standard translation models
make predictions on the condition of previous target contexts, we argue that
the above statistical metrics ignore target context information and may assign
inappropriate weights to target tokens. While one possible solution is to
directly take target contexts into these statistical metrics, the
target-context-aware statistical computing is extremely expensive, and the
corresponding storage overhead is unrealistic. To solve the above issues, we
propose a target-context-aware metric, named conditional bilingual mutual
information (CBMI), which makes it feasible to supplement target context
information for statistical metrics. Particularly, our CBMI can be formalized
as the log quotient of the translation model probability and language model
probability by decomposing the conditional joint distribution. Thus CBMI can be
efficiently calculated during model training without any pre-specific
statistical calculations and large storage overhead. Furthermore, we propose an
effective adaptive training approach based on both the token- and
sentence-level CBMI. Experimental results on WMT14 English-German and WMT19
Chinese-English tasks show our approach can significantly outperform the
Transformer baseline and other related methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twitter Dataset for 2022 Russo-Ukrainian Crisis. (arXiv:2203.02955v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02955">
<div class="article-summary-box-inner">
<span><p>Online Social Networks (OSNs) play a significant role in information sharing
during a crisis. The data collected during such a crisis can reflect the large
scale public opinions and sentiment. In addition, OSN data can also be used to
study different campaigns that are employed by various entities to engineer
public opinions. Such information sharing campaigns can range from spreading
factual information to propaganda and misinformation. We provide a Twitter
dataset of the 2022 Russo-Ukrainian conflict. In the first release, we share
over 1.6 million tweets shared during the 1st week of the crisis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Optical-Flow-Guided Motion and Detection-Based Appearance for Temporal Sentence Grounding. (arXiv:2203.02966v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02966">
<div class="article-summary-box-inner">
<span><p>Temporal sentence grounding aims to localize a target segment in an untrimmed
video semantically according to a given sentence query. Most previous works
focus on learning frame-level features of each whole frame in the entire video,
and directly match them with the textual information. Such frame-level feature
extraction leads to the obstacles of these methods in distinguishing ambiguous
video frames with complicated contents and subtle appearance differences, thus
limiting their performance. In order to differentiate fine-grained appearance
similarities among consecutive frames, some state-of-the-art methods
additionally employ a detection model like Faster R-CNN to obtain detailed
object-level features in each frame for filtering out the redundant background
contents. However, these methods suffer from missing motion analysis since the
object detection module in Faster R-CNN lacks temporal modeling. To alleviate
the above limitations, in this paper, we propose a novel Motion- and
Appearance-guided 3D Semantic Reasoning Network (MA3SRN), which incorporates
optical-flow-guided motion-aware, detection-based appearance-aware, and
3D-aware object-level features to better reason the spatial-temporal object
relations for accurately modelling the activity among consecutive frames.
Specifically, we first develop three individual branches for motion,
appearance, and 3D encoding separately to learn fine-grained motion-guided,
appearance-guided, and 3D-aware object features, respectively. Then, both
motion and appearance information from corresponding branches are associated to
enhance the 3D-aware features for the final precise grounding. Extensive
experiments on three challenging datasets (ActivityNet Caption, Charades-STA
and TACoS) demonstrate that the proposed MA3SRN model achieves a new
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Implicit Discourse Relation Recognition. (arXiv:2203.02982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02982">
<div class="article-summary-box-inner">
<span><p>A discourse containing one or more sentences describes daily issues and
events for people to communicate their thoughts and opinions. As sentences are
normally consist of multiple text segments, correct understanding of the theme
of a discourse should take into consideration of the relations in between text
segments. Although sometimes a connective exists in raw texts for conveying
relations, it is more often the cases that no connective exists in between two
text segments but some implicit relation does exist in between them. The task
of implicit discourse relation recognition (IDRR) is to detect implicit
relation and classify its sense between two text segments without a connective.
Indeed, the IDRR task is important to diverse downstream natural language
processing tasks, such as text summarization, machine translation and so on.
This article provides a comprehensive and up-to-date survey for the IDRR task.
We first summarize the task definition and data sources widely used in the
field. We categorize the main solution approaches for the IDRR task from the
viewpoint of its development history. In each solution category, we present and
analyze the most representative methods, including their origins, ideas,
strengths and weaknesses. We also present performance comparisons for those
solutions experimented on a public corpus with standard data processing
procedures. Finally, we discuss future research directions for discourse
relation analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Key-value Memory Enhanced Multi-step Graph Reasoning for Knowledge-based Visual Question Answering. (arXiv:2203.02985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02985">
<div class="article-summary-box-inner">
<span><p>Knowledge-based visual question answering (VQA) is a vision-language task
that requires an agent to correctly answer image-related questions using
knowledge that is not presented in the given image. It is not only a more
challenging task than regular VQA but also a vital step towards building a
general VQA system. Most existing knowledge-based VQA systems process knowledge
and image information similarly and ignore the fact that the knowledge base
(KB) contains complete information about a triplet, while the extracted image
information might be incomplete as the relations between two objects are
missing or wrongly detected. In this paper, we propose a novel model named
dynamic knowledge memory enhanced multi-step graph reasoning (DMMGR), which
performs explicit and implicit reasoning over a key-value knowledge memory
module and a spatial-aware image graph, respectively. Specifically, the memory
module learns a dynamic knowledge representation and generates a
knowledge-aware question representation at each reasoning step. Then, this
representation is used to guide a graph attention operator over the
spatial-aware image graph. Our model achieves new state-of-the-art accuracy on
the KRVQR and FVQA datasets. We also conduct ablation experiments to prove the
effectiveness of each component of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Coreference Relations in Visual Dialog. (arXiv:2203.02986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02986">
<div class="article-summary-box-inner">
<span><p>Visual dialog is a vision-language task where an agent needs to answer a
series of questions grounded in an image based on the understanding of the
dialog history and the image. The occurrences of coreference relations in the
dialog makes it a more challenging task than visual question-answering. Most
previous works have focused on learning better multi-modal representations or
on exploring different ways of fusing visual and language features, while the
coreferences in the dialog are mainly ignored. In this paper, based on
linguistic knowledge and discourse features of human dialog we propose two soft
constraints that can improve the model's ability of resolving coreferences in
dialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset
shows that our model, which integrates two novel and linguistically inspired
soft constraints in a deep transformer neural architecture, obtains new
state-of-the-art performance in terms of recall at 1 and other evaluation
metrics compared to current existing models and this without pretraining on
other vision-language datasets. Our qualitative results also demonstrate the
effectiveness of the method that we propose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Advances in Neural Text Generation: A Task-Agnostic Survey. (arXiv:2203.03047v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03047">
<div class="article-summary-box-inner">
<span><p>In recent years much effort has been devoted to applying neural models to the
task of natural language generation. The challenge is to generate natural
human-like text, and to control the generation process. This paper presents a
task-agnostic survey of recent advances in neural text generation. These
advances have been achieved by numerous developments, which we group under the
following four headings: data construction, neural frameworks, training and
inference strategies, and evaluation metrics. Finally we discuss the future
directions for the development of neural text generation including neural
pipelines and exploiting back-ground knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leashing the Inner Demons: Self-Detoxification for Language Models. (arXiv:2203.03072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03072">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) can reproduce (or amplify) toxic language seen during
training, which poses a risk to their practical application. In this paper, we
conduct extensive experiments to study this phenomenon. We analyze the impact
of prompts, decoding strategies and training corpora on the output toxicity.
Based on our findings, we propose a simple yet effective method for language
models to "detoxify" themselves without an additional large corpus or external
discriminator. Compared to a supervised baseline, our proposed method shows
better toxicity reduction with good generation quality in the generated content
under multiple settings. Warning: some examples shown in the paper may contain
uncensored offensive content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ILDAE: Instance-Level Difficulty Analysis of Evaluation Data. (arXiv:2203.03073v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03073">
<div class="article-summary-box-inner">
<span><p>Knowledge of questions' difficulty level helps a teacher in several ways,
such as estimating students' potential quickly by asking carefully selected
questions and improving quality of examination by modifying trivial and hard
questions. Can we extract such benefits of instance difficulty in NLP? To this
end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE)
in a large-scale setup of 23 datasets and demonstrate its five novel
applications: 1) conducting efficient-yet-accurate evaluations with fewer
instances saving computational cost and time, 2) improving quality of existing
evaluation datasets by repairing erroneous and trivial instances, 3) selecting
the best model based on application requirements, 4) analyzing dataset
characteristics for guiding future data creation, 5) estimating Out-of-Domain
performance reliably. Comprehensive experiments for these applications result
in several interesting findings, such as evaluation using just 5% instances
(selected via ILDAE) achieves as high as 0.93 Kendall correlation with
evaluation using complete dataset and computing weighted accuracy using
difficulty scores leads to 5.2% higher correlation with Out-of-Domain
performance. We release the difficulty scores and hope our analyses and
findings will bring more attention to this important yet understudied field of
leveraging instance difficulty in evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking. (arXiv:2203.03123v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03123">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking (DST) aims to extract essential information from
multi-turn dialogue situations and take appropriate actions. A belief state,
one of the core pieces of information, refers to the subject and its specific
content, and appears in the form of \texttt{domain-slot-value}. The trained
model predicts "accumulated" belief states in every turn, and joint goal
accuracy and slot accuracy are mainly used to evaluate the prediction; however,
we specify that the current evaluation metrics have a critical limitation when
evaluating belief states accumulated as the dialogue proceeds, especially in
the most used MultiWOZ dataset. Additionally, we propose \textbf{relative slot
accuracy} to complement existing metrics. Relative slot accuracy does not
depend on the number of predefined slots, and allows intuitive evaluation by
assigning relative scores according to the turn of each dialogue. This study
also encourages not solely the reporting of joint goal accuracy, but also
various complementary metrics in DST tasks for the sake of a realistic
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models. (arXiv:2203.03131v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03131">
<div class="article-summary-box-inner">
<span><p>Recently the prompt-tuning paradigm has attracted significant attention. By
only tuning continuous prompts with a frozen pre-trained language model (PLM),
prompt-tuning takes a step towards deploying a shared frozen PLM to serve
numerous downstream tasks. Although prompt-tuning shows good performance on
certain natural language understanding (NLU) tasks, its effectiveness on
natural language generation (NLG) tasks is still under-explored. In this paper,
we argue that one of the factors hindering the development of prompt-tuning on
NLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different
from the pretraining corpus). For example, our preliminary exploration reveals
a large performance gap between prompt-tuning and fine-tuning when unfamiliar
inputs occur frequently in NLG tasks. This motivates us to propose
input-tuning, which fine-tunes both the continuous prompts and the input
representations, leading to a more effective way to adapt unfamiliar inputs to
frozen PLMs. Our proposed input-tuning is conceptually simple and empirically
powerful. Experimental results on seven NLG tasks demonstrate that input-tuning
is significantly and consistently better than prompt-tuning. Furthermore, on
three of these tasks, input-tuning can achieve a comparable or even better
performance than fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Online Dialogue Response Generation. (arXiv:2203.03168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03168">
<div class="article-summary-box-inner">
<span><p>Although pre-trained sequence-to-sequence models have achieved great success
in dialogue response generation, chatbots still suffer from generating
inconsistent responses in real-world practice, especially in multi-turn
settings. We argue that this can be caused by a discrepancy between training
and real-world testing. At training time, chatbot generates the response with
the golden context, while it has to generate based on the context consisting of
both user utterances and the model predicted utterances during real-world
testing. With the growth of the number of utterances, this discrepancy becomes
more serious in the multi-turn settings. In this paper, we propose a
hierarchical sampling-based method consisting of both utterance-level sampling
and semi-utterance-level sampling, to alleviate the discrepancy, which
implicitly increases the dialogue coherence. We further adopt reinforcement
learning and re-ranking methods to explicitly optimize the dialogue coherence
during training and inference, respectively. Empirical experiments show the
effectiveness of the proposed methods for improving the robustness of chatbots
in real practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A theory of interaction semantics. (arXiv:2007.06258v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.06258">
<div class="article-summary-box-inner">
<span><p>The aim of this article is to delineate a theory of interaction semantics and
thereby provide a proper understanding of the "meaning" of the exchanged
characters within an interaction. The key idea is to approach the semantics of
an interaction as we do for a formal language. This approach consists of two
steps: first to assign values to variables and second to provide meaning by an
interpretation function. A natural choice for the variables are the state
functions of the interacting systems, assigning values at each time step.
Thereby the description of a system's behaviour with an input/output-transition
system (I/O-TS) becomes a representation of the variable-to-value
assignments.To identify the interpretation function I propose to model the
interaction of systems based on Shannon's theory of information with the
protocol concept, complemented by decisions to form a "game in interactive form
(GIF)". Decisions in this sense determine the transition relation and thereby
create a transition function. Then the natural choice for the interpretation
function is the transition function of the GIF. In this sense, the
interpretation of the interaction becomes its execution. Now we can say that
the interpretation of the characters during the GIF's execution results in
their meaning, the result of the mapping. Equivalent meaning is based on
resulting equivalent states of the GIF. Based on the decisions we can partition
any GIF into a deterministic traditional automaton where the states represent
equivalance classes of GIF-states related to a single decision. Except for the
utility function, this automaton is equivalent to a traditional game in
extensive form. Thus, traditional game theory actually abstracts from
interactions and deals with the meaning of decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparsifying Transformer Models with Trainable Representation Pooling. (arXiv:2009.05169v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.05169">
<div class="article-summary-box-inner">
<span><p>We propose a novel method to sparsify attention in the Transformer model by
learning to select the most-informative token representations during the
training process, thus focusing on the task-specific parts of an input. A
reduction of quadratic time and memory complexity to sublinear was achieved due
to a robust trainable top-$k$ operator. Our experiments on a challenging long
document summarization task show that even our simple baseline performs
comparably to the current SOTA, and with trainable pooling, we can retain its
top quality, while being $1.8\times$ faster during training, $4.5\times$ faster
during inference, and up to $13\times$ more computationally efficient in the
decoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Cross-lingual Semantic Parsing. (arXiv:2104.07554v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07554">
<div class="article-summary-box-inner">
<span><p>Recent work in cross-lingual semantic parsing has successfully applied
machine translation to localize parsers to new languages. However, these
advances assume access to high-quality machine translation systems and word
alignment tools. We remove these assumptions and study cross-lingual semantic
parsing as a zero-shot problem, without parallel data (i.e., utterance-logical
form pairs) for new languages. We propose a multi-task encoder-decoder model to
transfer parsing knowledge to additional languages using only English-logical
form paired data and in-domain natural language corpora in each new language.
Our model encourages language-agnostic encodings by jointly optimizing for
logical-form generation with auxiliary objectives designed for cross-lingual
latent representation alignment. Our parser performs significantly above
translation-based baselines and, in some cases, competes with the supervised
upper-bound.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dependency Parsing as MRC-based Span-Span Prediction. (arXiv:2105.07654v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07654">
<div class="article-summary-box-inner">
<span><p>Higher-order methods for dependency parsing can partially but not fully
address the issue that edges in dependency trees should be constructed at the
text span/subtree level rather than word level. In this paper, we propose a new
method for dependency parsing to address this issue. The proposed method
constructs dependency trees by directly modeling span-span (in other words,
subtree-subtree) relations. It consists of two modules: the {\it text span
proposal module} which proposes candidate text spans, each of which represents
a subtree in the dependency tree denoted by (root, start, end); and the {\it
span linking module}, which constructs links between proposed spans. We use the
machine reading comprehension (MRC) framework as the backbone to formalize the
span linking module, where one span is used as a query to extract the text
span/subtree it should be linked to. The proposed method has the following
merits: (1) it addresses the fundamental problem that edges in a dependency
tree should be constructed between subtrees; (2) the MRC framework allows the
method to retrieve missing spans in the span proposal stage, which leads to
higher recall for eligible spans. Extensive experiments on the PTB, CTB and
Universal Dependencies (UD) benchmarks demonstrate the effectiveness of the
proposed method. The code is available at
\url{https://github.com/ShannonAI/mrc-for-dependency-parsing}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ByT5: Towards a token-free future with pre-trained byte-to-byte models. (arXiv:2105.13626v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13626">
<div class="article-summary-box-inner">
<span><p>Most widely-used pre-trained language models operate on sequences of tokens
corresponding to word or subword units. By comparison, token-free models that
operate directly on raw text (bytes or characters) have many benefits: they can
process text in any language out of the box, they are more robust to noise, and
they minimize technical debt by removing complex and error-prone text
preprocessing pipelines. Since byte or character sequences are longer than
token sequences, past work on token-free models has often introduced new model
architectures designed to amortize the cost of operating directly on raw text.
In this paper, we show that a standard Transformer architecture can be used
with minimal modifications to process byte sequences. We characterize the
trade-offs in terms of parameter count, training FLOPs, and inference speed,
and show that byte-level models are competitive with their token-level
counterparts. We also demonstrate that byte-level models are significantly more
robust to noise and perform better on tasks that are sensitive to spelling and
pronunciation. As part of our contribution, we release a new set of pre-trained
byte-level Transformer models based on the T5 architecture, as well as all code
and data used in our experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Nearest Neighbor Machine Translation. (arXiv:2105.14528v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14528">
<div class="article-summary-box-inner">
<span><p>Though nearest neighbor Machine Translation ($k$NN-MT)
\citep{khandelwal2020nearest} has proved to introduce significant performance
boosts over standard neural MT systems, it is prohibitively slow since it uses
the entire reference corpus as the datastore for the nearest neighbor search.
This means each step for each beam in the beam search has to search over the
entire reference corpus. $k$NN-MT is thus two-orders slower than vanilla MT
models, making it hard to be applied to real-world applications, especially
online services. In this work, we propose Fast $k$NN-MT to address this issue.
Fast $k$NN-MT constructs a significantly smaller datastore for the nearest
neighbor search: for each word in a source sentence, Fast $k$NN-MT first
selects its nearest token-level neighbors, which is limited to tokens that are
the same as the query token. Then at each decoding step, in contrast to using
the entire corpus as the datastore, the search space is limited to target
tokens corresponding to the previously selected reference source tokens. This
strategy avoids search through the whole datastore for nearest neighbors and
drastically improves decoding efficiency. Without loss of performance, Fast
$k$NN-MT is two-orders faster than $k$NN-MT, and is only two times slower than
the standard NMT model. Fast $k$NN-MT enables the practical use of $k$NN-MT
systems in real-world MT applications. The code is available at
\url{https://github.com/ShannonAI/fast-knn-nmt}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08087">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence (AI), along with the recent progress in biomedical
language understanding, is gradually changing medical practice. With the
development of biomedical language understanding benchmarks, AI applications
are widely used in the medical field. However, most benchmarks are limited to
English, which makes it challenging to replicate many of the successes in
English for other languages. To facilitate research in this direction, we
collect real-world biomedical data and present the first Chinese Biomedical
Language Understanding Evaluation (CBLUE) benchmark: a collection of natural
language understanding tasks including named entity recognition, information
extraction, clinical diagnosis normalization, single-sentence/sentence-pair
classification, and an associated online platform for model evaluation,
comparison, and analysis. To establish evaluation on these tasks, we report
empirical results with the current 11 pre-trained Chinese models, and
experimental results show that state-of-the-art neural models perform by far
worse than the human ceiling. Our benchmark is released at
\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&amp;lang=en-us}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion. (arXiv:2106.08657v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08657">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (DocRE) aims to extract semantic relations
among entity pairs in a document. Typical DocRE methods blindly take the full
document as input, while a subset of the sentences in the document, noted as
the evidence, are often sufficient for humans to predict the relation of an
entity pair. In this paper, we propose an evidence-enhanced framework, Eider,
that empowers DocRE by efficiently extracting evidence and effectively fusing
the extracted evidence in inference. We first jointly train an RE model with a
lightweight evidence extraction model, which is efficient in both memory and
runtime. Empirically, even training the evidence model on silver labels
constructed by our heuristic rules can lead to better RE performance. We
further design a simple yet effective inference process that makes RE
predictions on both extracted evidence and the full document, then fuses the
predictions through a blending layer. This allows Eider to focus on important
sentences while still having access to the complete information in the
document. Extensive experiments show that Eider outperforms state-of-the-art
methods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning. (arXiv:2108.00578v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00578">
<div class="article-summary-box-inner">
<span><p>Neural models command state-of-the-art performance across NLP tasks,
including ones involving "reasoning". Models claiming to reason about the
evidence presented to them should attend to the correct parts of the input
avoiding spurious patterns therein, be self-consistent in their predictions
across inputs, and be immune to biases derived from their pre-training in a
nuanced, context-sensitive fashion. {\em Do the prevalent *BERT-family of
models do so?} In this paper, we study this question using the problem of
reasoning on tabular data. Tabular inputs are especially well-suited for the
study -- they admit systematic probes targeting the properties listed above.
Our experiments demonstrate that a RoBERTa-based model, representative of the
current state-of-the-art, fails at reasoning on the following counts: it (a)
ignores relevant parts of the evidence, (b) is over-sensitive to annotation
artifacts, and (c) relies on the knowledge encoded in the pre-trained language
model rather than the evidence presented in its tabular inputs. Finally,
through inoculation experiments, we show that fine-tuning the model on
perturbed data does not help it overcome the above challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuning Pretrained Language Models With Label Attention for Biomedical Text Classification. (arXiv:2108.11809v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11809">
<div class="article-summary-box-inner">
<span><p>The massive scale and growth of textual biomedical data have made its
indexing and classification increasingly important. However, existing research
on this topic mainly utilized convolutional and recurrent neural networks,
which generally achieve inferior performance than the novel transformers. On
the other hand, systems that apply transformers only focus on the target
documents, overlooking the rich semantic information that label descriptions
contain. To address this gap, we develop a transformer-based biomedical text
classifier that considers label information. The system achieves this with a
label attention module incorporated into the fine-tuning process of pretrained
language models (PTMs). Our results on two public medical datasets show that
the proposed fine-tuning scheme outperforms the vanilla PTMs and
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Grammar-Learning Trajectories of Neural Language Models. (arXiv:2109.06096v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06096">
<div class="article-summary-box-inner">
<span><p>The learning trajectories of linguistic phenomena in humans provide insight
into linguistic representation, beyond what can be gleaned from inspecting the
behavior of an adult speaker. To apply a similar approach to analyze neural
language models (NLM), it is first necessary to establish that different models
are similar enough in the generalizations they make. In this paper, we show
that NLMs with different initialization, architecture, and training data
acquire linguistic phenomena in a similar order, despite their different end
performance. These findings suggest that there is some mutual inductive bias
that underlies these models' learning of linguistic phenomena. Taking
inspiration from psycholinguistics, we argue that studying this inductive bias
is an opportunity to study the linguistic representation implicit in NLMs.
</p>
<p>Leveraging these findings, we compare the relative performance on different
phenomena at varying learning stages with simpler reference models. Results
suggest that NLMs exhibit consistent "developmental" stages. Moreover, we find
the learning trajectory to be approximately one-dimensional: given an NLM with
a certain overall performance, it is possible to predict what linguistic
generalizations it has already acquired. Initial analysis of these stages
presents phenomena clusters (notably morphological ones), whose performance
progresses in unison, suggesting a potential link between the generalizations
behind them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSTL -- From English Requirements to Signal Temporal Logic. (arXiv:2109.10294v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10294">
<div class="article-summary-box-inner">
<span><p>Formal methods provide very powerful tools and techniques for the design and
analysis of complex systems. Their practical application remains however
limited, due to the widely accepted belief that formal methods require
extensive expertise and a steep learning curve. Writing correct formal
specifications in form of logical formulas is still considered to be a
difficult and error prone task.
</p>
<p>In this paper we propose DeepSTL, a tool and technique for the translation of
informal requirements, given as free English sentences, into Signal Temporal
Logic (STL), a formal specification language for cyber-physical systems, used
both by academia and advanced research labs in industry. A major challenge to
devise such a translator is the lack of publicly available informal
requirements and formal specifications. We propose a two-step workflow to
address this challenge. We first design a grammar-based generation technique of
synthetic data, where each output is a random STL formula and its associated
set of possible English translations. In the second step, we use a
state-of-the-art transformer-based neural translation technique, to train an
accurate attentional translator of English to STL. The experimental results
show high translation quality for patterns of English requirements that have
been well trained, making this workflow promising to be extended for processing
more complex translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval. (arXiv:2110.07524v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07524">
<div class="article-summary-box-inner">
<span><p>Training dense passage representations via contrastive learning has been
shown effective for Open-Domain Passage Retrieval (ODPR). Existing studies
focus on further optimizing by improving negative sampling strategy or extra
pretraining. However, these studies keep unknown in capturing passage with
internal representation conflicts from improper modeling granularity. This work
thus presents a refined model on the basis of a smaller granularity, contextual
sentences, to alleviate the concerned conflicts. In detail, we introduce an
in-passage negative sampling strategy to encourage a diverse generation of
sentence representations within the same passage. Experiments on three
benchmark datasets verify the efficacy of our method, especially on datasets
where conflicts are severe. Extensive experiments further present good
transferability of our method across datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Fine-Grained Reasoning for Fake News Detection. (arXiv:2110.15064v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15064">
<div class="article-summary-box-inner">
<span><p>The detection of fake news often requires sophisticated reasoning skills,
such as logically combining information by considering word-level subtle clues.
In this paper, we move towards fine-grained reasoning for fake news detection
by better reflecting the logical processes of human thinking and enabling the
modeling of subtle clues. In particular, we propose a fine-grained reasoning
framework by following the human information-processing model, introduce a
mutual-reinforcement-based method for incorporating human knowledge about which
evidence is more important, and design a prior-aware bi-channel kernel graph
network to model subtle differences between pieces of evidence. Extensive
experiments show that our model outperforms the state-of-the-art methods and
demonstrate the explainability of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Labeling for Massively Multilingual Speech Recognition. (arXiv:2111.00161v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00161">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning through pseudo-labeling has become a staple of
state-of-the-art monolingual speech recognition systems. In this work, we
extend pseudo-labeling to massively multilingual speech recognition with 60
languages. We propose a simple pseudo-labeling recipe that works well even with
low-resource languages: train a supervised multilingual model, fine-tune it
with semi-supervised learning on a target language, generate pseudo-labels for
that language, and train a final model using pseudo-labels for all languages,
either from scratch or by fine-tuning. Experiments on the labeled Common Voice
and unlabeled VoxPopuli datasets show that our recipe can yield a model with
better performance for many languages that also transfers well to LibriSpeech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalabilty. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for text-to-image and image-to-text
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation tasks without any finetuning or extra
object detection frameworks. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial results of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViNMT: Neural Machine Translation Toolkit. (arXiv:2112.15272v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15272">
<div class="article-summary-box-inner">
<span><p>We present an open-source toolkit for neural machine translation (NMT). The
new toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along
with many other improvements detailed below, in order to create a
self-contained, simple to use, consistent and comprehensive framework for
Machine Translation tasks of various domains. It is tooled to support both
bilingual and multilingual translation tasks, starting from building the model
from respective corpora, to inferring new predictions or packaging the model to
serving-capable JIT format.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training. (arXiv:2201.10207v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10207">
<div class="article-summary-box-inner">
<span><p>We introduce a new approach for speech pre-training named SPIRAL which works
by learning denoising representation of perturbed data in a teacher-student
framework. Specifically, given a speech utterance, we first feed the utterance
to a teacher network to obtain corresponding representation. Then the same
utterance is perturbed and fed to a student network. The student network is
trained to output representation resembling that of the teacher. At the same
time, the teacher network is updated as moving average of student's weights
over training steps. In order to prevent representation collapse, we apply an
in-utterance contrastive loss as pre-training objective and impose position
randomization on the input to the teacher. SPIRAL achieves competitive or
better results compared to state-of-the-art speech pre-training method wav2vec
2.0, with significant reduction of training cost (80% for BASE model, 65% for
LARGE model). Furthermore, we address the problem of noise-robustness that is
critical to real-world speech applications. We propose multi-condition
pre-training by perturbing the student's input with various types of additive
noise. We demonstrate that multi-condition pre-trained SPIRAL models are more
robust to noisy speech (9.0% - 13.3% relative word error rate reduction on real
noisy test data), compared to applying multi-condition training solely in the
fine-tuning stage. Source code is available at
https://github.com/huawei-noah/Speech-Backbones/tree/main/SPIRAL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02113">
<div class="article-summary-box-inner">
<span><p>Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKGC/tree/main/GenKGC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable N-gram Objective on Abstractive Summarization. (arXiv:2202.04003v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04003">
<div class="article-summary-box-inner">
<span><p>ROUGE is a standard automatic evaluation metric based on n-grams for
sequence-to-sequence tasks, while cross-entropy loss is an essential objective
of neural network language model that optimizes at a unigram level. We present
differentiable n-gram objectives, attempting to alleviate the discrepancy
between training criterion and evaluating criterion. The objective maximizes
the probabilistic weight of matched sub-sequences, and the novelty of our work
is the objective weights the matched sub-sequences equally and does not ceil
the number of matched sub-sequences by the ground truth count of n-grams in
reference sequence. We jointly optimize cross-entropy loss and the proposed
objective, providing decent ROUGE score enhancement over abstractive
summarization dataset CNN/DM and XSum, outperforming alternative n-gram
objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenderedNews: Une approche computationnelle des \'ecarts de repr\'esentation des genres dans la presse fran\c{c}aise. (arXiv:2202.05682v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05682">
<div class="article-summary-box-inner">
<span><p>In this article, we present {\it GenderedNews}
(\url{https://gendered-news.imag.fr}), an online dashboard which gives weekly
measures of gender imbalance in French online press. We use Natural Language
Processing (NLP) methods to quantify gender inequalities in the media, in the
wake of global projects like the Global Media Monitoring Project. Such projects
are instrumental in highlighting gender imbalance in the media and its very
slow evolution. However, their generalisation is limited by their sampling and
cost in terms of time, data and staff. Automation allows us to offer
complementary measures to quantify inequalities in gender representation. We
understand representation as the presence and distribution of men and women
mentioned and quoted in the news -- as opposed to representation as
stereotypification. In this paper, we first review different means adopted by
previous studies on gender inequality in the media : qualitative content
analysis, quantitative content analysis and computational methods. We then
detail the methods adopted by {\it GenderedNews} and the two metrics
implemented: the masculinity rate of mentions and the proportion of men quoted
in online news. We describe the data collected daily (seven main titles of
French online news media) and the methodology behind our metrics, as well as a
few visualisations. We finally propose to illustrate possible analysis of our
data by conducting an in-depth observation of a sample of two months of our
database.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation. (arXiv:2202.12350v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12350">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) algorithms have become very successful, but
they still struggle when applied to out-of-distribution examples. In this paper
we propose a controllable generation approach in order to deal with this domain
adaptation (DA) challenge. Given an input text example, our DoCoGen algorithm
generates a domain-counterfactual textual example (D-con) - that is similar to
the original in all aspects, including the task label, but its domain is
changed to a desired one. Importantly, DoCoGen is trained using only unlabeled
examples from multiple domains - no NLP task labels or parallel pairs of
textual examples and their domain-counterfactuals are required. We show that
DoCoGen can generate coherent counterfactuals consisting of multiple sentences.
We use the D-cons generated by DoCoGen to augment a sentiment classifier and a
multi-label intent classifier in 20 and 78 DA setups, respectively, where
source-domain labeled data is scarce. Our model outperforms strong baselines
and improves the accuracy of a state-of-the-art unsupervised DA algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning, Natural Language Processing, and Explainable Artificial Intelligence in the Biomedical Domain. (arXiv:2202.12678v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12678">
<div class="article-summary-box-inner">
<span><p>In this article, we first give an introduction to artificial intelligence and
its applications in biology and medicine in Section 1. Deep learning methods
are then described in Section 2. We narrow down the focus of the study on
textual data in Section 3, where natural language processing and its
applications in the biomedical domain are described. In Section 4, we give an
introduction to explainable artificial intelligence and discuss the importance
of explainability of artificial intelligence systems, especially in the
biomedical domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Performance of Automated Essay Scoring by using back-translation essays and adjusted scores. (arXiv:2203.00354v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00354">
<div class="article-summary-box-inner">
<span><p>Automated essay scoring plays an important role in judging students' language
abilities in education. Traditional approaches use handcrafted features to
score and are time-consuming and complicated. Recently, neural network
approaches have improved performance without any feature engineering. Unlike
other natural language processing tasks, only a small number of datasets are
publicly available for automated essay scoring, and the size of the dataset is
not sufficiently large. Considering that the performance of a neural network is
closely related to the size of the dataset, the lack of data limits the
performance improvement of the automated essay scoring model. In this paper, we
proposed a method to increase the number of essay-score pairs using
back-translation and score adjustment and applied it to the Automated Student
Assessment Prize dataset for augmentation. We evaluated the effectiveness of
the augmented data using models from prior work. In addition, performance was
evaluated in a model using long short-term memory, which is widely used for
automated essay scoring. The performance of the models was improved by using
augmented data to train the models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crossed-Time Delay Neural Network for Speaker Recognition. (arXiv:2006.00452v3 [eess.AS] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.00452">
<div class="article-summary-box-inner">
<span><p>Time Delay Neural Network (TDNN) is a well-performing structure for DNN-based
speaker recognition systems. In this paper we introduce a novel structure
Crossed-Time Delay Neural Network (CTDNN) to enhance the performance of current
TDNN. Inspired by the multi-filters setting of convolution layer from
convolution neural network, we set multiple time delay units each with
different context size at the bottom layer and construct a multilayer parallel
network. The proposed CTDNN gives significant improvements over original TDNN
on both speaker verification and identification tasks. It outperforms in
VoxCeleb1 dataset in verification experiment with a 2.6% absolute Equal Error
Rate improvement. In few shots condition CTDNN reaches 90.4% identification
accuracy, which doubles the identification accuracy of original TDNN. We also
compare the proposed CTDNN with another new variant of TDNN, FTDNN, which shows
that our model has a 36% absolute identification accuracy improvement under few
shots condition and can better handle training of a larger batch in a shorter
training time, which better utilize the calculation resources. The code of the
new model is released at https://github.com/chenllliang/CTDNN
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">ARM 4-BIT PQ: SIMD-based Acceleration for Approximate Nearest Neighbor Search on ARM. (arXiv:2203.02505v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02505">
<div class="article-summary-box-inner">
<span><p>We accelerate the 4-bit product quantization (PQ) on the ARM architecture.
Notably, the drastic performance of the conventional 4-bit PQ strongly relies
on x64-specific SIMD register, such as AVX2; hence, we cannot yet achieve such
good performance on ARM. To fill this gap, we first bundle two 128-bit
registers as one 256-bit component. We then apply shuffle operations for each
using the ARM-specific NEON instruction. By making this simple but critical
modification, we achieve a dramatic speedup for the 4-bit PQ on an ARM
architecture. Experiments show that the proposed method consistently achieves a
10x improvement over the naive PQ with the same accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cellular Segmentation and Composition in Routine Histology Images using Deep Learning. (arXiv:2203.02510v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02510">
<div class="article-summary-box-inner">
<span><p>Identification and quantification of nuclei in colorectal cancer haematoxylin
\&amp; eosin (H\&amp;E) stained histology images is crucial to prognosis and patient
management. In computational pathology these tasks are referred to as nuclear
segmentation, classification and composition and are used to extract meaningful
interpretable cytological and architectural features for downstream analysis.
The CoNIC challenge poses the task of automated nuclei segmentation,
classification and composition into six different types of nuclei from the
largest publicly known nuclei dataset - Lizard. In this regard, we have
developed pipelines for the prediction of nuclei segmentation using HoVer-Net
and ALBRT for cellular composition. On testing on the preliminary test set,
HoVer-Net achieved a PQ of 0.58, a PQ+ of 0.58 and finally a mPQ+ of 0.35. For
the prediction of cellular composition with ALBRT on the preliminary test set,
we achieved an overall $R^2$ score of 0.53, consisting of 0.84 for lymphocytes,
0.70 for epithelial cells, 0.70 for plasma and .060 for eosinophils.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive Pseudo Labeling and Informative Active Annotation. (arXiv:2203.02533v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02533">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel semi-supervised learning (SSL) framework
named BoostMIS that combines adaptive pseudo labeling and informative active
annotation to unleash the potential of medical image SSL models: (1) BoostMIS
can adaptively leverage the cluster assumption and consistency regularization
of the unlabeled data according to the current learning status. This strategy
can adaptively generate one-hot ``hard'' labels converted from task model
predictions for better task model training. (2) For the unselected unlabeled
images with low confidence, we introduce an Active learning (AL) algorithm to
find the informative samples as the annotation candidates by exploiting virtual
adversarial perturbation and model's density-aware entropy. These informative
candidates are subsequently fed into the next training cycle for better SSL
label propagation. Notably, the adaptive pseudo-labeling and informative active
annotation form a learning closed-loop that are mutually collaborative to boost
medical image SSL. To verify the effectiveness of the proposed method, we
collected a metastatic epidural spinal cord compression (MESCC) dataset that
aims to optimize MESCC diagnosis and classification for improved specialist
referral and treatment. We conducted an extensive experimental study of
BoostMIS on MESCC and another public dataset COVIDx. The experimental results
verify our framework's effectiveness and generalisability for different medical
image datasets with a significant improvement over various state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Pruning is All You Need for Pruning CNNs at Initialization. (arXiv:2203.02549v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02549">
<div class="article-summary-box-inner">
<span><p>Pruning is a popular technique for reducing the model size and computational
cost of convolutional neural networks (CNNs). However, a slow retraining or
fine-tuning procedure is often required to recover the accuracy loss caused by
pruning. Recently, a new research direction on weight pruning,
pruning-at-initialization (PAI), is proposed to directly prune CNNs before
training so that fine-tuning or retraining can be avoided. While PAI has shown
promising results in reducing the model size, existing approaches rely on
fine-grained weight pruning which requires unstructured sparse matrix
computation, making it difficult to achieve real speedup in practice unless the
sparsity is very high.
</p>
<p>This work is the first to show that fine-grained weight pruning is in fact
not necessary for PAI. Instead, the layerwise compression ratio is the main
critical factor to determine the accuracy of a CNN model pruned at
initialization. Based on this key observation, we propose PreCropping, a
structured hardware-efficient model compression scheme. PreCropping directly
compresses the model at the channel level following the layerwise compression
ratio. Compared to weight pruning, the proposed scheme is regular and dense in
both storage and computation without sacrificing accuracy. In addition, since
PreCropping compresses CNNs at initialization, the computational and memory
costs of CNNs are reduced for both training and inference on commodity
hardware. We empirically demonstrate our approaches on several modern CNN
architectures, including ResNet, ShuffleNet, and MobileNet for both CIFAR-10
and ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building 3D Generative Models from Minimal Data. (arXiv:2203.02554v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02554">
<div class="article-summary-box-inner">
<span><p>We propose a method for constructing generative models of 3D objects from a
single 3D mesh and improving them through unsupervised low-shot learning from
2D images. Our method produces a 3D morphable model that represents shape and
albedo in terms of Gaussian processes. Whereas previous approaches have
typically built 3D morphable models from multiple high-quality 3D scans through
principal component analysis, we build 3D morphable models from a single scan
or template. As we demonstrate in the face domain, these models can be used to
infer 3D reconstructions from 2D data (inverse graphics) or 3D data
(registration). Specifically, we show that our approach can be used to perform
face recognition using only a single 3D template (one scan total, not one per
person). We extend our model to a preliminary unsupervised learning framework
that enables the learning of the distribution of 3D faces using one 3D template
and a small number of 2D images. This approach could also provide a model for
the origins of face perception in human infants, who appear to start with an
innate face template and subsequently develop a flexible system for perceiving
the 3D structure of any novel face from experience with only 2D images of a
relatively small number of familiar faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation. (arXiv:2203.02557v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02557">
<div class="article-summary-box-inner">
<span><p>Image-to-image translation has broad applications in art, design, and
scientific simulations. The original CycleGAN model emphasizes one-to-one
mapping via a cycle-consistent loss, while more recent works promote
one-to-many mapping to boost the diversity of the translated images. With
scientific simulation and one-to-one needs in mind, this work examines if
equipping CycleGAN with a vision transformer (ViT) and employing advanced
generative adversarial network (GAN) training techniques can achieve better
performance. The resulting UNet ViT Cycle-consistent GAN (UVCGAN) model is
compared with previous best-performing models on open benchmark image-to-image
translation datasets, Selfie2Anime and CelebA. UVCGAN performs better and
retains a strong correlation between the original and translated images. An
accompanying ablation study shows that the gradient penalty and BERT-like
pre-training also contribute to the improvement.~To promote reproducibility and
open science, the source code, hyperparameter configurations, and pre-trained
model will be made available at: https://github.com/LS4GAN/uvcga.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Energy Efficiency and Robustness of tinyML Computer Vision using Log-Gradient Input Images. (arXiv:2203.02571v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02571">
<div class="article-summary-box-inner">
<span><p>This paper studies the merits of applying log-gradient input images to
convolutional neural networks (CNNs) for tinyML computer vision (CV). We show
that log gradients enable: (i) aggressive 1.5-bit quantization of first-layer
inputs, (ii) potential CNN resource reductions, and (iii) inherent robustness
to illumination changes (1.7% accuracy loss across 1/32...8 brightness
variation vs. up to 10% for JPEG). We establish these results using the PASCAL
RAW image data set and through a combination of experiments using neural
architecture search and a fixed three-layer network. The latter reveal that
training on log-gradient images leads to higher filter similarity, making the
CNN more prunable. The combined benefits of aggressive first-layer
quantization, CNN resource reductions, and operation without tight exposure
control and image signal processing (ISP) are helpful for pushing tinyML CV
toward its ultimate efficiency limits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning. (arXiv:2203.02573v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02573">
<div class="article-summary-box-inner">
<span><p>Most methods for conditional video synthesis use a single modality as the
condition. This comes with major limitations. For example, it is problematic
for a model conditioned on an image to generate a specific motion trajectory
desired by the user since there is no means to provide motion information.
Conversely, language information can describe the desired motion, while not
precisely defining the content of the video. This work presents a multimodal
video generation framework that benefits from text and images provided jointly
or separately. We leverage the recent progress in quantized representations for
videos and apply a bidirectional transformer with multiple modalities as inputs
to predict a discrete video representation. To improve video quality and
consistency, we propose a new video token trained with self-learning and an
improved mask-prediction algorithm for sampling video tokens. We introduce text
augmentation to improve the robustness of the textual representation and
diversity of generated videos. Our framework can incorporate various visual
modalities, such as segmentation masks, drawings, and partially occluded
images. It can generate much longer sequences than the one used for training.
In addition, our model can extract visual information as suggested by the text
prompt, e.g., "an object in image one is moving northeast", and generate
corresponding videos. We run evaluations on three public datasets and a newly
collected dataset labeled with facial attributes, achieving state-of-the-art
generation results on all four.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style-ERD: Responsive and Coherent Online Motion Style Transfer. (arXiv:2203.02574v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02574">
<div class="article-summary-box-inner">
<span><p>Motion style transfer is a common method for enriching character animation.
Motion style transfer algorithms are often designed for offline settings where
motions are processed in segments. However, for online animation applications,
such as realtime avatar animation from motion capture, motions need to be
processed as a stream with minimal latency. In this work, we realize a
flexible, high-quality motion style transfer method for this setting. We
propose a novel style transfer model, Style-ERD, to stylize motions in an
online manner with an Encoder-Recurrent-Decoder structure, along with a novel
discriminator that combines feature attention and temporal attention. Our
method stylizes motions into multiple target styles with a unified model.
Although our method targets online settings, it outperforms previous offline
methods in motion realism and style expressiveness and provides significant
gains in runtime efficiency
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Learning of Reusable Abstract Models for Object Goal Navigation. (arXiv:2203.02583v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02583">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel approach to incrementally learn an Abstract
Model of an unknown environment, and show how an agent can reuse the learned
model for tackling the Object Goal Navigation task. The Abstract Model is a
finite state machine in which each state is an abstraction of a state of the
environment, as perceived by the agent in a certain position and orientation.
The perceptions are high-dimensional sensory data (e.g., RGB-D images), and the
abstraction is reached by exploiting image segmentation and the Taskonomy model
bank. The learning of the Abstract Model is accomplished by executing actions,
observing the reached state, and updating the Abstract Model with the acquired
information. The learned models are memorized by the agent, and they are reused
whenever it recognizes to be in an environment that corresponds to the stored
model. We investigate the effectiveness of the proposed approach for the Object
Goal Navigation task, relying on public benchmarks. Our results show that the
reuse of learned Abstract Models can boost performance on Object Goal
Navigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept-based Explanations for Out-Of-Distribution Detectors. (arXiv:2203.02586v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02586">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection plays a crucial role in ensuring the safe
deployment of deep neural network (DNN) classifiers. While a myriad of methods
have focused on improving the performance of OOD detectors, a critical gap
remains in interpreting their decisions. We help bridge this gap by providing
explanations for OOD detectors based on learned high-level concepts. We first
propose two new metrics for assessing the effectiveness of a particular set of
concepts for explaining OOD detectors: 1) detection completeness, which
quantifies the sufficiency of concepts for explaining an OOD-detector's
decisions, and 2) concept separability, which captures the distributional
separation between in-distribution and OOD data in the concept space. Based on
these metrics, we propose a framework for learning a set of concepts that
satisfy the desired properties of detection completeness and concept
separability and demonstrate the framework's effectiveness in providing
concept-based explanations for diverse OOD techniques. We also show how to
identify prominent concepts that contribute to the detection results via a
modified Shapley value-based importance score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Quality Index Metric and Method for Online Self-Assessment of Autonomous Vehicles Sensory Perception. (arXiv:2203.02588v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02588">
<div class="article-summary-box-inner">
<span><p>Perception is critical to autonomous driving safety. Camera-based object
detection is one of the most important methods for autonomous vehicle
perception. Current camera-based object detection solutions for autonomous
driving cannot provide feedback on the detection performance for each frame. We
propose an evaluation metric, namely the perception quality index (PQI), to
assess the camera-based object detection algorithm performance and provide the
perception quality feedback frame by frame. The method of the PQI generation is
by combining the fine-grained saliency map intensity with the object detection
algorithm's output results. Furthermore, we developed a superpixel-based
attention network (SPA-NET) to predict the proposed PQI evaluation metric by
using raw image pixels and superpixels as input. The proposed evaluation metric
and prediction network are tested on three open-source datasets. The proposed
evaluation metric can correctly assess the camera-based perception quality
under the autonomous driving environment according to the experiment results.
The network regression R-square values determine the comparison among models.
It is shown that a Perception Quality Index is useful in self-evaluating a
cameras visual scene perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geodesic Gramian Denoising Applied to the Images Contaminated With Noise Sampled From Diverse Probability Distributions. (arXiv:2203.02600v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02600">
<div class="article-summary-box-inner">
<span><p>As quotidian use of sophisticated cameras surges, people in modern society
are more interested in capturing fine-quality images. However, the quality of
the images might be inferior to people's expectations due to the noise
contamination in the images. Thus, filtering out the noise while preserving
vital image features is an essential requirement. Current existing denoising
methods have their own assumptions on the probability distribution in which the
contaminated noise is sampled for the method to attain its expected denoising
performance. In this paper, we utilize our recent Gramian-based filtering
scheme to remove noise sampled from five prominent probability distributions
from selected images. This method preserves image smoothness by adopting
patches partitioned from the image, rather than pixels, and retains vital image
features by performing denoising on the manifold underlying the patch space
rather than in the image domain. We validate its denoising performance, using
three benchmark computer vision test images applied to two state-of-the-art
denoising methods, namely BM3D and K-SVD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plant Species Recognition with Optimized 3D Polynomial Neural Networks and Variably Overlapping Time-Coherent Sliding Window. (arXiv:2203.02611v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02611">
<div class="article-summary-box-inner">
<span><p>Recently, the EAGL-I system was developed to rapidly create massive labeled
datasets of plants intended to be commonly used by farmers and researchers to
create AI-driven solutions in agriculture. As a result, a publicly available
plant species recognition dataset composed of 40,000 images with different
sizes consisting of 8 plant species was created with the system in order to
demonstrate its capabilities. This paper proposes a novel method, called
Variably Overlapping Time-Coherent Sliding Window (VOTCSW), that transforms a
dataset composed of images with variable size to a 3D representation with fixed
size that is suitable for convolutional neural networks, and demonstrates that
this representation is more informative than resizing the images of the dataset
to a given size. We theoretically formalized the use cases of the method as
well as its inherent properties and we proved that it has an oversampling and a
regularization effect on the data. By combining the VOTCSW method with the 3D
extension of a recently proposed machine learning model called 1-Dimensional
Polynomial Neural Networks, we were able to create a model that achieved a
state-of-the-art accuracy of 99.9% on the dataset created by the EAGL-I system,
surpassing well-known architectures such as ResNet and Inception. In addition,
we created a heuristic algorithm that enables the degree reduction of any
pre-trained N-Dimensional Polynomial Neural Network and which compresses it
without altering its performance, thus making the model faster and lighter.
Furthermore, we established that the currently available dataset could not be
used for machine learning in its present form, due to a substantial class
imbalance between the training set and the test set. Hence, we created a
specific preprocessing and a model development framework that enabled us to
improve the accuracy from 49.23% to 99.9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Train Unstable Looped Tensor Network. (arXiv:2203.02617v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02617">
<div class="article-summary-box-inner">
<span><p>A rising problem in the compression of Deep Neural Networks is how to reduce
the number of parameters in convolutional kernels and the complexity of these
layers by low-rank tensor approximation. Canonical polyadic tensor
decomposition (CPD) and Tucker tensor decomposition (TKD) are two solutions to
this problem and provide promising results. However, CPD often fails due to
degeneracy, making the networks unstable and hard to fine-tune. TKD does not
provide much compression if the core tensor is big. This motivates using a
hybrid model of CPD and TKD, a decomposition with multiple Tucker models with
small core tensor, known as block term decomposition (BTD). This paper proposes
a more compact model that further compresses the BTD by enforcing core tensors
in BTD identical. We establish a link between the BTD with shared parameters
and a looped chain tensor network (TC). Unfortunately, such strongly
constrained tensor networks (with loop) encounter severe numerical instability,
as proved by y (Landsberg, 2012) and (Handschuh, 2015a). We study perturbation
of chain tensor networks, provide interpretation of instability in TC,
demonstrate the problem. We propose novel methods to gain the stability of the
decomposition results, keep the network robust and attain better approximation.
Experimental results will confirm the superiority of the proposed methods in
compression of well-known CNNs, and TC decomposition under challenging
scenarios
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Important Object Identification with Semi-Supervised Learning for Autonomous Driving. (arXiv:2203.02634v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02634">
<div class="article-summary-box-inner">
<span><p>Accurate identification of important objects in the scene is a prerequisite
for safe and high-quality decision making and motion planning of intelligent
agents (e.g., autonomous vehicles) that navigate in complex and dynamic
environments. Most existing approaches attempt to employ attention mechanisms
to learn importance weights associated with each object indirectly via various
tasks (e.g., trajectory prediction), which do not enforce direct supervision on
the importance estimation. In contrast, we tackle this task in an explicit way
and formulate it as a binary classification ("important" or "unimportant")
problem. We propose a novel approach for important object identification in
egocentric driving scenarios with relational reasoning on the objects in the
scene. Besides, since human annotations are limited and expensive to obtain, we
present a semi-supervised learning pipeline to enable the model to learn from
unlimited unlabeled data. Moreover, we propose to leverage the auxiliary tasks
of ego vehicle behavior prediction to further improve the accuracy of
importance estimation. The proposed approach is evaluated on a public
egocentric driving dataset (H3D) collected in complex traffic scenarios. A
detailed ablative study is conducted to demonstrate the effectiveness of each
model component and the training strategy. Our approach also outperforms
rule-based baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training privacy-preserving video analytics pipelines by suppressing features that reveal information about private attributes. (arXiv:2203.02635v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02635">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are increasingly deployed for scene analytics, including
to evaluate the attention and reaction of people exposed to out-of-home
advertisements. However, the features extracted by a deep neural network that
was trained to predict a specific, consensual attribute (e.g. emotion) may also
encode and thus reveal information about private, protected attributes (e.g.
age or gender). In this work, we focus on such leakage of private information
at inference time. We consider an adversary with access to the features
extracted by the layers of a deployed neural network and use these features to
predict private attributes. To prevent the success of such an attack, we modify
the training of the network using a confusion loss that encourages the
extraction of features that make it difficult for the adversary to accurately
predict private attributes. We validate this training approach on image-based
tasks using a publicly available dataset. Results show that, compared to the
original network, the proposed PrivateNet can reduce the leakage of private
information of a state-of-the-art emotion recognition classifier by 2.88% for
gender and by 13.06% for age group, with a minimal effect on task accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Crowd Counting via Multifaceted Attention. (arXiv:2203.02636v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02636">
<div class="article-summary-box-inner">
<span><p>This paper focuses on the challenging crowd counting task. As large-scale
variations often exist within crowd images, neither fixed-size convolution
kernel of CNN nor fixed-size attention of recent vision transformers can well
handle this kind of variation. To address this problem, we propose a
Multifaceted Attention Network (MAN) to improve transformer models in local
spatial relation encoding. MAN incorporates global attention from a vanilla
transformer, learnable local attention, and instance attention into a counting
model. Firstly, the local Learnable Region Attention (LRA) is proposed to
assign attention exclusively for each feature location dynamically. Secondly,
we design the Local Attention Regularization to supervise the training of LRA
by minimizing the deviation among the attention for different feature
locations. Finally, we provide an Instance Attention mechanism to focus on the
most important instances dynamically during training. Extensive experiments on
four challenging crowd counting datasets namely ShanghaiTech, UCF-QNRF, JHU++,
and NWPU have validated the proposed method. Codes:
https://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cluster-based Contrastive Disentangling for Generalized Zero-Shot Learning. (arXiv:2203.02648v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02648">
<div class="article-summary-box-inner">
<span><p>Generalized Zero-Shot Learning (GZSL) aims to recognize both seen and unseen
classes by training only the seen classes, in which the instances of unseen
classes tend to be biased towards the seen class. In this paper, we propose a
Cluster-based Contrastive Disentangling (CCD) method to improve GZSL by
alleviating the semantic gap and domain shift problems. Specifically, we first
cluster the batch data to form several sets containing similar classes. Then,
we disentangle the visual features into semantic-unspecific and
semantic-matched variables, and further disentangle the semantic-matched
variables into class-shared and class-unique variables according to the
clustering results. The disentangled learning module with random swapping and
semantic-visual alignment bridges the semantic gap. Moreover, we introduce
contrastive learning on semantic-matched and class-unique variables to learn
high intra-set and intra-class similarity, as well as inter-set and inter-class
discriminability. Then, the generated visual features conform to the underlying
characteristics of general images and have strong discriminative information,
which alleviates the domain shift problem well. We evaluate our proposed method
on four datasets and achieve state-of-the-art results in both conventional and
generalized settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble Knowledge Guided Sub-network Search and Fine-tuning for Filter Pruning. (arXiv:2203.02651v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02651">
<div class="article-summary-box-inner">
<span><p>Conventional NAS-based pruning algorithms aim to find the sub-network with
the best validation performance. However, validation performance does not
successfully represent test performance, i.e., potential performance. Also,
although fine-tuning the pruned network to restore the performance drop is an
inevitable process, few studies have handled this issue. This paper proposes a
novel sub-network search and fine-tuning method that is named Ensemble
Knowledge Guidance (EKG). First, we experimentally prove that the fluctuation
of the loss landscape is an effective metric to evaluate the potential
performance. In order to search a sub-network with the smoothest loss landscape
at a low cost, we propose a pseudo-supernet built by an ensemble sub-network
knowledge distillation. Next, we propose a novel fine-tuning that re-uses the
information of the search phase. We store the interim sub-networks, that is,
the by-products of the search phase, and transfer their knowledge into the
pruned network. Note that EKG is easy to be plugged-in and computationally
efficient. For example, in the case of ResNet-50, about 45% of FLOPS is removed
without any performance drop in only 315 GPU hours. The implemented code is
available at https://github.com/sseung0703/EKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Large-scale Comprehensive Dataset and Copy-overlap Aware Evaluation Protocol for Segment-level Video Copy Detection. (arXiv:2203.02654v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02654">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce VCSL (Video Copy Segment Localization), a new
comprehensive segment-level annotated video copy dataset. Compared with
existing copy detection datasets restricted by either video-level annotation or
small-scale, VCSL not only has two orders of magnitude more segment-level
labelled data, with 160k realistic video copy pairs containing more than 280k
localized copied segment pairs, but also covers a variety of video categories
and a wide range of video duration. All the copied segments inside each
collected video pair are manually extracted and accompanied by precisely
annotated starting and ending timestamps. Alongside the dataset, we also
propose a novel evaluation protocol that better measures the prediction
accuracy of copy overlapping segments between a video pair and shows improved
adaptability in different scenarios. By benchmarking several baseline and
state-of-the-art segment-level video copy detection methods with the proposed
dataset and evaluation metric, we provide a comprehensive analysis that
uncovers the strengths and weaknesses of current approaches, hoping to open up
promising directions for future works. The VCSL dataset, metric and benchmark
codes are all publicly available at https://github.com/alipay/VCSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-visual speech separation based on joint feature representation with cross-modal attention. (arXiv:2203.02655v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02655">
<div class="article-summary-box-inner">
<span><p>Multi-modal based speech separation has exhibited a specific advantage on
isolating the target character in multi-talker noisy environments.
Unfortunately, most of current separation strategies prefer a straightforward
fusion based on feature learning of each single modality, which is far from
sufficient consideration of inter-relationships between modalites. Inspired by
learning joint feature representations from audio and visual streams with
attention mechanism, in this study, a novel cross-modal fusion strategy is
proposed to benefit the whole framework with semantic correlations between
different modalities. To further improve audio-visual speech separation, the
dense optical flow of lip motion is incorporated to strengthen the robustness
of visual representation. The evaluation of the proposed work is performed on
two public audio-visual speech separation benchmark datasets. The overall
improvement of the performance has demonstrated that the additional motion
network effectively enhances the visual representation of the combined lip
images and audio signal, as well as outperforming the baseline in terms of all
metrics with the proposed cross-modal fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers. (arXiv:2203.02664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02664">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised semantic segmentation (WSSS) with image-level labels is an
important and challenging task. Due to the high training efficiency, end-to-end
solutions for WSSS have received increasing attention from the community.
However, current methods are mainly based on convolutional neural networks and
fail to explore the global information properly, thus usually resulting in
incomplete object regions. In this paper, to address the aforementioned
problem, we introduce Transformers, which naturally integrate global
information, to generate more integral initial pseudo labels for end-to-end
WSSS. Motivated by the inherent consistency between the self-attention in
Transformers and the semantic affinity, we propose an Affinity from Attention
(AFA) module to learn semantic affinity from the multi-head self-attention
(MHSA) in Transformers. The learned affinity is then leveraged to refine the
initial pseudo labels for segmentation. In addition, to efficiently derive
reliable affinity labels for supervising AFA and ensure the local consistency
of pseudo labels, we devise a Pixel-Adaptive Refinement module that
incorporates low-level image appearance information to refine the pseudo
labels. We perform extensive experiments and our method achieves 66.0% and
38.9% mIoU on the PASCAL VOC 2012 and MS COCO 2014 datasets, respectively,
significantly outperforming recent end-to-end methods and several multi-stage
competitors. Code is available at https://github.com/rulixiang/afa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross Language Image Matching for Weakly Supervised Semantic Segmentation. (arXiv:2203.02668v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02668">
<div class="article-summary-box-inner">
<span><p>It has been widely known that CAM (Class Activation Map) usually only
activates discriminative object regions and falsely includes lots of
object-related backgrounds. As only a fixed set of image-level object labels
are available to the WSSS (weakly supervised semantic segmentation) model, it
could be very difficult to suppress those diverse background regions consisting
of open set objects. In this paper, we propose a novel Cross Language Image
Matching (CLIMS) framework, based on the recently introduced Contrastive
Language-Image Pre-training (CLIP) model, for WSSS. The core idea of our
framework is to introduce natural language supervision to activate more
complete object regions and suppress closely-related open background regions.
In particular, we design object, background region and text label matching
losses to guide the model to excite more reasonable object regions for CAM of
each category. In addition, we design a co-occurring background suppression
loss to prevent the model from activating closely-related background regions,
with a predefined set of class-related background text descriptions. These
designs enable the proposed CLIMS to generate a more complete and compact
activation map for the target objects. Extensive experiments on PASCAL VOC2012
dataset show that our CLIMS significantly outperforms the previous
state-of-the-art methods. Code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Newton-PnP: Real-time Visual Navigation for Autonomous Toy-Drones. (arXiv:2203.02686v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02686">
<div class="article-summary-box-inner">
<span><p>The Perspective-n-Point problem aims to estimate the relative pose between a
calibrated monocular camera and a known 3D model, by aligning pairs of 2D
captured image points to their corresponding 3D points in the model. We suggest
an algorithm that runs on weak IoT devices in real-time but still provides
provable theoretical guarantees for both running time and correctness. Existing
solvers provide only one of these requirements. Our main motivation was to turn
the popular DJI's Tello Drone (&lt;90gr, &lt;\$100) into an autonomous drone that
navigates in an indoor environment with no external human/laptop/sensor, by
simply attaching a Raspberry PI Zero (&lt;9gr, &lt;\$25) to it. This tiny
micro-processor takes as input a real-time video from a tiny RGB camera, and
runs our PnP solver on-board. Extensive experimental results, open source code,
and a demonstration video are included.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection. (arXiv:2203.02688v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02688">
<div class="article-summary-box-inner">
<span><p>The recently proposed camouflaged object detection (COD) attempts to segment
objects that are visually blended into their surroundings, which is extremely
complex and difficult in real-world scenarios. Apart from high intrinsic
similarity between the camouflaged objects and their background, the objects
are usually diverse in scale, fuzzy in appearance, and even severely occluded.
To deal with these problems, we propose a mixed-scale triplet network,
\textbf{ZoomNet}, which mimics the behavior of humans when observing vague
images, i.e., zooming in and out. Specifically, our ZoomNet employs the zoom
strategy to learn the discriminative mixed-scale semantics by the designed
scale integration unit and hierarchical mixed-scale unit, which fully explores
imperceptible clues between the candidate objects and background surroundings.
Moreover, considering the uncertainty and ambiguity derived from
indistinguishable textures, we construct a simple yet effective regularization
constraint, uncertainty-aware loss, to promote the model to accurately produce
predictions with higher confidence in candidate regions. Without bells and
whistles, our proposed highly task-friendly model consistently surpasses the
existing 23 state-of-the-art methods on four public datasets. Besides, the
superior performance over the recent cutting-edge models on the SOD task also
verifies the effectiveness and generality of our model. The code will be
available at \url{https://github.com/lartpang/ZoomNet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated and Generalized Person Re-identification through Domain and Feature Hallucinating. (arXiv:2203.02689v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02689">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the problem of federated domain generalization
(FedDG) for person re-identification (re-ID), which aims to learn a generalized
model with multiple decentralized labeled source domains. An empirical method
(FedAvg) trains local models individually and averages them to obtain the
global model for further local fine-tuning or deploying in unseen target
domains. One drawback of FedAvg is neglecting the data distributions of other
clients during local training, making the local model overfit local data and
producing a poorly-generalized global model. To solve this problem, we propose
a novel method, called "Domain and Feature Hallucinating (DFH)", to produce
diverse features for learning generalized local and global models.
Specifically, after each model aggregation process, we share the Domain-level
Feature Statistics (DFS) among different clients without violating data
privacy. During local training, the DFS are used to synthesize novel domain
statistics with the proposed domain hallucinating, which is achieved by
re-weighting DFS with random weights. Then, we propose feature hallucinating to
diversify local features by scaling and shifting them to the distribution of
the obtained novel domain. The synthesized novel features retain the original
pair-wise similarities, enabling us to utilize them to optimize the model in a
supervised manner. Extensive experiments verify that the proposed DFH can
effectively improve the generalization ability of the global model. Our method
achieves the state-of-the-art performance for FedDG on four large-scale re-ID
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IDmUNet: A new image decomposition induced network for sparse feature segmentation. (arXiv:2203.02690v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02690">
<div class="article-summary-box-inner">
<span><p>UNet and its variants are among the most popular methods for medical image
segmentation. Despite their successes in task generality, most of them consider
little mathematical modeling behind specific applications. In this paper, we
focus on the sparse feature segmentation task and make a task-oriented network
design, in which the target objects are sparsely distributed and the background
is hard to be mathematically modeled. We start from an image decomposition
model with sparsity regularization, and propose a deep unfolding network,
namely IDNet, based on an iterative solver, scaled alternating direction method
of multipliers (scaled-ADMM). The IDNet splits raw inputs into double feature
layers. Then a new task-oriented segmentation network is constructed, dubbed as
IDmUNet, based on the proposed IDNets and a mini-UNet. Because of the sparsity
prior and deep unfolding method in the structure design, this IDmUNet combines
the advantages of mathematical modeling and data-driven approaches. Firstly,
our approach has mathematical interpretability and can achieve favorable
performance with far fewer learnable parameters. Secondly, our IDmUNet is
robust in a simple end-to-end training with explainable behaviors. In the
experiments of retinal vessel segmentation (RVS), IDmUNet produces the
state-of-the-art results with only 0.07m parameters, whereas SA-UNet, one of
the latest variants of UNet, contains 0.54m and the original UNet 31.04m.
Moreover, the training procedure of our network converges faster without
overfitting phenomenon. This decomposition-based network construction strategy
can be generalized to other problems with mathematically clear targets and
complicated unclear backgrounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-resolution Coastline Extraction in SAR Images via MISP-GGD Superpixel Segmentation. (arXiv:2203.02708v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02708">
<div class="article-summary-box-inner">
<span><p>High accuracy coastline/shoreline extraction from SAR imagery is a crucial
step in a number of maritime and coastal monitoring applications. We present a
method based on image segmentation using the Generalised Gamma Mixture Model
superpixel algorithm (MISP-GGD). MISP-GGD produces superpixels adhering with
great accuracy to object edges in the image, such as the coastline.
Unsupervised clustering of the generated superpixels according to textural and
radiometric features allows for generation of a land/water mask from which a
highly accurate coastline can be extracted. We present results of our proposed
method on a number of SAR images of varying characteristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Efficient and Scalable Sharpness-Aware Minimization. (arXiv:2203.02714v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02714">
<div class="article-summary-box-inner">
<span><p>Recently, Sharpness-Aware Minimization (SAM), which connects the geometry of
the loss landscape and generalization, has demonstrated significant performance
boosts on training large-scale models such as vision transformers. However, the
update rule of SAM requires two sequential (non-parallelizable) gradient
computations at each step, which can double the computational overhead. In this
paper, we propose a novel algorithm LookSAM - that only periodically calculates
the inner gradient ascent, to significantly reduce the additional training cost
of SAM. The empirical results illustrate that LookSAM achieves similar accuracy
gains to SAM while being tremendously faster - it enjoys comparable
computational complexity with first-order optimizers such as SGD or Adam. To
further evaluate the performance and scalability of LookSAM, we incorporate a
layer-wise modification and perform experiments in the large-batch training
scenario, which is more prone to converge to sharp local minima. We are the
first to successfully scale up the batch size when training Vision Transformers
(ViTs). With a 64k batch size, we are able to train ViTs from scratch in
minutes while maintaining competitive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Dual Dense Connection Network for Video Super-resolution. (arXiv:2203.02723v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02723">
<div class="article-summary-box-inner">
<span><p>Video super-resolution (VSR) refers to the reconstruction of high-resolution
(HR) video from the corresponding low-resolution (LR) video. Recently, VSR has
received increasing attention. In this paper, we propose a novel dual dense
connection network that can generate high-quality super-resolution (SR)
results. The input frames are creatively divided into reference frame,
pre-temporal group and post-temporal group, representing information in
different time periods. This grouping method provides accurate information of
different time periods without causing time information disorder. Meanwhile, we
produce a new loss function, which is beneficial to enhance the convergence
ability of the model. Experiments show that our model is superior to other
advanced models in Vid4 datasets and SPMCS-11 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An End-to-End Approach for Seam Carving Detection using Deep Neural Networks. (arXiv:2203.02728v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02728">
<div class="article-summary-box-inner">
<span><p>Seam carving is a computational method capable of resizing images for both
reduction and expansion based on its content, instead of the image geometry.
Although the technique is mostly employed to deal with redundant information,
i.e., regions composed of pixels with similar intensity, it can also be used
for tampering images by inserting or removing relevant objects. Therefore,
detecting such a process is of extreme importance regarding the image security
domain. However, recognizing seam-carved images does not represent a
straightforward task even for human eyes, and robust computation tools capable
of identifying such alterations are very desirable. In this paper, we propose
an end-to-end approach to cope with the problem of automatic seam carving
detection that can obtain state-of-the-art results. Experiments conducted over
public and private datasets with several tampering configurations evidence the
suitability of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaxDropoutV2: An Improved Method to Drop out Neurons in Convolutional Neural Networks. (arXiv:2203.02740v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02740">
<div class="article-summary-box-inner">
<span><p>In the last decade, exponential data growth supplied the machine
learning-based algorithms' capacity and enabled their usage in daily life
activities. Additionally, such an improvement is partially explained due to the
advent of deep learning techniques, i.e., stacks of simple architectures that
end up in more complex models. Although both factors produce outstanding
results, they also pose drawbacks regarding the learning process since training
complex models denotes an expensive task and results are prone to overfit the
training data. A supervised regularization technique called MaxDropout was
recently proposed to tackle the latter, providing several improvements
concerning traditional regularization approaches. In this paper, we present its
improved version called MaxDropoutV2. Results considering two public datasets
show that the model performs faster than the standard version and, in most
cases, provides more accurate results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaFormer: A Unified Meta Framework for Fine-Grained Recognition. (arXiv:2203.02751v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02751">
<div class="article-summary-box-inner">
<span><p>Fine-Grained Visual Classification(FGVC) is the task that requires
recognizing the objects belonging to multiple subordinate categories of a
super-category. Recent state-of-the-art methods usually design sophisticated
learning pipelines to tackle this task. However, visual information alone is
often not sufficient to accurately differentiate between fine-grained visual
categories. Nowadays, the meta-information (e.g., spatio-temporal prior,
attribute, and text description) usually appears along with the images. This
inspires us to ask the question: Is it possible to use a unified and simple
framework to utilize various meta-information to assist in fine-grained
identification? To answer this problem, we explore a unified and strong
meta-framework(MetaFormer) for fine-grained visual classification. In practice,
MetaFormer provides a simple yet effective approach to address the joint
learning of vision and various meta-information. Moreover, MetaFormer also
provides a strong baseline for FGVC without bells and whistles. Extensive
experiments demonstrate that MetaFormer can effectively use various
meta-information to improve the performance of fine-grained recognition. In a
fair comparison, MetaFormer can outperform the current SotA approaches with
only vision information on the iNaturalist2017 and iNaturalist2018 datasets.
Adding meta-information, MetaFormer can exceed the current SotA approaches by
5.9% and 5.3%, respectively. Moreover, MetaFormer can achieve 92.3% and 92.7%
on CUB-200-2011 and NABirds, which significantly outperforms the SotA
approaches. The source code and pre-trained models are released
athttps://github.com/dqshuai/MetaFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DrawingInStyles: Portrait Image Generation and Editing with Spatially Conditioned StyleGAN. (arXiv:2203.02762v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02762">
<div class="article-summary-box-inner">
<span><p>The research topic of sketch-to-portrait generation has witnessed a boost of
progress with deep learning techniques. The recently proposed StyleGAN
architectures achieve state-of-the-art generation ability but the original
StyleGAN is not friendly for sketch-based creation due to its unconditional
generation nature. To address this issue, we propose a direct conditioning
strategy to better preserve the spatial information under the StyleGAN
framework. Specifically, we introduce Spatially Conditioned StyleGAN
(SC-StyleGAN for short), which explicitly injects spatial constraints to the
original StyleGAN generation process. We explore two input modalities, sketches
and semantic maps, which together allow users to express desired generation
results more precisely and easily. Based on SC-StyleGAN, we present
DrawingInStyles, a novel drawing interface for non-professional users to easily
produce high-quality, photo-realistic face images with precise control, either
from scratch or editing existing ones. Qualitative and quantitative evaluations
show the superior generation ability of our method to existing and alternative
solutions. The usability and expressiveness of our system are confirmed by a
user study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation. (arXiv:2203.02764v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02764">
<div class="article-summary-box-inner">
<span><p>Most existing works in vision-and-language navigation (VLN) focus on either
discrete or continuous environments, training agents that cannot generalize
across the two. The fundamental difference between the two setups is that
discrete navigation assumes prior knowledge of the connectivity graph of the
environment, so that the agent can effectively transfer the problem of
navigation with low-level controls to jumping from node to node with high-level
actions by grounding to an image of a navigable direction. To bridge the
discrete-to-continuous gap, we propose a predictor to generate a set of
candidate waypoints during navigation, so that agents designed with high-level
actions can be transferred to and trained in continuous environments. We refine
the connectivity graph of Matterport3D to fit the continuous
Habitat-Matterport3D, and train the waypoints predictor with the refined graphs
to produce accessible waypoints at each time step. Moreover, we demonstrate
that the predicted waypoints can be augmented during training to diversify the
views and paths, and therefore enhance agent's generalization ability. Through
extensive experiments we show that agents navigating in continuous environments
with predicted waypoints perform significantly better than agents using
low-level actions, which reduces the absolute discrete-to-continuous gap by
11.76% Success Weighted by Path Length (SPL) for the Cross-Modal Matching Agent
and 18.24% SPL for the Recurrent VLN-BERT. Our agents, trained with a simple
imitation learning objective, outperform previous methods by a large margin,
achieving new state-of-the-art results on the testing environments of the
R2R-CE and the RxR-CE datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Part-aware Instance Segmentation for Industrial Bin Picking. (arXiv:2203.02767v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02767">
<div class="article-summary-box-inner">
<span><p>Industrial bin picking is a challenging task that requires accurate and
robust segmentation of individual object instances. Particularly, industrial
objects can have irregular shapes, that is, thin and concave, whereas in
bin-picking scenarios, objects are often closely packed with strong occlusion.
To address these challenges, we formulate a novel part-aware instance
segmentation pipeline. The key idea is to decompose industrial objects into
correlated approximate convex parts and enhance the object-level segmentation
with part-level segmentation. We design a part-aware network to predict part
masks and part-to-part offsets, followed by a part aggregation module to
assemble the recognized parts into instances. To guide the network learning, we
also propose an automatic label decoupling scheme to generate ground-truth
part-level labels from instance-level labels. Finally, we contribute the first
instance segmentation dataset, which contains a variety of industrial objects
that are thin and have non-trivial shapes. Extensive experimental results on
various industrial objects demonstrate that our method can achieve the best
segmentation results compared with the state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Be So Dense: Sparse-to-Sparse GAN Training Without Sacrificing Performance. (arXiv:2203.02770v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02770">
<div class="article-summary-box-inner">
<span><p>Generative adversarial networks (GANs) have received an upsurging interest
since being proposed due to the high quality of the generated data. While
achieving increasingly impressive results, the resource demands associated with
the large model size hinders the usage of GANs in resource-limited scenarios.
For inference, the existing model compression techniques can reduce the model
complexity with comparable performance. However, the training efficiency of
GANs has less been explored due to the fragile training process of GANs. In
this paper, we, for the first time, explore the possibility of directly
training sparse GAN from scratch without involving any dense or pre-training
steps. Even more unconventionally, our proposed method enables directly
training sparse unbalanced GANs with an extremely sparse generator from
scratch. Instead of training full GANs, we start with sparse GANs and
dynamically explore the parameter space spanned over the generator throughout
training. Such a sparse-to-sparse training procedure enhances the capacity of
the highly sparse generator progressively while sticking to a fixed small
parameter budget with appealing training and inference efficiency gains.
Extensive experiments with modern GAN architectures validate the effectiveness
of our method. Our sparsified GANs, trained from scratch in one single run, are
able to outperform the ones learned by expensive iterative pruning and
re-training. Perhaps most importantly, we find instead of inheriting parameters
from expensive pre-trained GANs, directly training sparse GANs from scratch can
be a much more efficient solution. For example, only training with a 80% sparse
generator and a 70% sparse discriminator, our method can achieve even better
performance than the dense BigGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rib Suppression in Digital Chest Tomosynthesis. (arXiv:2203.02772v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02772">
<div class="article-summary-box-inner">
<span><p>Digital chest tomosynthesis (DCT) is a technique to produce sectional 3D
images of a human chest for pulmonary disease screening, with 2D X-ray
projections taken within an extremely limited range of angles. However, under
the limited angle scenario, DCT contains strong artifacts caused by the
presence of ribs, jamming the imaging quality of the lung area. Recently, great
progress has been achieved for rib suppression in a single X-ray image, to
reveal a clearer lung texture. We firstly extend the rib suppression problem to
the 3D case at the software level. We propose a $\textbf{T}$omosynthesis
$\textbf{RI}$b Su$\textbf{P}$pression and $\textbf{L}$ung
$\textbf{E}$nhancement $\textbf{Net}$work (TRIPLE-Net) to model the 3D rib
component and provide a rib-free DCT. TRIPLE-Net takes the advantages from both
2D and 3D domains, which model the ribs in DCT with the exact FBP procedure and
3D depth information, respectively. The experiments on simulated datasets and
clinical data have shown the effectiveness of TRIPLE-Net to preserve lung
details as well as improve the imaging quality of pulmonary diseases. Finally,
an expert user study confirms our findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Dual-Student with Differentiable Spatial Warping for Semi-Supervised Semantic Segmentation. (arXiv:2203.02792v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02792">
<div class="article-summary-box-inner">
<span><p>A common challenge posed to robust semantic segmentation is the expensive
data annotation cost. Existing semi-supervised solutions show great potential
toward solving this problem. Their key idea is constructing consistency
regularization with unsupervised data augmentation from unlabeled data for
model training. The perturbations for unlabeled data enable the consistency
training loss, which benefits semi-supervised semantic segmentation. However,
these perturbations destroy image context and introduce unnatural boundaries,
which is harmful for semantic segmentation. Besides, the widely adopted
semi-supervised learning framework, i.e. mean-teacher, suffers performance
limitation since the student model finally converges to the teacher model. In
this paper, first of all, we propose a context friendly differentiable
geometric warping to conduct unsupervised data augmentation; secondly, a novel
adversarial dual-student framework is proposed to improve the Mean-Teacher from
the following two aspects: (1) dual student models are learnt independently
except for a stabilization constraint to encourage exploiting model
diversities; (2) adversarial training scheme is applied to both students and
the discriminators are resorted to distinguish reliable pseudo-label of
unlabeled data for self-training. Effectiveness is validated via extensive
experiments on PASCAL VOC2012 and Citescapes. Our solution significantly
improves the performance and state-of-the-art results are achieved on both
datasets. Remarkably, compared with fully supervision, our solution achieves
comparable mIoU of 73.4% using only 12.5% annotated data on PASCAL VOC2012.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning Applications in Diagnosis, Treatment and Prognosis of Lung Cancer. (arXiv:2203.02794v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02794">
<div class="article-summary-box-inner">
<span><p>The recent development of imaging and sequencing technologies enables
systematic advances in the clinical study of lung cancer. Meanwhile, the human
mind is limited in effectively handling and fully utilizing the accumulation of
such enormous amounts of data. Machine learning-based approaches play a
critical role in integrating and analyzing these large and complex datasets,
which have extensively characterized lung cancer through the use of different
perspectives from these accrued data. In this article, we provide an overview
of machine learning-based approaches that strengthen the varying aspects of
lung cancer diagnosis and therapy, including early detection, auxiliary
diagnosis, prognosis prediction and immunotherapy practice. Moreover, we
highlight the challenges and opportunities for future applications of machine
learning in lung cancer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Dirichlet Process Gaussian Mixtures for Segmentation on Noisy Hyperspectral Images. (arXiv:2203.02820v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02820">
<div class="article-summary-box-inner">
<span><p>Image segmentation is a fundamental step for the interpretation of Remote
Sensing Images. Clustering or segmentation methods usually precede the
classification task and are used as support tools for manual labeling. The most
common algorithms, such as k-means, mean-shift, and MRS, require an extra
manual step to find the scale parameter. The segmentation results are severely
affected if the parameters are not correctly tuned and diverge from the optimal
values. Additionally, the search for the optimal scale is a costly task, as it
requires a comprehensive hyper-parameter search. This paper proposes and
evaluates a method for segmentation of Hyperspectral Images using the Dirichlet
Process Gaussian Mixture Model. Our model can self-regulate the parameters
until it finds the optimal values of scale and the number of clusters in a
given dataset. The results demonstrate the potential of our method to find
objects in a Hyperspectral Image while bypassing the burden of manual search of
the optimal parameters. In addition, our model also produces similar results on
noisy datasets, while previous research usually required a pre-processing task
for noise reduction and spectral smoothing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region Proposal Rectification Towards Robust Instance Segmentation of Biological Images. (arXiv:2203.02846v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02846">
<div class="article-summary-box-inner">
<span><p>Top-down instance segmentation framework has shown its superiority in object
detection compared to the bottom-up framework. While it is efficient in
addressing over-segmentation, top-down instance segmentation suffers from
over-crop problem. However, a complete segmentation mask is crucial for
biological image analysis as it delivers important morphological properties
such as shapes and volumes. In this paper, we propose a region proposal
rectification (RPR) module to address this challenging incomplete segmentation
problem. In particular, we offer a progressive ROIAlign module to introduce
neighbor information into a series of ROIs gradually. The ROI features are fed
into an attentive feed-forward network (FFN) for proposal box regression. With
additional neighbor information, the proposed RPR module shows significant
improvement in correction of region proposal locations and thereby exhibits
favorable instance segmentation performances on three biological image datasets
compared to state-of-the-art baseline methods. Experimental results demonstrate
that the proposed RPR module is effective in both anchor-based and anchor-free
top-down instance segmentation approaches, suggesting the proposed method can
be applied to general top-down instance segmentation of biological images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Self-Supervised Category-Level Object Pose and Size Estimation. (arXiv:2203.02884v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02884">
<div class="article-summary-box-inner">
<span><p>This work presents a self-supervised framework for category-level object pose
and size estimation from a single depth image. Unlike previous works that rely
on time-consuming and labor-intensive ground truth pose labels for supervision,
we leverage the geometric consistency residing in point clouds of the same
shape for self-supervision. Specifically, given a normalized category template
mesh in the object-coordinate system and the partially observed object instance
in the scene, our key idea is to apply differentiable shape deformation,
registration, and rendering to enforce geometric consistency between the
predicted and the observed scene object point cloud. We evaluate our approach
on real-world datasets and find that our approach outperforms the simple
traditional baseline by large margins while being competitive with some
fully-supervised approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-class Token Transformer for Weakly Supervised Semantic Segmentation. (arXiv:2203.02891v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02891">
<div class="article-summary-box-inner">
<span><p>This paper proposes a new transformer-based framework to learn class-specific
object localization maps as pseudo labels for weakly supervised semantic
segmentation (WSSS). Inspired by the fact that the attended regions of the
one-class token in the standard vision transformer can be leveraged to form a
class-agnostic localization map, we investigate if the transformer model can
also effectively capture class-specific attention for more discriminative
object localization by learning multiple class tokens within the transformer.
To this end, we propose a Multi-class Token Transformer, termed as MCTformer,
which uses multiple class tokens to learn interactions between the class tokens
and the patch tokens. The proposed MCTformer can successfully produce
class-discriminative object localization maps from class-to-patch attentions
corresponding to different class tokens. We also propose to use a patch-level
pairwise affinity, which is extracted from the patch-to-patch transformer
attention, to further refine the localization maps. Moreover, the proposed
framework is shown to fully complement the Class Activation Mapping (CAM)
method, leading to remarkably superior WSSS results on the PASCAL VOC and MS
COCO datasets. These results underline the importance of the class token for
WSSS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Framework of Chromosome Straightening with ViT-Patch GAN. (arXiv:2203.02901v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02901">
<div class="article-summary-box-inner">
<span><p>Chromosomes exhibit non-rigid and non-articulated nature with varying degrees
of curvature. Chromosome straightening is an essential step for subsequent
karyotype construction, pathological diagnosis and cytogenetic map development.
However, robust chromosome straightening remains challenging, due to the
unavailability of training images, distorted chromosome details and shapes
after straightening, as well as poor generalization capability. We propose a
novel architecture, ViT-Patch GAN, consisting of a motion transformation
generator and a Vision Transformer-based patch (ViT-Patch) discriminator. The
generator learns the motion representation of chromosomes for straightening.
With the help of the ViT-Patch discriminator, the straightened chromosomes
retain more shape and banding pattern details. The proposed framework is
trained on a small dataset and is able to straighten chromosome images with
state-of-the-art performance for two large datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Image-specific Prototype Exploration for Weakly Supervised Semantic Segmentation. (arXiv:2203.02909v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02909">
<div class="article-summary-box-inner">
<span><p>Weakly Supervised Semantic Segmentation (WSSS) based on image-level labels
has attracted much attention due to low annotation costs. Existing methods
often rely on Class Activation Mapping (CAM) that measures the correlation
between image pixels and classifier weight. However, the classifier focuses
only on the discriminative regions while ignoring other useful information in
each image, resulting in incomplete localization maps. To address this issue,
we propose a Self-supervised Image-specific Prototype Exploration (SIPE) that
consists of an Image-specific Prototype Exploration (IPE) and a
General-Specific Consistency (GSC) loss. Specifically, IPE tailors prototypes
for every image to capture complete regions, formed our Image-Specific CAM
(IS-CAM), which is realized by two sequential steps. In addition, GSC is
proposed to construct the consistency of general CAM and our specific IS-CAM,
which further optimizes the feature representation and empowers a
self-correction ability of prototype exploration. Extensive experiments are
conducted on PASCAL VOC 2012 and MS COCO 2014 segmentation benchmark and
results show our SIPE achieves new state-of-the-art performance using only
image-level labels. The code is available at
https://github.com/chenqi1126/SIPE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Dual-task Correlation for Pose Guided Person Image Generation. (arXiv:2203.02910v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02910">
<div class="article-summary-box-inner">
<span><p>Pose Guided Person Image Generation (PGPIG) is the task of transforming a
person image from the source pose to a given target pose. Most of the existing
methods only focus on the ill-posed source-to-target task and fail to capture
reasonable texture mapping. To address this problem, we propose a novel
Dual-task Pose Transformer Network (DPTN), which introduces an auxiliary task
(i.e., source-to-source task) and exploits the dual-task correlation to promote
the performance of PGPIG. The DPTN is of a Siamese structure, containing a
source-to-source self-reconstruction branch, and a transformation branch for
source-to-target generation. By sharing partial weights between them, the
knowledge learned by the source-to-source task can effectively assist the
source-to-target learning. Furthermore, we bridge the two branches with a
proposed Pose Transformer Module (PTM) to adaptively explore the correlation
between features from dual tasks. Such correlation can establish the
fine-grained mapping of all the pixels between the sources and the targets, and
promote the source texture transmission to enhance the details of the generated
target images. Extensive experiments show that our DPTN outperforms
state-of-the-arts in terms of both PSNR and LPIPS. In addition, our DPTN only
contains 9.79 million parameters, which is significantly smaller than other
approaches. Our code is available at:
https://github.com/PangzeCheung/Dual-task-Pose-Transformer-Network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PanFormer: a Transformer Based Model for Pan-sharpening. (arXiv:2203.02916v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02916">
<div class="article-summary-box-inner">
<span><p>Pan-sharpening aims at producing a high-resolution (HR) multi-spectral (MS)
image from a low-resolution (LR) multi-spectral (MS) image and its
corresponding panchromatic (PAN) image acquired by a same satellite. Inspired
by a new fashion in recent deep learning community, we propose a novel
Transformer based model for pan-sharpening. We explore the potential of
Transformer in image feature extraction and fusion. Following the successful
development of vision transformers, we design a two-stream network with the
self-attention to extract the modality-specific features from the PAN and MS
modalities and apply a cross-attention module to merge the spectral and spatial
features. The pan-sharpened image is produced from the enhanced fused features.
Extensive experiments on GaoFen-2 and WorldView-3 images demonstrate that our
Transformer based model achieves impressive results and outperforms many
existing CNN based methods, which shows the great potential of introducing
Transformer to the pan-sharpening task. Codes are available at
https://github.com/zhysora/PanFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation. (arXiv:2203.02925v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02925">
<div class="article-summary-box-inner">
<span><p>Weakly supervised temporal action localization aims to localize temporal
boundaries of actions and simultaneously identify their categories with only
video-level category labels. Many existing methods seek to generate pseudo
labels for bridging the discrepancy between classification and localization,
but usually only make use of limited contextual information for pseudo label
generation. To alleviate this problem, we propose a representative snippet
summarization and propagation framework. Our method seeks to mine the
representative snippets in each video for propagating information between video
snippets to generate better pseudo labels. For each video, its own
representative snippets and the representative snippets from a memory bank are
propagated to update the input features in an intra- and inter-video manner.
The pseudo labels are generated from the temporal class activation maps of the
updated features to rectify the predictions of the main branch. Our method
obtains superior performance in comparison to the existing methods on two
benchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in
terms of average mAP on THUMOS14.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks. (arXiv:2203.02928v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02928">
<div class="article-summary-box-inner">
<span><p>The challenge of interpreting predictions from deep neural networks has
prompted the development of numerous interpretability methods. Many of
interpretability methods attempt to quantify the importance of input features
with respect to the class probabilities, and are called importance estimators
or saliency maps. A popular approach to evaluate such interpretability methods
is to perturb input features deemed important for predictions and observe the
decrease in accuracy. However, perturbation-based evaluation methods may
confound the sources of accuracy degradation. We conduct computational
experiments that allow to empirically estimate the $\textit{fidelity}$ of
interpretability methods and the contribution of perturbation artifacts. All
considered importance estimators clearly outperform a random baseline, which
contradicts the findings of ROAR [<a href="/abs/1806.10758">arXiv:1806.10758</a>]. We further compare our
results to the crop-and-resize evaluation framework [<a href="/abs/1705.07857">arXiv:1705.07857</a>], which
are largely in agreement. Our study suggests that we can estimate the impact of
artifacts and thus empirically evaluate interpretability methods without
retraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Parasitic Eggs from Microscopy Images and the emergence of a new dataset. (arXiv:2203.02940v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02940">
<div class="article-summary-box-inner">
<span><p>Automatic detection of parasitic eggs in microscopy images has the potential
to increase the efficiency of human experts whilst also providing an objective
assessment. The time saved by such a process would both help ensure a prompt
treatment to patients, and off-load excessive work from experts' shoulders.
Advances in deep learning inspired us to exploit successful architectures for
detection, adapting them to tackle a different domain. We propose a framework
that exploits two such state-of-the-art models. Specifically, we demonstrate
results produced by both a Generative Adversarial Network (GAN) and
Faster-RCNN, for image enhancement and object detection respectively, on
microscopy images of varying quality. The use of these techniques yields
encouraging results, though further improvements are still needed for certain
egg types whose detection still proves challenging. As a result, a new dataset
has been created and made publicly available, providing an even wider range of
classes and variability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Steering Multi-Annotations per Sample for Multi-Task Learning. (arXiv:2203.02946v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02946">
<div class="article-summary-box-inner">
<span><p>The study of multi-task learning has drawn great attention from the
community. Despite the remarkable progress, the challenge of optimally learning
different tasks simultaneously remains to be explored. Previous works attempt
to modify the gradients from different tasks. Yet these methods give a
subjective assumption of the relationship between tasks, and the modified
gradient may be less accurate. In this paper, we introduce Stochastic Task
Allocation~(STA), a mechanism that addresses this issue by a task allocation
approach, in which each sample is randomly allocated a subset of tasks. For
further progress, we propose Interleaved Stochastic Task Allocation~(ISTA) to
iteratively allocate all tasks to each example during several consecutive
iterations. We evaluate STA and ISTA on various datasets and applications:
NYUv2, Cityscapes, and COCO for scene understanding and instance segmentation.
Our experiments show both STA and ISTA outperform current state-of-the-art
methods. The code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Precise Point Spread Function Estimation. (arXiv:2203.02953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02953">
<div class="article-summary-box-inner">
<span><p>Point spread function (PSF) plays a crucial role in many fields, such as
shape from focus/defocus, depth estimation, and imaging process in fluorescence
microscopy. However, the mathematical model of the defocus process is still
unclear because several variables in the point spread function are hard to
measure accurately, such as the f-number of cameras, the physical size of a
pixel, the focus depth, etc. In this work, we develop a precise mathematical
model of the camera's point spread function to describe the defocus process. We
first derive the mathematical algorithm for the PSF and extract two parameters
A and e. A is the composite of camera's f-number, pixel-size, output scale, and
scaling factor of the circle of confusion; e is the deviation of the focus
depth. We design a novel metric based on the defocus histogram to evaluate the
difference between the simulated focused image and the actual focused image to
obtain optimal A and e. We also construct a hardware system consisting of a
focusing system and a structured light system to acquire the all-in-focus
image, the focused image with corresponding focus depth, and the depth map in
the same view. The three types of images, as a dataset, are used to obtain the
precise PSF. Our experiments on standard planes and actual objects show that
the proposed algorithm can accurately describe the defocus process. The
accuracy of our algorithm is further proved by evaluating the difference among
the actual focused images, the focused image generated by our algorithm, the
focused image generated by others. The results show that the loss of our
algorithm is 40% less than others on average. The dataset, code, and model are
available on GitHub: https://github.com/cubhe/
precise-point-spread-function-estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Perspective on Robotic Telepresence and Teleoperation using Cognition: Are we there yet?. (arXiv:2203.02959v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02959">
<div class="article-summary-box-inner">
<span><p>Telepresence and teleoperation robotics have attracted a great amount of
attention in the last 10 years. With the Artificial Intelligence (AI)
revolution already being started, we can see a wide range of robotic
applications being realized. Intelligent robotic systems are being deployed
both in industrial and domestic environments. Telepresence is the idea of being
present in a remote location virtually or via robotic avatars. Similarly, the
idea of operating a robot from a remote location for various tasks is called
teleoperation. These technologies find significant application in health care,
education, surveillance, disaster recovery, and corporate/government sectors.
But question still remains about their maturity, security and safety levels. We
also need to think about enhancing the user experience and trust in such
technologies going into the next generation of computing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Optical-Flow-Guided Motion and Detection-Based Appearance for Temporal Sentence Grounding. (arXiv:2203.02966v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02966">
<div class="article-summary-box-inner">
<span><p>Temporal sentence grounding aims to localize a target segment in an untrimmed
video semantically according to a given sentence query. Most previous works
focus on learning frame-level features of each whole frame in the entire video,
and directly match them with the textual information. Such frame-level feature
extraction leads to the obstacles of these methods in distinguishing ambiguous
video frames with complicated contents and subtle appearance differences, thus
limiting their performance. In order to differentiate fine-grained appearance
similarities among consecutive frames, some state-of-the-art methods
additionally employ a detection model like Faster R-CNN to obtain detailed
object-level features in each frame for filtering out the redundant background
contents. However, these methods suffer from missing motion analysis since the
object detection module in Faster R-CNN lacks temporal modeling. To alleviate
the above limitations, in this paper, we propose a novel Motion- and
Appearance-guided 3D Semantic Reasoning Network (MA3SRN), which incorporates
optical-flow-guided motion-aware, detection-based appearance-aware, and
3D-aware object-level features to better reason the spatial-temporal object
relations for accurately modelling the activity among consecutive frames.
Specifically, we first develop three individual branches for motion,
appearance, and 3D encoding separately to learn fine-grained motion-guided,
appearance-guided, and 3D-aware object features, respectively. Then, both
motion and appearance information from corresponding branches are associated to
enhance the 3D-aware features for the final precise grounding. Extensive
experiments on three challenging datasets (ActivityNet Caption, Charades-STA
and TACoS) demonstrate that the proposed MA3SRN model achieves a new
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Key-value Memory Enhanced Multi-step Graph Reasoning for Knowledge-based Visual Question Answering. (arXiv:2203.02985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02985">
<div class="article-summary-box-inner">
<span><p>Knowledge-based visual question answering (VQA) is a vision-language task
that requires an agent to correctly answer image-related questions using
knowledge that is not presented in the given image. It is not only a more
challenging task than regular VQA but also a vital step towards building a
general VQA system. Most existing knowledge-based VQA systems process knowledge
and image information similarly and ignore the fact that the knowledge base
(KB) contains complete information about a triplet, while the extracted image
information might be incomplete as the relations between two objects are
missing or wrongly detected. In this paper, we propose a novel model named
dynamic knowledge memory enhanced multi-step graph reasoning (DMMGR), which
performs explicit and implicit reasoning over a key-value knowledge memory
module and a spatial-aware image graph, respectively. Specifically, the memory
module learns a dynamic knowledge representation and generates a
knowledge-aware question representation at each reasoning step. Then, this
representation is used to guide a graph attention operator over the
spatial-aware image graph. Our model achieves new state-of-the-art accuracy on
the KRVQR and FVQA datasets. We also conduct ablation experiments to prove the
effectiveness of each component of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Coreference Relations in Visual Dialog. (arXiv:2203.02986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02986">
<div class="article-summary-box-inner">
<span><p>Visual dialog is a vision-language task where an agent needs to answer a
series of questions grounded in an image based on the understanding of the
dialog history and the image. The occurrences of coreference relations in the
dialog makes it a more challenging task than visual question-answering. Most
previous works have focused on learning better multi-modal representations or
on exploring different ways of fusing visual and language features, while the
coreferences in the dialog are mainly ignored. In this paper, based on
linguistic knowledge and discourse features of human dialog we propose two soft
constraints that can improve the model's ability of resolving coreferences in
dialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset
shows that our model, which integrates two novel and linguistically inspired
soft constraints in a deep transformer neural architecture, obtains new
state-of-the-art performance in terms of recall at 1 and other evaluation
metrics compared to current existing models and this without pretraining on
other vision-language datasets. Our qualitative results also demonstrate the
effectiveness of the method that we propose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Aware Latent Space Exploration for Face Image Restoration. (arXiv:2203.03005v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03005">
<div class="article-summary-box-inner">
<span><p>For image restoration, most existing deep learning based methods tend to
overfit the training data leading to bad results when encountering unseen
degradations out of the assumptions for training. To improve the robustness,
generative adversarial network (GAN) prior based methods have been proposed,
revealing a promising capability to restore photo-realistic and high-quality
results. But these methods suffer from semantic confusion, especially on
semantically significant images such as face images. In this paper, we propose
a semantic-aware latent space exploration method for image restoration (SAIR).
By explicitly modeling referenced semantics information, SAIR can consistently
restore severely degraded images not only to high-resolution highly-realistic
looks but also to correct semantics. Quantitative and qualitative experiments
collectively demonstrate the effectiveness of the proposed SAIR. Our code can
be found in https://github.com/Liamkuo/SAIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos. (arXiv:2203.03014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03014">
<div class="article-summary-box-inner">
<span><p>With the assumption that a video dataset is multimodality annotated in which
auditory and visual modalities both are labeled or class-relevant, current
multimodal methods apply modality fusion or cross-modality attention. However,
effectively leveraging the audio modality in vision-specific annotated videos
for action recognition is of particular challenge. To tackle this challenge, we
propose a novel audio-visual framework that effectively leverages the audio
modality in any solely vision-specific annotated dataset. We adopt the language
models (e.g., BERT) to build a semantic audio-video label dictionary (SAVLD)
that maps each video label to its most K-relevant audio labels in which SAVLD
serves as a bridge between audio and video datasets. Then, SAVLD along with a
pretrained audio multi-label model are used to estimate the audio-visual
modality relevance during the training phase. Accordingly, a novel learnable
irrelevant modality dropout (IMD) is proposed to completely drop out the
irrelevant audio modality and fuse only the relevant modalities. Moreover, we
present a new two-stream video Transformer for efficiently modeling the visual
modalities. Results on several vision-specific annotated datasets including
Kinetics400 and UCF-101 validated our framework as it outperforms most relevant
action recognition methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highly Accurate Dichotomous Image Segmentation. (arXiv:2203.03041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03041">
<div class="article-summary-box-inner">
<span><p>We present a systematic study on a new task called dichotomous image
segmentation (DIS), which aims to segment highly accurate objects from natural
images. To this end, we collected the first large-scale dataset, called DIS5K,
which contains 5,470 high-resolution (e.g., 2K, 4K or larger) images covering
camouflaged, salient, or meticulous objects in various backgrounds. All images
are annotated with extremely fine-grained labels. In addition, we introduce a
simple intermediate supervision baseline (IS-Net) using both feature-level and
mask-level guidance for DIS model training. Without tricks, IS-Net outperforms
various cutting-edge baselines on the proposed DIS5K, making it a general
self-learned supervision network that can help facilitate future research in
DIS. Further, we design a new metric called human correction efforts (HCE)
which approximates the number of mouse clicking operations required to correct
the false positives and false negatives. HCE is utilized to measure the gap
between models and real-world applications and thus can complement existing
metrics. Finally, we conduct the largest-scale benchmark, evaluating 16
representative segmentation models, providing a more insightful discussion
regarding object complexities, and showing several potential applications
(e.g., background removal, art design, 3D reconstruction). Hoping these efforts
can open up promising directions for both academic and industries. We will
release our DIS5Kdataset, IS-Net baseline, HCE metric, and the complete
benchmark results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social-Implicit: Rethinking Trajectory Prediction Evaluation and The Effectiveness of Implicit Maximum Likelihood Estimation. (arXiv:2203.03057v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03057">
<div class="article-summary-box-inner">
<span><p>Best-of-N (BoN) Average Displacement Error (ADE)/ Final Displacement Error
(FDE) is the most used metric for evaluating trajectory prediction models. Yet,
the BoN does not quantify the whole generated samples, resulting in an
incomplete view of the model's prediction quality and performance. We propose a
new metric, Average Mahalanobis Distance (AMD) to tackle this issue. AMD is a
metric that quantifies how close the whole generated samples are to the ground
truth. We also introduce the Average Maximum Eigenvalue (AMV) metric that
quantifies the overall spread of the predictions. Our metrics are validated
empirically by showing that the ADE/FDE is not sensitive to distribution
shifts, giving a biased sense of accuracy, unlike the AMD/AMV metrics. We
introduce the usage of Implicit Maximum Likelihood Estimation (IMLE) as a
replacement for traditional generative models to train our model,
Social-Implicit. IMLE training mechanism aligns with AMD/AMV objective of
predicting trajectories that are close to the ground truth with a tight spread.
Social-Implicit is a memory efficient deep model with only 5.8K parameters that
runs in real time of about 580Hz and achieves competitive results. Interactive
demo of the problem can be seen here
\url{https://www.abduallahmohamed.com/social-implicit-amdamv-adefde-demo}. Code
is available at \url{https://github.com/abduallahmohamed/Social-Implicit}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual vs. Reality: External Validation of COVID-19 Classifiers using XCAT Phantoms for Chest Computed Tomography. (arXiv:2203.03074v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03074">
<div class="article-summary-box-inner">
<span><p>Research studies of artificial intelligence models in medical imaging have
been hampered by poor generalization. This problem has been especially
concerning over the last year with numerous applications of deep learning for
COVID-19 diagnosis. Virtual imaging trials (VITs) could provide a solution for
objective evaluation of these models. In this work utilizing the VITs, we
created the CVIT-COVID dataset including 180 virtually imaged computed
tomography (CT) images from simulated COVID-19 and normal phantom models under
different COVID-19 morphology and imaging properties. We evaluated the
performance of an open-source, deep-learning model from the University of
Waterloo trained with multi-institutional data and an in-house model trained
with the open clinical dataset called MosMed. We further validated the model's
performance against open clinical data of 305 CT images to understand virtual
vs. real clinical data performance. The open-source model was published with
nearly perfect performance on the original Waterloo dataset but showed a
consistent performance drop in external testing on another clinical dataset
(AUC=0.77) and our simulated CVIT-COVID dataset (AUC=0.55). The in-house model
achieved an AUC of 0.87 while testing on the internal test set (MosMed test
set). However, performance dropped to an AUC of 0.65 and 0.69 when evaluated on
clinical and our simulated CVIT-COVID dataset. The VIT framework offered
control over imaging conditions, allowing us to show there was no change in
performance as CT exposure was changed from 28.5 to 57 mAs. The VIT framework
also provided voxel-level ground truth, revealing that performance of in-house
model was much higher at AUC=0.87 for diffuse COVID-19 infection size &gt;2.65%
lung volume versus AUC=0.52 for focal disease with &lt;2.65% volume. The virtual
imaging framework enabled these uniquely rigorous analyses of model
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GlideNet: Global, Local and Intrinsic based Dense Embedding NETwork for Multi-category Attributes Prediction. (arXiv:2203.03079v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03079">
<div class="article-summary-box-inner">
<span><p>Attaching attributes (such as color, shape, state, action) to object
categories is an important computer vision problem. Attribute prediction has
seen exciting recent progress and is often formulated as a multi-label
classification problem. Yet significant challenges remain in: 1) predicting
diverse attributes over multiple categories, 2) modeling attributes-category
dependency, 3) capturing both global and local scene context, and 4) predicting
attributes of objects with low pixel-count. To address these issues, we propose
a novel multi-category attribute prediction deep architecture named GlideNet,
which contains three distinct feature extractors. A global feature extractor
recognizes what objects are present in a scene, whereas a local one focuses on
the area surrounding the object of interest. Meanwhile, an intrinsic feature
extractor uses an extension of standard convolution dubbed Informed Convolution
to retrieve features of objects with low pixel-count. GlideNet uses gating
mechanisms with binary masks and its self-learned category embedding to combine
the dense embeddings. Collectively, the Global-Local-Intrinsic blocks
comprehend the scene's global context while attending to the characteristics of
the local object of interest. Finally, using the combined features, an
interpreter predicts the attributes, and the length of the output is determined
by the category, thereby removing unnecessary attributes. GlideNet can achieve
compelling results on two recent and challenging datasets -- VAW and CAR -- for
large-scale attribute prediction. For instance, it obtains more than 5\% gain
over state of the art in the mean recall (mR) metric. GlideNet's advantages are
especially apparent when predicting attributes of objects with low pixel counts
as well as attributes that demand global context understanding. Finally, we
show that GlideNet excels in training starved real-world scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HAR-GCNN: Deep Graph CNNs for Human Activity Recognition From Highly Unlabeled Mobile Sensor Data. (arXiv:2203.03087v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03087">
<div class="article-summary-box-inner">
<span><p>The problem of human activity recognition from mobile sensor data applies to
multiple domains, such as health monitoring, personal fitness, daily life
logging, and senior care. A critical challenge for training human activity
recognition models is data quality. Acquiring balanced datasets containing
accurate activity labels requires humans to correctly annotate and potentially
interfere with the subjects' normal activities in real-time. Despite the
likelihood of incorrect annotation or lack thereof, there is often an inherent
chronology to human behavior. For example, we take a shower after we exercise.
This implicit chronology can be used to learn unknown labels and classify
future activities. In this work, we propose HAR-GCCN, a deep graph CNN model
that leverages the correlation between chronologically adjacent sensor
measurements to predict the correct labels for unclassified activities that
have at least one activity label. We propose a new training strategy enforcing
that the model predicts the missing activity labels by leveraging the known
ones. HAR-GCCN shows superior performance relative to previously used baseline
methods, improving classification accuracy by about 25% and up to 68% on
different datasets. Code is available at
\url{https://github.com/abduallahmohamed/HAR-GCNN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPPF: Towards Robust Category-Level 9D Pose Estimation in the Wild. (arXiv:2203.03089v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03089">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the problem of category-level 9D pose estimation in
the wild, given a single RGB-D frame. Using supervised data of real-world 9D
poses is tedious and erroneous, and also fails to generalize to unseen
scenarios. Besides, category-level pose estimation requires a method to be able
to generalize to unseen objects at test time, which is also challenging.
Drawing inspirations from traditional point pair features (PPFs), in this
paper, we design a novel Category-level PPF (CPPF) voting method to achieve
accurate, robust and generalizable 9D pose estimation in the wild. To obtain
robust pose estimation, we sample numerous point pairs on an object, and for
each pair our model predicts necessary SE(3)-invariant voting statistics on
object centers, orientations and scales. A novel coarse-to-fine voting
algorithm is proposed to eliminate noisy point pair samples and generate final
predictions from the population. To get rid of false positives in the
orientation voting process, an auxiliary binary disambiguating classification
task is introduced for each sampled point pair. In order to detect objects in
the wild, we carefully design our sim-to-real pipeline by training on synthetic
point clouds only, unless objects have ambiguous poses in geometry. Under this
circumstance, color information is leveraged to disambiguate these poses.
Results on standard benchmarks show that our method is on par with current
state of the arts with real-world training data. Extensive experiments further
show that our method is robust to noise and gives promising results under
extremely challenging scenarios. Our code is available on
https://github.com/qq456cvb/CPPF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Behavior Recognition Based on the Integration of Multigranular Motion Features. (arXiv:2203.03097v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03097">
<div class="article-summary-box-inner">
<span><p>The recognition of behaviors in videos usually requires a combinatorial
analysis of the spatial information about objects and their dynamic action
information in the temporal dimension. Specifically, behavior recognition may
even rely more on the modeling of temporal information containing short-range
and long-range motions; this contrasts with computer vision tasks involving
images that focus on the understanding of spatial information. However, current
solutions fail to jointly and comprehensively analyze short-range motion
between adjacent frames and long-range temporal aggregations at large scales in
videos. In this paper, we propose a novel behavior recognition method based on
the integration of multigranular (IMG) motion features. In particular, we
achieve reliable motion information modeling through the synergy of a channel
attention-based short-term motion feature enhancement module (CMEM) and a
cascaded long-term motion feature integration module (CLIM). We evaluate our
model on several action recognition benchmarks such as HMDB51,
Something-Something and UCF101. The experimental results demonstrate that our
approach outperforms the previous state-of-the-art methods, which confirms its
effectiveness and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentially Private Federated Learning with Local Regularization and Sparsification. (arXiv:2203.03106v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03106">
<div class="article-summary-box-inner">
<span><p>User-level differential privacy (DP) provides certifiable privacy guarantees
to the information that is specific to any user's data in federated learning.
Existing methods that ensure user-level DP come at the cost of severe accuracy
decrease. In this paper, we study the cause of model performance degradation in
federated learning under user-level DP guarantee. We find the key to solving
this issue is to naturally restrict the norm of local updates before executing
operations that guarantee DP. To this end, we propose two techniques, Bounded
Local Update Regularization and Local Update Sparsification, to increase model
quality without sacrificing privacy. We provide theoretical analysis on the
convergence of our framework and give rigorous privacy guarantees. Extensive
experiments show that our framework significantly improves the privacy-utility
trade-off over the state-of-the-arts for federated learning with user-level DP
guarantee.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer. (arXiv:2203.03121v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03121">
<div class="article-summary-box-inner">
<span><p>While deep face recognition (FR) systems have shown amazing performance in
identification and verification, they also arouse privacy concerns for their
excessive surveillance on users, especially for public face images widely
spread on social networks. Recently, some studies adopt adversarial examples to
protect photos from being identified by unauthorized face recognition systems.
However, existing methods of generating adversarial face images suffer from
many limitations, such as awkward visual, white-box setting, weak
transferability, making them difficult to be applied to protect face privacy in
reality. In this paper, we propose adversarial makeup transfer GAN (AMT-GAN), a
novel face protection method aiming at constructing adversarial face images
that preserve stronger black-box transferability and better visual quality
simultaneously. AMT-GAN leverages generative adversarial networks (GAN) to
synthesize adversarial face images with makeup transferred from reference
images. In particular, we introduce a new regularization module along with a
joint training strategy to reconcile the conflicts between the adversarial
noises and the cycle consistence loss in makeup transfer, achieving a desirable
balance between the attack strength and visual changes. Extensive experiments
verify that compared with state of the arts, AMT-GAN can not only preserve a
comfortable visual quality, but also achieve a higher attack success rate over
commercial FR APIs, including Face++, Aliyun, and Microsoft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning. (arXiv:2203.03137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03137">
<div class="article-summary-box-inner">
<span><p>The key challenge of zero-shot learning (ZSL) is how to infer the latent
semantic knowledge between visual and attribute features on seen classes, and
thus achieving a desirable knowledge transfer to unseen classes. Prior works
either simply align the global features of an image with its associated class
semantic vector or utilize unidirectional attention to learn the limited latent
semantic representations, which could not effectively discover the intrinsic
semantic knowledge e.g., attribute semantics) between visual and attribute
features. To solve the above dilemma, we propose a Mutually Semantic
Distillation Network (MSDN), which progressively distills the intrinsic
semantic representations between visual and attribute features for ZSL. MSDN
incorporates an attribute$\rightarrow$visual attention sub-net that learns
attribute-based visual features, and a visual$\rightarrow$attribute attention
sub-net that learns visual-based attribute features. By further introducing a
semantic distillation loss, the two mutual attention sub-nets are capable of
learning collaboratively and teaching each other throughout the training
process. The proposed MSDN yields significant improvements over the strong
baselines, leading to new state-of-the-art performances on three popular
challenging benchmarks, i.e., CUB, SUN, and AWA2. Our codes have been available
at: \url{https://github.com/shiming-chen/MSDN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end video instance segmentation via spatial-temporal graph neural networks. (arXiv:2203.03145v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03145">
<div class="article-summary-box-inner">
<span><p>Video instance segmentation is a challenging task that extends image instance
segmentation to the video domain. Existing methods either rely only on
single-frame information for the detection and segmentation subproblems or
handle tracking as a separate post-processing step, which limit their
capability to fully leverage and share useful spatial-temporal information for
all the subproblems. In this paper, we propose a novel graph-neural-network
(GNN) based method to handle the aforementioned limitation. Specifically, graph
nodes representing instance features are used for detection and segmentation
while graph edges representing instance relations are used for tracking. Both
inter and intra-frame information is effectively propagated and shared via
graph updates and all the subproblems (i.e. detection, segmentation and
tracking) are jointly optimized in an unified framework. The performance of our
method shows great improvement on the YoutubeVIS validation dataset compared to
existing methods and achieves 35.2% AP with a ResNet-50 backbone, operating at
22 FPS. Code is available at <a href="http://github.com/lucaswithai/visgraph.git">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Construction of Distribution-Free Prediction Intervals for an Image Regression Problem in Semiconductor Manufacturing. (arXiv:2203.03150v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03150">
<div class="article-summary-box-inner">
<span><p>The high-volume manufacturing of the next generation of semiconductor devices
requires advances in measurement signal analysis. Many in the semiconductor
manufacturing community have reservations about the adoption of deep learning;
they instead prefer other model-based approaches for some image regression
problems, and according to the 2021 IEEE International Roadmap for Devices and
Systems (IRDS) report on Metrology a SEMI standardization committee may endorse
this philosophy. The semiconductor manufacturing community does, however,
communicate a need for state-of-the-art statistical analyses to reduce
measurement uncertainty. Prediction intervals which characterize the
reliability of the predictive performance of regression models can impact
decisions, build trust in machine learning, and be applied to other regression
models. However, we are not aware of effective and sufficiently simple
distribution-free approaches that offer valid coverage for important classes of
image data, so we consider the distribution-free conformal prediction and
conformalized quantile regression framework.The image regression problem that
is the focus of this paper pertains to line edge roughness (LER) estimation
from noisy scanning electron microscopy images. LER affects semiconductor
device performance and reliability as well as the yield of the manufacturing
process; the 2021 IRDS emphasizes the crucial importance of LER by devoting a
white paper to it in addition to mentioning or discussing it in the reports of
multiple international focus teams. It is not immediately apparent how to
effectively use normalized conformal prediction and quantile regression for LER
estimation. The modeling techniques we apply appear to be novel for finding
distribution-free prediction intervals for image data and will be presented at
the 2022 SEMI Advanced Semiconductor Manufacturing Conference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SingleSketch2Mesh : Generating 3D Mesh model from Sketch. (arXiv:2203.03157v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03157">
<div class="article-summary-box-inner">
<span><p>Sketching is an important activity in any design process. Designers and
stakeholders share their ideas through hand-drawn sketches. These sketches are
further used to create 3D models. Current methods to generate 3D models from
sketches are either manual or tightly coupled with 3D modeling platforms.
Therefore, it requires users to have an experience of sketching on such
platform. Moreover, most of the existing approaches are based on geometric
manipulation and thus cannot be generalized. We propose a novel AI based
ensemble approach, SingleSketch2Mesh, for generating 3D models from hand-drawn
sketches. Our approach is based on Generative Networks and Encoder-Decoder
Architecture to generate 3D mesh model from a hand-drawn sketch. We evaluate
our solution with existing solutions. Our approach outperforms existing
approaches on both - quantitative and qualitative evaluation criteria.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Satellite Image-based Localization via Learned Embeddings. (arXiv:1704.01133v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1704.01133">
<div class="article-summary-box-inner">
<span><p>We propose a vision-based method that localizes a ground vehicle using
publicly available satellite imagery as the only prior knowledge of the
environment. Our approach takes as input a sequence of ground-level images
acquired by the vehicle as it navigates, and outputs an estimate of the
vehicle's pose relative to a georeferenced satellite image. We overcome the
significant viewpoint and appearance variations between the images through a
neural multi-view model that learns location-discriminative embeddings in which
ground-level images are matched with their corresponding satellite view of the
scene. We use this learned function as an observation model in a filtering
framework to maintain a distribution over the vehicle's pose. We evaluate our
method on different benchmark datasets and demonstrate its ability localize
ground-level images in environments novel relative to training, despite the
challenges of significant viewpoint and appearance variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theme-Aware Aesthetic Distribution Prediction with Full Resolution Photos. (arXiv:1908.01308v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.01308">
<div class="article-summary-box-inner">
<span><p>Aesthetic quality assessment (AQA) is a challenging task due to complex
aesthetic factors. Currently, it is common to conduct AQA using deep neural
networks that require fixed-size inputs. Existing methods mainly transform
images by resizing, cropping, and padding or employ adaptive pooling to
alternately capture the aesthetic features from fixed-size inputs. However,
these transformations potentially damage aesthetic features. To address this
issue, we propose a simple but effective method to accomplish full-resolution
image AQA by combining image padding with region of image (RoM) pooling.
Padding turns inputs into the same size. RoM pooling pools image features and
discards extra padded features to eliminate the side effects of padding. In
addition, the image aspect ratios are encoded and fused with visual features to
remedy the shape information loss of RoM pooling. Furthermore, we observe that
the same image may receive different aesthetic evaluations under different
themes, which we call theme criterion bias. Hence, a theme-aware model that
uses theme information to guide model predictions is proposed. Finally, we
design an attention-based feature fusion module to effectively utilize both the
shape and theme information. Extensive experiments prove the effectiveness of
the proposed method over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Compensation for Deep Conditional Generative Networks. (arXiv:2001.08559v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.08559">
<div class="article-summary-box-inner">
<span><p>In recent years, unsupervised/weakly-supervised conditional generative
adversarial networks (GANs) have achieved many successes on the task of
modeling and generating data. However, one of their weaknesses lies in their
poor ability to separate, or disentangle, the different factors that
characterize the representation encoded in their latent space. To address this
issue, we propose a novel structure for unsupervised conditional GANs powered
by a novel Information Compensation Connection (IC-Connection). The proposed
IC-Connection enables GANs to compensate for information loss incurred during
deconvolution operations. In addition, to quantify the degree of
disentanglement on both discrete and continuous latent variables, we design a
novel evaluation procedure. Our empirical results suggest that our method
achieves better disentanglement compared to the state-of-the-art GANs in a
conditional generation setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Dynamic Vision Sensors. (arXiv:2006.00422v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.00422">
<div class="article-summary-box-inner">
<span><p>As an alternative sensing paradigm, dynamic vision sensors (DVS) have been
recently explored to tackle scenarios where conventional sensors result in high
data rate and processing time. This paper presents a hybrid event-frame
approach for detecting and tracking objects recorded by a stationary
neuromorphic sensor, thereby exploiting the sparse DVS output in a low-power
setting for traffic monitoring. Specifically, we propose a hardware efficient
processing pipeline that optimizes memory and computational needs that enable
long-term battery powered usage for IoT applications. To exploit the background
removal property of a static DVS, we propose an event-based binary image
creation that signals presence or absence of events in a frame duration. This
reduces memory requirement and enables usage of simple algorithms like median
filtering and connected component labeling for denoise and region proposal
respectively. To overcome the fragmentation issue, a YOLO inspired neural
network based detector and classifier to merge fragmented region proposals has
been proposed. Finally, a new overlap based tracker was implemented, exploiting
overlap between detections and tracks is proposed with heuristics to overcome
occlusion. The proposed pipeline is evaluated with more than 5 hours of traffic
recording spanning three different locations on two different neuromorphic
sensors (DVS and CeleX) and demonstrate similar performance. Compared to
existing event-based feature trackers, our method provides similar accuracy
while needing approx 6 times less computes. To the best of our knowledge, this
is the first time a stationary DVS based traffic monitoring solution is
extensively compared to simultaneously recorded RGB frame-based methods while
showing tremendous promise by outperforming state-of-the-art deep learning
solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EDropout: Energy-Based Dropout and Pruning of Deep Neural Networks. (arXiv:2006.04270v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04270">
<div class="article-summary-box-inner">
<span><p>Dropout is a well-known regularization method by sampling a sub-network from
a larger deep neural network and training different sub-networks on different
subsets of the data. Inspired by the dropout concept, we propose EDropout as an
energy-based framework for pruning neural networks in classification tasks. In
this approach, a set of binary pruning state vectors (population) represents a
set of corresponding sub-networks from an arbitrary provided original neural
network. An energy loss function assigns a scalar energy loss value to each
pruning state. The energy-based model stochastically evolves the population to
find states with lower energy loss. The best pruning state is then selected and
applied to the original network. Similar to dropout, the kept weights are
updated using backpropagation in a probabilistic model. The energy-based model
again searches for better pruning states and the cycle continuous. Indeed, this
procedure is in fact switching between the energy model, which manages the
pruning states, and the probabilistic model, which updates the temporarily
unpruned weights, in each iteration. The population can dynamically converge to
a pruning state. This can be interpreted as dropout leading to pruning the
network. From an implementation perspective, EDropout can prune typical neural
networks without modification of the network architecture. We evaluated the
proposed method on different flavours of ResNets, AlexNet, and SqueezeNet on
the Kuzushiji, Fashion, CIFAR-10, CIFAR-100, and Flowers datasets, and compared
the pruning rate and classification performance of the models. On average the
networks trained with EDropout achieved a pruning rate of more than $50\%$ of
the trainable parameters with approximately $&lt;5\%$ and $&lt;1\%$ drop of Top-1 and
Top-5 classification accuracy, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisImages: A Fine-Grained Expert-Annotated Visualization Dataset. (arXiv:2007.04584v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.04584">
<div class="article-summary-box-inner">
<span><p>Images in visualization publications contain rich information, e.g., novel
visualization designs and implicit design patterns of visualizations. A
systematic collection of these images can contribute to the community in many
aspects, such as literature analysis and automated tasks for visualization. In
this paper, we build and make public a dataset, VisImages, which collects
12,267 images with captions from 1,397 papers in IEEE InfoVis and VAST. Built
upon a comprehensive visualization taxonomy, the dataset includes 35,096
visualizations and their bounding boxes in the images.We demonstrate the
usefulness of VisImages through three use cases: 1) investigating the use of
visualizations in the publications with VisImages Explorer, 2) training and
benchmarking models for visualization classification, and 3) localizing
visualizations in the visual analytics systems automatically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flower: A Friendly Federated Learning Research Framework. (arXiv:2007.14390v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.14390">
<div class="article-summary-box-inner">
<span><p>Federated Learning (FL) has emerged as a promising technique for edge devices
to collaboratively learn a shared prediction model, while keeping their
training data on the device, thereby decoupling the ability to do machine
learning from the need to store the data in the cloud. However, FL is difficult
to implement realistically, both in terms of scale and systems heterogeneity.
Although there are a number of research frameworks available to simulate FL
algorithms, they do not support the study of scalable FL workloads on
heterogeneous edge devices.
</p>
<p>In this paper, we present Flower -- a comprehensive FL framework that
distinguishes itself from existing platforms by offering new facilities to
execute large-scale FL experiments and consider richly heterogeneous FL device
scenarios. Our experiments show Flower can perform FL experiments up to 15M in
client size using only a pair of high-end GPUs. Researchers can then seamlessly
migrate experiments to real devices to examine other parts of the design space.
We believe Flower provides the community with a critical new tool for FL study
and development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Out of Distribution Adversarial Attack using Latent Space Poisoning. (arXiv:2012.05027v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05027">
<div class="article-summary-box-inner">
<span><p>Traditional adversarial attacks rely upon the perturbations generated by
gradients from the network which are generally safeguarded by gradient guided
search to provide an adversarial counterpart to the network. In this paper, we
propose a novel mechanism of generating adversarial examples where the actual
image is not corrupted rather its latent space representation is utilized to
tamper with the inherent structure of the image while maintaining the
perceptual quality intact and to act as legitimate data samples. As opposed to
gradient-based attacks, the latent space poisoning exploits the inclination of
classifiers to model the independent and identical distribution of the training
dataset and tricks it by producing out of distribution samples. We train a
disentangled variational autoencoder (beta-VAE) to model the data in latent
space and then we add noise perturbations using a class-conditioned
distribution function to the latent space under the constraint that it is
misclassified to the target label. Our empirical results on MNIST, SVHN, and
CelebA dataset validate that the generated adversarial examples can easily fool
robust l_0, l_2, l_inf norm classifiers designed using provably robust defense
mechanisms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry Enhancements from Visual Content: Going Beyond Ground Truth. (arXiv:2012.08248v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08248">
<div class="article-summary-box-inner">
<span><p>This work presents a new cyclic architecture that extracts high-frequency
patterns from images and re-insert them as geometric features. This procedure
allows us to enhance the resolution of low-cost depth sensors capturing fine
details on the one hand and being loyal to the scanned ground truth on the
other. We present state-of-the-art results for depth super-resolution tasks and
as well as visually attractive, enhanced generated 3D models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noisy Label Learning for Large-scale Medical Image Classification. (arXiv:2103.04053v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04053">
<div class="article-summary-box-inner">
<span><p>The classification accuracy of deep learning models depends not only on the
size of their training sets, but also on the quality of their labels. In
medical image classification, large-scale datasets are becoming abundant, but
their labels will be noisy when they are automatically extracted from radiology
reports using natural language processing tools. Given that deep learning
models can easily overfit these noisy-label samples, it is important to study
training approaches that can handle label noise. In this paper, we adapt a
state-of-the-art (SOTA) noisy-label multi-class training approach to learn a
multi-label classifier for the dataset Chest X-ray14, which is a large scale
dataset known to contain label noise in the training set. Given that this
dataset also has label noise in the testing set, we propose a new theoretically
sound method to estimate the performance of the model on a hidden clean testing
data, given the result on the noisy testing data. Using our clean data
performance estimation, we notice that the majority of label noise on Chest
X-ray14 is present in the class 'No Finding', which is intuitively correct
because this is the most likely class to contain one or more of the 14 diseases
due to labelling mistakes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Neural Networks Learn Meta-Structures from Noisy Labels in Semantic Segmentation. (arXiv:2103.11594v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11594">
<div class="article-summary-box-inner">
<span><p>How deep neural networks (DNNs) learn from noisy labels has been studied
extensively in image classification but much less in image segmentation. So
far, our understanding of the learning behavior of DNNs trained by noisy
segmentation labels remains limited. In this study, we address this deficiency
in both binary segmentation of biological microscopy images and multi-class
segmentation of natural images. We generate extremely noisy labels by randomly
sampling a small fraction (e.g., 10%) or flipping a large fraction (e.g., 90%)
of the ground truth labels. When trained with these noisy labels, DNNs provide
largely the same segmentation performance as trained by the original ground
truth. This indicates that DNNs learn structures hidden in labels rather than
pixel-level labels per se in their supervised training for semantic
segmentation. We refer to these hidden structures in labels as meta-structures.
When DNNs are trained by labels with different perturbations to the
meta-structure, we find consistent degradation in their segmentation
performance. In contrast, incorporation of meta-structure information
substantially improves performance of an unsupervised segmentation model
developed for binary semantic segmentation. We define meta-structures
mathematically as spatial density distributions and show both theoretically and
experimentally how this formulation explains key observed learning behavior of
DNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfinityGAN: Towards Infinite-Pixel Image Synthesis. (arXiv:2104.03963v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03963">
<div class="article-summary-box-inner">
<span><p>We present a novel framework, InfinityGAN, for arbitrary-sized image
generation. The task is associated with several key challenges. First, scaling
existing models to an arbitrarily large image size is resource-constrained, in
terms of both computation and availability of large-field-of-view training
data. InfinityGAN trains and infers in a seamless patch-by-patch manner with
low computational resources. Second, large images should be locally and
globally consistent, avoid repetitive patterns, and look realistic. To address
these, InfinityGAN disentangles global appearances, local structures, and
textures. With this formulation, we can generate images with spatial size and
level of details not attainable before. Experimental evaluation validates that
InfinityGAN generates images with superior realism compared to baselines and
features parallelizable inference. Finally, we show several applications
unlocked by our approach, such as spatial style fusion, multi-modal
outpainting, and image inbetweening. All applications can be operated with
arbitrary input and output sizes. Please find the full version of the paper at
https://openreview.net/forum?id=ufGMqIM0a4b .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Grounding with Transformers. (arXiv:2105.04281v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04281">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a transformer based approach for visual grounding.
Unlike previous proposal-and-rank frameworks that rely heavily on pretrained
object detectors or proposal-free frameworks that upgrade an off-the-shelf
one-stage detector by fusing textual embeddings, our approach is built on top
of a transformer encoder-decoder and is independent of any pretrained detectors
or word embedding models. Termed VGTR -- Visual Grounding with TRansformers,
our approach is designed to learn semantic-discriminative visual features under
the guidance of the textual description without harming their location ability.
This information flow enables our VGTR to have a strong capability in capturing
context-level semantics of both vision and language modalities, rendering us to
aggregate accurate visual clues implied by the description to locate the
interested object instance. Experiments show that our method outperforms
state-of-the-art proposal-free approaches by a considerable margin on five
benchmarks while maintaining fast inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation. (arXiv:2105.07059v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07059">
<div class="article-summary-box-inner">
<span><p>Contrastive learning (CL) aims to learn useful representation without relying
on expert annotations in the context of medical image segmentation. Existing
approaches mainly contrast a single positive vector (i.e., an augmentation of
the same image) against a set of negatives within the entire remainder of the
batch by simply mapping all input features into the same constant vector.
Despite the impressive empirical performance, those methods have the following
shortcomings: (1) it remains a formidable challenge to prevent the collapsing
problems to trivial solutions; and (2) we argue that not all voxels within the
same image are equally positive since there exist the dissimilar anatomical
structures with the same image. In this work, we present a novel Contrastive
Voxel-wise Representation Learning (CVRL) method to effectively learn low-level
and high-level features by capturing 3D spatial context and rich anatomical
information along both the feature and the batch dimensions. Specifically, we
first introduce a novel CL strategy to ensure feature diversity promotion among
the 3D representation dimensions. We train the framework through bi-level
contrastive optimization (i.e., low-level and high-level) on 3D images.
Experiments on two benchmark datasets and different labeled settings
demonstrate the superiority of our proposed framework. More importantly, we
also prove that our method inherits the benefit of hardness-aware property from
the standard CL approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I2C2W: Image-to-Character-to-Word Transformers for Accurate Scene Text Recognition. (arXiv:2105.08383v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08383">
<div class="article-summary-box-inner">
<span><p>Leveraging the advances of natural language processing, most recent scene
text recognizers adopt an encoder-decoder architecture where text images are
first converted to representative features and then a sequence of characters
via `sequential decoding'. However, scene text images suffer from rich noises
of different sources such as complex background and geometric distortions which
often confuse the decoder and lead to incorrect alignment of visual features at
noisy decoding time steps. This paper presents I2C2W, a novel scene text
recognition technique that is tolerant to geometric and photometric degradation
by decomposing scene text recognition into two inter-connected tasks. The first
task focuses on image-to-character (I2C) mapping which detects a set of
character candidates from images based on different alignments of visual
features in an non-sequential way. The second task tackles character-to-word
(C2W) mapping which recognizes scene text by decoding words from the detected
character candidates. The direct learning from character semantics (instead of
noisy image features) corrects falsely detected character candidates
effectively which improves the final text recognition accuracy greatly.
Extensive experiments over nine public datasets show that the proposed I2C2W
outperforms the state-of-the-art by large margins for challenging scene text
datasets with various curvature and perspective distortions. It also achieves
very competitive recognition performance over multiple normal scene text
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A structured latent space for human body motion generation. (arXiv:2106.04387v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04387">
<div class="article-summary-box-inner">
<span><p>This work investigates learning a structured latent space to represent and
generate temporally and spatially dense 4D human body motion. Once trained, the
proposed model generates a multi-frame sequence of dense 3D meshes based on a
single point in a low-dimensional latent space. Learning a generative model of
human motion with an underlying structured latent space is important for a wide
set of applications in computer vision and graphics, including virtual and
augmented reality, 3D telepresence, and content generation for entertainment
applications. We learn this latent motion representation in a data-driven
framework that builds upon two existing lines of works. The first analyzes
temporally dense skeletal data to capture the global displacement, poses and
temporal evolution of the motion, while the second analyzes static densely
captured human scans in 3D to represent realistic 3D human body surfaces in a
lowdimensional space. Building upon the respective advantages of these two
concepts allows our model to simultaneously represent temporal motion
information for sequences of varying duration and detailed 3D geometry at every
time instant of the motion. We experimentally demonstrate that the resulting
latent space is structured in the sense that similar motions form clusters in
this space, and use our latent space to generate plausible interpolations
between different actions. We also illustrate the benefits of the approach for
4D human motion completion, showing promising abilities of our model to learn
spatio-temporal features of human motion
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Affiliate: Mutual Centralized Learning for Few-shot Classification. (arXiv:2106.05517v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05517">
<div class="article-summary-box-inner">
<span><p>Few-shot learning (FSL) aims to learn a classifier that can be easily adapted
to accommodate new tasks not seen during training, given only a few examples.
To handle the limited-data problem in few-shot regimes, recent methods tend to
collectively use a set of local features to densely represent an image instead
of using a mixed global feature. They generally explore a unidirectional
query-to-support paradigm in FSL, e.g., find the nearest/optimal support
feature for each query feature and aggregate these local matches for a joint
classification. In this paper, we propose a new method Mutual Centralized
Learning (MCL) to fully affiliate the two disjoint sets of dense features in a
bidirectional paradigm. We associate each local feature with a particle that
can bidirectionally random walk in a discrete feature space by the
affiliations. To estimate the class probability, we propose the features'
accessibility that measures the expected number of visits to the support
features of that class in a Markov process. We relate our method to learning a
centrality on an affiliation network and demonstrate its capability to be
plugged in existing methods by highlighting centralized local features.
Experiments show that our method achieves the state-of-the-art on both
miniImageNet and tieredImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anatomy-XNet: A Semi-Supervised Anatomy Aware Convolutional Neural Network for Thoracic Disease Classification. (arXiv:2106.05915v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05915">
<div class="article-summary-box-inner">
<span><p>Thoracic disease detection from chest radiographs using deep learning methods
has been an active area of research in the last decade. Most previous methods
attempt to focus on the diseased organs of the image by identifying spatial
regions responsible for significant contributions to the model's prediction. In
contrast, expert radiologists first locate the prominent anatomical structures
before determining if those regions are anomalous. Therefore, integrating
anatomical knowledge within deep learning models could bring substantial
improvement in automatic disease classification. Motivated by this, we propose
Anatomy-XNet, an anatomy-aware attention-based thoracic disease classification
network that prioritizes the spatial features guided by the pre-identified
anatomy regions. We adopt a semi-supervised learning method by utilizing
available small-scale organ-level annotation to localize the anatomy regions in
large-scale datasets where the organ-level annotations are absent. The proposed
Anatomy-XNet uses the pre-trained DenseNet-121 as the backbone network with two
corresponding structured modules, the Anatomy Aware Attention (AAA) and
Probabilistic Weighted Average Pooling (PWAP), in a cohesive framework for
anatomical attention learning. We experimentally show that our proposed method
sets a new state-of-the-art benchmark by achieving an AUC score of 85.66%,
91.13%, and, 84.04% on three publicly available large-scale CXR datasets--NIH,
Stanford CheXpert, and MIMIC-CXR, respectively. This not only proves the
efficacy of utilizing the anatomy segmentation knowledge to improve the
thoracic disease classification but also demonstrates the generalizability of
the proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving Deep into the Generalization of Vision Transformers under Distribution Shifts. (arXiv:2106.07617v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07617">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViTs) have achieved impressive performance on various
vision tasks, yet their generalization under distribution shifts (DS) is rarely
understood. In this work, we comprehensively study the out-of-distribution
(OOD) generalization of ViTs. For systematic investigation, we first present a
taxonomy of DS. We then perform extensive evaluations of ViT variants under
different DS and compare their generalization with Convolutional Neural Network
(CNN) models. Important observations are obtained: 1) ViTs learn weaker biases
on backgrounds and textures, while they are equipped with stronger inductive
biases towards shapes and structures, which is more consistent with human
cognitive traits. Therefore, ViTs generalize better than CNNs under DS. With
the same or less amount of parameters, ViTs are ahead of corresponding CNNs by
more than 5% in top-1 accuracy under most types of DS. 2) As the model scale
increases, ViTs strengthen these biases and thus gradually narrow the
in-distribution and OOD performance gap. To further improve the generalization
of ViTs, we design the Generalization-Enhanced ViTs (GE-ViTs) from the
perspectives of adversarial learning, information theory, and self-supervised
learning. By comprehensively investigating these GE-ViTs and comparing with
their corresponding CNN models, we observe: 1) For the enhanced model, larger
ViTs still benefit more for the OOD generalization. 2) GE-ViTs are more
sensitive to the hyper-parameters than their corresponding CNN models. We
design a smoother learning strategy to achieve a stable training process and
obtain performance improvements on OOD data by 4% from vanilla ViTs. We hope
our comprehensive study could shed light on the design of more generalizable
learning architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Transformer: A unified architecture for predicting multiple agent trajectories. (arXiv:2106.08417v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08417">
<div class="article-summary-box-inner">
<span><p>Predicting the motion of multiple agents is necessary for planning in dynamic
environments. This task is challenging for autonomous driving since agents
(e.g. vehicles and pedestrians) and their associated behaviors may be diverse
and influence one another. Most prior work have focused on predicting
independent futures for each agent based on all past motion, and planning
against these independent predictions. However, planning against independent
predictions can make it challenging to represent the future interaction
possibilities between different agents, leading to sub-optimal planning. In
this work, we formulate a model for predicting the behavior of all agents
jointly, producing consistent futures that account for interactions between
agents. Inspired by recent language modeling approaches, we use a masking
strategy as the query to our model, enabling one to invoke a single model to
predict agent behavior in many ways, such as potentially conditioned on the
goal or full future trajectory of the autonomous vehicle or the behavior of
other agents in the environment. Our model architecture employs attention to
combine features across road elements, agent interactions, and time steps. We
evaluate our approach on autonomous driving datasets for both marginal and
joint motion prediction, and achieve state of the art performance across two
popular datasets. Through combining a scene-centric approach, agent permutation
equivariant model, and a sequence masking strategy, we show that our model can
unify a variety of motion prediction tasks from joint motion predictions to
conditioned prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResViT: Residual vision transformers for multi-modal medical image synthesis. (arXiv:2106.16031v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.16031">
<div class="article-summary-box-inner">
<span><p>Generative adversarial models with convolutional neural network (CNN)
backbones have recently been established as state-of-the-art in numerous
medical image synthesis tasks. However, CNNs are designed to perform local
processing with compact filters, and this inductive bias compromises learning
of contextual features. Here, we propose a novel generative adversarial
approach for medical image synthesis, ResViT, that leverages the contextual
sensitivity of vision transformers along with the precision of convolution
operators and realism of adversarial learning.} ResViT's generator employs a
central bottleneck comprising novel aggregated residual transformer (ART)
blocks that synergistically combine residual convolutional and transformer
modules. Residual connections in ART blocks promote diversity in captured
representations, while a channel compression module distills task-relevant
information. A weight sharing strategy is introduced among ART blocks to
mitigate computational burden. A unified implementation is introduced to avoid
the need to rebuild separate synthesis models for varying source-target
modality configurations. Comprehensive demonstrations are performed for
synthesizing missing sequences in multi-contrast MRI, and CT images from MRI.
Our results indicate superiority of ResViT against competing CNN- and
transformer-based methods in terms of qualitative observations and quantitative
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A review on vision-based analysis for automatic dietary assessment. (arXiv:2108.02947v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02947">
<div class="article-summary-box-inner">
<span><p>Background: Maintaining a healthy diet is vital to avoid health-related
issues, e.g., undernutrition, obesity and many non-communicable diseases. An
indispensable part of the health diet is dietary assessment. Traditional manual
recording methods are not only burdensome but time-consuming, and contain
substantial biases and errors. Recent advances in Artificial Intelligence (AI),
especially computer vision technologies, have made it possible to develop
automatic dietary assessment solutions, which are more convenient, less
time-consuming and even more accurate to monitor daily food intake. Scope and
approach: This review presents Vision-Based Dietary Assessment (VBDA)
architectures, including multi-stage architecture and end-to-end one. The
multi-stage dietary assessment generally consists of three stages: food image
analysis, volume estimation and nutrient derivation. The prosperity of deep
learning makes VBDA gradually move to an end-to-end implementation, which
applies food images to a single network to directly estimate the nutrition. The
recently proposed end-to-end methods are also discussed. We further analyze
existing dietary assessment datasets, indicating that one large-scale benchmark
is urgently needed, and finally highlight critical challenges and future trends
for VBDA. Key findings and conclusions: After thorough exploration, we find
that multi-task end-to-end deep learning approaches are one important trend of
VBDA. Despite considerable research progress, many challenges remain for VBDA
due to the meal complexity. We also provide the latest ideas for future
development of VBDA, e.g., fine-grained food analysis and accurate volume
estimation. This review aims to encourage researchers to propose more practical
solutions for VBDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03823">
<div class="article-summary-box-inner">
<span><p>Deep learning-based computer-aided diagnosis is gradually deployed to review
and analyze medical images. However, this paradigm is restricted in real-world
clinical applications due to the poor robustness and generalization. The issue
is more sinister with a lack of training data. In this paper, we address the
challenge from the transfer learning point of view. Different from the common
setting that transferring knowledge from the natural image domain to the
medical image domain, we find the knowledge from the same domain further boosts
the model robustness and generalization. Therefore, we propose a novel
two-stage framework for robust generalized medical image segmentation. Firstly,
an unsupervised tile-wise autoencoder pretraining architecture is proposed to
learn local and global knowledge. Secondly, the downstream segmentation model
coupled with an auxiliary reconstruction network is designed. The
reconstruction branch encourages the model to capture more general semantic
features. Experiments of lung segmentation on multi chest X-ray datasets are
conducted. Comprehensive results demonstrate the superior robustness of the
proposed framework to corruption and high generalization performance on unseen
datasets, especially under the scenario of the limited training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertify: Attacks Against Neural Network Certification. (arXiv:2108.11299v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11299">
<div class="article-summary-box-inner">
<span><p>Certifiers for neural networks have made great progress towards provable
robustness guarantees against evasion attacks using adversarial examples.
However, introducing certifiers into deep learning systems also opens up new
attack vectors, which need to be considered before deployment. In this work, we
conduct the first systematic analysis of training-time attacks against
certifiers in practical application pipelines, identifying new threat vectors
that can be exploited to degrade the overall system. Using these insights, we
design two backdoor attacks against network certifiers, which can drastically
reduce certified robustness. For example, adding 1% poisoned data points during
training is sufficient to reduce certified robustness by up to 95 percentage
points, effectively rendering the certifier useless. We analyze how such novel
attacks can compromise the overall system's integrity or availability. Our
extensive experiments across multiple datasets, model architectures, and
certifiers demonstrate the wide applicability of these attacks. A first
investigation into potential defenses shows that current approaches are
insufficient to mitigate the issue, highlighting the need for new, more
specific solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FBSNet: A Fast Bilateral Symmetrical Network for Real-Time Semantic Segmentation. (arXiv:2109.00699v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00699">
<div class="article-summary-box-inner">
<span><p>Real-time semantic segmentation, which can be visually understood as the
pixel-level classification task on the input image, currently has broad
application prospects, especially in the fast-developing fields of autonomous
driving and drone navigation. However, the huge burden of calculation together
with redundant parameters are still the obstacles to its technological
development. In this paper, we propose a Fast Bilateral Symmetrical Network
(FBSNet) to alleviate the above challenges. Specifically, FBSNet employs a
symmetrical encoder-decoder structure with two branches, semantic information
branch and spatial detail branch. The Semantic Information Branch (SIB) is the
main branch with semantic architecture to acquire the contextual information of
the input image and meanwhile acquire sufficient receptive field. While the
Spatial Detail Branch (SDB) is a shallow and simple network used to establish
local dependencies of each pixel for preserving details, which is essential for
restoring the original resolution during the decoding phase. Meanwhile, a
Feature Aggregation Module (FAM) is designed to effectively combine the output
of these two branches. Experimental results of Cityscapes and CamVid show that
the proposed FBSNet can strike a good balance between accuracy and efficiency.
Specifically, it obtains 70.9\% and 68.9\% mIoU along with the inference speed
of 90 fps and 120 fps on these two test datasets, respectively, with only 0.62
million parameters on a single RTX 2080Ti GPU. The code is available at
https://github.com/IVIPLab/FBSNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications. (arXiv:2109.02740v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02740">
<div class="article-summary-box-inner">
<span><p>We address the problem of estimating the shape of a person's head, defined as
the geometry of the complete head surface, from a video taken with a single
moving camera, and determining the alignment of the fitted 3D head for all
video frames, irrespective of the person's pose. 3D head reconstructions
commonly tend to focus on perfecting the face reconstruction, leaving the scalp
to a statistical approximation. Our goal is to reconstruct the head model of
each person to enable future mixed reality applications. To do this, we recover
a dense 3D reconstruction and camera information via structure-from-motion and
multi-view stereo. These are then used in a new two-stage fitting process to
recover the 3D head shape by iteratively fitting a 3D morphable model of the
head with the dense reconstruction in canonical space and fitting it to each
person's head, using both traditional facial landmarks and scalp features
extracted from the head's segmentation mask. Our approach recovers consistent
geometry for varying head shapes, from videos taken by different people, with
different smartphones, and in a variety of environments from living rooms to
outdoor spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grouptron: Dynamic Multi-Scale Graph Convolutional Networks for Group-Aware Dense Crowd Trajectory Forecasting. (arXiv:2109.14128v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14128">
<div class="article-summary-box-inner">
<span><p>Accurate, long-term forecasting of pedestrian trajectories in highly dynamic
and interactive scenes is a long-standing challenge. Recent advances in using
data-driven approaches have achieved significant improvements in terms of
prediction accuracy. However, the lack of group-aware analysis has limited the
performance of forecasting models. This is especially nonnegligible in highly
crowded scenes, where pedestrians are moving in groups and the interactions
between groups are extremely complex and dynamic. In this paper, we present
Grouptron, a multi-scale dynamic forecasting framework that leverages
pedestrian group detection and utilizes individual-level, group-level and
scene-level information for better understanding and representation of the
scenes. Our approach employs spatio-temporal clustering algorithms to identify
pedestrian groups, creates spatio-temporal graphs at the individual, group, and
scene levels. It then uses graph neural networks to encode dynamics at
different scales and aggregate the embeddings for trajectory prediction. We
conducted extensive comparisons and ablation experiments to demonstrate the
effectiveness of our approach. Our method achieves 9.3% decrease in final
displacement error (FDE) compared with state-of-the-art methods on ETH/UCY
benchmark datasets, and 16.1% decrease in FDE in more crowded scenes where
extensive human group interactions are more frequently present.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Early-Learning Correction for Segmentation from Noisy Annotations. (arXiv:2110.03740v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03740">
<div class="article-summary-box-inner">
<span><p>Deep learning in the presence of noisy annotations has been studied
extensively in classification, but much less in segmentation tasks. In this
work, we study the learning dynamics of deep segmentation networks trained on
inaccurately-annotated data. We discover a phenomenon that has been previously
reported in the context of classification: the networks tend to first fit the
clean pixel-level labels during an "early-learning" phase, before eventually
memorizing the false annotations. However, in contrast to classification,
memorization in segmentation does not arise simultaneously for all semantic
categories. Inspired by these findings, we propose a new method for
segmentation from noisy annotations with two key elements. First, we detect the
beginning of the memorization phase separately for each category during
training. This allows us to adaptively correct the noisy annotations in order
to exploit early learning. Second, we incorporate a regularization term that
enforces consistency across scales to boost robustness against annotation
noise. Our method outperforms standard approaches on a medical-imaging
segmentation task where noises are synthesized to mimic human annotation
errors. It also provides robustness to realistic noisy annotations present in
weakly-supervised semantic segmentation, achieving state-of-the-art results on
PASCAL VOC 2012. Code is available at https://github.com/Kangningthu/ADELE
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Editing as Teleoperation: A Case Study in 6DoF Kit Assembly. (arXiv:2110.04450v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04450">
<div class="article-summary-box-inner">
<span><p>Studies in robot teleoperation have been centered around action
specifications -- from continuous joint control to discrete end-effector pose
control. However, these robot-centric interfaces often require skilled
operators with extensive robotics expertise. To make teleoperation accessible
to non-expert users, we propose the framework "Scene Editing as Teleoperation"
(SEaT), where the key idea is to transform the traditional "robot-centric"
interface into a "scene-centric" interface -- instead of controlling the robot,
users focus on specifying the task's goal by manipulating digital twins of the
real-world objects. As a result, a user can perform teleoperation without any
expert knowledge of the robot hardware. To achieve this goal, we utilize a
category-agnostic scene-completion algorithm that translates the real-world
workspace (with unknown objects) into a manipulable virtual scene
representation and an action-snapping algorithm that refines the user input
before generating the robot's action plan. To train the algorithms, we
procedurally generated a large-scale, diverse kit-assembly dataset that
contains object-kit pairs that mimic real-world object-kitting tasks. Our
experiments in simulation and on a real-world system demonstrate that our
framework improves both the efficiency and success rate for 6DoF kit-assembly
tasks. A user study demonstrates that SEaT framework participants achieve a
higher task success rate and report a lower subjective workload compared to an
alternative robot-centric interface. Video can be found at
https://www.youtube.com/watch?v=-NdR3mkPbQQ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSGB: Target-Selective Gradient Backprop for Probing CNN Visual Saliency. (arXiv:2110.05182v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05182">
<div class="article-summary-box-inner">
<span><p>The explanation for deep neural networks has drawn extensive attention in the
deep learning community over the past few years. In this work, we study the
visual saliency, a.k.a. visual explanation, to interpret convolutional neural
networks. Compared to iteration based saliency methods, single backward pass
based saliency methods benefit from faster speed, and they are widely used in
downstream visual tasks. Thus, we focus on single backward pass based methods.
However, existing methods in this category struggle to uccessfully produce
fine-grained saliency maps concentrating on specific target classes. That said,
producing faithful saliency maps satisfying both target-selectiveness and
fine-grainedness using a single backward pass is a challenging problem in the
field. To mitigate this problem, we revisit the gradient flow inside the
network, and find that the entangled semantics and original weights may disturb
the propagation of target-relevant saliency. Inspired by those observations, we
propose a novel visual saliency method, termed Target-Selective Gradient
Backprop (TSGB), which leverages rectification operations to effectively
emphasize target classes and further efficiently propagate the saliency to the
image space, thereby generating target-selective and fine-grained saliency
maps. The proposed TSGB consists of two components, namely, TSGB-Conv and
TSGB-FC, which rectify the gradients for convolutional layers and
fully-connected layers, respectively. Extensive qualitative and quantitative
experiments on the ImageNet and Pascal VOC datasets show that the proposed
method achieves more accurate and reliable results than the other competitive
methods. Code is available at https://github.com/123fxdx/CNNvisualizationTSGB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Optimal Conformal Classifiers. (arXiv:2110.09192v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09192">
<div class="article-summary-box-inner">
<span><p>Modern deep learning based classifiers show very high accuracy on test data
but this does not provide sufficient guarantees for safe deployment, especially
in high-stake AI applications such as medical diagnosis. Usually, predictions
are obtained without a reliable uncertainty estimate or a formal guarantee.
Conformal prediction (CP) addresses these issues by using the classifier's
predictions, e.g., its probability estimates, to predict confidence sets
containing the true class with a user-specified probability. However, using CP
as a separate processing step after training prevents the underlying model from
adapting to the prediction of confidence sets. Thus, this paper explores
strategies to differentiate through CP during training with the goal of
training model with the conformal wrapper end-to-end. In our approach,
conformal training (ConfTr), we specifically "simulate" conformalization on
mini-batches during training. Compared to standard training, ConfTr reduces the
average confidence set size (inefficiency) of state-of-the-art CP methods
applied after training. Moreover, it allows to "shape" the confidence sets
predicted at test time, which is difficult for standard CP. On experiments with
several datasets, we show ConfTr can influence how inefficiency is distributed
across classes, or guide the composition of confidence sets in terms of the
included classes, while retaining the guarantees offered by CP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Scoring System of HER2 in Pathological Images under the Microscope. (arXiv:2110.12900v2 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12900">
<div class="article-summary-box-inner">
<span><p>Breast cancer is the most common cancer among women worldwide. The human
epidermal growth factor receptor 2 (HER2) with immunohistochemical (IHC) is
widely used for pathological evaluation to provide the appropriate therapy for
patients with breast cancer. However, the deficiency of pathologists and
subjective and susceptible to inter-observer variation of visual diagnosis are
the main challenges. Recently, with the rapid development of artificial
intelligence (AI) in disease diagnosis, several automated HER2 scoring methods
using traditional computer vision or machine learning methods indicate the
improvement of the HER2 diagnostic accuracy, but the unreasonable
interpretation in pathology, as well as the expensive and ethical issues for
annotation, make these methods still have a long way to deploy in hospitals to
ease pathologists' burden in real. In this paper, we propose a HER2 automated
scoring system that strictly follows the HER2 scoring guidelines simulating the
real workflow of HER2 scores diagnosis by pathologists. Unlike the previous
work, our method considers the positive control of HER2 to make sure the assay
performance for each slide, eliminating work for repeated comparison between
the current field of view (FOV) and positive control FOV, especially for the
borderline cases. Besides, for each selected FOV under the microscope, our
system provides real-time HER2 scores analysis and visualizations of the
membrane staining intensity and completeness corresponding with the cell
classifications. Our rigorous workflow along with the flexible interactive
adjustion in demand substantially assists pathologists to finish the HER2
diagnosis faster and improves the robustness and accuracy. The proposed system
will be embedded in our Thorough Eye platform for deployment in hospitals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised GAN Detector. (arXiv:2111.06575v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06575">
<div class="article-summary-box-inner">
<span><p>Although the recent advancement in generative models brings diverse
advantages to society, it can also be abused with malicious purposes, such as
fraud, defamation, and fake news. To prevent such cases, vigorous research is
conducted to distinguish the generated images from the real images, but
challenges still remain to distinguish the unseen generated images outside of
the training settings. Such limitations occur due to data dependency arising
from the model's overfitting issue to the training data generated by specific
GANs. To overcome this issue, we adopt a self-supervised scheme to propose a
novel framework. Our proposed method is composed of the artificial fingerprint
generator reconstructing the high-quality artificial fingerprints of GAN images
for detailed analysis, and the GAN detector distinguishing GAN images by
learning the reconstructed artificial fingerprints. To improve the
generalization of the artificial fingerprint generator, we build multiple
autoencoders with different numbers of upconvolution layers. With numerous
ablation studies, the robust generalization of our method is validated by
outperforming the generalization of the previous state-of-the-art algorithms,
even without utilizing the GAN images of the training dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection. (arXiv:2111.09099v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09099">
<div class="article-summary-box-inner">
<span><p>Anomaly detection is commonly pursued as a one-class classification problem,
where models can only learn from normal training samples, while being evaluated
on both normal and abnormal test samples. Among the successful approaches for
anomaly detection, a distinguished category of methods relies on predicting
masked information (e.g. patches, future frames, etc.) and leveraging the
reconstruction error with respect to the masked information as an abnormality
score. Different from related methods, we propose to integrate the
reconstruction-based functionality into a novel self-supervised predictive
architectural building block. The proposed self-supervised block is generic and
can easily be incorporated into various state-of-the-art anomaly detection
methods. Our block starts with a convolutional layer with dilated filters,
where the center area of the receptive field is masked. The resulting
activation maps are passed through a channel attention module. Our block is
equipped with a loss that minimizes the reconstruction error with respect to
the masked area in the receptive field. We demonstrate the generality of our
block by integrating it into several state-of-the-art frameworks for anomaly
detection on image and video, providing empirical evidence that shows
considerable performance improvements on MVTec AD, Avenue, and ShanghaiTech. We
release our code as open source at https://github.com/ristea/sspcab.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IntraQ: Learning Synthetic Images with Intra-Class Heterogeneity for Zero-Shot Network Quantization. (arXiv:2111.09136v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09136">
<div class="article-summary-box-inner">
<span><p>Learning to synthesize data has emerged as a promising direction in zero-shot
quantization (ZSQ), which represents neural networks by low-bit integer without
accessing any of the real data. In this paper, we observe an interesting
phenomenon of intra-class heterogeneity in real data and show that existing
methods fail to retain this property in their synthetic images, which causes a
limited performance increase. To address this issue, we propose a novel
zero-shot quantization method referred to as IntraQ. First, we propose a local
object reinforcement that locates the target objects at different scales and
positions of the synthetic images. Second, we introduce a marginal distance
constraint to form class-related features distributed in a coarse area. Lastly,
we devise a soft inception loss which injects a soft prior label to prevent the
synthetic images from being overfitting to a fixed object. Our IntraQ is
demonstrated to well retain the intra-class heterogeneity in the synthetic
images and also observed to perform state-of-the-art. For example, compared to
the advanced ZSQ, our IntraQ obtains 9.17\% increase of the top-1 accuracy on
ImageNet when all layers of MobileNetV1 are quantized to 4-bit. Code is at
https://github.com/viperit/InterQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation. (arXiv:2111.10502v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10502">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the problem of jointly estimating the optical flow
and scene flow from synchronized 2D and 3D data. Previous methods either employ
a complex pipeline which splits the joint task into independent stages, or fuse
2D and 3D information in an "early-fusion" or "late-fusion" manner. Such
one-size-fits-all approaches suffer from a dilemma of failing to fully utilize
the characteristic of each modality or to maximize the inter-modality
complementarity. To address the problem, we propose a novel end-to-end
framework, called CamLiFlow. It consists of 2D and 3D branches with multiple
bidirectional connections between them in specific layers. Different from
previous work, we apply a point-based 3D branch to better extract the geometric
features and design a symmetric learnable operator to fuse dense image features
and sparse point features. We also propose a transformation for point clouds to
solve the non-linear issue of 3D-2D projection. Experiments show that CamLiFlow
achieves better performance with fewer parameters. Our method ranks 1st on the
KITTI Scene Flow benchmark, outperforming the previous art with 1/7 parameters.
Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalabilty. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for text-to-image and image-to-text
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation tasks without any finetuning or extra
object detection frameworks. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial results of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lepard: Learning partial point cloud matching in rigid and deformable scenes. (arXiv:2111.12591v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12591">
<div class="article-summary-box-inner">
<span><p>We present Lepard, a Learning based approach for partial point cloud matching
in rigid and deformable scenes. The key characteristics are the following
techniques that exploit 3D positional knowledge for point cloud matching: 1) An
architecture that disentangles point cloud representation into feature space
and 3D position space. 2) A position encoding method that explicitly reveals 3D
relative distance information through the dot product of vectors. 3) A
repositioning technique that modifies the crosspoint-cloud relative positions.
Ablation studies demonstrate the effectiveness of the above techniques. In
rigid cases, Lepard combined with RANSAC and ICP demonstrates state-of-the-art
registration recall of 93.9% / 71.3% on the 3DMatch / 3DLoMatch. In deformable
cases, Lepard achieves +27.1% / +34.8% higher non-rigid feature matching recall
than the prior art on our newly constructed 4DMatch / 4DLoMatch benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maximum Consensus by Weighted Influences of Monotone Boolean Functions. (arXiv:2112.00953v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00953">
<div class="article-summary-box-inner">
<span><p>Robust model fitting is a fundamental problem in computer vision: used to
pre-process raw data in the presence of outliers. Maximisation of Consensus
(MaxCon) is one of the most popular robust criteria and widely used. Recently
(Tennakoon et al. CVPR2021), a connection has been made between MaxCon and
estimation of influences of a Monotone Boolean function. Equipping the Boolean
cube with different measures and adopting different sampling strategies (two
sides of the same coin) can have differing effects: which leads to the current
study. This paper studies the concept of weighted influences for solving
MaxCon. In particular, we study endowing the Boolean cube with the Bernoulli
measure and performing biased (as opposed to uniform) sampling. Theoretically,
we prove the weighted influences, under this measure, of points belonging to
larger structures are smaller than those of points belonging to smaller
structures in general. We also consider another "natural" family of
sampling/weighting strategies, sampling with uniform measure concentrated on a
particular (Hamming) level of the cube.
</p>
<p>Based on weighted sampling, we modify the algorithm of Tennakoon et al., and
test on both synthetic and real datasets. This paper is not promoting a new
approach per se, but rather studying the issue of weighted sampling.
Accordingly, we are not claiming to have produced a superior algorithm: rather
we show some modest gains of Bernoulli sampling, and we illuminate some of the
interactions between structure in data and weighted sampling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equal Bits: Enforcing Equally Distributed Binary Network Weights. (arXiv:2112.03406v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03406">
<div class="article-summary-box-inner">
<span><p>Binary networks are extremely efficient as they use only two symbols to
define the network: $\{+1,-1\}$. One can make the prior distribution of these
symbols a design choice. The recent IR-Net of Qin et al. argues that imposing a
Bernoulli distribution with equal priors (equal bit ratios) over the binary
weights leads to maximum entropy and thus minimizes information loss. However,
prior work cannot precisely control the binary weight distribution during
training, and therefore cannot guarantee maximum entropy. Here, we show that
quantizing using optimal transport can guarantee any bit ratio, including equal
ratios. We investigate experimentally that equal bit ratios are indeed
preferable and show that our method leads to optimization benefits. We show
that our quantization method is effective when compared to state-of-the-art
binarization methods, even when using binary weight pruning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TCGL: Temporal Contrastive Graph for Self-supervised Video Representation Learning. (arXiv:2112.03587v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03587">
<div class="article-summary-box-inner">
<span><p>Video self-supervised learning is a challenging task, which requires
significant expressive power from the model to leverage rich spatial-temporal
knowledge and generate effective supervisory signals from large amounts of
unlabeled videos. However, existing methods fail to increase the temporal
diversity of unlabeled videos and ignore elaborately modeling multi-scale
temporal dependencies in an explicit way. To overcome these limitations, we
take advantage of the multi-scale temporal dependencies within videos and
proposes a novel video self-supervised learning framework named Temporal
Contrastive Graph Learning (TCGL), which jointly models the inter-snippet and
intra-snippet temporal dependencies for temporal representation learning with a
hybrid graph contrastive learning strategy. Specifically, a Spatial-Temporal
Knowledge Discovering (STKD) module is first introduced to extract
motion-enhanced spatial-temporal representations from videos based on the
frequency domain analysis of discrete cosine transform. To explicitly model
multi-scale temporal dependencies of unlabeled videos, our TCGL integrates the
prior knowledge about the frame and snippet orders into graph structures, i.e.,
the intra-/inter- snippet Temporal Contrastive Graphs (TCG). Then, specific
contrastive learning modules are designed to maximize the agreement between
nodes in different graph views. To generate supervisory signals for unlabeled
videos, we introduce an Adaptive Snippet Order Prediction (ASOP) module which
leverages the relational knowledge among video snippets to learn the global
context representation and recalibrate the channel-wise features adaptively.
Experimental results demonstrate the superiority of our TCGL over the
state-of-the-art methods on large-scale action recognition and video retrieval
benchmarks.The code is publicly available at
https://github.com/YangLiu9208/TCGL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Early Stopping for Deep Image Prior. (arXiv:2112.06074v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06074">
<div class="article-summary-box-inner">
<span><p>Deep image prior (DIP) and its variants have showed remarkable potential for
solving inverse problems in computer vision, without any extra training data.
Practical DIP models are often substantially overparameterized. During the
fitting process, these models learn mostly the desired visual content first,
and then pick up the potential modeling and observational noise, i.e.,
overfitting. Thus, the practicality of DIP often depends critically on good
early stopping (ES) that captures the transition period. In this regard, the
majority of DIP works for vision tasks only demonstrates the potential of the
models -- reporting the peak performance against the ground truth, but provides
no clue about how to operationally obtain near-peak performance without access
to the groundtruth. In this paper, we set to break this practicality barrier of
DIP, and propose an efficient ES strategy, which consistently detects near-peak
performance across several vision tasks and DIP variants. Based on a simple
measure of dispersion of consecutive DIP reconstructions, our ES method not
only outpaces the existing ones -- which only work in very narrow domains, but
also remains effective when combined with a number of methods that try to
mitigate the overfitting. The code is available at
https://github.com/sun-umn/Early_Stopping_for_DIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JoJoGAN: One Shot Face Stylization. (arXiv:2112.11641v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11641">
<div class="article-summary-box-inner">
<span><p>A style mapper applies some fixed style to its input images (so, for example,
taking faces to cartoons). This paper describes a simple procedure -- JoJoGAN
-- to learn a style mapper from a single example of the style. JoJoGAN uses a
GAN inversion procedure and StyleGAN's style-mixing property to produce a
substantial paired dataset from a single example style. The paired dataset is
then used to fine-tune a StyleGAN. An image can then be style mapped by
GAN-inversion followed by the fine-tuned StyleGAN. JoJoGAN needs just one
reference and as little as 30 seconds of training time. JoJoGAN can use extreme
style references (say, animal faces) successfully. Furthermore, one can control
what aspects of the style are used and how much of the style is applied.
Qualitative and quantitative evaluation show that JoJoGAN produces high quality
high resolution images that vastly outperform the current state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reflash Dropout in Image Super-Resolution. (arXiv:2112.12089v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12089">
<div class="article-summary-box-inner">
<span><p>Dropout is designed to relieve the overfitting problem in high-level vision
tasks but is rarely applied in low-level vision tasks, like image
super-resolution (SR). As a classic regression problem, SR exhibits a different
behaviour as high-level tasks and is sensitive to the dropout operation.
However, in this paper, we show that appropriate usage of dropout benefits SR
networks and improves the generalization ability. Specifically, dropout is
better embedded at the end of the network and is significantly helpful for the
multi-degradation settings. This discovery breaks our common sense and inspires
us to explore its working mechanism. We further use two analysis tools -- one
is from recent network interpretation works, and the other is specially
designed for this task. The analysis results provide side proofs to our
experimental findings and show us a new perspective to understand SR networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation and Focal Loss. (arXiv:2201.01501v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01501">
<div class="article-summary-box-inner">
<span><p>Depth estimation is solved as a regression or classification problem in
existing learning-based multi-view stereo methods. Although these two
representations have recently demonstrated their excellent performance, they
still have apparent shortcomings, e.g., regression methods tend to overfit due
to the indirect learning cost volume, and classification methods cannot
directly infer the exact depth due to its discrete prediction. In this paper,
we propose a novel representation, termed Unification, to unify the advantages
of regression and classification. It can directly constrain the cost volume
like classification methods, but also realize the sub-pixel depth prediction
like regression methods. To excavate the potential of unification, we design a
new loss function named Unified Focal Loss, which is more uniform and
reasonable to combat the challenge of sample imbalance. Combining these two
unburdened modules, we present a coarse-to-fine framework, that we call
UniMVSNet. The results of ranking first on both DTU and Tanks and Temples
benchmarks verify that our model not only performs the best but also has the
best generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-NeRF: Point-based Neural Radiance Fields. (arXiv:2201.08845v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08845">
<div class="article-summary-box-inner">
<span><p>Volumetric neural rendering methods like NeRF generate high-quality view
synthesis results but are optimized per-scene leading to prohibitive
reconstruction time. On the other hand, deep multi-view stereo methods can
quickly reconstruct scene geometry via direct network inference. Point-NeRF
combines the advantages of these two approaches by using neural 3D point
clouds, with associated neural features, to model a radiance field. Point-NeRF
can be rendered efficiently by aggregating neural point features near scene
surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can
be initialized via direct inference of a pre-trained deep network to produce a
neural point cloud; this point cloud can be finetuned to surpass the visual
quality of NeRF with 30X faster training time. Point-NeRF can be combined with
other 3D reconstruction methods and handles the errors and outliers in such
methods via a novel pruning and growing mechanism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Minimize the Remainder in Supervised Learning. (arXiv:2201.09193v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09193">
<div class="article-summary-box-inner">
<span><p>The learning process of deep learning methods usually updates the model's
parameters in multiple iterations. Each iteration can be viewed as the
first-order approximation of Taylor's series expansion. The remainder, which
consists of higher-order terms, is usually ignored in the learning process for
simplicity. This learning scheme empowers various multimedia based
applications, such as image retrieval, recommendation system, and video search.
Generally, multimedia data (e.g., images) are semantics-rich and
high-dimensional, hence the remainders of approximations are possibly non-zero.
In this work, we consider the remainder to be informative and study how it
affects the learning process. To this end, we propose a new learning approach,
namely gradient adjustment learning (GAL), to leverage the knowledge learned
from the past training iterations to adjust vanilla gradients, such that the
remainders are minimized and the approximations are improved. The proposed GAL
is model- and optimizer-agnostic, and is easy to adapt to the standard learning
framework. It is evaluated on three tasks, i.e., image classification, object
detection, and regression, with state-of-the-art models and optimizers. The
experiments show that the proposed GAL consistently enhances the evaluated
models, whereas the ablation studies validate various aspects of the proposed
GAL. The code is available at
\url{https://github.com/luoyan407/gradient_adjustment.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalised Image Outpainting with U-Transformer. (arXiv:2201.11403v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11403">
<div class="article-summary-box-inner">
<span><p>While most present image outpainting conducts horizontal extrapolation, we
study the generalised image outpainting problem that extrapolates visual
context all-side around a given image. To this end, we develop a novel
transformer-based generative adversarial network called U-Transformer able to
extend image borders with plausible structure and details even for complicated
scenery images. Specifically, we design a generator as an encoder-to-decoder
structure embedded with the popular Swin Transformer blocks. As such, our novel
framework can better cope with image long-range dependencies which are
crucially important for generalised image outpainting. We propose additionally
a U-shaped structure and multi-view Temporal Spatial Predictor network to
reinforce image self-reconstruction as well as unknown-part prediction smoothly
and realistically. We experimentally demonstrate that our proposed method could
produce visually appealing results for generalized image outpainting against
the state-of-the-art image outpainting approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-confidence Samples Matter for Domain Adaptation. (arXiv:2202.02802v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02802">
<div class="article-summary-box-inner">
<span><p>Domain adaptation (DA) aims to transfer knowledge from a label-rich source
domain to a related but label-scarce target domain. The conventional DA
strategy is to align the feature distributions of the two domains. Recently,
increasing researches have focused on self-training or other semi-supervised
algorithms to explore the data structure of the target domain. However, the
bulk of them depend largely on confident samples in order to build reliable
pseudo labels, prototypes or cluster centers. Representing the target data
structure in such a way would overlook the huge low-confidence samples,
resulting in sub-optimal transferability that is biased towards the samples
similar to the source domain. To overcome this issue, we propose a novel
contrastive learning method by processing low-confidence samples, which
encourages the model to make use of the target data structure through the
instance discrimination process. To be specific, we create positive and
negative pairs only using low-confidence samples, and then re-represent the
original features with the classifier weights rather than directly utilizing
them, which can better encode the task-specific semantic information.
Furthermore, we combine cross-domain mixup to augment the proposed contrastive
loss. Consequently, the domain gap can be well bridged through contrastive
learning of intermediate representations across domains. We evaluate the
proposed method in both unsupervised and semi-supervised DA settings, and
extensive experimental results on benchmarks reveal that our method is
effective and achieves state-of-the-art performance. The code can be found in
https://github.com/zhyx12/MixLRCo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyper-relationship Learning Network for Scene Graph Generation. (arXiv:2202.07271v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07271">
<div class="article-summary-box-inner">
<span><p>Generating informative scene graphs from images requires integrating and
reasoning from various graph components, i.e., objects and relationships.
However, current scene graph generation (SGG) methods, including the unbiased
SGG methods, still struggle to predict informative relationships due to the
lack of 1) high-level inference such as transitive inference between
relationships and 2) efficient mechanisms that can incorporate all interactions
of graph components. To address the issues mentioned above, we devise a
hyper-relationship learning network, termed HLN, for SGG. Specifically, the
proposed HLN stems from hypergraphs and two graph attention networks (GATs) are
designed to infer relationships: 1) the object-relationship GAT or OR-GAT to
explore interactions between objects and relationships, and 2) the
hyper-relationship GAT or HR-GAT to integrate transitive inference of
hyper-relationships, i.e., the sequential relationships between three objects
for transitive reasoning. As a result, HLN significantly improves the
performance of scene graph generation by integrating and reasoning from object
interactions, relationship interactions, and transitive inference of
hyper-relationships. We evaluate HLN on the most popular SGG dataset, i.e., the
Visual Genome dataset, and the experimental results demonstrate its great
superiority over recent state-of-the-art methods. For example, the proposed HLN
improves the recall per relationship from 11.3\% to 13.1\%, and maintains the
recall per image from 19.8\% to 34.9\%. We will release the source code and
pretrained models on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Emerging Research Directions in Abstract Visual Reasoning. (arXiv:2202.10284v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10284">
<div class="article-summary-box-inner">
<span><p>Abstract Visual Reasoning (AVR) problems are commonly used to approximate
human intelligence. They test the ability of applying previously gained
knowledge, experience and skills in a completely new setting, which makes them
particularly well-suited for this task. Recently, the AVR problems have become
popular as a proxy to study machine intelligence, which has led to emergence of
new distinct types of problems and multiple benchmark sets. In this work we
review this emerging AVR research and propose a taxonomy to categorise the AVR
tasks along 5 dimensions: input shapes, hidden rules, target task, cognitive
function, and main challenge. The perspective taken in this survey allows to
characterise AVR problems with respect to their shared and distinct properties,
provides a unified view on the existing approaches for solving AVR tasks, shows
how the AVR problems relate to practical applications, and outlines promising
directions for future work. One of them refers to the observation that in the
machine learning literature different tasks are considered in isolation, which
is in the stark contrast with the way the AVR tasks are used to measure human
intelligence, where multiple types of problems are combined within a single IQ
test.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CG-SSD: Corner Guided Single Stage 3D Object Detection from LiDAR Point Cloud. (arXiv:2202.11868v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11868">
<div class="article-summary-box-inner">
<span><p>At present, the anchor-based or anchor-free models that use LiDAR point
clouds for 3D object detection use the center assigner strategy to infer the 3D
bounding boxes. However, in a real world scene, the LiDAR can only acquire a
limited object surface point clouds, but the center point of the object does
not exist. Obtaining the object by aggregating the incomplete surface point
clouds will bring a loss of accuracy in direction and dimension estimation. To
address this problem, we propose a corner-guided anchor-free single-stage 3D
object detection model (CG-SSD ).Firstly, 3D sparse convolution backbone
network composed of residual layers and sub-manifold sparse convolutional
layers are used to construct bird's eye view (BEV) features for further deeper
feature mining by a lite U-shaped network; Secondly, a novel corner-guided
auxiliary module (CGAM) is proposed to incorporate corner supervision signals
into the neural network. CGAM is explicitly designed and trained to detect
partially visible and invisible corners to obtains a more accurate object
feature representation, especially for small or partial occluded objects;
Finally, the deep features from both the backbone networks and CGAM module are
concatenated and fed into the head module to predict the classification and 3D
bounding boxes of the objects in the scene. The experiments demonstrate CG-SSD
achieves the state-of-art performance on the ONCE benchmark for supervised 3D
object detection using single frame point cloud data, with 62.77%mAP.
Additionally, the experiments on ONCE and Waymo Open Dataset show that CGAM can
be extended to most anchor-based models which use the BEV feature to detect
objects, as a plug-in and bring +1.17%-+14.27%AP improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TeachAugment: Data Augmentation Optimization Using Teacher Knowledge. (arXiv:2202.12513v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12513">
<div class="article-summary-box-inner">
<span><p>Optimization of image transformation functions for the purpose of data
augmentation has been intensively studied. In particular, adversarial data
augmentation strategies, which search augmentation maximizing task loss, show
significant improvement in the model generalization for many tasks. However,
the existing methods require careful parameter tuning to avoid excessively
strong deformations that take away image features critical for acquiring
generalization. In this paper, we propose a data augmentation optimization
method based on the adversarial strategy called TeachAugment, which can produce
informative transformed images to the model without requiring careful tuning by
leveraging a teacher model. Specifically, the augmentation is searched so that
augmented images are adversarial for the target model and recognizable for the
teacher model. We also propose data augmentation using neural networks, which
simplifies the search space design and allows for updating of the data
augmentation using the gradient method. We show that TeachAugment outperforms
existing methods in experiments of image classification, semantic segmentation,
and unsupervised representation learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Name Your Style: An Arbitrary Artist-aware Image Style Transfer. (arXiv:2202.13562v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13562">
<div class="article-summary-box-inner">
<span><p>Image style transfer has attracted widespread attention in the past few
years. Despite its remarkable results, it requires additional style images
available as references, making it less flexible and inconvenient. Using text
is the most natural way to describe the style. More importantly, text can
describe implicit abstract styles, like styles of specific artists or art
movements. In this paper, we propose a text-driven image style transfer (TxST)
that leverages advanced image-text encoders to control arbitrary style
transfer. We introduce a contrastive training strategy to effectively extract
style descriptions from the image-text model (i.e., CLIP), which aligns
stylization with the text description. To this end, we also propose a novel and
efficient attention module that explores cross-attentions to fuse style and
content features. Finally, we achieve an arbitrary artist-aware image style
transfer to learn and transfer specific artistic characters such as Picasso,
oil painting, or a rough sketch. Extensive experiments demonstrate that our
approach outperforms the state-of-the-art methods on both image and textual
styles. Moreover, it can mimic the styles of one or many artists to achieve
attractive results, thus highlighting a promising direction in image style
transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial samples for deep monocular 6D object pose estimation. (arXiv:2203.00302v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00302">
<div class="article-summary-box-inner">
<span><p>Estimating 6D object pose from an RGB image is important for many real-world
applications such as autonomous driving and robotic grasping. Recent deep
learning models have achieved significant progress on this task but their
robustness received little research attention. In this work, for the first
time, we study adversarial samples that can fool deep learning models with
imperceptible perturbations to input image. In particular, we propose a Unified
6D pose estimation Attack, namely U6DA, which can successfully attack several
state-of-the-art (SOTA) deep learning models for 6D pose estimation. The key
idea of our U6DA is to fool the models to predict wrong results for object
instance localization and shape that are essential for correct 6D pose
estimation. Specifically, we explore a transfer-based black-box attack to 6D
pose estimation. We design the U6DA loss to guide the generation of adversarial
examples, the loss aims to shift the segmentation attention map away from its
original position. We show that the generated adversarial samples are not only
effective for direct 6D pose estimation models, but also are able to attack
two-stage models regardless of their robust RANSAC modules. Extensive
experiments were conducted to demonstrate the effectiveness, transferability,
and anti-defense capability of our U6DA on large-scale public benchmarks. We
also introduce a new U6DA-Linemod dataset for robustness study of the 6D pose
estimation task. Our codes and dataset will be available at
\url{https://github.com/cuge1995/U6DA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation. (arXiv:2203.01072v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01072">
<div class="article-summary-box-inner">
<span><p>This paper proposes a universal framework, called OVE6D, for model-based 6D
object pose estimation from a single depth image and a target object mask. Our
model is trained using purely synthetic data rendered from ShapeNet, and,
unlike most of the existing methods, it generalizes well on new real-world
objects without any fine-tuning. We achieve this by decomposing the 6D pose
into viewpoint, in-plane rotation around the camera optical axis and
translation, and introducing novel lightweight modules for estimating each
component in a cascaded manner. The resulting network contains less than 4M
parameters while demonstrating excellent performance on the challenging T-LESS
and Occluded LINEMOD datasets without any dataset-specific training. We show
that OVE6D outperforms some contemporary deep learning-based pose estimation
methods specifically trained for individual objects or datasets with real-world
training data.
</p>
<p>The implementation and the pre-trained model will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Human Motion Prediction: A Survey. (arXiv:2203.01593v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01593">
<div class="article-summary-box-inner">
<span><p>3D human motion prediction, predicting future poses from a given sequence, is
an issue of great significance and challenge in computer vision and machine
intelligence, which can help machines in understanding human behaviors. Due to
the increasing development and understanding of Deep Neural Networks (DNNs) and
the availability of large-scale human motion datasets, the human motion
prediction has been remarkably advanced with a surge of interest among academia
and industrial community. In this context, a comprehensive survey on 3D human
motion prediction is conducted for the purpose of retrospecting and analyzing
relevant works from existing released literature. In addition, a pertinent
taxonomy is constructed to categorize these existing approaches for 3D human
motion prediction. In this survey, relevant methods are categorized into three
categories: human pose representation, network structure design, and
\textit{prediction target}. We systematically review all relevant journal and
conference papers in the field of human motion prediction since 2015, which are
presented in detail based on proposed categorizations in this survey.
Furthermore, the outline for the public benchmark datasets, evaluation
criteria, and performance comparisons are respectively presented in this paper.
The limitations of the state-of-the-art methods are discussed as well, hoping
for paving the way for future explorations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntax-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2203.01601v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01601">
<div class="article-summary-box-inner">
<span><p>Handwritten mathematical expression recognition (HMER) is a challenging task
that has many potential applications. Recent methods for HMER have achieved
outstanding performance with an encoder-decoder architecture. However, these
methods adhere to the paradigm that the prediction is made "from one character
to another", which inevitably yields prediction errors due to the complicated
structures of mathematical expressions or crabbed handwritings. In this paper,
we propose a simple and efficient method for HMER, which is the first to
incorporate syntax information into an encoder-decoder network. Specifically,
we present a set of grammar rules for converting the LaTeX markup sequence of
each expression into a parsing tree; then, we model the markup sequence
prediction as a tree traverse process with a deep neural network. In this way,
the proposed method can effectively describe the syntax context of expressions,
avoiding the structure prediction errors of HMER. Experiments on two benchmark
datasets demonstrate that our method achieves significantly better recognition
performance than prior arts. To further validate the effectiveness of our
method, we create a large-scale dataset consisting of 100k handwritten
mathematical expression images acquired from ten thousand writers. The source
code, new dataset, and pre-trained models of this work will be publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with Fuchs dystrophy. (arXiv:2203.01882v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01882">
<div class="article-summary-box-inner">
<span><p>To estimate the corneal endothelial parameters from specular microscopy
images depicting cornea guttata (Fuchs endothelial dystrophy), we propose a new
deep learning methodology that includes a novel attention mechanism named
feedback non-local attention (fNLA). Our approach first infers the cell edges,
then selects the cells that are well detected, and finally applies a
postprocessing method to correct mistakes and provide the binary segmentation
from which the corneal parameters are estimated (cell density [ECD],
coefficient of variation [CV], and hexagonality [HEX]). In this study, we
analyzed 1203 images acquired with a Topcon SP-1P microscope, 500 of which
contained guttae. Manual segmentation was performed in all images. We compared
the results of different networks (UNet, ResUNeXt, DenseUNets, UNet++) and
found that DenseUNets with fNLA provided the best performance, with a mean
absolute error of 23.16 [cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%]
in HEX, which was 3-6 times smaller than the error obtained by Topcon's
built-in software. Our approach handled the cells affected by guttae remarkably
well, detecting cell edges occluded by small guttae while discarding areas
covered by large guttae. fNLA made use of the local information, providing
sharper edges in guttae areas and better results in the selection of
well-detected cells. Overall, the proposed method obtained reliable and
accurate estimations in extremely challenging specular images with guttae,
being the first method in the literature to solve this problem adequately. Code
is available in our GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TCTrack: Temporal Contexts for Aerial Tracking. (arXiv:2203.01885v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01885">
<div class="article-summary-box-inner">
<span><p>Temporal contexts among consecutive frames are far from being fully utilized
in existing visual trackers. In this work, we present TCTrack, a comprehensive
framework to fully exploit temporal contexts for aerial tracking. The temporal
contexts are incorporated at \textbf{two levels}: the extraction of
\textbf{features} and the refinement of \textbf{similarity maps}. Specifically,
for feature extraction, an online temporally adaptive convolution is proposed
to enhance the spatial features using temporal information, which is achieved
by dynamically calibrating the convolution weights according to the previous
frames. For similarity map refinement, we propose an adaptive temporal
transformer, which first effectively encodes temporal knowledge in a
memory-efficient way, before the temporal knowledge is decoded for accurate
adjustment of the similarity map. TCTrack is effective and efficient:
evaluation on four aerial tracking benchmarks shows its impressive performance;
real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX
Xavier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NUQ: A Noise Metric for Diffusion MRI via Uncertainty Discrepancy Quantification. (arXiv:2203.01921v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01921">
<div class="article-summary-box-inner">
<span><p>Diffusion MRI (dMRI) is the only non-invasive technique sensitive to tissue
micro-architecture, which can, in turn, be used to reconstruct tissue
microstructure and white matter pathways. The accuracy of such tasks is
hampered by the low signal-to-noise ratio in dMRI. Today, the noise is
characterized mainly by visual inspection of residual maps and estimated
standard deviation. However, it is hard to estimate the impact of noise on
downstream tasks based only on such qualitative assessments. To address this
issue, we introduce a novel metric, Noise Uncertainty Quantification (NUQ), for
quantitative image quality analysis in the absence of a ground truth reference
image. NUQ uses a recent Bayesian formulation of dMRI models to estimate the
uncertainty of microstructural measures. Specifically, NUQ uses the maximum
mean discrepancy metric to compute a pooled quality score by comparing samples
drawn from the posterior distribution of the microstructure measures. We show
that NUQ allows a fine-grained analysis of noise, capturing details that are
visually imperceptible. We perform qualitative and quantitative comparisons on
real datasets, showing that NUQ generates consistent scores across different
denoisers and acquisitions. Lastly, by using NUQ on a cohort of schizophrenics
and controls, we quantify the substantial impact of denoising on group
differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining. (arXiv:2203.01969v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01969">
<div class="article-summary-box-inner">
<span><p>Retrospective analysis of brain MRI scans acquired in the clinic has the
potential to enable neuroimaging studies with sample sizes much larger than
those found in research datasets. However, analysing such clinical images "in
the wild" is challenging, since subjects are scanned with highly variable
protocols (MR contrast, resolution, orientation, etc.). Nevertheless, recent
advances in convolutional neural networks (CNNs) and domain randomisation for
image segmentation, best represented by the publicly available method SynthSeg,
may enable morphometry of clinical MRI at scale. In this work, we first
evaluate SynthSeg on an uncurated, heterogeneous dataset of more than 10,000
scans acquired at Massachusetts General Hospital. We show that SynthSeg is
generally robust, but frequently falters on scans with low signal-to-noise
ratio or poor tissue contrast. Next, we propose SynthSeg+, a novel method that
greatly mitigates these problems using a hierarchy of conditional segmentation
and denoising CNNs. We show that this method is considerably more robust than
SynthSeg, while also outperforming cascaded networks and state-of-the-art
segmentation denoising methods. Finally, we apply our approach to a
proof-of-concept volumetric study of ageing, where it closely replicates
atrophy patterns observed in research studies conducted on high-quality, 1mm,
T1-weighted scans. The code and trained model are publicly available at
https://github.com/BBillot/SynthSeg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Neural Architecture Search for Lightweight Dense Prediction Networks. (arXiv:2203.01994v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01994">
<div class="article-summary-box-inner">
<span><p>We present LDP, a lightweight dense prediction neural architecture search
(NAS) framework. Starting from a pre-defined generic backbone, LDP applies the
novel Assisted Tabu Search for efficient architecture exploration. LDP is fast
and suitable for various dense estimation problems, unlike previous NAS methods
that are either computational demanding or deployed only for a single subtask.
The performance of LPD is evaluated on monocular depth estimation, semantic
segmentation, and image super-resolution tasks on diverse datasets, including
NYU-Depth-v2, KITTI, Cityscapes, COCO-stuff, DIV2K, Set5, Set14, BSD100,
Urban100. Experiments show that the proposed framework yields consistent
improvements on all tested dense prediction tasks, while being $5\%-315\%$ more
compact in terms of the number of model parameters than prior arts.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-08 23:07:35.136143925 UTC">2022-03-08 23:07:35 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>