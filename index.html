<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-23T01:30:00Z">03-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model. (arXiv:2203.11199v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11199">
<div class="article-summary-box-inner">
<span><p>Recently, the problem of robustness of pre-trained language models (PrLMs)
has received increasing research interest. Latest studies on adversarial
attacks achieve high attack success rates against PrLMs, claiming that PrLMs
are not robust. However, we find that the adversarial samples that PrLMs fail
are mostly non-natural and do not appear in reality. We question the validity
of current evaluation of robustness of PrLMs based on these non-natural
adversarial samples and propose an anomaly detector to evaluate the robustness
of PrLMs with more natural adversarial samples. We also investigate two
applications of the anomaly detector: (1) In data augmentation, we employ the
anomaly detector to force generating augmented data that are distinguished as
non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply
the anomaly detector to a defense framework to enhance the robustness of PrLMs.
It can be used to defend all types of attacks and achieves higher accuracy on
both adversarial samples and compliant samples than other defense frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization. (arXiv:2203.11239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11239">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve
state-of-the-art performance on many generative NLP tasks. However, such models
pose a great challenge in resource-constrained scenarios owing to their large
memory requirements and high latency. To alleviate this issue, we propose to
jointly distill and quantize the model, where knowledge is transferred from the
full-precision teacher model to the quantized and distilled low-precision
student model. Empirical analyses show that, despite the challenging nature of
generative tasks, we were able to achieve a 16.5x model footprint compression
ratio with little performance drop relative to the full-precision counterparts
on multiple summarization and QA datasets. We further pushed the limit of
compression ratio to 27.7x and presented the performance-efficiency trade-off
for generative tasks using pre-trained models. To the best of our knowledge,
this is the first work aiming to effectively distill and quantize
sequence-to-sequence pre-trained models for language generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Classification of Long Documents Using Transformers. (arXiv:2203.11258v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11258">
<div class="article-summary-box-inner">
<span><p>Several methods have been proposed for classifying long textual documents
using Transformers. However, there is a lack of consensus on a benchmark to
enable a fair comparison among different approaches. In this paper, we provide
a comprehensive evaluation of the relative efficacy measured against various
baselines and diverse datasets -- both in terms of accuracy as well as time and
space overheads. Our datasets cover binary, multi-class, and multi-label
classification tasks and represent various ways information is organized in a
long text (e.g. information that is critical to making the classification
decision is at the beginning or towards the end of the document). Our results
show that more complex models often fail to outperform simple baselines and
yield inconsistent performance across datasets. These findings emphasize the
need for future studies to consider comprehensive baselines and datasets that
better represent the task of long document classification to develop robust
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error. (arXiv:2203.11317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11317">
<div class="article-summary-box-inner">
<span><p>Discourse analysis allows us to attain inferences of a text document that
extend beyond the sentence-level. The current performance of discourse models
is very low on texts outside of the training distribution's coverage,
diminishing the practical utility of existing models. There is need for a
measure that can inform us to what extent our model generalizes from the
training to the test sample when these samples may be drawn from distinct
distributions. While this can be estimated via distribution shift, we argue
that this does not directly correlate with change in the observed error of a
classifier (i.e. error-gap). Thus, we propose to use a statistic from the
theoretical domain adaptation literature which can be directly tied to
error-gap. We study the bias of this statistic as an estimator of error-gap
both theoretically and through a large-scale empirical study of over 2400
experiments on 6 discourse datasets from domains including, but not limited to:
news, biomedical texts, TED talks, Reddit posts, and fiction. Our results not
only motivate our proposal and help us to understand its limitations, but also
provide insight on the properties of discourse models and datasets which
improve performance in domain adaptation. For instance, we find that non-news
datasets are slightly easier to transfer to than news datasets when the
training and test sets are very different. Our code and an associated Python
package are available to allow practitioners to make more informed model and
dataset choices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Speech Recognition Decoding via Layer Aggregation. (arXiv:2203.11325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11325">
<div class="article-summary-box-inner">
<span><p>Recently proposed speech recognition systems are designed to predict using
representations generated by their top layers, employing greedy decoding which
isolates each timestep from the rest of the sequence. Aiming for improved
performance, a beam search algorithm is frequently utilized and a language
model is incorporated to assist with ranking the top candidates. In this work,
we experiment with several speech recognition models and find that logits
predicted using the top layers may hamper beam search from achieving optimal
results. Specifically, we show that fined-tuned Wav2Vec 2.0 and HuBERT yield
highly confident predictions, and hypothesize that the predictions are based on
local information and may not take full advantage of the information encoded in
intermediate layers. To this end, we perform a layer analysis to reveal and
visualize how predictions evolve throughout the inference flow. We then propose
a prediction method that aggregates the top M layers, potentially leveraging
useful information encoded in intermediate layers and relaxing model
confidence. We showcase the effectiveness of our approach via beam search
decoding, conducting our experiments on Librispeech test and dev sets and
achieving WER, and CER reduction of up to 10% and 22%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On The Robustness of Offensive Language Classifiers. (arXiv:2203.11331v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11331">
<div class="article-summary-box-inner">
<span><p>Social media platforms are deploying machine learning based offensive
language classification systems to combat hateful, racist, and other forms of
offensive speech at scale. However, despite their real-world deployment, we do
not yet comprehensively understand the extent to which offensive language
classifiers are robust against adversarial attacks. Prior work in this space is
limited to studying robustness of offensive language classifiers against
primitive attacks such as misspellings and extraneous spaces. To address this
gap, we systematically analyze the robustness of state-of-the-art offensive
language classifiers against more crafty adversarial attacks that leverage
greedy- and attention-based word selection and context-aware embeddings for
word replacement. Our results on multiple datasets show that these crafty
adversarial attacks can degrade the accuracy of offensive language classifiers
by more than 50% while also being able to preserve the readability and meaning
of the modified text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels. (arXiv:2203.11364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11364">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models derive substantial linguistic and factual
knowledge from the massive corpora on which they are trained, and prompt
engineering seeks to align these models to specific tasks. Unfortunately,
existing prompt engineering methods require significant amounts of labeled
data, access to model parameters, or both. We introduce a new method for
selecting prompt templates \textit{without labeled examples} and
\textit{without direct access to the model}. Specifically, over a set of
candidate templates, we choose the template that maximizes the mutual
information between the input and the corresponding model output. Across 8
datasets representing 7 distinct NLP tasks, we show that when a template has
high mutual information, it also has high accuracy on the task. On the largest
model, selecting prompts with our method gets 90\% of the way from the average
prompt accuracy to the best prompt accuracy and requires no ground truth
labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language modeling via stochastic processes. (arXiv:2203.11370v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11370">
<div class="article-summary-box-inner">
<span><p>Modern language models can generate high-quality short texts. However, they
often meander or are incoherent when generating longer texts. These issues
arise from the next-token-only language modeling objective. To address these
issues, we introduce Time Control (TC), a language model that implicitly plans
via a latent stochastic process. TC does this by learning a representation
which maps the dynamics of how text changes in a document to the dynamics of a
stochastic process of interest. Using this representation, the language model
can generate text by first implicitly generating a document plan via a
stochastic process, and then generating text that is consistent with this
latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a
variety of text domains, TC improves performance on text infilling and
discourse coherence. On long text generation settings, TC preserves the text
structure both in terms of ordering (up to +40% better) and text length
consistency (up to +17% better). Human evaluators also prefer TC's output 28.6%
more than the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Textual Out-of-Domain Detection without In-Domain Labels. (arXiv:2203.11396v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11396">
<div class="article-summary-box-inner">
<span><p>In many real-world settings, machine learning models need to identify user
inputs that are out-of-domain (OOD) so as to avoid performing wrong actions.
This work focuses on a challenging case of OOD detection, where no labels for
in-domain data are accessible (e.g., no intent labels for the intent
classification task). To this end, we first evaluate different language model
based approaches that predict likelihood for a sequence of tokens. Furthermore,
we propose a novel representation learning based method by combining
unsupervised clustering and contrastive learning so that better data
representations for OOD detection can be learned. Through extensive
experiments, we demonstrate that this method can significantly outperform
likelihood-based methods and can be even competitive to the state-of-the-art
supervised approaches with label information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Achieving Conversational Goals with Unsupervised Post-hoc Knowledge Injection. (arXiv:2203.11399v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11399">
<div class="article-summary-box-inner">
<span><p>A limitation of current neural dialog models is that they tend to suffer from
a lack of specificity and informativeness in generated responses, primarily due
to dependence on training data that covers a limited variety of scenarios and
conveys limited knowledge. One way to alleviate this issue is to extract
relevant knowledge from external sources at decoding time and incorporate it
into the dialog response. In this paper, we propose a post-hoc
knowledge-injection technique where we first retrieve a diverse set of relevant
knowledge snippets conditioned on both the dialog history and an initial
response from an existing dialog model. We construct multiple candidate
responses, individually injecting each retrieved snippet into the initial
response using a gradient-based decoding method, and then select the final
response with an unsupervised ranking step. Our experiments in goal-oriented
and knowledge-grounded dialog settings demonstrate that human annotators judge
the outputs from the proposed method to be more engaging and informative
compared to responses from prior dialog systems. We further show that
knowledge-augmentation promotes success in achieving conversational goals in
both experimental settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLSP 2021 Shared Task: Vietnamese Machine Reading Comprehension. (arXiv:2203.11400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11400">
<div class="article-summary-box-inner">
<span><p>One of the emerging research trends in natural language understanding is
machine reading comprehension (MRC) which is the task to find answers to human
questions based on textual data. Existing Vietnamese datasets for MRC research
concentrate solely on answerable questions. However, in reality, questions can
be unanswerable for which the correct answer is not stated in the given textual
data. To address the weakness, we provide the research community with a
benchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question
answering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a
benchmark dataset for the shared task on Vietnamese MRC at the Eighth Workshop
on Vietnamese Language and Speech Processing (VLSP 2021). This task attracted
77 participant teams from 34 universities and other organizations. In this
article, we present details of the organization of the shared task, an overview
of the methods employed by shared-task participants, and the results. The
highest performances are 77.24% EM and 67.43% F1-score on the private test set.
The Vietnamese MRC systems proposed by the top 3 teams use XLM-RoBERTa, a
powerful pre-trained language model using the transformer architecture. The
UIT-ViQuAD 2.0 dataset motivates more researchers to explore Vietnamese machine
reading comprehension, question answering, and question generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective. (arXiv:2203.11401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11401">
<div class="article-summary-box-inner">
<span><p>Prior research has discussed and illustrated the need to consider linguistic
norms at the community level when studying taboo (hateful/offensive/toxic etc.)
language. However, a methodology for doing so, that is firmly founded on
community language norms is still largely absent. This can lead both to biases
in taboo text classification and limitations in our understanding of the causes
of bias. We propose a method to study bias in taboo classification and
annotation where a community perspective is front and center. This is
accomplished by using special classifiers tuned for each community's language.
In essence, these classifiers represent community level language norms. We use
these to study bias and find, for example, biases are largest against African
Americans (7/10 datasets and all 3 classifiers examined). In contrast to
previous papers we also study other communities and find, for example, strong
biases against South Asians. In a small scale user study we illustrate our key
idea which is that common utterances, i.e., those with high alignment scores
with a community (community classifier confidence scores) are unlikely to be
regarded taboo. Annotators who are community members contradict taboo
classification decisions and annotations in a majority of instances. This paper
is a significant step toward reducing false positive taboo decisions that over
time harm minority communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Confidence for Transformer-based Neural Machine Translation. (arXiv:2203.11413v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11413">
<div class="article-summary-box-inner">
<span><p>Confidence estimation aims to quantify the confidence of the model
prediction, providing an expectation of success. A well-calibrated confidence
estimate enables accurate failure prediction and proper risk measurement when
given noisy samples and out-of-distribution data in real-world settings.
However, this task remains a severe challenge for neural machine translation
(NMT), where probabilities from softmax distribution fail to describe when the
model is probably mistaken. To address this problem, we propose an unsupervised
confidence estimate learning jointly with the training of the NMT model. We
explain confidence as how many hints the NMT model needs to make a correct
prediction, and more hints indicate low confidence. Specifically, the NMT model
is given the option to ask for hints to improve translation accuracy at the
cost of some slight penalty. Then, we approximate their level of confidence by
counting the number of hints the model uses. We demonstrate that our learned
confidence estimate achieves high accuracy on extensive sentence/word-level
quality estimation tasks. Analytical results verify that our confidence
estimate can correctly assess underlying risk in two real-world scenarios: (1)
discovering noisy samples and (2) detecting out-of-domain data. We further
propose a novel confidence-based instance-specific label smoothing approach
based on our learned confidence estimate, which outperforms standard label
smoothing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Abstractive Grounded Summarization of Podcast Transcripts. (arXiv:2203.11425v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11425">
<div class="article-summary-box-inner">
<span><p>Podcasts have recently shown a rapid rise in popularity. Summarization of
podcast transcripts is of practical benefit to both content providers and
consumers. It helps consumers to quickly decide whether they will listen to the
podcasts and reduces the cognitive load of content providers to write
summaries. Nevertheless, podcast summarization faces significant challenges
including factual inconsistencies with respect to the inputs. The problem is
exacerbated by speech disfluencies and recognition errors in transcripts of
spoken language. In this paper, we explore a novel abstractive summarization
method to alleviate these challenges. Specifically, our approach learns to
produce an abstractive summary while grounding summary segments in specific
portions of the transcript to allow for full inspection of summary details. We
conduct a series of analyses of the proposed approach on a large podcast
dataset and show that the approach can achieve promising results. Grounded
summaries bring clear benefits in locating the summary and transcript segments
that contain inconsistent information, and hence significantly improve
summarization quality in both automatic and human evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-guided Disentangled Tuning for Pretrained Language Models. (arXiv:2203.11431v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11431">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) trained on large-scale unlabeled corpus are
typically fine-tuned on task-specific downstream datasets, which have produced
state-of-the-art results on various NLP tasks. However, the data discrepancy
issue in domain and scale makes fine-tuning fail to efficiently capture
task-specific patterns, especially in the low data regime. To address this
issue, we propose Task-guided Disentangled Tuning (TDT) for PLMs, which
enhances the generalization of representations by disentangling task-relevant
signals from the entangled representations. For a given task, we introduce a
learnable confidence model to detect indicative guidance from context, and
further propose a disentangled regularization to mitigate the over-reliance
problem. Experimental results on GLUE and CLUE benchmarks show that TDT gives
consistently better results than fine-tuning with different PLMs, and extensive
analysis demonstrates the effectiveness and robustness of our method. Code is
available at https://github.com/lemon0830/TDT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demo of the Linguistic Field Data Management and Analysis System -- LiFE. (arXiv:2203.11443v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11443">
<div class="article-summary-box-inner">
<span><p>In the proposed demo, we will present a new software - Linguistic Field Data
Management and Analysis System - LiFE (https://github.com/kmi-linguistics/life)
- an open-source, web-based linguistic data management and analysis application
that allows for systematic storage, management, sharing and usage of linguistic
data collected from the field. The application allows users to store lexical
items, sentences, paragraphs, audio-visual content with rich glossing /
annotation; generate interactive and print dictionaries; and also train and use
natural language processing tools and models for various purposes using this
data. Since its a web-based application, it also allows for seamless
collaboration among multiple persons and sharing the data, models, etc with
each other.
</p>
<p>The system uses the Python-based Flask framework and MongoDB in the backend
and HTML, CSS and Javascript at the frontend. The interface allows creation of
multiple projects that could be shared with the other users. At the backend,
the application stores the data in RDF format so as to allow its release as
Linked Data over the web using semantic web technologies - as of now it makes
use of the OntoLex-Lemon for storing the lexical data and Ligt for storing the
interlinear glossed text and then internally linking it to the other linked
lexicons and databases such as DBpedia and WordNet. Furthermore it provides
support for training the NLP systems using scikit-learn and HuggingFace
Transformers libraries as well as make use of any model trained using these
libraries - while the user interface itself provides limited options for tuning
the system, an externally-trained model could be easily incorporated within the
application; similarly the dataset itself could be easily exported into a
standard machine-readable format like JSON or CSV that could be consumed by
other programs and pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data. (arXiv:2203.11476v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11476">
<div class="article-summary-box-inner">
<span><p>Human speakers encode information into raw speech which is then decoded by
the listeners. This complex relationship between encoding (production) and
decoding (perception) is often modeled separately. Here, we test how decoding
of lexical and sublexical semantic information can emerge automatically from
raw speech in unsupervised generative deep convolutional networks that combine
both the production and perception principle. We introduce, to our knowledge,
the most challenging objective in unsupervised lexical learning: an
unsupervised network that must learn to assign unique representations for
lexical items with no direct access to training data. We train several models
(ciwGAN and fiwGAN by [1]) and test how the networks classify raw acoustic
lexical items in the unobserved test data. Strong evidence in favor of lexical
learning emerges. The architecture that combines the production and perception
principles is thus able to learn to decode unique information from raw acoustic
data in an unsupervised manner without ever accessing real training data. We
propose a technique to explore lexical and sublexical learned representations
in the classifier network. The results bear implications for both unsupervised
speech synthesis and recognition as well as for unsupervised semantic modeling
as language models increasingly bypass text and operate from raw acoustics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11480">
<div class="article-summary-box-inner">
<span><p>Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approaches for Improving the Performance of Fake News Detection in Bangla: Imbalance Handling and Model Stacking. (arXiv:2203.11486v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11486">
<div class="article-summary-box-inner">
<span><p>Imbalanced datasets can lead to biasedness into the detection of fake news.
In this work, we present several strategies for resolving the imbalance issue
for fake news detection in Bangla with a comparative assessment of proposed
methodologies. Additionally, we propose a technique for improving performance
even when the dataset is imbalanced. We applied our proposed approaches to
BanFakeNews, a dataset developed for the purpose of detecting fake news in
Bangla comprising of 50K instances but is significantly skewed, with 97% of
majority instances. We obtained a 93.1% F1-score using data manipulation
manipulation techniques such as SMOTE, and a 79.1% F1-score using without data
manipulation approaches such as Stacked Generalization. Without implementing
these techniques, the F1-score would have been 67.6% for baseline models. We
see this work as an important step towards paving the way of fake news
detection in Bangla. By implementing these strategies the obstacles of
imbalanced dataset can be removed and improvement in the performance can be
achieved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factual Consistency of Multilingual Pretrained Language Models. (arXiv:2203.11552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11552">
<div class="article-summary-box-inner">
<span><p>Pretrained language models can be queried for factual knowledge, with
potential applications in knowledge base acquisition and tasks that require
inference. However, for that, we need to know how reliable this knowledge is,
and recent work has shown that monolingual English language models lack
consistency when predicting factual knowledge, that is, they fill-in-the-blank
differently for paraphrases describing the same fact. In this paper, we extend
the analysis of consistency to a multilingual setting. We introduce a resource,
mParaRel, and investigate (i) whether multilingual language models such as
mBERT and XLM-R are more consistent than their monolingual counterparts; and
(ii) if such models are equally consistent across languages. We find that mBERT
is as inconsistent as English BERT in English paraphrases, but that both mBERT
and XLM-R exhibit a high degree of inconsistency in English and even more so
for all the other 45 languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Text-to-Speech Pipeline, Evaluation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis. (arXiv:2203.11562v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11562">
<div class="article-summary-box-inner">
<span><p>Speech synthesis has come a long way as current text-to-speech (TTS) models
can now generate natural human-sounding speech. However, most of the TTS
research focuses on using adult speech data and there has been very limited
work done on child speech synthesis. This study developed and validated a
training pipeline for fine-tuning state-of-the-art (SOTA) neural TTS models
using child speech datasets. This approach adopts a multispeaker TTS retuning
workflow to provide a transfer-learning pipeline. A publicly available child
speech dataset was cleaned to provide a smaller subset of approximately 19
hours, which formed the basis of our fine-tuning experiments. Both subjective
and objective evaluations were performed using a pretrained MOSNet for
objective evaluation and a novel subjective framework for mean opinion score
(MOS) evaluations. Subjective evaluations achieved the MOS of 3.92 for speech
intelligibility, 3.85 for voice naturalness, and 3.96 for voice consistency.
Objective evaluation using a pretrained MOSNet showed a strong correlation
between real and synthetic child voices. The final trained model was able to
synthesize child-like speech from reference audio samples as short as 5
seconds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utterance Rewriting with Contrastive Learning in Multi-turn Dialogue. (arXiv:2203.11587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11587">
<div class="article-summary-box-inner">
<span><p>Context modeling plays a significant role in building multi-turn dialogue
systems. In order to make full use of context information, systems can use
Incomplete Utterance Rewriting(IUR) methods to simplify the multi-turn dialogue
into single-turn by merging current utterance and context information into a
self-contained utterance. However, previous approaches ignore the intent
consistency between the original query and rewritten query. The detection of
omitted or coreferred locations in the original query can be further improved.
In this paper, we introduce contrastive learning and multi-task learning to
jointly model the problem. Our method benefits from carefully designed
self-supervised objectives, which act as auxiliary tasks to capture semantics
at both sentence-level and token-level. The experiments show that our proposed
model achieves state-of-the-art performance on several public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation. (arXiv:2203.11591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11591">
<div class="article-summary-box-inner">
<span><p>Pre-training has been adopted in a few of recent works for
Vision-and-Language Navigation (VLN). However, previous pre-training methods
for VLN either lack the ability to predict future actions or ignore the
trajectory contexts, which are essential for a greedy navigation process. In
this work, to promote the learning of spatio-temporal visual-textual
correspondence as well as the agent's capability of decision making, we propose
a novel history-and-order aware pre-training paradigm (HOP) with VLN-specific
objectives that exploit the past observations and support future action
prediction. Specifically, in addition to the commonly used Masked Language
Modeling (MLM) and Trajectory-Instruction Matching (TIM), we design two proxy
tasks to model temporal order information: Trajectory Order Modeling (TOM) and
Group Order Modeling (GOM). Moreover, our navigation action prediction is also
enhanced by introducing the task of Action Prediction with History (APH), which
takes into account the history visual perceptions. Extensive experimental
results on four downstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the
effectiveness of our proposed method compared against several state-of-the-art
agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Relation-Specific Representations for Few-shot Knowledge Graph Completion. (arXiv:2203.11639v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11639">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed increasing interest in few-shot knowledge graph
completion (FKGC), which aims to infer unseen query triples for a few-shot
relation using a handful of reference triples of the relation. The primary
focus of existing FKGC methods lies in learning the relation representations
that can reflect the common information shared by the query and reference
triples. To this end, these methods learn the embeddings of entities with their
direct neighbors, and use the concatenation of the entity embeddings as the
relation representations. However, the entity embeddings learned only from
direct neighborhoods may have low expressiveness when the entity has sparse
neighbors or shares a common local neighborhood with other entities. Moreover,
the embeddings of two entities are insufficient to represent the semantic
information of their relationship, especially when they have multiple
relations. To address these issues, we propose a Relation-Specific Context
Learning (RSCL) framework, which exploits graph contexts of triples to capture
the semantic information of relations and entities simultaneously.
Specifically, we first extract graph contexts for each triple, which can
provide long-term entity-relation dependencies. To model the graph contexts, we
then develop a hierarchical relation-specific learner to learn global and local
relation-specific representations for relations by capturing contextualized
information of triples and incorporating local information of entities.
Finally, we utilize the learned representations to predict the likelihood of
the query triples. Experimental results on two public datasets demonstrate that
RSCL outperforms state-of-the-art FKGC methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are You Misinformed? A Study of Covid-Related Fake News in Bengali on Facebook. (arXiv:2203.11669v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11669">
<div class="article-summary-box-inner">
<span><p>Our opinions and views of life can be shaped by how we perceive the opinions
of others on social media like Facebook. This dependence has increased during
COVID-19 periods when we have fewer means to connect with others. However, fake
news related to COVID-19 has become a significant problem on Facebook. Bengali
is the seventh most spoken language worldwide, yet we are aware of no previous
research that studied the prevalence of COVID-19 related fake news in Bengali
on Facebook. In this paper, we develop machine learning models to detect fake
news in Bengali automatically. The best performing model is BERT, with an
F1-score of 0.97. We apply BERT on all Facebook Bengali posts related to
COVID-19. We find 10 topics in the COVID-19 Bengali fake news grouped into
three categories: System (e.g., medical system), belief (e.g., religious
rituals), and social (e.g., scientific awareness).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation. (arXiv:2203.11670v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11670">
<div class="article-summary-box-inner">
<span><p>Building models of natural language processing (NLP) is challenging in
low-resource scenarios where only limited data are available.
Optimization-based meta-learning algorithms achieve promising results in
low-resource scenarios by adapting a well-generalized model initialization to
handle new tasks. Nonetheless, these approaches suffer from the memorization
overfitting issue, where the model tends to memorize the meta-training tasks
while ignoring support sets when adapting to new tasks. To address this issue,
we propose a memory imitation meta-learning (MemIML) method that enhances the
model's reliance on support sets for task adaptation. Specifically, we
introduce a task-specific memory module to store support set information and
construct an imitation module to force query sets to imitate the behaviors of
some representative support-set samples stored in the memory. A theoretical
analysis is provided to prove the effectiveness of our method, and empirical
results also demonstrate that our method outperforms competitive baselines on
both text classification and generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning in Sentiment Analysis. (arXiv:2203.11702v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11702">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) task aims to associate a piece of text
with a set of aspects and meanwhile infer their respective sentimental
polarities. Up to now, the state-of-the-art approaches are built upon
fine-tuning of various pre-trained language models. They commonly aim to learn
the aspect-specific representation in the corpus. Unfortunately, the aspect is
often expressed implicitly through a set of representatives and thus renders
implicit mapping process unattainable unless sufficient labeled examples.
</p>
<p>In this paper, we propose to jointly address aspect categorization and
aspect-based sentiment subtasks in a unified framework. Specifically, we first
introduce a simple but effective mechanism that collaborates the semantic and
syntactic information to construct auxiliary-sentences for the implicit aspect.
Then, we encourage BERT to learn the aspect-specific representation in response
to the automatically constructed auxiliary-sentence instead of the aspect
itself. Finally, we empirically evaluate the performance of the proposed
solution by a comparative study on real benchmark datasets for both ABSA and
Targeted-ABSA tasks. Our extensive experiments show that it consistently
achieves state-of-the-art performance in terms of aspect categorization and
aspect-based sentiment across all datasets and the improvement margins are
considerable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Detection, Rapidly React: Unseen Rumors Detection based on Continual Prompt-Tuning. (arXiv:2203.11720v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11720">
<div class="article-summary-box-inner">
<span><p>Since open social platforms allow for a large and continuous flow of
unverified information, rumors can emerge unexpectedly and spread quickly.
However, existing rumor detection (RD) models often assume the same training
and testing distributions and cannot cope with the continuously changing social
network environment. This paper proposes a Continual Prompt-Tuning RD (CPT-RD)
framework, which avoids catastrophic forgetting of upstream tasks during
sequential task learning and enables knowledge transfer between domain tasks.
To avoid forgetting, we optimize and store task-special soft-prompt for each
domain. Furthermore, we also propose several strategies to transfer knowledge
of upstream tasks to deal with emergencies and a task-conditioned prompt-wise
hypernetwork (TPHNet) to consolidate past domains, enabling bidirectional
knowledge transfer. Finally, CPT-RD is evaluated on English and Chinese RD
datasets and is effective and efficient compared to state-of-the-art baselines,
without data replay techniques and with only a few parameter tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments. (arXiv:2203.11764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11764">
<div class="article-summary-box-inner">
<span><p>Building on current work on multilingual hate speech (e.g., Ousidhoum et al.
(2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present
XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages
from Brazil, Germany, India and Kenya. The key novelty is that we directly
involve the affected communities in collecting and annotating the data - as
opposed to giving companies and governments control over defining and
combatting hate speech. This inclusive approach results in datasets more
representative of actually occurring online speech and is likely to facilitate
the removal of the social media content that marginalized communities view as
causing the most harm. Based on XTREMESPEECH, we establish novel tasks with
accompanying baselines, provide evidence that cross-country training is
generally not feasible due to cultural differences between countries and
perform an interpretability analysis of BERT's predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SU-NLP at SemEval-2022 Task 11: Complex Named Entity Recognition with Entity Linking. (arXiv:2203.11841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11841">
<div class="article-summary-box-inner">
<span><p>This paper describes the system proposed by Sabanc{\i} University Natural
Language Processing Group in the SemEval-2022 MultiCoNER task. We developed an
unsupervised entity linking pipeline that detects potential entity mentions
with the help of Wikipedia and also uses the corresponding Wikipedia context to
help the classifier in finding the named entity type of that mention. Our
results showed that our pipeline improved performance significantly, especially
for complex entities in low-context settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Girl Has A Name, And It's ... Adversarial Authorship Attribution for Deobfuscation. (arXiv:2203.11849v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11849">
<div class="article-summary-box-inner">
<span><p>Recent advances in natural language processing have enabled powerful
privacy-invasive authorship attribution. To counter authorship attribution,
researchers have proposed a variety of rule-based and learning-based text
obfuscation approaches. However, existing authorship obfuscation approaches do
not consider the adversarial threat model. Specifically, they are not evaluated
against adversarially trained authorship attributors that are aware of
potential obfuscation. To fill this gap, we investigate the problem of
adversarial authorship attribution for deobfuscation. We show that
adversarially trained authorship attributors are able to degrade the
effectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluate
the effectiveness of adversarial training when the attributor makes incorrect
assumptions about whether and which obfuscator was used. While there is a a
clear degradation in attribution accuracy, it is noteworthy that this
degradation is still at or above the attribution accuracy of the attributor
that is not adversarially trained at all. Our results underline the need for
stronger obfuscation approaches that are resistant to deobfuscation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Computational Approach to Understand Mental Health from Reddit: Knowledge-aware Multitask Learning Framework. (arXiv:2203.11856v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11856">
<div class="article-summary-box-inner">
<span><p>Analyzing gender is critical to study mental health (MH) support in CVD
(cardiovascular disease). The existing studies on using social media for
extracting MH symptoms consider symptom detection and tend to ignore user
context, disease, or gender. The current study aims to design and evaluate a
system to capture how MH symptoms associated with CVD are expressed differently
with the gender on social media. We observe that the reliable detection of MH
symptoms expressed by persons with heart disease in user posts is challenging
because of the co-existence of (dis)similar MH symptoms in one post and due to
variation in the description of symptoms based on gender. We collect a corpus
of $150k$ items (posts and comments) annotated using the subreddit labels and
transfer learning approaches. We propose GeM, a novel task-adaptive multi-task
learning approach to identify the MH symptoms in CVD patients based on gender.
Specifically, we adapt a knowledge-assisted RoBERTa based bi-encoder model to
capture CVD-related MH symptoms. Moreover, it enhances the reliability for
differentiating the gender language in MH symptoms when compared to the
state-of-art language models. Our model achieves high (statistically
significant) performance and predicts four labels of MH issues and two gender
labels, which outperforms RoBERTa, improving the recall by 2.14% on the symptom
identification task and by 2.55% on the gender identification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer based ensemble for emotion detection. (arXiv:2203.11899v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11899">
<div class="article-summary-box-inner">
<span><p>Detecting emotions in languages is important to accomplish a complete
interaction between humans and machines. This paper describes our contribution
to the WASSA 2022 shared task which handles this crucial task of emotion
detection. We have to identify the following emotions: sadness, surprise,
neutral, anger, fear, disgust, joy based on a given essay text. We are using an
ensemble of ELECTRA and BERT models to tackle this problem achieving an F1
score of 62.76%. Our codebase (https://bit.ly/WASSA_shared_task) and our WandB
project (https://wandb.ai/acl_wassa_pictxmanipal/acl_wassa) is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11933">
<div class="article-summary-box-inner">
<span><p>Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these harms. Prior proposed bias
measurements lack robustness and feature degradation occurs when mitigating
bias without access to pretraining data. We address both of these challenges in
this paper: First, we evaluate different bias measures and propose the use of
retrieval metrics to image-text representations via a bias measuring framework.
Second, we investigate debiasing methods and show that optimizing for
adversarial loss via learnable token embeddings minimizes various bias measures
without substantially degrading feature representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking. (arXiv:2103.00293v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00293">
<div class="article-summary-box-inner">
<span><p>Augmentation of task-oriented dialogues has followed standard methods used
for plain-text such as back-translation, word-level manipulation, and
paraphrasing despite its richly annotated structure. In this work, we introduce
an augmentation framework that utilizes belief state annotations to match turns
from various dialogues and form new synthetic dialogues in a bottom-up manner.
Unlike other augmentation strategies, it operates with as few as five examples.
Our augmentation strategy yields significant improvements when both adapting a
DST model to a new domain, and when adapting a language model to the DST task,
on evaluations with TRADE and TOD-BERT models. Further analysis shows that our
model performs better on seen values during training, and it is also more
robust to unseen values. We conclude that exploiting belief state annotations
enhances dialogue augmentation and results in improved models in n-shot
training scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InFillmore: Frame-Guided Language Generation with Bidirectional Context. (arXiv:2103.04941v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04941">
<div class="article-summary-box-inner">
<span><p>We propose a structured extension to bidirectional-context conditional
language generation, or "infilling," inspired by Frame Semantic theory
(Fillmore, 1976). Guidance is provided through two approaches: (1) model
fine-tuning, conditioning directly on observed symbolic frames, and (2) a novel
extension to disjunctive lexically constrained decoding that leverages frame
semantic lexical units. Automatic and human evaluations confirm that
frame-guided generation allows for explicit manipulation of intended infill
semantics, with minimal loss in distinguishability from human-generated text.
Our methods flexibly apply to a variety of use scenarios, and we provide a
codebase and interactive demo available from
https://nlp.jhu.edu/demos/infillmore.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey. (arXiv:2104.06951v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06951">
<div class="article-summary-box-inner">
<span><p>The development of deep learning techniques has allowed Neural Machine
Translation (NMT) models to become extremely powerful, given sufficient
training data and training time. However, systems struggle when translating
text from a new domain with a distinct style or vocabulary. Fine-tuning on
in-domain data allows good domain adaptation, but requires sufficient relevant
bilingual data. Even if this is available, simple fine-tuning can cause
overfitting to new data and `catastrophic forgetting' of previously learned
behaviour.
</p>
<p>We concentrate on robust approaches to domain adaptation for NMT,
particularly where a system may need to translate across multiple domains. We
divide techniques into those revolving around data selection or generation,
model architecture, parameter adaptation procedure, and inference procedure. We
finally highlight the benefits of domain adaptation and multi-domain adaptation
techniques to other lines of NMT research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines. (arXiv:2104.08790v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08790">
<div class="article-summary-box-inner">
<span><p>Even to a simple and short news headline, readers react in a multitude of
ways: cognitively (e.g. inferring the writer's intent), emotionally (e.g.
feeling distrust), and behaviorally (e.g. sharing the news with their friends).
Such reactions are instantaneous and yet complex, as they rely on factors that
go beyond interpreting factual content of news. We propose Misinfo Reaction
Frames (MRF), a pragmatic formalism for modeling how readers might react to a
news headline. In contrast to categorical schema, our free-text dimensions
provide a more nuanced way of understanding intent beyond being benign or
malicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced
dataset of reactions to over 25k news headlines focusing on global crises: the
Covid-19 pandemic, climate change, and cancer. Empirical results confirm that
it is indeed possible for neural models to predict the prominent patterns of
readers' reactions to previously unseen news headlines. Additionally, our user
study shows that displaying machine-generated MRF implications alongside news
headlines to readers can increase their trust in real news while decreasing
their trust in misinformation. Our work demonstrates the feasibility and
importance of pragmatic inferences on news headlines to help enhance AI-guided
misinformation detection and mitigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trakhtenbrot's Theorem in Coq: Finite Model Theory through the Constructive Lens. (arXiv:2104.14445v4 [cs.LO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14445">
<div class="article-summary-box-inner">
<span><p>We study finite first-order satisfiability (FSAT) in the constructive setting
of dependent type theory. Employing synthetic accounts of enumerability and
decidability, we give a full classification of FSAT depending on the
first-order signature of non-logical symbols. On the one hand, our development
focuses on Trakhtenbrot's theorem, stating that FSAT is undecidable as soon as
the signature contains an at least binary relation symbol. Our proof proceeds
by a many-one reduction chain starting from the Post correspondence problem. On
the other hand, we establish the decidability of FSAT for monadic first-order
logic, i.e. where the signature only contains at most unary function and
relation symbols, as well as the enumerability of FSAT for arbitrary enumerable
signatures. To showcase an application of Trakhtenbrot's theorem, we continue
our reduction chain with a many-one reduction from FSAT to separation logic.
All our results are mechanised in the framework of a growing Coq library of
synthetic undecidability proofs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03842">
<div class="article-summary-box-inner">
<span><p>Error correction techniques have been used to refine the output sentences
from automatic speech recognition (ASR) models and achieve a lower word error
rate (WER) than original ASR outputs. Previous works usually use a
sequence-to-sequence model to correct an ASR output sentence autoregressively,
which causes large latency and cannot be deployed in online ASR services. A
straightforward solution to reduce latency, inspired by non-autoregressive
(NAR) neural machine translation, is to use an NAR sequence generation model
for ASR error correction, which, however, comes at the cost of significantly
increased ASR error rate. In this paper, observing distinctive error patterns
and correction operations (i.e., insertion, deletion, and substitution) in ASR,
we propose FastCorrect, a novel NAR error correction model based on edit
alignment. In training, FastCorrect aligns each source token from an ASR output
sentence to the target tokens from the corresponding ground-truth sentence
based on the edit distance between the source and target sentences, and
extracts the number of target tokens corresponding to each source token during
edition/correction, which is then used to train a length predictor and to
adjust the source tokens to match the length of the target sentence for
parallel generation. In inference, the token number predicted by the length
predictor is used to adjust the source tokens for target sequence generation.
Experiments on the public AISHELL-1 dataset and an internal industrial-scale
ASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)
it speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER
reduction) compared with the autoregressive correction model; and 2) it
outperforms the popular NAR models adopted in neural machine translation and
text edition by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning. (arXiv:2105.05557v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05557">
<div class="article-summary-box-inner">
<span><p>Open pit mines left many regions worldwide inhospitable or uninhabitable. To
put these regions back into use, entire stretches of land must be
renaturalized. For the sustainable subsequent use or transfer to a new primary
use, many contaminated sites and soil information have to be permanently
managed. In most cases, this information is available in the form of expert
reports in unstructured data collections or file folders, which in the best
case are digitized. Due to size and complexity of the data, it is difficult for
a single person to have an overview of this data in order to be able to make
reliable statements. This is one of the most important obstacles to the rapid
transfer of these areas to after-use. An information-based approach to this
issue supports fulfilling several Sustainable Development Goals regarding
environment issues, health and climate action. We use a stack of Optical
Character Recognition, Text Classification, Active Learning and Geographic
Information System Visualization to effectively mine and visualize this
information. Subsequently, we link the extracted information to geographic
coordinates and visualize them using a Geographic Information System. Active
Learning plays a vital role because our dataset provides no training data. In
total, we process nine categories and actively learn their representation in
our dataset. We evaluate the OCR, Active Learning and Text Classification
separately to report the performance of the system. Active Learning and text
classification results are twofold: Whereas our categories about restrictions
work sufficient ($&gt;$.85 F1), the seven topic-oriented categories were
complicated for human coders and hence the results achieved mediocre evaluation
scores ($&lt;$.70 F1).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The MultiBERTs: BERT Reproductions for Robustness Analysis. (arXiv:2106.16163v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.16163">
<div class="article-summary-box-inner">
<span><p>Experiments with pre-trained models such as BERT are often based on a single
checkpoint. While the conclusions drawn apply to the artifact tested in the
experiment (i.e., the particular instance of the model), it is not always clear
whether they hold for the more general procedure which includes the
architecture, training data, initialization scheme, and loss function. Recent
work has shown that repeating the pre-training process can lead to
substantially different performance, suggesting that an alternate strategy is
needed to make principled statements about procedures. To enable researchers to
draw more robust conclusions, we introduce the MultiBERTs, a set of 25
BERT-Base checkpoints, trained with similar hyper-parameters as the original
BERT model but differing in random weight initialization and shuffling of
training data. We also define the Multi-Bootstrap, a non-parametric bootstrap
method for statistical inference designed for settings where there are multiple
pre-trained models and limited test data. To illustrate our approach, we
present a case study of gender bias in coreference resolution, in which the
Multi-Bootstrap lets us measure effects that may not be detected with a single
checkpoint. We release our models and statistical library along with an
additional set of 140 intermediate checkpoints captured during pre-training to
facilitate research on learning dynamics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct speech-to-speech translation with discrete units. (arXiv:2107.05604v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05604">
<div class="article-summary-box-inner">
<span><p>We present a direct speech-to-speech translation (S2ST) model that translates
speech from one language to speech in another language without relying on
intermediate text generation. We tackle the problem by first applying a
self-supervised discrete speech encoder on the target speech and then training
a sequence-to-sequence speech-to-unit translation (S2UT) model to predict the
discrete representations of the target speech. When target text transcripts are
available, we design a joint speech and text training framework that enables
the model to generate dual modality output (speech and text) simultaneously in
the same inference pass. Experiments on the Fisher Spanish-English dataset show
that the proposed framework yields improvement of 6.7 BLEU compared with a
baseline direct S2ST model that predicts spectrogram features. When trained
without any text transcripts, our model performance is comparable to models
that predict spectrograms and are trained with text supervision, showing the
potential of our system for translation between unwritten languages. Audio
samples are available at
https://facebookresearch.github.io/speech_translation/direct_s2st_units/index.html .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning When to Translate for Streaming Speech. (arXiv:2109.07368v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07368">
<div class="article-summary-box-inner">
<span><p>How to find proper moments to generate partial sentence translation given a
streaming speech input? Existing approaches waiting-and-translating for a fixed
duration often break the acoustic units in speech, since the boundaries between
acoustic units in speech are not even. In this paper, we propose MoSST, a
simple yet effective method for translating streaming speech content. Given a
usually long speech sequence, we develop an efficient monotonic segmentation
module inside an encoder-decoder model to accumulate acoustic information
incrementally and detect proper speech unit boundaries for the input in speech
translation task. Experiments on multiple translation directions of the MuST-C
dataset show that MoSST outperforms existing methods and achieves the best
trade-off between translation quality (BLEU) and latency. Our code is available
at https://github.com/dqqcasia/mosst.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Trade-offs of Domain Adaptation for Neural Language Models. (arXiv:2109.10274v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10274">
<div class="article-summary-box-inner">
<span><p>This work connects language model adaptation with concepts of machine
learning theory. We consider a training setup with a large out-of-domain set
and a small in-domain set. We derive how the benefit of training a model on
either set depends on the size of the sets and the distance between their
underlying distributions. We analyze how out-of-domain pre-training before
in-domain fine-tuning achieves better generalization than either solution
independently. Finally, we present how adaptation techniques based on data
selection, such as importance sampling, intelligent data selection and
influence functions, can be presented in a common framework which highlights
their similarity and also their subtle differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Sequence to Sequence Learning for Compositional Generalization. (arXiv:2110.04655v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04655">
<div class="article-summary-box-inner">
<span><p>There is mounting evidence that existing neural network models, in particular
the very popular sequence-to-sequence architecture, struggle to systematically
generalize to unseen compositions of seen components. We demonstrate that one
of the reasons hindering compositional generalization relates to
representations being entangled. We propose an extension to
sequence-to-sequence models which encourages disentanglement by adaptively
re-encoding (at each time step) the source input. Specifically, we condition
the source representations on the newly decoded target context which makes it
easier for the encoder to exploit specialized information for each prediction
rather than capturing it all in a single forward pass. Experimental results on
semantic parsing and machine translation empirically show that our proposal
delivers more disentangled representations and better generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding. (arXiv:2110.07476v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07476">
<div class="article-summary-box-inner">
<span><p>Event extraction is typically modeled as a multi-class classification problem
where event types and argument roles are treated as atomic symbols. These
approaches are usually limited to a set of pre-defined types. We propose a
novel event extraction framework that uses event types and argument roles as
natural language queries to extract candidate triggers and arguments from the
input text. With the rich semantics in the queries, our framework benefits from
the attention mechanisms to better capture the semantic correlation between the
event types or argument roles and the input text. Furthermore, the
query-and-extract formulation allows our approach to leverage all available
event annotations from various ontologies as a unified model. Experiments on
ACE and ERE demonstrate that our approach achieves state-of-the-art performance
on each dataset and significantly outperforms existing methods on zero-shot
event extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models. (arXiv:2110.08173v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08173">
<div class="article-summary-box-inner">
<span><p>Knowledge probing is crucial for understanding the knowledge transfer
mechanism behind the pre-trained language models (PLMs). Despite the growing
progress of probing knowledge for PLMs in the general domain, specialised areas
such as biomedical domain are vastly under-explored. To catalyse the research
in this direction, we release a well-curated biomedical knowledge probing
benchmark, MedLAMA, which is constructed based on the Unified Medical Language
System (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs
and probing approaches on our benchmark, reaching at most 3% of acc@10. While
highlighting various sources of domain-specific challenges that amount to this
underwhelming performance, we illustrate that the underlying PLMs have a higher
potential for probing tasks. To achieve this, we propose Contrastive-Probe, a
novel self-supervised contrastive probing approach, that adjusts the underlying
PLMs without using any probing data. While Contrastive-Probe pushes the acc@10
to 28%, the performance gap still remains notable. Our human expert evaluation
suggests that the probing performance of our Contrastive-Probe is still
under-estimated as UMLS still does not include the full spectrum of factual
knowledge. We hope MedLAMA and Contrastive-Probe facilitate further
developments of more suited probing techniques for this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Open Natural Language Processing Development Framework for EHR-based Clinical Research: A case demonstration using the National COVID Cohort Collaborative (N3C). (arXiv:2110.10780v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10780">
<div class="article-summary-box-inner">
<span><p>While we pay attention to the latest advances in clinical natural language
processing (NLP), we can notice some resistance in the clinical and
translational research community to adopt NLP models due to limited
transparency, interpretability, and usability. In this study, we proposed an
open natural language processing development framework. We evaluated it through
the implementation of NLP algorithms for the National COVID Cohort
Collaborative (N3C). Based on the interests in information extraction from
COVID-19 related clinical notes, our work includes 1) an open data annotation
process using COVID-19 signs and symptoms as the use case, 2) a
community-driven ruleset composing platform, and 3) a synthetic text data
generation workflow to generate texts for information extraction tasks without
involving human subjects. The corpora were derived from texts from three
different institutions (Mayo Clinic, University of Kentucky, University of
Minnesota). The gold standard annotations were tested with a single
institution's (Mayo) ruleset. This resulted in performances of 0.876, 0.706,
and 0.694 in F-scores for Mayo, Minnesota, and Kentucky test datasets,
respectively. The study as a consortium effort of the N3C NLP subgroup
demonstrates the feasibility of creating a federated NLP algorithm development
and benchmarking platform to enhance multi-institution clinical NLP study and
adoption. Although we use COVID-19 as a use case in this effort, our framework
is general enough to be applied to other domains of interest in clinical NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Review-based Recommenders. (arXiv:2110.14747v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14747">
<div class="article-summary-box-inner">
<span><p>Just as user preferences change with time, item reviews also reflect those
same preference changes. In a nutshell, if one is to sequentially incorporate
review content knowledge into recommender systems, one is naturally led to
dynamical models of text. In the present work we leverage the known power of
reviews to enhance rating predictions in a way that (i) respects the causality
of review generation and (ii) includes, in a bidirectional fashion, the ability
of ratings to inform language review models and vice-versa, language
representations that help predict ratings end-to-end. Moreover, our
representations are time-interval aware and thus yield a continuous-time
representation of the dynamics. We provide experiments on real-world datasets
and show that our methodology is able to outperform several state-of-the-art
models. Source code for all models can be found at [1].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v9 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for image-to-text and text-to-image
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial result of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ditch the Gold Standard: Re-evaluating Conversational Question Answering. (arXiv:2112.08812v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08812">
<div class="article-summary-box-inner">
<span><p>Conversational question answering aims to provide natural-language answers to
users in information-seeking conversations. Existing conversational QA
benchmarks compare models with pre-collected human-human conversations, using
ground-truth answers provided in conversational history. It remains unclear
whether we can rely on this static evaluation for model development and whether
current systems can well generalize to real-world human-machine conversations.
In this work, we conduct the first large-scale human evaluation of
state-of-the-art conversational QA systems, where human evaluators converse
with models and judge the correctness of their answers. We find that the
distribution of human machine conversations differs drastically from that of
human-human conversations, and there is a disagreement between human and
gold-history evaluation in terms of model ranking. We further investigate how
to improve automatic evaluations, and propose a question rewriting mechanism
based on predicted history, which better correlates with human judgments.
Finally, we analyze the impact of various modeling strategies and discuss
future directions towards building better conversational question answering
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Translation from Signed to Spoken Languages: State of the Art and Challenges. (arXiv:2202.03086v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03086">
<div class="article-summary-box-inner">
<span><p>Automatic translation from signed to spoken languages is an interdisciplinary
research domain, lying on the intersection of computer vision, machine
translation and linguistics. Nevertheless, research in this domain is performed
mostly by computer scientists in isolation. As the domain is becoming
increasingly popular - the majority of scientific papers on the topic of sign
language translation have been published in the past three years - we provide
an overview of the state of the art as well as some required background in the
different related disciplines. We give a high-level introduction to sign
language linguistics and machine translation to illustrate the requirements of
automatic sign language translation. We present a systematic literature review
to illustrate the state of the art in the domain and then, harking back to the
requirements, lay out several challenges for future research. We find that
significant advances have been made on the shoulders of spoken language machine
translation research. However, current approaches are often not linguistically
motivated or are not adapted to the different input modality of sign languages.
We explore challenges related to the representation of sign language data, the
collection of datasets, the need for interdisciplinary research and
requirements for moving beyond research, towards applications. Based on our
findings, we advocate for interdisciplinary research and to base future
research on linguistic analysis of sign languages. Furthermore, the inclusion
of deaf and hearing end users of sign language translation applications in use
case identification, data collection and evaluation is of the utmost importance
in the creation of useful sign language translation models. We recommend
iterative, human-in-the-loop, design and development of sign language
translation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CampNet: Context-Aware Mask Prediction for End-to-End Text-Based Speech Editing. (arXiv:2202.09950v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09950">
<div class="article-summary-box-inner">
<span><p>The text-based speech editor allows the editing of speech through intuitive
cutting, copying, and pasting operations to speed up the process of editing
speech. However, the major drawback of current systems is that edited speech
often sounds unnatural due to cut-copy-paste operation. In addition, it is not
obvious how to synthesize records according to a new word not appearing in the
transcript. This paper proposes a novel end-to-end text-based speech editing
method called context-aware mask prediction network (CampNet). The model can
simulate the text-based speech editing process by randomly masking part of
speech and then predicting the masked region by sensing the speech context. It
can solve unnatural prosody in the edited region and synthesize the speech
corresponding to the unseen words in the transcript. Secondly, for the possible
operation of text-based speech editing, we design three text-based operations
based on CampNet: deletion, insertion, and replacement. These operations can
cover various situations of speech editing. Thirdly, to synthesize the speech
corresponding to long text in insertion and replacement operations, a
word-level autoregressive generation method is proposed. Fourthly, we propose a
speaker adaptation method using only one sentence for CampNet and explore the
ability of few-shot learning based on CampNet, which provides a new idea for
speech forgery tasks. The subjective and objective experiments on VCTK and
LibriTTS datasets show that the speech editing results based on CampNet are
better than TTS technology, manual editing, and VoCo method. We also conduct
detailed ablation experiments to explore the effect of the CampNet structure on
its performance. Finally, the experiment shows that speaker adaptation with
only one sentence can further improve the naturalness of speech. Examples of
generated speech can be found at https://hairuo55.github.io/CampNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech. (arXiv:2202.12163v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12163">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel language identification system based on
conformer layers. We propose an attentive temporal pooling mechanism to allow
the model to carry information in long-form audio via a recurrent form, such
that the inference can be performed in a streaming fashion. Additionally, a
simple domain adaptation mechanism is introduced to allow adapting an existing
language identification model to a new domain where the prior language
distribution is different. We perform a comparative study of different model
topologies under different constraints of model size, and find that
conformer-base models outperform LSTM and transformer based models. Our
experiments also show that attentive temporal pooling and domain adaptation
significantly improve the model accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Pre-training for AMR Parsing and Generation. (arXiv:2203.07836v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07836">
<div class="article-summary-box-inner">
<span><p>Abstract meaning representation (AMR) highlights the core semantic
information of text in a graph structure. Recently, pre-trained language models
(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,
respectively. However, PLMs are typically pre-trained on textual data, thus are
sub-optimal for modeling structural knowledge. To this end, we investigate
graph self-supervised training to improve the structure awareness of PLMs over
AMR graphs. In particular, we introduce two graph auto-encoding strategies for
graph-to-graph pre-training and four tasks to integrate text and graph
information during pre-training. We further design a unified framework to
bridge the gap between pre-training and fine-tuning tasks. Experiments on both
AMR parsing and AMR-to-text generation show the superiority of our model. To
our knowledge, we are the first to consider pre-training on semantic graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References. (arXiv:2203.08928v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08928">
<div class="article-summary-box-inner">
<span><p>We consider the problem of pretraining a two-stage open-domain question
answering (QA) system (retriever + reader) with strong transfer capabilities.
The key challenge is how to construct a large amount of high-quality
question-answer-context triplets without task-specific annotations.
Specifically, the triplets should align well with downstream tasks by: (i)
covering a wide range of domains (for open-domain applications), (ii) linking a
question to its semantically relevant context with supporting evidence (for
training the retriever), and (iii) identifying the correct answer in the
context (for training the reader). Previous pretraining approaches generally
fall short of one or more of these requirements. In this work, we automatically
construct a large-scale corpus that meets all three criteria by consulting
millions of references cited within Wikipedia. The well-aligned pretraining
signals benefit both the retriever and the reader significantly. Our pretrained
retriever leads to 2%-10% absolute gains in top-20 accuracy. And with our
pretrained reader, the entire system improves by up to 4% in exact match.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Dual Read/Write Paths for Simultaneous Machine Translation. (arXiv:2203.09163v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09163">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) outputs translation while reading
source sentence and hence requires a policy to decide whether to wait for the
next source word (READ) or generate a target word (WRITE), the actions of which
form a read/write path. Although the read/write path is essential to SiMT
performance, no direct supervision is given to the path in the existing
methods. In this paper, we propose a method of dual-path SiMT which introduces
duality constraints to direct the read/write path. According to duality
constraints, the read/write path in source-to-target and target-to-source SiMT
models can be mapped to each other. As a result, the two SiMT models can be
optimized jointly by forcing their read/write paths to satisfy the mapping.
Experiments on En-Vi and De-En tasks show that our method can outperform strong
baselines under all latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models. (arXiv:2203.10326v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10326">
<div class="article-summary-box-inner">
<span><p>We investigate what kind of structural knowledge learned in neural network
encoders is transferable to processing natural language. We design artificial
languages with structural properties that mimic natural language, pretrain
encoders on the data, and see how much performance the encoder exhibits on
downstream tasks in natural language. Our experimental results show that
pretraining with an artificial language with a nesting dependency structure
provides some knowledge transferable to natural language. A follow-up probing
analysis indicates that its success in the transfer is related to the amount of
encoded contextual information and what is transferred is the knowledge of
position-aware context dependence of language. Our results provide insights
into how neural network encoders process human languages and the source of
cross-lingual transferability of recent multilingual language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XTREME-S: Evaluating Cross-lingual Speech Representations. (arXiv:2203.10752v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10752">
<div class="article-summary-box-inner">
<span><p>We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual
speech representations in many languages. XTREME-S covers four task families:
speech recognition, classification, speech-to-text translation and retrieval.
Covering 102 languages from 10+ language families, 3 different domains and 4
task families, XTREME-S aims to simplify multilingual speech representation
evaluation, as well as catalyze research in "universal" speech representation
learning. This paper describes the new benchmark and establishes the first
speech-only and speech-text baselines using XLS-R and mSLAM on all downstream
tasks. We motivate the design choices and detail how to use the benchmark.
Datasets and fine-tuning scripts are made easily accessible at
https://hf.co/datasets/google/xtreme_s.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin. (arXiv:2203.10430v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10430">
<div class="article-summary-box-inner">
<span><p>Polyphone disambiguation is the most crucial task in Mandarin
grapheme-to-phoneme (g2p) conversion. Previous studies have approached this
problem using pre-trained language models, restricted output, and extra
information from Part-Of-Speech (POS) tagging. Inspired by these strategies, we
propose a novel approach, called g2pW, which adapts learnable softmax-weights
to condition the outputs of BERT with the polyphonic character of interest and
its POS tagging. Rather than using the hard mask as in previous works, our
experiments show that learning a soft-weighting function for the candidate
phonemes benefits performance. In addition, our proposed g2pW does not require
extra pre-trained POS tagging models while using POS tags as auxiliary features
since we train the POS tagging model simultaneously with the unified encoder.
Experimental results show that our g2pW outperforms existing methods on the
public CPP dataset. All codes, model weights, and a user-friendly package are
publicly available.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. (arXiv:2203.11205v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11205">
<div class="article-summary-box-inner">
<span><p>Mammography, or breast X-ray, is the most widely used imaging modality to
detect cancer and other breast diseases. Recent studies have shown that deep
learning-based computer-assisted detection and diagnosis (CADe or CADx) tools
have been developed to support physicians and improve the accuracy of
interpreting mammography. However, most published datasets of mammography are
either limited on sample size or digitalized from screen-film mammography
(SFM), hindering the development of CADe and CADx tools which are developed
based on full-field digital mammography (FFDM). To overcome this challenge, we
introduce VinDr-Mammo - a new benchmark dataset of FFDM for detecting and
diagnosing breast cancer and other diseases in mammography. The dataset
consists of 5,000 mammography exams, each of which has four standard views and
is double read with disagreement (if any) being resolved by arbitration. It is
created for the assessment of Breast Imaging Reporting and Data System
(BI-RADS) and density at the breast level. In addition, the dataset also
provides the category, location, and BI-RADS assessment of non-benign findings.
We make VinDr-Mammo publicly available on PhysioNet as a new imaging resource
to promote advances in developing CADe and CADx tools for breast cancer
screening.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phase Recognition in Contrast-Enhanced CT Scans based on Deep Learning and Random Sampling. (arXiv:2203.11206v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11206">
<div class="article-summary-box-inner">
<span><p>A fully automated system for interpreting abdominal computed tomography (CT)
scans with multiple phases of contrast enhancement requires an accurate
classification of the phases. This work aims at developing and validating a
precise, fast multi-phase classifier to recognize three main types of contrast
phases in abdominal CT scans. We propose in this study a novel method that uses
a random sampling mechanism on top of deep CNNs for the phase recognition of
abdominal CT scans of four different phases: non-contrast, arterial, venous,
and others. The CNNs work as a slice-wise phase prediction, while the random
sampling selects input slices for the CNN models. Afterward, majority voting
synthesizes the slice-wise results of the CNNs, to provide the final prediction
at scan level. Our classifier was trained on 271,426 slices from 830
phase-annotated CT scans, and when combined with majority voting on 30% of
slices randomly chosen from each scan, achieved a mean F1-score of 92.09% on
our internal test set of 358 scans. The proposed method was also evaluated on 2
external test sets: CTPAC-CCRCC (N = 242) and LiTS (N = 131), which were
annotated by our experts. Although a drop in performance has been observed, the
model performance remained at a high level of accuracy with a mean F1-score of
76.79% and 86.94% on CTPAC-CCRCC and LiTS datasets, respectively. Our
experimental results also showed that the proposed method significantly
outperformed the state-of-the-art 3D approaches while requiring less
computation time for inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Effect of Pre-Processing and Model Complexity for Plastic Analysis Using Short-Wave-Infrared Hyper-Spectral Imaging. (arXiv:2203.11209v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11209">
<div class="article-summary-box-inner">
<span><p>The importance of plastic waste recycling is undeniable. In this respect,
computer vision and deep learning enable solutions through the automated
analysis of short-wave-infrared hyper-spectral images of plastics. In this
paper, we offer an exhaustive empirical study to show the importance of
efficient model selection for resolving the task of hyper-spectral image
segmentation of various plastic flakes using deep learning. We assess the
complexity level of generic and specialized models and infer their performance
capacity: generic models are often unnecessarily complex. We introduce two
variants of a specialized hyper-spectral architecture, PlasticNet, that
outperforms several well-known segmentation architectures in both performance
as well as computational complexity. In addition, we shed lights on the
significance of signal pre-processing within the realm of hyper-spectral
imaging. To complete our contribution, we introduce the largest, most versatile
hyper-spectral dataset of plastic flakes of four primary polymer types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Patterns and Transformations from One Sequence of Images with Shape-invariant Lie Group Transformer. (arXiv:2203.11210v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11210">
<div class="article-summary-box-inner">
<span><p>An effective way to model the complex real world is to view the world as a
composition of basic components of objects and transformations. Although humans
through development understand the compositionality of the real world, it is
extremely difficult to equip robots with such a learning mechanism. In recent
years, there has been significant research on autonomously learning
representations of the world using the deep learning; however, most studies
have taken a statistical approach, which requires a large number of training
data. Contrary to such existing methods, we take a novel algebraic approach for
representation learning based on a simpler and more intuitive formulation that
the observed world is the combination of multiple independent patterns and
transformations that are invariant to the shape of patterns. Since the shape of
patterns can be viewed as the invariant features against symmetric
transformations such as translation or rotation, we can expect that the
patterns can naturally be extracted by expressing transformations with
symmetric Lie group transformers and attempting to reconstruct the scene with
them. Based on this idea, we propose a model that disentangles the scenes into
the minimum number of basic components of patterns and Lie transformations from
only one sequence of images, by introducing the learnable shape-invariant Lie
group transformers as transformation components. Experiments show that given
one sequence of images in which two objects are moving independently, the
proposed model can discover the hidden distinct objects and multiple
shape-invariant transformations that constitute the scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ME-Net: Multi-Encoder Net Framework for Brain Tumor Segmentation. (arXiv:2203.11213v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11213">
<div class="article-summary-box-inner">
<span><p>Glioma is the most common and aggressive brain tumor. Magnetic resonance
imaging (MRI) plays a vital role to evaluate tumors for the arrangement of
tumor surgery and the treatment of subsequent procedures. However, the manual
segmentation of the MRI image is strenuous, which limits its clinical
application. With the development of deep learning, a large number of automatic
segmentation methods have been developed, but most of them stay in 2D images,
which leads to subpar performance. Moreover, the serious voxel imbalance
between the brain tumor and the background as well as the different sizes and
locations of the brain tumor makes the segmentation of 3D images a challenging
problem. Aiming at segmenting 3D MRI, we propose a model for brain tumor
segmentation with multiple encoders. The structure contains four encoders and
one decoder. The four encoders correspond to the four modalities of the MRI
image, perform one-to-one feature extraction, and then merge the feature maps
of the four modalities into the decoder. This method reduces the difficulty of
feature extraction and greatly improves model performance. We also introduced a
new loss function named "Categorical Dice", and set different weights for
different segmented regions at the same time, which solved the problem of voxel
imbalance. We evaluated our approach using the online BraTS 2020 Challenge
verification. Our proposed method can achieve promising results in the
validation set compared to the state-of-the-art approaches with Dice scores of
0.70249, 0.88267, and 0.73864 for the intact tumor, tumor core, and enhanced
tumor, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A survey on GANs for computer vision: Recent research, analysis and taxonomy. (arXiv:2203.11242v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11242">
<div class="article-summary-box-inner">
<span><p>In the last few years, there have been several revolutions in the field of
deep learning, mainly headlined by the large impact of Generative Adversarial
Networks (GANs). GANs not only provide an unique architecture when defining
their models, but also generate incredible results which have had a direct
impact on society. Due to the significant improvements and new areas of
research that GANs have brought, the community is constantly coming up with new
researches that make it almost impossible to keep up with the times. Our survey
aims to provide a general overview of GANs, showing the latest architectures,
optimizations of the loss functions, validation metrics and application areas
of the most widely recognized variants. The efficiency of the different
variants of the model architecture will be evaluated, as well as showing the
best application area; as a vital part of the process, the different metrics
for evaluating the performance of GANs and the frequently used loss functions
will be analyzed. The final objective of this survey is to provide a summary of
the evolution and performance of the GANs which are having better results to
guide future researchers in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contribution of Different Handwriting Modalities to Differential Diagnosis of Parkinson's Disease. (arXiv:2203.11269v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11269">
<div class="article-summary-box-inner">
<span><p>In this paper, we evaluate the contribution of different handwriting
modalities to the diagnosis of Parkinson's disease. We analyse on-surface
movement, in-air movement and pressure exerted on the tablet surface.
Especially in-air movement and pressure-based features have been rarely taken
into account in previous studies. We show that pressure and in-air movement
also possess information that is relevant for the diagnosis of Parkinson's
Disease (PD) from handwriting. In addition to the conventional kinematic and
spatio-temporal features, we present a group of the novel features based on
entropy and empirical mode decomposition of the handwriting signal. The
presented results indicate that handwriting can be used as biomarker for PD
providing classification performance around 89% area under the ROC curve (AUC)
for PD classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction. (arXiv:2203.11283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11283">
<div class="article-summary-box-inner">
<span><p>While NeRF has shown great success for neural reconstruction and rendering,
its limited MLP capacity and long per-scene optimization times make it
challenging to model large-scale indoor scenes. In contrast, classical 3D
reconstruction methods can handle large-scale scenes but do not produce
realistic renderings. We propose NeRFusion, a method that combines the
advantages of NeRF and TSDF-based fusion techniques to achieve efficient
large-scale reconstruction and photo-realistic rendering. We process the input
image sequence to predict per-frame local radiance fields via direct network
inference. These are then fused using a novel recurrent neural network that
incrementally reconstructs a global, sparse scene representation in real-time
at 22 fps. This global volume can be further fine-tuned to boost rendering
quality. We demonstrate that NeRFusion achieves state-of-the-art quality on
both large-scale indoor and small-scale object scenes, with substantially
faster reconstruction than NeRF and other recent methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Contrastive Objective for Learning Disentangled Representations. (arXiv:2203.11284v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11284">
<div class="article-summary-box-inner">
<span><p>Learning representations of images that are invariant to sensitive or
unwanted attributes is important for many tasks including bias removal and
cross domain retrieval. Here, our objective is to learn representations that
are invariant to the domain (sensitive attribute) for which labels are
provided, while being informative over all other image attributes, which are
unlabeled. We present a new approach, proposing a new domain-wise contrastive
objective for ensuring invariant representations. This objective crucially
restricts negative image pairs to be drawn from the same domain, which enforces
domain invariance whereas the standard contrastive objective does not. This
domain-wise objective is insufficient on its own as it suffers from shortcut
solutions resulting in feature suppression. We overcome this issue by a
combination of a reconstruction constraint, image augmentations and
initialization with pre-trained weights. Our analysis shows that the choice of
augmentations is important, and that a misguided choice of augmentations can
harm the invariance and informativeness objectives. In an extensive evaluation,
our method convincingly outperforms the state-of-the-art in terms of
representation invariance, representation informativeness, and training speed.
Furthermore, we find that in some cases our method can achieve excellent
results even without the reconstruction constraint, leading to a much faster
and resource efficient training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Network for Future Hand Segmentation from Egocentric Video. (arXiv:2203.11305v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11305">
<div class="article-summary-box-inner">
<span><p>We introduce the novel problem of anticipating a time series of future hand
masks from egocentric video. A key challenge is to model the stochasticity of
future head motions, which globally impact the head-worn camera video analysis.
To this end, we propose a novel deep generative model -- EgoGAN, which uses a
3D Fully Convolutional Network to learn a spatio-temporal video representation
for pixel-wise visual anticipation, generates future head motion using
Generative Adversarial Network (GAN), and then predicts the future hand masks
based on the video representation and the generated future head motion. We
evaluate our method on both the EPIC-Kitchens and the EGTEA Gaze+ datasets. We
conduct detailed ablation studies to validate the design choices of our
approach. Furthermore, we compare our method with previous state-of-the-art
methods on future image segmentation and show that our method can more
accurately predict future hand masks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Quantised Neural Networks with STE Variants: the Additive Noise Annealing Algorithm. (arXiv:2203.11323v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11323">
<div class="article-summary-box-inner">
<span><p>Training quantised neural networks (QNNs) is a non-differentiable
optimisation problem since weights and features are output by piecewise
constant functions. The standard solution is to apply the straight-through
estimator (STE), using different functions during the inference and gradient
computation steps. Several STE variants have been proposed in the literature
aiming to maximise the task accuracy of the trained network. In this paper, we
analyse STE variants and study their impact on QNN training. We first observe
that most such variants can be modelled as stochastic regularisations of stair
functions; although this intuitive interpretation is not new, our rigorous
discussion generalises to further variants. Then, we analyse QNNs mixing
different regularisations, finding that some suitably synchronised smoothing of
each layer map is required to guarantee pointwise compositional convergence to
the target discontinuous function. Based on these theoretical insights, we
propose additive noise annealing (ANA), a new algorithm to train QNNs
encompassing standard STE and its variants as special cases. When testing ANA
on the CIFAR-10 image classification benchmark, we find that the major impact
on task accuracy is not due to the qualitative shape of the regularisations but
to the proper synchronisation of the different STE variants used in a network,
in accordance with the theoretical results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Matching with Overlapping Attention for Optical Flow Estimation. (arXiv:2203.11335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11335">
<div class="article-summary-box-inner">
<span><p>Optical flow estimation is a fundamental task in computer vision. Recent
direct-regression methods using deep neural networks achieve remarkable
performance improvement. However, they do not explicitly capture long-term
motion correspondences and thus cannot handle large motions effectively. In
this paper, inspired by the traditional matching-optimization methods where
matching is introduced to handle large displacements before energy-based
optimizations, we introduce a simple but effective global matching step before
the direct regression and develop a learning-based matching-optimization
framework, namely GMFlowNet. In GMFlowNet, global matching is efficiently
calculated by applying argmax on 4D cost volumes. Additionally, to improve the
matching quality, we propose patch-based overlapping attention to extract large
context features. Extensive experiments demonstrate that GMFlowNet outperforms
RAFT, the most popular optimization-only method, by a large margin and achieves
state-of-the-art performance on standard benchmarks. Thanks to the matching and
overlapping attention, GMFlowNet obtains major improvements on the predictions
for textureless regions and large motions. Our code is made publicly available
at https://github.com/xiaofeng94/GMFlowNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmenting Medical Instruments in Minimally Invasive Surgeries using AttentionMask. (arXiv:2203.11358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11358">
<div class="article-summary-box-inner">
<span><p>Precisely locating and segmenting medical instruments in images of minimally
invasive surgeries, medical instrument segmentation, is an essential first step
for several tasks in medical image processing. However, image degradations,
small instruments, and the generalization between different surgery types make
medical instrument segmentation challenging. To cope with these challenges, we
adapt the object proposal generation system AttentionMask and propose a
dedicated post-processing to select promising proposals. The results on the
ROBUST-MIS Challenge 2019 show that our adapted AttentionMask system is a
strong foundation for generating state-of-the-art performance. Our evaluation
in an object proposal generation framework shows that our adapted AttentionMask
system is robust to image degradations, generalizes well to unseen types of
surgeries, and copes well with small instruments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio visual character profiles for detecting background characters in entertainment media. (arXiv:2203.11368v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11368">
<div class="article-summary-box-inner">
<span><p>An essential goal of computational media intelligence is to support
understanding how media stories -- be it news, commercial or entertainment
media -- represent and reflect society and these portrayals are perceived.
People are a central element of media stories. This paper focuses on
understanding the representation and depiction of background characters in
media depictions, primarily movies and TV shows. We define the background
characters as those who do not participate vocally in any scene throughout the
movie and address the problem of localizing background characters in videos. We
use an active speaker localization system to extract high-confidence
face-speech associations and generate audio-visual profiles for talking
characters in a movie by automatically clustering them. Using a face
verification system, we then prune all the face-tracks which match any of the
generated character profiles and obtain the background character face-tracks.
We curate a background character dataset which provides annotations for
background character for a set of TV shows, and use it to evaluate the
performance of the background character detection framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperShot: Few-Shot Learning by Kernel HyperNetworks. (arXiv:2203.11378v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11378">
<div class="article-summary-box-inner">
<span><p>Few-shot models aim at making predictions using a minimal number of labeled
examples from a given task. The main challenge in this area is the one-shot
setting where only one element represents each class. We propose HyperShot -
the fusion of kernels and hypernetwork paradigm. Compared to reference
approaches that apply a gradient-based adjustment of the parameters, our model
aims to switch the classification module parameters depending on the task's
embedding. In practice, we utilize a hypernetwork, which takes the aggregated
information from support data and returns the classifier's parameters
handcrafted for the considered problem. Moreover, we introduce the kernel-based
representation of the support examples delivered to hypernetwork to create the
parameters of the classification module. Consequently, we rely on relations
between embeddings of the support examples instead of direct feature values
provided by the backbone models. Thanks to this approach, our model can adapt
to highly different tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survival Analysis for Idiopathic Pulmonary Fibrosis using CT Images and Incomplete Clinical Data. (arXiv:2203.11391v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11391">
<div class="article-summary-box-inner">
<span><p>Idiopathic Pulmonary Fibrosis (IPF) is an inexorably progressive fibrotic
lung disease with a variable and unpredictable rate of progression. CT scans of
the lungs inform clinical assessment of IPF patients and contain pertinent
information related to disease progression. In this work, we propose a
multi-modal method that uses neural networks and memory banks to predict the
survival of IPF patients using clinical and imaging data. The majority of
clinical IPF patient records have missing data (e.g. missing lung function
tests). To this end, we propose a probabilistic model that captures the
dependencies between the observed clinical variables and imputes missing ones.
This principled approach to missing data imputation can be naturally combined
with a deep survival analysis model. We show that the proposed framework yields
significantly better survival analysis results than baselines in terms of
concordance index and integrated Brier score. Our work also provides insights
into novel image-based biomarkers that are linked to mortality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Convex Objects Image Segmentation via Proximal Alternating Direction Method of Multipliers. (arXiv:2203.11395v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11395">
<div class="article-summary-box-inner">
<span><p>This paper focuses on the issue of image segmentation with convex shape
prior. Firstly, we use binary function to represent convex object(s). The
convex shape prior turns out to be a simple quadratic inequality constraint on
the binary indicator function associated with each object. An image
segmentation model incorporating convex shape prior into a probability-based
method is proposed. Secondly, a new algorithm is designed to solve involved
optimization problem, which is a challenging task because of the quadratic
inequality constraint. To tackle this difficulty, we relax and linearize the
quadratic inequality constraint to reduce it to solve a sequence of convex
minimization problems. For each convex problem, an efficient proximal
alternating direction method of multipliers is developed to solve it. The
convergence of the algorithm follows some existing results in the optimization
literature. Moreover, an interactive procedure is introduced to improve the
accuracy of segmentation gradually. Numerical experiments on natural and
medical images demonstrate that the proposed method is superior to some
existing methods in terms of segmentation accuracy and computational time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Real World Dataset for Multi-view 3D Reconstruction. (arXiv:2203.11397v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11397">
<div class="article-summary-box-inner">
<span><p>We present a dataset of 371 3D models of everyday tabletop objects along with
their 320,000 real world RGB and depth images. Accurate annotations of camera
poses and object poses for each image are performed in a semi-automated fashion
to facilitate the use of the dataset for myriad 3D applications like shape
reconstruction, object pose estimation, shape retrieval etc. We primarily focus
on learned multi-view 3D reconstruction due to the lack of appropriate real
world benchmark for the task and demonstrate that our dataset can fill that
gap. The entire annotated dataset along with the source code for the annotation
tools and evaluation baselines will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hindsight is 20/20: Leveraging Past Traversals to Aid 3D Perception. (arXiv:2203.11405v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11405">
<div class="article-summary-box-inner">
<span><p>Self-driving cars must detect vehicles, pedestrians, and other traffic
participants accurately to operate safely. Small, far-away, or highly occluded
objects are particularly challenging because there is limited information in
the LiDAR point clouds for detecting them. To address this challenge, we
leverage valuable information from the past: in particular, data collected in
past traversals of the same scene. We posit that these past data, which are
typically discarded, provide rich contextual information for disambiguating the
above-mentioned challenging cases. To this end, we propose a novel, end-to-end
trainable Hindsight framework to extract this contextual information from past
traversals and store it in an easy-to-query data structure, which can then be
leveraged to aid future 3D object detection of the same scene. We show that
this framework is compatible with most modern 3D detection architectures and
can substantially improve their average precision on multiple autonomous
driving datasets, most notably by more than 300% on the challenging cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gated Domain-Invariant Feature Disentanglement for Domain Generalizable Object Detection. (arXiv:2203.11432v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11432">
<div class="article-summary-box-inner">
<span><p>For Domain Generalizable Object Detection (DGOD), Disentangled Representation
Learning (DRL) helps a lot by explicitly disentangling Domain-Invariant
Representations (DIR) from Domain-Specific Representations (DSR). Considering
the domain category is an attribute of input data, it should be feasible for
networks to fit a specific mapping which projects DSR into feature channels
exclusive to domain-specific information, and thus much cleaner disentanglement
of DIR from DSR can be achieved simply on channel dimension. Inspired by this
idea, we propose a novel DRL method for DGOD, which is termed Gated
Domain-Invariant Feature Disentanglement (GDIFD). In GDIFD, a Channel Gate
Module (CGM) learns to output channel gate signals close to either 0 or 1,
which can mask out the channels exclusive to domain-specific information
helpful for domain recognition. With the proposed GDIFD, the backbone in our
framework can fit the desired mapping easily, which enables the channel-wise
disentanglement. In experiments, we demonstrate that our approach is highly
effective and achieves state-of-the-art DGOD performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making DeepFakes more spurious: evading deep face forgery detection via trace removal attack. (arXiv:2203.11433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11433">
<div class="article-summary-box-inner">
<span><p>DeepFakes are raising significant social concerns. Although various DeepFake
detectors have been developed as forensic countermeasures, these detectors are
still vulnerable to attacks. Recently, a few attacks, principally adversarial
attacks, have succeeded in cloaking DeepFake images to evade detection.
However, these attacks have typical detector-specific designs, which require
prior knowledge about the detector, leading to poor transferability. Moreover,
these attacks only consider simple security scenarios. Less is known about how
effective they are in high-level scenarios where either the detectors or the
attacker's knowledge varies. In this paper, we solve the above challenges with
presenting a novel detector-agnostic trace removal attack for DeepFake
anti-forensics. Instead of investigating the detector side, our attack looks
into the original DeepFake creation pipeline, attempting to remove all
detectable natural DeepFake traces to render the fake images more "authentic".
To implement this attack, first, we perform a DeepFake trace discovery,
identifying three discernible traces. Then a trace removal network (TR-Net) is
proposed based on an adversarial learning framework involving one generator and
multiple discriminators. Each discriminator is responsible for one individual
trace representation to avoid cross-trace interference. These discriminators
are arranged in parallel, which prompts the generator to remove various traces
simultaneously. To evaluate the attack efficacy, we crafted heterogeneous
security scenarios where the detectors were embedded with different levels of
defense and the attackers' background knowledge of data varies. The
experimental results show that the proposed attack can significantly compromise
the detection accuracy of six state-of-the-art DeepFake detectors while causing
only a negligible loss in visual quality to the original DeepFake samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Representation Learning as Multimodal Variational Inference. (arXiv:2203.11437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11437">
<div class="article-summary-box-inner">
<span><p>This paper proposes a probabilistic extension of SimSiam, a recent
self-supervised learning (SSL) method. SimSiam trains a model by maximizing the
similarity between image representations of different augmented views of the
same image. Although uncertainty-aware machine learning has been getting
general like deep variational inference, SimSiam and other SSL are
insufficiently uncertainty-aware, which could lead to limitations on its
potential. The proposed extension is to make SimSiam uncertainty-aware based on
variational inference. Our main contributions are twofold: Firstly, we clarify
the theoretical relationship between non-contrastive SSL and multimodal
variational inference. Secondly, we introduce a novel SSL called variational
inference SimSiam (VI-SimSiam), which incorporates the uncertainty by involving
spherical posterior distributions. Our experiment shows that VI-SimSiam
outperforms SimSiam in classification tasks in ImageNette and ImageWoof by
successfully estimating the representation uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Learning for AU Detection Based on Multi-Head Fused Transformers. (arXiv:2203.11441v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11441">
<div class="article-summary-box-inner">
<span><p>Multi-modal learning has been intensified in recent years, especially for
applications in facial analysis and action unit detection whilst there still
exist two main challenges in terms of 1) relevant feature learning for
representation and 2) efficient fusion for multi-modalities. Recently, there
are a number of works have shown the effectiveness in utilizing the attention
mechanism for AU detection, however, most of them are binding the region of
interest (ROI) with features but rarely apply attention between features of
each AU. On the other hand, the transformer, which utilizes a more efficient
self-attention mechanism, has been widely used in natural language processing
and computer vision tasks but is not fully explored in AU detection tasks. In
this paper, we propose a novel end-to-end Multi-Head Fused Transformer (MFT)
method for AU detection, which learns AU encoding features representation from
different modalities by transformer encoder and fuses modalities by another
fusion transformer module. Multi-head fusion attention is designed in the
fusion transformer module for the effective fusion of multiple modalities. Our
approach is evaluated on two public multi-modal AU databases, BP4D, and BP4D+,
and the results are superior to the state-of-the-art algorithms and baseline
models. We further analyze the performance of AU detection from different
modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Associating Objects with Scalable Transformers for Video Object Segmentation. (arXiv:2203.11442v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11442">
<div class="article-summary-box-inner">
<span><p>This paper investigates how to realize better and more efficient embedding
learning to tackle the semi-supervised video object segmentation under
challenging multi-object scenarios. The state-of-the-art methods learn to
decode features with a single positive object and thus have to match and
segment each target separately under multi-object scenarios, consuming multiple
times computation resources. To solve the problem, we propose an Associating
Objects with Transformers (AOT) approach to match and decode multiple objects
jointly and collaboratively. In detail, AOT employs an identification mechanism
to associate multiple targets into the same high-dimensional embedding space.
Thus, we can simultaneously process multiple objects' matching and segmentation
decoding as efficiently as processing a single object. To sufficiently model
multi-object association, a Long Short-Term Transformer (LSTT) is devised to
construct hierarchical matching and propagation. Based on AOT, we further
propose a more flexible and robust framework, Associating Objects with Scalable
Transformers (AOST), in which a scalable version of LSTT is designed to enable
run-time adaptation of accuracy-efficiency trade-offs. Besides, AOST introduces
a better layer-wise manner to couple identification and vision embeddings. We
conduct extensive experiments on multi-object and single-object benchmarks to
examine AOT series frameworks. Compared to the state-of-the-art competitors,
our methods can maintain times of run-time efficiency with superior
performance. Notably, we achieve new state-of-the-art performance on three
popular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test
(87.0%/84.7%), and DAVIS 2016 (93.0%). Project page:
https://github.com/z-x-yang/AOT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Manipulating UAV Imagery for Satellite Model Training, Calibration and Testing. (arXiv:2203.11447v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11447">
<div class="article-summary-box-inner">
<span><p>Modern livestock farming is increasingly data driven and frequently relies on
efficient remote sensing to gather data over wide areas. High resolution
satellite imagery is one such data source, which is becoming more accessible
for farmers as coverage increases and cost falls. Such images can be used to
detect and track animals, monitor pasture changes, and understand land use.
Many of the data driven models being applied to these tasks require ground
truthing at resolutions higher than satellites can provide. Simultaneously,
there is a lack of available aerial imagery focused on farmland changes that
occur over days or weeks, such as herd movement. With this goal in mind, we
present a new multi-temporal dataset of high resolution UAV imagery which is
artificially degraded to match satellite data quality. An empirical blurring
metric is used to calibrate the degradation process against actual satellite
imagery of the area. UAV surveys were flown repeatedly over several weeks, for
specific farm locations. This 5cm/pixel data is sufficiently high resolution to
accurately ground truth cattle locations, and other factors such as grass
cover. From 33 wide area UAV surveys, 1869 patches were extracted and
artificially degraded using an accurate satellite optical model to simulate
satellite data. Geographic patches from multiple time periods are aligned and
presented as sets, providing a multi-temporal dataset that can be used for
detecting changes on farms. The geo-referenced images and 27,853 manually
annotated cattle labels are made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Textures in Zero-shot Understanding of Fine-Grained Domains. (arXiv:2203.11449v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11449">
<div class="article-summary-box-inner">
<span><p>Textures can be used to describe the appearance of objects in a wide range of
fine-grained domains. Textures are localized and one can often refer to their
properties in a manner that is independent of the object identity. Moreover,
there is a rich vocabulary to describe textures corresponding to properties
such as their color, pattern, structure, periodicity, stochasticity, and
others. Motivated by this, we study the effectiveness of large-scale language
and vision models (e.g., CLIP) at recognizing texture attributes in natural
images. We first conduct a systematic study of CLIP on texture datasets where
we find that it has good coverage for a wide range of texture terms. CLIP can
also handle compositional phrases that consist of color and pattern terms
(e.g., red dots or yellow stripes). We then show how these attributes allow for
zero-shot fine-grained categorization on existing datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DepthGAN: GAN-based Depth Generation of Indoor Scenes from Semantic Layouts. (arXiv:2203.11453v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11453">
<div class="article-summary-box-inner">
<span><p>Limited by the computational efficiency and accuracy, generating complex 3D
scenes remains a challenging problem for existing generation networks. In this
work, we propose DepthGAN, a novel method of generating depth maps with only
semantic layouts as input. First, we introduce a well-designed cascade of
transformer blocks as our generator to capture the structural correlations in
depth maps, which makes a balance between global feature aggregation and local
attention. Meanwhile, we propose a cross-attention fusion module to guide edge
preservation efficiently in depth generation, which exploits additional
appearance supervision information. Finally, we conduct extensive experiments
on the perspective views of the Structured3d panorama dataset and demonstrate
that our DepthGAN achieves superior performance both on quantitative results
and visual effects in the depth generation task.Furthermore, 3D indoor scenes
can be reconstructed by our generated depth maps with reasonable structure and
spatial coherency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization. (arXiv:2203.11471v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11471">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute
human pose estimation with calibrated camera. Accurate and generalizable
absolute 3D human pose estimation from monocular 2D pose input is an ill-posed
problem. To address this challenge, we convert the input from pixel space to 3D
normalized rays. This conversion makes our approach robust to camera intrinsic
parameter changes. To deal with the in-the-wild camera extrinsic parameter
variations, Ray3D explicitly takes the camera extrinsic parameters as an input
and jointly models the distribution between the 3D pose rays and camera
extrinsic parameters. This novel network design is the key to the outstanding
generalizability of Ray3D approach. To have a comprehensive understanding of
how the camera intrinsic and extrinsic parameter variations affect the accuracy
of absolute 3D key-point localization, we conduct in-depth systematic
experiments on three single person 3D benchmarks as well as one synthetic
benchmark. These experiments demonstrate that our method significantly
outperforms existing state-of-the-art models. Our code and the synthetic
dataset are available at https://github.com/YxZhxn/Ray3D .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Remember Intentions: Retrospective-Memory-based Trajectory Prediction. (arXiv:2203.11474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11474">
<div class="article-summary-box-inner">
<span><p>To realize trajectory prediction, most previous methods adopt the
parameter-based approach, which encodes all the seen past-future instance pairs
into model parameters. However, in this way, the model parameters come from all
seen instances, which means a huge amount of irrelevant seen instances might
also involve in predicting the current situation, disturbing the performance.
To provide a more explicit link between the current situation and the seen
instances, we imitate the mechanism of retrospective memory in neuropsychology
and propose MemoNet, an instance-based approach that predicts the movement
intentions of agents by looking for similar scenarios in the training data. In
MemoNet, we design a pair of memory banks to explicitly store representative
instances in the training set, acting as prefrontal cortex in the neural
system, and a trainable memory addresser to adaptively search a current
situation with similar instances in the memory bank, acting like basal ganglia.
During prediction, MemoNet recalls previous memory by using the memory
addresser to index related instances in the memory bank. We further propose a
two-step trajectory prediction system, where the first step is to leverage
MemoNet to predict the destination and the second step is to fulfill the whole
trajectory according to the predicted destinations. Experiments show that the
proposed MemoNet improves the FDE by 20.3%/10.2%/28.3% from the previous best
method on SDD/ETH-UCY/NBA datasets. Experiments also show that our MemoNet has
the ability to trace back to specific instances during prediction, promoting
more interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11480">
<div class="article-summary-box-inner">
<span><p>Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed Differential Privacy in Computer Vision. (arXiv:2203.11481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11481">
<div class="article-summary-box-inner">
<span><p>We introduce AdaMix, an adaptive differentially private algorithm for
training deep neural network classifiers using both private and public image
data. While pre-training language models on large public datasets has enabled
strong differential privacy (DP) guarantees with minor loss of accuracy, a
similar practice yields punishing trade-offs in vision tasks. A few-shot or
even zero-shot learning baseline that ignores private data can outperform
fine-tuning on a large private dataset. AdaMix incorporates few-shot training,
or cross-modal zero-shot learning, on public data prior to private fine-tuning,
to improve the trade-off. AdaMix reduces the error increase from the
non-private upper bound from the 167-311\% of the baseline, on average across 6
datasets, to 68-92\% depending on the desired privacy level selected by the
user. AdaMix tackles the trade-off arising in visual classification, whereby
the most privacy sensitive data, corresponding to isolated points in
representation space, are also critical for high classification accuracy. In
addition, AdaMix comes with strong theoretical privacy guarantees and
convergence analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation. (arXiv:2203.11483v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11483">
<div class="article-summary-box-inner">
<span><p>With the advent of convolutional neural networks, stereo matching algorithms
have recently gained tremendous progress. However, it remains a great challenge
to accurately extract disparities from real-world image pairs taken by
consumer-level devices like smartphones, due to practical complicating factors
such as thin structures, non-ideal rectification, camera module inconsistencies
and various hard-case scenes. In this paper, we propose a set of innovative
designs to tackle the problem of practical stereo matching: 1) to better
recover fine depth details, we design a hierarchical network with recurrent
refinement to update disparities in a coarse-to-fine manner, as well as a
stacked cascaded architecture for inference; 2) we propose an adaptive group
correlation layer to mitigate the impact of erroneous rectification; 3) we
introduce a new synthetic dataset with special attention to difficult cases for
better generalizing to real-world scenes. Our results not only rank 1st on both
Middlebury and ETH3D benchmarks, outperforming existing state-of-the-art
methods by a notable margin, but also exhibit high-quality details for
real-life photos, which clearly demonstrates the efficacy of our contributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSD-KD: A Self-supervised Diverse Knowledge Distillation Method for Lightweight Skin Lesion Classification Using Dermoscopic Images. (arXiv:2203.11490v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11490">
<div class="article-summary-box-inner">
<span><p>Skin cancer is one of the most common types of malignancy, affecting a large
population and causing a heavy economic burden worldwide. Over the last few
years, computer-aided diagnosis has been rapidly developed and make great
progress in healthcare and medical practices due to the advances in artificial
intelligence. However, most studies in skin cancer detection keep pursuing high
prediction accuracies without considering the limitation of computing resources
on portable devices. In this case, knowledge distillation (KD) has been proven
as an efficient tool to help improve the adaptability of lightweight models
under limited resources, meanwhile keeping a high-level representation
capability. To bridge the gap, this study specifically proposes a novel method,
termed SSD-KD, that unifies diverse knowledge into a generic KD framework for
skin diseases classification. Our method models an intra-instance relational
feature representation and integrates it with existing KD research. A dual
relational knowledge distillation architecture is self-supervisedly trained
while the weighted softened outputs are also exploited to enable the student
model to capture richer knowledge from the teacher model. To demonstrate the
effectiveness of our method, we conduct experiments on ISIC 2019, a large-scale
open-accessed benchmark of skin diseases dermoscopic images. Experiments show
that our distilled lightweight model can achieve an accuracy as high as 85% for
the classification tasks of 8 different skin diseases with minimal parameters
and computing requirements. Ablation studies confirm the effectiveness of our
intra- and inter-instance relational knowledge integration strategy. Compared
with state-of-the-art knowledge distillation techniques, the proposed method
demonstrates improved performances for multi-diseases classification on the
large-scale dermoscopy database.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FrameHopper: Selective Processing of Video Frames in Detection-driven Real-Time Video Analytics. (arXiv:2203.11493v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11493">
<div class="article-summary-box-inner">
<span><p>Detection-driven real-time video analytics require continuous detection of
objects contained in the video frames using deep learning models like YOLOV3,
EfficientDet. However, running these detectors on each and every frame in
resource-constrained edge devices is computationally intensive. By taking the
temporal correlation between consecutive video frames into account, we note
that detection outputs tend to be overlapping in successive frames. Elimination
of similar consecutive frames will lead to a negligible drop in performance
while offering significant performance benefits by reducing overall computation
and communication costs. The key technical questions are, therefore, (a) how to
identify which frames to be processed by the object detector, and (b) how many
successive frames can be skipped (called skip-length) once a frame is selected
to be processed. The overall goal of the process is to keep the error due to
skipping frames as small as possible. We introduce a novel error vs processing
rate optimization problem with respect to the object detection task that
balances between the error rate and the fraction of frames filtering.
Subsequently, we propose an off-line Reinforcement Learning (RL)-based
algorithm to determine these skip-lengths as a state-action policy of the RL
agent from a recorded video and then deploy the agent online for live video
streams. To this end, we develop FrameHopper, an edge-cloud collaborative video
analytics framework, that runs a lightweight trained RL agent on the camera and
passes filtered frames to the server where the object detection model runs for
a set of applications. We have tested our approach on a number of live videos
captured from real-life scenarios and show that FrameHopper processes only a
handful of frames but produces detection results closer to the oracle solution
and outperforms recent state-of-the-art solutions in most cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers. (arXiv:2203.11496v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11496">
<div class="article-summary-box-inner">
<span><p>LiDAR and camera are two important sensors for 3D object detection in
autonomous driving. Despite the increasing popularity of sensor fusion in this
field, the robustness against inferior image conditions, e.g., bad illumination
and sensor misalignment, is under-explored. Existing fusion methods are easily
affected by such conditions, mainly due to a hard association of LiDAR points
and image pixels, established by calibration matrices. We propose TransFusion,
a robust solution to LiDAR-camera fusion with a soft-association mechanism to
handle inferior image conditions. Specifically, our TransFusion consists of
convolutional backbones and a detection head based on a transformer decoder.
The first layer of the decoder predicts initial bounding boxes from a LiDAR
point cloud using a sparse set of object queries, and its second decoder layer
adaptively fuses the object queries with useful image features, leveraging both
spatial and contextual relationships. The attention mechanism of the
transformer enables our model to adaptively determine where and what
information should be taken from the image, leading to a robust and effective
fusion strategy. We additionally design an image-guided query initialization
strategy to deal with objects that are difficult to detect in point clouds.
TransFusion achieves state-of-the-art performance on large-scale datasets. We
provide extensive experiments to demonstrate its robustness against degenerated
image quality and calibration errors. We also extend the proposed method to the
3D tracking task and achieve the 1st place in the leaderboard of nuScenes
tracking, showing its effectiveness and generalization capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rebalanced Siamese Contrastive Mining for Long-Tailed Recognition. (arXiv:2203.11506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11506">
<div class="article-summary-box-inner">
<span><p>Deep neural networks perform poorly on heavily class-imbalanced datasets.
Given the promising performance of contrastive learning, we propose
$\mathbf{Re}$balanced $\mathbf{S}$iamese $\mathbf{Co}$ntrastive
$\mathbf{m}$ining ( $\mathbf{ResCom}$) to tackle imbalanced recognition. Based
on the mathematical analysis and simulation results, we claim that supervised
contrastive learning suffers a dual class-imbalance problem at both the
original batch and Siamese batch levels, which is more serious than long-tailed
classification learning. In this paper, at the original batch level, we
introduce a class-balanced supervised contrastive loss to assign adaptive
weights for different classes. At the Siamese batch level, we present a
class-balanced queue, which maintains the same number of keys for all classes.
Furthermore, we note that the contrastive loss gradient with respect to the
contrastive logits can be decoupled into the positives and negatives, and easy
positives and easy negatives will make the contrastive gradient vanish. We
propose supervised hard positive and negative pairs mining to pick up
informative pairs for contrastive computation and improve representation
learning. Finally, to approximately maximize the mutual information between the
two views, we propose Siamese Balanced Softmax and joint it with the
contrastive loss for one-stage training. ResCom outperforms the previous
methods by large margins on multiple long-tailed recognition benchmarks. Our
code will be made publicly available at:
https://github.com/dvlab-research/ResCom.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Deraining: Where Contrastive Learning Meets Self-similarity. (arXiv:2203.11509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11509">
<div class="article-summary-box-inner">
<span><p>Image deraining is a typical low-level image restoration task, which aims at
decomposing the rainy image into two distinguishable layers: the clean image
layer and the rain layer. Most of the existing learning-based deraining methods
are supervisedly trained on synthetic rainy-clean pairs. The domain gap between
the synthetic and real rains makes them less generalized to different real
rainy scenes. Moreover, the existing methods mainly utilize the property of the
two layers independently, while few of them have considered the mutually
exclusive relationship between the two layers. In this work, we propose a novel
non-local contrastive learning (NLCL) method for unsupervised image deraining.
Consequently, we not only utilize the intrinsic self-similarity property within
samples but also the mutually exclusive property between the two layers, so as
to better differ the rain layer from the clean image. Specifically, the
non-local self-similarity image layer patches as the positives are pulled
together and similar rain layer patches as the negatives are pushed away. Thus
the similar positive/negative samples that are close in the original space
benefit us to enrich more discriminative representation. Apart from the
self-similarity sampling strategy, we analyze how to choose an appropriate
feature encoder in NLCL. Extensive experiments on different real rainy datasets
demonstrate that the proposed method obtains state-of-the-art performance in
real deraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Network-based Efficient Dense Point Cloud Generation using Unsigned Distance Fields. (arXiv:2203.11537v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11537">
<div class="article-summary-box-inner">
<span><p>Dense point cloud generation from a sparse or incomplete point cloud is a
crucial and challenging problem in 3D computer vision and computer graphics. So
far, the existing methods are either computationally too expensive, suffer from
limited resolution, or both. In addition, some methods are strictly limited to
watertight surfaces -- another major obstacle for a number of applications. To
address these issues, we propose a lightweight Convolutional Neural Network
that learns and predicts the unsigned distance field for arbitrary 3D shapes
for dense point cloud generation using the recently emerged concept of implicit
function learning. Experiments demonstrate that the proposed architecture
achieves slightly better quality results than the state of the art with 87%
less model parameters and 40% less GPU memory usage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask Usage Recognition using Vision Transformer with Transfer Learning and Data Augmentation. (arXiv:2203.11542v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11542">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has disrupted various levels of society. The use of
masks is essential in preventing the spread of COVID-19 by identifying an image
of a person using a mask. Although only 23.1% of people use masks correctly,
Artificial Neural Networks (ANN) can help classify the use of good masks to
help slow the spread of the Covid-19 virus. However, it requires a large
dataset to train an ANN that can classify the use of masks correctly.
MaskedFace-Net is a suitable dataset consisting of 137016 digital images with 4
class labels, namely Mask, Mask Chin, Mask Mouth Chin, and Mask Nose Mouth.
Mask classification training utilizes Vision Transformers (ViT) architecture
with transfer learning method using pre-trained weights on ImageNet-21k, with
random augmentation. In addition, the hyper-parameters of training of 20
epochs, an Stochastic Gradient Descent (SGD) optimizer with a learning rate of
0.03, a batch size of 64, a Gaussian Cumulative Distribution (GeLU) activation
function, and a Cross-Entropy loss function are used to be applied on the
training of three architectures of ViT, namely Base-16, Large-16, and Huge-14.
Furthermore, comparisons of with and without augmentation and transfer learning
are conducted. This study found that the best classification is transfer
learning and augmentation using ViT Huge-14. Using this method on
MaskedFace-Net dataset, the research reaches an accuracy of 0.9601 on training
data, 0.9412 on validation data, and 0.9534 on test data. This research shows
that training the ViT model with data augmentation and transfer learning
improves classification of the mask usage, even better than convolutional-based
Residual Network (ResNet).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frugal Learning of Virtual Exemplars for Label-Efficient Satellite Image Change Detection. (arXiv:2203.11559v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11559">
<div class="article-summary-box-inner">
<span><p>In this paper, we devise a novel interactive satellite image change detection
algorithm based on active learning. The proposed framework is iterative and
relies on a question and answer model which asks the oracle (user) questions
about the most informative display (subset of critical images), and according
to the user's responses, updates change detections. The contribution of our
framework resides in a novel display model which selects the most
representative and diverse virtual exemplars that adversely challenge the
learned change detection functions, thereby leading to highly discriminating
functions in the subsequent iterations of active learning. Extensive
experiments, conducted on the challenging task of interactive satellite image
change detection, show the superiority of the proposed virtual display model
against the related work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcement-based frugal learning for satellite image change detection. (arXiv:2203.11564v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11564">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel interactive satellite image change
detection algorithm based on active learning. The proposed approach is
iterative and asks the user (oracle) questions about the targeted changes and
according to the oracle's responses updates change detections. We consider a
probabilistic framework which assigns to each unlabeled sample a relevance
measure modeling how critical is that sample when training change detection
functions. These relevance measures are obtained by minimizing an objective
function mixing diversity, representativity and uncertainty. These criteria
when combined allow exploring different data modes and also refining change
detections. To further explore the potential of this objective function, we
consider a reinforcement learning approach that finds the best combination of
diversity, representativity and uncertainty, through active learning
iterations, leading to better generalization as corroborated through
experiments in interactive satellite image change detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-layer Clustering-based Residual Sparsifying Transform for Low-dose CT Image Reconstruction. (arXiv:2203.11565v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11565">
<div class="article-summary-box-inner">
<span><p>The recently proposed sparsifying transform models incur low computational
cost and have been applied to medical imaging. Meanwhile, deep models with
nested network structure reveal great potential for learning features in
different layers. In this study, we propose a network-structured sparsifying
transform learning approach for X-ray computed tomography (CT), which we refer
to as multi-layer clustering-based residual sparsifying transform (MCST)
learning. The proposed MCST scheme learns multiple different unitary transforms
in each layer by dividing each layer's input into several classes. We apply the
MCST model to low-dose CT (LDCT) reconstruction by deploying the learned MCST
model into the regularizer in penalized weighted least squares (PWLS)
reconstruction. We conducted LDCT reconstruction experiments on XCAT phantom
data and Mayo Clinic data and trained the MCST model with 2 (or 3) layers and
with 5 clusters in each layer. The learned transforms in the same layer showed
rich features while additional information is extracted from representation
residuals. Our simulation results demonstrate that PWLS-MCST achieves better
image reconstruction quality than the conventional FBP method and PWLS with
edge-preserving (EP) regularizer. It also outperformed recent advanced methods
like PWLS with a learned multi-layer residual sparsifying transform prior
(MARS) and PWLS with a union of learned transforms (ULTRA), especially for
displaying clear edges and preserving subtle details.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Patch Exiting for Scalable Single Image Super-Resolution. (arXiv:2203.11589v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11589">
<div class="article-summary-box-inner">
<span><p>Since the future of computing is heterogeneous, scalability is a crucial
problem for single image super-resolution. Recent works try to train one
network, which can be deployed on platforms with different capacities. However,
they rely on the pixel-wise sparse convolution, which is not hardware-friendly
and achieves limited practical speedup. As image can be divided into patches,
which have various restoration difficulties, we present a scalable method based
on Adaptive Patch Exiting (APE) to achieve more practical speedup.
Specifically, we propose to train a regressor to predict the incremental
capacity of each layer for the patch. Once the incremental capacity is below
the threshold, the patch can exit at the specific layer. Our method can easily
adjust the trade-off between performance and efficiency by changing the
threshold of incremental capacity. Furthermore, we propose a novel strategy to
enable the network training of our method. We conduct extensive experiments
across various backbones, datasets and scaling factors to demonstrate the
advantages of our method. Code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding Alignment. (arXiv:2203.11590v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11590">
<div class="article-summary-box-inner">
<span><p>This paper investigates the problem of temporally interpolating dynamic 3D
point clouds with large non-rigid deformation. We formulate the problem as
estimation of point-wise trajectories (i.e., smooth curves) and further reason
that temporal irregularity and under-sampling are two major challenges. To
tackle the challenges, we propose IDEA-Net, an end-to-end deep learning
framework, which disentangles the problem under the assistance of the
explicitly learned temporal consistency. Specifically, we propose a temporal
consistency learning module to align two consecutive point cloud frames
point-wisely, based on which we can employ linear interpolation to obtain
coarse trajectories/in-between frames. To compensate the high-order nonlinear
components of trajectories, we apply aligned feature embeddings that encode
local geometry properties to regress point-wise increments, which are combined
with the coarse estimations. We demonstrate the effectiveness of our method on
various point cloud sequences and observe large improvement over
state-of-the-art methods both quantitatively and visually. Our framework can
bring benefits to 3D motion data acquisition. The source code is publicly
available at https://github.com/ZENGYIMING-EAMON/IDEA-Net.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation. (arXiv:2203.11591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11591">
<div class="article-summary-box-inner">
<span><p>Pre-training has been adopted in a few of recent works for
Vision-and-Language Navigation (VLN). However, previous pre-training methods
for VLN either lack the ability to predict future actions or ignore the
trajectory contexts, which are essential for a greedy navigation process. In
this work, to promote the learning of spatio-temporal visual-textual
correspondence as well as the agent's capability of decision making, we propose
a novel history-and-order aware pre-training paradigm (HOP) with VLN-specific
objectives that exploit the past observations and support future action
prediction. Specifically, in addition to the commonly used Masked Language
Modeling (MLM) and Trajectory-Instruction Matching (TIM), we design two proxy
tasks to model temporal order information: Trajectory Order Modeling (TOM) and
Group Order Modeling (GOM). Moreover, our navigation action prediction is also
enhanced by introducing the task of Action Prediction with History (APH), which
takes into account the history visual perceptions. Extensive experimental
results on four downstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the
effectiveness of our proposed method compared against several state-of-the-art
agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Negative Pair Generation toward Well-discriminative Feature Space for Face Recognition. (arXiv:2203.11593v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11593">
<div class="article-summary-box-inner">
<span><p>The goal of face recognition (FR) can be viewed as a pair similarity
optimization problem, maximizing a similarity set $\mathcal{S}^p$ over positive
pairs, while minimizing similarity set $\mathcal{S}^n$ over negative pairs.
Ideally, it is expected that FR models form a well-discriminative feature space
(WDFS) that satisfies $\inf{\mathcal{S}^p} &gt; \sup{\mathcal{S}^n}$. With regard
to WDFS, the existing deep feature learning paradigms (i.e., metric and
classification losses) can be expressed as a unified perspective on different
pair generation (PG) strategies. Unfortunately, in the metric loss (ML), it is
infeasible to generate negative pairs taking all classes into account in each
iteration because of the limited mini-batch size. In contrast, in
classification loss (CL), it is difficult to generate extremely hard negative
pairs owing to the convergence of the class weight vectors to their center.
This leads to a mismatch between the two similarity distributions of the
sampled pairs and all negative pairs. Thus, this paper proposes a unified
negative pair generation (UNPG) by combining two PG strategies (i.e., MLPG and
CLPG) from a unified perspective to alleviate the mismatch. UNPG introduces
useful information about negative pairs using MLPG to overcome the CLPG
deficiency. Moreover, it includes filtering the similarities of noisy negative
pairs to guarantee reliable convergence and improved performance. Exhaustive
experiments show the superiority of UNPG by achieving state-of-the-art
performance across recent loss functions on public benchmark datasets. Our code
and pretrained models are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Residual Networks for Gaze Mapping on Indian Roads. (arXiv:2203.11611v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11611">
<div class="article-summary-box-inner">
<span><p>In the recent past, greater accessibility to powerful computational resources
has enabled progress in the field of Deep Learning and Computer Vision to grow
by leaps and bounds. This in consequence has lent progress to the domain of
Autonomous Driving and Navigation Systems. Most of the present research work
has been focused on driving scenarios in the European or American roads. Our
paper draws special attention to the Indian driving context. To this effect, we
propose a novel architecture, DR-Gaze, which is used to map the driver's gaze
onto the road. We compare our results with previous works and state-of-the-art
results on the DGAZE dataset. Our code will be made publicly available upon
acceptance of our paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-resolution Iterative Feedback Network for Camouflaged Object Detection. (arXiv:2203.11624v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11624">
<div class="article-summary-box-inner">
<span><p>Spotting camouflaged objects that are visually assimilated into the
background is tricky for both object detection algorithms and humans who are
usually confused or cheated by the perfectly intrinsic similarities between the
foreground objects and the background surroundings. To tackle this challenge,
we aim to extract the high-resolution texture details to avoid the detail
degradation that causes blurred vision in edges and boundaries. We introduce a
novel HitNet to refine the low-resolution representations by high-resolution
features in an iterative feedback manner, essentially a global loop-based
connection among the multi-scale resolutions. In addition, an iterative
feedback loss is proposed to impose more constraints on each feedback
connection. Extensive experiments on four challenging datasets demonstrate that
our \ourmodel~breaks the performance bottleneck and achieves significant
improvements compared with 29 state-of-the-art methods. To address the data
scarcity in camouflaged scenarios, we provide an application example by
employing cross-domain learning to extract the features that can reflect the
camouflaged object properties and embed the features into salient objects,
thereby generating more camouflaged training samples from the diverse salient
object datasets The code will be available at
https://github.com/HUuxiaobin/HitNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QS-Craft: Learning to Quantize, Scrabble and Craft for Conditional Human Motion Animation. (arXiv:2203.11632v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11632">
<div class="article-summary-box-inner">
<span><p>This paper studies the task of conditional Human Motion Animation (cHMA).
Given a source image and a driving video, the model should animate the new
frame sequence, in which the person in the source image should perform a
similar motion as the pose sequence from the driving video. Despite the success
of Generative Adversarial Network (GANs) methods in image and video synthesis,
it is still very challenging to conduct cHMA due to the difficulty in
efficiently utilizing the conditional guided information such as images or
poses, and generating images of good visual quality. To this end, this paper
proposes a novel model of learning to Quantize, Scrabble, and Craft (QS-Craft)
for conditional human motion animation. The key novelties come from the newly
introduced three key steps: quantize, scrabble and craft. Particularly, our
QS-Craft employs transformer in its structure to utilize the attention
architectures. The guided information is represented as a pose coordinate
sequence extracted from the driving videos. Extensive experiments on human
motion datasets validate the efficacy of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos. (arXiv:2203.11637v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11637">
<div class="article-summary-box-inner">
<span><p>Human actions often induce changes of object states such as "cutting an
apple", "cleaning shoes" or "pouring coffee". In this paper, we seek to
temporally localize object states (e.g. "empty" and "full" cup) together with
the corresponding state-modifying actions ("pouring coffee") in long uncurated
videos with minimal supervision. The contributions of this work are threefold.
First, we develop a self-supervised model for jointly learning state-modifying
actions together with the corresponding object states from an uncurated set of
videos from the Internet. The model is self-supervised by the causal ordering
signal, i.e. initial object state $\rightarrow$ manipulating action
$\rightarrow$ end state. Second, to cope with noisy uncurated training data,
our model incorporates a noise adaptive weighting module supervised by a small
number of annotated still images, that allows to efficiently filter out
irrelevant videos during training. Third, we collect a new dataset with more
than 2600 hours of video and 34 thousand changes of object states, and manually
annotate a part of this data to validate our approach. Our results demonstrate
substantial improvements over prior work in both action and object
state-recognition in video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic State Estimation in Cloth Manipulation Tasks. (arXiv:2203.11647v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11647">
<div class="article-summary-box-inner">
<span><p>Understanding of deformable object manipulations such as textiles is a
challenge due to the complexity and high dimensionality of the problem.
Particularly, the lack of a generic representation of semantic states (e.g.,
\textit{crumpled}, \textit{diagonally folded}) during a continuous manipulation
process introduces an obstacle to identify the manipulation type. In this
paper, we aim to solve the problem of semantic state estimation in cloth
manipulation tasks. For this purpose, we introduce a new large-scale
fully-annotated RGB image dataset showing various human demonstrations of
different complicated cloth manipulations. We provide a set of baseline deep
networks and benchmark them on the problem of semantic state estimation using
our proposed dataset. Furthermore, we investigate the scalability of our
semantic state estimation framework in robot monitoring tasks of long and
complex cloth manipulations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Salient Object Detection Using Point Supervison. (arXiv:2203.11652v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11652">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art saliency detection models rely heavily on large
datasets of accurate pixel-wise annotations, but manually labeling pixels is
time-consuming and labor-intensive. There are some weakly supervised methods
developed for alleviating the problem, such as image label, bounding box label,
and scribble label, while point label still has not been explored in this
field. In this paper, we propose a novel weakly-supervised salient object
detection method using point supervision. To infer the saliency map, we first
design an adaptive masked flood filling algorithm to generate pseudo labels.
Then we develop a transformer-based point-supervised saliency detection model
to produce the first round of saliency maps. However, due to the sparseness of
the label, the weakly supervised model tends to degenerate into a general
foreground detection model. To address this issue, we propose a Non-Salient
Suppression (NSS) method to optimize the erroneous saliency maps generated in
the first round and leverage them for the second round of training. Moreover,
we build a new point-supervised dataset (P-DUTS) by relabeling the DUTS
dataset. In P-DUTS, there is only one labeled point for each salient object.
Comprehensive experiments on five largest benchmark datasets demonstrate our
method outperforms the previous state-of-the-art methods trained with the
stronger supervision and even surpass several fully supervised state-of-the-art
models. The code is available at: https://github.com/shuyonggao/PSOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Scene Graph Generation with Data Transfer. (arXiv:2203.11654v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11654">
<div class="article-summary-box-inner">
<span><p>Scene graph generation (SGG) aims to extract (subject, predicate, object)
triplets in images. Recent works have made a steady progress on SGG, and
provide useful tools for high-level vision and language understanding. However,
due to the data distribution problems including long-tail distribution and
semantic ambiguity, the predictions of current SGG models tend to collapse to
several frequent but uninformative predicates (e.g., \textit{on}, \textit{at}),
which limits practical application of these models in downstream tasks. To deal
with the problems above, we propose a novel Internal and External Data Transfer
(IETrans) method, which can be applied in a play-and-plug fashion and expanded
to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the
data distribution problem by automatically creating an enhanced dataset that
provides more sufficient and coherent annotations for all predicates. By
training on the transferred dataset, a Neural Motif model doubles the macro
performance while maintaining competitive micro performance. The data and code
for this paper are publicly available at
\url{https://github.com/waxnkw/IETrans-SGG.pytorch}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Channel Self-Supervision for Online Knowledge Distillation. (arXiv:2203.11660v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11660">
<div class="article-summary-box-inner">
<span><p>Recently, researchers have shown an increased interest in the online
knowledge distillation. Adopting an one-stage and end-to-end training fashion,
online knowledge distillation uses aggregated intermediated predictions of
multiple peer models for training. However, the absence of a powerful teacher
model may result in the homogeneity problem between group peers, affecting the
effectiveness of group distillation adversely. In this paper, we propose a
novel online knowledge distillation method, \textbf{C}hannel
\textbf{S}elf-\textbf{S}upervision for Online Knowledge Distillation (CSS),
which structures diversity in terms of input, target, and network to alleviate
the homogenization problem. Specifically, we construct a dual-network
multi-branch structure and enhance inter-branch diversity through
self-supervised learning, adopting the feature-level transformation and
augmenting the corresponding labels. Meanwhile, the dual network structure has
a larger space of independent parameters to resist the homogenization problem
during distillation. Extensive quantitative experiments on CIFAR-100 illustrate
that our method provides greater diversity than OKDDip and we also give pretty
performance improvement, even over the state-of-the-art such as PCL. The
results on three fine-grained datasets (StanfordDogs, StanfordCars,
CUB-200-211) also show the significant generalization capability of our
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNNs and Transformers Perceive Hybrid Images Similar to Humans. (arXiv:2203.11678v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11678">
<div class="article-summary-box-inner">
<span><p>Hybrid images is a technique to generate images with two interpretations that
change as a function of viewing distance. It has been utilized to study
multiscale processing of images by the human visual system. Using 63,000 hybrid
images across 10 fruit categories, here we show that predictions of deep
learning vision models qualitatively matches with the human perception of these
images. Our results provide yet another evidence in support of the hypothesis
that Convolutional Neural Networks (CNNs) and Transformers are good at modeling
the feedforward sweep of information in the ventral stream of visual cortex.
Code and data is available at https://github.com/aliborji/hybrid_images.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-attention for ViT-backed Continual Learning. (arXiv:2203.11684v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11684">
<div class="article-summary-box-inner">
<span><p>Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Learned Block-Based Image Compression with Block-Level Masked Convolutions and Asymptotic Closed Loop Training. (arXiv:2203.11686v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11686">
<div class="article-summary-box-inner">
<span><p>Learned image compression research has achieved state-of-the-art compression
performance with auto-encoder based neural network architectures, where the
image is mapped via convolutional neural networks (CNN) into a latent
representation that is quantized and processed again with CNN to obtain the
reconstructed image. CNN operate on entire input images. On the other hand,
traditional state-of-the-art image and video compression methods process images
with a block-by-block processing approach for various reasons. Very recently,
work on learned image compression with block based approaches have also
appeared, which use the auto-encoder architecture on large blocks of the input
image and introduce additional neural networks that perform intra/spatial
prediction and deblocking/post-processing functions. This paper explores an
alternative learned block-based image compression approach in which neither an
explicit intra prediction neural network nor an explicit deblocking neural
network is used. A single auto-encoder neural network with block-level masked
convolutions is used and the block size is much smaller (8x8). By using
block-level masked convolutions, each block is processed using reconstructed
neighboring left and upper blocks both at the encoder and decoder. Hence, the
mutual information between adjacent blocks is exploited during compression and
each block is reconstructed using neighboring blocks, resolving the need for
explicit intra prediction and deblocking neural networks. Since the explored
system is a closed loop system, a special optimization procedure, the
asymptotic closed loop design, is used with standard stochastic gradient
descent based training. The experimental results indicate competitive image
compression performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11692">
<div class="article-summary-box-inner">
<span><p>This manuscript describes the panoptic segmentation method we devised for our
submission to the CONIC challenge at ISBI 2022. Key features of our method are
a weighted loss that we specifically engineered for semantic segmentation of
highly imbalanced cell types, and an existing state-of-the art nuclei instance
segmentation model, which we combine in a Hovernet-like architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical Flow Based Motion Detection for Autonomous Driving. (arXiv:2203.11693v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11693">
<div class="article-summary-box-inner">
<span><p>Motion detection is a fundamental but challenging task for autonomous
driving. In particular scenes like highway, remote objects have to be paid
extra attention for better controlling decision. Aiming at distant vehicles, we
train a neural network model to classify the motion status using optical flow
field information as the input. The experiments result in high accuracy,
showing that our idea is viable and promising. The trained model also achieves
an acceptable performance for nearby vehicles. Our work is implemented in
PyTorch. Open tools including nuScenes, FastFlowNet and RAFT are used.
Visualization videos are available at
https://www.youtube.com/playlist?list=PLVVrWgq4OrlBnRebmkGZO1iDHEksMHKGk .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CP2: Copy-Paste Contrastive Pretraining for Semantic Segmentation. (arXiv:2203.11709v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11709">
<div class="article-summary-box-inner">
<span><p>Recent advances in self-supervised contrastive learning yield good
image-level representation, which favors classification tasks but usually
neglects pixel-level detailed information, leading to unsatisfactory transfer
performance to dense prediction tasks such as semantic segmentation. In this
work, we propose a pixel-wise contrastive learning method called CP2
(Copy-Paste Contrastive Pretraining), which facilitates both image- and
pixel-level representation learning and therefore is more suitable for
downstream dense prediction tasks. In detail, we copy-paste a random crop from
an image (the foreground) onto different background images and pretrain a
semantic segmentation model with the objective of 1) distinguishing the
foreground pixels from the background pixels, and 2) identifying the composed
images that share the same foreground.Experiments show the strong performance
of CP2 in downstream semantic segmentation: By finetuning CP2 pretrained models
on PASCAL VOC 2012, we obtain 78.6% mIoU with a ResNet-50 and 79.5% with a
ViT-S.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Network to Restore Low-Dose Digital Breast Tomosynthesis Projections in a Variance Stabilization Domain. (arXiv:2203.11722v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11722">
<div class="article-summary-box-inner">
<span><p>Digital breast tomosynthesis (DBT) exams should utilize the lowest possible
radiation dose while maintaining sufficiently good image quality for accurate
medical diagnosis. In this work, we propose a convolution neural network (CNN)
to restore low-dose (LD) DBT projections to achieve an image quality equivalent
to a standard full-dose (FD) acquisition. The proposed network architecture
benefits from priors in terms of layers that were inspired by traditional
model-based (MB) restoration methods, considering a model-based deep learning
approach, where the network is trained to operate in the variance stabilization
transformation (VST) domain. To accurately control the network operation point,
in terms of noise and blur of the restored image, we propose a loss function
that minimizes the bias and matches residual noise between the input and the
output. The training dataset was composed of clinical data acquired at the
standard FD and low-dose pairs obtained by the injection of quantum noise. The
network was tested using real DBT projections acquired with a physical
anthropomorphic breast phantom. The proposed network achieved superior results
in terms of the mean normalized squared error (MNSE), training time and noise
spatial correlation compared with networks trained with traditional data-driven
methods. The proposed approach can be extended for other medical imaging
application that requires LD acquisitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Anomaly Detection in Medical Images with a Memory-augmented Multi-level Cross-attentional Masked Autoencoder. (arXiv:2203.11725v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11725">
<div class="article-summary-box-inner">
<span><p>Unsupervised anomaly detection (UAD) aims to find anomalous images by
optimising a detector using a training set that contains only normal images.
UAD approaches can be based on reconstruction methods, self-supervised
approaches, and Imagenet pre-trained models. Reconstruction methods, which
detect anomalies from image reconstruction errors, are advantageous because
they do not rely on the design of problem-specific pretext tasks needed by
self-supervised approaches, and on the unreliable translation of models
pre-trained from non-medical datasets. However, reconstruction methods may fail
because they can have low reconstruction errors even for anomalous images. In
this paper, we introduce a new reconstruction-based UAD approach that addresses
this low-reconstruction error issue for anomalous images. Our UAD approach, the
memory-augmented multi-level cross-attentional masked autoencoder (MemMC-MAE),
is a transformer-based approach, consisting of a novel memory-augmented
self-attention operator for the encoder and a new multi-level cross-attention
operator for the decoder. MemMC-MAE masks large parts of the input image during
its reconstruction, reducing the risk that it will produce low reconstruction
errors because anomalies are likely to be masked and cannot be reconstructed.
However, when the anomaly is not masked, then the normal patterns stored in the
encoder's memory combined with the decoder's multi-level cross-attention will
constrain the accurate reconstruction of the anomaly. We show that our method
achieves SOTA anomaly detection and localisation on colonoscopy and Covid-19
Chest X-ray datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-enabled Assessment of Cardiac Systolic and Diastolic Function from Echocardiography. (arXiv:2203.11726v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11726">
<div class="article-summary-box-inner">
<span><p>Left ventricular (LV) function is an important factor in terms of patient
management, outcome, and long-term survival of patients with heart disease. The
most recently published clinical guidelines for heart failure recognise that
over reliance on only one measure of cardiac function (LV ejection fraction) as
a diagnostic and treatment stratification biomarker is suboptimal. Recent
advances in AI-based echocardiography analysis have shown excellent results on
automated estimation of LV volumes and LV ejection fraction. However, from
time-varying 2-D echocardiography acquisition, a richer description of cardiac
function can be obtained by estimating functional biomarkers from the complete
cardiac cycle. In this work we propose for the first time an AI approach for
deriving advanced biomarkers of systolic and diastolic LV function from 2-D
echocardiography based on segmentations of the full cardiac cycle. These
biomarkers will allow clinicians to obtain a much richer picture of the heart
in health and disease. The AI model is based on the 'nn-Unet' framework and was
trained and tested using four different databases. Results show excellent
agreement between manual and automated analysis and showcase the potential of
the advanced systolic and diastolic biomarkers for patient stratification.
Finally, for a subset of 50 cases, we perform a correlation analysis between
clinical biomarkers derived from echocardiography and CMR and we show excellent
agreement between the two modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProgressiveMotionSeg: Mutually Reinforced Framework for Event-Based Motion Segmentation. (arXiv:2203.11732v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11732">
<div class="article-summary-box-inner">
<span><p>Dynamic Vision Sensor (DVS) can asynchronously output the events reflecting
apparent motion of objects with microsecond resolution, and shows great
application potential in monitoring and other fields. However, the output event
stream of existing DVS inevitably contains background activity noise (BA noise)
due to dark current and junction leakage current, which will affect the
temporal correlation of objects, resulting in deteriorated motion estimation
performance. Particularly, the existing filter-based denoising methods cannot
be directly applied to suppress the noise in event stream, since there is no
spatial correlation. To address this issue, this paper presents a novel
progressive framework, in which a Motion Estimation (ME) module and an Event
Denoising (ED) module are jointly optimized in a mutually reinforced manner.
Specifically, based on the maximum sharpness criterion, ME module divides the
input event into several segments by adaptive clustering in a motion
compensating warp field, and captures the temporal correlation of event stream
according to the clustered motion parameters. Taking temporal correlation as
guidance, ED module calculates the confidence that each event belongs to real
activity events, and transmits it to ME module to update energy function of
motion segmentation for noise suppression. The two steps are iteratively
updated until stable motion segmentation results are obtained. Extensive
experimental results on both synthetic and real datasets demonstrate the
superiority of our proposed approaches against the State-Of-The-Art (SOTA)
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring and Evaluating Image Restoration Potential in Dynamic Scenes. (arXiv:2203.11754v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11754">
<div class="article-summary-box-inner">
<span><p>In dynamic scenes, images often suffer from dynamic blur due to superposition
of motions or low signal-noise ratio resulted from quick shutter speed when
avoiding motions. Recovering sharp and clean results from the captured images
heavily depends on the ability of restoration methods and the quality of the
input. Although existing research on image restoration focuses on developing
models for obtaining better restored results, fewer have studied to evaluate
how and which input image leads to superior restored quality. In this paper, to
better study an image's potential value that can be explored for restoration,
we propose a novel concept, referring to image restoration potential (IRP).
Specifically, We first establish a dynamic scene imaging dataset containing
composite distortions and applied image restoration processes to validate the
rationality of the existence to IRP. Based on this dataset, we investigate
several properties of IRP and propose a novel deep model to accurately predict
IRP values. By gradually distilling and selective fusing the degradation
features, the proposed model shows its superiority in IRP prediction. Thanks to
the proposed model, we are then able to validate how various image restoration
related applications are benefited from IRP prediction. We show the potential
usages of IRP as a filtering principle to select valuable frames, an auxiliary
guidance to improve restoration models, and even an indicator to optimize
camera settings for capturing better images under dynamic scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Framework for Assessment of Learning-based Detectors in Realistic Conditions with Application to Deepfake Detection. (arXiv:2203.11797v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11797">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks have shown remarkable results on multiple
detection tasks. Despite the significant progress, the performance of such
detectors are often assessed in public benchmarks under non-realistic
conditions. Specifically, impact of conventional distortions and processing
operations such as compression, noise, and enhancement are not sufficiently
studied. This paper proposes a rigorous framework to assess performance of
learning-based detectors in more realistic situations. An illustrative example
is shown under deepfake detection context. Inspired by the assessment results,
a data augmentation strategy based on natural image degradation process is
designed, which significantly improves the generalization ability of two
deepfake detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network. (arXiv:2203.11799v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11799">
<div class="article-summary-box-inner">
<span><p>Blind-spot network (BSN) and its variants have made significant advances in
self-supervised denoising. Nevertheless, they are still bound to synthetic
noisy inputs due to less practical assumptions like pixel-wise independent
noise. Hence, it is challenging to deal with spatially correlated real-world
noise using self-supervised BSN. Recently, pixel-shuffle downsampling (PD) has
been proposed to remove the spatial correlation of real-world noise. However,
it is not trivial to integrate PD and BSN directly, which prevents the fully
self-supervised denoising model on real-world images. We propose an Asymmetric
PD (AP) to address this issue, which introduces different PD stride factors for
training and inference. We systematically demonstrate that the proposed AP can
resolve inherent trade-offs caused by specific PD stride factors and make BSN
applicable to practical scenarios. To this end, we develop AP-BSN, a
state-of-the-art self-supervised denoising method for real-world sRGB images.
We further propose random-replacing refinement, which significantly improves
the performance of our AP-BSN without any additional parameters. Extensive
studies demonstrate that our method outperforms the other self-supervised and
even unpaired denoising methods by a large margin, without using any additional
knowledge, e.g., noise level, regarding the underlying unknown noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Approach to Improve Learning-based Deepfake Detection in Realistic Conditions. (arXiv:2203.11807v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11807">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks have achieved exceptional results on
multiple detection and recognition tasks. However, the performance of such
detectors are often evaluated in public benchmarks under constrained and
non-realistic situations. The impact of conventional distortions and processing
operations found in imaging workflows such as compression, noise, and
enhancement are not sufficiently studied. Currently, only a few researches have
been done to improve the detector robustness to unseen perturbations. This
paper proposes a more effective data augmentation scheme based on real-world
image degradation process. This novel technique is deployed for deepfake
detection tasks and has been evaluated by a more realistic assessment
framework. Extensive experiments show that the proposed data augmentation
scheme improves generalization ability to unpredictable data distortions and
unseen datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Broad Study of Pre-training for Domain Generalization and Adaptation. (arXiv:2203.11819v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11819">
<div class="article-summary-box-inner">
<span><p>Deep models must learn robust and transferable representations in order to
perform well on new domains. While domain transfer methods (e.g., domain
adaptation, domain generalization) have been proposed to learn transferable
representations across domains, they are typically applied to ResNet backbones
pre-trained on ImageNet. Thus, existing works pay little attention to the
effects of pre-training on domain transfer tasks. In this paper, we provide a
broad study and in-depth analysis of pre-training for domain adaptation and
generalization, namely: network architectures, size, pre-training loss, and
datasets. We observe that simply using a state-of-the-art backbone outperforms
existing state-of-the-art domain adaptation baselines and set new baselines on
Office-Home and DomainNet improving by 10.7\% and 5.5\%. We hope that this work
can provide more insights for future domain transfer research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Was that so hard? Estimating human classification difficulty. (arXiv:2203.11824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11824">
<div class="article-summary-box-inner">
<span><p>When doctors are trained to diagnose a specific disease, they learn faster
when presented with cases in order of increasing difficulty. This creates the
need for automatically estimating how difficult it is for doctors to classify a
given case. In this paper, we introduce methods for estimating how hard it is
for a doctor to diagnose a case represented by a medical image, both when
ground truth difficulties are available for training, and when they are not.
Our methods are based on embeddings obtained with deep metric learning.
Additionally, we introduce a practical method for obtaining ground truth human
difficulty for each image case in a dataset using self-assessed certainty. We
apply our methods to two different medical datasets, achieving high Kendall
rank correlation coefficients, showing that we outperform existing methods by a
large margin on our problem and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-View Panorama Image Synthesis. (arXiv:2203.11832v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11832">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the problem of synthesizing a ground-view panorama
image conditioned on a top-view aerial image, which is a challenging problem
due to the large gap between the two image domains with different view-points.
Instead of learning cross-view mapping in a feedforward pass, we propose a
novel adversarial feedback GAN framework named PanoGAN with two key components:
an adversarial feedback module and a dual branch discrimination strategy.
First, the aerial image is fed into the generator to produce a target panorama
image and its associated segmentation map in favor of model training with
layout semantics. Second, the feature responses of the discriminator encoded by
our adversarial feedback module are fed back to the generator to refine the
intermediate representations, so that the generation performance is continually
improved through an iterative generation process. Third, to pursue
high-fidelity and semantic consistency of the generated panorama image, we
propose a pixel-segmentation alignment mechanism under the dual branch
discrimiantion strategy to facilitate cooperation between the generator and the
discriminator. Extensive experimental results on two challenging cross-view
image datasets show that PanoGAN enables high-quality panorama image generation
with more convincing details than state-of-the-art approaches. The source code
and trained models are available at \url{https://github.com/sswuai/PanoGAN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Generalization in Federated Learning by Seeking Flat Minima. (arXiv:2203.11834v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11834">
<div class="article-summary-box-inner">
<span><p>Models trained in federated settings often suffer from degraded performances
and fail at generalizing, especially when facing heterogeneous scenarios. In
this work, we investigate such behavior through the lens of geometry of the
loss and Hessian eigenspectrum, linking the model's lack of generalization
capacity to the sharpness of the solution. Motivated by prior studies
connecting the sharpness of the loss surface and the generalization gap, we
show that i) training clients locally with Sharpness-Aware Minimization (SAM)
or its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on
the server-side can substantially improve generalization in Federated Learning
and help bridging the gap with centralized models. By seeking parameters in
neighborhoods having uniform low loss, the model converges towards flatter
minima and its generalization significantly improves in both homogeneous and
heterogeneous scenarios. Empirical results demonstrate the effectiveness of
those optimizers across a variety of benchmark vision datasets (e.g.
CIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,
semantic segmentation, domain generalization).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Real-time Junk Food Recognition System based on Machine Learning. (arXiv:2203.11836v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11836">
<div class="article-summary-box-inner">
<span><p>$ $As a result of bad eating habits, humanity may be destroyed. People are
constantly on the lookout for tasty foods, with junk foods being the most
common source. As a consequence, our eating patterns are shifting, and we're
gravitating toward junk food more than ever, which is bad for our health and
increases our risk of acquiring health problems. Machine learning principles
are applied in every aspect of our lives, and one of them is object recognition
via image processing. However, because foods vary in nature, this procedure is
crucial, and traditional methods like ANN, SVM, KNN, PLS etc., will result in a
low accuracy rate. All of these issues were defeated by the Deep Neural
Network. In this work, we created a fresh dataset of 10,000 data points from 20
junk food classifications to try to recognize junk foods. All of the data in
the data set was gathered using the Google search engine, which is thought to
be one-of-a-kind in every way. The goal was achieved using Convolution Neural
Network (CNN) technology, which is well-known for image processing. We achieved
a 98.05\% accuracy rate throughout the research, which was satisfactory. In
addition, we conducted a test based on a real-life event, and the outcome was
extraordinary. Our goal is to advance this research to the next level, so that
it may be applied to a future study. Our ultimate goal is to create a system
that would encourage people to avoid eating junk food and to be
health-conscious. \keywords{ Machine Learning \and junk food \and object
detection \and YOLOv3 \and custom food dataset.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImageNet Challenging Classification with the Raspberry Pi: An Incremental Local Stochastic Gradient Descent Algorithm. (arXiv:2203.11853v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11853">
<div class="article-summary-box-inner">
<span><p>With rising powerful, low-cost embedded devices, the edge computing has
become an increasingly popular choice. In this paper, we propose a new
incremental local stochastic gradient descent (SGD) tailored on the Raspberry
Pi to deal with large ImageNet dataset having 1,261,405 images with 1,000
classes. The local SGD splits the data block into $k$ partitions using $k$means
algorithm and then it learns in the parallel way SGD models in each data
partition to classify the data locally. The incremental local SGD sequentially
loads small data blocks of the training dataset to learn local SGD models. The
numerical test results on Imagenet dataset show that our incremental local SGD
algorithm with the Raspberry Pi 4 is faster and more accurate than the
state-of-the-art linear SVM run on a PC Intel(R) Core i7-4790 CPU, 3.6 GHz, 4
cores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating natural images with direct Patch Distributions Matching. (arXiv:2203.11862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11862">
<div class="article-summary-box-inner">
<span><p>Many traditional computer vision algorithms generate realistic images by
requiring that each patch in the generated image be similar to a patch in a
training image and vice versa. Recently, this classical approach has been
replaced by adversarial training with a patch discriminator. The adversarial
approach avoids the computational burden of finding nearest neighbors of
patches but often requires very long training times and may fail to match the
distribution of patches. In this paper we leverage the recently developed
Sliced Wasserstein Distance and develop an algorithm that explicitly and
efficiently minimizes the distance between patch distributions in two images.
Our method is conceptually simple, requires no training and can be implemented
in a few lines of codes. On a number of image generation tasks we show that our
results are often superior to single-image-GANs, require no training, and can
generate high quality images in a few seconds. Our implementation is available
at https://github.com/ariel415el/GPDM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Vocabulary DETR with Conditional Matching. (arXiv:2203.11876v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11876">
<div class="article-summary-box-inner">
<span><p>Open-vocabulary object detection, which is concerned with the problem of
detecting novel objects guided by natural language, has gained increasing
attention from the community. Ideally, we would like to extend an
open-vocabulary detector such that it can produce bounding box predictions
based on user inputs in form of either natural language or exemplar image. This
offers great flexibility and user experience for human-computer interaction. To
this end, we propose a novel open-vocabulary detector based on DETR -- hence
the name OV-DETR -- which, once trained, can detect any object given its class
name or an exemplar image. The biggest challenge of turning DETR into an
open-vocabulary detector is that it is impossible to calculate the
classification cost matrix of novel classes without access to their labeled
images. To overcome this challenge, we formulate the learning objective as a
binary matching one between input queries (class name or exemplar image) and
the corresponding objects, which learns useful correspondence to generalize to
unseen queries during testing. For training, we choose to condition the
Transformer decoder on the input embeddings obtained from a pre-trained
vision-language model like CLIP, in order to enable matching for both text and
image queries. With extensive experiments on LVIS and COCO datasets, we
demonstrate that our OV-DETR -- the first end-to-end Transformer-based
open-vocabulary detector -- achieves non-trivial improvements over current
state of the arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Under the Hood of Transformer Networks for Trajectory Forecasting. (arXiv:2203.11878v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11878">
<div class="article-summary-box-inner">
<span><p>Transformer Networks have established themselves as the de-facto
state-of-the-art for trajectory forecasting but there is currently no
systematic study on their capability to model the motion patterns of people,
without interactions with other individuals nor the social context. This paper
proposes the first in-depth study of Transformer Networks (TF) and
Bidirectional Transformers (BERT) for the forecasting of the individual motion
of people, without bells and whistles. We conduct an exhaustive evaluation of
input/output representations, problem formulations and sequence modeling,
including a novel analysis of their capability to predict multi-modal futures.
Out of comparative evaluation on the ETH+UCY benchmark, both TF and BERT are
top performers in predicting individual motions, definitely overcoming RNNs and
LSTMs. Furthermore, they remain within a narrow margin wrt more complex
techniques, which include both social interactions and scene contexts. Source
code will be released for all conducted experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11894">
<div class="article-summary-box-inner">
<span><p>In this work we demonstrate the vulnerability of vision transformers (ViTs)
to gradient-based inversion attacks. During this attack, the original data
batch is reconstructed given model weights and the corresponding gradients. We
introduce a method, named GradViT, that optimizes random noise into naturally
looking images via an iterative process. The optimization objective consists of
(i) a loss on matching the gradients, (ii) image prior in the form of distance
to batch-normalization statistics of a pretrained CNN model, and (iii) a total
variation regularization on patches to guide correct recovery locations. We
propose a unique loss scheduling function to overcome local minima during
optimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and
observe unprecedentedly high fidelity and closeness to the original (hidden)
data. During the analysis we find that vision transformers are significantly
more vulnerable than previously studied CNNs due to the presence of the
attention mechanism. Our method demonstrates new state-of-the-art results for
gradient inversion in both qualitative and quantitative metrics. Project page
at https://gradvit.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection, Recognition, and Tracking: A Survey. (arXiv:2203.11900v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11900">
<div class="article-summary-box-inner">
<span><p>For humans, object detection, recognition, and tracking are innate. These
provide the ability for human to perceive their environment and objects within
their environment. This ability however doesn't translate well in computers. In
Computer Vision and Multimedia, it is becoming increasingly more important to
detect, recognize and track objects in images and/or videos. Many of these
applications, such as facial recognition, surveillance, animation, are used for
tracking features and/or people. However, these tasks prove challenging for
computers to do effectively, as there is a significant amount of data to parse
through. Therefore, many techniques and algorithms are needed and therefore
researched to try to achieve human like perception. In this literature review,
we focus on some novel techniques on object detection and recognition, and how
to apply tracking algorithms to the detected features to track the objects'
movements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling faster and more reliable sonographic assessment of gestational age through machine learning. (arXiv:2203.11903v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11903">
<div class="article-summary-box-inner">
<span><p>Fetal ultrasounds are an essential part of prenatal care and can be used to
estimate gestational age (GA). Accurate GA assessment is important for
providing appropriate prenatal care throughout pregnancy and identifying
complications such as fetal growth disorders. Since derivation of GA from
manual fetal biometry measurements (head, abdomen, femur) are
operator-dependent and time-consuming, there have been a number of research
efforts focused on using artificial intelligence (AI) models to estimate GA
using standard biometry images, but there is still room to improve the accuracy
and reliability of these AI systems for widescale adoption. To improve GA
estimates, without significant change to provider workflows, we leverage AI to
interpret standard plane ultrasound images as well as 'fly-to' ultrasound
videos, which are 5-10s videos automatically recorded as part of the standard
of care before the still image is captured. We developed and validated three AI
models: an image model using standard plane images, a video model using fly-to
videos, and an ensemble model (combining both image and video). All three were
statistically superior to standard fetal biometry-based GA estimates derived by
expert sonographers, the ensemble model has the lowest mean absolute error
(MAE) compared to the clinical standard fetal biometry (mean difference: -1.51
$\pm$ 3.96 days, 95% CI [-1.9, -1.1]) on a test set that consisted of 404
participants. We showed that our models outperform standard biometry by a more
substantial margin on fetuses that were small for GA. Our AI models have the
potential to empower trained operators to estimate GA with higher accuracy
while reducing the amount of time required and user variability in measurement
acquisition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Neural Predictivity in the Visual Cortex with Gated Recurrent Connections. (arXiv:2203.11910v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11910">
<div class="article-summary-box-inner">
<span><p>Computational models of vision have traditionally been developed in a
bottom-up fashion, by hierarchically composing a series of straightforward
operations - i.e. convolution and pooling - with the aim of emulating simple
and complex cells in the visual cortex, resulting in the introduction of deep
convolutional neural networks (CNNs). Nevertheless, data obtained with recent
neuronal recording techniques support that the nature of the computations
carried out in the ventral visual stream is not completely captured by current
deep CNN models. To fill the gap between the ventral visual stream and deep
models, several benchmarks have been designed and organized into the
Brain-Score platform, granting a way to perform multi-layer (V1, V2, V4, IT)
and behavioral comparisons between the two counterparts. In our work, we aim to
shift the focus on architectures that take into account lateral recurrent
connections, a ubiquitous feature of the ventral visual stream, to devise
adaptive receptive fields. Through recurrent connections, the input s
long-range spatial dependencies can be captured in a local multi-step fashion
and, as introduced with Gated Recurrent CNNs (GRCNN), the unbounded expansion
of the neuron s receptive fields can be modulated through the use of gates. In
order to increase the robustness of our approach and the biological fidelity of
the activations, we employ specific data augmentation techniques in line with
several of the scoring benchmarks. Enforcing some form of invariance, through
heuristics, was found to be beneficial for better neural predictivity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focal Modulation Networks. (arXiv:2203.11926v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11926">
<div class="article-summary-box-inner">
<span><p>In this work, we propose focal modulation network (FocalNet in short), where
self-attention (SA) is completely replaced by a focal modulation module that is
more effective and efficient for modeling token interactions. Focal modulation
comprises three components: $(i)$ hierarchical contextualization, implemented
using a stack of depth-wise convolutional layers, to encode visual contexts
from short to long ranges at different granularity levels, $(ii)$ gated
aggregation to selectively aggregate context features for each visual token
(query) based on its content, and $(iii)$ modulation or element-wise affine
transformation to fuse the aggregated features into the query vector. Extensive
experiments show that FocalNets outperform the state-of-the-art SA counterparts
(e.g., Swin Transformers) with similar time and memory cost on the tasks of
image classification, object detection, and semantic segmentation.
Specifically, our FocalNets with tiny and base sizes achieve 82.3% and 83.9%
top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K, it attains
86.5% and 87.3% top-1 accuracy when finetuned with resolution 224$\times$224
and 384$\times$384, respectively. FocalNets exhibit remarkable superiority when
transferred to downstream tasks. For object detection with Mask R-CNN, our
FocalNet base trained with 1$\times$ already surpasses Swin trained with
3$\times$ schedule (49.0 v.s. 48.5). For semantic segmentation with UperNet,
FocalNet base evaluated at single-scale outperforms Swin evaluated at
multi-scale (50.5 v.s. 49.7). These results render focal modulation a favorable
alternative to SA for effective and efficient visual modeling in real-world
applications. Code is available at https://github.com/microsoft/FocalNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset Distillation by Matching Training Trajectories. (arXiv:2203.11932v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11932">
<div class="article-summary-box-inner">
<span><p>Dataset distillation is the task of synthesizing a small dataset such that a
model trained on the synthetic set will match the test accuracy of the model
trained on the full dataset. In this paper, we propose a new formulation that
optimizes our distilled data to guide networks to a similar state as those
trained on real data across many training steps. Given a network, we train it
for several iterations on our distilled data and optimize the distilled data
with respect to the distance between the synthetically trained parameters and
the parameters trained on real data. To efficiently obtain the initial and
target network parameters for large-scale datasets, we pre-compute and store
training trajectories of expert networks trained on the real dataset. Our
method handily outperforms existing methods and also allows us to distill
higher-resolution visual data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11933">
<div class="article-summary-box-inner">
<span><p>Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these harms. Prior proposed bias
measurements lack robustness and feature degradation occurs when mitigating
bias without access to pretraining data. We address both of these challenges in
this paper: First, we evaluate different bias measures and propose the use of
retrieval metrics to image-text representations via a bias measuring framework.
Second, we investigate debiasing methods and show that optimizing for
adversarial loss via learnable token embeddings minimizes various bias measures
without substantially degrading feature representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from All Vehicles. (arXiv:2203.11934v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11934">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a system to train driving policies from experiences
collected not just from the ego-vehicle, but all vehicles that it observes.
This system uses the behaviors of other agents to create more diverse driving
scenarios without collecting additional data. The main difficulty in learning
from other vehicles is that there is no sensor information. We use a set of
supervisory tasks to learn an intermediate representation that is invariant to
the viewpoint of the controlling vehicle. This not only provides a richer
signal at training time but also allows more complex reasoning during
inference. Learning how all vehicles drive helps predict their behavior at test
time and can avoid collisions. We evaluate this system in closed-loop driving
simulations. Our system outperforms all prior methods on the public CARLA
Leaderboard by a wide margin, improving driving score by 25 and route
completion rate by 24 points. Our method won the 2021 CARLA Autonomous Driving
challenge. Demo videos are available at https://dotchen.github.io/LAV/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">4D-OR: Semantic Scene Graphs for OR Domain Modeling. (arXiv:2203.11937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11937">
<div class="article-summary-box-inner">
<span><p>Surgical procedures are conducted in highly complex operating rooms (OR),
comprising different actors, devices, and interactions. To date, only medically
trained human experts are capable of understanding all the links and
interactions in such a demanding environment. This paper aims to bring the
community one step closer to automated, holistic and semantic understanding and
modeling of OR domain. Towards this goal, for the first time, we propose using
semantic scene graphs (SSG) to describe and summarize the surgical scene. The
nodes of the scene graphs represent different actors and objects in the room,
such as medical staff, patients, and medical equipment, whereas edges are the
relationships between them. To validate the possibilities of the proposed
representation, we create the first publicly available 4D surgical SSG dataset,
4D-OR, containing ten simulated total knee replacement surgeries recorded with
six RGB-D sensors in a realistic OR simulation center. 4D-OR includes 6734
frames and is richly annotated with SSGs, human and object poses, and clinical
roles. We propose an end-to-end neural network-based SSG generation pipeline,
with a rate of success of 0.75 macro F1, indeed being able to infer semantic
reasoning in the OR. We further demonstrate the representation power of our
scene graphs by using it for the problem of clinical role prediction, where we
achieve 0.85 macro F1. The code and dataset will be made available upon
acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">{\phi}-SfT: Shape-from-Template with a Physics-Based Deformation Model. (arXiv:2203.11938v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11938">
<div class="article-summary-box-inner">
<span><p>Shape-from-Template (SfT) methods estimate 3D surface deformations from a
single monocular RGB camera while assuming a 3D state known in advance (a
template). This is an important yet challenging problem due to the
under-constrained nature of the monocular setting. Existing SfT techniques
predominantly use geometric and simplified deformation models, which often
limits their reconstruction abilities. In contrast to previous works, this
paper proposes a new SfT approach explaining 2D observations through physical
simulations accounting for forces and material properties. Our differentiable
physics simulator regularises the surface evolution and optimises the material
elastic properties such as bending coefficients, stretching stiffness and
density. We use a differentiable renderer to minimise the dense reprojection
error between the estimated 3D states and the input images and recover the
deformation parameters using an adaptive gradient-based optimisation. For the
evaluation, we record with an RGB-D camera challenging real surfaces exposed to
physical forces with various material properties and textures. Our approach
significantly reduces the 3D reconstruction error compared to multiple
competing methods. For the source code and data, see
https://4dqv.mpi-inf.mpg.de/phi-SfT/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v7 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.13216">
<div class="article-summary-box-inner">
<span><p>Deep learning has revolutionized computer vision utilizing the increased
availability of big data and the power of parallel computational units such as
graphical processing units. The vast majority of deep learning research is
conducted using images as training data, however the biomedical domain is rich
in physiological signals that are used for diagnosis and prediction problems.
It is still an open research question how to best utilize signals to train deep
neural networks.
</p>
<p>In this paper we define the term Signal2Image (S2Is) as trainable or
non-trainable prefix modules that convert signals, such as
Electroencephalography (EEG), to image-like representations making them
suitable for training image-based deep neural networks defined as `base
models'. We compare the accuracy and time performance of four S2Is (`signal as
image', spectrogram, one and two layer Convolutional Neural Networks (CNNs))
combined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet)
along with the depth-wise and 1D variations of the latter. We also provide
empirical evidence that the one layer CNN S2I performs better in eleven out of
fifteen tested models than non-trainable S2Is for classifying EEG signals and
we present visual comparisons of the outputs of the S2Is.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparsely Activated Networks. (arXiv:1907.06592v8 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.06592">
<div class="article-summary-box-inner">
<span><p>Previous literature on unsupervised learning focused on designing structural
priors with the aim of learning meaningful features. However, this was done
without considering the description length of the learned representations which
is a direct and unbiased measure of the model complexity. In this paper, first
we introduce the $\varphi$ metric that evaluates unsupervised models based on
their reconstruction accuracy and the degree of compression of their internal
representations. We then present and define two activation functions (Identity,
ReLU) as base of reference and three sparse activation functions (top-k
absolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize
the previously defined $\varphi$. We lastly present Sparsely Activated Networks
(SANs) that consist of kernels with shared weights that, during encoding, are
convolved with the input and then passed through a sparse activation function.
During decoding, the same weights are convolved with the sparse activation map
and subsequently the partial reconstructions from each weight are summed to
reconstruct the input. We compare SANs using the five previously defined
activation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,
FMNIST) and show that models that are selected using $\varphi$ have small
description representation length and consist of interpretable kernels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">kDecay: Just adding k-decay items on Learning-Rate Schedule to improve Neural Networks. (arXiv:2004.05909v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.05909">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that optimizing the Learning Rate (LR) schedule can be
a very accurate and efficient way to train deep neural networks. We observe
that the rate of change (ROC) of LR has correlation with the training process,
but how to use this relationship to control the training to achieve the purpose
of improving accuracy? We propose a new method, k-decay, just add an extra item
to the commonly used and easy LR schedule(exp, cosine and polynomial), is
effectively improves the performance of these schedule, also better than the
state-of-the-art algorithms of LR shcedule such as SGDR, CLR and AutoLRS. In
the k-decay, by adjusting the hyper-parameter \(k\), to generate different LR
schedule, when k increases, the performance is improved. We evaluate the
k-decay method on CIFAR And ImageNet datasets with different neural networks
(ResNet, Wide ResNet). Our experiments show that this method can improve on
most of them. The accuracy has been improved by 1.08\% on the CIFAR-10 dataset
and by 2.07 \% on the CIFAR-100 dataset. On the ImageNet, accuracy is improved
by 1.25\%. Our method is not only a general method to be applied other LR
Shcedule, but also has no additional computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical Shape Analysis of Brain Arterial Networks (BAN). (arXiv:2007.04793v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.04793">
<div class="article-summary-box-inner">
<span><p>Structures of brain arterial networks (BANs) - that are complex arrangements
of individual arteries, their branching patterns, and inter-connectivities -
play an important role in characterizing and understanding brain physiology.
One would like tools for statistically analyzing the shapes of BANs, i.e.
quantify shape differences, compare population of subjects, and study the
effects of covariates on these shapes. This paper mathematically represents and
statistically analyzes BAN shapes as elastic shape graphs. Each elastic shape
graph is made up of nodes that are connected by a number of 3D curves, and
edges, with arbitrary shapes. We develop a mathematical representation, a
Riemannian metric and other geometrical tools, such as computations of
geodesics, means and covariances, and PCA for analyzing elastic graphs and
BANs. This analysis is applied to BANs after separating them into four
components -- top, bottom, left, and right. This framework is then used to
generate shape summaries of BANs from 92 subjects, and to study the effects of
age and gender on shapes of BAN components. We conclude that while gender
effects require further investigation, the age has a clear, quantifiable effect
on BAN shapes. Specifically, we find an increased variance in BAN shapes as age
increases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A range characterization of the single-quadrant ADRT. (arXiv:2010.05360v2 [math.NA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05360">
<div class="article-summary-box-inner">
<span><p>This work characterizes the range of the single-quadrant approximate discrete
Radon transform (ADRT) of square images. The characterization follows from a
set of linear constraints on the codomain. We show that for data satisfying
these constraints, the exact and fast inversion formula [Rim, Appl. Math. Lett.
102 106159, 2020] yields a square image in a stable manner. The range
characterization is obtained by first showing that the ADRT is a bijection
between images supported on infinite half-strips, then identifying the linear
subspaces that stay finitely supported under the inversion formula.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DS-Net: Dynamic Spatiotemporal Network for Video Salient Object Detection. (arXiv:2012.04886v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04886">
<div class="article-summary-box-inner">
<span><p>As moving objects always draw more attention of human eyes, the temporal
motive information is always exploited complementarily with spatial information
to detect salient objects in videos. Although efficient tools such as optical
flow have been proposed to extract temporal motive information, it often
encounters difficulties when used for saliency detection due to the movement of
camera or the partial movement of salient objects. In this paper, we
investigate the complimentary roles of spatial and temporal information and
propose a novel dynamic spatiotemporal network (DS-Net) for more effective
fusion of spatiotemporal information. We construct a symmetric two-bypass
network to explicitly extract spatial and temporal features. A dynamic weight
generator (DWG) is designed to automatically learn the reliability of
corresponding saliency branch. And a top-down cross attentive aggregation (CAA)
procedure is designed so as to facilitate dynamic complementary aggregation of
spatiotemporal features. Finally, the features are modified by spatial
attention with the guidance of coarse saliency map and then go through decoder
part for final saliency map. Experimental results on five benchmarks VOS,
DAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method
achieves superior performance than state-of-the-art algorithms. The source code
is available at https://github.com/TJUMMG/DS-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13450">
<div class="article-summary-box-inner">
<span><p>Digital watermarking is widely used for copyright protection. Traditional 3D
watermarking approaches or commercial software are typically designed to embed
messages into 3D meshes, and later retrieve the messages directly from
distorted/undistorted watermarked 3D meshes. However, in many cases, users only
have access to rendered 2D images instead of 3D meshes. Unfortunately,
retrieving messages from 2D renderings of 3D meshes is still challenging and
underexplored. We introduce a novel end-to-end learning framework to solve this
problem through: 1) an encoder to covertly embed messages in both mesh geometry
and textures; 2) a differentiable renderer to render watermarked 3D objects
from different camera angles and under varied lighting conditions; 3) a decoder
to recover the messages from 2D rendered images. From our experiments, we show
that our model can learn to embed information visually imperceptible to humans,
and to retrieve the embedded information from 2D renderings that undergo 3D
distortions. In addition, we demonstrate that our method can also work with
other renderers, such as ray tracers and real-time renderers with and without
fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Oriented RepPoints for Aerial Object Detection. (arXiv:2105.11111v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11111">
<div class="article-summary-box-inner">
<span><p>In contrast to the generic object, aerial targets are often non-axis aligned
with arbitrary orientations having the cluttered surroundings. Unlike the
mainstreamed approaches regressing the bounding box orientations, this paper
proposes an effective adaptive points learning approach to aerial object
detection by taking advantage of the adaptive points representation, which is
able to capture the geometric information of the arbitrary-oriented instances.
To this end, three oriented conversion functions are presented to facilitate
the classification and localization with accurate orientation. Moreover, we
propose an effective quality assessment and sample assignment scheme for
adaptive points learning toward choosing the representative oriented reppoints
samples during training, which is able to capture the non-axis aligned features
from adjacent objects or background noises. A spatial constraint is introduced
to penalize the outlier points for roust adaptive learning. Experimental
results on four challenging aerial datasets including DOTA, HRSC2016, UCAS-AOD
and DIOR-R, demonstrate the efficacy of our proposed approach. The source code
is availabel at: https://github.com/LiWentomng/OrientedRepPoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pho(SC)-CTC -- A Hybrid Approach Towards Zero-shot Word Image Recognition. (arXiv:2105.15093v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15093">
<div class="article-summary-box-inner">
<span><p>Annotating words in a historical document image archive for word image
recognition purpose demands time and skilled human resource (like historians,
paleographers). In a real-life scenario, obtaining sample images for all
possible words is also not feasible. However, Zero-shot learning methods could
aptly be used to recognize unseen/out-of-lexicon words in such historical
document images. Based on previous state-of-the-art method for zero-shot word
recognition Pho(SC)Net, we propose a hybrid model based on the CTC framework
(Pho(SC)-CTC) that takes advantage of the rich features learned by Pho(SC)Net
followed by a connectionist temporal classification (CTC) framework to perform
the final classification. Encouraging results were obtained on two publicly
available historical document datasets and one synthetic handwritten dataset,
which justifies the efficacy of Pho(SC)-CTC and Pho(SC)Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Than Meets the Eye: Self-Supervised Depth Reconstruction From Brain Activity. (arXiv:2106.05113v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05113">
<div class="article-summary-box-inner">
<span><p>In the past few years, significant advancements were made in reconstruction
of observed natural images from fMRI brain recordings using deep-learning
tools. Here, for the first time, we show that dense 3D depth maps of observed
2D natural images can also be recovered directly from fMRI brain recordings. We
use an off-the-shelf method to estimate the unknown depth maps of natural
images. This is applied to both: (i) the small number of images presented to
subjects in an fMRI scanner (images for which we have fMRI recordings -
referred to as "paired" data), and (ii) a very large number of natural images
with no fMRI recordings ("unpaired data"). The estimated depth maps are then
used as an auxiliary reconstruction criterion to train for depth reconstruction
directly from fMRI. We propose two main approaches: Depth-only recovery and
joint image-depth RGBD recovery. Because the number of available "paired"
training data (images with fMRI) is small, we enrich the training data via
self-supervised cycle-consistent training on many "unpaired" data (natural
images &amp; depth maps without fMRI). This is achieved using our newly defined and
trained Depth-based Perceptual Similarity metric as a reconstruction criterion.
We show that predicting the depth map directly from fMRI outperforms its
indirect sequential recovery from the reconstructed images. We further show
that activations from early cortical visual areas dominate our depth
reconstruction results, and propose means to characterize fMRI voxels by their
degree of depth-information tuning. This work adds an important layer of
decoded information, extending the current envelope of visual brain decoding
capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Anytime Learning at Macroscale. (arXiv:2106.09563v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09563">
<div class="article-summary-box-inner">
<span><p>In many practical applications of machine learning data arrives sequentially
over time in large chunks. Practitioners have then to decide how to allocate
their computational budget in order to obtain the best performance at any point
in time. Online learning theory for convex optimization suggests that the best
strategy is to use data as soon as it arrives. However, this might not be the
best strategy when using deep non-linear networks, particularly when these
perform multiple passes over each chunk of data rendering the overall
distribution non i.i.d.. In this paper, we formalize this learning setting in
the simplest scenario in which each data chunk is drawn from the same
underlying distribution, and make a first attempt at empirically answering the
following questions: How long should the learner wait before training on the
newly arrived chunks? What architecture should the learner adopt? Should the
learner increase capacity over time as more data is observed? We probe this
learning setting using convolutional neural networks trained on classic
computer vision benchmarks as well as a large transformer model trained on a
large-scale language modeling task. Code is available at
\url{www.github.com/facebookresearch/ALMA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Orthonormal Product Quantization Network for Scalable Face Image Retrieval. (arXiv:2107.00327v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00327">
<div class="article-summary-box-inner">
<span><p>Existing deep quantization methods provided an efficient solution for
large-scale image retrieval. However, the significant intra-class variations
like pose, illumination, and expressions in face images, still pose a challenge
for face image retrieval. In light of this, face image retrieval requires
sufficiently powerful learning metrics, which are absent in current deep
quantization works. Moreover, to tackle the growing unseen identities in the
query stage, face image retrieval drives more demands regarding model
generalization and system scalability than general image retrieval tasks. This
paper integrates product quantization with orthonormal constraints into an
end-to-end deep learning framework to effectively retrieve face images.
Specifically, a novel scheme that uses predefined orthonormal vectors as
codewords is proposed to enhance the quantization informativeness and reduce
codewords' redundancy. A tailored loss function maximizes discriminability
among identities in each quantization subspace for both the quantized and
original features. An entropy-based regularization term is imposed to reduce
the quantization error. Experiments are conducted on four commonly-used face
datasets under both seen and unseen identities retrieval settings. Our method
outperforms all the compared deep hashing/quantization state-of-the-arts under
both settings. Results validate the effectiveness of the proposed orthonormal
codewords in improving models' standard retrieval performance and
generalization ability. Combing with further experiments on two general image
datasets, it demonstrates the broad superiority of our method for scalable
image retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with Self-Supervision and Gated Adapters. (arXiv:2107.09783v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09783">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on a less explored, but more realistic and complex
problem of domain adaptation in LiDAR semantic segmentation. There is a
significant drop in performance of an existing segmentation model when training
(source domain) and testing (target domain) data originate from different LiDAR
sensors. To overcome this shortcoming, we propose an unsupervised domain
adaptation framework that leverages unlabeled target domain data for
self-supervision, coupled with an unpaired mask transfer strategy to mitigate
the impact of domain shifts. Furthermore, we introduce the gated adapter module
with a small number of parameters into the network to account for target
domain-specific information. Experiments adapting from both real-to-real and
synthetic-to-real LiDAR semantic segmentation benchmarks demonstrate the
significant improvement over prior arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Abnormal Hand Movement for Aiding in Autism Detection: Machine Learning Study. (arXiv:2108.07917v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07917">
<div class="article-summary-box-inner">
<span><p>A formal autism diagnosis can be an inefficient and lengthy process. Families
may wait months or longer before receiving a diagnosis for their child despite
evidence that earlier intervention leads to better treatment outcomes. Digital
technologies which detect the presence of behaviors related to autism can scale
access to pediatric diagnoses. This work aims to demonstrate the feasibility of
deep learning technologies for detecting hand flapping from unstructured home
videos as a first step towards validating whether models and digital
technologies can be leveraged to aid with autism diagnoses. We used the
Self-Stimulatory Behavior Dataset (SSBD), which contains 75 videos of hand
flapping, head banging, and spinning exhibited by children. From all the hand
flapping videos, we extracted 100 positive and control videos of hand flapping,
each between 2 to 5 seconds in duration. Utilizing both
landmark-driven-approaches and MobileNet V2's pretrained convolutional layers,
our highest performing model achieved a testing F1 score of 84% (90% precision
and 80% recall) when evaluating with 5-fold cross validation 100 times. This
work provides the first step towards developing precise deep learning methods
for activity detection of autism-related behaviors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model CLIP. (arXiv:2109.02748v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02748">
<div class="article-summary-box-inner">
<span><p>In an out-of-distribution (OOD) detection problem, samples of known
classes(also called in-distribution classes) are used to train a special
classifier. In testing, the classifier can (1) classify the test samples of
known classes to their respective classes and also (2) detect samples that do
not belong to any of the known classes (i.e., they belong to some unknown or
OOD classes). This paper studies the problem of zero-shot
out-of-distribution(OOD) detection, which still performs the same two tasks in
testing but has no training except using the given known class names. This
paper proposes a novel yet simple method (called ZOC) to solve the problem. ZOC
builds on top of the recent advances in zero-shot classification through
multi-modal representation learning. It first extends the pre-trained
language-vision model CLIP by training a text-based image description generator
on top of CLIP. In testing, it uses the extended model to generate candidate
unknown class names for each test sample and computes a confidence score based
on both the known class names and candidate unknown class names for zero-shot
OOD detection. Experimental results on 5 benchmark datasets for OOD detection
demonstrate that ZOC outperforms the baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer Hashing for Image Retrieval. (arXiv:2109.12564v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12564">
<div class="article-summary-box-inner">
<span><p>Deep learning has shown a tremendous growth in hashing techniques for image
retrieval. Recently, Transformer has emerged as a new architecture by utilizing
self-attention without convolution. Transformer is also extended to Vision
Transformer (ViT) for the visual recognition with a promising performance on
ImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)
for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone
network and add the hashing head. The proposed VTS model is fine tuned for
hashing under six different image retrieval frameworks, including Deep
Supervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network
(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)
with their objective functions. We perform the extensive experiments on
CIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image
retrieval outperforms the recent state-of-the-art hashing techniques with a
great margin. We also find the proposed VTS model as the backbone network is
better than the existing networks, such as AlexNet and ResNet. The code is
released at \url{https://github.com/shivram1987/VisionTransformerHashing}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Efficient Multi-Agent Cooperative Visual Exploration. (arXiv:2110.05734v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05734">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of cooperative visual exploration where multiple agents
need to jointly explore unseen regions as fast as possible based on visual
signals. Classical planning-based methods often suffer from expensive
computation overhead at each step and a limited expressiveness of complex
cooperation strategy. By contrast, reinforcement learning (RL) has recently
become a popular paradigm for tackling this challenge due to its modeling
capability of arbitrarily complex strategies and minimal inference overhead. In
this paper, we extend the state-of-the-art single-agent visual navigation
method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a
novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages
a transformer-based architecture, Spatial-TeamFormer, which effectively
captures spatial relations and intra-agent interactions via hierarchical
spatial self-attentions. In addition, we also implement a few multi-agent
enhancements to process local information from each agent for an aligned
spatial representation and more precise planning. Finally, we perform policy
distillation to extract a meta policy to significantly improve the
generalization capability of final policy. We call this overall solution,
Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms
classical planning-based baselines for the first time in a photo-realistic 3D
simulator, Habitat. Code and videos can be found at
https://sites.google.com/view/maans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are we ready for a new paradigm shift? A Survey on Visual Deep MLP. (arXiv:2111.04060v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04060">
<div class="article-summary-box-inner">
<span><p>Recently, the proposed deep MLP models have stirred up a lot of interest in
the vision community. Historically, the availability of larger datasets
combined with increased computing capacity leads to paradigm shifts. This
review paper provides detailed discussions on whether MLP can be a new paradigm
for computer vision. We compare the intrinsic connections and differences
between convolution, self-attention mechanism, and Token-mixing MLP in detail.
Advantages and limitations of Token-mixing MLP are provided, followed by
careful analysis of recent MLP-like variants, from module design to network
architecture, and their applications. In the GPU era, the locally and globally
weighted summations are the current mainstreams, represented by the convolution
and self-attention mechanism, as well as MLP. We suggest the further
development of paradigm to be considered alongside the next-generation
computing devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DR-VNet: Retinal Vessel Segmentation via Dense Residual UNet. (arXiv:2111.04739v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04739">
<div class="article-summary-box-inner">
<span><p>Accurate retinal vessel segmentation is an important task for many
computer-aided diagnosis systems. Yet, it is still a challenging problem due to
the complex vessel structures of an eye. Numerous vessel segmentation methods
have been proposed recently, however more research is needed to deal with poor
segmentation of thin and tiny vessels. To address this, we propose a new deep
learning pipeline combining the efficiency of residual dense net blocks and,
residual squeeze and excitation blocks. We validate experimentally our approach
on three datasets and show that our pipeline outperforms current state of the
art techniques on the sensitivity metric relevant to assess capture of small
vessels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08918">
<div class="article-summary-box-inner">
<span><p>Recent works with an implicit neural function shed light on representing
images in arbitrary resolution. However, a standalone multi-layer perceptron
shows limited performance in learning high-frequency components. In this paper,
we propose a Local Texture Estimator (LTE), a dominant-frequency estimator for
natural images, enabling an implicit function to capture fine details while
reconstructing images in a continuous manner. When jointly trained with a deep
super-resolution (SR) architecture, LTE is capable of characterizing image
textures in 2D Fourier space. We show that an LTE-based neural function
achieves favorable performance against existing deep SR methods within an
arbitrary-scale factor. Furthermore, we demonstrate that our implementation
takes the shortest running time compared to previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepCurrents: Learning Implicit Representations of Shapes with Boundaries. (arXiv:2111.09383v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09383">
<div class="article-summary-box-inner">
<span><p>Recent techniques have been successful in reconstructing surfaces as level
sets of learned functions (such as signed distance fields) parameterized by
deep neural networks. Many of these methods, however, learn only closed
surfaces and are unable to reconstruct shapes with boundary curves. We propose
a hybrid shape representation that combines explicit boundary curves with
implicit learned interiors. Using machinery from geometric measure theory, we
parameterize currents using deep networks and use stochastic gradient descent
to solve a minimal surface problem. By modifying the metric according to target
geometry coming, e.g., from a mesh or point cloud, we can use this approach to
represent arbitrary surfaces, learning implicitly defined shapes with
explicitly defined boundary curves. We further demonstrate learning families of
shapes jointly parameterized by boundary curves and latent codes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation. (arXiv:2111.10502v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10502">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the problem of jointly estimating the optical flow
and scene flow from synchronized 2D and 3D data. Previous methods either employ
a complex pipeline that splits the joint task into independent stages, or fuse
2D and 3D information in an "early-fusion" or "late-fusion" manner. Such
one-size-fits-all approaches suffer from a dilemma of failing to fully utilize
the characteristic of each modality or to maximize the inter-modality
complementarity. To address the problem, we propose a novel end-to-end
framework, called CamLiFlow. It consists of 2D and 3D branches with multiple
bidirectional connections between them in specific layers. Different from
previous work, we apply a point-based 3D branch to better extract the geometric
features and design a symmetric learnable operator to fuse dense image features
and sparse point features. Experiments show that CamLiFlow achieves better
performance with fewer parameters. Our method ranks 1st on the KITTI Scene Flow
benchmark, outperforming the previous art with 1/7 parameters. Code is
available at https://github.com/MCG-NJU/CamLiFlow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v9 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for image-to-text and text-to-image
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial result of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch Feature Swapping for Bodies and Faces. (arXiv:2111.12448v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12448">
<div class="article-summary-box-inner">
<span><p>Learning a disentangled, interpretable, and structured latent representation
in 3D generative models of faces and bodies is still an open problem. The
problem is particularly acute when control over identity features is required.
In this paper, we propose an intuitive yet effective self-supervised approach
to train a 3D shape variational autoencoder (VAE) which encourages a
disentangled latent representation of identity features. Curating the
mini-batch generation by swapping arbitrary features across different shapes
allows to define a loss function leveraging known differences and similarities
in the latent representations. Experimental results conducted on 3D meshes show
that state-of-the-art methods for latent disentanglement are not able to
disentangle identity features of faces and bodies. Our proposed method properly
decouples the generation of such features while maintaining good representation
and reconstruction capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACPL: Anti-curriculum Pseudo-labelling for Semi-supervised Medical Image Classification. (arXiv:2111.12918v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12918">
<div class="article-summary-box-inner">
<span><p>Effective semi-supervised learning (SSL) in medical image analysis (MIA) must
address two challenges: 1) work effectively on both multi-class (e.g., lesion
classification) and multi-label (e.g., multiple-disease diagnosis) problems,
and 2) handle imbalanced learning (because of the high variance in disease
prevalence). One strategy to explore in SSL MIA is based on the pseudo
labelling strategy, but it has a few shortcomings. Pseudo-labelling has in
general lower accuracy than consistency learning, it is not specifically
designed for both multi-class and multi-label problems, and it can be
challenged by imbalanced learning. In this paper, unlike traditional methods
that select confident pseudo label by threshold, we propose a new SSL
algorithm, called anti-curriculum pseudo-labelling (ACPL), which introduces
novel techniques to select informative unlabelled samples, improving training
balance and allowing the model to work for both multi-label and multi-class
problems, and to estimate pseudo labels by an accurate ensemble of classifiers
(improving pseudo label accuracy). We run extensive experiments to evaluate
ACPL on two public medical image classification benchmarks: Chest X-Ray14 for
thorax disease multi-label classification and ISIC2018 for skin lesion
multi-class classification. Our method outperforms previous SOTA SSL methods on
both datasets
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Self-Ensemble for Semantic Segmentation. (arXiv:2111.13280v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13280">
<div class="article-summary-box-inner">
<span><p>Ensemble of predictions is known to perform better than individual
predictions taken separately. However, for tasks that require heavy
computational resources, e.g. semantic segmentation, creating an ensemble of
learners that needs to be trained separately is hardly tractable. In this work,
we propose to leverage the performance boost offered by ensemble methods to
enhance the semantic segmentation, while avoiding the traditional heavy
training cost of the ensemble. Our self-ensemble approach takes advantage of
the multi-scale features set produced by feature pyramid network methods to
feed independent decoders, thus creating an ensemble within a single model.
Similar to the ensemble, the final prediction is the aggregation of the
prediction made by each learner. In contrast to previous works, our model can
be trained end-to-end, alleviating the traditional cumbersome multi-stage
training of ensembles. Our self-ensemble approach outperforms the current
state-of-the-art on the benchmark datasets Pascal Context and COCO-Stuff-10K
for semantic segmentation and is competitive on ADE20K and Cityscapes. Code is
publicly available at github.com/WalBouss/SenFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Annotator Preference and Stochastic Annotation Error for Medical Image Segmentation. (arXiv:2111.13410v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13410">
<div class="article-summary-box-inner">
<span><p>Manual annotation of medical images is highly subjective, leading to
inevitable and huge annotation biases. Deep learning models may surpass human
performance on a variety of tasks, but they may also mimic or amplify these
biases. Although we can have multiple annotators and fuse their annotations to
reduce stochastic errors, we cannot use this strategy to handle the bias caused
by annotators' preferences. In this paper, we highlight the issue of
annotator-related biases on medical image segmentation tasks, and propose a
Preference-involved Annotation Distribution Learning (PADL) framework to
address it from the perspective of disentangling an annotator's preference from
stochastic errors using distribution learning so as to produce not only a meta
segmentation but also the segmentation possibly made by each annotator. Under
this framework, a stochastic error modeling (SEM) module estimates the meta
segmentation map and average stochastic error map, and a series of human
preference modeling (HPM) modules estimate each annotator's segmentation and
the corresponding stochastic error. We evaluated our PADL framework on two
medical image benchmarks with different imaging modalities, which have been
annotated by multiple medical professionals, and achieved promising performance
on all five medical image segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IDR: Self-Supervised Image Denoising via Iterative Data Refinement. (arXiv:2111.14358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14358">
<div class="article-summary-box-inner">
<span><p>The lack of large-scale noisy-clean image pairs restricts supervised
denoising methods' deployment in actual applications. While existing
unsupervised methods are able to learn image denoising without ground-truth
clean images, they either show poor performance or work under impractical
settings (e.g., paired noisy images). In this paper, we present a practical
unsupervised image denoising method to achieve state-of-the-art denoising
performance. Our method only requires single noisy images and a noise model,
which is easily accessible in practical raw image denoising. It performs two
steps iteratively: (1) Constructing a noisier-noisy dataset with random noise
from the noise model; (2) training a model on the noisier-noisy dataset and
using the trained model to refine noisy images to obtain the targets used in
the next round. We further approximate our full iterative method with a fast
algorithm for more efficient training while keeping its original high
performance. Experiments on real-world, synthetic, and correlated noise show
that our proposed unsupervised denoising approach has superior performances
over existing unsupervised methods and competitive performance with supervised
methods. In addition, we argue that existing denoising datasets are of low
quality and contain only a small number of scenes. To evaluate raw image
denoising performance in real-world applications, we build a high-quality raw
image dataset SenseNoise-500 that contains 500 real-life scenes. The dataset
can serve as a strong benchmark for better evaluating raw image denoising. Code
and dataset will be released at https://github.com/zhangyi-3/IDR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PTTR: Relational 3D Point Cloud Object Tracking with Transformer. (arXiv:2112.02857v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02857">
<div class="article-summary-box-inner">
<span><p>In a point cloud sequence, 3D object tracking aims to predict the location
and orientation of an object in the current search point cloud given a template
point cloud. Motivated by the success of transformers, we propose Point
Tracking TRansformer (PTTR), which efficiently predicts high-quality 3D
tracking results in a coarse-to-fine manner with the help of transformer
operations. PTTR consists of three novel designs. 1) Instead of random
sampling, we design Relation-Aware Sampling to preserve relevant points to
given templates during subsampling. 2) Furthermore, we propose a Point Relation
Transformer (PRT) consisting of a self-attention and a cross-attention module.
The global self-attention operation captures long-range dependencies to enhance
encoded point features for the search area and the template, respectively.
Subsequently, we generate the coarse tracking results by matching the two sets
of point features via cross-attention. 3) Based on the coarse tracking results,
we employ a novel Prediction Refinement Module to obtain the final refined
prediction. In addition, we create a large-scale point cloud single object
tracking benchmark based on the Waymo Open Dataset. Extensive experiments show
that PTTR achieves superior point cloud tracking in both accuracy and
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E$^2$(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition. (arXiv:2112.03596v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03596">
<div class="article-summary-box-inner">
<span><p>Event cameras are novel bio-inspired sensors, which asynchronously capture
pixel-level intensity changes in the form of "events". Due to their sensing
mechanism, event cameras have little to no motion blur, a very high temporal
resolution and require significantly less power and memory than traditional
frame-based cameras. These characteristics make them a perfect fit to several
real-world applications such as egocentric action recognition on wearable
devices, where fast camera motion and limited power challenge traditional
vision sensors. However, the ever-growing field of event-based vision has, to
date, overlooked the potential of event cameras in such applications. In this
paper, we show that event data is a very valuable modality for egocentric
action recognition. To do so, we introduce N-EPIC-Kitchens, the first
event-based camera extension of the large-scale EPIC-Kitchens dataset. In this
context, we propose two strategies: (i) directly processing event-camera data
with traditional video-processing architectures (E$^2$(GO)) and (ii) using
event-data to distill optical flow information (E$^2$(GO)MO). On our proposed
benchmark, we show that event data provides a comparable performance to RGB and
optical flow, yet without any additional flow computation at deploy time, and
an improved performance of up to 4% with respect to RGB only information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shaping Visual Representations with Attributes for Few-Shot Learning. (arXiv:2112.06398v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06398">
<div class="article-summary-box-inner">
<span><p>Few-shot recognition aims to recognize novel categories under low-data
regimes. Some recent few-shot recognition methods introduce auxiliary semantic
modality, i.e., category attribute information, into representation learning,
which enhances the feature discrimination and improves the recognition
performance. Most of these existing methods only consider the attribute
information of support set while ignoring the query set, resulting in a
potential loss of performance. In this letter, we propose a novel
attribute-shaped learning (ASL) framework, which can jointly perform query
attributes generation and discriminative visual representation learning for
few-shot recognition. Specifically, a visual-attribute generator (VAG) is
constructed to predict the attributes of queries. By leveraging the attributes
information, an attribute-visual attention module (AVAM) is designed, which can
adaptively utilize attributes and visual representations to learn more
discriminative features. Under the guidance of attribute modality, our method
can learn enhanced semantic-aware representation for classification.
Experiments demonstrate that our method can achieve competitive results on CUB
and SUN benchmarks. Our source code is available at:
\url{https://github.com/chenhaoxing/ASL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Prompt for Continual Learning. (arXiv:2112.08654v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08654">
<div class="article-summary-box-inner">
<span><p>The mainstream paradigm behind continual learning has been to adapt the model
parameters to non-stationary data distributions, where catastrophic forgetting
is the central challenge. Typical methods rely on a rehearsal buffer or known
task identity at test time to retrieve learned knowledge and address
forgetting, while this work presents a new paradigm for continual learning that
aims to train a more succinct memory system without accessing task identity at
test time. Our method learns to dynamically prompt (L2P) a pre-trained model to
learn tasks sequentially under different task transitions. In our proposed
framework, prompts are small learnable parameters, which are maintained in a
memory space. The objective is to optimize prompts to instruct the model
prediction and explicitly manage task-invariant and task-specific knowledge
while maintaining model plasticity. We conduct comprehensive experiments under
popular image classification benchmarks with different challenging continual
learning settings, where L2P consistently outperforms prior state-of-the-art
methods. Surprisingly, L2P achieves competitive results against rehearsal-based
methods even without a rehearsal buffer and is directly applicable to
challenging task-agnostic continual learning. Source code is available at
https://github.com/google-research/l2p.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. (arXiv:2201.12329v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12329">
<div class="article-summary-box-inner">
<span><p>We present in this paper a novel query formulation using dynamic anchor boxes
for DETR (DEtection TRansformer) and offer a deeper understanding of the role
of queries in DETR. This new formulation directly uses box coordinates as
queries in Transformer decoders and dynamically updates them layer-by-layer.
Using box coordinates not only helps using explicit positional priors to
improve the query-to-feature similarity and eliminate the slow training
convergence issue in DETR, but also allows us to modulate the positional
attention map using the box width and height information. Such a design makes
it clear that queries in DETR can be implemented as performing soft ROI pooling
layer-by-layer in a cascade manner. As a result, it leads to the best
performance on MS-COCO benchmark among the DETR-like detection models under the
same setting, e.g., AP 45.7\% using ResNet50-DC5 as backbone trained in 50
epochs. We also conducted extensive experiments to confirm our analysis and
verify the effectiveness of our methods. Code is available at
\url{https://github.com/SlongLiu/DAB-DETR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HCSC: Hierarchical Contrastive Selective Coding. (arXiv:2202.00455v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00455">
<div class="article-summary-box-inner">
<span><p>Hierarchical semantic structures naturally exist in an image dataset, in
which several semantically relevant image clusters can be further integrated
into a larger cluster with coarser-grained semantics. Capturing such structures
with image representations can greatly benefit the semantic understanding on
various downstream tasks. Existing contrastive representation learning methods
lack such an important model capability. In addition, the negative pairs used
in these methods are not guaranteed to be semantically distinct, which could
further hamper the structural correctness of learned image representations. To
tackle these limitations, we propose a novel contrastive learning framework
called Hierarchical Contrastive Selective Coding (HCSC). In this framework, a
set of hierarchical prototypes are constructed and also dynamically updated to
represent the hierarchical semantic structures underlying the data in the
latent space. To make image representations better fit such semantic
structures, we employ and further improve conventional instance-wise and
prototypical contrastive learning via an elaborate pair selection scheme. This
scheme seeks to select more diverse positive pairs with similar semantics and
more precise negative pairs with truly distinct semantics. On extensive
downstream tasks, we verify the superior performance of HCSC over
state-of-the-art contrastive methods, and the effectiveness of major model
components is proved by plentiful analytical studies. We build a comprehensive
model zoo in Sec. D. Our source code and model weights are available at
https://github.com/gyfastas/HCSC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A hybrid 2-stage vision transformer for artificial intelligence-assisted 5 class pathologic diagnosis of gastric endoscopic biopsies: a diagnostic tool for guiding gastric cancer treatment. (arXiv:2202.08510v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08510">
<div class="article-summary-box-inner">
<span><p>Gastric endoscopic screening is an effective way to decide appropriate
gastric cancer (GC) treatment at an early stage, reducing GC-associated
mortality rate. Although artificial intelligence (AI) has brought a great
promise to assist pathologist to screen digitalized whole slide images,
automatic classification systems for guiding proper GC treatment based on
clinical guideline are still lacking. We propose an AI system classifying 5
classes of GC histology, which can be perfectly matched to general GC treatment
guidance. The AI system was designed to mimic the way pathologist understand
slides through multi-scale self-attention mechanism using a 2-stage Vision
Transformer network. The AI system performance was evaluated on 876 internal
endoscopic slides and 336 external endoscopic slides from clinical cohort. We
further evaluated practical usability of the AI system on observation of
AI-assisted 6 pathologist performance. The AI system demonstrates clinical
capability by achieving class-average diagnostic sensitivity of above 85% for
both internal and external cohort analysis. Furthermore, AI-assisted
pathologists showed significantly improved diagnostic sensitivity by 10% within
18% saved screening time compared to human pathologists (p-values of 0.006 and
0.030, respectively). The reliable performance of the AI system in multi-center
cohort testing and its clinical applicability demonstrate that AI-assisted
endoscopic CG screening would help reduce the workload of limited pathologists.
Furthermore, the AI system has a great potential for providing presumptive
pathologic opinion for deciding proper treatment for early GC patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"If you could see me through my eyes": Predicting Pedestrian Perception. (arXiv:2202.13981v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13981">
<div class="article-summary-box-inner">
<span><p>Pedestrians are particularly vulnerable road users in urban traffic. With the
arrival of autonomous driving, novel technologies can be developed specifically
to protect pedestrians. We propose a machine learning toolchain to train
artificial neural networks as models of pedestrian behavior. In a preliminary
study, we use synthetic data from simulations of a specific pedestrian crossing
scenario to train a variational autoencoder and a long short-term memory
network to predict a pedestrian's future visual perception. We can accurately
predict a pedestrian's future perceptions within relevant time horizons. By
iteratively feeding these predicted frames into these networks, they can be
used as simulations of pedestrians as indicated by our results. Such trained
networks can later be used to predict pedestrian behaviors even from the
perspective of the autonomous car. Another future extension will be to re-train
these networks with real-world video data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Colar: Effective and Efficient Online Action Detection by Consulting Exemplars. (arXiv:2203.01057v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01057">
<div class="article-summary-box-inner">
<span><p>Online action detection has attracted increasing research interests in recent
years. Current works model historical dependencies and anticipate the future to
perceive the action evolution within a video segment and improve the detection
accuracy. However, the existing paradigm ignores category-level modeling and
does not pay sufficient attention to efficiency. Considering a category, its
representative frames exhibit various characteristics. Thus, the category-level
modeling can provide complimentary guidance to the temporal dependencies
modeling. This paper develops an effective exemplar-consultation mechanism that
first measures the similarity between a frame and exemplary frames, and then
aggregates exemplary features based on the similarity weights. This is also an
efficient mechanism, as both similarity measurement and feature aggregation
require limited computations. Based on the exemplar-consultation mechanism, the
long-term dependencies can be captured by regarding historical frames as
exemplars, while the category-level modeling can be achieved by regarding
representative frames from a category as exemplars. Due to the complementarity
from the category-level modeling, our method employs a lightweight architecture
but achieves new high performance on three benchmarks. In addition, using a
spatio-temporal network to tackle video frames, our method makes a good
trade-off between effectiveness and efficiency. Code is available at
https://github.com/VividLe/Online-Action-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PanFormer: a Transformer Based Model for Pan-sharpening. (arXiv:2203.02916v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02916">
<div class="article-summary-box-inner">
<span><p>Pan-sharpening aims at producing a high-resolution (HR) multi-spectral (MS)
image from a low-resolution (LR) multi-spectral (MS) image and its
corresponding panchromatic (PAN) image acquired by a same satellite. Inspired
by a new fashion in recent deep learning community, we propose a novel
Transformer based model for pan-sharpening. We explore the potential of
Transformer in image feature extraction and fusion. Following the successful
development of vision transformers, we design a two-stream network with the
self-attention to extract the modality-specific features from the PAN and MS
modalities and apply a cross-attention module to merge the spectral and spatial
features. The pan-sharpened image is produced from the enhanced fused features.
Extensive experiments on GaoFen-2 and WorldView-3 images demonstrate that our
Transformer based model achieves impressive results and outperforms many
existing CNN based methods, which shows the great potential of introducing
Transformer to the pan-sharpening task. Codes are available at
https://github.com/zhysora/PanFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-Visual MLP for Scoring Sport. (arXiv:2203.03990v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03990">
<div class="article-summary-box-inner">
<span><p>Figure skating scoring is a challenging task because it requires judging
players' technical moves as well as coordination with the background music.
Prior learning-based work cannot solve it well for two reasons: 1) each move in
figure skating changes quickly, hence simply applying traditional frame
sampling will lose a lot of valuable information, especially in a 3-5 minutes
lasting video, so an extremely long-range representation learning is necessary;
2) prior methods rarely considered the critical audio-visual relationship in
their models. Thus, we introduce a multimodal MLP architecture, named
Skating-Mixer. It extends the MLP-Mixer-based framework into a multimodal
fashion and effectively learns long-term representations through our designed
memory recurrent unit (MRU). Aside from the model, we also collected a
high-quality audio-visual FS1000 dataset, which contains over 1000 videos on 8
types of programs with 7 different rating metrics, overtaking other datasets in
both quantity and diversity. Experiments show the proposed method outperforms
SOTAs over all major metrics on the public Fis-V and our FS1000 dataset. In
addition, we include an analysis applying our method to recent competitions
that occurred in Beijing 2022 Winter Olympic Games, proving our method has
strong robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape-invariant 3D Adversarial Point Clouds. (arXiv:2203.04041v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04041">
<div class="article-summary-box-inner">
<span><p>Adversary and invisibility are two fundamental but conflict characters of
adversarial perturbations. Previous adversarial attacks on 3D point cloud
recognition have often been criticized for their noticeable point outliers,
since they just involve an "implicit constrain" like global distance loss in
the time-consuming optimization to limit the generated noise. While point cloud
is a highly structured data format, it is hard to constrain its perturbation
with a simple loss or metric properly. In this paper, we propose a novel
Point-Cloud Sensitivity Map to boost both the efficiency and imperceptibility
of point perturbations. This map reveals the vulnerability of point cloud
recognition models when encountering shape-invariant adversarial noises. These
noises are designed along the shape surface with an "explicit constrain"
instead of extra distance loss. Specifically, we first apply a reversible
coordinate transformation on each point of the point cloud input, to reduce one
degree of point freedom and limit its movement on the tangent plane. Then we
calculate the best attacking direction with the gradients of the transformed
point cloud obtained on the white-box model. Finally we assign each point with
a non-negative score to construct the sensitivity map, which benefits both
white-box adversarial invisibility and black-box query-efficiency extended in
our work. Extensive evaluations prove that our method can achieve the superior
performance on various point cloud recognition models, with its satisfying
adversarial imperceptibility and strong resistance to different point cloud
defense settings. Our code is available at: https://github.com/shikiw/SI-Adv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abandoning the Bayer-Filter to See in the Dark. (arXiv:2203.04042v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04042">
<div class="article-summary-box-inner">
<span><p>Low-light image enhancement - a pervasive but challenging problem, plays a
central role in enhancing the visibility of an image captured in a poor
illumination environment. Due to the fact that not all photons can pass the
Bayer-Filter on the sensor of the color camera, in this work, we first present
a De-Bayer-Filter simulator based on deep neural networks to generate a
monochrome raw image from the colored raw image. Next, a fully convolutional
network is proposed to achieve the low-light image enhancement by fusing
colored raw data with synthesized monochrome raw data. Channel-wise attention
is also introduced to the fusion process to establish a complementary
interaction between features from colored and monochrome raw images. To train
the convolutional networks, we propose a dataset with monochrome and color raw
pairs named Mono-Colored Raw paired dataset (MCR) collected by using a
monochrome camera without Bayer-Filter and a color camera with Bayer-Filter.
The proposed pipeline take advantages of the fusion of the virtual monochrome
and the color raw images and our extensive experiments indicate that
significant improvement can be achieved by leveraging raw sensor data and
data-driven learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Density-Aware Voxels for LiDAR 3D Object Detection. (arXiv:2203.05662v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05662">
<div class="article-summary-box-inner">
<span><p>LiDAR has become one of the primary 3D object detection sensors in autonomous
driving. However, LiDAR's diverging point pattern with increasing distance
results in a non-uniform sampled point cloud ill-suited to discretized
volumetric feature extraction. Current methods either rely on voxelized point
clouds or use inefficient farthest point sampling to mitigate detrimental
effects caused by density variation but largely ignore point density as a
feature and its predictable relationship with distance from the LiDAR sensor.
Our proposed solution, Point Density-Aware Voxel network (PDV), is an
end-to-end two stage LiDAR 3D object detection architecture that is designed to
account for these point density variations. PDV efficiently localizes voxel
features from the 3D sparse convolution backbone through voxel point centroids.
The spatially localized voxel features are then aggregated through a
density-aware RoI grid pooling module using kernel density estimation (KDE) and
self-attention with point density positional encoding. Finally, we exploit
LiDAR's point density to distance relationship to refine our final bounding box
confidences. PDV outperforms all state-of-the-art methods on the Waymo Open
Dataset and achieves competitive results on the KITTI dataset. We provide a
code release for PDV which is available at https://github.com/TRAILab/PDV.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation. (arXiv:2203.06558v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06558">
<div class="article-summary-box-inner">
<span><p>Training a generalizable 3D part segmentation network is quite challenging
but of great importance in real-world applications. To tackle this problem,
some works design task-specific solutions by translating human understanding of
the task to machine's learning process, which faces the risk of missing the
optimal strategy since machines do not necessarily understand in the exact
human way. Others try to use conventional task-agnostic approaches designed for
domain generalization problems with no task prior knowledge considered. To
solve the above issues, we propose AutoGPart, a generic method enabling
training generalizable 3D part segmentation networks with the task prior
considered. AutoGPart builds a supervision space with geometric prior knowledge
encoded, and lets the machine to search for the optimal supervisions from the
space for a specific segmentation task automatically. Extensive experiments on
three generalizable 3D part segmentation tasks are conducted to demonstrate the
effectiveness and versatility of AutoGPart. We demonstrate that the performance
of segmentation networks using simple backbones can be significantly improved
when trained with supervisions searched by our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. (arXiv:2203.08344v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08344">
<div class="article-summary-box-inner">
<span><p>We aim to improve the performance of regressing hand keypoints and segmenting
pixel-level hand masks under new imaging conditions (e.g., outdoors) when we
only have labeled images taken under very different conditions (e.g., indoors).
In the real world, it is important that the model trained for both tasks works
under various imaging conditions. However, their variation covered by existing
labeled hand datasets is limited. Thus, it is necessary to adapt the model
trained on the labeled images (source) to unlabeled images (target) with unseen
imaging conditions. While self-training domain adaptation methods (i.e.,
learning from the unlabeled target images in a self-supervised manner) have
been developed for both tasks, their training may degrade performance when the
predictions on the target images are noisy. To avoid this, it is crucial to
assign a low importance (confidence) weight to the noisy predictions during
self-training. In this paper, we propose to utilize the divergence of two
predictions to estimate the confidence of the target image for both tasks.
These predictions are given from two separate networks, and their divergence
helps identify the noisy predictions. To integrate our proposed confidence
estimation into self-training, we propose a teacher-student framework where the
two networks (teachers) provide supervision to a network (student) for
self-training, and the teachers are learned from the student by knowledge
distillation. Our experiments show its superiority over state-of-the-art
methods in adaptation settings with different lighting, grasping objects,
backgrounds, and camera viewpoints. Our method improves by 4% the multi-task
score on HO3D compared to the latest adversarial adaptation method. We also
validate our method on Ego4D, egocentric videos with rapid changes in imaging
conditions outdoors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding. (arXiv:2203.08481v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08481">
<div class="article-summary-box-inner">
<span><p>Visual grounding, i.e., localizing objects in images according to natural
language queries, is an important topic in visual language understanding. The
most effective approaches for this task are based on deep learning, which
generally require expensive manually labeled image-query or patch-query pairs.
To eliminate the heavy dependence on human annotations, we present a novel
method, named Pseudo-Q, to automatically generate pseudo language queries for
supervised training. Our method leverages an off-the-shelf object detector to
identify visual objects from unlabeled images, and then language queries for
these objects are obtained in an unsupervised fashion with a pseudo-query
generation module. Then, we design a task-related query prompt module to
specifically tailor generated pseudo language queries for visual grounding
tasks. Further, in order to fully capture the contextual relationships between
images and language queries, we develop a visual-language model equipped with
multi-level cross-modality attention mechanism. Extensive experimental results
demonstrate that our method has two notable benefits: (1) it can reduce human
annotation costs significantly, e.g., 31% on RefCOCO without degrading original
model's performance under the fully supervised setting, and (2) without bells
and whistles, it achieves superior or comparable performance compared to
state-of-the-art weakly-supervised visual grounding methods on all the five
datasets we have experimented. Code is available at
https://github.com/LeapLabTHU/Pseudo-Q.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topology-Preserving Shape Reconstruction and Registration via Neural Diffeomorphic Flow. (arXiv:2203.08652v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08652">
<div class="article-summary-box-inner">
<span><p>Deep Implicit Functions (DIFs) represent 3D geometry with continuous signed
distance functions learned through deep neural nets. Recently DIFs-based
methods have been proposed to handle shape reconstruction and dense point
correspondences simultaneously, capturing semantic relationships across shapes
of the same class by learning a DIFs-modeled shape template. These methods
provide great flexibility and accuracy in reconstructing 3D shapes and
inferring correspondences. However, the point correspondences built from these
methods do not intrinsically preserve the topology of the shapes, unlike
mesh-based template matching methods. This limits their applications on 3D
geometries where underlying topological structures exist and matter, such as
anatomical structures in medical images. In this paper, we propose a new model
called Neural Diffeomorphic Flow (NDF) to learn deep implicit shape templates,
representing shapes as conditional diffeomorphic deformations of templates,
intrinsically preserving shape topologies. The diffeomorphic deformation is
realized by an auto-decoder consisting of Neural Ordinary Differential Equation
(NODE) blocks that progressively map shapes to implicit templates. We conduct
extensive experiments on several medical image organ segmentation datasets to
evaluate the effectiveness of NDF on reconstructing and aligning shapes. NDF
achieves consistently state-of-the-art organ shape reconstruction and
registration results in both accuracy and quality. The source code is publicly
available at https://github.com/Siwensun/Neural_Diffeomorphic_Flow--NDF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Occlusion Fields: An Implicit Representation for Non-Line-of-Sight Surface Reconstruction. (arXiv:2203.08657v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08657">
<div class="article-summary-box-inner">
<span><p>Non-line-of-sight reconstruction (NLoS) is a novel indirect imaging modality
that aims to recover objects or scene parts outside the field of view from
measurements of light that is indirectly scattered off a directly visible,
diffuse wall. Despite recent advances in acquisition and reconstruction
techniques, the well-posedness of the problem at large, and the recoverability
of objects and their shapes in particular, remains an open question. The
commonly employed Fermat path criterion is rather conservative with this
regard, as it classifies some surfaces as unrecoverable, although they
contribute to the signal.
</p>
<p>In this paper, we use a simpler necessary criterion for an opaque surface
patch to be recoverable. Such piece of surface must be directly visible from
some point on the wall, and it must occlude the space behind itself. Inspired
by recent advances in neural implicit representations, we devise a new
representation and reconstruction technique for NLoS scenes that unifies the
treatment of recoverability with the reconstruction itself. Our approach, which
we validate on various synthetic and experimental datasets, exhibits
interesting properties. Unlike memory-inefficient volumetric representations,
ours allows to infer adaptively tessellated surfaces from time-of-flight
measurements of moderate resolution. It can further recover features beyond the
Fermat path criterion, and it is robust to significant amounts of
self-occlusion. We believe that this is the first time that these properties
have been achieved in one system that, as an additional benefit, is trainable
and hence suited for data-driven approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Know your sensORs -- A Modality Study For Surgical Action Classification. (arXiv:2203.08674v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08674">
<div class="article-summary-box-inner">
<span><p>The surgical operating room (OR) presents many opportunities for automation
and optimization. Videos from various sources in the OR are becoming
increasingly available. The medical community seeks to leverage this wealth of
data to develop automated methods to advance interventional care, lower costs,
and improve overall patient outcomes. Existing datasets from OR room cameras
are thus far limited in size or modalities acquired, leaving it unclear which
sensor modalities are best suited for tasks such as recognizing surgical action
from videos. This study demonstrates that surgical action recognition
performance can vary depending on the image modalities used. We perform a
methodical analysis on several commonly available sensor modalities, presenting
two fusion approaches that improve classification performance. The analyses are
carried out on a set of multi-view RGB-D video recordings of 18 laparoscopic
procedures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-directional Object-context Prioritization Learning for Saliency Ranking. (arXiv:2203.09416v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09416">
<div class="article-summary-box-inner">
<span><p>The saliency ranking task is recently proposed to study the visual behavior
that humans would typically shift their attention over different objects of a
scene based on their degrees of saliency. Existing approaches focus on learning
either object-object or object-scene relations. Such a strategy follows the
idea of object-based attention in Psychology, but it tends to favor those
objects with strong semantics (e.g., humans), resulting in unrealistic saliency
ranking. We observe that spatial attention works concurrently with object-based
attention in the human visual recognition system. During the recognition
process, the human spatial attention mechanism would move, engage, and
disengage from region to region (i.e., context to context). This inspires us to
model the region-level interactions, in addition to the object-level reasoning,
for saliency ranking. To this end, we propose a novel bi-directional method to
unify spatial attention and object-based attention for saliency ranking. Our
model includes two novel modules: (1) a selective object saliency (SOS) module
that models objectbased attention via inferring the semantic representation of
the salient object, and (2) an object-context-object relation (OCOR) module
that allocates saliency ranks to objects by jointly modeling the object-context
and context-object interactions of the salient objects. Extensive experiments
show that our approach outperforms existing state-of-theart methods. Our code
and pretrained model are available at https://github.com/GrassBro/OCOR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09481">
<div class="article-summary-box-inner">
<span><p>Denoising diffusion probabilistic models are a promising new class of
generative models that are competitive with GANs on perceptual metrics. In this
paper, we explore their potential for sequentially generating video. Inspired
by recent advances in neural video compression, we use denoising diffusion
models to stochastically generate a residual to a deterministic next-frame
prediction. We compare this approach to two sequential VAE and two GAN
baselines on four datasets, where we test the generated frames for perceptual
quality and forecasting accuracy against ground truth frames. We find
significant improvements in terms of perceptual quality on all data and
improvements in terms of frame forecasting for complex high-resolution videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MatchFormer: Interleaving Attention in Transformers for Feature Matching. (arXiv:2203.09645v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09645">
<div class="article-summary-box-inner">
<span><p>Local feature matching is a computationally intensive task at the subpixel
level. While detector-based methods coupled with feature descriptors struggle
in low-texture scenes, CNN-based methods with a sequential extract-to-match
pipeline, fail to make use of the matching capacity of the encoder and tend to
overburden the decoder for matching. In contrast, we propose a novel
hierarchical extract-and-match transformer, termed as MatchFormer. Inside each
stage of the hierarchical encoder, we interleave self-attention for feature
extraction and cross-attention for feature matching, enabling a human-intuitive
extract-and-match scheme. Such a match-aware encoder releases the overloaded
decoder and makes the model highly efficient. Further, combining self- and
cross-attention on multi-scale features in a hierarchical architecture improves
matching robustness, particularly in low-texture indoor scenes or with less
outdoor training data. Thanks to such a strategy, MatchFormer is a multi-win
solution in efficiency, robustness, and precision. Compared to the previous
best method in indoor pose estimation, our lite MatchFormer has only 45%
GFLOPs, yet achieves a +1.3% precision gain and a 41% running speed boost. The
large MatchFormer reaches state-of-the-art on four different benchmarks,
including indoor pose estimation (ScanNet), outdoor pose estimation
(MegaDepth), homography estimation and image matching (HPatch), and visual
localization (InLoc). Code will be made publicly available at
https://github.com/jamycheung/MatchFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TVConv: Efficient Translation Variant Convolution for Layout-aware Visual Processing. (arXiv:2203.10489v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10489">
<div class="article-summary-box-inner">
<span><p>As convolution has empowered many smart applications, dynamic convolution
further equips it with the ability to adapt to diverse inputs. However, the
static and dynamic convolutions are either layout-agnostic or
computation-heavy, making it inappropriate for layout-specific applications,
e.g., face recognition and medical image segmentation. We observe that these
applications naturally exhibit the characteristics of large intra-image
(spatial) variance and small cross-image variance. This observation motivates
our efficient translation variant convolution (TVConv) for layout-aware visual
processing. Technically, TVConv is composed of affinity maps and a
weight-generating block. While affinity maps depict pixel-paired relationships
gracefully, the weight-generating block can be explicitly overparameterized for
better training while maintaining efficient inference. Although conceptually
simple, TVConv significantly improves the efficiency of the convolution and can
be readily plugged into various network architectures. Extensive experiments on
face recognition show that TVConv reduces the computational cost by up to 3.1x
and improves the corresponding throughput by 2.3x while maintaining a high
accuracy compared to the depthwise convolution. Moreover, for the same
computation cost, we boost the mean accuracy by up to 4.21%. We also conduct
experiments on the optic disc/cup segmentation task and obtain better
generalization performance, which helps mitigate the critical data scarcity
issue. Code is available at https://github.com/JierunChen/TVConv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization. (arXiv:2203.10492v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10492">
<div class="article-summary-box-inner">
<span><p>Recently self-supervised representation learning has drawn considerable
attention from the scene text recognition community. Different from previous
studies using contrastive learning, we tackle the issue from an alternative
perspective, i.e., by formulating the representation learning scheme in a
generative manner. Typically, the neighboring image patches among one text line
tend to have similar styles, including the strokes, textures, colors, etc.
Motivated by this common sense, we augment one image patch and use its
neighboring patch as guidance to recover itself. Specifically, we propose a
Similarity-Aware Normalization (SimAN) module to identify the different
patterns and align the corresponding styles from the guiding patch. In this
way, the network gains representation capability for distinguishing complex
patterns such as messy strokes and cluttered backgrounds. Experiments show that
the proposed SimAN significantly improves the representation quality and
achieves promising performance. Moreover, we surprisingly find that our
self-supervised generative network has impressive potential for data synthesis,
text image editing, and font interpolation, which suggests that the proposed
SimAN has a wide range of practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRISPnet: Color Rendition ISP Net. (arXiv:2203.10562v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10562">
<div class="article-summary-box-inner">
<span><p>Image signal processors (ISPs) are historically grown legacy software systems
for reconstructing color images from noisy raw sensor measurements. They are
usually composited of many heuristic blocks for denoising, demosaicking, and
color restoration. Color reproduction in this context is of particular
importance, since the raw colors are often severely distorted, and each smart
phone manufacturer has developed their own characteristic heuristics for
improving the color rendition, for example of skin tones and other visually
important colors.
</p>
<p>In recent years there has been strong interest in replacing the historically
grown ISP systems with deep learned pipelines. Much progress has been made in
approximating legacy ISPs with such learned models. However, so far the focus
of these efforts has been on reproducing the structural features of the images,
with less attention paid to color rendition.
</p>
<p>Here we present CRISPnet, the first learned ISP model to specifically target
color rendition accuracy relative to a complex, legacy smart phone ISP. We
achieve this by utilizing both image metadata (like a legacy ISP would), as
well as by learning simple global semantics based on image classification --
similar to what a legacy ISP does to determine the scene type. We also
contribute a new ISP image dataset consisting of both high dynamic range
monitor data, as well as real-world data, both captured with an actual cell
phone ISP pipeline under a variety of lighting conditions, exposure times, and
gain settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transform your Smartphone into a DSLR Camera: Learning the ISP in the Wild. (arXiv:2203.10636v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10636">
<div class="article-summary-box-inner">
<span><p>We propose a trainable Image Signal Processing (ISP) framework that produces
DSLR quality images given RAW images captured by a smartphone. To address the
color misalignments between training image pairs, we employ a color-conditional
ISP network and optimize a novel parametric color mapping between each input
RAW and reference DSLR image. During inference, we predict the target color
image by designing a color prediction network with efficient Global Context
Transformer modules. The latter effectively leverage global information to
learn consistent color and tone mappings. We further propose a robust masked
aligned loss to identify and discard regions with inaccurate motion estimation
during training. Lastly, we introduce the ISP in the Wild (ISPW) dataset,
consisting of weakly paired phone RAW and DSLR sRGB images. We extensively
evaluate our method, setting a new state-of-the-art on two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation. (arXiv:2203.10739v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10739">
<div class="article-summary-box-inner">
<span><p>Sparsely annotated semantic segmentation (SASS) aims to train a segmentation
network with coarse-grained (i.e., point-, scribble-, and block-wise)
supervisions, where only a small proportion of pixels are labeled in each
image. In this paper, we propose a novel tree energy loss for SASS by providing
semantic guidance for unlabeled pixels. The tree energy loss represents images
as minimum spanning trees to model both low-level and high-level pair-wise
affinities. By sequentially applying these affinities to the network
prediction, soft pseudo labels for unlabeled pixels are generated in a
coarse-to-fine manner, achieving dynamic online self-training. The tree energy
loss is effective and easy to be incorporated into existing frameworks by
combining it with a traditional segmentation loss. Compared with previous SASS
methods, our method requires no multistage training strategies, alternating
optimization procedures, additional supervised data, or time-consuming
post-processing while outperforming them in all SASS settings. Code is
available at https://github.com/megvii-research/TreeEnergyLoss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperbolic Vision Transformers: Combining Improvements in Metric Learning. (arXiv:2203.10833v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10833">
<div class="article-summary-box-inner">
<span><p>Metric learning aims to learn a highly discriminative model encouraging the
embeddings of similar classes to be close in the chosen metrics and pushed
apart for dissimilar ones. The common recipe is to use an encoder to extract
embeddings and a distance-based loss function to match the representations --
usually, the Euclidean distance is utilized. An emerging interest in learning
hyperbolic data embeddings suggests that hyperbolic geometry can be beneficial
for natural data. Following this line of work, we propose a new
hyperbolic-based model for metric learning. At the core of our method is a
vision transformer with output embeddings mapped to hyperbolic space. These
embeddings are directly optimized using modified pairwise cross-entropy loss.
We evaluate the proposed model with six different formulations on four datasets
achieving the new state-of-the-art performance. The source code is available at
https://github.com/htdt/hyp_metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computational ergonomics for task delegation in Human-Robot Collaboration: spatiotemporal adaptation of the robot to the human through contactless gesture recognition. (arXiv:2203.11007v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11007">
<div class="article-summary-box-inner">
<span><p>The high prevalence of work-related musculoskeletal disorders (WMSDs) could
be addressed by optimizing Human-Robot Collaboration (HRC) frameworks for
manufacturing applications. In this context, this paper proposes two hypotheses
for ergonomically effective task delegation and HRC. The first hypothesis
states that it is possible to quantify ergonomically professional tasks using
motion data from a reduced set of sensors. Then, the most dangerous tasks can
be delegated to a collaborative robot. The second hypothesis is that by
including gesture recognition and spatial adaptation, the ergonomics of an HRC
scenario can be improved by avoiding needless motions that could expose
operators to ergonomic risks and by lowering the physical effort required of
operators. An HRC scenario for a television manufacturing process is optimized
to test both hypotheses. For the ergonomic evaluation, motion primitives with
known ergonomic risks were modeled for their detection in professional tasks
and to estimate a risk score based on the European Assembly Worksheet (EAWS). A
Deep Learning gesture recognition module trained with egocentric television
assembly data was used to complement the collaboration between the human
operator and the robot. Additionally, a skeleton-tracking algorithm provided
the robot with information about the operator's pose, allowing it to spatially
adapt its motion to the operator's anthropometrics. Three experiments were
conducted to determine the effect of gesture recognition and spatial adaptation
on the operator's range of motion. The rate of spatial adaptation was used as a
key performance indicator (KPI), and a new KPI for measuring the reduction in
the operator's motion is presented in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Operator Sketching for Deep Unrolling Networks. (arXiv:2203.11156v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11156">
<div class="article-summary-box-inner">
<span><p>In this work we propose a new paradigm for designing efficient deep unrolling
networks using operator sketching. The deep unrolling networks are currently
the state-of-the-art solutions for imaging inverse problems. However, for
high-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI
imaging, the deep unrolling schemes typically become inefficient both in terms
of memory and computation, due to the need of computing multiple times the
high-dimensional forward and adjoint operators. Recently researchers have found
that such limitations can be partially addressed by stochastic unrolling with
subsets of operators, inspired by the success of stochastic first-order
optimization. In this work, we propose a further acceleration upon stochastic
unrolling, using sketching techniques to approximate products in the
high-dimensional image space. The operator sketching can be jointly applied
with stochastic unrolling for the best acceleration and compression
performance. Our numerical experiments on X-ray CT image reconstruction
demonstrate the remarkable effectiveness of our sketched unrolling schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A Review and Experimental Evaluation. (arXiv:2010.06255v5 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06255">
<div class="article-summary-box-inner">
<span><p>Aerial tracking, which has exhibited its omnipresent dedication and splendid
performance, is one of the most active applications in the remote sensing
field. Especially, unmanned aerial vehicle (UAV)-based remote sensing system,
equipped with a visual tracking approach, has been widely used in aviation,
navigation, agriculture,transportation, and public security, etc. As is
mentioned above, the UAV-based aerial tracking platform has been gradually
developed from research to practical application stage, reaching one of the
main aerial remote sensing technologies in the future. However, due to the
real-world onerous situations, e.g., harsh external challenges, the vibration
of the UAV mechanical structure (especially under strong wind conditions), the
maneuvering flight in complex environment, and the limited computation
resources onboard, accuracy, robustness, and high efficiency are all crucial
for the onboard tracking methods. Recently, the discriminative correlation
filter (DCF)-based trackers have stood out for their high computational
efficiency and appealing robustness on a single CPU, and have flourished in the
UAV visual tracking community. In this work, the basic framework of the
DCF-based trackers is firstly generalized, based on which, 23 state-of-the-art
DCF-based trackers are orderly summarized according to their innovations for
solving various issues. Besides, exhaustive and quantitative experiments have
been extended on various prevailing UAV tracking benchmarks, i.e., UAV123,
UAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903
frames in total. The experiments show the performance, verify the feasibility,
and demonstrate the current challenges of DCF-based trackers onboard UAV
tracking.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-23 23:13:35.259058844 UTC">2022-03-23 23:13:35 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>