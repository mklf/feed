<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-13T01:30:00Z">07-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2021. (arXiv:2207.05133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05133">
<div class="article-summary-box-inner">
<span><p>Automatic detection of fake news is a highly important task in the
contemporary world. This study reports the 2nd shared task called
UrduFake@FIRE2021 on identifying fake news detection in Urdu. The goal of the
shared task is to motivate the community to come up with efficient methods for
solving this vital problem, particularly for the Urdu language. The task is
posed as a binary classification problem to label a given news article as a
real or a fake news article. The organizers provide a dataset comprising news
in five domains: (i) Health, (ii) Sports, (iii) Showbiz, (iv) Technology, and
(v) Business, split into training and testing sets. The training set contains
1300 annotated news articles -- 750 real news, 550 fake news, while the testing
set contains 300 news articles -- 200 real, 100 fake news. 34 teams from 7
different countries (China, Egypt, Israel, India, Mexico, Pakistan, and UAE)
registered to participate in the UrduFake@FIRE2021 shared task. Out of those,
18 teams submitted their experimental results, and 11 of those submitted their
technical reports, which is substantially higher compared to the UrduFake
shared task in 2020 when only 6 teams submitted their technical reports. The
technical reports submitted by the participants demonstrated different data
representation techniques ranging from count-based BoW features to word vector
embeddings as well as the use of numerous machine learning algorithms ranging
from traditional SVM to various neural network architectures including
Transformers such as BERT and RoBERTa. In this year's competition, the best
performing system obtained an F1-macro score of 0.679, which is lower than the
past year's best result of 0.907 F1-macro. Admittedly, while training sets from
the past and the current years overlap to a large extent, the testing set
provided this year is completely different.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UrduFake@FIRE2021: Shared Track on Fake News Identification in Urdu. (arXiv:2207.05144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05144">
<div class="article-summary-box-inner">
<span><p>This study reports the second shared task named as UrduFake@FIRE2021 on
identifying fake news detection in Urdu language. This is a binary
classification problem in which the task is to classify a given news article
into two classes: (i) real news, or (ii) fake news. In this shared task, 34
teams from 7 different countries (China, Egypt, Israel, India, Mexico,
Pakistan, and UAE) registered to participate in the shared task, 18 teams
submitted their experimental results and 11 teams submitted their technical
reports. The proposed systems were based on various count-based features and
used different classifiers as well as neural network architectures. The
stochastic gradient descent (SGD) algorithm outperformed other classifiers and
achieved 0.679 F-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Models perform Abductive Commonsense Reasoning?. (arXiv:2207.05155v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05155">
<div class="article-summary-box-inner">
<span><p>Abductive Reasoning is a task of inferring the most plausible hypothesis
given a set of observations. In literature, the community has approached to
solve this challenge by classifying/generating a likely hypothesis that does
not contradict with a past observation and future observation. Some of the most
well-known benchmarks that tackle this problem are aNLI and aNLG (pronounced as
alpha-NLI and alpha-NLG). In this report, I review over some of the
methodologies that were attempted to solve this challenge, re-implement the
baseline models, and analyze some of the weaknesses that current approaches
have. The code and the re-implemented results are available at this link.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Induction enabling Recommending and Trend Analysis: A Corporate Research Community Use Case. (arXiv:2207.05188v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05188">
<div class="article-summary-box-inner">
<span><p>A research division plays an important role of driving innovation in an
organization. Drawing insights, following trends, keeping abreast of new
research, and formulating strategies are increasingly becoming more challenging
for both researchers and executives as the amount of information grows in both
velocity and volume. In this paper we present a use case of how a corporate
research community, IBM Research, utilizes Semantic Web technologies to induce
a unified Knowledge Graph from both structured and textual data obtained by
integrating various applications used by the community related to research
projects, academic papers, datasets, achievements and recognition. In order to
make the Knowledge Graph more accessible to application developers, we
identified a set of common patterns for exploiting the induced knowledge and
exposed them as APIs. Those patterns were born out of user research which
identified the most valuable use cases or user pain points to be alleviated. We
outline two distinct scenarios: recommendation and analytics for business use.
We will discuss these scenarios in detail and provide an empirical evaluation
on entity recommendation specifically. The methodology used and the lessons
learned from this work can be applied to other organizations facing similar
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Neural Numeric-To-Text Generation From Temporal Personal Health Data. (arXiv:2207.05194v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05194">
<div class="article-summary-box-inner">
<span><p>With an increased interest in the production of personal health technologies
designed to track user data (e.g., nutrient intake, step counts), there is now
more opportunity than ever to surface meaningful behavioral insights to
everyday users in the form of natural language. This knowledge can increase
their behavioral awareness and allow them to take action to meet their health
goals. It can also bridge the gap between the vast collection of personal
health data and the summary generation required to describe an individual's
behavioral tendencies. Previous work has focused on rule-based time-series data
summarization methods designed to generate natural language summaries of
interesting patterns found within temporal personal health data. We examine
recurrent, convolutional, and Transformer-based encoder-decoder models to
automatically generate natural language summaries from numeric temporal
personal health data. We showcase the effectiveness of our models on real user
health data logged in MyFitnessPal and show that we can automatically generate
high-quality natural language summaries. Our work serves as a first step
towards the ambitious goal of automatically generating novel and meaningful
temporal summaries from personal health data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models (Mostly) Know What They Know. (arXiv:2207.05221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05221">
<div class="article-summary-box-inner">
<span><p>We study whether language models can evaluate the validity of their own
claims and predict which questions they will be able to answer correctly. We
first show that larger models are well-calibrated on diverse multiple choice
and true/false questions when they are provided in the right format. Thus we
can approach self-evaluation on open-ended sampling tasks by asking models to
first propose answers, and then to evaluate the probability "P(True)" that
their answers are correct. We find encouraging performance, calibration, and
scaling for P(True) on a diverse array of tasks. Performance at self-evaluation
further improves when we allow models to consider many of their own samples
before predicting the validity of one specific possibility. Next, we
investigate whether models can be trained to predict "P(IK)", the probability
that "I know" the answer to a question, without reference to any particular
proposed answer. Models perform well at predicting P(IK) and partially
generalize across tasks, though they struggle with calibration of P(IK) on new
tasks. The predicted P(IK) probabilities also increase appropriately in the
presence of relevant source materials in the context, and to the presence of
hints towards the solution of mathematical word problems. We hope these
observations lay the groundwork for training more honest models, and for
investigating how honesty generalizes to cases where models are trained on
objectives other than the imitation of human writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bootstrapping a User-Centered Task-Oriented Dialogue System. (arXiv:2207.05223v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05223">
<div class="article-summary-box-inner">
<span><p>We present TacoBot, a task-oriented dialogue system built for the inaugural
Alexa Prize TaskBot Challenge, which assists users in completing multi-step
cooking and home improvement tasks. TacoBot is designed with a user-centered
principle and aspires to deliver a collaborative and accessible dialogue
experience. Towards that end, it is equipped with accurate language
understanding, flexible dialogue management, and engaging response generation.
Furthermore, TacoBot is backed by a strong search engine and an automated
end-to-end test suite. In bootstrapping the development of TacoBot, we explore
a series of data augmentation strategies to train advanced neural language
processing models and continuously improve the dialogue experience with
collected real conversations. At the end of the semifinals, TacoBot achieved an
average rating of 3.55/5.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Korean Sign Language Augmentation (KoSLA) Corpus with Data Augmentation Technique. (arXiv:2207.05261v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05261">
<div class="article-summary-box-inner">
<span><p>We present an efficient framework of corpus for sign language translation.
Aided with a simple but dramatic data augmentation technique, our method
converts text into annotated forms with minimum information loss. Sign
languages are composed of manual signals, non-manual signals, and iconic
features. According to professional sign language interpreters, non-manual
signals such as facial expressions and gestures play an important role in
conveying exact meaning. By considering the linguistic features of sign
language, our proposed framework is a first and unique attempt to build a
multimodal sign language augmentation corpus (hereinafter referred to as the
KoSLA corpus) containing both manual and non-manual modalities. The corpus we
built demonstrates confident results in the hospital context, showing improved
performance with augmented datasets. To overcome data scarcity, we resorted to
data augmentation techniques such as synonym replacement to boost the
efficiency of our translation model and available data, while maintaining
grammatical and semantic structures of sign language. For the experimental
support, we verify the effectiveness of data augmentation technique and
usefulness of our corpus by performing a translation task between normal
sentences and sign language annotations on two tokenizers. The result was
convincing, proving that the BLEU scores with the KoSLA corpus were
significant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Table Question Answering: Recent Advances. (arXiv:2207.05270v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05270">
<div class="article-summary-box-inner">
<span><p>Table Question Answering (Table QA) refers to providing precise answers from
tables to answer a user's question. In recent years, there have been a lot of
works on table QA, but there is a lack of comprehensive surveys on this
research topic. Hence, we aim to provide an overview of available datasets and
representative methods in table QA. We classify existing methods for table QA
into five categories according to their techniques, which include
semantic-parsing-based, generative, extractive, matching-based, and
retriever-reader-based methods. Moreover, as table QA is still a challenging
task for existing methods, we also identify and outline several key challenges
and discuss the potential future directions of table QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Few-Shot Named Entity Linking by Meta-Learning. (arXiv:2207.05280v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05280">
<div class="article-summary-box-inner">
<span><p>Entity linking aims to link ambiguous mentions to their corresponding
entities in a knowledge base, which is significant and fundamental for various
downstream applications, e.g., knowledge base completion, question answering,
and information extraction. While great efforts have been devoted to this task,
most of these studies follow the assumption that large-scale labeled data is
available. However, when the labeled data is insufficient for specific domains
due to labor-intensive annotation work, the performance of existing algorithms
will suffer an intolerable decline. In this paper, we endeavor to solve the
problem of few-shot entity linking, which only requires a minimal amount of
in-domain labeled data and is more practical in real situations. Specifically,
we firstly propose a novel weak supervision strategy to generate non-trivial
synthetic entity-mention pairs based on mention rewriting. Since the quality of
the synthetic data has a critical impact on effective model training, we
further design a meta-learning mechanism to assign different weights to each
synthetic entity-mention pair automatically. Through this way, we can
profoundly exploit rich and precious semantic information to derive a
well-trained entity linking model under the few-shot setting. The experiments
on real-world datasets show that the proposed method can extensively improve
the state-of-the-art few-shot entity linking model and achieve impressive
performance when only a small amount of labeled data is available. Moreover, we
also demonstrate the outstanding ability of the model's transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLM-ICD: Automatic ICD Coding with Pretrained Language Models. (arXiv:2207.05289v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05289">
<div class="article-summary-box-inner">
<span><p>Automatically classifying electronic health records (EHRs) into diagnostic
codes has been challenging to the NLP community. State-of-the-art methods
treated this problem as a multilabel classification problem and proposed
various architectures to model this problem. However, these systems did not
leverage the superb performance of pretrained language models, which achieved
superb performance on natural language understanding tasks. Prior work has
shown that pretrained language models underperformed on this task with the
regular finetuning scheme. Therefore, this paper aims at analyzing the causes
of the underperformance and developing a framework for automatic ICD coding
with pretrained language models. We spotted three main issues through the
experiments: 1) large label space, 2) long input sequences, and 3) domain
mismatch between pretraining and fine-tuning. We propose PLMICD, a framework
that tackles the challenges with various strategies. The experimental results
show that our proposed framework can overcome the challenges and achieves
state-of-the-art performance in terms of multiple metrics on the benchmark
MIMIC data. The source code is available at https://github.com/MiuLab/PLM-ICD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations. (arXiv:2207.05324v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05324">
<div class="article-summary-box-inner">
<span><p>Translation, rotation, and scaling are three commonly used geometric
manipulation operations in image processing. Besides, some of them are
successfully used in developing effective knowledge graph embedding (KGE)
models such as TransE and RotatE. Inspired by the synergy, we propose a new KGE
model by leveraging all three operations in this work. Since translation,
rotation, and scaling operations are cascaded to form a compound one, the new
model is named CompoundE. By casting CompoundE in the framework of group
theory, we show that quite a few scoring-function-based KGE models are special
cases of CompoundE. CompoundE extends the simple distance-based relation to
relation-dependent compound operations on head and/or tail entities. To
demonstrate the effectiveness of CompoundE, we conduct experiments on three
popular KG completion datasets. Experimental results show that CompoundE
consistently achieves the state of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Huqariq: A Multilingual Speech Corpus of Native Languages of Peru for Speech Recognition. (arXiv:2207.05498v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05498">
<div class="article-summary-box-inner">
<span><p>The Huqariq corpus is a multilingual collection of speech from native
Peruvian languages. The transcribed corpus is intended for the research and
development of speech technologies to preserve endangered languages in Peru.
Huqariq is primarily designed for the development of automatic speech
recognition, language identification and text-to-speech tools. In order to
achieve corpus collection sustainably, we employ the crowdsourcing methodology.
Huqariq includes four native languages of Peru, and it is expected that by the
end of the year 2022, it can reach up to 20 native languages out of the 48
native languages in Peru. The corpus has 220 hours of transcribed audio
recorded by more than 500 volunteers, making it the largest speech corpus for
native languages in Peru. In order to verify the quality of the corpus, we
present speech recognition experiments using 220 hours of fully transcribed
audio.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoeticTTS -- Controllable Poetry Reading for Literary Studies. (arXiv:2207.05549v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05549">
<div class="article-summary-box-inner">
<span><p>Speech synthesis for poetry is challenging due to specific intonation
patterns inherent to poetic speech. In this work, we propose an approach to
synthesise poems with almost human like naturalness in order to enable literary
scholars to systematically examine hypotheses on the interplay between text,
spoken realisation, and the listener's perception of poems. To meet these
special requirements for literary studies, we resynthesise poems by cloning
prosodic values from a human reference recitation, and afterwards make use of
fine-grained prosody control to manipulate the synthetic speech in a
human-in-the-loop setting to alter the recitation w.r.t. specific phenomena. We
find that finetuning our TTS model on poetry captures poetic intonation
patterns to a large extent which is beneficial for prosody cloning and
manipulation and verify the success of our approach both in an objective
evaluation as well as in human studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Paraphrases to Study Properties of Contextual Embeddings. (arXiv:2207.05553v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05553">
<div class="article-summary-box-inner">
<span><p>We use paraphrases as a unique source of data to analyze contextualized
embeddings, with a particular focus on BERT. Because paraphrases naturally
encode consistent word and phrase semantics, they provide a unique lens for
investigating properties of embeddings. Using the Paraphrase Database's
alignments, we study words within paraphrases as well as phrase
representations. We find that contextual embeddings effectively handle
polysemous words, but give synonyms surprisingly different representations in
many cases. We confirm previous findings that BERT is sensitive to word order,
but find slightly different patterns than prior work in terms of the level of
contextualization across BERT's layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear-time calculation of the expected sum of edge lengths in planar linearizations of trees. (arXiv:2207.05564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05564">
<div class="article-summary-box-inner">
<span><p>Dependency graphs have proven to be a very successful model to represent the
syntactic structure of sentences of human languages. In these graphs, widely
accepted to be trees, vertices are words and arcs connect
syntactically-dependent words. The tendency of these dependencies to be short
has been demonstrated using random baselines for the sum of the lengths of the
edges or its variants. A ubiquitous baseline is the expected sum in projective
orderings (wherein edges do not cross and the root word of the sentence is not
covered by any edge). It was shown that said expected value can be computed in
$O(n)$ time. In this article we focus on planar orderings (where the root word
can be covered) and present two main results. First, we show the relationship
between the expected sum in planar arrangements and the expected sum in
projective arrangements. Second, we also derive a $O(n)$-time algorithm to
calculate the expected value of the sum of edge lengths. These two results stem
from another contribution of the present article, namely a characterization of
planarity that, given a sentence, yields either the number of planar
permutations or an efficient algorithm to generate uniformly random planar
permutations of the words. Our research paves the way for replicating past
research on dependency distance minimization using random planar linearizations
as random baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inner Monologue: Embodied Reasoning through Planning with Language Models. (arXiv:2207.05608v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05608">
<div class="article-summary-box-inner">
<span><p>Recent works have shown how the reasoning capabilities of Large Language
Models (LLMs) can be applied to domains beyond natural language processing,
such as planning and interaction for robots. These embodied problems require an
agent to understand many semantic aspects of the world: the repertoire of
skills available, how these skills influence the world, and how changes to the
world map back to the language. LLMs planning in embodied environments need to
consider not just what skills to do, but also how and when to do them - answers
that change over time in response to the agent's own choices. In this work, we
investigate to what extent LLMs used in such embodied contexts can reason over
sources of feedback provided through natural language, without any additional
training. We propose that by leveraging environment feedback, LLMs are able to
form an inner monologue that allows them to more richly process and plan in
robotic control scenarios. We investigate a variety of sources of feedback,
such as success detection, scene description, and human interaction. We find
that closed-loop language feedback significantly improves high-level
instruction completion on three domains, including simulated and real table top
rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen
environment in the real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Cross-lingual Transfer is Under-specified Optimization. (arXiv:2207.05666v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05666">
<div class="article-summary-box-inner">
<span><p>Pretrained multilingual encoders enable zero-shot cross-lingual transfer, but
often produce unreliable models that exhibit high performance variance on the
target language. We postulate that this high variance results from zero-shot
cross-lingual transfer solving an under-specified optimization problem. We show
that any linear-interpolated model between the source language monolingual
model and source + target bilingual model has equally low source language
generalization error, yet the target language generalization error reduces
smoothly and linearly as we move from the monolingual to bilingual model,
suggesting that the model struggles to identify good solutions for both source
and target languages using the source language alone. Additionally, we show
that zero-shot solution lies in non-flat region of target language error
generalization surface, causing the high variance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Contribution of Lyrics and Acoustics to Collaborative Understanding of Mood. (arXiv:2207.05680v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05680">
<div class="article-summary-box-inner">
<span><p>In this work, we study the association between song lyrics and mood through a
data-driven analysis. Our data set consists of nearly one million songs, with
song-mood associations derived from user playlists on the Spotify streaming
platform. We take advantage of state-of-the-art natural language processing
models based on transformers to learn the association between the lyrics and
moods. We find that a pretrained transformer-based language model in a
zero-shot setting -- i.e., out of the box with no further training on our data
-- is powerful for capturing song-mood associations. Moreover, we illustrate
that training on song-mood associations results in a highly accurate model that
predicts these associations for unseen songs. Furthermore, by comparing the
prediction of a model using lyrics with one using acoustic features, we observe
that the relative importance of lyrics for mood prediction in comparison with
acoustics depends on the specific mood. Finally, we verify if the models are
capturing the same information about lyrics and acoustics as humans through an
annotation task where we obtain human judgments of mood-song relevance based on
lyrics and acoustics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The MuSe 2022 Multimodal Sentiment Analysis Challenge: Humor, Emotional Reactions, and Stress. (arXiv:2207.05691v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05691">
<div class="article-summary-box-inner">
<span><p>The Multimodal Sentiment Analysis Challenge (MuSe) 2022 is dedicated to
multimodal sentiment and emotion recognition. For this year's challenge, we
feature three datasets: (i) the Passau Spontaneous Football Coach Humor
(Passau-SFCH) dataset that contains audio-visual recordings of German football
coaches, labelled for the presence of humour; (ii) the Hume-Reaction dataset in
which reactions of individuals to emotional stimuli have been annotated with
respect to seven emotional expression intensities, and (iii) the Ulm-Trier
Social Stress Test (Ulm-TSST) dataset comprising of audio-visual data labelled
with continuous emotion values (arousal and valence) of people in stressful
dispositions. Using the introduced datasets, MuSe 2022 2022 addresses three
contemporary affective computing problems: in the Humor Detection Sub-Challenge
(MuSe-Humor), spontaneous humour has to be recognised; in the Emotional
Reactions Sub-Challenge (MuSe-Reaction), seven fine-grained `in-the-wild'
emotions have to be predicted; and in the Emotional Stress Sub-Challenge
(MuSe-Stress), a continuous prediction of stressed emotion values is featured.
The challenge is designed to attract different research communities,
encouraging a fusion of their disciplines. Mainly, MuSe 2022 targets the
communities of audio-visual emotion recognition, health informatics, and
symbolic sentiment analysis. This baseline paper describes the datasets as well
as the feature sets extracted from them. A recurrent neural network with LSTM
cells is used to set competitive baseline results on the test partitions for
each sub-challenge. We report an Area Under the Curve (AUC) of .8480 for
MuSe-Humor; .2801 mean (from 7-classes) Pearson's Correlations Coefficient for
MuSe-Reaction, as well as .4931 Concordance Correlation Coefficient (CCC) and
.4761 for valence and arousal in MuSe-Stress, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lip-Listening: Mixing Senses to Understand Lips using Cross Modality Knowledge Distillation for Word-Based Models. (arXiv:2207.05692v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05692">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a technique to transfer speech recognition
capabilities from audio speech recognition systems to visual speech
recognizers, where our goal is to utilize audio data during lipreading model
training. Impressive progress in the domain of speech recognition has been
exhibited by audio and audio-visual systems. Nevertheless, there is still much
to be explored with regards to visual speech recognition systems due to the
visual ambiguity of some phonemes. To this end, the development of visual
speech recognition models is crucial given the instability of audio models. The
main contributions of this work are i) building on recent state-of-the-art
word-based lipreading models by integrating sequence-level and frame-level
Knowledge Distillation (KD) to their systems; ii) leveraging audio data during
training visual models, a feat which has not been utilized in prior word-based
work; iii) proposing the Gaussian-shaped averaging in frame-level KD, as an
efficient technique that aids the model in distilling knowledge at the sequence
model encoder. This work proposes a novel and competitive architecture for
lip-reading, as we demonstrate a noticeable improvement in performance, setting
a new benchmark equals to 88.64% on the LRW dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Do Multilingual Encoders Learn Cross-lingual Representation?. (arXiv:2207.05737v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05737">
<div class="article-summary-box-inner">
<span><p>NLP systems typically require support for more than one language. As
different languages have different amounts of supervision, cross-lingual
transfer benefits languages with little to no training data by transferring
from other languages. From an engineering perspective, multilingual NLP
benefits development and maintenance by serving multiple languages with a
single system. Both cross-lingual transfer and multilingual NLP rely on
cross-lingual representations serving as the foundation. As BERT revolutionized
representation learning and NLP, it also revolutionized cross-lingual
representations and cross-lingual transfer. Multilingual BERT was released as a
replacement for single-language BERT, trained with Wikipedia data in 104
languages.
</p>
<p>Surprisingly, without any explicit cross-lingual signal, multilingual BERT
learns cross-lingual representations in addition to representations for
individual languages. This thesis first shows such surprising cross-lingual
effectiveness compared against prior art on various tasks. Naturally, it raises
a set of questions, most notably how do these multilingual encoders learn
cross-lingual representations. In exploring these questions, this thesis will
analyze the behavior of multilingual models in a variety of settings on high
and low resource languages. We also look at how to inject different
cross-lingual signals into multilingual encoders, and the optimization behavior
of cross-lingual transfer with these models. Together, they provide a better
understanding of multilingual encoders on cross-lingual transfer. Our findings
will lead us to suggested improvements to multilingual encoders and
cross-lingual transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05679">
<div class="article-summary-box-inner">
<span><p>Differentially Private (DP) learning has seen limited success for building
large deep learning models of text, and attempts at straightforwardly applying
Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have
resulted in large performance drops and high computational overhead. We show
that this performance drop can be mitigated with (1) the use of large
pretrained models; (2) hyperparameters that suit DP optimization; and (3)
fine-tuning objectives aligned with the pretraining procedure. With these
factors set right, we obtain private NLP models that outperform
state-of-the-art private training approaches and strong non-private baselines
-- by directly fine-tuning pretrained models with DP optimization on
moderately-sized corpora. To address the computational challenge of running
DP-SGD with large Transformers, we propose a memory saving technique that
allows clipping in DP-SGD to run without instantiating per-example gradients
for any layer in the model. The technique enables privately training
Transformers with almost the same memory cost as non-private training at a
modest run-time overhead. Contrary to conventional wisdom that DP optimization
fails at learning high-dimensional models (due to noise that scales with
dimension) empirical results reveal that private learning with pretrained
models tends to not suffer from dimension-dependent performance degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"That's so cute!": The CARE Dataset for Affective Response Detection. (arXiv:2201.11895v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11895">
<div class="article-summary-box-inner">
<span><p>Social media plays an increasing role in our communication with friends and
family, and our consumption of information and entertainment. Hence, to design
effective ranking functions for posts on social media, it would be useful to
predict the affective response to a post (e.g., whether the user is likely to
be humored, inspired, angered, informed). Similar to work on emotion
recognition (which focuses on the affect of the publisher of the post), the
traditional approach to recognizing affective response would involve an
expensive investment in human annotation of training data.
</p>
<p>We introduce CARE$_{db}$, a dataset of 230k social media posts annotated
according to 7 affective responses using the Common Affective Response
Expression (CARE) method. The CARE method is a means of leveraging the signal
that is present in comments that are posted in response to a post, providing
high-precision evidence about the affective response of the readers to the post
without human annotation. Unlike human annotation, the annotation process we
describe here can be iterated upon to expand the coverage of the method,
particularly for new affective responses. We present experiments that
demonstrate that the CARE annotations compare favorably with crowd-sourced
annotations. Finally, we use CARE$_{db}$ to train competitive BERT-based models
for predicting affective response as well as emotion detection, demonstrating
the utility of the dataset for related tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParaNames: A Massively Multilingual Entity Name Corpus. (arXiv:2202.14035v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.14035">
<div class="article-summary-box-inner">
<span><p>We introduce ParaNames, a multilingual parallel name resource consisting of
118 million names spanning across 400 languages. Names are provided for 13.6
million entities which are mapped to standardized entity types (PER/LOC/ORG).
Using Wikidata as a source, we create the largest resource of this type
to-date. We describe our approach to filtering and standardizing the data to
provide the best quality possible. ParaNames is useful for multilingual
language processing, both in defining tasks for name
translation/transliteration and as supplementary data for tasks such as named
entity recognition and linking. We demonstrate an application of ParaNames by
training a multilingual model for canonical name translation to and from
English. Our resource is released under a Creative Commons license (CC BY 4.0)
at https://github.com/bltlab/paranames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Similarity and Content-based Phonetic Self Attention for Speech Recognition. (arXiv:2203.10252v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10252">
<div class="article-summary-box-inner">
<span><p>Transformer-based speech recognition models have achieved great success due
to the self-attention (SA) mechanism that utilizes every frame in the feature
extraction process. Especially, SA heads in lower layers capture various
phonetic characteristics by the query-key dot product, which is designed to
compute the pairwise relationship between frames. In this paper, we propose a
variant of SA to extract more representative phonetic features. The proposed
phonetic self-attention (phSA) is composed of two different types of phonetic
attention; one is similarity-based and the other is content-based. In short,
similarity-based attention captures the correlation between frames while
content-based attention only considers each frame without being affected by
other frames. We identify which parts of the original dot product equation are
related to two different attention patterns and improve each part with simple
modifications. Our experiments on phoneme classification and speech recognition
show that replacing SA with phSA for lower layers improves the recognition
performance without increasing the latency and the parameter size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment. (arXiv:2203.15937v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15937">
<div class="article-summary-box-inner">
<span><p>Current leading mispronunciation detection and diagnosis (MDD) systems
achieve promising performance via end-to-end phoneme recognition. One challenge
of such end-to-end solutions is the scarcity of human-annotated phonemes on
natural L2 speech. In this work, we leverage unlabeled L2 speech via a
pseudo-labeling (PL) procedure and extend the fine-tuning approach based on
pre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec
2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples
plus the created pseudo-labeled L2 speech samples. Our pseudo labels are
dynamic and are produced by an ensemble of the online model on-the-fly, which
ensures that our model is robust to pseudo label noise. We show that
fine-tuning with pseudo labels achieves a 5.35% phoneme error rate reduction
and 2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning
baseline. The proposed PL method is also shown to outperform conventional
offline PL methods. Compared to the state-of-the-art MDD systems, our MDD
solution produces a more accurate and consistent phonetic error diagnosis. In
addition, we conduct an open test on a separate UTD-4Accents dataset, where our
system recognition outputs show a strong correlation with human perception,
based on accentedness and intelligibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Role of Task Transferability in Large-Scale Multi-Task Learning. (arXiv:2204.11117v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11117">
<div class="article-summary-box-inner">
<span><p>Recent work has found that multi-task training with a large number of diverse
tasks can uniformly improve downstream performance on unseen target tasks. In
contrast, literature on task transferability has established that the choice of
intermediate tasks can heavily affect downstream task performance. In this
work, we aim to disentangle the effect of scale and relatedness of tasks in
multi-task representation learning. We find that, on average, increasing the
scale of multi-task learning, in terms of the number of tasks, indeed results
in better learned representations than smaller multi-task setups. However, if
the target tasks are known ahead of time, then training on a smaller set of
related tasks is competitive to the large-scale multi-task training at a
reduced computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding. (arXiv:2205.06153v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06153">
<div class="article-summary-box-inner">
<span><p>Data augmentation is an effective approach to tackle over-fitting. Many
previous works have proposed different data augmentations strategies for NLP,
such as noise injection, word replacement, back-translation etc. Though
effective, they missed one important characteristic of
language--compositionality, meaning of a complex expression is built from its
sub-parts. Motivated by this, we propose a compositional data augmentation
approach for natural language understanding called TreeMix. Specifically,
TreeMix leverages constituency parsing tree to decompose sentences into
constituent sub-structures and the Mixup data augmentation technique to
recombine them to generate new sentences. Compared with previous approaches,
TreeMix introduces greater diversity to the samples generated and encourages
models to learn compositionality of NLP data. Extensive experiments on text
classification and SCAN demonstrate that TreeMix outperforms current
state-of-the-art data augmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07136">
<div class="article-summary-box-inner">
<span><p>Per-example gradient clipping is a key algorithmic step that enables
practical differential private (DP) training for deep learning models. The
choice of clipping norm $R$, however, is shown to be vital for achieving high
accuracy under DP. We propose an easy-to-use replacement, called AutoClipping,
that eliminates the need to tune $R$ for any DP optimizers, including DP-SGD,
DP-Adam, DP-LAMB and many others. The automatic variants are as private and
computationally efficient as existing DP optimizers, but require no DP-specific
hyperparameters and thus make DP training as amenable as the standard
non-private training. We give a rigorous convergence analysis of automatic
DP-SGD in the non-convex setting, which shows that it enjoys an asymptotic
convergence rate that matches the standard SGD. We also demonstrate on various
language and vision tasks that automatic clipping outperforms or matches the
state-of-the-art, and can be easily employed with minimal changes to existing
codebases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation of Transformer-based Language Models Revisited. (arXiv:2206.14366v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14366">
<div class="article-summary-box-inner">
<span><p>In the past few years, transformer-based pre-trained language models have
achieved astounding success in both industry and academia. However, the large
model size and high run-time latency are serious impediments to applying them
in practice, especially on mobile phones and Internet of Things (IoT) devices.
To compress the model, considerable literature has grown up around the theme of
knowledge distillation (KD) recently. Nevertheless, how KD works in
transformer-based models is still unclear. We tease apart the components of KD
and propose a unified KD framework. Through the framework, systematic and
extensive experiments that spent over 23,000 GPU hours render a comprehensive
analysis from the perspectives of knowledge types, matching strategies,
width-depth trade-off, initialization, model size, etc. Our empirical results
shed light on the distillation in the pre-train language model and with
relative significant improvement over previous state-of-the-arts(SOTA).
Finally, we provide a best-practice guideline for the KD in transformer-based
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-specific Characteristic Assistance for Code-switching Speech Recognition. (arXiv:2206.14580v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14580">
<div class="article-summary-box-inner">
<span><p>Dual-encoder structure successfully utilizes two language-specific encoders
(LSEs) for code-switching speech recognition. Because LSEs are initialized by
two pre-trained language-specific models (LSMs), the dual-encoder structure can
exploit sufficient monolingual data and capture the individual language
attributes. However, most existing methods have no language constraints on LSEs
and underutilize language-specific knowledge of LSMs. In this paper, we propose
a language-specific characteristic assistance (LSCA) method to mitigate the
above problems. Specifically, during training, we introduce two
language-specific losses as language constraints and generate corresponding
language-specific targets for them. During decoding, we take the decoding
abilities of LSMs into account by combining the output probabilities of two
LSMs and the mixture model to obtain the final predictions. Experiments show
that either the training or decoding method of LSCA can improve the model's
performance. Furthermore, the best result can obtain up to 15.4% relative error
reduction on the code-switching test set by combining the training and decoding
methods of LSCA. Moreover, the system can process code-switching speech
recognition tasks well without extra shared parameters or even retraining based
on two pre-trained LSMs by using our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VEM$^2$L: A Plug-and-play Framework for Fusing Text and Structure Knowledge on Sparse Knowledge Graph Completion. (arXiv:2207.01528v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01528">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph Completion has been widely studied recently to complete
missing elements within triples via mainly modeling graph structural features,
but performs sensitive to the sparsity of graph structure. Relevant texts like
entity names and descriptions, acting as another expression form for Knowledge
Graphs (KGs), are expected to solve this challenge. Several methods have been
proposed to utilize both structure and text messages with two encoders, but
only achieved limited improvements due to the failure to balance weights
between them. And reserving both structural and textual encoders during
inference also suffers from heavily overwhelmed parameters. Motivated by
Knowledge Distillation, we view knowledge as mappings from input to output
probabilities and propose a plug-and-play framework VEM2L over sparse KGs to
fuse knowledge extracted from text and structure messages into a unity.
Specifically, we partition knowledge acquired by models into two nonoverlapping
parts: one part is relevant to the fitting capacity upon training triples,
which could be fused by motivating two encoders to learn from each other on
training sets; the other reflects the generalization ability upon unobserved
queries. And correspondingly, we propose a new fusion strategy proved by
Variational EM algorithm to fuse the generalization ability of models, during
which we also apply graph densification operations to further alleviate the
sparse graph problem. By combining these two fusion methods, we propose VEM2L
framework finally. Both detailed theoretical evidence, as well as quantitative
and qualitative experiments, demonstrates the effectiveness and efficiency of
our proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition. (arXiv:2207.04697v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04697">
<div class="article-summary-box-inner">
<span><p>The research and applications of multimodal emotion recognition have become
increasingly popular recently. However, multimodal emotion recognition faces
the challenge of lack of data. To solve this problem, we propose to use
transfer learning which leverages state-of-the-art pre-trained models including
wav2vec 2.0 and BERT for this task. Multi-level fusion approaches including
coattention-based early fusion and late fusion with the models trained on both
embeddings are explored. Also, a multi-granularity framework which extracts not
only frame-level speech embeddings but also segment-level embeddings including
phone, syllable and word-level speech embeddings is proposed to further boost
the performance. By combining our coattention-based early fusion model and late
fusion model with the multi-granularity feature extraction framework, we obtain
result that outperforms best baseline approaches by 1.3% unweighted accuracy
(UA) on the IEMOCAP dataset.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to segment prostate cancer by aggressiveness from scribbles in bi-parametric MRI. (arXiv:2207.05056v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05056">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a deep U-Net based model to tackle the challenging
task of prostate cancer segmentation by aggressiveness in MRI based on weak
scribble annotations. This model extends the size constraint loss proposed by
Kervadec et al. 1 in the context of multiclass detection and segmentation task.
This model is of high clinical interest as it allows training on prostate
biopsy samples and avoids time-consuming full annotation process. Performance
is assessed on a private dataset (219 patients) where the full ground truth is
available as well as on the ProstateX-2 challenge database, where only biopsy
results at different localisations serve as reference. We show that we can
approach the fully-supervised baseline in grading the lesions by using only
6.35% of voxels for training. We report a lesion-wise Cohen's kappa score of
0.29 $\pm$ 0.07 for the weak model versus 0.32 $\pm$ 0.05 for the baseline. We
also report a kappa score (0.276 $\pm$ 0.037) on the ProstateX-2 challenge
dataset with our weak U-Net trained on a combination of ProstateX-2 and our
dataset, which is the highest reported value on this challenge dataset for a
segmentation task to our knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Histopathological Imaging Classification of Breast Tissue for Cancer Diagnosis Support Using Deep Learning Models. (arXiv:2207.05057v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05057">
<div class="article-summary-box-inner">
<span><p>According to some medical imaging techniques, breast histopathology images
called Hematoxylin and Eosin are considered as the gold standard for cancer
diagnoses. Based on the idea of dividing the pathologic image (WSI) into
multiple patches, we used the window [512,512] sliding from left to right and
sliding from top to bottom, each sliding step overlapping by 50% to augmented
data on a dataset of 400 images which were gathered from the ICIAR 2018 Grand
Challenge. Then use the EffficientNet model to classify and identify the
histopathological images of breast cancer into 4 types: Normal, Benign,
Carcinoma, Invasive Carcinoma. The EffficientNet model is a recently developed
model that uniformly scales the width, depth, and resolution of the network
with a set of fixed scaling factors that are well suited for training images
with high resolution. And the results of this model give a rather competitive
classification efficiency, achieving 98% accuracy on the training set and 93%
on the evaluation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Domain Disentanglement for Generalized Multi-source Domain Adaptation. (arXiv:2207.05070v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05070">
<div class="article-summary-box-inner">
<span><p>A typical multi-source domain adaptation (MSDA) approach aims to transfer
knowledge learned from a set of labeled source domains, to an unlabeled target
domain. Nevertheless, prior works strictly assume that each source domain
shares the identical group of classes with the target domain, which could
hardly be guaranteed as the target label space is not observable. In this
paper, we consider a more versatile setting of MSDA, namely Generalized
Multi-source Domain Adaptation, wherein the source domains are partially
overlapped, and the target domain is allowed to contain novel categories that
are not presented in any source domains. This new setting is more elusive than
any existing domain adaptation protocols due to the coexistence of the domain
and category shifts across the source and target domains. To address this
issue, we propose a variational domain disentanglement (VDD) framework, which
decomposes the domain representations and semantic features for each instance
by encouraging dimension-wise independence. To identify the target samples of
unknown classes, we leverage online pseudo labeling, which assigns the
pseudo-labels to unlabeled target data based on the confidence scores.
Quantitative and qualitative experiments conducted on two benchmark datasets
demonstrate the validity of the proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RUSH: Robust Contrastive Learning via Randomized Smoothing. (arXiv:2207.05127v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05127">
<div class="article-summary-box-inner">
<span><p>Recently, adversarial training has been incorporated in self-supervised
contrastive pre-training to augment label efficiency with exciting adversarial
robustness. However, the robustness came at a cost of expensive adversarial
training. In this paper, we show a surprising fact that contrastive
pre-training has an interesting yet implicit connection with robustness, and
such natural robustness in the pre trained representation enables us to design
a powerful robust algorithm against adversarial attacks, RUSH, that combines
the standard contrastive pre-training and randomized smoothing. It boosts both
standard accuracy and robust accuracy, and significantly reduces training costs
as compared with adversarial training. We use extensive empirical studies to
show that the proposed RUSH outperforms robust classifiers from adversarial
training, by a significant margin on common benchmarks (CIFAR-10, CIFAR-100,
and STL-10) under first-order attacks. In particular, under
$\ell_{\infty}$-norm perturbations of size 8/255 PGD attack on CIFAR-10, our
model using ResNet-18 as backbone reached 77.8% robust accuracy and 87.9%
standard accuracy. Our work has an improvement of over 15% in robust accuracy
and a slight improvement in standard accuracy, compared to the
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FreeREA: Training-Free Evolution-based Architecture Search. (arXiv:2207.05135v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05135">
<div class="article-summary-box-inner">
<span><p>In the last decade, most research in Machine Learning contributed to the
improvement of existing models, with the aim of increasing the performance of
neural networks for the solution of a variety of different tasks. However, such
advancements often come at the cost of an increase of model memory and
computational requirements. This represents a significant limitation for the
deployability of research output in realistic settings, where the cost, the
energy consumption, and the complexity of the framework play a crucial role. To
solve this issue, the designer should search for models that maximise the
performance while limiting its footprint. Typical approaches to reach this goal
rely either on manual procedures, which cannot guarantee the optimality of the
final design, or upon Neural Architecture Search algorithms to automatise the
process, at the expenses of extremely high computational time. This paper
provides a solution for the fast identification of a neural network that
maximises the model accuracy while preserving size and computational
constraints typical of tiny devices. Our approach, named FreeREA, is a custom
cell-based evolution NAS algorithm that exploits an optimised combination of
training-free metrics to rank architectures during the search, thus without
need of model training. Our experiments, carried out on the common benchmarks
NAS-Bench-101 and NATS-Bench, demonstrate that i) FreeREA is the first method
able to provide very accurate models in minutes of search time; ii) it
outperforms State of the Art training-based and training-free techniques in all
the datasets and benchmarks considered, and iii) it can easily generalise to
constrained scenarios, representing a competitive solution for fast Neural
Architecture Search in generic constrained applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Effective Multi-Label Recognition Attacks via Knowledge Graph Consistency. (arXiv:2207.05137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05137">
<div class="article-summary-box-inner">
<span><p>Many real-world applications of image recognition require multi-label
learning, whose goal is to find all labels in an image. Thus, robustness of
such systems to adversarial image perturbations is extremely important.
However, despite a large body of recent research on adversarial attacks, the
scope of the existing works is mainly limited to the multi-class setting, where
each image contains a single label. We show that the naive extensions of
multi-class attacks to the multi-label setting lead to violating label
relationships, modeled by a knowledge graph, and can be detected using a
consistency verification scheme. Therefore, we propose a graph-consistent
multi-label attack framework, which searches for small image perturbations that
lead to misclassifying a desired target set while respecting label hierarchies.
By extensive experiments on two datasets and using several multi-label
recognition models, we show that our method generates extremely successful
attacks that, unlike naive multi-label perturbations, can produce model
predictions consistent with the knowledge graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerated Deep Lossless Image Coding with Unified Paralleleized GPU Coding Architecture. (arXiv:2207.05152v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05152">
<div class="article-summary-box-inner">
<span><p>We propose Deep Lossless Image Coding (DLIC), a full resolution learned
lossless image compression algorithm. Our algorithm is based on a neural
network combined with an entropy encoder. The neural network performs a density
estimation on each pixel of the source image. The density estimation is then
used to code the target pixel, beating FLIF in terms of compression rate.
Similar approaches have been attempted. However, long run times make them
unfeasible for real world applications. We introduce a parallelized GPU based
implementation, allowing for encoding and decoding of grayscale, 8-bit images
in less than one second. Because DLIC uses a neural network to estimate the
probabilities used for the entropy coder, DLIC can be trained on domain
specific image data. We demonstrate this capability by adapting and training
DLIC with Magnet Resonance Imaging (MRI) images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoising single images by feature ensemble revisited. (arXiv:2207.05176v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05176">
<div class="article-summary-box-inner">
<span><p>Image denoising is still a challenging issue in many computer vision
sub-domains. Recent studies show that significant improvements are made
possible in a supervised setting. However, few challenges, such as spatial
fidelity and cartoon-like smoothing remain unresolved or decisively overlooked.
Our study proposes a simple yet efficient architecture for the denoising
problem that addresses the aforementioned issues. The proposed architecture
revisits the concept of modular concatenation instead of long and deeper
cascaded connections, to recover a cleaner approximation of the given image. We
find that different modules can capture versatile representations, and
concatenated representation creates a richer subspace for low-level image
restoration. The proposed architecture's number of parameters remains smaller
than the number for most of the previous networks and still achieves
significant improvements over the current state-of-the-art networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Activities of People Worldwide. (arXiv:2207.05182v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05182">
<div class="article-summary-box-inner">
<span><p>Every day, humans perform many closely related activities that involve subtle
discriminative motions, such as putting on a shirt vs. putting on a jacket, or
shaking hands vs. giving a high five. Activity recognition by ethical visual AI
could provide insights into our patterns of daily life, however existing
activity recognition datasets do not capture the massive diversity of these
human activities around the world. To address this limitation, we introduce
Collector, a free mobile app to record video while simultaneously annotating
objects and activities of consented subjects. This new data collection platform
was used to curate the Consented Activities of People (CAP) dataset, the first
large-scale, fine-grained activity dataset of people worldwide. The CAP dataset
contains 1.45M video clips of 512 fine grained activity labels of daily life,
collected by 780 subjects in 33 countries. We provide activity classification
and activity detection benchmarks for this dataset, and analyze baseline
results to gain insight into how people around with world perform common
activities. The dataset, benchmarks, evaluation tools, public leaderboards and
mobile apps are available for use at visym.github.io/cap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-level instance-group discrimination with pretext-invariant learning for colitis scoring. (arXiv:2207.05192v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05192">
<div class="article-summary-box-inner">
<span><p>Inflammatory bowel disease (IBD), in particular ulcerative colitis (UC), is
graded by endoscopists and this assessment is the basis for risk stratification
and therapy monitoring. Presently, endoscopic characterisation is largely
operator dependant leading to sometimes undesirable clinical outcomes for
patients with IBD. We focus on the Mayo Endoscopic Scoring (MES) system which
is widely used but requires the reliable identification of subtle changes in
mucosal inflammation. Most existing deep learning classification methods cannot
detect these fine-grained changes which make UC grading such a challenging
task. In this work, we introduce a novel patch-level instance-group
discrimination with pretext-invariant representation learning (PLD-PIRL) for
self-supervised learning (SSL). Our experiments demonstrate both improved
accuracy and robustness compared to the baseline supervised network and several
state-of-the-art SSL methods. Compared to the baseline (ResNet50) supervised
classification our proposed PLD-PIRL obtained an improvement of 4.75% on
hold-out test data and 6.64% on unseen center test data for top-1 accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Uncertainty Benefits Multi-Agent Multi-Modal Trajectory Forecasting. (arXiv:2207.05195v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05195">
<div class="article-summary-box-inner">
<span><p>In multi-modal multi-agent trajectory forecasting, two major challenges have
not been fully tackled: 1) how to measure the uncertainty brought by the
interaction module that causes correlations among the predicted trajectories of
multiple agents; 2) how to rank the multiple predictions and select the optimal
predicted trajectory. In order to handle these challenges, this work first
proposes a novel concept, collaborative uncertainty (CU), which models the
uncertainty resulting from interaction modules. Then we build a general
CU-aware regression framework with an original permutation-equivariant
uncertainty estimator to do both tasks of regression and uncertainty
estimation. Further, we apply the proposed framework to current SOTA
multi-agent multi-modal forecasting systems as a plugin module, which enables
the SOTA systems to 1) estimate the uncertainty in the multi-agent multi-modal
trajectory forecasting task; 2) rank the multiple predictions and select the
optimal one based on the estimated uncertainty. We conduct extensive
experiments on a synthetic dataset and two public large-scale multi-agent
trajectory forecasting benchmarks. Experiments show that: 1) on the synthetic
dataset, the CU-aware regression framework allows the model to appropriately
approximate the ground-truth Laplace distribution; 2) on the multi-agent
trajectory forecasting benchmarks, the CU-aware regression framework steadily
helps SOTA systems improve their performances. Specially, the proposed
framework helps VectorNet improve by 262 cm regarding the Final Displacement
Error of the chosen optimal prediction on the nuScenes dataset; 3) for
multi-agent multi-modal trajectory forecasting systems, prediction uncertainty
is positively correlated with future stochasticity; and 4) the estimated CU
values are highly related to the interactive information among agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time And Robust 3D Object Detection with Roadside LiDARs. (arXiv:2207.05200v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05200">
<div class="article-summary-box-inner">
<span><p>This work aims to address the challenges in autonomous driving by focusing on
the 3D perception of the environment using roadside LiDARs. We design a 3D
object detection model that can detect traffic participants in roadside LiDARs
in real-time. Our model uses an existing 3D detector as a baseline and improves
its accuracy. To prove the effectiveness of our proposed modules, we train and
evaluate the model on three different vehicle and infrastructure datasets. To
show the domain adaptation ability of our detector, we train it on an
infrastructure dataset from China and perform transfer learning on a different
dataset recorded in Germany. We do several sets of experiments and ablation
studies for each module in the detector that show that our model outperforms
the baseline by a significant margin, while the inference speed is at 45 Hz (22
ms). We make a significant contribution with our LiDAR-based 3D detector that
can be used for smart city applications to provide connected and automated
vehicles with a far-reaching view. Vehicles that are connected to the roadside
sensors can get information about other vehicles around the corner to improve
their path and maneuver planning and to increase road traffic safety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Novel Object Detection with Weakly Supervised Detection Transformers. (arXiv:2207.05205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05205">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object detection (WSOD) enables object detectors to be
trained using image-level class labels. However, the practical application of
current WSOD models is limited, as they operate at small scales and require
extensive training and refinement. We propose the Weakly Supervised Detection
Transformer, which enables efficient knowledge transfer from a large-scale
pretraining dataset to WSOD finetuning on hundreds of novel objects. We
leverage pretrained knowledge to improve the multiple instance learning
framework used in WSOD, and experiments show our approach outperforms the
state-of-the-art on datasets with twice the novel classes than previously
shown.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Susceptibility of Continual Learning Against Adversarial Attacks. (arXiv:2207.05225v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05225">
<div class="article-summary-box-inner">
<span><p>The recent advances in continual (incremental or lifelong) learning have
concentrated on the prevention of forgetting that can lead to catastrophic
consequences, but there are two outstanding challenges that must be addressed.
The first is the evaluation of the robustness of the proposed methods. The
second is ensuring the security of learned tasks remains largely unexplored.
This paper presents a comprehensive study of the susceptibility of the
continually learned tasks (including both current and previously learned tasks)
that are vulnerable to forgetting. Such vulnerability of tasks against
adversarial attacks raises profound issues in data integrity and privacy. We
consider the task incremental learning (Task-IL) scenario and explore three
regularization-based experiments, three replay-based experiments, and one
hybrid technique based on the reply and exemplar approach. We examine the
robustness of these methods. In particular, we consider cases where we
demonstrate that any class belonging to the current or previously learned tasks
is prone to misclassification. Our observations highlight the potential
limitations of existing Task-IL approaches. Our empirical study recommends that
the research community consider the robustness of the proposed continual
learning approaches and invest extensive efforts in mitigating catastrophic
forgetting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regression Metric Loss: Learning a Semantic Representation Space for Medical Images. (arXiv:2207.05231v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05231">
<div class="article-summary-box-inner">
<span><p>Regression plays an essential role in many medical imaging applications for
estimating various clinical risk or measurement scores. While training
strategies and loss functions have been studied for the deep neural networks in
medical image classification tasks, options for regression tasks are very
limited. One of the key challenges is that the high-dimensional feature
representation learned by existing popular loss functions like Mean Squared
Error or L1 loss is hard to interpret. In this paper, we propose a novel
Regression Metric Loss (RM-Loss), which endows the representation space with
the semantic meaning of the label space by finding a representation manifold
that is isometric to the label space. Experiments on two regression tasks, i.e.
coronary artery calcium score estimation and bone age assessment, show that
RM-Loss is superior to the existing popular regression losses on both
performance and interpretability. Code is available at
https://github.com/DIAL-RPI/Regression-Metric-Loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Human Vision Inspired Action Recognition using Adaptive Spatiotemporal Sampling. (arXiv:2207.05249v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05249">
<div class="article-summary-box-inner">
<span><p>Adaptive sampling that exploits the spatiotemporal redundancy in videos is
critical for always-on action recognition on wearable devices with limited
computing and battery resources. The commonly used fixed sampling strategy is
not context-aware and may under-sample the visual content, and thus adversely
impacts both computation efficiency and accuracy. Inspired by the concepts of
foveal vision and pre-attentive processing from the human visual perception
mechanism, we introduce a novel adaptive spatiotemporal sampling scheme for
efficient action recognition. Our system pre-scans the global scene context at
low-resolution and decides to skip or request high-resolution features at
salient regions for further processing. We validate the system on EPIC-KITCHENS
and UCF-101 datasets for action recognition, and show that our proposed
approach can greatly speed up inference with a tolerable loss of accuracy
compared with those from state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Proposals for Efficient Object Detection. (arXiv:2207.05252v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05252">
<div class="article-summary-box-inner">
<span><p>Object detection is a basic computer vision task to loccalize and categorize
objects in a given image. Most state-of-the-art detection methods utilize a
fixed number of proposals as an intermediate representation of object
candidates, which is unable to adapt to different computational constraints
during inference. In this paper, we propose a simple yet effective method which
is adaptive to different computational resources by generating dynamic
proposals for object detection. We first design a module to make a single
query-based model to be able to inference with different numbers of proposals.
Further, we extend it to a dynamic model to choose the number of proposals
according to the input image, greatly reducing computational costs. Our method
achieves significant speed-up across a wide range of detection models including
two-stage and query-based models while obtaining similar or even better
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hunting Group Clues with Transformers for Social Group Activity Recognition. (arXiv:2207.05254v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05254">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel framework for social group activity recognition.
As an expanded task of group activity recognition, social group activity
recognition requires recognizing multiple sub-group activities and identifying
group members. Most existing methods tackle both tasks by refining region
features and then summarizing them into activity features. Such heuristic
feature design renders the effectiveness of features susceptible to incomplete
person localization and disregards the importance of scene contexts.
Furthermore, region features are sub-optimal to identify group members because
the features may be dominated by those of people in the regions and have
different semantics. To overcome these drawbacks, we propose to leverage
attention modules in transformers to generate effective social group features.
Our method is designed in such a way that the attention modules identify and
then aggregate features relevant to social group activities, generating an
effective feature for each social group. Group member information is embedded
into the features and thus accessed by feed-forward networks. The outputs of
feed-forward networks represent groups so concisely that group members can be
identified with simple Hungarian matching between groups and individuals.
Experimental results show that our method outperforms state-of-the-art methods
on the Volleyball and Collective Activity datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Normalized Feature Distillation for Semantic Segmentation. (arXiv:2207.05256v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05256">
<div class="article-summary-box-inner">
<span><p>As a promising approach in model compression, knowledge distillation improves
the performance of a compact model by transferring the knowledge from a
cumbersome one. The kind of knowledge used to guide the training of the student
is important. Previous distillation methods in semantic segmentation strive to
extract various forms of knowledge from the features, which involve elaborate
manual design relying on prior information and have limited performance gains.
In this paper, we propose a simple yet effective feature distillation method
called normalized feature distillation (NFD), aiming to enable effective
distillation with the original features without the need to manually design new
forms of knowledge. The key idea is to prevent the student from focusing on
imitating the magnitude of the teacher's feature response by normalization. Our
method achieves state-of-the-art distillation results for semantic segmentation
on Cityscapes, VOC 2012, and ADE20K datasets. Code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Certifiable Estimation with Preconditioned Eigensolvers. (arXiv:2207.05257v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05257">
<div class="article-summary-box-inner">
<span><p>Convex (specifically semidefinite) relaxation provides a powerful approach to
constructing robust machine perception systems, enabling the recovery of
certifiably globally optimal solutions of challenging estimation problems in
many practical settings. However, solving the large-scale semidefinite
relaxations underpinning this approach remains a formidable computational
challenge. A dominant cost in many state-of-the-art (Burer-Monteiro
factorization-based) certifiable estimation methods is solution verification
(testing the global optimality of a given candidate solution), which entails
computing a minimum eigenpair of a certain symmetric certificate matrix. In
this paper, we show how to significantly accelerate this verification step, and
thereby the overall speed of certifiable estimation methods. First, we show
that the certificate matrices arising in the Burer-Monteiro approach
generically possess spectra that make the verification problem expensive to
solve using standard iterative eigenvalue methods. We then show how to address
this challenge using preconditioned eigensolvers; specifically, we design a
specialized solution verification algorithm based upon the locally optimal
block preconditioned conjugate gradient (LOBPCG) method together with a simple
yet highly effective algebraic preconditioner. Experimental evaluation on a
variety of simulated and real-world examples shows that our proposed
verification scheme is very effective in practice, accelerating solution
verification by up to 280x, and the overall Burer-Monteiro method by up to 16x,
versus the standard Lanczos method when applied to relaxations derived from
large-scale SLAM benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Architecture Knowledge Distillation. (arXiv:2207.05273v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05273">
<div class="article-summary-box-inner">
<span><p>Transformer attracts much attention because of its ability to learn global
relations and superior performance. In order to achieve higher performance, it
is natural to distill complementary knowledge from Transformer to convolutional
neural network (CNN). However, most existing knowledge distillation methods
only consider homologous-architecture distillation, such as distilling
knowledge from CNN to CNN. They may not be suitable when applying to
cross-architecture scenarios, such as from Transformer to CNN. To deal with
this problem, a novel cross-architecture knowledge distillation method is
proposed. Specifically, instead of directly mimicking output/intermediate
features of the teacher, a partially cross attention projector and a group-wise
linear projector are introduced to align the student features with the
teacher's in two projected feature spaces. And a multi-view robust training
scheme is further presented to improve the robustness and stability of the
framework. Extensive experiments show that the proposed method outperforms 14
state-of-the-arts on both small-scale and large-scale datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Photonic Reconfigurable Accelerators for Efficient Inference of CNNs with Mixed-Sized Tensors. (arXiv:2207.05278v1 [cs.AR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05278">
<div class="article-summary-box-inner">
<span><p>Photonic Microring Resonator (MRR) based hardware accelerators have been
shown to provide disruptive speedup and energy-efficiency improvements for
processing deep Convolutional Neural Networks (CNNs). However, previous
MRR-based CNN accelerators fail to provide efficient adaptability for CNNs with
mixed-sized tensors. One example of such CNNs is depthwise separable CNNs.
Performing inferences of CNNs with mixed-sized tensors on such inflexible
accelerators often leads to low hardware utilization, which diminishes the
achievable performance and energy efficiency from the accelerators. In this
paper, we present a novel way of introducing reconfigurability in the MRR-based
CNN accelerators, to enable dynamic maximization of the size compatibility
between the accelerator hardware components and the CNN tensors that are
processed using the hardware components. We classify the state-of-the-art
MRR-based CNN accelerators from prior works into two categories, based on the
layout and relative placements of the utilized hardware components in the
accelerators. We then use our method to introduce reconfigurability in
accelerators from these two classes, to consequently improve their parallelism,
the flexibility of efficiently mapping tensors of different sizes, speed, and
overall energy efficiency. We evaluate our reconfigurable accelerators against
three prior works for the area proportionate outlook (equal hardware area for
all accelerators). Our evaluation for the inference of four modern CNNs
indicates that our designed reconfigurable CNN accelerators provide
improvements of up to 1.8x in Frames-Per-Second (FPS) and up to 1.5x in FPS/W,
compared to an MRR-based accelerator from prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PseudoClick: Interactive Image Segmentation with Click Imitation. (arXiv:2207.05282v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05282">
<div class="article-summary-box-inner">
<span><p>The goal of click-based interactive image segmentation is to obtain precise
object segmentation masks with limited user interaction, i.e., by a minimal
number of user clicks. Existing methods require users to provide all the
clicks: by first inspecting the segmentation mask and then providing points on
mislabeled regions, iteratively. We ask the question: can our model directly
predict where to click, so as to further reduce the user interaction cost? To
this end, we propose {\PseudoClick}, a generic framework that enables existing
segmentation networks to propose candidate next clicks. These automatically
generated clicks, termed pseudo clicks in this work, serve as an imitation of
human clicks to refine the segmentation mask.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Inlier and Outlier Specification for Improved Out-of-Distribution Detection. (arXiv:2207.05286v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05286">
<div class="article-summary-box-inner">
<span><p>Accurately detecting out-of-distribution (OOD) data with varying levels of
semantic and covariate shifts with respect to the in-distribution (ID) data is
critical for deployment of safe and reliable models. This is particularly the
case when dealing with highly consequential applications (e.g. medical imaging,
self-driving cars, etc). The goal is to design a detector that can accept
meaningful variations of the ID data, while also rejecting examples from OOD
regimes. In practice, this dual objective can be realized by enforcing
consistency using an appropriate scoring function (e.g., energy) and
calibrating the detector to reject a curated set of OOD data (referred to as
outlier exposure or shortly OE). While OE methods are widely adopted,
assembling representative OOD datasets is both costly and challenging due to
the unpredictability of real-world scenarios, hence the recent trend of
designing OE-free detectors. In this paper, we make a surprising finding that
controlled generalization to ID variations and exposure to diverse (synthetic)
outlier examples are essential to simultaneously improving semantic and
modality shift detection. In contrast to existing methods, our approach samples
inliers in the latent space, and constructs outlier examples via negative data
augmentation. Through a rigorous empirical study on medical imaging benchmarks
(MedMNIST, ISIC2019 and NCT), we demonstrate significant performance gains
($15\% - 35\%$ in AUROC) over existing OE-free, OOD detection approaches under
both semantic and modality shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaAge: Meta-Learning Personalized Age Estimators. (arXiv:2207.05288v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05288">
<div class="article-summary-box-inner">
<span><p>Different people age in different ways. Learning a personalized age estimator
for each person is a promising direction for age estimation given that it
better models the personalization of aging processes. However, most existing
personalized methods suffer from the lack of large-scale datasets due to the
high-level requirements: identity labels and enough samples for each person to
form a long-term aging pattern. In this paper, we aim to learn personalized age
estimators without the above requirements and propose a meta-learning method
named MetaAge for age estimation. Unlike most existing personalized methods
that learn the parameters of a personalized estimator for each person in the
training set, our method learns the mapping from identity information to age
estimator parameters. Specifically, we introduce a personalized estimator
meta-learner, which takes identity features as the input and outputs the
parameters of customized estimators. In this way, our method learns the meta
knowledge without the above requirements and seamlessly transfers the learned
meta knowledge to the test set, which enables us to leverage the existing
large-scale age datasets without any additional annotations. Extensive
experimental results on three benchmark datasets including MORPH II, ChaLearn
LAP 2015 and ChaLearn LAP 2016 databases demonstrate that our MetaAge
significantly boosts the performance of existing personalized methods and
outperforms the state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trusted Multi-Scale Classification Framework for Whole Slide Image. (arXiv:2207.05290v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05290">
<div class="article-summary-box-inner">
<span><p>Despite remarkable efforts been made, the classification of gigapixels
whole-slide image (WSI) is severely restrained from either the constrained
computing resources for the whole slides, or limited utilizing of the knowledge
from different scales. Moreover, most of the previous attempts lacked of the
ability of uncertainty estimation. Generally, the pathologists often jointly
analyze WSI from the different magnifications. If the pathologists are
uncertain by using single magnification, then they will change the
magnification repeatedly to discover various features of the tissues. Motivated
by the diagnose process of the pathologists, in this paper, we propose a
trusted multi-scale classification framework for the WSI. Leveraging the Vision
Transformer as the backbone for multi branches, our framework can jointly
classification modeling, estimating the uncertainty of each magnification of a
microscope and integrate the evidence from different magnification. Moreover,
to exploit discriminative patches from WSIs and reduce the requirement for
computation resources, we propose a novel patch selection schema using
attention rollout and non-maximum suppression. To empirically investigate the
effectiveness of our approach, empirical experiments are conducted on our WSI
classification tasks, using two benchmark databases. The obtained results
suggest that the trusted framework can significantly improve the WSI
classification performance compared with the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Hard-Positive Query Mining for DETR-based Human-Object Interaction Detection. (arXiv:2207.05293v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05293">
<div class="article-summary-box-inner">
<span><p>Human-Object Interaction (HOI) detection is a core task for high-level image
understanding. Recently, Detection Transformer (DETR)-based HOI detectors have
become popular due to their superior performance and efficient structure.
However, these approaches typically adopt fixed HOI queries for all testing
images, which is vulnerable to the location change of objects in one specific
image. Accordingly, in this paper, we propose to enhance DETR's robustness by
mining hard-positive queries, which are forced to make correct predictions
using partial visual cues. First, we explicitly compose hard-positive queries
according to the ground-truth (GT) position of labeled human-object pairs for
each training image. Specifically, we shift the GT bounding boxes of each
labeled human-object pair so that the shifted boxes cover only a certain
portion of the GT ones. We encode the coordinates of the shifted boxes for each
labeled human-object pair into an HOI query. Second, we implicitly construct
another set of hard-positive queries by masking the top scores in
cross-attention maps of the decoder layers. The masked attention maps then only
cover partial important cues for HOI predictions. Finally, an alternate
strategy is proposed that efficiently combines both types of hard queries. In
each iteration, both DETR's learnable queries and one selected type of
hard-positive queries are adopted for loss computation. Experimental results
show that our proposed approach can be widely applied to existing DETR-based
HOI detectors. Moreover, we consistently achieve state-of-the-art performance
on three benchmarks: HICO-DET, V-COCO, and HOI-A. Code is available at
https://github.com/MuchHair/HQM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SD-GAN: Semantic Decomposition for Face Image Synthesis with Discrete Attribute. (arXiv:2207.05300v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05300">
<div class="article-summary-box-inner">
<span><p>Manipulating latent code in generative adversarial networks (GANs) for facial
image synthesis mainly focuses on continuous attribute synthesis (e.g., age,
pose and emotion), while discrete attribute synthesis (like face mask and
eyeglasses) receives less attention. Directly applying existing works to facial
discrete attributes may cause inaccurate results. In this work, we propose an
innovative framework to tackle challenging facial discrete attribute synthesis
via semantic decomposing, dubbed SD-GAN. To be concrete, we explicitly
decompose the discrete attribute representation into two components, i.e. the
semantic prior basis and offset latent representation. The semantic prior basis
shows an initializing direction for manipulating face representation in the
latent space. The offset latent presentation obtained by 3D-aware semantic
fusion network is proposed to adjust prior basis. In addition, the fusion
network integrates 3D embedding for better identity preservation and discrete
attribute synthesis. The combination of prior basis and offset latent
representation enable our method to synthesize photo-realistic face images with
discrete attributes. Notably, we construct a large and valuable dataset MEGN
(Face Mask and Eyeglasses images crawled from Google and Naver) for completing
the lack of discrete attributes in the existing dataset. Extensive qualitative
and quantitative experiments demonstrate the state-of-the-art performance of
our method. Our code is available at: https://github.com/MontaEllis/SD-GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Deep Supervision. (arXiv:2207.05306v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05306">
<div class="article-summary-box-inner">
<span><p>The success of deep learning is usually accompanied by the growth in neural
network depth. However, the traditional training method only supervises the
neural network at its last layer and propagates the supervision layer-by-layer,
which leads to hardship in optimizing the intermediate layers. Recently, deep
supervision has been proposed to add auxiliary classifiers to the intermediate
layers of deep neural networks. By optimizing these auxiliary classifiers with
the supervised task loss, the supervision can be applied to the shallow layers
directly. However, deep supervision conflicts with the well-known observation
that the shallow layers learn low-level features instead of task-biased
high-level semantic features. To address this issue, this paper proposes a
novel training framework named Contrastive Deep Supervision, which supervises
the intermediate layers with augmentation-based contrastive learning.
Experimental results on nine popular datasets with eleven models demonstrate
its effects on general image classification, fine-grained image classification
and object detection in supervised learning, semi-supervised learning and
knowledge distillation. Codes have been released in Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Outpainting by Queries. (arXiv:2207.05312v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05312">
<div class="article-summary-box-inner">
<span><p>Image outpainting, which is well studied with Convolution Neural Network
(CNN) based framework, has recently drawn more attention in computer vision.
However, CNNs rely on inherent inductive biases to achieve effective sample
learning, which may degrade the performance ceiling. In this paper, motivated
by the flexible self-attention mechanism with minimal inductive biases in
transformer architecture, we reframe the generalised image outpainting problem
as a patch-wise sequence-to-sequence autoregression problem, enabling
query-based image outpainting. Specifically, we propose a novel hybrid
vision-transformer-based encoder-decoder framework, named \textbf{Query}
\textbf{O}utpainting \textbf{TR}ansformer (\textbf{QueryOTR}), for
extrapolating visual context all-side around a given image. Patch-wise mode's
global modeling capacity allows us to extrapolate images from the attention
mechanism's query standpoint. A novel Query Expansion Module (QEM) is designed
to integrate information from the predicted queries based on the encoder's
output, hence accelerating the convergence of the pure transformer even with a
relatively small dataset. To further enhance connectivity between each patch,
the proposed Patch Smoothing Module (PSM) re-allocates and averages the
overlapped regions, thus providing seamless predicted images. We experimentally
show that QueryOTR could generate visually appealing results smoothly and
realistically against the state-of-the-art image outpainting approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CANF-VC: Conditional Augmented Normalizing Flows for Video Compression. (arXiv:2207.05315v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05315">
<div class="article-summary-box-inner">
<span><p>This paper presents an end-to-end learning-based video compression system,
termed CANF-VC, based on conditional augmented normalizing flows (ANF). Most
learned video compression systems adopt the same hybrid-based coding
architecture as the traditional codecs. Recent research on conditional coding
has shown the sub-optimality of the hybrid-based coding and opens up
opportunities for deep generative models to take a key role in creating new
coding frameworks. CANF-VC represents a new attempt that leverages the
conditional ANF to learn a video generative model for conditional inter-frame
coding. We choose ANF because it is a special type of generative model, which
includes variational autoencoder as a special case and is able to achieve
better expressiveness. CANF-VC also extends the idea of conditional coding to
motion coding, forming a purely conditional coding framework. Extensive
experimental results on commonly used datasets confirm the superiority of
CANF-VC to the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twin identification over viewpoint change: A deep convolutional neural network surpasses humans. (arXiv:2207.05316v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05316">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks (DCNNs) have achieved human-level accuracy
in face identification (Phillips et al., 2018), though it is unclear how
accurately they discriminate highly-similar faces. Here, humans and a DCNN
performed a challenging face-identity matching task that included identical
twins. Participants (N=87) viewed pairs of face images of three types:
same-identity, general imposter pairs (different identities from similar
demographic groups), and twin imposter pairs (identical twin siblings). The
task was to determine whether the pairs showed the same person or different
people. Identity comparisons were tested in three viewpoint-disparity
conditions: frontal to frontal, frontal to 45-degree profile, and frontal to
90-degree profile. Accuracy for discriminating matched-identity pairs from
twin-imposters and general imposters was assessed in each viewpoint-disparity
condition. Humans were more accurate for general-imposter pairs than
twin-imposter pairs, and accuracy declined with increased viewpoint disparity
between the images in a pair. A DCNN trained for face identification (Ranjan et
al., 2018) was tested on the same image pairs presented to humans. Machine
performance mirrored the pattern of human accuracy, but with performance at or
above all humans in all but one condition. Human and machine similarity scores
were compared across all image-pair types. This item-level analysis showed that
human and machine similarity ratings correlated significantly in six of nine
image-pair types [range r=0.38 to r=0.63], suggesting general accord between
the perception of face similarity by humans and the DCNN. These findings also
contribute to our understanding of DCNN performance for discriminating
high-resemblance faces, demonstrate that the DCNN performs at a level at or
above humans, and suggest a degree of parity between the features used by
humans and the DCNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPO: Change Robust Panorama to Point Cloud Localization. (arXiv:2207.05317v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05317">
<div class="article-summary-box-inner">
<span><p>We present CPO, a fast and robust algorithm that localizes a 2D panorama with
respect to a 3D point cloud of a scene possibly containing changes. To robustly
handle scene changes, our approach deviates from conventional feature point
matching, and focuses on the spatial context provided from panorama images.
Specifically, we propose efficient color histogram generation and subsequent
robust localization using score maps. By utilizing the unique equivariance of
spherical projections, we propose very fast color histogram generation for a
large number of camera poses without explicitly rendering images for all
candidate poses. We accumulate the regional consistency of the panorama and
point cloud as 2D/3D score maps, and use them to weigh the input color values
to further increase robustness. The weighted color distribution quickly finds
good initial poses and achieves stable convergence for gradient-based
optimization. CPO is lightweight and achieves effective localization in all
tested scenarios, showing stable performance despite scene changes, repetitive
structures, or featureless regions, which are typical challenges for visual
localization with perspective cameras.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Certified Adversarial Robustness via Anisotropic Randomized Smoothing. (arXiv:2207.05327v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05327">
<div class="article-summary-box-inner">
<span><p>Randomized smoothing has achieved great success for certified robustness
against adversarial perturbations. Given any arbitrary classifier, randomized
smoothing can guarantee the classifier's prediction over the perturbed input
with provable robustness bound by injecting noise into the classifier. However,
all of the existing methods rely on fixed i.i.d. probability distribution to
generate noise for all dimensions of the data (e.g., all the pixels in an
image), which ignores the heterogeneity of inputs and data dimensions. Thus,
existing randomized smoothing methods cannot provide optimal protection for all
the inputs. To address this limitation, we propose the first anisotropic
randomized smoothing method which ensures provable robustness guarantee based
on pixel-wise noise distributions. Also, we design a novel CNN-based noise
generator to efficiently fine-tune the pixel-wise noise distributions for all
the pixels in each input. Experimental results demonstrate that our method
significantly outperforms the state-of-the-art randomized smoothing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robotic Detection of a Human-Comprehensible Gestural Language for Underwater Multi-Human-Robot Collaboration. (arXiv:2207.05331v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05331">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a motion-based robotic communication framework that
enables non-verbal communication among autonomous underwater vehicles (AUVs)
and human divers. We design a gestural language for AUV-to-AUV communication
which can be easily understood by divers observing the conversation unlike
typical radio frequency, light, or audio based AUV communication. To allow AUVs
to visually understand a gesture from another AUV, we propose a deep network
(RRCommNet) which exploits a self-attention mechanism to learn to recognize
each message by extracting maximally discriminative spatio-temporal features.
We train this network on diverse simulated and real-world data. Our
experimental evaluations, both in simulation and in closed-water robot trials,
demonstrate that the proposed RRCommNet architecture is able to decipher
gesture-based messages with an average accuracy of 88-94% on simulated data,
73-83% on real data (depending on the version of the model used). Further, by
performing a message transcription study with human participants, we also show
that the proposed language can be understood by humans, with an overall
transcription accuracy of 88%. Finally, we discuss the inference runtime of
RRCommNet on embedded GPU hardware, for real-time use on board AUVs in the
field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IDEA: Increasing Text Diversity via Online Multi-Label Recognition for Vision-Language Pre-training. (arXiv:2207.05333v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05333">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pre-training (VLP) with large-scale image-text pairs has
demonstrated superior performance in various fields. However, the image-text
pairs co-occurrent on the Internet typically lack explicit alignment
information, which is suboptimal for VLP. Existing methods proposed to adopt an
off-the-shelf object detector to utilize additional image tag information.
However, the object detector is time-consuming and can only identify the
pre-defined object categories, limiting the model capacity. Inspired by the
observation that the texts incorporate incomplete fine-grained image
information, we introduce IDEA, which stands for increasing text diversity via
online multi-label recognition for VLP. IDEA shows that multi-label learning
with image tags extracted from the texts can be jointly optimized during VLP.
Moreover, IDEA can identify valuable image tags online to provide more explicit
textual supervision. Comprehensive experiments demonstrate that IDEA can
significantly boost the performance on multiple downstream datasets with a
small extra computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cycle Self-Training for Semi-Supervised Object Detection with Distribution Consistency Reweighting. (arXiv:2207.05334v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05334">
<div class="article-summary-box-inner">
<span><p>Recently, many semi-supervised object detection (SSOD) methods adopt
teacher-student framework and have achieved state-of-the-art results. However,
the teacher network is tightly coupled with the student network since the
teacher is an exponential moving average (EMA) of the student, which causes a
performance bottleneck. To address the coupling problem, we propose a Cycle
Self-Training (CST) framework for SSOD, which consists of two teachers T1 and
T2, two students S1 and S2. Based on these networks, a cycle self-training
mechanism is built, i.e.,
S1${\rightarrow}$T1${\rightarrow}$S2${\rightarrow}$T2${\rightarrow}$S1. For
S${\rightarrow}$T, we also utilize the EMA weights of the students to update
the teachers. For T${\rightarrow}$S, instead of providing supervision for its
own student S1(S2) directly, the teacher T1(T2) generates pseudo-labels for the
student S2(S1), which looses the coupling effect. Moreover, owing to the
property of EMA, the teacher is most likely to accumulate the biases from the
student and make the mistakes irreversible. To mitigate the problem, we also
propose a distribution consistency reweighting strategy, where pseudo-labels
are reweighted based on distribution consistency across the teachers T1 and T2.
With the strategy, the two students S2 and S1 can be trained robustly with
noisy pseudo labels to avoid confirmation biases. Extensive experiments prove
the superiority of CST by consistently improving the AP over the baseline and
outperforming state-of-the-art methods by 2.1% absolute AP improvements with
scarce labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Contrastive Learning for Spatio-temporal Representation. (arXiv:2207.05340v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05340">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has shown promising potential in self-supervised
spatio-temporal representation learning. Most works naively sample different
clips to construct positive and negative pairs. However, we observe that this
formulation inclines the model towards the background scene bias. The
underlying reasons are twofold. First, the scene difference is usually more
noticeable and easier to discriminate than the motion difference. Second, the
clips sampled from the same video often share similar backgrounds but have
distinct motions. Simply regarding them as positive pairs will draw the model
to the static background rather than the motion pattern. To tackle this
challenge, this paper presents a novel dual contrastive formulation.
Concretely, we decouple the input RGB video sequence into two complementary
modes, static scene and dynamic motion. Then, the original RGB features are
pulled closer to the static features and the aligned dynamic features,
respectively. In this way, the static scene and the dynamic motion are
simultaneously encoded into the compact RGB representation. We further conduct
the feature space decoupling via activation maps to distill static- and
dynamic-related features. We term our method as \textbf{D}ual
\textbf{C}ontrastive \textbf{L}earning for spatio-temporal
\textbf{R}epresentation (DCLR). Extensive experiments demonstrate that DCLR
learns effective spatio-temporal representations and obtains state-of-the-art
or comparable performance on UCF-101, HMDB-51, and Diving-48 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Graph Transformer for Video Question Answering. (arXiv:2207.05342v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05342">
<div class="article-summary-box-inner">
<span><p>This paper proposes a Video Graph Transformer (VGT) model for Video Quetion
Answering (VideoQA). VGT's uniqueness are two-fold: 1) it designs a dynamic
graph transformer module which encodes video by explicitly capturing the visual
objects, their relations, and dynamics for complex spatio-temporal reasoning;
and 2) it exploits disentangled video and text Transformers for relevance
comparison between the video and text to perform QA, instead of entangled
cross-modal Transformer for answer classification. Vision-text communication is
done by additional cross-modal interaction modules. With more reasonable video
encoding and QA solution, we show that VGT can achieve much better performances
on VideoQA tasks that challenge dynamic relation reasoning than prior arts in
the pretraining-free scenario. Its performances even surpass those models that
are pretrained with millions of external data. We further show that VGT can
also benefit a lot from self-supervised cross-modal pretraining, yet with
orders of magnitude smaller data. These results clearly demonstrate the
effectiveness and superiority of VGT, and reveal its potential for more
data-efficient pretraining. With comprehensive analyses and some heuristic
observations, we hope that VGT can promote VQA research beyond coarse
recognition/description towards fine-grained relation reasoning in realistic
videos. Our code is available at https://github.com/sail-sg/VGT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HEAD: HEtero-Assists Distillation for Heterogeneous Object Detectors. (arXiv:2207.05345v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05345">
<div class="article-summary-box-inner">
<span><p>Conventional knowledge distillation (KD) methods for object detection mainly
concentrate on homogeneous teacher-student detectors. However, the design of a
lightweight detector for deployment is often significantly different from a
high-capacity detector. Thus, we investigate KD among heterogeneous
teacher-student pairs for a wide application. We observe that the core
difficulty for heterogeneous KD (hetero-KD) is the significant semantic gap
between the backbone features of heterogeneous detectors due to the different
optimization manners. Conventional homogeneous KD (homo-KD) methods suffer from
such a gap and are hard to directly obtain satisfactory performance for
hetero-KD. In this paper, we propose the HEtero-Assists Distillation (HEAD)
framework, leveraging heterogeneous detection heads as assistants to guide the
optimization of the student detector to reduce this gap. In HEAD, the assistant
is an additional detection head with the architecture homogeneous to the
teacher head attached to the student backbone. Thus, a hetero-KD is transformed
into a homo-KD, allowing efficient knowledge transfer from the teacher to the
student. Moreover, we extend HEAD into a Teacher-Free HEAD (TF-HEAD) framework
when a well-trained teacher detector is unavailable. Our method has achieved
significant improvement compared to current detection KD methods. For example,
on the MS-COCO dataset, TF-HEAD helps R18 RetinaNet achieve 33.9 mAP (+2.2),
while HEAD further pushes the limit to 36.2 mAP (+4.5).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">eX-ViT: A Novel eXplainable Vision Transformer for Weakly Supervised Semantic Segmentation. (arXiv:2207.05358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05358">
<div class="article-summary-box-inner">
<span><p>Recently vision transformer models have become prominent models for a range
of vision tasks. These models, however, are usually opaque with weak feature
interpretability. Moreover, there is no method currently built for an
intrinsically interpretable transformer, which is able to explain its reasoning
process and provide a faithful explanation. To close these crucial gaps, we
propose a novel vision transformer dubbed the eXplainable Vision Transformer
(eX-ViT), an intrinsically interpretable transformer model that is able to
jointly discover robust interpretable features and perform the prediction.
Specifically, eX-ViT is composed of the Explainable Multi-Head Attention
(E-MHA) module, the Attribute-guided Explainer (AttE) module and the
self-supervised attribute-guided loss. The E-MHA tailors explainable attention
weights that are able to learn semantically interpretable representations from
local patches in terms of model decisions with noise robustness. Meanwhile,
AttE is proposed to encode discriminative attribute features for the target
object through diverse attribute discovery, which constitutes faithful evidence
for the model's predictions. In addition, a self-supervised attribute-guided
loss is developed for our eX-ViT, which aims at learning enhanced
representations through the attribute discriminability mechanism and attribute
diversity mechanism, to localize diverse and discriminative attributes and
generate more robust explanations. As a result, we can uncover faithful and
robust interpretations with diverse attributes through the proposed eX-ViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CP3: Unifying Point Cloud Completion by Pretrain-Prompt-Predict Paradigm. (arXiv:2207.05359v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05359">
<div class="article-summary-box-inner">
<span><p>Point cloud completion aims to predict complete shape from its partial
observation. Current approaches mainly consist of generation and refinement
stages in a coarse-to-fine style. However, the generation stage often lacks
robustness to tackle different incomplete variations, while the refinement
stage blindly recovers point clouds without the semantic awareness. To tackle
these challenges, we unify point cloud Completion by a generic
Pretrain-Prompt-Predict paradigm, namely CP3. Inspired by prompting approaches
from NLP, we creatively reinterpret point cloud generation and refinement as
the prompting and predicting stages, respectively. Then, we introduce a concise
self-supervised pretraining stage before prompting. It can effectively increase
robustness of point cloud generation, by an Incompletion-Of-Incompletion (IOI)
pretext task. Moreover, we develop a novel Semantic Conditional Refinement
(SCR) network at the predicting stage. It can discriminatively modulate
multi-scale refinement with the guidance of semantics. Finally, extensive
experiments demonstrate that our CP3 outperforms the state-of-the-art methods
with a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image and Model Transformation with Secret Key for Vision Transformer. (arXiv:2207.05366v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05366">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a combined use of transformed images and vision
transformer (ViT) models transformed with a secret key. We show for the first
time that models trained with plain images can be directly transformed to
models trained with encrypted images on the basis of the ViT architecture, and
the performance of the transformed models is the same as models trained with
plain images when using test images encrypted with the key. In addition, the
proposed scheme does not require any specially prepared data for training
models or network modification, so it also allows us to easily update the
secret key. In an experiment, the effectiveness of the proposed scheme is
evaluated in terms of performance degradation and model protection performance
in an image classification task on the CIFAR-10 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking gradient weights' influence over saliency map estimation. (arXiv:2207.05374v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05374">
<div class="article-summary-box-inner">
<span><p>Class activation map (CAM) helps to formulate saliency maps that aid in
interpreting the deep neural network's prediction. Gradient-based methods are
generally faster than other branches of vision interpretability and independent
of human guidance. The performance of CAM-like studies depends on the governing
model's layer response, and the influences of the gradients. Typical
gradient-oriented CAM studies rely on weighted aggregation for saliency map
estimation by projecting the gradient maps into single weight values, which may
lead to over generalized saliency map. To address this issue, we use a global
guidance map to rectify the weighted aggregation operation during saliency
estimation, where resultant interpretations are comparatively clean er and
instance-specific. We obtain the global guidance map by performing elementwise
multiplication between the feature maps and their corresponding gradient maps.
To validate our study, we compare the proposed study with eight different
saliency visualizers. In addition, we use seven commonly used evaluation
metrics for quantitative comparison. The proposed scheme achieves significant
improvement over the test images from the ImageNet, MS-COCO 14, and PASCAL VOC
2012 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Occluded Human Body Capture with Self-Supervised Spatial-Temporal Motion Prior. (arXiv:2207.05375v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05375">
<div class="article-summary-box-inner">
<span><p>Although significant progress has been achieved on monocular maker-less human
motion capture in recent years, it is still hard for state-of-the-art methods
to obtain satisfactory results in occlusion scenarios. There are two main
reasons: the one is that the occluded motion capture is inherently ambiguous as
various 3D poses can map to the same 2D observations, which always results in
an unreliable estimation. The other is that no sufficient occluded human data
can be used for training a robust model. To address the obstacles, our key-idea
is to employ non-occluded human data to learn a joint-level spatial-temporal
motion prior for occluded human with a self-supervised strategy. To further
reduce the gap between synthetic and real occlusion data, we build the first 3D
occluded motion dataset~(OcMotion), which can be used for both training and
testing. We encode the motions in 2D maps and synthesize occlusions on
non-occluded data for the self-supervised training. A spatial-temporal layer is
then designed to learn joint-level correlations. The learned prior reduces the
ambiguities of occlusions and is robust to diverse occlusion types, which is
then adopted to assist the occluded human motion capture. Experimental results
show that our method can generate accurate and coherent human motions from
occluded videos with good generalization ability and runtime efficiency. The
dataset and code are publicly available at
\url{https://github.com/boycehbz/CHOMP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Neural Rendering using Anime Character Sheets. (arXiv:2207.05378v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05378">
<div class="article-summary-box-inner">
<span><p>Drawing images of characters at desired poses is an essential but laborious
task in anime production. In this paper, we present the Collaborative Neural
Rendering~(CoNR) method to create new images from a few arbitrarily posed
reference images available in character sheets. In general, the high diversity
of body shapes of anime characters defies the employment of universal body
models for real-world humans, like SMPL. To overcome this difficulty, CoNR uses
a compact and easy-to-obtain landmark encoding to avoid creating a unified UV
mapping in the pipeline. In addition, CoNR's performance can be significantly
increased when having multiple reference images by using feature space
cross-view dense correspondence and warping in a specially designed neural
network construct. Moreover, we collect a character sheet dataset containing
over 700,000 hand-drawn and synthesized images of diverse poses to facilitate
research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency Domain Model Augmentation for Adversarial Attack. (arXiv:2207.05382v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05382">
<div class="article-summary-box-inner">
<span><p>For black-box attacks, the gap between the substitute model and the victim
model is usually large, which manifests as a weak attack performance. Motivated
by the observation that the transferability of adversarial examples can be
improved by attacking diverse models simultaneously, model augmentation methods
which simulate different models by using transformed images are proposed.
However, existing transformations for spatial domain do not translate to
significantly diverse augmented models. To tackle this issue, we propose a
novel spectrum simulation attack to craft more transferable adversarial
examples against both normally trained and defense models. Specifically, we
apply a spectrum transformation to the input and thus perform the model
augmentation in the frequency domain. We theoretically prove that the
transformation derived from frequency domain leads to a diverse spectrum
saliency map, an indicator we proposed to reflect the diversity of substitute
models. Notably, our method can be generally combined with existing attacks.
Extensive experiments on the ImageNet dataset demonstrate the effectiveness of
our method, \textit{e.g.}, attacking nine state-of-the-art defense models with
an average success rate of \textbf{95.4\%}. Our code is available in
\url{https://github.com/yuyang-long/SSA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Shadow Generation Using Pixel Height Maps. (arXiv:2207.05385v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05385">
<div class="article-summary-box-inner">
<span><p>Shadows are essential for realistic image compositing. Physics-based shadow
rendering methods require 3D geometries, which are not always available. Deep
learning-based shadow synthesis methods learn a mapping from the light
information to an object's shadow without explicitly modeling the shadow
geometry. Still, they lack control and are prone to visual artifacts. We
introduce pixel heigh, a novel geometry representation that encodes the
correlations between objects, ground, and camera pose. The pixel height can be
calculated from 3D geometries, manually annotated on 2D images, and can also be
predicted from a single-view RGB image by a supervised approach. It can be used
to calculate hard shadows in a 2D image based on the projective geometry,
providing precise control of the shadows' direction and shape. Furthermore, we
propose a data-driven soft shadow generator to apply softness to a hard shadow
based on a softness input parameter. Qualitative and quantitative evaluations
demonstrate that the proposed pixel height significantly improves the quality
of the shadow generation while allowing for controllability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wound Segmentation with Dynamic Illumination Correction and Dual-view Semantic Fusion. (arXiv:2207.05388v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05388">
<div class="article-summary-box-inner">
<span><p>Wound image segmentation is a critical component for the clinical diagnosis
and in-time treatment of wounds. Recently, deep learning has become the
mainstream methodology for wound image segmentation. However, the
pre-processing of the wound image, such as the illumination correction, is
required before the training phase as the performance can be greatly improved.
The correction procedure and the training of deep models are independent of
each other, which leads to sub-optimal segmentation performance as the fixed
illumination correction may not be suitable for all images. To address
aforementioned issues, an end-to-end dual-view segmentation approach was
proposed in this paper, by incorporating a learn-able illumination correction
module into the deep segmentation models. The parameters of the module can be
learned and updated during the training stage automatically, while the
dual-view fusion can fully employ the features from both the raw images and the
enhanced ones. To demonstrate the effectiveness and robustness of the proposed
framework, the extensive experiments are conducted on the benchmark datasets.
The encouraging results suggest that our framework can significantly improve
the segmentation performance, compared to the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Condensation Distillation. (arXiv:2207.05409v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05409">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) transfers the knowledge from a high-capacity
teacher network to strengthen a smaller student. Existing methods focus on
excavating the knowledge hints and transferring the whole knowledge to the
student. However, the knowledge redundancy arises since the knowledge shows
different values to the student at different learning stages. In this paper, we
propose Knowledge Condensation Distillation (KCD). Specifically, the knowledge
value on each sample is dynamically estimated, based on which an
Expectation-Maximization (EM) framework is forged to iteratively condense a
compact knowledge set from the teacher to guide the student learning. Our
approach is easy to build on top of the off-the-shelf KD methods, with no extra
training parameters and negligible computation overhead. Thus, it presents one
new perspective for KD, in which the student that actively identifies teacher's
knowledge in line with its aptitude can learn to learn more effectively and
efficiently. Experiments on standard benchmarks manifest that the proposed KCD
can well boost the performance of student model with even higher distillation
efficiency. Code is available at https://github.com/dzy3/KCD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Baseline for Detecting Out-of-Distribution Examples in Image Captioning. (arXiv:2207.05418v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05418">
<div class="article-summary-box-inner">
<span><p>Image captioning research achieved breakthroughs in recent years by
developing neural models that can generate diverse and high-quality
descriptions for images drawn from the same distribution as training images.
However, when facing out-of-distribution (OOD) images, such as corrupted
images, or images containing unknown objects, the models fail in generating
relevant captions.
</p>
<p>In this paper, we consider the problem of OOD detection in image captioning.
We formulate the problem and suggest an evaluation setup for assessing the
model's performance on the task. Then, we analyze and show the effectiveness of
the caption's likelihood score at detecting and rejecting OOD images, which
implies that the relatedness between the input image and the generated caption
is encapsulated within the score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniNet: Unified Architecture Search with Convolution, Transformer, and MLP. (arXiv:2207.05420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05420">
<div class="article-summary-box-inner">
<span><p>Recently, transformer and multi-layer perceptron (MLP) architectures have
achieved impressive results on various vision tasks. However, how to
effectively combine those operators to form high-performance hybrid visual
architectures still remains a challenge. In this work, we study the learnable
combination of convolution, transformer, and MLP by proposing a novel unified
architecture search approach. Our approach contains two key designs to achieve
the search for high-performance networks. First, we model the very different
searchable operators in a unified form, and thus enable the operators to be
characterized with the same set of configuration parameters. In this way, the
overall search space size is significantly reduced, and the total search cost
becomes affordable. Second, we propose context-aware downsampling modules
(DSMs) to mitigate the gap between the different types of operators. Our
proposed DSMs are able to better adapt features from different types of
operators, which is important for identifying high-performance hybrid
architectures. Finally, we integrate configurable operators and DSMs into a
unified search space and search with a Reinforcement Learning-based search
algorithm to fully explore the optimal combination of the operators. To this
end, we search a baseline network and scale it up to obtain a family of models,
named UniNets, which achieve much better accuracy and efficiency than previous
ConvNets and Transformers. In particular, our UniNet-B5 achieves 84.9% top-1
accuracy on ImageNet, outperforming EfficientNet-B7 and BoTNet-T7 with 44% and
55% fewer FLOPs respectively. By pretraining on the ImageNet-21K, our UniNet-B6
achieves 87.4%, outperforming Swin-L with 51% fewer FLOPs and 41% fewer
parameters. Code is available at https://github.com/Sense-X/UniNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Domain Generalization by Learning without Forgetting: Application in Retail Checkout. (arXiv:2207.05422v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05422">
<div class="article-summary-box-inner">
<span><p>Designing an automatic checkout system for retail stores at the human level
accuracy is challenging due to similar appearance products and their various
poses. This paper addresses the problem by proposing a method with a two-stage
pipeline. The first stage detects class-agnostic items, and the second one is
dedicated to classify product categories. We also track the objects across
video frames to avoid duplicated counting. One major challenge is the domain
gap because the models are trained on synthetic data but tested on the real
images. To reduce the error gap, we adopt domain generalization methods for the
first-stage detector. In addition, model ensemble is used to enhance the
robustness of the 2nd-stage classifier. The method is evaluated on the AI City
challenge 2022 -- Track 4 and gets the F1 score $40\%$ on the test A set. Code
is released at the link https://github.com/cybercore-co-ltd/aicity22-track4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Diverse Tone Styles for Image Retouching. (arXiv:2207.05430v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05430">
<div class="article-summary-box-inner">
<span><p>Image retouching, aiming to regenerate the visually pleasing renditions of
given images, is a subjective task where the users are with different aesthetic
sensations. Most existing methods deploy a deterministic model to learn the
retouching style from a specific expert, making it less flexible to meet
diverse subjective preferences. Besides, the intrinsic diversity of an expert
due to the targeted processing on different images is also deficiently
described. To circumvent such issues, we propose to learn diverse image
retouching with normalizing flow-based architectures. Unlike current flow-based
methods which directly generate the output image, we argue that learning in a
style domain could (i) disentangle the retouching styles from the image
content, (ii) lead to a stable style presentation form, and (iii) avoid the
spatial disharmony effects. For obtaining meaningful image tone style
representations, a joint-training pipeline is delicately designed, which is
composed of a style encoder, a conditional RetouchNet, and the image tone style
normalizing flow (TSFlow) module. In particular, the style encoder predicts the
target style representation of an input image, which serves as the conditional
information in the RetouchNet for retouching, while the TSFlow maps the style
representation vector into a Gaussian distribution in the forward pass. After
training, the TSFlow can generate diverse image tone style vectors by sampling
from the Gaussian distribution. Extensive experiments on MIT-Adobe FiveK and
PPR10K datasets show that our proposed method performs favorably against
state-of-the-art methods and is effective in generating diverse results to
satisfy different human aesthetic preferences. Source code and pre-trained
models are publicly available at https://github.com/SSRHeart/TSFlow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synergistic Self-supervised and Quantization Learning. (arXiv:2207.05432v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05432">
<div class="article-summary-box-inner">
<span><p>With the success of self-supervised learning (SSL), it has become a
mainstream paradigm to fine-tune from self-supervised pretrained models to
boost the performance on downstream tasks. However, we find that current SSL
models suffer severe accuracy drops when performing low-bit quantization,
prohibiting their deployment in resource-constrained applications. In this
paper, we propose a method called synergistic self-supervised and quantization
learning (SSQL) to pretrain quantization-friendly self-supervised models
facilitating downstream deployment. SSQL contrasts the features of the
quantized and full precision models in a self-supervised fashion, where the
bit-width for the quantized model is randomly selected in each step. SSQL not
only significantly improves the accuracy when quantized to lower bit-widths,
but also boosts the accuracy of full precision models in most cases. By only
training once, SSQL can then benefit various downstream tasks at different
bit-widths simultaneously. Moreover, the bit-width flexibility is achieved
without additional storage overhead, requiring only one copy of weights during
training and inference. We theoretically analyze the optimization process of
SSQL, and conduct exhaustive experiments on various benchmarks to further
demonstrate the effectiveness of our method. Our code is available at
https://github.com/megvii-research/SSQL-ECCV2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Category-Level 6D Object Pose and Size Estimation using Self-Supervised Deep Prior Deformation Networks. (arXiv:2207.05444v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05444">
<div class="article-summary-box-inner">
<span><p>It is difficult to precisely annotate object instances and their semantics in
3D space, and as such, synthetic data are extensively used for these tasks,
e.g., category-level 6D object pose and size estimation. However, the easy
annotations in synthetic domains bring the downside effect of synthetic-to-real
(Sim2Real) domain gap. In this work, we aim to address this issue in the task
setting of Sim2Real, unsupervised domain adaptation for category-level 6D
object pose and size estimation. We propose a method that is built upon a novel
Deep Prior Deformation Network, shortened as DPDN. DPDN learns to deform
features of categorical shape priors to match those of object observations, and
is thus able to establish deep correspondence in the feature space for direct
regression of object poses and sizes. To reduce the Sim2Real domain gap, we
formulate a novel self-supervised objective upon DPDN via consistency learning;
more specifically, we apply two rigid transformations to each object
observation in parallel, and feed them into DPDN respectively to yield dual
sets of predictions; on top of the parallel learning, an inter-consistency term
is employed to keep cross consistency between dual predictions for improving
the sensitivity of DPDN to pose changes, while individual intra-consistency
ones are used to enforce self-adaptation within each learning itself. We train
DPDN on both training sets of the synthetic CAMERA25 and real-world REAL275
datasets; our results outperform the existing methods on REAL275 test set under
both the unsupervised and supervised settings. Ablation studies also verify the
efficacy of our designs. Our code is released publicly at
https://github.com/JiehongLin/Self-DPDN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Effects of Image Quality Degradation on Minutiae- and Ridge-Based Automatic Fingerprint Recognition. (arXiv:2207.05447v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05447">
<div class="article-summary-box-inner">
<span><p>The effect of image quality degradation on the verification performance of
automatic fingerprint recognition is investigated. We study the performance of
two fingerprint matchers based on minutiae and ridge information under varying
fingerprint image quality. The ridge-based system is found to be more robust to
image quality degradation than the minutiae-based system for a number of
different image quality criteria.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A review of schemes for fingerprint image quality computation. (arXiv:2207.05449v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05449">
<div class="article-summary-box-inner">
<span><p>Fingerprint image quality affects heavily the performance of fingerprint
recognition systems. This paper reviews existing approaches for fingerprint
image quality computation. We also implement, test and compare a selection of
them using the MCYT database including 9000 fingerprint images. Experimental
results show that most of the algorithms behave similarly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransFA: Transformer-based Representation for Face Attribute Evaluation. (arXiv:2207.05456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05456">
<div class="article-summary-box-inner">
<span><p>Face attribute evaluation plays an important role in video surveillance and
face analysis. Although methods based on convolution neural networks have made
great progress, they inevitably only deal with one local neighborhood with
convolutions at a time. Besides, existing methods mostly regard face attribute
evaluation as the individual multi-label classification task, ignoring the
inherent relationship between semantic attributes and face identity
information. In this paper, we propose a novel \textbf{trans}former-based
representation for \textbf{f}ace \textbf{a}ttribute evaluation method
(\textbf{TransFA}), which could effectively enhance the attribute
discriminative representation learning in the context of attention mechanism.
The multiple branches transformer is employed to explore the inter-correlation
between different attributes in similar semantic regions for attribute feature
learning. Specially, the hierarchical identity-constraint attribute loss is
designed to train the end-to-end architecture, which could further integrate
face identity discriminative information to boost performance. Experimental
results on multiple face attribute benchmarks demonstrate that the proposed
TransFA achieves superior performances compared with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the limits of perceptual quality measures for enhanced underwater images. (arXiv:2207.05470v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05470">
<div class="article-summary-box-inner">
<span><p>The appearance of objects in underwater images is degraded by the selective
attenuation of light, which reduces contrast and causes a colour cast. This
degradation depends on the water environment, and increases with depth and with
the distance of the object from the camera. Despite an increasing volume of
works in underwater image enhancement and restoration, the lack of a commonly
accepted evaluation measure is hindering the progress as it is difficult to
compare methods. In this paper, we review commonly used colour accuracy
measures, such as colour reproduction error and CIEDE2000, and no-reference
image quality measures, such as UIQM, UCIQE and CCF, which have not yet been
systematically validated. We show that none of the no-reference quality
measures satisfactorily rates the quality of enhanced underwater images and
discuss their main shortcomings. Images and results are available at
https://puiqe.eecs.qmul.ac.uk.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel conservative chaos driven dynamic DNA coding for image encryption. (arXiv:2207.05475v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05475">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel conservative chaotic standard map-driven
dynamic DNA coding (encoding, addition, subtraction and decoding) for the image
encryption. The proposed image encryption algorithm is a dynamic DNA coding
algorithm i.e., for the encryption of each pixel different rules for encoding,
addition/subtraction, decoding etc. are randomly selected based on the
pseudorandom sequences generated with the help of the conservative chaotic
standard map. We propose a novel way to generate pseudo-random sequences
through the conservative chaotic standard map and also test them rigorously
through the most stringent test suite of pseudo-randomness, the NIST test
suite, before using them in the proposed image encryption algorithm. Our image
encryption algorithm incorporates a unique feed-forward and feedback mechanisms
to generate and modify the dynamic one-time pixels that are further used for
the encryption of each pixel of the plain image, therefore, bringing in the
desired sensitivity on plaintext as well as ciphertext. All the controlling
pseudorandom sequences used in the algorithm are generated for a different
value of the parameter (part of the secret key) with inter-dependency through
the iterates of the chaotic map (in the generation process) and therefore
possess extreme key sensitivity too. The performance and security analysis has
been executed extensively through histogram analysis, correlation analysis,
information entropy analysis, DNA sequence-based analysis, perceptual quality
analysis, key sensitivity analysis, plaintext sensitivity analysis, etc., The
results are promising and prove the robustness of the algorithm against various
common cryptanalytic attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VertXNet: Automatic Segmentation and Identification of Lumbar and Cervical Vertebrae from Spinal X-ray Images. (arXiv:2207.05476v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05476">
<div class="article-summary-box-inner">
<span><p>Manual annotation of vertebrae on spinal X-ray imaging is costly and
time-consuming due to bone shape complexity and image quality variations. In
this study, we address this challenge by proposing an ensemble method called
VertXNet, to automatically segment and label vertebrae in X-ray spinal images.
VertXNet combines two state-of-the-art segmentation models, namely U-Net and
Mask R-CNN to improve vertebrae segmentation. A main feature of VertXNet is to
also infer vertebrae labels thanks to its Mask R-CNN component (trained to
detect 'reference' vertebrae) on a given spinal X-ray image. VertXNet was
evaluated on an in-house dataset of lateral cervical and lumbar X-ray imaging
for ankylosing spondylitis (AS) patients. Our results show that VertXNet can
accurately label spinal X-rays (mean Dice of 0.9). It can be used to circumvent
the lack of annotated vertebrae without requiring human expert review. This
step is crucial to investigate clinical associations by solving the lack of
segmentation, a common bottleneck for most computational imaging projects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CorrI2P: Deep Image-to-Point Cloud Registration via Dense Correspondence. (arXiv:2207.05483v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05483">
<div class="article-summary-box-inner">
<span><p>Motivated by the intuition that the critical step of localizing a 2D image in
the corresponding 3D point cloud is establishing 2D-3D correspondence between
them, we propose the first feature-based dense correspondence framework for
addressing the image-to-point cloud registration problem, dubbed CorrI2P, which
consists of three modules, i.e., feature embedding, symmetric overlapping
region detection, and pose estimation through the established correspondence.
Specifically, given a pair of a 2D image and a 3D point cloud, we first
transform them into high-dimensional feature space and feed the resulting
features into a symmetric overlapping region detector to determine the region
where the image and point cloud overlap each other. Then we use the features of
the overlapping regions to establish the 2D-3D correspondence before running
EPnP within RANSAC to estimate the camera's pose. Experimental results on KITTI
and NuScenes datasets show that our CorrI2P outperforms state-of-the-art
image-to-point cloud registration methods significantly. We will make the code
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skeletal Human Action Recognition using Hybrid Attention based Graph Convolutional Network. (arXiv:2207.05493v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05493">
<div class="article-summary-box-inner">
<span><p>In skeleton-based action recognition, Graph Convolutional Networks model
human skeletal joints as vertices and connect them through an adjacency matrix,
which can be seen as a local attention mask. However, in most existing Graph
Convolutional Networks, the local attention mask is defined based on natural
connections of human skeleton joints and ignores the dynamic relations for
example between head, hands and feet joints. In addition, the attention
mechanism has been proven effective in Natural Language Processing and image
description, which is rarely investigated in existing methods. In this work, we
proposed a new adaptive spatial attention layer that extends local attention
map to global based on relative distance and relative angle information.
Moreover, we design a new initial graph adjacency matrix that connects head,
hands and feet, which shows visible improvement in terms of action recognition
accuracy. The proposed model is evaluated on two large-scale and challenging
datasets in the field of human activities in daily life: NTU-RGB+D and Kinetics
skeleton. The results demonstrate that our model has strong performance on both
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paint and Distill: Boosting 3D Object Detection with Semantic Passing Network. (arXiv:2207.05497v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05497">
<div class="article-summary-box-inner">
<span><p>3D object detection task from lidar or camera sensors is essential for
autonomous driving. Pioneer attempts at multi-modality fusion complement the
sparse lidar point clouds with rich semantic texture information from images at
the cost of extra network designs and overhead. In this work, we propose a
novel semantic passing framework, named SPNet, to boost the performance of
existing lidar-based 3D detection models with the guidance of rich context
painting, with no extra computation cost during inference. Our key design is to
first exploit the potential instructive semantic knowledge within the
ground-truth labels by training a semantic-painted teacher model and then guide
the pure-lidar network to learn the semantic-painted representation via
knowledge passing modules at different granularities: class-wise passing,
pixel-wise passing and instance-wise passing. Experimental results show that
the proposed SPNet can seamlessly cooperate with most existing 3D detection
frameworks with 1~5% AP gain and even achieve new state-of-the-art 3D detection
performance on the KITTI test benchmark. Code is available at:
https://github.com/jb892/SPNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modality-Aware Contrastive Instance Learning with Self-Distillation for Weakly-Supervised Audio-Visual Violence Detection. (arXiv:2207.05500v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05500">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised audio-visual violence detection aims to distinguish
snippets containing multimodal violence events with video-level labels. Many
prior works perform audio-visual integration and interaction in an early or
intermediate manner, yet overlooking the modality heterogeneousness over the
weakly-supervised setting. In this paper, we analyze the modality asynchrony
and undifferentiated instances phenomena of the multiple instance learning
(MIL) procedure, and further investigate its negative impact on
weakly-supervised audio-visual learning. To address these issues, we propose a
modality-aware contrastive instance learning with self-distillation (MACIL-SD)
strategy. Specifically, we leverage a lightweight two-stream network to
generate audio and visual bags, in which unimodal background, violent, and
normal instances are clustered into semi-bags in an unsupervised way. Then
audio and visual violent semi-bag representations are assembled as positive
pairs, and violent semi-bags are combined with background and normal instances
in the opposite modality as contrastive negative pairs. Furthermore, a
self-distillation module is applied to transfer unimodal visual knowledge to
the audio-visual model, which alleviates noises and closes the semantic gap
between unimodal and multimodal features. Experiments show that our framework
outperforms previous methods with lower complexity on the large-scale
XD-Violence dataset. Results also demonstrate that our proposed approach can be
used as plug-in modules to enhance other networks. Codes are available at
https://github.com/JustinYuu/MACIL_SD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios. (arXiv:2207.05501v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05501">
<div class="article-summary-box-inner">
<span><p>Due to the complex attention mechanisms and model design, most existing
vision Transformers (ViTs) can not perform as efficiently as convolutional
neural networks (CNNs) in realistic industrial deployment scenarios, e.g.
TensorRT and CoreML. This poses a distinct challenge: Can a visual neural
network be designed to infer as fast as CNNs and perform as powerful as ViTs?
Recent works have tried to design CNN-Transformer hybrid architectures to
address this issue, yet the overall performance of these works is far away from
satisfactory. To end these, we propose a next generation vision Transformer for
efficient deployment in realistic industrial scenarios, namely Next-ViT, which
dominates both CNNs and ViTs from the perspective of latency/accuracy
trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer
Block (NTB) are respectively developed to capture local and global information
with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is
designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts
performance in various downstream tasks. Extensive experiments show that
Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer
hybrid architectures with respect to the latency/accuracy trade-off across
various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.4 mAP (from
40.4 to 45.8) on COCO detection and 8.2% mIoU (from 38.8% to 47.0%) on ADE20K
segmentation under similar latency. Meanwhile, it achieves comparable
performance with CSWin, while the inference speed is accelerated by 3.6x. On
CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on
COCO detection and 3.5% mIoU (from 45.2% to 48.7%) on ADE20K segmentation under
similar latency. Code will be released recently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferability-Guided Cross-Domain Cross-Task Transfer Learning. (arXiv:2207.05510v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05510">
<div class="article-summary-box-inner">
<span><p>We propose two novel transferability metrics F-OTCE (Fast Optimal Transport
based Conditional Entropy) and JC-OTCE (Joint Correspondence OTCE) to evaluate
how much the source model (task) can benefit the learning of the target task
and to learn more transferable representations for cross-domain cross-task
transfer learning. Unlike the existing metric that requires evaluating the
empirical transferability on auxiliary tasks, our metrics are auxiliary-free
such that they can be computed much more efficiently. Specifically, F-OTCE
estimates transferability by first solving an Optimal Transport (OT) problem
between source and target distributions, and then uses the optimal coupling to
compute the Negative Conditional Entropy between source and target labels. It
can also serve as a loss function to maximize the transferability of the source
model before finetuning on the target task. Meanwhile, JC-OTCE improves the
transferability robustness of F-OTCE by including label distances in the OT
problem, though it may incur additional computation cost. Extensive experiments
demonstrate that F-OTCE and JC-OTCE outperform state-of-the-art auxiliary-free
metrics by 18.85% and 28.88%, respectively in correlation coefficient with the
ground-truth transfer accuracy. By eliminating the training cost of auxiliary
tasks, the two metrics reduces the total computation time of the previous
method from 43 minutes to 9.32s and 10.78s, respectively, for a pair of tasks.
When used as a loss function, F-OTCE shows consistent improvements on the
transfer accuracy of the source model in few-shot classification experiments,
with up to 4.41% accuracy gain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compound Prototype Matching for Few-shot Action Recognition. (arXiv:2207.05515v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05515">
<div class="article-summary-box-inner">
<span><p>Few-shot action recognition aims to recognize novel action classes using only
a small number of labeled training samples. In this work, we propose a novel
approach that first summarizes each video into compound prototypes consisting
of a group of global prototypes and a group of focused prototypes, and then
compares video similarity based on the prototypes. Each global prototype is
encouraged to summarize a specific aspect from the entire video, for example,
the start/evolution of the action. Since no clear annotation is provided for
the global prototypes, we use a group of focused prototypes to focus on certain
timestamps in the video. We compare video similarity by matching the compound
prototypes between the support and query videos. The global prototypes are
directly matched to compare videos from the same perspective, for example, to
compare whether two actions start similarly. For the focused prototypes, since
actions have various temporal variations in the videos, we apply bipartite
matching to allow the comparison of actions with different temporal positions
and shifts. Experiments demonstrate that our proposed method achieves
state-of-the-art results on multiple benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracking Objects as Pixel-wise Distributions. (arXiv:2207.05518v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05518">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking (MOT) requires detecting and associating objects
through frames. Unlike tracking via detected bounding boxes or tracking objects
as points, we propose tracking objects as pixel-wise distributions. We
instantiate this idea on a transformer-based architecture, P3AFormer, with
pixel-wise propagation, prediction, and association. P3AFormer propagates
pixel-wise features guided by flow information to pass messages between frames.
Furthermore, P3AFormer adopts a meta-architecture to produce multi-scale object
feature maps. During inference, a pixel-wise association procedure is proposed
to recover object connections through frames based on the pixel-wise
prediction. P3AFormer yields 81.2\% in terms of MOTA on the MOT17 benchmark --
the first among all transformer networks to reach 80\% MOTA in literature.
P3AFormer also outperforms state-of-the-arts on the MOT20 and KITTI benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-term Leap Attention, Short-term Periodic Shift for Video Classification. (arXiv:2207.05526v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05526">
<div class="article-summary-box-inner">
<span><p>Video transformer naturally incurs a heavier computation burden than a static
vision transformer, as the former processes $T$ times longer sequence than the
latter under the current attention of quadratic complexity $(T^2N^2)$. The
existing works treat the temporal axis as a simple extension of spatial axes,
focusing on shortening the spatio-temporal sequence by either generic pooling
or local windowing without utilizing temporal redundancy.
</p>
<p>However, videos naturally contain redundant information between neighboring
frames; thereby, we could potentially suppress attention on visually similar
frames in a dilated manner. Based on this hypothesis, we propose the LAPS, a
long-term ``\textbf{\textit{Leap Attention}}'' (LA), short-term
``\textbf{\textit{Periodic Shift}}'' (\textit{P}-Shift) module for video
transformers, with $(2TN^2)$ complexity. Specifically, the ``LA'' groups
long-term frames into pairs, then refactors each discrete pair via attention.
The ``\textit{P}-Shift'' exchanges features between temporal neighbors to
confront the loss of short-term dynamics. By replacing a vanilla 2D attention
with the LAPS, we could adapt a static transformer into a video one, with zero
extra parameters and neglectable computation overhead ($\sim$2.6\%).
Experiments on the standard Kinetics-400 benchmark demonstrate that our LAPS
transformer could achieve competitive performances in terms of accuracy, FLOPs,
and Params among CNN and transformer SOTAs. We open-source our project in
\sloppy
\href{https://github.com/VideoNetworks/LAPS-transformer}{\textit{\color{magenta}{https://github.com/VideoNetworks/LAPS-transformer}}} .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Camera Pose Auto-Encoders for Improving Pose Regression. (arXiv:2207.05530v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05530">
<div class="article-summary-box-inner">
<span><p>Absolute pose regressor (APR) networks are trained to estimate the pose of
the camera given a captured image. They compute latent image representations
from which the camera position and orientation are regressed. APRs provide a
different tradeoff between localization accuracy, runtime, and memory, compared
to structure-based localization schemes that provide state-of-the-art accuracy.
In this work, we introduce Camera Pose Auto-Encoders (PAEs), multilayer
perceptrons that are trained via a Teacher-Student approach to encode camera
poses using APRs as their teachers. We show that the resulting latent pose
representations can closely reproduce APR performance and demonstrate their
effectiveness for related tasks. Specifically, we propose a light-weight
test-time optimization in which the closest train poses are encoded and used to
refine camera position estimation. This procedure achieves a new
state-of-the-art position accuracy for APRs, on both the CambridgeLandmarks and
7Scenes benchmarks. We also show that train images can be reconstructed from
the learned pose encoding, paving the way for integrating visual information
from the train set at a low memory cost. Our code and pre-trained models are
available at https://github.com/yolish/camera-pose-auto-encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing Excess Resources in Training Neural Networks. (arXiv:2207.05532v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05532">
<div class="article-summary-box-inner">
<span><p>In this work, we suggest Kernel Filtering Linear Overparameterization (KFLO),
where a linear cascade of filtering layers is used during training to improve
network performance in test time. We implement this cascade in a kernel
filtering fashion, which prevents the trained architecture from becoming
unnecessarily deeper. This also allows using our approach with almost any
network architecture and let combining the filtering layers into a single layer
in test time. Thus, our approach does not add computational complexity during
inference. We demonstrate the advantage of KFLO on various network models and
datasets in supervised learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DTG-SSOD: Dense Teacher Guidance for Semi-Supervised Object Detection. (arXiv:2207.05536v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05536">
<div class="article-summary-box-inner">
<span><p>The Mean-Teacher (MT) scheme is widely adopted in semi-supervised object
detection (SSOD). In MT, the sparse pseudo labels, offered by the final
predictions of the teacher (e.g., after Non Maximum Suppression (NMS)
post-processing), are adopted for the dense supervision for the student via
hand-crafted label assignment. However, the sparse-to-dense paradigm
complicates the pipeline of SSOD, and simultaneously neglects the powerful
direct, dense teacher supervision. In this paper, we attempt to directly
leverage the dense guidance of teacher to supervise student training, i.e., the
dense-to-dense paradigm. Specifically, we propose the Inverse NMS Clustering
(INC) and Rank Matching (RM) to instantiate the dense supervision, without the
widely used, conventional sparse pseudo labels. INC leads the student to group
candidate boxes into clusters in NMS as the teacher does, which is implemented
by learning grouping information revealed in NMS procedure of the teacher.
After obtaining the same grouping scheme as the teacher via INC, the student
further imitates the rank distribution of the teacher over clustered candidates
through Rank Matching. With the proposed INC and RM, we integrate Dense Teacher
Guidance into Semi-Supervised Object Detection (termed DTG-SSOD), successfully
abandoning sparse pseudo labels and enabling more informative learning on
unlabeled data. On COCO benchmark, our DTG-SSOD achieves state-of-the-art
performance under various labelling ratios. For example, under 10% labelling
ratio, DTG-SSOD improves the supervised baseline from 26.9 to 35.9 mAP,
outperforming the previous best method Soft Teacher by 1.9 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Markovian Gaussian Process Variational Autoencoders. (arXiv:2207.05543v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05543">
<div class="article-summary-box-inner">
<span><p>Deep generative models are widely used for modelling high-dimensional time
series, such as video animations, audio and climate data. Sequential
variational autoencoders have been successfully considered for many
applications, with many variant models relying on discrete-time methods and
recurrent neural networks (RNNs). On the other hand, continuous-time methods
have recently gained attraction, especially in the context of
irregularly-sampled time series, where they can better handle the data than
discrete-time methods. One such class are Gaussian process variational
autoencoders (GPVAEs), where the VAE prior is set as a Gaussian process (GPs),
allowing inductive biases to be explicitly encoded via the kernel function and
interpretability of the latent space. However, a major limitation of GPVAEs is
that it inherits the same cubic computational cost as GPs. In this work, we
leverage the equivalent discrete state space representation of Markovian GPs to
enable a linear-time GP solver via Kalman filtering and smoothing. We show via
corrupt and missing frames tasks that our method performs favourably,
especially on the latter where it outperforms RNN-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LightViT: Towards Light-Weight Convolution-Free Vision Transformers. (arXiv:2207.05557v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05557">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) are usually considered to be less light-weight
than convolutional neural networks (CNNs) due to the lack of inductive bias.
Recent works thus resort to convolutions as a plug-and-play module and embed
them in various ViT counterparts. In this paper, we argue that the
convolutional kernels perform information aggregation to connect all tokens;
however, they would be actually unnecessary for light-weight ViTs if this
explicit aggregation could function in a more homogeneous way. Inspired by
this, we present LightViT as a new family of light-weight ViTs to achieve
better accuracy-efficiency balance upon the pure transformer blocks without
convolution. Concretely, we introduce a global yet efficient aggregation scheme
into both self-attention and feed-forward network (FFN) of ViTs, where
additional learnable tokens are introduced to capture global dependencies; and
bi-dimensional channel and spatial attentions are imposed over token
embeddings. Experiments show that our model achieves significant improvements
on image classification, object detection, and semantic segmentation tasks. For
example, our LightViT-T achieves 78.7% accuracy on ImageNet with only 0.7G
FLOPs, outperforming PVTv2-B0 by 8.2% while 11% faster on GPU. Code is
available at https://github.com/hunto/LightViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Label Relationships in Human Affect. (arXiv:2207.05577v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05577">
<div class="article-summary-box-inner">
<span><p>Human affect and mental state estimation in an automated manner, face a
number of difficulties, including learning from labels with poor or no temporal
resolution, learning from few datasets with little data (often due to
confidentiality constraints) and, (very) long, in-the-wild videos. For these
reasons, deep learning methodologies tend to overfit, that is, arrive at latent
representations with poor generalisation performance on the final regression
task. To overcome this, in this work, we introduce two complementary
contributions. First, we introduce a novel relational loss for multilabel
regression and ordinal problems that regularises learning and leads to better
generalisation. The proposed loss uses label vector inter-relational
information to learn better latent representations by aligning batch label
distances to the distances in the latent feature space. Second, we utilise a
two-stage attention architecture that estimates a target for each clip by using
features from the neighbouring clips as temporal context. We evaluate the
proposed methodology on both continuous affect and schizophrenia severity
estimation problems, as there are methodological and contextual parallels
between the two. Experimental results demonstrate that the proposed methodology
outperforms all baselines. In the domain of schizophrenia, the proposed
methodology outperforms previous state-of-the-art by a large margin, achieving
a PCC of up to 78% performance close to that of human experts (85%) and much
higher than previous works (uplift of up to 40%). In the case of affect
recognition, we outperform previous vision-based methods in terms of CCC on
both the OMG and the AMIGOS datasets. Specifically for AMIGOS, we outperform
previous SoTA CCC for both arousal and valence by 9% and 13% respectively, and
in the OMG dataset we outperform previous vision works by up to 5% for both
arousal and valence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Video Instance Segmentation via Robust Context Fusion. (arXiv:2207.05580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05580">
<div class="article-summary-box-inner">
<span><p>Video instance segmentation (VIS) aims at classifying, segmenting and
tracking object instances in video sequences. Recent transformer-based neural
networks have demonstrated their powerful capability of modeling
spatio-temporal correlations for the VIS task. Relying on video- or clip-level
input, they suffer from high latency and computational cost. We propose a
robust context fusion network to tackle VIS in an online fashion, which
predicts instance segmentation frame-by-frame with a few preceding frames. To
acquire the precise and temporal-consistent prediction for each frame
efficiently, the key idea is to fuse effective and compact context from
reference frames into the target frame. Considering the different effects of
reference and target frames on the target prediction, we first summarize
contextual features through importance-aware compression. A transformer encoder
is adopted to fuse the compressed context. Then, we leverage an
order-preserving instance embedding to convey the identity-aware information
and correspond the identities to predicted instance masks. We demonstrate that
our robust fusion network achieves the best performance among existing online
VIS methods and is even better than previously published clip-level methods on
the Youtube-VIS 2019 and 2021 benchmarks. In addition, visual objects often
have acoustic signatures that are naturally synchronized with them in
audio-bearing video recordings. By leveraging the flexibility of our context
fusion network on multi-modal data, we further investigate the influence of
audios on the video-dense prediction task, which has never been discussed in
existing works. We build up an Audio-Visual Instance Segmentation dataset, and
demonstrate that acoustic signals in the wild scenarios could benefit the VIS
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ego-motion Estimation Based on Fusion of Images and Events. (arXiv:2207.05588v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05588">
<div class="article-summary-box-inner">
<span><p>Event camera is a novel bio-inspired vision sensor that outputs event stream.
In this paper, we propose a novel data fusion algorithm called EAS to fuse
conventional intensity images with the event stream. The fusion result is
applied to some ego-motion estimation frameworks, and is evaluated on a public
dataset acquired in dim scenes. In our 3-DoF rotation estimation framework, EAS
achieves the highest estimation accuracy among intensity images and
representations of events including event slice, TS and SITS. Compared with
original images, EAS reduces the average APE by 69%, benefiting from the
inclusion of more features for tracking. The result shows that our algorithm
effectively leverages the high dynamic range of event cameras to improve the
performance of the ego-motion estimation framework based on optical flow
tracking in difficult illumination conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Real-time High-Definition Image Snow Removal: Efficient Pyramid Network with Asymmetrical Encoder-decoder Architecture. (arXiv:2207.05605v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05605">
<div class="article-summary-box-inner">
<span><p>In winter scenes, the degradation of images taken under snow can be pretty
complex, where the spatial distribution of snowy degradation is varied from
image to image. Recent methods adopt deep neural networks to directly recover
clean scenes from snowy images. However, due to the paradox caused by the
variation of complex snowy degradation, achieving reliable High-Definition
image desnowing performance in real time is a considerable challenge. We
develop a novel Efficient Pyramid Network with asymmetrical encoder-decoder
architecture for real-time HD image desnowing. The general idea of our proposed
network is to utilize the multi-scale feature flow fully and implicitly mine
clean cues from features. Compared with previous state-of-the-art desnowing
methods, our approach achieves a better complexity-performance trade-off and
effectively handles the processing difficulties of HD and Ultra-HD images.
</p>
<p>The extensive experiments on three large-scale image desnowing datasets
demonstrate that our method surpasses all state-of-the-art approaches by a
large margin both quantitatively and qualitatively, boosting the PSNR metric
from 31.76 dB to 34.10 dB on the CSD test dataset and from 28.29 dB to 30.87 dB
on the SRRS test dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inner Monologue: Embodied Reasoning through Planning with Language Models. (arXiv:2207.05608v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05608">
<div class="article-summary-box-inner">
<span><p>Recent works have shown how the reasoning capabilities of Large Language
Models (LLMs) can be applied to domains beyond natural language processing,
such as planning and interaction for robots. These embodied problems require an
agent to understand many semantic aspects of the world: the repertoire of
skills available, how these skills influence the world, and how changes to the
world map back to the language. LLMs planning in embodied environments need to
consider not just what skills to do, but also how and when to do them - answers
that change over time in response to the agent's own choices. In this work, we
investigate to what extent LLMs used in such embodied contexts can reason over
sources of feedback provided through natural language, without any additional
training. We propose that by leveraging environment feedback, LLMs are able to
form an inner monologue that allows them to more richly process and plan in
robotic control scenarios. We investigate a variety of sources of feedback,
such as success detection, scene description, and human interaction. We find
that closed-loop language feedback significantly improves high-level
instruction completion on three domains, including simulated and real table top
rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen
environment in the real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning for Online Semi-Supervised General Continual Learning. (arXiv:2207.05615v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05615">
<div class="article-summary-box-inner">
<span><p>We study Online Continual Learning with missing labels and propose SemiCon, a
new contrastive loss designed for partly labeled data. We demonstrate its
efficiency by devising a memory-based method trained on an unlabeled data
stream, where every data added to memory is labeled using an oracle. Our
approach outperforms existing semi-supervised methods when few labels are
available, and obtain similar results to state-of-the-art supervised methods
while using only 2.6% of labels on Split-CIFAR10 and 10% of labels on
Split-CIFAR100.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LudVision -- Remote Detection of Exotic Invasive Aquatic Floral Species using Drone-Mounted Multispectral Data. (arXiv:2207.05620v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05620">
<div class="article-summary-box-inner">
<span><p>Remote sensing is the process of detecting and monitoring the physical
characteristics of an area by measuring its reflected and emitted radiation at
a distance. It is being broadly used to monitor ecosystems, mainly for their
preservation. Ever-growing reports of invasive species have affected the
natural balance of ecosystems. Exotic invasive species have a critical impact
when introduced into new ecosystems and may lead to the extinction of native
species. In this study, we focus on Ludwigia peploides, considered by the
European Union as an aquatic invasive species. Its presence can negatively
impact the surrounding ecosystem and human activities such as agriculture,
fishing, and navigation. Our goal was to develop a method to identify the
presence of the species. We used images collected by a drone-mounted
multispectral sensor to achieve this, creating our LudVision data set. To
identify the targeted species on the collected images, we propose a new method
for detecting Ludwigia p. in multispectral images. The method is based on
existing state-of-the-art semantic segmentation methods modified to handle
multispectral data. The proposed method achieved a producer's accuracy of 0.799
and a user's accuracy of 0.955.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSP-Former: Multi-Scale Projection Transformer for Single Image Desnowing. (arXiv:2207.05621v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05621">
<div class="article-summary-box-inner">
<span><p>Image restoration of snow scenes in severe weather is a difficult task. Snow
images have complex degradations and are cluttered over clean images, changing
the distribution of clean images. The previous methods based on CNNs are
challenging to remove perfectly in restoring snow scenes due to their local
inductive biases' lack of a specific global modeling ability. In this paper, we
apply the vision transformer to the task of snow removal from a single image.
Specifically, we propose a parallel network architecture split along the
channel, performing local feature refinement and global information modeling
separately. We utilize a channel shuffle operation to combine their respective
strengths to enhance network performance. Second, we propose the MSP module,
which utilizes multi-scale avgpool to aggregate information of different sizes
and simultaneously performs multi-scale projection self-attention on multi-head
self-attention to improve the representation ability of the model under
different scale degradations. Finally, we design a lightweight and simple local
capture module, which can refine the local capture capability of the model.
</p>
<p>In the experimental part, we conduct extensive experiments to demonstrate the
superiority of our method. We compared the previous snow removal methods on
three snow scene datasets. The experimental results show that our method
surpasses the state-of-the-art methods with fewer parameters and computation.
We achieve substantial growth by 1.99dB and SSIM 0.03 on the CSD test dataset.
On the SRRS and Snow100K datasets, we also increased PSNR by 2.47dB and 1.62dB
compared with the Transweather approach and improved by 0.03 in SSIM. In the
visual comparison section, our MSP-Former also achieves better visual effects
than existing methods, proving the usability of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GANzzle: Reframing jigsaw puzzle solving as a retrieval task using a generative mental image. (arXiv:2207.05634v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05634">
<div class="article-summary-box-inner">
<span><p>Puzzle solving is a combinatorial challenge due to the difficulty of matching
adjacent pieces. Instead, we infer a mental image from all pieces, which a
given piece can then be matched against avoiding the combinatorial explosion.
Exploiting advancements in Generative Adversarial methods, we learn how to
reconstruct the image given a set of unordered pieces, allowing the model to
learn a joint embedding space to match an encoding of each piece to the cropped
layer of the generator. Therefore we frame the problem as a R@1 retrieval task,
and then solve the linear assignment using differentiable Hungarian attention,
making the process end-to-end. In doing so our model is puzzle size agnostic,
in contrast to prior deep learning methods which are single size. We evaluate
on two new large-scale datasets, where our model is on par with deep learning
methods, while generalizing to multiple puzzle sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Attacks on Crowd Counting. (arXiv:2207.05641v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05641">
<div class="article-summary-box-inner">
<span><p>Crowd counting is a regression task that estimates the number of people in a
scene image, which plays a vital role in a range of safety-critical
applications, such as video surveillance, traffic monitoring and flow control.
In this paper, we investigate the vulnerability of deep learning based crowd
counting models to backdoor attacks, a major security threat to deep learning.
A backdoor attack implants a backdoor trigger into a target model via data
poisoning so as to control the model's predictions at test time. Different from
image classification models on which most of existing backdoor attacks have
been developed and tested, crowd counting models are regression models that
output multi-dimensional density maps, thus requiring different techniques to
manipulate.
</p>
<p>In this paper, we propose two novel Density Manipulation Backdoor Attacks
(DMBA$^{-}$ and DMBA$^{+}$) to attack the model to produce arbitrarily large or
small density estimations. Experimental results demonstrate the effectiveness
of our DMBA attacks on five classic crowd counting models and four types of
datasets. We also provide an in-depth analysis of the unique challenges of
backdooring crowd counting models and reveal two key elements of effective
attacks: 1) full and dense triggers and 2) manipulation of the ground truth
counts or density maps. Our work could help evaluate the vulnerability of crowd
counting models to potential backdoor attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Docent: A content-based recommendation system to discover contemporary art. (arXiv:2207.05648v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05648">
<div class="article-summary-box-inner">
<span><p>Recommendation systems have been widely used in various domains such as
music, films, e-shopping etc. After mostly avoiding digitization, the art world
has recently reached a technological turning point due to the pandemic, making
online sales grow significantly as well as providing quantitative online data
about artists and artworks. In this work, we present a content-based
recommendation system on contemporary art relying on images of artworks and
contextual metadata of artists. We gathered and annotated artworks with
advanced and art-specific information to create a completely unique database
that was used to train our models. With this information, we built a proximity
graph between artworks. Similarly, we used NLP techniques to characterize the
practices of the artists and we extracted information from exhibitions and
other event history to create a proximity graph between artists. The power of
graph analysis enables us to provide an artwork recommendation system based on
a combination of visual and contextual information from artworks and artists.
After an assessment by a team of art specialists, we get an average final
rating of 75% of meaningful artworks when compared to their professional
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Gradient Reactivation for Backward Compatible Person Re-identification. (arXiv:2207.05658v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05658">
<div class="article-summary-box-inner">
<span><p>We study the backward compatible problem for person re-identification
(Re-ID), which aims to constrain the features of an updated new model to be
comparable with the existing features from the old model in galleries. Most of
the existing works adopt distillation-based methods, which focus on pushing new
features to imitate the distribution of the old ones. However, the
distillation-based methods are intrinsically sub-optimal since it forces the
new feature space to imitate the inferior old feature space. To address this
issue, we propose the Ranking-based Backward Compatible Learning (RBCL), which
directly optimizes the ranking metric between new features and old features.
Different from previous methods, RBCL only pushes the new features to find
best-ranking positions in the old feature space instead of strictly alignment,
and is in line with the ultimate goal of backward retrieval. However, the sharp
sigmoid function used to make the ranking metric differentiable also incurs the
gradient vanish issue, therefore stems the ranking refinement during the later
period of training. To address this issue, we propose the Dynamic Gradient
Reactivation (DGR), which can reactivate the suppressed gradients by adding
dynamic computed constant during forward step. To further help targeting the
best-ranking positions, we include the Neighbor Context Agents (NCAs) to
approximate the entire old feature space during training. Unlike previous works
which only test on the in-domain settings, we make the first attempt to
introduce the cross-domain settings (including both supervised and
unsupervised), which are more meaningful and difficult. The experimental
results on all five settings show that the proposed RBCL outperforms previous
state-of-the-art methods by large margins under all settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RE-Tagger: A light-weight Real-Estate Image Classifier. (arXiv:2207.05696v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05696">
<div class="article-summary-box-inner">
<span><p>Real-estate image tagging is one of the essential use-cases to save efforts
involved in manual annotation and enhance the user experience. This paper
proposes an end-to-end pipeline (referred to as RE-Tagger) for the real-estate
image classification problem. We present a two-stage transfer learning approach
using custom InceptionV3 architecture to classify images into different
categories (i.e., bedroom, bathroom, kitchen, balcony, hall, and others).
Finally, we released the application as REST API hosted as a web application
running on 2 cores machine with 2 GB RAM. The demo video is available here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tell Me the Evidence? Dual Visual-Linguistic Interaction for Answer Grounding. (arXiv:2207.05703v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05703">
<div class="article-summary-box-inner">
<span><p>Answer grounding aims to reveal the visual evidence for visual question
answering (VQA), which entails highlighting relevant positions in the image
when answering questions about images. Previous attempts typically tackle this
problem using pretrained object detectors, but without the flexibility for
objects not in the predefined vocabulary. However, these black-box methods
solely concentrate on the linguistic generation, ignoring the visual
interpretability. In this paper, we propose Dual Visual-Linguistic Interaction
(DaVI), a novel unified end-to-end framework with the capability for both
linguistic answering and visual grounding. DaVI innovatively introduces two
visual-linguistic interaction mechanisms: 1) visual-based linguistic encoder
that understands questions incorporated with visual features and produces
linguistic-oriented evidence for further answer decoding, and 2)
linguistic-based visual decoder that focuses visual features on the
evidence-related regions for answer grounding. This way, our approach ranked
the 1st place in the answer grounding track of 2022 VizWiz Grand Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M-FUSE: Multi-frame Fusion for Scene Flow Estimation. (arXiv:2207.05704v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05704">
<div class="article-summary-box-inner">
<span><p>Recently, neural network for scene flow estimation show impressive results on
automotive data such as the KITTI benchmark. However, despite of using
sophisticated rigidity assumptions and parametrizations, such networks are
typically limited to only two frame pairs which does not allow them to exploit
temporal information. In our paper we address this shortcoming by proposing a
novel multi-frame approach that considers an additional preceding stereo pair.
To this end, we proceed in two steps: Firstly, building upon the recent RAFT-3D
approach, we develop an advanced two-frame baseline by incorporating an
improved stereo method. Secondly, and even more importantly, exploiting the
specific modeling concepts of RAFT-3D, we propose a U-Net like architecture
that performs a fusion of forward and backward flow estimates and hence allows
to integrate temporal information on demand. Experiments on the KITTI benchmark
do not only show that the advantages of the improved baseline and the temporal
fusion approach complement each other, they also demonstrate that the computed
scene flow is highly accurate. More precisely, our approach ranks second
overall and first for the even more challenging foreground objects, in total
outperforming the original RAFT-3D method by more than 16%. Code is available
at https://github.com/cv-stuttgart/M-FUSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Experimental Design for Computed Tomography with the Linearised Deep Image Prior. (arXiv:2207.05714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05714">
<div class="article-summary-box-inner">
<span><p>We investigate adaptive design based on a single sparse pilot scan for
generating effective scanning strategies for computed tomography
reconstruction. We propose a novel approach using the linearised deep image
prior. It allows incorporating information from the pilot measurements into the
angle selection criteria, while maintaining the tractability of a conjugate
Gaussian-linear model. On a synthetically generated dataset with preferential
directions, linearised DIP design allows reducing the number of scans by up to
30% relative to an equidistant angle baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Fairness of Visual Attribute Predictors. (arXiv:2207.05727v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05727">
<div class="article-summary-box-inner">
<span><p>The performance of deep neural networks for image recognition tasks such as
predicting a smiling face is known to degrade with under-represented classes of
sensitive attributes. We address this problem by introducing fairness-aware
regularization losses based on batch estimates of Demographic Parity, Equalized
Odds, and a novel Intersection-over-Union measure. The experiments performed on
facial and medical images from CelebA, UTKFace, and the SIIM-ISIC melanoma
classification challenge show the effectiveness of our proposed fairness losses
for bias mitigation as they improve model fairness while maintaining high
classification performance. To the best of our knowledge, our work is the first
attempt to incorporate these types of losses in an end-to-end training scheme
for mitigating biases of visual attribute predictors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physical Passive Patch Adversarial Attacks on Visual Odometry Systems. (arXiv:2207.05729v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05729">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are known to be susceptible to adversarial perturbations
-- small perturbations that alter the output of the network and exist under
strict norm limitations. While such perturbations are usually discussed as
tailored to a specific input, a universal perturbation can be constructed to
alter the model's output on a set of inputs. Universal perturbations present a
more realistic case of adversarial attacks, as awareness of the model's exact
input is not required. In addition, the universal attack setting raises the
subject of generalization to unseen data, where given a set of inputs, the
universal perturbations aim to alter the model's output on out-of-sample data.
In this work, we study physical passive patch adversarial attacks on visual
odometry-based autonomous navigation systems. A visual odometry system aims to
infer the relative camera motion between two corresponding viewpoints, and is
frequently used by vision-based autonomous navigation systems to estimate their
state. For such navigation systems, a patch adversarial perturbation poses a
severe security issue, as it can be used to mislead a system onto some
collision course. To the best of our knowledge, we show for the first time that
the error margin of a visual odometry model can be significantly increased by
deploying patch adversarial attacks in the scene. We provide evaluation on
synthetic closed-loop drone navigation data and demonstrate that a comparable
vulnerability exists in real data. A reference implementation of the proposed
method and the reported experiments is provided at
https://github.com/patchadversarialattacks/patchadversarialattacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">1st Place Solution to the EPIC-Kitchens Action Anticipation Challenge 2022. (arXiv:2207.05730v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05730">
<div class="article-summary-box-inner">
<span><p>In this report, we describe the technical details of our submission to the
EPIC-Kitchens Action Anticipation Challenge 2022. In this competition, we
develop the following two approaches. 1) Anticipation Time Knowledge
Distillation using the soft labels learned by the teacher model as knowledge to
guide the student network to learn the information of anticipation time; 2)
Verb-Noun Relation Module for building the relationship between verbs and
nouns. Our method achieves state-of-the-art results on the testing set of
EPIC-Kitchens Action Anticipation Challenge 2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Skeleton-aware Graph Convolutional Network for Human-Object Interaction Detection. (arXiv:2207.05733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05733">
<div class="article-summary-box-inner">
<span><p>Detecting human-object interactions is essential for comprehensive
understanding of visual scenes. In particular, spatial connections between
humans and objects are important cues for reasoning interactions. To this end,
we propose a skeleton-aware graph convolutional network for human-object
interaction detection, named SGCN4HOI. Our network exploits the spatial
connections between human keypoints and object keypoints to capture their
fine-grained structural interactions via graph convolutions. It fuses such
geometric features with visual features and spatial configuration features
obtained from human-object pairs. Furthermore, to better preserve the object
structural information and facilitate human-object interaction detection, we
propose a novel skeleton-based object keypoints representation. The performance
of SGCN4HOI is evaluated in the public benchmark V-COCO dataset. Experimental
results show that the proposed approach outperforms the state-of-the-art
pose-based models and achieves competitive performance against other models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer for NeRF-Based View Synthesis from a Single Input Image. (arXiv:2207.05736v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05736">
<div class="article-summary-box-inner">
<span><p>Although neural radiance fields (NeRF) have shown impressive advances for
novel view synthesis, most methods typically require multiple input images of
the same scene with accurate camera poses. In this work, we seek to
substantially reduce the inputs to a single unposed image. Existing approaches
condition on local image features to reconstruct a 3D object, but often render
blurry predictions at viewpoints that are far away from the source view. To
address this issue, we propose to leverage both the global and local features
to form an expressive 3D representation. The global features are learned from a
vision transformer, while the local features are extracted from a 2D
convolutional network. To synthesize a novel view, we train a multilayer
perceptron (MLP) network conditioned on the learned 3D representation to
perform volume rendering. This novel 3D representation allows the network to
reconstruct unseen regions without enforcing constraints like symmetry or
canonical coordinate systems. Our method can render novel views from only a
single input image and generalize across multiple object categories using a
single model. Quantitative and qualitative evaluations demonstrate that the
proposed method achieves state-of-the-art performance and renders richer
details than existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-structure bone segmentation in pediatric MR images with combined regularization from shape priors and adversarial network. (arXiv:2009.07092v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.07092">
<div class="article-summary-box-inner">
<span><p>Morphological and diagnostic evaluation of pediatric musculoskeletal system
is crucial in clinical practice. However, most segmentation models do not
perform well on scarce pediatric imaging data. We propose a new pre-trained
regularized convolutional encoder-decoder network for the challenging task of
segmenting heterogeneous pediatric magnetic resonance (MR) images. To this end,
we have conceived a novel optimization scheme for the segmentation network
which comprises additional regularization terms to the loss function. In order
to obtain globally consistent predictions, we incorporate a shape priors based
regularization, derived from a non-linear shape representation learnt by an
auto-encoder. Additionally, an adversarial regularization computed by a
discriminator is integrated to encourage precise delineations. The proposed
method is evaluated for the task of multi-bone segmentation on two scarce
pediatric imaging datasets from ankle and shoulder joints, comprising
pathological as well as healthy examinations. The proposed method performed
either better or at par with previously proposed approaches for Dice,
sensitivity, specificity, maximum symmetric surface distance, average symmetric
surface distance, and relative absolute volume difference metrics. We
illustrate that the proposed approach can be easily integrated into various
bone segmentation strategies and can improve the prediction accuracy of models
pre-trained on large non-medical images databases. The obtained results bring
new perspectives for the management of pediatric musculoskeletal disorders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biometrics in the Era of COVID-19: Challenges and Opportunities. (arXiv:2102.09258v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09258">
<div class="article-summary-box-inner">
<span><p>Since early 2020 the COVID-19 pandemic has had a considerable impact on many
aspects of daily life. A range of different measures have been implemented
worldwide to reduce the rate of new infections and to manage the pressure on
national health services. A primary strategy has been to reduce gatherings and
the potential for transmission through the prioritisation of remote working and
education. Enhanced hand hygiene and the use of facial masks have decreased the
spread of pathogens when gatherings are unavoidable. These particular measures
present challenges for reliable biometric recognition, e.g. for facial-, voice-
and hand-based biometrics. At the same time, new challenges create new
opportunities and research directions, e.g. renewed interest in non-constrained
iris or periocular recognition, touch-less fingerprint- and vein-based
authentication and the use of biometric characteristics for disease detection.
This article presents an overview of the research carried out to address those
challenges and emerging opportunities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Classification Network. (arXiv:2103.10994v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10994">
<div class="article-summary-box-inner">
<span><p>We present Self-Classifier -- a novel self-supervised end-to-end
classification learning approach. Self-Classifier learns labels and
representations simultaneously in a single-stage end-to-end manner by
optimizing for same-class prediction of two augmented views of the same sample.
To guarantee non-degenerate solutions (i.e., solutions where all labels are
assigned to the same class) we propose a mathematically motivated variant of
the cross-entropy loss that has a uniform prior asserted on the predicted
labels. In our theoretical analysis, we prove that degenerate solutions are not
in the set of optimal solutions of our approach. Self-Classifier is simple to
implement and scalable. Unlike other popular unsupervised classification and
contrastive representation learning approaches, it does not require any form of
pre-training, expectation-maximization, pseudo-labeling, external clustering, a
second network, stop-gradient operation, or negative pairs. Despite its
simplicity, our approach sets a new state of the art for unsupervised
classification of ImageNet; and even achieves comparable to state-of-the-art
results for unsupervised representation learning. Code is available at
https://github.com/elad-amrani/self-classifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-supervised Part-Attention and Mentored Networks for Vehicle Re-Identification. (arXiv:2107.08228v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08228">
<div class="article-summary-box-inner">
<span><p>Vehicle re-identification (Re-ID) aims to retrieve images with the same
vehicle ID across different cameras. Current part-level feature learning
methods typically detect vehicle parts via uniform division, outside tools, or
attention modeling. However, such part features often require expensive
additional annotations and cause sub-optimal performance in case of unreliable
part mask predictions. In this paper, we propose a weakly-supervised
Part-Attention Network (PANet) and Part-Mentored Network (PMNet) for Vehicle
Re-ID. Firstly, PANet localizes vehicle parts via part-relevant channel
recalibration and cluster-based mask generation without vehicle part
supervisory information. Secondly, PMNet leverages teacher-student guided
learning to distill vehicle part-specific features from PANet and performs
multi-scale global-part feature extraction. During inference, PMNet can
adaptively extract discriminative part features without part localization by
PANet, preventing unstable part mask predictions. We address this Re-ID issue
as a multi-task problem and adopt Homoscedastic Uncertainty to learn the
optimal weighing of ID losses. Experiments are conducted on two public
benchmarks, showing that our approach outperforms recent methods, which require
no extra annotations by an average increase of 3.0% in CMC@5 on VehicleID and
over 1.4% in mAP on VeRi776. Moreover, our method can extend to the occluded
vehicle Re-ID task and exhibits good generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Video Compression using GANs for Detail Synthesis and Propagation. (arXiv:2107.12038v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12038">
<div class="article-summary-box-inner">
<span><p>We present the first neural video compression method based on generative
adversarial networks (GANs). Our approach significantly outperforms previous
neural and non-neural video compression methods in a user study, setting a new
state-of-the-art in visual quality for neural methods. We show that the GAN
loss is crucial to obtain this high visual quality. Two components make the GAN
loss effective: we i) synthesize detail by conditioning the generator on a
latent extracted from the warped previous reconstruction to then ii) propagate
this detail with high-quality flow. We find that user studies are required to
compare methods, i.e., none of our quantitative metrics were able to predict
all studies. We present the network design choices in detail, and ablate them
with user studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RigNet: Repetitive Image Guided Network for Depth Completion. (arXiv:2107.13802v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13802">
<div class="article-summary-box-inner">
<span><p>Depth completion deals with the problem of recovering dense depth maps from
sparse ones, where color images are often used to facilitate this task. Recent
approaches mainly focus on image guided learning frameworks to predict dense
depth. However, blurry guidance in the image and unclear structure in the depth
still impede the performance of the image guided frameworks. To tackle these
problems, we explore a repetitive design in our image guided network to
gradually and sufficiently recover depth values. Specifically, the repetition
is embodied in both the image guidance branch and depth generation branch. In
the former branch, we design a repetitive hourglass network to extract
discriminative image features of complex environments, which can provide
powerful contextual instruction for depth prediction. In the latter branch, we
introduce a repetitive guidance module based on dynamic convolution, in which
an efficient convolution factorization is proposed to simultaneously reduce its
complexity and progressively model high-frequency structures. Extensive
experiments show that our method achieves superior or competitive results on
KITTI benchmark and NYUv2 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Similarity Encoder for Deep GAN Inversion. (arXiv:2108.10201v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10201">
<div class="article-summary-box-inner">
<span><p>Current deep generative adversarial networks (GANs) can synthesize
high-quality (HQ) images, so learning representation with GANs is favorable.
GAN inversion is one of emerging approaches that study how to invert images
into latent space. Existing GAN encoders can invert images on StyleGAN, but
cannot adapt to other deep GANs. We propose a novel approach to address this
issue. By evaluating diverse similarity in latent vectors and images, we design
an adaptive encoder, named diverse similarity encoder (DSE), that can be
expanded to a variety of state-of-the-art GANs. DSE makes GANs reconstruct
higher fidelity images from HQ images, no matter whether they are synthesized
or real images. DSE has unified convolutional blocks and adapts well to
mainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cell Multi-Bernoulli (Cell-MB) Sensor Control for Multi-object Search-While-Tracking (SWT). (arXiv:2108.11236v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11236">
<div class="article-summary-box-inner">
<span><p>Information-driven control can be used to develop intelligent sensors that
can optimize their measurement value based on environmental feedback. In object
tracking applications, sensor actions are chosen based on the expected
reduction in uncertainty also known as information gain. Random finite set
(RFS) theory provides a formalism for quantifying and estimating information
gain in multi-object tracking problems. However, estimating information gain in
these applications remains computationally challenging. This paper presents a
new tractable approximation of the RFS expected information gain applicable to
sensor control for multi-object search and tracking. Unlike existing RFS
approaches, the information gain approximation presented in this paper
considers the contributions of non-ideal noisy measurements, missed detections,
false alarms, and object appearance/disappearance. The effectiveness of the
information-driven sensor control is demonstrated through two multi-vehicle
search-while-tracking experiments using real video data from remote terrestrial
and satellite sensors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PC$^2$-PU: Patch Correlation and Point Correlation for Effective Point Cloud Upsampling. (arXiv:2109.09337v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09337">
<div class="article-summary-box-inner">
<span><p>Point cloud upsampling is to densify a sparse point set acquired from 3D
sensors, providing a denser representation for the underlying surface. Existing
methods divide the input points into small patches and upsample each patch
separately, however, ignoring the global spatial consistency between patches.
In this paper, we present a novel method PC$^2$-PU, which explores
patch-to-patch and point-to-point correlations for more effective and robust
point cloud upsampling. Specifically, our network has two appealing designs:
(i) We take adjacent patches as supplementary inputs to compensate the loss
structure information within a single patch and introduce a Patch Correlation
Module to capture the difference and similarity between patches. (ii) After
augmenting each patch's geometry, we further introduce a Point Correlation
Module to reveal the relationship of points inside each patch to maintain the
local spatial consistency. Extensive experiments on both synthetic and real
scanned datasets demonstrate that our method surpasses previous upsampling
methods, particularly with the noisy inputs. The code and data are at
\url{https://github.com/chenlongwhu/PC2-PU.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Propagating State Uncertainty Through Trajectory Forecasting. (arXiv:2110.03267v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03267">
<div class="article-summary-box-inner">
<span><p>Uncertainty pervades through the modern robotic autonomy stack, with nearly
every component (e.g., sensors, detection, classification, tracking, behavior
prediction) producing continuous or discrete probabilistic distributions.
Trajectory forecasting, in particular, is surrounded by uncertainty as its
inputs are produced by (noisy) upstream perception and its outputs are
predictions that are often probabilistic for use in downstream planning.
However, most trajectory forecasting methods do not account for upstream
uncertainty, instead taking only the most-likely values. As a result,
perceptual uncertainties are not propagated through forecasting and predictions
are frequently overconfident. To address this, we present a novel method for
incorporating perceptual state uncertainty in trajectory forecasting, a key
component of which is a new statistical distance-based loss function which
encourages predicting uncertainties that better match upstream perception. We
evaluate our approach both in illustrative simulations and on large-scale,
real-world data, demonstrating its efficacy in propagating perceptual state
uncertainty through prediction and producing more calibrated predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MM-Pyramid: Multimodal Pyramid Attentional Network for Audio-Visual Event Localization and Video Parsing. (arXiv:2111.12374v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12374">
<div class="article-summary-box-inner">
<span><p>Recognizing and localizing events in videos is a fundamental task for video
understanding. Since events may occur in auditory and visual modalities,
multimodal detailed perception is essential for complete scene comprehension.
Most previous works attempted to analyze videos from a holistic perspective.
However, they do not consider semantic information at multiple scales, which
makes the model difficult to localize events in different lengths. In this
paper, we present a Multimodal Pyramid Attentional Network
(\textbf{MM-Pyramid}) for event localization. Specifically, we first propose
the attentive feature pyramid module. This module captures temporal pyramid
features via several stacking pyramid units, each of them is composed of a
fixed-size attention block and dilated convolution block. We also design an
adaptive semantic fusion module, which leverages a unit-level attention block
and a selective fusion block to integrate pyramid features interactively.
Extensive experiments on audio-visual event localization and weakly-supervised
audio-visual video parsing tasks verify the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">diffConv: Analyzing Irregular Point Clouds with an Irregular View. (arXiv:2111.14658v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14658">
<div class="article-summary-box-inner">
<span><p>Standard spatial convolutions assume input data with a regular neighborhood
structure. Existing methods typically generalize convolution to the irregular
point cloud domain by fixing a regular "view" through e.g. a fixed neighborhood
size, where the convolution kernel size remains the same for each point.
However, since point clouds are not as structured as images, the fixed neighbor
number gives an unfortunate inductive bias. We present a novel graph
convolution named Difference Graph Convolution (diffConv), which does not rely
on a regular view. diffConv operates on spatially-varying and density-dilated
neighborhoods, which are further adapted by a learned masked attention
mechanism. Experiments show that our model is very robust to the noise,
obtaining state-of-the-art performance in 3D shape classification and scene
understanding tasks, along with a faster inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EAGAN: Efficient Two-stage Evolutionary Architecture Search for GANs. (arXiv:2111.15097v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15097">
<div class="article-summary-box-inner">
<span><p>Generative adversarial networks (GANs) have proven successful in image
generation tasks. However, GAN training is inherently unstable. Although many
works try to stabilize it by manually modifying GAN architecture, it requires
much expertise. Neural architecture search (NAS) has become an attractive
solution to search GANs automatically. The early NAS-GANs search only
generators to reduce search complexity but lead to a sub-optimal GAN. Some
recent works try to search both generator (G) and discriminator (D), but they
suffer from the instability of GAN training. To alleviate the instability, we
propose an efficient two-stage evolutionary algorithm-based NAS framework to
search GANs, namely EAGAN. We decouple the search of G and D into two stages,
where stage-1 searches G with a fixed D and adopts the many-to-one training
strategy, and stage-2 searches D with the optimal G found in stage-1 and adopts
the one-to-one training and weight-resetting strategies to enhance the
stability of GAN training. Both stages use the non-dominated sorting method to
produce Pareto-front architectures under multiple objectives (e.g., model size,
Inception Score (IS), and Fr\'echet Inception Distance (FID)). EAGAN is applied
to the unconditional image generation task and can efficiently finish the
search on the CIFAR-10 dataset in 1.2 GPU days. Our searched GANs achieve
competitive results (IS=8.81$\pm$0.10, FID=9.91) on the CIFAR-10 dataset and
surpass prior NAS-GANs on the STL-10 dataset (IS=10.44$\pm$0.087, FID=22.18).
Source code: https://github.com/marsggbo/EAGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Channel Encoding Transformer for Point Cloud Analysis. (arXiv:2112.02507v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02507">
<div class="article-summary-box-inner">
<span><p>Transformer plays an increasingly important role in various computer vision
areas and remarkable achievements have also been made in point cloud analysis.
Since they mainly focus on point-wise transformer, an adaptive channel encoding
transformer is proposed in this paper. Specifically, a channel convolution
called Transformer-Conv is designed to encode the channel. It can encode
feature channels by capturing the potential relationship between coordinates
and features. Compared with simply assigning attention weight to each channel,
our method aims to encode the channel adaptively. In addition, our network
adopts the neighborhood search method of low-level and high-level dual semantic
receptive fields to improve the performance. Extensive experiments show that
our method is superior to state-of-the-art point cloud classification and
segmentation methods on three benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Image Restoration by Revisiting Global Information Aggregation. (arXiv:2112.04491v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04491">
<div class="article-summary-box-inner">
<span><p>Global operations, such as global average pooling, are widely used in
top-performance image restorers. They aggregate global information from input
features along entire spatial dimensions but behave differently during training
and inference in image restoration tasks: they are based on different regions,
namely the cropped patches (from images) and the full-resolution images. This
paper revisits global information aggregation and finds that the image-based
features during inference have a different distribution than the patch-based
features during training. This train-test inconsistency negatively impacts the
performance of models, which is severely overlooked by previous works. To
reduce the inconsistency and improve test-time performance, we propose a simple
method called Test-time Local Converter (TLC). Our TLC converts global
operations to local ones only during inference so that they aggregate features
within local spatial regions rather than the entire large images. The proposed
method can be applied to various global modules (e.g., normalization, channel
and spatial attention) with negligible costs. Without the need for any
fine-tuning, TLC improves state-of-the-art results on several image restoration
tasks, including single-image motion deblurring, video deblurring, defocus
deblurring, and image denoising. In particular, with TLC, our Restormer-Local
improves the state-of-the-art result in single image deblurring from 32.92 dB
to 33.57 dB on GoPro dataset. The code is available at
https://github.com/megvii-research/tlc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer Vision. (arXiv:2112.09290v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09290">
<div class="article-summary-box-inner">
<span><p>In recent years, person detection and human pose estimation have made great
strides, helped by large-scale labeled datasets. However, these datasets had no
guarantees or analysis of human activities, poses, or context diversity.
Additionally, privacy, legal, safety, and ethical concerns may limit the
ability to collect more human data. An emerging alternative to real-world data
that alleviates some of these issues is synthetic data. However, creation of
synthetic data generators is incredibly challenging and prevents researchers
from exploring their usefulness. Therefore, we release a human-centric
synthetic data generator PeopleSansPeople which contains simulation-ready 3D
human assets, a parameterized lighting and camera system, and generates 2D and
3D bounding box, instance and semantic segmentation, and COCO pose labels.
Using PeopleSansPeople, we performed benchmark synthetic data training using a
Detectron2 Keypoint R-CNN variant [1]. We found that pre-training a network
using synthetic data and fine-tuning on various sizes of real-world data
resulted in a keypoint AP increase of $+38.03$ ($44.43 \pm 0.17$ vs. $6.40$)
for few-shot transfer (limited subsets of COCO-person train [2]), and an
increase of $+1.47$ ($63.47 \pm 0.19$ vs. $62.00$) for abundant real data
regimes, outperforming models trained with the same real data alone. We also
found that our models outperformed those pre-trained with ImageNet with a
keypoint AP increase of $+22.53$ ($44.43 \pm 0.17$ vs. $21.90$) for few-shot
transfer and $+1.07$ ($63.47 \pm 0.19$ vs. $62.40$) for abundant real data
regimes. This freely-available data generator should enable a wide range of
research into the emerging field of simulation to real transfer learning in the
critical area of human-centric computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis. (arXiv:2112.10325v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10325">
<div class="article-summary-box-inner">
<span><p>Due to the constraints of the imaging device and high cost in operation time,
computer tomography (CT) scans are usually acquired with low intra-slice
resolution. Improving the intra-slice resolution is beneficial to the disease
diagnosis for both human experts and computer-aided systems. To this end, this
paper builds a novel medical slice synthesis to increase the between-slice
resolution. Considering that the ground-truth intermediate medical slices are
always absent in clinical practice, we introduce the incremental cross-view
mutual distillation strategy to accomplish this task in the self-supervised
learning manner. Specifically, we model this problem from three different
views: slice-wise interpolation from axial view and pixel-wise interpolation
from coronal and sagittal views. Under this circumstance, the models learned
from different views can distill valuable knowledge to guide the learning
processes of each other. We can repeat this process to make the models
synthesize intermediate slice data with increasing inter-slice resolution. To
demonstrate the effectiveness of the proposed approach, we conduct
comprehensive experiments on a large-scale CT dataset. Quantitative and
qualitative comparison results show that our method outperforms
state-of-the-art algorithms by clear margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial-Sketch Synthesis: A New Challenge. (arXiv:2112.15439v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15439">
<div class="article-summary-box-inner">
<span><p>This paper aims to conduct a comprehensive study on facial-sketch synthesis
(FSS). However, due to the high costs of obtaining hand-drawn sketch datasets,
there lacks a complete benchmark for assessing the development of FSS
algorithms over the last decade. We first introduce a high-quality dataset for
FSS, named FS2K, which consists of 2,104 image-sketch pairs spanning three
types of sketch styles, image backgrounds, lighting conditions, skin colors,
and facial attributes. FS2K differs from previous FSS datasets in difficulty,
diversity, and scalability and should thus facilitate the progress of FSS
research. Second, we present the largest-scale FSS investigation by reviewing
89 classical methods, including 25 handcrafted feature-based facial-sketch
synthesis approaches, 29 general translation methods, and 35 image-to-sketch
approaches. Besides, we elaborate comprehensive experiments on the existing 19
cutting-edge models. Third, we present a simple baseline for FSS, named FSGAN.
With only two straightforward components, i.e., facial-aware masking and
style-vector expansion, FSGAN surpasses the performance of all previous
state-of-the-art models on the proposed FS2K dataset by a large margin.
Finally, we conclude with lessons learned over the past years and point out
several unsolved challenges. Our code is available at
https://github.com/DengPingFan/FSGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persistent Homology for Breast Tumor Classification using Mammogram Scans. (arXiv:2201.02295v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02295">
<div class="article-summary-box-inner">
<span><p>An Important tool in the field topological data analysis is known as
persistent Homology (PH) which is used to encode abstract representation of the
homology of data at different resolutions in the form of persistence diagram
(PD). In this work we build more than one PD representation of a single image
based on a landmark selection method, known as local binary patterns, that
encode different types of local textures from images. We employed different PD
vectorizations using persistence landscapes, persistence images, persistence
binning (Betti Curve) and statistics. We tested the effectiveness of proposed
landmark based PH on two publicly available breast abnormality detection
datasets using mammogram scans. Sensitivity of landmark based PH obtained is
over 90% in both datasets for the detection of abnormal breast scans. Finally,
experimental results give new insights on using different types of PD
vectorizations which help in utilising PH in conjunction with machine learning
classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Label Assignment for Object Detection by Combining Predicted IoUs and Anchor IoUs. (arXiv:2201.09396v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09396">
<div class="article-summary-box-inner">
<span><p>Label assignment plays a significant role in modern object detection models.
Detection models may yield totally different performances with different label
assignment strategies. For anchor-based detection models, the IoU (Intersection
over Union) threshold between the anchors and their corresponding ground truth
bounding boxes is the key element since the positive samples and negative
samples are divided by the IoU threshold. Early object detectors simply utilize
the fixed threshold for all training samples, while recent detection algorithms
focus on adaptive thresholds based on the distribution of the IoUs to the
ground truth boxes. In this paper, we introduce a simple while effective
approach to perform label assignment dynamically based on the training status
with predictions. By introducing the predictions in label assignment, more
high-quality samples with higher IoUs to the ground truth objects are selected
as the positive samples, which could reduce the discrepancy between the
classification scores and the IoU scores, and generate more high-quality
boundary boxes. Our approach shows improvements in the performance of the
detection models with the adaptive label assignment algorithm and lower
bounding box losses for those positive samples, indicating more samples with
higher-quality predicted boxes are selected as positives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Global Ranking-Aware Neural Architecture Ranker for Efficient Image Classifier Search. (arXiv:2201.12725v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12725">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) is a powerful tool for automating effective
image processing DNN designing. The ranking has been advocated to design an
efficient performance predictor for NAS. The previous contrastive method solves
the ranking problem by comparing pairs of architectures and predicting their
relative performance. However, it only focuses on the rankings between two
involved architectures and neglects the overall quality distributions of the
search space, which may suffer generalization issues. A predictor, namely
Neural Architecture Ranker (NAR) which concentrates on the global quality tier
of specific architecture, is proposed to tackle such problems caused by the
local perspective. The NAR explores the quality tiers of the search space
globally and classifies each individual to the tier they belong to according to
its global ranking. Thus, the predictor gains the knowledge of the performance
distributions of the search space which helps to generalize its ranking ability
to the datasets more easily. Meanwhile, the global quality distribution
facilitates the search phase by directly sampling candidates according to the
statistics of quality tiers, which is free of training a search algorithm,
e.g., Reinforcement Learning (RL) or Evolutionary Algorithm (EA), thus it
simplifies the NAS pipeline and saves the computational overheads. The proposed
NAR achieves better performance than the state-of-the-art methods on two widely
used datasets for NAS research. On the vast search space of NAS-Bench-101, the
NAR easily finds the architecture with top 0.01$\unicode{x2030}$ performance
only by sampling. It also generalizes well to different image datasets of
NAS-Bench-201, i.e., CIFAR-10, CIFAR-100, and ImageNet-16-120 by identifying
the optimal architectures for each of them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CGiS-Net: Aggregating Colour, Geometry and Implicit Semantic Features for Indoor Place Recognition. (arXiv:2202.02070v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02070">
<div class="article-summary-box-inner">
<span><p>We describe a novel approach to indoor place recognition from RGB point
clouds based on aggregating low-level colour and geometry features with
high-level implicit semantic features. It uses a 2-stage deep learning
framework, in which the first stage is trained for the auxiliary task of
semantic segmentation and the second stage uses features from layers in the
first stage to generate discriminate descriptors for place recognition. The
auxiliary task encourages the features to be semantically meaningful, hence
aggregating the geometry and colour in the RGB point cloud data with implicit
semantic information. We use an indoor place recognition dataset derived from
the ScanNet dataset for training and evaluation, with a test set comprising
3,608 point clouds generated from 100 different rooms. Comparison with a
traditional feature-based method and four state-of-the-art deep learning
methods demonstrate that our approach significantly outperforms all five
methods, achieving, for example, a top-3 average recall rate of 75% compared
with 41% for the closest rival method. Our code is available at:
https://github.com/YuhangMing/Semantic-Indoor-Place-Recognition
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equivariance versus Augmentation for Spherical Images. (arXiv:2202.03990v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03990">
<div class="article-summary-box-inner">
<span><p>We analyze the role of rotational equivariance in convolutional neural
networks (CNNs) applied to spherical images. We compare the performance of the
group equivariant networks known as S2CNNs and standard non-equivariant CNNs
trained with an increasing amount of data augmentation. The chosen
architectures can be considered baseline references for the respective design
paradigms. Our models are trained and evaluated on single or multiple items
from the MNIST or FashionMNIST dataset projected onto the sphere. For the task
of image classification, which is inherently rotationally invariant, we find
that by considerably increasing the amount of data augmentation and the size of
the networks, it is possible for the standard CNNs to reach at least the same
performance as the equivariant network. In contrast, for the inherently
equivariant task of semantic segmentation, the non-equivariant networks are
consistently outperformed by the equivariant networks with significantly fewer
parameters. We also analyze and compare the inference latency and training
times of the different networks, enabling detailed tradeoff considerations
between equivariant architectures and data augmentation for practical problems.
The equivariant spherical networks used in the experiments are available at
https://github.com/JanEGerken/sem_seg_s2cnn .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Metric Learning-Based Semi-Supervised Regression With Alternate Learning. (arXiv:2202.11388v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11388">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel deep metric learning-based semi-supervised
regression (DML-S2R) method for parameter estimation problems. The proposed
DML-S2R method aims to mitigate the problems of insufficient amount of labeled
samples without collecting any additional sample with a target value. To this
end, it is made up of two main steps: i) pairwise similarity modeling with
scarce labeled data; and ii) triplet-based metric learning with abundant
unlabeled data. The first step aims to model pairwise sample similarities by
using a small number of labeled samples. This is achieved by estimating the
target value differences of labeled samples with a Siamese neural network
(SNN). The second step aims to learn a triplet-based metric space (in which
similar samples are close to each other and dissimilar samples are far apart
from each other) when the number of labeled samples is insufficient. This is
achieved by employing the SNN of the first step for triplet-based deep metric
learning that exploits not only labeled samples but also unlabeled samples. For
the end-to-end training of DML-S2R, we investigate an alternate learning
strategy for the two steps. Due to this strategy, the encoded information in
each step becomes a guidance for learning phase of the other step. The
experimental results confirm the success of DML-S2R compared to the
state-of-the-art semi-supervised regression methods. The code of the proposed
method is publicly available at https://git.tu-berlin.de/rsim/DML-S2R.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Self-Supervised Cross-Modal Image Retrieval Method In Remote Sensing. (arXiv:2202.11429v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11429">
<div class="article-summary-box-inner">
<span><p>Due to the availability of multi-modal remote sensing (RS) image archives,
one of the most important research topics is the development of cross-modal RS
image retrieval (CM-RSIR) methods that search semantically similar images
across different modalities. Existing CM-RSIR methods require the availability
of a high quality and quantity of annotated training images. The collection of
a sufficient number of reliable labeled images is time consuming, complex and
costly in operational scenarios, and can significantly affect the final
accuracy of CM-RSIR. In this paper, we introduce a novel self-supervised
CM-RSIR method that aims to: i) model mutual-information between different
modalities in a self-supervised manner; ii) retain the distributions of
modal-specific feature spaces similar to each other; and iii) define the most
similar images within each modality without requiring any annotated training
image. To this end, we propose a novel objective including three loss functions
that simultaneously: i) maximize mutual information of different modalities for
inter-modal similarity preservation; ii) minimize the angular distance of
multi-modal image tuples for the elimination of inter-modal discrepancies; and
iii) increase cosine similarity of the most similar images within each modality
for the characterization of intra-modal similarities. Experimental results show
the effectiveness of the proposed method compared to state-of-the-art methods.
The code of the proposed method is publicly available at
https://git.tu-berlin.de/rsim/SS-CM-RSIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SWIS: Self-Supervised Representation Learning For Writer Independent Offline Signature Verification. (arXiv:2202.13078v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13078">
<div class="article-summary-box-inner">
<span><p>Writer independent offline signature verification is one of the most
challenging tasks in pattern recognition as there is often a scarcity of
training data. To handle such data scarcity problem, in this paper, we propose
a novel self-supervised learning (SSL) framework for writer independent offline
signature verification. To our knowledge, this is the first attempt to utilize
self-supervised setting for the signature verification task. The objective of
self-supervised representation learning from the signature images is achieved
by minimizing the cross-covariance between two random variables belonging to
different feature directions and ensuring a positive cross-covariance between
the random variables denoting the same feature direction. This ensures that the
features are decorrelated linearly and the redundant information is discarded.
Through experimental results on different data sets, we obtained encouraging
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Hard Example Mining Approach for Single Shot Object Detectors. (arXiv:2202.13080v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13080">
<div class="article-summary-box-inner">
<span><p>Hard example mining methods generally improve the performance of the object
detectors, which suffer from imbalanced training sets. In this work, two
existing hard example mining approaches (LRM and focal loss, FL) are adapted
and combined in a state-of-the-art real-time object detector, YOLOv5. The
effectiveness of the proposed approach for improving the performance on hard
examples is extensively evaluated. The proposed method increases mAP by 3%
compared to using the original loss function and around 1-2% compared to using
the hard-mining methods (LRM or FL) individually on 2021 Anti-UAV Challenge
Dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained unsupervised anomaly segmentation. (arXiv:2203.01671v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01671">
<div class="article-summary-box-inner">
<span><p>Current unsupervised anomaly localization approaches rely on generative
models to learn the distribution of normal images, which is later used to
identify potential anomalous regions derived from errors on the reconstructed
images. However, a main limitation of nearly all prior literature is the need
of employing anomalous images to set a class-specific threshold to locate the
anomalies. This limits their usability in realistic scenarios, where only
normal data is typically accessible. Despite this major drawback, only a
handful of works have addressed this limitation, by integrating supervision on
attention maps during training. In this work, we propose a novel formulation
that does not require accessing images with abnormalities to define the
threshold. Furthermore, and in contrast to very recent work, the proposed
constraint is formulated in a more principled manner, leveraging well-known
knowledge in constrained optimization. In particular, the equality constraint
on the attention maps in prior work is replaced by an inequality constraint,
which allows more flexibility. In addition, to address the limitations of
penalty-based functions we employ an extension of the popular log-barrier
methods to handle the constraint. Last, we propose an alternative
regularization term that maximizes the Shannon entropy of the attention maps,
reducing the amount of hyperparameters of the proposed model. Comprehensive
experiments on two publicly available datasets on brain lesion segmentation
demonstrate that the proposed approach substantially outperforms relevant
literature, establishing new state-of-the-art results for unsupervised lesion
segmentation, and without the need to access anomalous images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highly Accurate Dichotomous Image Segmentation. (arXiv:2203.03041v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03041">
<div class="article-summary-box-inner">
<span><p>We present a systematic study on a new task called dichotomous image
segmentation (DIS) , which aims to segment highly accurate objects from natural
images. To this end, we collected the first large-scale DIS dataset, called
DIS5K, which contains 5,470 high-resolution (e.g., 2K, 4K or larger) images
covering camouflaged, salient, or meticulous objects in various backgrounds.
DIS is annotated with extremely fine-grained labels. Besides, we introduce a
simple intermediate supervision baseline (IS-Net) using both feature-level and
mask-level guidance for DIS model training. IS-Net outperforms various
cutting-edge baselines on the proposed DIS5K, making it a general self-learned
supervision network that can facilitate future research in DIS. Further, we
design a new metric called human correction efforts (HCE) which approximates
the number of mouse clicking operations required to correct the false positives
and false negatives. HCE is utilized to measure the gap between models and
real-world applications and thus can complement existing metrics. Finally, we
conduct the largest-scale benchmark, evaluating 16 representative segmentation
models, providing a more insightful discussion regarding object complexities,
and showing several potential applications (e.g., background removal, art
design, 3D reconstruction). Hoping these efforts can open up promising
directions for both academic and industries. Project page:
https://xuebinqin.github.io/dis/index.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CF-ViT: A General Coarse-to-Fine Method for Vision Transformer. (arXiv:2203.03821v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03821">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) have made many breakthroughs in computer vision
tasks. However, considerable redundancy arises in the spatial dimension of an
input image, leading to massive computational costs. Therefore, We propose a
coarse-to-fine vision transformer (CF-ViT) to relieve computational burden
while retaining performance in this paper. Our proposed CF-ViT is motivated by
two important observations in modern ViT models: (1) The coarse-grained patch
splitting can locate informative regions of an input image. (2) Most images can
be well recognized by a ViT model in a small-length token sequence. Therefore,
our CF-ViT implements network inference in a two-stage manner. At coarse
inference stage, an input image is split into a small-length patch sequence for
a computationally economical classification. If not well recognized, the
informative patches are identified and further re-split in a fine-grained
granularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For
example, without any compromise on performance, CF-ViT reduces 53% FLOPs of
LV-ViT, and also achieves 2.01x throughput.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaCC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer. (arXiv:2203.03952v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03952">
<div class="article-summary-box-inner">
<span><p>Recently, vision transformers started to show impressive results which
outperform large convolution based models significantly. However, in the area
of small models for mobile or resource constrained devices, ConvNet still has
its own advantages in both performance and model complexity. We propose
PaCC-Net, a pure ConvNet based backbone model that further strengthens these
advantages by fusing the merits of vision transformers into ConvNets.
Specifically, we propose position aware circular convolution (PaCC), a
light-weight convolution op which boasts a global receptive field while
producing location sensitive features as in local convolutions. We combine the
PaCCs and squeeze-exictation ops to form a meta-former like model block, which
further has the attention mechanism like transformers. The aforementioned block
can be used in plug-and-play manner to replace relevant blocks in ConvNets or
transformers. Experiment results show that the proposed PaCC-Net achieves
better performance than popular light-weight ConvNets and vision transformer
based models in common vision tasks and datasets, while having fewer parameters
and faster inference speed. For classification on ImageNet-1k, PaCC-Net
achieves 78.6% top-1 accuracy with about 5.0 million parameters, saving 11%
parameters and 13% computational cost but gaining 0.2% higher accuracy and 23%
faster inference speed (on ARM based Rockchip RK3288) compared with MobileViT,
and uses only 0.5 times parameters but gaining 2.7% accuracy compared with
DeIT. On MS-COCO object detection and PASCAL VOC segmentation tasks, PaCC-Net
also shows better performance. Source code is available at
https://github.com/hkzhang91/PaCC-Net
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unpaired Deep Image Dehazing Using Contrastive Disentanglement Learning. (arXiv:2203.07677v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07677">
<div class="article-summary-box-inner">
<span><p>We offer a practical unpaired learning based image dehazing network from an
unpaired set of clear and hazy images. This paper provides a new perspective to
treat image dehazing as a two-class separated factor disentanglement task, i.e,
the task-relevant factor of clear image reconstruction and the task-irrelevant
factor of haze-relevant distribution. To achieve the disentanglement of these
two-class factors in deep feature space, contrastive learning is introduced
into a CycleGAN framework to learn disentangled representations by guiding the
generated images to be associated with latent factors. With such formulation,
the proposed contrastive disentangled dehazing method (CDD-GAN) employs
negative generators to cooperate with the encoder network to update
alternately, so as to produce a queue of challenging negative adversaries. Then
these negative adversaries are trained end-to-end together with the backbone
representation network to enhance the discriminative information and promote
factor disentanglement performance by maximizing the adversarial contrastive
loss. During the training, we further show that hard negative examples can
suppress the task-irrelevant factors and unpaired clear exemples can enhance
the task-relevant factors, in order to better facilitate haze removal and help
image restoration. Extensive experiments on both synthetic and real-world
datasets demonstrate that our method performs favorably against existing
unpaired dehazing baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupled Knowledge Distillation. (arXiv:2203.08679v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08679">
<div class="article-summary-box-inner">
<span><p>State-of-the-art distillation methods are mainly based on distilling deep
features from intermediate layers, while the significance of logit distillation
is greatly overlooked. To provide a novel viewpoint to study logit
distillation, we reformulate the classical KD loss into two parts, i.e., target
class knowledge distillation (TCKD) and non-target class knowledge distillation
(NCKD). We empirically investigate and prove the effects of the two parts: TCKD
transfers knowledge concerning the "difficulty" of training samples, while NCKD
is the prominent reason why logit distillation works. More importantly, we
reveal that the classical KD loss is a coupled formulation, which (1)
suppresses the effectiveness of NCKD and (2) limits the flexibility to balance
these two parts. To address these issues, we present Decoupled Knowledge
Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently
and flexibly. Compared with complex feature-based methods, our DKD achieves
comparable or even better results and has better training efficiency on
CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object
detection tasks. This paper proves the great potential of logit distillation,
and we hope it will be helpful for future research. The code is available at
https://github.com/megvii-research/mdistiller.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PanoFormer: Panorama Transformer for Indoor 360 Depth Estimation. (arXiv:2203.09283v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09283">
<div class="article-summary-box-inner">
<span><p>Existing panoramic depth estimation methods based on convolutional neural
networks (CNNs) focus on removing panoramic distortions, failing to perceive
panoramic structures efficiently due to the fixed receptive field in CNNs. This
paper proposes the panorama transformer (named PanoFormer) to estimate the
depth in panorama images, with tangent patches from spherical domain, learnable
token flows, and panorama specific metrics. In particular, we divide patches on
the spherical tangent domain into tokens to reduce the negative effect of
panoramic distortions. Since the geometric structures are essential for depth
estimation, a self-attention module is redesigned with an additional learnable
token flow. In addition, considering the characteristic of the spherical
domain, we present two panorama-specific metrics to comprehensively evaluate
the panoramic depth estimation models' performance. Extensive experiments
demonstrate that our approach significantly outperforms the state-of-the-art
(SOTA) methods. Furthermore, the proposed method can be effectively extended to
solve semantic panorama segmentation, a similar pixel2pixel task. Code will be
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion. (arXiv:2203.09855v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09855">
<div class="article-summary-box-inner">
<span><p>In this paper, we formulate a potentially valuable panoramic depth completion
(PDC) task as panoramic 3D cameras often produce 360{\deg} depth with missing
data in complex scenes. Its goal is to recover dense panoramic depths from raw
sparse ones and panoramic RGB images. To deal with the PDC task, we train a
deep network that takes both depth and image as inputs for the dense panoramic
depth recovery. However, it needs to face a challenging optimization problem of
the network parameters due to its non-convex objective function. To address
this problem, we propose a simple yet effective approach termed M{^3}PT:
multi-modal masked pre-training. Specifically, during pre-training, we
simultaneously cover up patches of the panoramic RGB image and sparse depth by
shared random mask, then reconstruct the sparse depth in the masked regions. To
our best knowledge, it is the first time that we show the effectiveness of
masked pre-training in a multi-modal vision task, instead of the single-modal
task resolved by masked autoencoders (MAE). Different from MAE where
fine-tuning completely discards the decoder part of pre-training, there is no
architectural difference between the pre-training and fine-tuning stages in our
M$^{3}$PT as they only differ in the prediction density, which potentially
makes the transfer learning more convenient and effective. Extensive
experiments verify the effectiveness of M{^3}PT on three panoramic datasets.
Notably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,
51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transform your Smartphone into a DSLR Camera: Learning the ISP in the Wild. (arXiv:2203.10636v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10636">
<div class="article-summary-box-inner">
<span><p>We propose a trainable Image Signal Processing (ISP) framework that produces
DSLR quality images given RAW images captured by a smartphone. To address the
color misalignments between training image pairs, we employ a color-conditional
ISP network and optimize a novel parametric color mapping between each input
RAW and reference DSLR image. During inference, we predict the target color
image by designing a color prediction network with efficient Global Context
Transformer modules. The latter effectively leverage global information to
learn consistent color and tone mappings. We further propose a robust masked
aligned loss to identify and discard regions with inaccurate motion estimation
during training. Lastly, we introduce the ISP in the Wild (ISPW) dataset,
consisting of weakly paired phone RAW and DSLR sRGB images. We extensively
evaluate our method, setting a new state-of-the-art on two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Salient Object Detection Using Point Supervision. (arXiv:2203.11652v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11652">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art saliency detection models rely heavily on large
datasets of accurate pixel-wise annotations, but manually labeling pixels is
time-consuming and labor-intensive. There are some weakly supervised methods
developed for alleviating the problem, such as image label, bounding box label,
and scribble label, while point label still has not been explored in this
field. In this paper, we propose a novel weakly-supervised salient object
detection method using point supervision. To infer the saliency map, we first
design an adaptive masked flood filling algorithm to generate pseudo labels.
Then we develop a transformer-based point-supervised saliency detection model
to produce the first round of saliency maps. However, due to the sparseness of
the label, the weakly supervised model tends to degenerate into a general
foreground detection model. To address this issue, we propose a Non-Salient
Suppression (NSS) method to optimize the erroneous saliency maps generated in
the first round and leverage them for the second round of training. Moreover,
we build a new point-supervised dataset (P-DUTS) by relabeling the DUTS
dataset. In P-DUTS, there is only one labeled point for each salient object.
Comprehensive experiments on five largest benchmark datasets demonstrate our
method outperforms the previous state-of-the-art methods trained with the
stronger supervision and even surpass several fully supervised state-of-the-art
models. The code is available at: https://github.com/shuyonggao/PSOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Compressed Sensing via Global Image Tokens. (arXiv:2203.12861v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12861">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNN) have demonstrated outstanding Compressed
Sensing (CS) performance compared to traditional, hand-crafted methods.
However, they are broadly limited in terms of generalisability, inductive bias
and difficulty to model long distance relationships. Transformer neural
networks (TNN) overcome such issues by implementing an attention mechanism
designed to capture dependencies between inputs. However, high-resolution tasks
typically require vision Transformers (ViT) to decompose an image into
patch-based tokens, limiting inputs to inherently local contexts. We propose a
novel image decomposition that naturally embeds images into low-resolution
inputs. These Kaleidoscope tokens (KD) provide a mechanism for global
attention, at the same computational cost as a patch-based approach. To
showcase this development, we replace CNN components in a well-known CS-MRI
neural network with TNN blocks and demonstrate the improvements afforded by KD.
We also propose an ensemble of image tokens, which enhance overall image
quality and reduces model size. Supplementary material is available:
https://github.com/uqmarlonbran/TCS.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality High-Frequency Transformer for MR Image Super-Resolution. (arXiv:2203.15314v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15314">
<div class="article-summary-box-inner">
<span><p>Improving the resolution of magnetic resonance (MR) image data is critical to
computer-aided diagnosis and brain function analysis. Higher resolution helps
to capture more detailed content, but typically induces to lower
signal-to-noise ratio and longer scanning time. To this end, MR image
super-resolution has become a widely-interested topic in recent times. Existing
works establish extensive deep models with the conventional architectures based
on convolutional neural networks (CNN). In this work, to further advance this
research field, we make an early effort to build a Transformer-based MR image
super-resolution framework, with careful designs on exploring valuable domain
prior knowledge. Specifically, we consider two-fold domain priors including the
high-frequency structure prior and the inter-modality context prior, and
establish a novel Transformer architecture, called Cross-modality
high-frequency Transformer (Cohf-T), to introduce such priors into
super-resolving the low-resolution (LR) MR images. Experiments on two datasets
indicate that Cohf-T achieves new state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CycDA: Unsupervised Cycle Domain Adaptation from Image to Video. (arXiv:2203.16244v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16244">
<div class="article-summary-box-inner">
<span><p>Although action recognition has achieved impressive results over recent
years, both collection and annotation of video training data are still
time-consuming and cost intensive. Therefore, image-to-video adaptation has
been proposed to exploit labeling-free web image source for adapting on
unlabeled target videos. This poses two major challenges: (1) spatial domain
shift between web images and video frames; (2) modality gap between image and
video data. To address these challenges, we propose Cycle Domain Adaptation
(CycDA), a cycle-based approach for unsupervised image-to-video domain
adaptation by leveraging the joint spatial information in images and videos on
the one hand and, on the other hand, training an independent spatio-temporal
model to bridge the modality gap. We alternate between the spatial and
spatio-temporal learning with knowledge transfer between the two in each cycle.
We evaluate our approach on benchmark datasets for image-to-video as well as
for mixed-source domain adaptation achieving state-of-the-art results and
demonstrating the benefits of our cyclic adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saliency in Augmented Reality. (arXiv:2204.08308v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08308">
<div class="article-summary-box-inner">
<span><p>With the rapid development of multimedia technology, Augmented Reality (AR)
has become a promising next-generation mobile platform. The primary theory
underlying AR is human visual confusion, which allows users to perceive the
real-world scenes and augmented contents (virtual-world scenes) simultaneously
by superimposing them together. To achieve good Quality of Experience (QoE), it
is important to understand the interaction between two scenarios, and
harmoniously display AR contents. However, studies on how this superimposition
will influence the human visual attention are lacking. Therefore, in this
paper, we mainly analyze the interaction effect between background (BG) scenes
and AR contents, and study the saliency prediction problem in AR. Specifically,
we first construct a Saliency in AR Dataset (SARD), which contains 450 BG
images, 450 AR images, as well as 1350 superimposed images generated by
superimposing BG and AR images in pair with three mixing levels. A large-scale
eye-tracking experiment among 60 subjects is conducted to collect eye movement
data. To better predict the saliency in AR, we propose a vector quantized
saliency prediction method and generalize it for AR saliency prediction. For
comparison, three benchmark methods are proposed and evaluated together with
our proposed method on our SARD. Experimental results demonstrate the
superiority of our proposed method on both of the common saliency prediction
problem and the AR saliency prediction problem over benchmark methods. Our
dataset and code are available at: https://github.com/DuanHuiyu/ARSaliency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMRotate: A Rotated Object Detection Benchmark using Pytorch. (arXiv:2204.13317v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13317">
<div class="article-summary-box-inner">
<span><p>We present an open-source toolbox, named MMRotate, which provides a coherent
algorithm framework of training, inferring, and evaluation for the popular
rotated object detection algorithm based on deep learning. MMRotate implements
18 state-of-the-art algorithms and supports the three most frequently used
angle definition methods. To facilitate future research and industrial
applications of rotated object detection-related problems, we also provide a
large number of trained models and detailed benchmarks to give insights into
the performance of rotated object detection. MMRotate is publicly released at
https://github.com/open-mmlab/mmrotate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty estimation for Cross-dataset performance in Trajectory prediction. (arXiv:2205.07310v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07310">
<div class="article-summary-box-inner">
<span><p>While a lot of work has been carried on developing trajectory prediction
methods, and various datasets have been proposed for benchmarking this task,
little study has been done so far on the generalizability and the
transferability of these methods across dataset. In this paper, we observe the
performance of two of the latest state-of-the-art trajectory prediction methods
across four different datasets (Argoverse, NuScenes, Interaction, Shifts). This
analysis allows to gain some insights on the generalizability proprieties of
most recent trajectory prediction models and to analyze which dataset is more
representative of real driving scenes and therefore enables better
transferability. Furthermore we present a novel method to estimate prediction
uncertainty and show how it could be used to achieve better performance across
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MESH2IR: Neural Acoustic Impulse Response Generator for Complex 3D Scenes. (arXiv:2205.09248v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09248">
<div class="article-summary-box-inner">
<span><p>We propose a mesh-based neural network (MESH2IR) to generate acoustic impulse
responses (IRs) for indoor 3D scenes represented using a mesh. The IRs are used
to create a high-quality sound experience in interactive applications and audio
processing. Our method can handle input triangular meshes with arbitrary
topologies (2K - 3M triangles). We present a novel training technique to train
MESH2IR using energy decay relief and highlight its benefits. We also show that
training MESH2IR on IRs preprocessed using our proposed technique significantly
improves the accuracy of IR generation. We reduce the non-linearity in the mesh
space by transforming 3D scene meshes to latent space using a graph convolution
network. Our MESH2IR is more than 200 times faster than a geometric acoustic
algorithm on a CPU and can generate more than 10,000 IRs per second on an
NVIDIA GeForce RTX 2080 Ti GPU for a given furnished indoor 3D scene. The
acoustic metrics are used to characterize the acoustic environment. We show
that the acoustic metrics of the IRs predicted from our MESH2IR match the
ground truth with less than 10% error. We also highlight the benefits of
MESH2IR on audio and speech processing applications such as speech
dereverberation and speech separation. To the best of our knowledge, ours is
the first neural-network-based approach to predict IRs from a given 3D scene
mesh in real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRT-ViT: TensorRT-oriented Vision Transformer. (arXiv:2205.09579v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09579">
<div class="article-summary-box-inner">
<span><p>We revisit the existing excellent Transformers from the perspective of
practical application. Most of them are not even as efficient as the basic
ResNets series and deviate from the realistic deployment scenario. It may be
due to the current criterion to measure computation efficiency, such as FLOPs
or parameters is one-sided, sub-optimal, and hardware-insensitive. Thus, this
paper directly treats the TensorRT latency on the specific hardware as an
efficiency metric, which provides more comprehensive feedback involving
computational capacity, memory cost, and bandwidth. Based on a series of
controlled experiments, this work derives four practical guidelines for
TensorRT-oriented and deployment-friendly network design, e.g., early CNN and
late Transformer at stage-level, early Transformer and late CNN at block-level.
Accordingly, a family of TensortRT-oriented Transformers is presented,
abbreviated as TRT-ViT. Extensive experiments demonstrate that TRT-ViT
significantly outperforms existing ConvNets and vision Transformers with
respect to the latency/accuracy trade-off across diverse visual tasks, e.g.,
image classification, object detection and semantic segmentation. For example,
at 82.7% ImageNet-1k top-1 accuracy, TRT-ViT is 2.7$\times$ faster than CSWin
and 2.0$\times$ faster than Twins. On the MS-COCO object detection task,
TRT-ViT achieves comparable performance with Twins, while the inference speed
is increased by 2.8$\times$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporally Precise Action Spotting in Soccer Videos Using Dense Detection Anchors. (arXiv:2205.10450v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10450">
<div class="article-summary-box-inner">
<span><p>We present a model for temporally precise action spotting in videos, which
uses a dense set of detection anchors, predicting a detection confidence and
corresponding fine-grained temporal displacement for each anchor. We experiment
with two trunk architectures, both of which are able to incorporate large
temporal contexts while preserving the smaller-scale features required for
precise localization: a one-dimensional version of a u-net, and a Transformer
encoder (TE). We also suggest best practices for training models of this kind,
by applying Sharpness-Aware Minimization (SAM) and mixup data augmentation. We
achieve a new state-of-the-art on SoccerNet-v2, the largest soccer video
dataset of its kind, with marked improvements in temporal localization.
Additionally, our ablations show: the importance of predicting the temporal
displacements; the trade-offs between the u-net and TE trunks; and the benefits
of training with SAM and mixup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13326">
<div class="article-summary-box-inner">
<span><p>This paper describes the methods submitted for evaluation to the SHREC 2022
track on pothole and crack detection in the road pavement. A total of 7
different runs for the semantic segmentation of the road surface are compared,
6 from the participants plus a baseline method. All methods exploit Deep
Learning techniques and their performance is tested using the same environment
(i.e.: a single Jupyter notebook). A training set, composed of 3836 semantic
segmentation image/mask pairs and 797 RGB-D video clips collected with the
latest depth cameras was made available to the participants. The methods are
then evaluated on the 496 image/mask pairs in the validation set, on the 504
pairs in the test set and finally on 8 video clips. The analysis of the results
is based on quantitative metrics for image segmentation and qualitative
analysis of the video clips. The participation and the results show that the
scenario is of great interest and that the use of RGB-D data is still
challenging in this context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Robustness and Generalization of Deep Neural Network with Confidence Threshold Reduction. (arXiv:2206.00913v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00913">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are easily attacked by imperceptible perturbation.
Presently, adversarial training (AT) is the most effective method to enhance
the robustness of the model against adversarial examples. However, because
adversarial training solved a min-max value problem, in comparison with natural
training, the robustness and generalization are contradictory, i.e., the
robustness improvement of the model will decrease the generalization of the
model. To address this issue, in this paper, a new concept, namely confidence
threshold (CT), is introduced and the reducing of the confidence threshold,
known as confidence threshold reduction (CTR), is proven to improve both the
generalization and robustness of the model. Specifically, to reduce the CT for
natural training (i.e., for natural training with CTR), we propose a
mask-guided divergence loss function (MDL) consisting of a cross-entropy loss
term and an orthogonal term. The empirical and theoretical analysis
demonstrates that the MDL loss improves the robustness and generalization of
the model simultaneously for natural training. However, the model robustness
improvement of natural training with CTR is not comparable to that of
adversarial training. Therefore, for adversarial training, we propose a
standard deviation loss function (STD), which minimizes the difference in the
probabilities of the wrong categories, to reduce the CT by being integrated
into the loss function of adversarial training. The empirical and theoretical
analysis demonstrates that the STD based loss function can further improve the
robustness of the adversarially trained model on basis of guaranteeing the
changeless or slight improvement of the natural accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Ego 3D Representation as Ray Tracing. (arXiv:2206.04042v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04042">
<div class="article-summary-box-inner">
<span><p>A self-driving perception model aims to extract 3D semantic representations
from multiple cameras collectively into the bird's-eye-view (BEV) coordinate
frame of the ego car in order to ground downstream planner. Existing perception
methods often rely on error-prone depth estimation of the whole scene or
learning sparse virtual 3D representations without the target geometry
structure, both of which remain limited in performance and/or capability. In
this paper, we present a novel end-to-end architecture for ego 3D
representation learning from an arbitrary number of unconstrained camera views.
Inspired by the ray tracing principle, we design a polarized grid of "imaginary
eyes" as the learnable ego 3D representation and formulate the learning process
with the adaptive attention mechanism in conjunction with the 3D-to-2D
projection. Critically, this formulation allows extracting rich 3D
representation from 2D images without any depth supervision, and with the
built-in geometry structure consistent w.r.t. BEV. Despite its simplicity and
versatility, extensive experiments on standard BEV visual tasks (e.g.,
camera-based 3D object detection and BEV segmentation) show that our model
outperforms all state-of-the-art alternatives significantly, with an extra
advantage in computational efficiency from multi-task learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantitative Imaging Principles Improves Medical Image Learning. (arXiv:2206.06663v3 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06663">
<div class="article-summary-box-inner">
<span><p>Fundamental differences between natural and medical images have recently
favored the use of self-supervised learning (SSL) over ImageNet transfer
learning for medical image applications. Differences between image types are
primarily due to the imaging modality and medical images utilize a wide range
of physics based techniques while natural images are captured using only
visible light. While many have demonstrated that SSL on medical images has
resulted in better downstream task performance, our work suggests that more
performance can be gained. The scientific principles which are used to acquire
medical images are not often considered when constructing learning problems.
For this reason, we propose incorporating quantitative imaging principles
during generative SSL to improve image quality and quantitative biological
accuracy. We show that this training schema results in better starting states
for downstream supervised training on limited data. Our model also generates
images that validate on clinical quantitative analysis software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07136">
<div class="article-summary-box-inner">
<span><p>Per-example gradient clipping is a key algorithmic step that enables
practical differential private (DP) training for deep learning models. The
choice of clipping norm $R$, however, is shown to be vital for achieving high
accuracy under DP. We propose an easy-to-use replacement, called AutoClipping,
that eliminates the need to tune $R$ for any DP optimizers, including DP-SGD,
DP-Adam, DP-LAMB and many others. The automatic variants are as private and
computationally efficient as existing DP optimizers, but require no DP-specific
hyperparameters and thus make DP training as amenable as the standard
non-private training. We give a rigorous convergence analysis of automatic
DP-SGD in the non-convex setting, which shows that it enjoys an asymptotic
convergence rate that matches the standard SGD. We also demonstrate on various
language and vision tasks that automatic clipping outperforms or matches the
state-of-the-art, and can be easily employed with minimal changes to existing
codebases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EyeNeRF: A Hybrid Representation for Photorealistic Synthesis, Animation and Relighting of Human Eyes. (arXiv:2206.08428v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08428">
<div class="article-summary-box-inner">
<span><p>A unique challenge in creating high-quality animatable and relightable 3D
avatars of people is modeling human eyes. The challenge of synthesizing eyes is
multifold as it requires 1) appropriate representations for the various
components of the eye and the periocular region for coherent viewpoint
synthesis, capable of representing diffuse, refractive and highly reflective
surfaces, 2) disentangling skin and eye appearance from environmental
illumination such that it may be rendered under novel lighting conditions, and
3) capturing eyeball motion and the deformation of the surrounding skin to
enable re-gazing. These challenges have traditionally necessitated the use of
expensive and cumbersome capture setups to obtain high-quality results, and
even then, modeling of the eye region holistically has remained elusive. We
present a novel geometry and appearance representation that enables
high-fidelity capture and photorealistic animation, view synthesis and
relighting of the eye region using only a sparse set of lights and cameras. Our
hybrid representation combines an explicit parametric surface model for the
eyeball with implicit deformable volumetric representations for the periocular
region and the interior of the eye. This novel hybrid model has been designed
to address the various parts of that challenging facial area - the explicit
eyeball surface allows modeling refraction and high-frequency specular
reflection at the cornea, whereas the implicit representation is well suited to
model lower-frequency skin reflection via spherical harmonics and can represent
non-surface structures such as hair or diffuse volumetric bodies, both of which
are a challenge for explicit surface models. We show that for high-resolution
close-ups of the eye, our model can synthesize high-fidelity animated gaze from
novel views under unseen illumination conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without Bells and Whistles. (arXiv:2206.10255v2 [eess.SY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10255">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking (MOT) is among crucial applications in modern advanced
driver assistance systems (ADAS) and autonomous driving (AD) systems. Global
nearest neighbor (GNN) filter, as the earliest random vector Bayesian tracking
framework, has been adopted in most of state-of-the-arts trackers and widely
accepted in the automotive industry. With the development of random finite set
(RFS) theory, the RFS Bayesian filters have been applied in MOT tasks recently.
However, their usefulness in the real traffic for ADAS and AD application is
still open to doubt. In this paper, we firstly demonstrate the latest RFS
Bayesian tracking framework could be superior to typical random vector Bayesian
tracking framework like GNN, via a systematic comparative study of both
traditional random vector Bayesian filters with rule-based heuristic track
maintenance and RFS Bayesian filters on nuScenes validation dataset. Then, we
propose a RFS-based tracker, namely Poisson multi-Bernoulli filter using the
global nearest neighbor (GNN-PMB), for LiDAR-based MOT tasks. This GNN-PMB
tracker is simple to use but can achieve competitive results on nuScenes
dataset. Specifically, the proposed GNN-PMB tracker outperforms most of the
state-of-the-art LiDAR-only trackers and LiDAR and camera fusion-based
trackers, ranks the 3rd among all LiDAR-only trackers on nuScenes tracking task
leader board1 at the time of submission. Our code is available at
https://github.com/chisyliu/gnn pmb tracker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UI Layers Merger: Merging UI layers via Visual Learning and Boundary Prior. (arXiv:2206.13389v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13389">
<div class="article-summary-box-inner">
<span><p>With the fast-growing GUI development workload in the Internet industry, some
work on intelligent methods attempted to generate maintainable front-end code
from UI screenshots. It can be more suitable for utilizing UI design drafts
that contain UI metadata. However, fragmented layers inevitably appear in the
UI design drafts which greatly reduces the quality of code generation. None of
the existing GUI automated techniques detects and merges the fragmented layers
to improve the accessibility of generated code. In this paper, we propose UI
Layers Merger (UILM), a vision-based method, which can automatically detect and
merge fragmented layers into UI components. Our UILM contains Merging Area
Detector (MAD) and a layers merging algorithm. MAD incorporates the boundary
prior knowledge to accurately detect the boundaries of UI components. Then, the
layers merging algorithm can search out the associated layers within the
components' boundaries and merge them into a whole part. We present a dynamic
data augmentation approach to boost the performance of MAD. We also construct a
large-scale UI dataset for training the MAD and testing the performance of
UILM. The experiment shows that the proposed method outperforms the best
baseline regarding merging area detection and achieves a decent accuracy
regarding layers merging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C2FTrans: Coarse-to-Fine Transformers for Medical Image Segmentation. (arXiv:2206.14409v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14409">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNN), the most prevailing architecture for
deep-learning based medical image analysis, are still functionally limited by
their intrinsic inductive biases and inadequate receptive fields. Transformer,
born to address this issue, has drawn explosive attention in natural language
processing and computer vision due to its remarkable ability in capturing
long-range dependency. However, most recent transformer-based methods for
medical image segmentation directly apply vanilla transformers as an auxiliary
module in CNN-based methods, resulting in severe detail loss due to the rigid
patch partitioning scheme in transformers. To address this problem, we propose
C2FTrans, a novel multi-scale architecture that formulates medical image
segmentation as a coarse-to-fine procedure. C2FTrans mainly consists of a
cross-scale global transformer (CGT) which addresses local contextual
similarity in CNN and a boundary-aware local transformer (BLT) which overcomes
boundary uncertainty brought by rigid patch partitioning in transformers.
Specifically, CGT builds global dependency across three different small-scale
feature maps to obtain rich global semantic features with an acceptable
computational cost, while BLT captures mid-range dependency by adaptively
generating windows around boundaries under the guidance of entropy to reduce
computational complexity and minimize detail loss based on large-scale feature
maps. Extensive experimental results on three public datasets demonstrate the
superior performance of C2FTrans against state-of-the-art CNN-based and
transformer-based methods with fewer parameters and lower FLOPs. We believe the
design of C2FTrans would further inspire future work on developing efficient
and lightweight transformers for medical image segmentation. The source code of
this paper is publicly available at https://github.com/xianlin7/C2FTrans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Lighter The Better: Rethinking Transformers in Medical Image Segmentation Through Adaptive Pruning. (arXiv:2206.14413v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14413">
<div class="article-summary-box-inner">
<span><p>Vision transformers have recently set off a new wave in the field of medical
image analysis due to their remarkable performance on various computer vision
tasks. However, recent hybrid-/transformer-based approaches mainly focus on the
benefits of transformers in capturing long-range dependency while ignoring the
issues of their daunting computational complexity, high training costs, and
redundant dependency. In this paper, we propose to employ adaptive pruning to
transformers for medical image segmentation and propose a lightweight and
effective hybrid network APFormer. To our best knowledge, this is the first
work on transformer pruning for medical image analysis tasks. The key features
of APFormer mainly are self-supervised self-attention (SSA) to improve the
convergence of dependency establishment, Gaussian-prior relative position
embedding (GRPE) to foster the learning of position information, and adaptive
pruning to eliminate redundant computations and perception information.
Specifically, SSA and GRPE consider the well-converged dependency distribution
and the Gaussian heatmap distribution separately as the prior knowledge of
self-attention and position embedding to ease the training of transformers and
lay a solid foundation for the following pruning operation. Then, adaptive
transformer pruning, both query-wise and dependency-wise, is performed by
adjusting the gate control parameters for both complexity reduction and
performance improvement. Extensive experiments on two widely-used datasets
demonstrate the prominent segmentation performance of APFormer against the
state-of-the-art methods with much fewer parameters and lower GFLOPs. More
importantly, we prove, through ablation studies, that adaptive pruning can work
as a plug-n-play module for performance improvement on other
hybrid-/transformer-based methods. Code is available at
https://github.com/xianlin7/APFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolarFormer: Multi-camera 3D Object Detection with Polar Transformers. (arXiv:2206.15398v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15398">
<div class="article-summary-box-inner">
<span><p>3D object detection in autonomous driving aims to reason "what" and "where"
the objects of interest present in a 3D world. Following the conventional
wisdom of previous 2D object detection, existing methods often adopt the
canonical Cartesian coordinate system with perpendicular axis. However, we
conjugate that this does not fit the nature of the ego car's perspective, as
each onboard camera perceives the world in shape of wedge intrinsic to the
imaging geometry with radical (non-perpendicular) axis. Hence, in this paper we
advocate the exploitation of the Polar coordinate system and propose a new
Polar Transformer (PolarFormer) for more accurate 3D object detection in the
bird's-eye-view (BEV) taking as input only multi-camera 2D images.
Specifically, we design a cross attention based Polar detection head without
restriction to the shape of input structure to deal with irregular Polar grids.
For tackling the unconstrained object scale variations along Polar's distance
dimension, we further introduce a multi-scalePolar representation learning
strategy. As a result, our model can make best use of the Polar representation
rasterized via attending to the corresponding image observation in a
sequence-to-sequence fashion subject to the geometric constraints. Thorough
experiments on the nuScenes dataset demonstrate that our PolarFormer
outperforms significantly state-of-the-art 3D object detection alternatives, as
well as yielding competitive performance on BEV semantic segmentation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Single-Frame 3D Object Detection by Simulating Multi-Frame Point Clouds. (arXiv:2207.01030v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01030">
<div class="article-summary-box-inner">
<span><p>To boost a detector for single-frame 3D object detection, we present a new
approach to train it to simulate features and responses following a detector
trained on multi-frame point clouds. Our approach needs multi-frame point
clouds only when training the single-frame detector, and once trained, it can
detect objects with only single-frame point clouds as inputs during the
inference. We design a novel Simulated Multi-Frame Single-Stage object Detector
(SMF-SSD) framework to realize the approach: multi-view dense object fusion to
densify ground-truth objects to generate a multi-frame point cloud;
self-attention voxel distillation to facilitate one-to-many knowledge transfer
from multi- to single-frame voxels; multi-scale BEV feature distillation to
transfer knowledge in low-level spatial and high-level semantic BEV features;
and adaptive response distillation to activate single-frame responses of high
confidence and accurate localization. Experimental results on the Waymo test
set show that our SMF-SSD consistently outperforms all state-of-the-art
single-frame 3D object detectors for all object classes of difficulty levels 1
and 2 in terms of both mAP and mAPH.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models. (arXiv:2207.01056v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01056">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pre-training (VLP) models have achieved state-of-the-art
performance in numerous cross-modal tasks. Since they are optimized to capture
the statistical properties of intra- and inter-modality, there remains risk to
learn social biases presented in the data as well. In this work, we (1)
introduce a counterfactual-based bias measurement \emph{CounterBias} to
quantify the social bias in VLP models by comparing the [MASK]ed prediction
probabilities of factual and counterfactual samples; (2) construct a novel
VL-Bias dataset including 24K image-text pairs for measuring gender bias in VLP
models, from which we observed that significant gender bias is prevalent in VLP
models; and (3) propose a VLP debiasing method \emph{FairVLP} to minimize the
difference in the [MASK]ed prediction probabilities between factual and
counterfactual image-text pairs for VLP debiasing. Although CounterBias and
FairVLP focus on social bias, they are generalizable to serve as tools and
provide new insights to probe and regularize more knowledge in VLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of ADHD based on Eye Movements during Natural Viewing. (arXiv:2207.01377v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01377">
<div class="article-summary-box-inner">
<span><p>Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental
disorder that is highly prevalent and requires clinical specialists to
diagnose. It is known that an individual's viewing behavior, reflected in their
eye movements, is directly related to attentional mechanisms and higher-order
cognitive processes. We therefore explore whether ADHD can be detected based on
recorded eye movements together with information about the video stimulus in a
free-viewing task. To this end, we develop an end-to-end deep learning-based
sequence model which we pre-train on a related task for which more data are
available. We find that the method is in fact able to detect ADHD and
outperforms relevant baselines. We investigate the relevance of the input
features in an ablation study. Interestingly, we find that the model's
performance is closely related to the content of the video, which provides
insights for future experimental designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric Capture. (arXiv:2207.02031v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02031">
<div class="article-summary-box-inner">
<span><p>To address the ill-posed problem caused by partial observations in monocular
human volumetric capture, we present AvatarCap, a novel framework that
introduces animatable avatars into the capture pipeline for high-fidelity
reconstruction in both visible and invisible regions. Our method firstly
creates an animatable avatar for the subject from a small number (~20) of 3D
scans as a prior. Then given a monocular RGB video of this subject, our method
integrates information from both the image observation and the avatar prior,
and accordingly recon-structs high-fidelity 3D textured models with dynamic
details regardless of the visibility. To learn an effective avatar for
volumetric capture from only few samples, we propose GeoTexAvatar, which
leverages both geometry and texture supervisions to constrain the
pose-dependent dynamics in a decomposed implicit manner. An avatar-conditioned
volumetric capture method that involves a canonical normal fusion and a
reconstruction network is further proposed to integrate both image observations
and avatar dynamics for high-fidelity reconstruction in both observed and
invisible regions. Overall, our method enables monocular human volumetric
capture with detailed and pose-dependent dynamics, and the experiments show
that our method outperforms state of the art. Code is available at
https://github.com/lizhe00/AvatarCap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Counterfactual Image Manipulation via CLIP. (arXiv:2207.02812v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02812">
<div class="article-summary-box-inner">
<span><p>Leveraging StyleGAN's expressivity and its disentangled latent codes,
existing methods can achieve realistic editing of different visual attributes
such as age and gender of facial images. An intriguing yet challenging problem
arises: Can generative models achieve counterfactual editing against their
learnt priors? Due to the lack of counterfactual samples in natural datasets,
we investigate this problem in a text-driven manner with
Contrastive-Language-Image-Pretraining (CLIP), which can offer rich semantic
knowledge even for various counterfactual concepts. Different from in-domain
manipulation, counterfactual manipulation requires more comprehensive
exploitation of semantic knowledge encapsulated in CLIP as well as more
delicate handling of editing directions for avoiding being stuck in local
minimum or undesired editing. To this end, we design a novel contrastive loss
that exploits predefined CLIP-space directions to guide the editing toward
desired directions from different perspectives. In addition, we design a simple
yet effective scheme that explicitly maps CLIP embeddings (of target text) to
the latent space and fuses them with latent codes for effective latent code
optimization and accurate editing. Extensive experiments show that our design
achieves accurate and realistic editing while driving by target texts with
various counterfactual concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Network Binarization via Contrastive Learning. (arXiv:2207.02970v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02970">
<div class="article-summary-box-inner">
<span><p>Neural network binarization accelerates deep models by quantizing their
weights and activations into 1-bit. However, there is still a huge performance
gap between Binary Neural Networks (BNNs) and their full-precision (FP)
counterparts. As the quantization error caused by weights binarization has been
reduced in earlier works, the activations binarization becomes the major
obstacle for further improvement of the accuracy. BNN characterises a unique
and interesting structure, where the binary and latent FP activations exist in
the same forward pass (i.e., $\text{Binarize}(\mathbf{a}_F) = \mathbf{a}_B$).
To mitigate the information degradation caused by the binarization operation
from FP to binary activations, we establish a novel contrastive learning
framework while training BNNs through the lens of Mutual Information (MI)
maximization. MI is introduced as the metric to measure the information shared
between binary and FP activations, which assists binarization with contrastive
learning. Specifically, the representation ability of the BNNs is greatly
strengthened via pulling the positive pairs with binary and FP activations from
the same input samples, as well as pushing negative pairs from different
samples (the number of negative pairs can be exponentially large). This
benefits the downstream tasks, not only classification but also segmentation
and depth estimation, etc. The experimental results show that our method can be
implemented as a pile-up module on existing state-of-the-art binarization
methods and can remarkably improve the performance over them on CIFAR-10/100
and ImageNet, in addition to the great generalization ability on NYUD-v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes for Automatic Reconstruction of Pulmonary Segments. (arXiv:2207.03078v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03078">
<div class="article-summary-box-inner">
<span><p>3D reconstruction of pulmonary segments plays an important role in surgical
treatment planning of lung cancer, which facilitates preservation of pulmonary
function and helps ensure low recurrence rates. However, automatic
reconstruction of pulmonary segments remains unexplored in the era of deep
learning. In this paper, we investigate what makes for automatic reconstruction
of pulmonary segments. First and foremost, we formulate, clinically and
geometrically, the anatomical definitions of pulmonary segments, and propose
evaluation metrics adhering to these definitions. Second, we propose ImPulSe
(Implicit Pulmonary Segment), a deep implicit surface model designed for
pulmonary segment reconstruction. The automatic reconstruction of pulmonary
segments by ImPulSe is accurate in metrics and visually appealing. Compared
with canonical segmentation methods, ImPulSe outputs continuous predictions of
arbitrary resolutions with higher training efficiency and fewer parameters.
Lastly, we experiment with different network inputs to analyze what matters in
the task of pulmonary segment reconstruction. Our code is available at
https://github.com/M3DV/ImPulSe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complementing Brightness Constancy with Deep Networks for Optical Flow Prediction. (arXiv:2207.03790v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03790">
<div class="article-summary-box-inner">
<span><p>State-of-the-art methods for optical flow estimation rely on deep learning,
which require complex sequential training schemes to reach optimal performances
on real-world data. In this work, we introduce the COMBO deep network that
explicitly exploits the brightness constancy (BC) model used in traditional
methods. Since BC is an approximate physical model violated in several
situations, we propose to train a physically-constrained network complemented
with a data-driven network. We introduce a unique and meaningful flow
decomposition between the physical prior and the data-driven complement,
including an uncertainty quantification of the BC model. We derive a joint
training scheme for learning the different components of the decomposition
ensuring an optimal cooperation, in a supervised but also in a semi-supervised
context. Experiments show that COMBO can improve performances over
state-of-the-art supervised networks, e.g. RAFT, reaching state-of-the-art
results on several benchmarks. We highlight how COMBO can leverage the BC model
and adapt to its limitations. Finally, we show that our semi-supervised method
can significantly simplify the training procedure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radiomics-Guided Global-Local Transformer for Weakly Supervised Pathology Localization in Chest X-Rays. (arXiv:2207.04394v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04394">
<div class="article-summary-box-inner">
<span><p>Before the recent success of deep learning methods for automated medical
image analysis, practitioners used handcrafted radiomic features to
quantitatively describe local patches of medical images. However, extracting
discriminative radiomic features relies on accurate pathology localization,
which is difficult to acquire in real-world settings. Despite advances in
disease classification and localization from chest X-rays, many approaches fail
to incorporate clinically-informed domain knowledge. For these reasons, we
propose a Radiomics-Guided Transformer (RGT) that fuses \textit{global} image
information with \textit{local} knowledge-guided radiomics information to
provide accurate cardiopulmonary pathology localization and classification
\textit{without any bounding box annotations}. RGT consists of an image
Transformer branch, a radiomics Transformer branch, and fusion layers that
aggregate image and radiomic information. Using the learned self-attention of
its image branch, RGT extracts a bounding box for which to compute radiomic
features, which are further processed by the radiomics branch; learned image
and radiomic features are then fused and mutually interact via cross-attention
layers. Thus, RGT utilizes a novel end-to-end feedback loop that can bootstrap
accurate pathology localization only using image-level disease labels.
Experiments on the NIH ChestXRay dataset demonstrate that RGT outperforms prior
works in weakly supervised disease localization (by an average margin of 3.6\%
over various intersection-over-union thresholds) and classification (by 1.1\%
in average area under the receiver operating characteristic curve). Code and
trained models will be released upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depthformer : Multiscale Vision Transformer For Monocular Depth Estimation With Local Global Information Fusion. (arXiv:2207.04535v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04535">
<div class="article-summary-box-inner">
<span><p>Attention-based models such as transformers have shown outstanding
performance on dense prediction tasks, such as semantic segmentation, owing to
their capability of capturing long-range dependency in an image. However, the
benefit of transformers for monocular depth prediction has seldom been explored
so far. This paper benchmarks various transformer-based models for the depth
estimation task on an indoor NYUV2 dataset and an outdoor KITTI dataset. We
propose a novel attention-based architecture, Depthformer for monocular depth
estimation that uses multi-head self-attention to produce the multiscale
feature maps, which are effectively combined by our proposed decoder network.
We also propose a Transbins module that divides the depth range into bins whose
center value is estimated adaptively per image. The final depth estimated is a
linear combination of bin centers for each pixel. Transbins module takes
advantage of the global receptive field using the transformer module in the
encoding stage. Experimental results on NYUV2 and KITTI depth estimation
benchmark demonstrate that our proposed method improves the state-of-the-art by
3.3%, and 3.3% respectively in terms of Root Mean Squared Error (RMSE). Code is
available at https://github.com/ashutosh1807/Depthformer.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry-aware Single-image Full-body Human Relighting. (arXiv:2207.04750v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04750">
<div class="article-summary-box-inner">
<span><p>Single-image human relighting aims to relight a target human under new
lighting conditions by decomposing the input image into albedo, shape and
lighting. Although plausible relighting results can be achieved, previous
methods suffer from both the entanglement between albedo and lighting and the
lack of hard shadows, which significantly decrease the realism. To tackle these
two problems, we propose a geometry-aware single-image human relighting
framework that leverages single-image geometry reconstruction for joint
deployment of traditional graphics rendering and neural rendering techniques.
For the de-lighting, we explore the shortcomings of UNet architecture and
propose a modified HRNet, achieving better disentanglement between albedo and
lighting. For the relighting, we introduce a ray tracing-based per-pixel
lighting representation that explicitly models high-frequency shadows and
propose a learning-based shading refinement module to restore realistic shadows
(including hard cast shadows) from the ray-traced shading maps. Our framework
is able to generate photo-realistic high-frequency shadows such as cast shadows
under challenging lighting conditions. Extensive experiments demonstrate that
our proposed method outperforms previous methods on both synthetic and real
images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Vision Transformer. (arXiv:2207.04976v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04976">
<div class="article-summary-box-inner">
<span><p>Prior works have proposed several strategies to reduce the computational cost
of self-attention mechanism. Many of these works consider decomposing the
self-attention procedure into regional and local feature extraction procedures
that each incurs a much smaller computational complexity. However, regional
information is typically only achieved at the expense of undesirable
information lost owing to down-sampling. In this paper, we propose a novel
Transformer architecture that aims to mitigate the cost issue, named Dual
Vision Transformer (Dual-ViT). The new architecture incorporates a critical
semantic pathway that can more efficiently compress token vectors into global
semantics with reduced order of complexity. Such compressed global semantics
then serve as useful prior information in learning finer pixel level details,
through another constructed pixel pathway. The semantic pathway and pixel
pathway are then integrated together and are jointly trained, spreading the
enhanced self-attention information in parallel through both of the pathways.
Dual-ViT is henceforth able to reduce the computational complexity without
compromising much accuracy. We empirically demonstrate that Dual-ViT provides
superior accuracy than SOTA Transformer architectures with reduced training
complexity. Source code is available at
\url{https://github.com/YehLi/ImageNetModel}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Continuous Grasping Function with a Dexterous Hand from Human Demonstrations. (arXiv:2207.05053v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05053">
<div class="article-summary-box-inner">
<span><p>We propose to learn to generate grasping motion for manipulation with a
dexterous hand using implicit functions. With continuous time inputs, the model
can generate a continuous and smooth grasping plan. We name the proposed model
Continuous Grasping Function (CGF). CGF is learned via generative modeling with
a Conditional Variational Autoencoder using 3D human demonstrations. We will
first convert the large-scale human-object interaction trajectories to robot
demonstrations via motion retargeting, and then use these demonstrations to
train CGF. During inference, we perform sampling with CGF to generate different
grasping plans in the simulator and select the successful ones to transfer to
the real robot. By training on diverse human data, our CGF allows
generalization to manipulate multiple objects. Compared to previous planning
algorithms, CGF is more efficient and achieves significant improvement on
success rate when transferred to grasping with the real Allegro Hand. Our
project page is at https://jianglongye.com/cgf .
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-13 23:08:06.980090887 UTC">2022-07-13 23:08:06 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>