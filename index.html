<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-28T01:30:00Z">09-28</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Pretrained Models for Automatic Summarization of Doctor-Patient Conversations. (arXiv:2109.12174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12174">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pretrained models for automatically summarizing doctor-patient
conversation transcripts presents many challenges: limited training data,
significant domain shift, long and noisy transcripts, and high target summary
variability. In this paper, we explore the feasibility of using pretrained
transformer models for automatically summarizing doctor-patient conversations
directly from transcripts. We show that fluent and adequate summaries can be
generated with limited training data by fine-tuning BART on a specially
constructed dataset. The resulting models greatly surpass the performance of an
average human annotator and the quality of previous published work for the
task. We evaluate multiple methods for handling long conversations, comparing
them to the obvious baseline of truncating the conversation to fit the
pretrained model length limit. We introduce a multistage approach that tackles
the task by learning two fine-tuned models: one for summarizing conversation
chunks into partial summaries, followed by one for rewriting the collection of
partial summaries into a complete summary. Using a carefully chosen fine-tuning
dataset, this method is shown to be effective at handling longer conversations,
improving the quality of generated summaries. We conduct both an automatic
evaluation (through ROUGE and two concept-based metrics focusing on medical
findings) and a human evaluation (through qualitative examples from literature,
assessing hallucination, generalization, fluency, and general quality of the
generated summaries).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Attention Sparsity in Transformers. (arXiv:2109.12188v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12188">
<div class="article-summary-box-inner">
<span><p>A bottleneck in transformer architectures is their quadratic complexity with
respect to the input sequence, which has motivated a body of work on efficient
sparse approximations to softmax. An alternative path, used by entmax
transformers, consists of having built-in exact sparse attention; however this
approach still requires quadratic computation. In this paper, we propose
Sparsefinder, a simple model trained to identify the sparsity pattern of entmax
attention before computing it. We experiment with three variants of our method,
based on distances, quantization, and clustering, on two tasks: machine
translation (attention in the decoder) and masked language modeling
(encoder-only). Our work provides a new angle to study model efficiency by
doing extensive analysis of the tradeoff between the sparsity and recall of the
predicted attention graph. This allows for detailed comparison between
different models, and may guide future benchmarks for sparse models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Truly Matters? Using Linguistic Cues for Analyzing the #BlackLivesMatter Movement and its Counter Protests: 2013 to 2020. (arXiv:2109.12192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12192">
<div class="article-summary-box-inner">
<span><p>Since the fatal shooting of 17-year old Black teenager Trayvon Martin in
February 2012 by a White neighborhood watchman, George Zimmerman in Sanford,
Florida, there has been a significant increase in digital activism addressing
police-brutality related and racially-motivated incidents in the United States.
In this work, we administer an innovative study of digital activism by
exploiting social media as an authoritative tool to examine and analyze the
linguistic cues and thematic relationships in these three mediums. We conduct a
multi-level text analysis on 36,984,559 tweets to investigate users' behaviors
to examine the language used and understand the impact of digital activism on
social media within each social movement on a sentence-level, word-level, and
topic-level. Our results show that excessive use of racially-related or
prejudicial hashtags were used by the counter protests which portray potential
discriminatory tendencies. Consequently, our findings highlight that social
activism done by Black Lives Matter activists does not diverge from the social
issues and topics involving police-brutality related and racially-motivated
killings of Black individuals due to the shape of its topical graph that topics
and conversations encircling the largest component directly relate to the topic
of Black Lives Matter. Finally, we see that both Blue Lives Matter and All
Lives Matter movements depict a different directive, as the topics of Blue
Lives Matter or All Lives Matter do not reside in the center. These findings
suggest that topics and conversations within each social movement are skewed,
random or possessed racially-related undertones, and thus, deviating from the
prominent social injustice issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style Control for Schema-Guided Natural Language Generation. (arXiv:2109.12211v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12211">
<div class="article-summary-box-inner">
<span><p>Natural Language Generation (NLG) for task-oriented dialogue systems focuses
on communicating specific content accurately, fluently, and coherently. While
these attributes are crucial for a successful dialogue, it is also desirable to
simultaneously accomplish specific stylistic goals, such as response length,
point-of-view, descriptiveness, sentiment, formality, and empathy. In this
work, we focus on stylistic control and evaluation for schema-guided NLG, with
joint goals of achieving both semantic and stylistic control. We experiment in
detail with various controlled generation methods for large pretrained language
models: specifically, conditional training, guided fine-tuning, and guided
decoding. We discuss their advantages and limitations, and evaluate them with a
broad range of automatic and human evaluation metrics. Our results show that
while high style accuracy and semantic correctness are easier to achieve for
more lexically-defined styles with conditional training, stylistic control is
also achievable for more semantically complex styles using discriminator-based
guided decoding methods. The results also suggest that methods that are more
scalable (with less hyper-parameters tuning) and that disentangle content
generation and stylistic variations are more effective at achieving semantic
correctness and style accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog. (arXiv:2109.12212v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12212">
<div class="article-summary-box-inner">
<span><p>Online conversations include more than just text. Increasingly, image-based
responses such as memes and animated gifs serve as culturally recognized and
often humorous responses in conversation. However, while NLP has broadened to
multimodal models, conversational dialog systems have largely focused only on
generating text replies. Here, we introduce a new dataset of 1.56M text-gif
conversation turns and introduce a new multimodal conversational model Pepe the
King Prawn for selecting gif-based replies. We demonstrate that our model
produces relevant and high-quality gif responses and, in a large randomized
control trial of multiple models replying to real users, we show that our model
replies with gifs that are significantly better received by the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation. (arXiv:2109.12242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12242">
<div class="article-summary-box-inner">
<span><p>Radiology report generation aims at generating descriptive text from
radiology images automatically, which may present an opportunity to improve
radiology reporting and interpretation. A typical setting consists of training
encoder-decoder models on image-report pairs with a cross entropy loss, which
struggles to generate informative sentences for clinical diagnoses since normal
findings dominate the datasets. To tackle this challenge and encourage more
clinically-accurate text outputs, we propose a novel weakly supervised
contrastive loss for medical report generation. Experimental results
demonstrate that our method benefits from contrasting target reports with
incorrect but semantically-close ones. It outperforms previous work on both
clinical correctness and text generation metrics for two public benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?. (arXiv:2109.12243v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12243">
<div class="article-summary-box-inner">
<span><p>We analyze the grounded SCAN (gSCAN) benchmark, which was recently proposed
to study systematic generalization for grounded language understanding. First,
we study which aspects of the original benchmark can be solved by commonly used
methods in multi-modal research. We find that a general-purpose
Transformer-based model with cross-modal attention achieves strong performance
on a majority of the gSCAN splits, surprisingly outperforming more specialized
approaches from prior work. Furthermore, our analysis suggests that many of the
remaining errors reveal the same fundamental challenge in systematic
generalization of linguistic constructs regardless of visual context. Second,
inspired by this finding, we propose challenging new tasks for gSCAN by
generating data to incorporate relations between objects in the visual
environment. Finally, we find that current models are surprisingly data
inefficient given the narrow scope of commands in gSCAN, suggesting another
challenge for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features. (arXiv:2109.12258v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12258">
<div class="article-summary-box-inner">
<span><p>We report two essential improvements in readability assessment: 1. three
novel features in advanced semantics and 2. the timely evidence that
traditional ML models (e.g. Random Forest, using handcrafted features) can
combine with transformers (e.g. RoBERTa) to augment model performance. First,
we explore suitable transformers and traditional ML models. Then, we extract
255 handcrafted linguistic features using self-developed extraction software.
Finally, we assemble those to create several hybrid models, achieving
state-of-the-art (SOTA) accuracy on popular datasets in readability assessment.
The use of handcrafted features help model performance on smaller datasets.
Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification
accuracy of 99%, a 20.3% increase from the previous SOTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering. (arXiv:2109.12264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12264">
<div class="article-summary-box-inner">
<span><p>Textual Question Answering (QA) aims to provide precise answers to user's
questions in natural language using unstructured data. One of the most popular
approaches to this goal is machine reading comprehension(MRC). In recent years,
many novel datasets and evaluation metrics based on classical MRC tasks have
been proposed for broader textual QA tasks. In this paper, we survey 47 recent
textual QA benchmark datasets and propose a new taxonomy from an application
point of view. In addition, We summarize 8 evaluation metrics of textual QA
tasks. Finally, we discuss current trends in constructing textual QA benchmarks
and suggest directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Learning to Repair Code and Generate Commit Message. (arXiv:2109.12296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12296">
<div class="article-summary-box-inner">
<span><p>We propose a novel task of jointly repairing program codes and generating
commit messages. Code repair and commit message generation are two essential
and related tasks for software development. However, existing work usually
performs the two tasks independently. We construct a multilingual triple
dataset including buggy code, fixed code, and commit messages for this novel
task. We provide the cascaded models as baseline, which are enhanced with
different training approaches, including the teacher-student method, the
multi-task method, and the back-translation method. To deal with the error
propagation problem of the cascaded method, the joint model is proposed that
can both repair the code and generate the commit message in a unified
framework. Experimental results show that the enhanced cascaded model with
teacher-student method and multitask-learning method achieves the best score on
different metrics of automated code repair, and the joint model behaves better
than the cascaded model on commit message generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuning Transformer Models to Build ASAG System. (arXiv:2109.12300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12300">
<div class="article-summary-box-inner">
<span><p>Research towards creating systems for automatic grading of student answers to
quiz and exam questions in educational settings has been ongoing since 1966.
Over the years, the problem was divided into many categories. Among them,
grading text answers were divided into short answer grading, and essay grading.
The goal of this work was to develop an ML-based short answer grading system. I
hence built a system which uses finetuning on Roberta Large Model pretrained on
STS benchmark dataset and have also created an interface to show the production
readiness of the system. I evaluated the performance of the system on the
Mohler extended dataset and SciEntsBank Dataset. The developed system achieved
a Pearsons Correlation of 0.82 and RMSE of 0.7 on the Mohler Dataset which
beats the SOTA performance on this dataset which is correlation of 0.805 and
RMSE of 0.793. Additionally, Pearsons Correlation of 0.79 and RMSE of 0.56 was
achieved on the SciEntsBank Dataset, which only reconfirms the robustness of
the system. A few observations during achieving these results included usage of
batch size of 1 produced better results than using batch size of 16 or 32 and
using huber loss as loss function performed well on this regression task. The
system was tried and tested on train and validation splits using various random
seeds and still has been tweaked to achieve a minimum of 0.76 of correlation
and a maximum 0.15 (out of 1) RMSE on any dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Neural Templates for Recommender Dialogue System. (arXiv:2109.12302v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12302">
<div class="article-summary-box-inner">
<span><p>Though recent end-to-end neural models have shown promising progress on
Conversational Recommender System (CRS), two key challenges still remain.
First, the recommended items cannot be always incorporated into the generated
replies precisely and appropriately. Second, only the items mentioned in the
training corpus have a chance to be recommended in the conversation. To tackle
these challenges, we introduce a novel framework called NTRD for recommender
dialogue system that decouples the dialogue generation from the item
recommendation. NTRD has two key components, i.e., response template generator
and item selector. The former adopts an encoder-decoder model to generate a
response template with slot locations tied to target items, while the latter
fills in slot locations with the proper items using a sufficient attention
mechanism. Our approach combines the strengths of both classical slot filling
approaches (that are generally controllable) and modern neural NLG approaches
(that are generally more natural and accurate). Extensive experiments on the
benchmark ReDial show our NTRD significantly outperforms the previous
state-of-the-art methods. Besides, our approach has the unique advantage to
produce novel items that do not appear in the training set of dialogue corpus.
The code is available at \url{https://github.com/jokieleung/NTRD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Graph-Based Neural Model for End-to-End Frame Semantic Parsing. (arXiv:2109.12319v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12319">
<div class="article-summary-box-inner">
<span><p>Frame semantic parsing is a semantic analysis task based on FrameNet which
has received great attention recently. The task usually involves three subtasks
sequentially: (1) target identification, (2) frame classification and (3)
semantic role labeling. The three subtasks are closely related while previous
studies model them individually, which ignores their intern connections and
meanwhile induces error propagation problem. In this work, we propose an
end-to-end neural model to tackle the task jointly. Concretely, we exploit a
graph-based method, regarding frame semantic parsing as a graph construction
problem. All predicates and roles are treated as graph nodes, and their
relations are taken as graph edges. Experiment results on two benchmark
datasets of frame semantic parsing show that our method is highly competitive,
resulting in better performance than pipeline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DziriBERT: a Pre-trained Language Model for the Algerian Dialect. (arXiv:2109.12346v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12346">
<div class="article-summary-box-inner">
<span><p>Pre-trained transformers are now the de facto models in Natural Language
Processing given their state-of-the-art results in many tasks and languages.
However, most of the current models have been trained on languages for which
large text resources are already available (such as English, French, Arabic,
etc.). Therefore, there is still a number of low-resource languages that need
more attention from the community. In this paper, we study the Algerian dialect
which has several specificities that make the use of Arabic or multilingual
models inappropriate. To address this issue, we collected more than one Million
Algerian tweets, and pre-trained the first Algerian language model: DziriBERT.
When compared to existing models, DziriBERT achieves the best results on two
Algerian downstream datasets. The obtained results show that pre-training a
dedicated model on a small dataset (150 MB) can outperform existing models that
have been trained on much more data (hundreds of GB). Finally, our model is
publicly available to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Reasoning with Context-Aware Linearization for Interpretable Fact Extraction and Verification. (arXiv:2109.12349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12349">
<div class="article-summary-box-inner">
<span><p>This paper presents an end-to-end system for fact extraction and verification
using textual and tabular evidence, the performance of which we demonstrate on
the FEVEROUS dataset. We experiment with both a multi-task learning paradigm to
jointly train a graph attention network for both the task of evidence
extraction and veracity prediction, as well as a single objective graph model
for solely learning veracity prediction and separate evidence extraction. In
both instances, we employ a framework for per-cell linearization of tabular
evidence, thus allowing us to treat evidence from tables as sequences. The
templates we employ for linearizing tables capture the context as well as the
content of table data. We furthermore provide a case study to show the
interpretability our approach. Our best performing system achieves a FEVEROUS
score of 0.23 and 53% label accuracy on the blind test data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Priming for Cross-Lingual Event Extraction. (arXiv:2109.12383v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12383">
<div class="article-summary-box-inner">
<span><p>We present a novel, language-agnostic approach to "priming" language models
for the task of event extraction, providing particularly effective performance
in low-resource and zero-shot cross-lingual settings. With priming, we augment
the input to the transformer stack's language model differently depending on
the question(s) being asked of the model at runtime. For instance, if the model
is being asked to identify arguments for the trigger "protested", we will
provide that trigger as part of the input to the language model, allowing it to
produce different representations for candidate arguments than when it is asked
about arguments for the trigger "arrest" elsewhere in the same sentence. We
show that by enabling the language model to better compensate for the deficits
of sparse and noisy training data, our approach improves both trigger and
argument detection and classification significantly over the state of the art
in a zero-shot cross-lingual setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sorting through the noise: Testing robustness of information processing in pre-trained language models. (arXiv:2109.12393v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12393">
<div class="article-summary-box-inner">
<span><p>Pre-trained LMs have shown impressive performance on downstream NLP tasks,
but we have yet to establish a clear understanding of their sophistication when
it comes to processing, retaining, and applying information presented in their
input. In this paper we tackle a component of this question by examining
robustness of models' ability to deploy relevant context information in the
face of distracting content. We present models with cloze tasks requiring use
of critical context information, and introduce distracting content to test how
robustly the models retain and use that critical information for prediction. We
also systematically manipulate the nature of these distractors, to shed light
on dynamics of models' use of contextual cues. We find that although models
appear in simple contexts to make predictions based on understanding and
applying relevant facts from prior context, the presence of distracting but
irrelevant content has clear impact in confusing model predictions. In
particular, models appear particularly susceptible to factors of semantic
similarity and word position. The findings are consistent with the conclusion
that LM predictions are driven in large part by superficial contextual cues,
rather than by robust representations of context meaning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Latent Space Clustering in Multi-filter Seq2Seq Model: A Reinforcement Learning Approach. (arXiv:2109.12399v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12399">
<div class="article-summary-box-inner">
<span><p>In sequence-to-sequence language processing tasks, sentences with
heterogeneous semantics or grammatical structures may increase the difficulty
of convergence while training the network. To resolve this problem, we
introduce a model that concentrates the each of the heterogeneous features in
the input-output sequences. Build upon the encoder-decoder architecture, we
design a latent-enhanced multi-filter seq2seq model (LMS2S) that analyzes the
latent space representations using a clustering algorithm. The representations
are generated from an encoder and a latent space enhancer. A cluster classifier
is applied to group the representations into clusters. A soft actor-critic
reinforcement learning algorithm is applied to the cluster classifier to
enhance the clustering quality by maximizing the Silhouette score. Then,
multiple filters are trained by the features only from their corresponding
clusters, the heterogeneity of the training data can be resolved accordingly.
Our experiments on semantic parsing and machine translation demonstrate the
positive correlation between the clustering quality and the model's
performance, as well as show the enhancement our model has made with respect to
the ordinary encoder-decoder model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MINIMAL: Mining Models for Data Free Universal Adversarial Triggers. (arXiv:2109.12406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12406">
<div class="article-summary-box-inner">
<span><p>It is well known that natural language models are vulnerable to adversarial
attacks, which are mostly input-specific in nature. Recently, it has been shown
that there also exist input-agnostic attacks in NLP models, called universal
adversarial triggers. However, existing methods to craft universal triggers are
data intensive. They require large amounts of data samples to generate
adversarial triggers, which are typically inaccessible by attackers. For
instance, previous works take 3000 data samples per class for the SNLI dataset
to generate adversarial triggers. In this paper, we present a novel data-free
approach, MINIMAL, to mine input-agnostic adversarial triggers from models.
Using the triggers produced with our data-free algorithm, we reduce the
accuracy of Stanford Sentiment Treebank's positive class from 93.6% to 9.6%.
Similarly, for the Stanford Natural Language Inference (SNLI), our single-word
trigger reduces the accuracy of the entailment class from 90.95% to less than
0.6\%. Despite being completely data-free, we get equivalent accuracy drops as
data-dependent methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coreference Resolution for the Biomedical Domain: A Survey. (arXiv:2109.12424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12424">
<div class="article-summary-box-inner">
<span><p>Issues with coreference resolution are one of the most frequently mentioned
challenges for information extraction from the biomedical literature. Thus, the
biomedical genre has long been the second most researched genre for coreference
resolution after the news domain, and the subject of a great deal of research
for NLP in general. In recent years this interest has grown enormously leading
to the development of a number of substantial datasets, of domain-specific
contextual language models, and of several architectures. In this paper we
review the state-of-the-art of coreference in the biomedical domain with a
particular attention on these most recent developments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deciding Whether to Ask Clarifying Questions in Large-Scale Spoken Language Understanding. (arXiv:2109.12451v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12451">
<div class="article-summary-box-inner">
<span><p>A large-scale conversational agent can suffer from understanding user
utterances with various ambiguities such as ASR ambiguity, intent ambiguity,
and hypothesis ambiguity. When ambiguities are detected, the agent should
engage in a clarifying dialog to resolve the ambiguities before committing to
actions. However, asking clarifying questions for all the ambiguity occurrences
could lead to asking too many questions, essentially hampering the user
experience. To trigger clarifying questions only when necessary for the user
satisfaction, we propose a neural self-attentive model that leverages the
hypotheses with ambiguities and contextual signals. We conduct extensive
experiments on five common ambiguity types using real data from a large-scale
commercial conversational agent and demonstrate significant improvement over a
set of baseline approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Selectively Learn for Weakly-supervised Paraphrase Generation. (arXiv:2109.12457v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12457">
<div class="article-summary-box-inner">
<span><p>Paraphrase generation is a longstanding NLP task that has diverse
applications for downstream NLP tasks. However, the effectiveness of existing
efforts predominantly relies on large amounts of golden labeled data. Though
unsupervised endeavors have been proposed to address this issue, they may fail
to generate meaningful paraphrases due to the lack of supervision signals. In
this work, we go beyond the existing paradigms and propose a novel approach to
generate high-quality paraphrases with weak supervision data. Specifically, we
tackle the weakly-supervised paraphrase generation problem by: (1) obtaining
abundant weakly-labeled parallel sentences via retrieval-based pseudo
paraphrase expansion; and (2) developing a meta-learning framework to
progressively select valuable samples for fine-tuning a pre-trained language
model, i.e., BART, on the sentential paraphrasing task. We demonstrate that our
approach achieves significant improvements over existing unsupervised
approaches, and is even comparable in performance with supervised
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Refinements for Lexically Constrained Text Generation with BART. (arXiv:2109.12487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12487">
<div class="article-summary-box-inner">
<span><p>Lexically constrained text generation aims to control the generated text by
incorporating some pre-specified keywords into the output. Previous work
injects lexical constraints into the output by controlling the decoding process
or refining the candidate output iteratively, which tends to generate generic
or ungrammatical sentences, and has high computational complexity. To address
these challenges, we propose Constrained BART (CBART) for lexically constrained
text generation. CBART leverages the pre-trained model BART and transfers part
of the generation burden from the decoder to the encoder by decomposing this
task into two sub-tasks, thereby improving the sentence quality. Concretely, we
extend BART by adding a token-level classifier over the encoder, aiming at
instructing the decoder where to replace and insert. Guided by the encoder, the
decoder refines multiple tokens of the input in one step by inserting tokens
before specific positions and re-predicting tokens with low confidence. To
further reduce the inference latency, the decoder predicts all tokens in
parallel. Experiment results on One-Billion-Word and Yelp show that CBART can
generate plausible text with high quality and diversity while significantly
accelerating inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Electoral Programs of German Parties 2021: A Computational Analysis Of Their Comprehensibility and Likeability Based On SentiArt. (arXiv:2109.12500v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12500">
<div class="article-summary-box-inner">
<span><p>The electoral programs of six German parties issued before the parliamentary
elections of 2021 are analyzed using state-of-the-art computational tools for
quantitative narrative, topic and sentiment analysis. We compare different
methods for computing the textual similarity of the programs, Jaccard Bag
similarity, Latent Semantic Analysis, doc2vec, and sBERT, the representational
and computational complexity increasing from the 1st to the 4th method. A new
similarity measure for entire documents derived from the Fowlkes Mallows Score
is applied to kmeans clustering of sBERT transformed sentences. Using novel
indices of the readability and emotion potential of texts computed via SentiArt
(Jacobs, 2019), our data shed light on the similarities and differences of the
programs regarding their length, main ideas, comprehensibility, likeability,
and semantic complexity. Among others, they reveal that the programs of the SPD
and CDU have the best chances to be comprehensible and likeable -all other
things being equal-, and they raise the important issue of which similarity
measure is optimal for comparing texts such as electoral programs which
necessarily share a lot of words. While such analyses can not replace
qualitative analyses or a deep reading of the texts, they offer predictions
that can be verified in empirical studies and may serve as a motivation for
changing aspects of future electoral programs potentially making them more
comprehensible and/or likeable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Linking Meets Deep Learning: Techniques and Solutions. (arXiv:2109.12520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12520">
<div class="article-summary-box-inner">
<span><p>Entity linking (EL) is the process of linking entity mentions appearing in
web text with their corresponding entities in a knowledge base. EL plays an
important role in the fields of knowledge engineering and data mining,
underlying a variety of downstream applications such as knowledge base
population, content analysis, relation extraction, and question answering. In
recent years, deep learning (DL), which has achieved tremendous success in
various domains, has also been leveraged in EL methods to surpass traditional
machine learning based methods and yield the state-of-the-art performance. In
this survey, we present a comprehensive review and analysis of existing DL
based EL methods. First of all, we propose a new taxonomy, which organizes
existing DL based EL methods using three axes: embedding, feature, and
algorithm. Then we systematically survey the representative EL methods along
the three axes of the taxonomy. Later, we introduce ten commonly used EL data
sets and give a quantitative performance analysis of DL based EL methods over
these data sets. Finally, we discuss the remaining limitations of existing
methods and highlight some promising future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioCopy: A Plug-And-Play Span Copy Mechanism in Seq2Seq Models. (arXiv:2109.12533v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12533">
<div class="article-summary-box-inner">
<span><p>Copy mechanisms explicitly obtain unchanged tokens from the source (input)
sequence to generate the target (output) sequence under the neural seq2seq
framework. However, most of the existing copy mechanisms only consider single
word copying from the source sentences, which results in losing essential
tokens while copying long spans. In this work, we propose a plug-and-play
architecture, namely BioCopy, to alleviate the problem aforementioned.
Specifically, in the training stage, we construct a BIO tag for each token and
train the original model with BIO tags jointly. In the inference stage, the
model will firstly predict the BIO tag at each time step, then conduct
different mask strategies based on the predicted BIO label to diminish the
scope of the probability distributions over the vocabulary list. Experimental
results on two separate generative tasks show that they all outperform the
baseline models by adding our BioCopy to the original model structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XLM-K: Improving Cross-Lingual Language Model Pre-Training with Multilingual Knowledge. (arXiv:2109.12573v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12573">
<div class="article-summary-box-inner">
<span><p>Cross-lingual pre-training has achieved great successes using monolingual and
bilingual plain text corpora. However, existing pre-trained models neglect
multilingual knowledge, which is language agnostic but comprises abundant
cross-lingual structure alignment. In this paper, we propose XLM-K, a
cross-lingual language model incorporating multilingual knowledge in
pre-training. XLM-K augments existing multilingual pre-training with two
knowledge tasks, namely Masked Entity Prediction Task and Object Entailment
Task. We evaluate XLM-K on MLQA, NER and XNLI. Experimental results clearly
demonstrate significant improvements over existing multilingual language
models. The results on MLQA and NER exhibit the superiority of XLM-K in
knowledge related tasks. The success in XNLI shows a better cross-lingual
transferability obtained in XLM-K. What is more, we provide a detailed probing
analysis to confirm the desired knowledge captured in our pre-training regimen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paradigm Shift in Natural Language Processing. (arXiv:2109.12575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12575">
<div class="article-summary-box-inner">
<span><p>In the era of deep learning, modeling for most NLP tasks has converged to
several mainstream paradigms. For example, we usually adopt the sequence
labeling paradigm to solve a bundle of tasks such as POS-tagging, NER,
Chunking, and adopt the classification paradigm to solve tasks like sentiment
analysis. With the rapid progress of pre-trained language models, recent years
have observed a rising trend of Paradigm Shift, which is solving one NLP task
by reformulating it as another one. Paradigm shift has achieved great success
on many tasks, becoming a promising way to improve model performance. Moreover,
some of these paradigms have shown great potential to unify a large number of
NLP tasks, making it possible to build a single model to handle diverse tasks.
In this paper, we review such phenomenon of paradigm shifts in recent years,
highlighting several paradigms that have the potential to solve different NLP
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12584">
<div class="article-summary-box-inner">
<span><p>In recent times, there has been definitive progress in the field of NLP, with
its applications growing as the utility of our language models increases with
advances in their performance. However, these models require a large amount of
computational power and data to train, consequently leading to large carbon
footprints. Therefore, is it imperative that we study the carbon efficiency and
look for alternatives to reduce the overall environmental impact of training
models, in particular large language models. In our work, we assess the
performance of models for machine translation, across multiple language pairs
to assess the difference in computational power required to train these models
for each of these language pairs and examine the various components of these
models to analyze aspects of our pipeline that can be optimized to reduce these
carbon emissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents. (arXiv:2109.12595v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12595">
<div class="article-summary-box-inner">
<span><p>We propose MultiDoc2Dial, a new task and dataset on modeling goal-oriented
dialogues grounded in multiple documents. Most previous works treat
document-grounded dialogue modeling as a machine reading comprehension task
based on a single given document or passage. In this work, we aim to address
more realistic scenarios where a goal-oriented information-seeking conversation
involves multiple topics, and hence is grounded on different documents. To
facilitate such a task, we introduce a new dataset that contains dialogues
grounded in multiple documents from four different domains. We also explore
modeling the dialogue-based and document-based context in the dataset. We
present strong baseline approaches and various experimental results, aiming to
support further research efforts on such a task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings. (arXiv:2109.12599v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12599">
<div class="article-summary-box-inner">
<span><p>Learning sentence embeddings from dialogues has drawn increasing attention
due to its low annotation cost and high domain adaptability. Conventional
approaches employ the siamese-network for this task, which obtains the sentence
embeddings through modeling the context-response semantic relevance by applying
a feed-forward network on top of the sentence encoders. However, as the
semantic textual similarity is commonly measured through the element-wise
distance metrics (e.g. cosine and L2 distance), such architecture yields a
large gap between training and evaluating. In this paper, we propose
DialogueCSE, a dialogue-based contrastive learning approach to tackle this
issue. DialogueCSE first introduces a novel matching-guided embedding (MGE)
mechanism, which generates a context-aware embedding for each candidate
response embedding (i.e. the context-free embedding) according to the guidance
of the multi-turn context-response matching matrices. Then it pairs each
context-aware embedding with its corresponding context-free embedding and
finally minimizes the contrastive loss across all pairs. We evaluate our model
on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing
Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results
show that our approach significantly outperforms the baselines across all three
datasets in terms of MAP and Spearman's correlation measures, demonstrating its
effectiveness. Further quantitative experiments show that our approach achieves
better performance when leveraging more dialogue context and remains robust
when less training data is provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Analysis of Euclidean vs. Graph-Based Framing for Bilingual Lexicon Induction from Word Embedding Spaces. (arXiv:2109.12640v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12640">
<div class="article-summary-box-inner">
<span><p>Much recent work in bilingual lexicon induction (BLI) views word embeddings
as vectors in Euclidean space. As such, BLI is typically solved by finding a
linear transformation that maps embeddings to a common space. Alternatively,
word embeddings may be understood as nodes in a weighted graph. This framing
allows us to examine a node's graph neighborhood without assuming a linear
transform, and exploits new techniques from the graph matching optimization
literature. These contrasting approaches have not been compared in BLI so far.
In this work, we study the behavior of Euclidean versus graph-based approaches
to BLI under differing data conditions and show that they complement each other
when combined. We release our code at
https://github.com/kellymarchisio/euc-v-graph-bli.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QA-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions. (arXiv:2109.12655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12655">
<div class="article-summary-box-inner">
<span><p>Multi-text applications, such as multi-document summarization, are typically
required to model redundancies across related texts. Current methods
confronting consolidation struggle to fuse overlapping information. In order to
explicitly represent content overlap, we propose to align predicate-argument
relations across texts, providing a potential scaffold for information
consolidation. We go beyond clustering coreferring mentions, and instead model
overlap with respect to redundancy at a propositional level, rather than merely
detecting shared referents. Our setting exploits QA-SRL, utilizing
question-answer pairs to capture predicate-argument relations, facilitating
laymen annotation of cross-text alignments. We employ crowd-workers for
constructing a dataset of QA-based alignments, and present a baseline QA
alignment model trained over our dataset. Analyses show that our new task is
semantically challenging, capturing content overlap beyond lexical similarity
and complements cross-document coreference with proposition-level links,
offering potential use for downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Question Answering Performance Using Knowledge Distillation and Active Learning. (arXiv:2109.12662v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12662">
<div class="article-summary-box-inner">
<span><p>Contemporary question answering (QA) systems, including transformer-based
architectures, suffer from increasing computational and model complexity which
render them inefficient for real-world applications with limited resources.
Further, training or even fine-tuning such models requires a vast amount of
labeled data which is often not available for the task at hand. In this
manuscript, we conduct a comprehensive analysis of the mentioned challenges and
introduce suitable countermeasures. We propose a novel knowledge distillation
(KD) approach to reduce the parameter and model complexity of a pre-trained
BERT system and utilize multiple active learning (AL) strategies for immense
reduction in annotation efforts. In particular, we demonstrate that our model
achieves the performance of a 6-layer TinyBERT and DistilBERT, whilst using
only 2% of their total parameters. Finally, by the integration of our AL
approaches into the BERT framework, we show that state-of-the-art results on
the SQuAD dataset can be achieved when we only use 20% of the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Prunability of Attention Heads in Multilingual BERT. (arXiv:2109.12683v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12683">
<div class="article-summary-box-inner">
<span><p>Large multilingual models, such as mBERT, have shown promise in crosslingual
transfer. In this work, we employ pruning to quantify the robustness and
interpret layer-wise importance of mBERT. On four GLUE tasks, the relative
drops in accuracy due to pruning have almost identical results on mBERT and
BERT suggesting that the reduced attention capacity of the multilingual models
does not affect robustness to pruning. For the crosslingual task XNLI, we
report higher drops in accuracy with pruning indicating lower robustness in
crosslingual transfer. Also, the importance of the encoder layers sensitively
depends on the language family and the pre-training corpus size. The top
layers, which are relatively more influenced by fine-tuning, encode important
information for languages similar to English (SVO) while the bottom layers,
which are relatively less influenced by fine-tuning, are particularly important
for agglutinative and low-resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting and Inferring Personal Attributes from Dialogue. (arXiv:2109.12702v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12702">
<div class="article-summary-box-inner">
<span><p>Personal attributes represent structured information about a person, such as
their hobbies, pets, family, likes and dislikes. In this work, we introduce the
tasks of extracting and inferring personal attributes from human-human
dialogue. We first demonstrate the benefit of incorporating personal attributes
in a social chit-chat dialogue model and task-oriented dialogue setting. Thus
motivated, we propose the tasks of personal attribute extraction and inference,
and then analyze the linguistic demands of these tasks. To meet these
challenges, we introduce a simple and extensible model that combines an
autoregressive language model utilizing constrained attribute generation with a
discriminative reranker. Our model outperforms strong baselines on extracting
personal attributes as well as inferring personal attributes that are not
contained verbatim in utterances and instead requires commonsense reasoning and
lexical inferences, which occur frequently in everyday conversation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding. (arXiv:2109.12742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12742">
<div class="article-summary-box-inner">
<span><p>The few-shot natural language understanding (NLU) task has attracted much
recent attention. However, prior methods have been evaluated under a disparate
set of protocols, which hinders fair comparison and measuring progress of the
field. To address this issue, we introduce an evaluation framework that
improves previous evaluation procedures in three key aspects, i.e., test
performance, dev-test correlation, and stability. Under this new evaluation
framework, we re-evaluate several state-of-the-art few-shot methods for NLU
tasks. Our framework reveals new insights: (1) both the absolute performance
and relative gap of the methods were not accurately estimated in prior
literature; (2) no single method dominates most tasks with consistent
performance; (3) improvements of some methods diminish with a larger pretrained
model; and (4) gains from different methods are often complementary and the
best combined model performs close to a strong fully-supervised baseline. We
open-source our toolkit, FewNLU, that implements our evaluation framework along
with a number of state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text to Insight: Accelerating Organic Materials Knowledge Extraction via Deep Learning. (arXiv:2109.12758v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12758">
<div class="article-summary-box-inner">
<span><p>Scientific literature is one of the most significant resources for sharing
knowledge. Researchers turn to scientific literature as a first step in
designing an experiment. Given the extensive and growing volume of literature,
the common approach of reading and manually extracting knowledge is too time
consuming, creating a bottleneck in the research cycle. This challenge spans
nearly every scientific domain. For the materials science, experimental data
distributed across millions of publications are extremely helpful for
predicting materials properties and the design of novel materials. However,
only recently researchers have explored computational approaches for knowledge
extraction primarily for inorganic materials. This study aims to explore
knowledge extraction for organic materials. We built a research dataset
composed of 855 annotated and 708,376 unannotated sentences drawn from 92,667
abstracts. We used named-entity-recognition (NER) with BiLSTM-CNN-CRF deep
learning model to automatically extract key knowledge from literature.
Early-phase results show a high potential for automated knowledge extraction.
The paper presents our findings and a framework for supervised knowledge
extraction that can be adapted to other scientific domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts. (arXiv:2109.12761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12761">
<div class="article-summary-box-inner">
<span><p>In order to better simulate the real human conversation process, models need
to generate dialogue utterances based on not only preceding textual contexts
but also visual contexts. However, with the development of multi-modal dialogue
learning, the dataset scale gradually becomes a bottleneck. In this report, we
release OpenViDial 2.0, a larger-scale open-domain multi-modal dialogue dataset
compared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a
total number of 5.6 million dialogue turns extracted from either movies or TV
series from different resources, and each dialogue turn is paired with its
corresponding visual context. We hope this large-scale dataset can help
facilitate future researches on open-domain multi-modal dialog generation,
e.g., multi-modal pretraining for dialogue generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rumour Detection via Zero-shot Cross-lingual Transfer Learning. (arXiv:2109.12773v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12773">
<div class="article-summary-box-inner">
<span><p>Most rumour detection models for social media are designed for one specific
language (mostly English). There are over 40 languages on Twitter and most
languages lack annotated resources to build rumour detection models. In this
paper we propose a zero-shot cross-lingual transfer learning framework that can
adapt a rumour detection model trained for a source language to another target
language. Our framework utilises pretrained multilingual language models (e.g.\
multilingual BERT) and a self-training loop to iteratively bootstrap the
creation of ''silver labels'' in the target language to adapt the model from
the source language to the target language. We evaluate our methodology on
English and Chinese rumour datasets and demonstrate that our model
substantially outperforms competitive benchmarks in both source and target
language rumour detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReINTEL Challenge 2020: A Comparative Study of Hybrid Deep Neural Network for Reliable Intelligence Identification on Vietnamese SNSs. (arXiv:2109.12777v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12777">
<div class="article-summary-box-inner">
<span><p>The overwhelming abundance of data has created a misinformation crisis.
Unverified sensationalism that is designed to grab the readers' short attention
span, when crafted with malice, has caused irreparable damage to our society's
structure. As a result, determining the reliability of an article has become a
crucial task. After various ablation studies, we propose a multi-input model
that can effectively leverage both tabular metadata and post content for the
task. Applying state-of-the-art finetuning techniques for the pretrained
component and training strategies for our complete model, we have achieved a
0.9462 ROC-score on the VLSP private test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Use of Graph Convolution Network and Contextual Sub-Tree forCommodity News Event Extraction. (arXiv:2109.12781v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12781">
<div class="article-summary-box-inner">
<span><p>Event extraction in commodity news is a less researched area as compared to
generic event extraction. However, accurate event extraction from commodity
news is useful in abroad range of applications such as under-standing event
chains and learning event-event relations, which can then be used for commodity
price prediction. The events found in commodity news exhibit characteristics
different from generic events, hence posing a unique challenge in event
extraction using existing methods. This paper proposes an effective use of
Graph Convolutional Networks(GCN) with a pruned dependency parse tree, termed
contextual sub-tree, for better event ex-traction in commodity news. The event
ex-traction model is trained using feature embed-dings from ComBERT, a
BERT-based masked language model that was produced through domain-adaptive
pre-training on a commodity news corpus. Experimental results show the
efficiency of the proposed solution, which out-performs existing methods with
F1 scores as high as 0.90. Furthermore, our pre-trained language model
outperforms GloVe by 23%, and BERT and RoBERTa by 7% in terms of argument roles
classification. For the goal of re-producibility, the code and trained models
are made publicly available1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiplicative Position-aware Transformer Models for Language Understanding. (arXiv:2109.12788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12788">
<div class="article-summary-box-inner">
<span><p>Transformer models, which leverage architectural improvements like
self-attention, perform remarkably well on Natural Language Processing (NLP)
tasks. The self-attention mechanism is position agnostic. In order to capture
positional ordering information, various flavors of absolute and relative
position embeddings have been proposed. However, there is no systematic
analysis on their contributions and a comprehensive comparison of these methods
is missing in the literature. In this paper, we review major existing position
embedding methods and compare their accuracy on downstream NLP tasks, using our
own implementations. We also propose a novel multiplicative embedding method
which leads to superior accuracy when compared to existing methods. Finally, we
show that our proposed embedding method, served as a drop-in replacement of the
default absolute position embedding, can improve the RoBERTa-base and
RoBERTa-large models on SQuAD1.1 and SQuAD2.0 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast-MD: Fast Multi-Decoder End-to-End Speech Translation with Non-Autoregressive Hidden Intermediates. (arXiv:2109.12804v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12804">
<div class="article-summary-box-inner">
<span><p>The multi-decoder (MD) end-to-end speech translation model has demonstrated
high translation quality by searching for better intermediate automatic speech
recognition (ASR) decoder states as hidden intermediates (HI). It is a two-pass
decoding model decomposing the overall task into ASR and machine translation
sub-tasks. However, the decoding speed is not fast enough for real-world
applications because it conducts beam search for both sub-tasks during
inference. We propose Fast-MD, a fast MD model that generates HI by
non-autoregressive (NAR) decoding based on connectionist temporal
classification (CTC) outputs followed by an ASR decoder. We investigated two
types of NAR HI: (1) parallel HI by using an autoregressive Transformer ASR
decoder and (2) masked HI by using Mask-CTC, which combines CTC and the
conditional masked language model. To reduce a mismatch in the ASR decoder
between teacher-forcing during training and conditioning on CTC outputs during
testing, we also propose sampling CTC outputs during training. Experimental
evaluations on three corpora show that Fast-MD achieved about 2x and 4x faster
decoding speed than that of the na\"ive MD model on GPU and CPU with comparable
translation quality. Adopting the Conformer encoder and intermediate CTC loss
further boosts its quality without sacrificing decoding speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Non-local Features for Neural Constituency Parsing. (arXiv:2109.12814v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12814">
<div class="article-summary-box-inner">
<span><p>Thanks to the strong representation power of neural encoders, neural
chart-based parsers have achieved highly competitive performance by using local
features. Recently, it has been shown that non-local features in CRF structures
lead to improvements. In this paper, we investigate injecting non-local
features into the training process of a local span-based parser, by predicting
constituent n-gram non-local patterns and ensuring consistency between
non-local patterns and local constituents. Results show that our simple method
gives better results than the CRF parser on both PTB and CTB. Besides, our
method achieves state-of-the-art BERT-based performance on PTB (95.92 F1) and
strong performance on CTB (92.31 F1).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negative Statements Considered Useful. (arXiv:2001.04425v6 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.04425">
<div class="article-summary-box-inner">
<span><p>Knowledge bases (KBs) about notable entities and their properties are an
important asset in applications such as search, question answering and
dialogue. All popular KBs capture virtually only positive statements, and
abstain from taking any stance on statements not stored in the KB. This paper
makes the case for explicitly stating salient statements that do not hold.
Negative statements are useful to overcome limitations of question answering
systems that are mainly geared for positive questions; they can also contribute
to informative summaries of entities. Due to the abundance of such invalid
statements, any effort to compile them needs to address ranking by saliency. We
present a statisticalinference method for compiling and ranking negative
statements, based on expectations from positive statements of related entities
in peer groups. Experimental results, with a variety of datasets, show that the
method can effectively discover notable negative statements, and extrinsic
studies underline their usefulness for entity summarization. Datasets and code
are released as resources for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VirTex: Learning Visual Representations from Textual Annotations. (arXiv:2006.06666v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.06666">
<div class="article-summary-box-inner">
<span><p>The de-facto approach to many vision tasks is to start from pretrained visual
representations, typically learned via supervised training on ImageNet. Recent
methods have explored unsupervised pretraining to scale to vast quantities of
unlabeled images. In contrast, we aim to learn high-quality visual
representations from fewer images. To this end, we revisit supervised
pretraining, and seek data-efficient alternatives to classification-based
pretraining. We propose VirTex -- a pretraining approach using semantically
dense captions to learn visual representations. We train convolutional networks
from scratch on COCO Captions, and transfer them to downstream recognition
tasks including image classification, object detection, and instance
segmentation. On all tasks, VirTex yields features that match or exceed those
learned on ImageNet -- supervised or unsupervised -- despite using up to ten
times fewer images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Twitter Dataset with Latent Topics, Sentiments and Emotions Attributes. (arXiv:2007.06954v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.06954">
<div class="article-summary-box-inner">
<span><p>This paper describes a large global dataset on people's social media
responses to the COVID-19 pandemic over the Twitter platform. From 28 January
2020 to 1 September 2021, we collected over 198 million Twitter posts from more
than 25 million unique users using four keywords: "corona", "wuhan", "nCov" and
"covid". Leveraging topic modeling techniques and pre-trained machine
learning-based emotion analytic algorithms, we labeled each tweet with
seventeen semantic attributes, including a) ten binary attributes indicating
the tweet's relevance or irrelevance to the top ten detected topics, b) five
quantitative emotion attributes indicating the degree of intensity of the
valence or sentiment (from 0: very negative to 1: very positive), and the
degree of intensity of fear, anger, happiness and sadness emotions (from 0: not
at all to 1: extremely intense), and c) two qualitative attributes indicating
the sentiment category (very negative, negative, neutral or mixed, positive,
very positive) and the dominant emotion category (fear, anger, happiness,
sadness, no specific emotion) the tweet is mainly expressing. We report the
descriptive statistics around these new attributes, their temporal
distributions, and the overall geographic representation of the dataset. The
paper concludes with an outline of the dataset's possible usage in
communication, psychology, public health, economics, and epidemiology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Pretrained Language Models for Graph-to-Text Generation. (arXiv:2007.08426v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.08426">
<div class="article-summary-box-inner">
<span><p>Graph-to-text generation aims to generate fluent texts from graph-based data.
In this paper, we investigate two recently proposed pretrained language models
(PLMs) and analyze the impact of different task-adaptive pretraining strategies
for PLMs in graph-to-text generation. We present a study across three graph
domains: meaning representations, Wikipedia knowledge graphs (KGs) and
scientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art
results and that task-adaptive pretraining strategies improve their performance
even further. In particular, we report new state-of-the-art BLEU scores of
49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative
improvement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis,
we identify possible reasons for the PLMs' success on graph-to-text tasks. We
find evidence that their knowledge about true facts helps them perform well
even when the input graph representation is reduced to a simple bag of node and
edge labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overcoming Conflicting Data when Updating a Neural Semantic Parser. (arXiv:2010.12675v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12675">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore how to use a small amount of new data to update a
task-oriented semantic parsing model when the desired output for some examples
has changed. When making updates in this way, one potential problem that arises
is the presence of conflicting data, or out-of-date labels in the original
training set. To evaluate the impact of this understudied problem, we propose
an experimental setup for simulating changes to a neural semantic parser. We
show that the presence of conflicting data greatly hinders learning of an
update, then explore several methods to mitigate its effect. Our multi-task and
data selection methods lead to large improvements in model accuracy compared to
a naive data-mixing strategy, and our best method closes 86% of the accuracy
gap between this baseline and an oracle upper bound.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Panoramic Survey of Natural Language Processing in the Arab World. (arXiv:2011.12631v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12631">
<div class="article-summary-box-inner">
<span><p>The term natural language refers to any system of symbolic communication
(spoken, signed or written) without intentional human planning and design. This
distinguishes natural languages such as Arabic and Japanese from artificially
constructed languages such as Esperanto or Python. Natural language processing
(NLP) is the sub-field of artificial intelligence (AI) focused on modeling
natural languages to build applications such as speech recognition and
synthesis, machine translation, optical character recognition (OCR), sentiment
analysis (SA), question answering, dialogue systems, etc. NLP is a highly
interdisciplinary field with connections to computer science, linguistics,
cognitive science, psychology, mathematics and others. Some of the earliest AI
applications were in NLP (e.g., machine translation); and the last decade
(2010-2020) in particular has witnessed an incredible increase in quality,
matched with a rise in public awareness, use, and expectations of what may have
seemed like science fiction in the past. NLP researchers pride themselves on
developing language independent models and tools that can be applied to all
human languages, e.g. machine translation systems can be built for a variety of
languages using the same basic mechanisms and models. However, the reality is
that some languages do get more attention (e.g., English and Chinese) than
others (e.g., Hindi and Swahili). Arabic, the primary language of the Arab
world and the religious language of millions of non-Arab Muslims is somewhere
in the middle of this continuum. Though Arabic NLP has many challenges, it has
seen many successes and developments. Next we discuss Arabic's main challenges
as a necessary background, and we present a brief history of Arabic NLP. We
then survey a number of its research areas, and close with a critical
discussion of the future of Arabic NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Investigation of Language Model Interpretability via Sentence Editing. (arXiv:2011.14039v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14039">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) like BERT are being used for almost all
language-related tasks, but interpreting their behavior still remains a
significant challenge and many important questions remain largely unanswered.
In this work, we re-purpose a sentence editing dataset, where faithful
high-quality human rationales can be automatically extracted and compared with
extracted model rationales, as a new testbed for interpretability. This enables
us to conduct a systematic investigation on an array of questions regarding
PLMs' interpretability, including the role of pre-training procedure,
comparison of rationale extraction methods, and different layers in the PLM.
The investigation generates new insights, for example, contrary to the common
understanding, we find that attention weights correlate well with human
rationales and work better than gradient-based saliency in extracting model
rationales. Both the dataset and code are available at
https://github.com/samuelstevens/sentence-editing-interpretability to
facilitate future interpretability research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Variable Models for Visual Question Answering. (arXiv:2101.06399v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06399">
<div class="article-summary-box-inner">
<span><p>Current work on Visual Question Answering (VQA) explore deterministic
approaches conditioned on various types of image and question features. We
posit that, in addition to image and question pairs, other modalities are
useful for teaching machine to carry out question answering. Hence in this
paper, we propose latent variable models for VQA where extra information (e.g.
captions and answer categories) are incorporated as latent variables, which are
observed during training but in turn benefit question-answering performance at
test time. Experiments on the VQA v2.0 benchmarking dataset demonstrate the
effectiveness of our proposed models: they improve over strong baselines,
especially those that do not rely on extensive language-vision pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AuGPT: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models. (arXiv:2102.05126v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05126">
<div class="article-summary-box-inner">
<span><p>Attention-based pre-trained language models such as GPT-2 brought
considerable progress to end-to-end dialogue modelling. However, they also
present considerable risks for task-oriented dialogue, such as lack of
knowledge grounding or diversity. To address these issues, we introduce
modified training objectives for language model finetuning, and we employ
massive data augmentation via back-translation to increase the diversity of the
training data. We further examine the possibilities of combining data from
multiples sources to improve performance on the target dataset. We carefully
evaluate our contributions with both human and automatic methods. Our model
substantially outperforms the baseline on the MultiWOZ data and shows
competitive performance with state of the art in both automatic and human
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intent Recognition and Unsupervised Slot Identification for Low Resourced Spoken Dialog Systems. (arXiv:2104.01287v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01287">
<div class="article-summary-box-inner">
<span><p>Intent Recognition and Slot Identification are crucial components in spoken
language understanding (SLU) systems. In this paper, we present a novel
approach towards both these tasks in the context of low resourced and unwritten
languages. We present an acoustic based SLU system that converts speech to its
phonetic transcription using a universal phone recognition system. We build a
word-free natural language understanding module that does intent recognition
and slot identification from these phonetic transcription. Our proposed SLU
system performs competitively for resource rich scenarios and significantly
outperforms existing approaches as the amount of available data reduces. We
observe more than 10% improvement for intent classification in Tamil and more
than 5% improvement for intent classification in Sinhala. We also present a
novel approach towards unsupervised slot identification using normalized
attention scores. This approach can be used for unsupervised slot labelling,
data augmentation and to generate data for a new slot in a one-shot way with
only one speech recording
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-PLUG: Knowledge-injected Pre-trained Language Model for Natural Language Understanding and Generation in E-Commerce. (arXiv:2104.06960v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06960">
<div class="article-summary-box-inner">
<span><p>Existing pre-trained language models (PLMs) have demonstrated the
effectiveness of self-supervised learning for a broad range of natural language
processing (NLP) tasks. However, most of them are not explicitly aware of
domain-specific knowledge, which is essential for downstream tasks in many
domains, such as tasks in e-commerce scenarios. In this paper, we propose
K-PLUG, a knowledge-injected pre-trained language model based on the
encoder-decoder transformer that can be transferred to both natural language
understanding and generation tasks. We verify our method in a diverse range of
e-commerce scenarios that require domain-specific knowledge. Specifically, we
propose five knowledge-aware self-supervised pre-training objectives to
formulate the learning of domain-specific knowledge, including e-commerce
domain-specific knowledge-bases, aspects of product entities, categories of
product entities, and unique selling propositions of product entities. K-PLUG
achieves new state-of-the-art results on a suite of domain-specific NLP tasks,
including product knowledge base completion, abstractive product summarization,
and multi-turn dialogue, significantly outperforms baselines across the board,
which demonstrates that the proposed method effectively learns a diverse set of
domain-specific knowledge for both language understanding and generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08315">
<div class="article-summary-box-inner">
<span><p>Large language models have shown promising results in zero-shot settings
(Brown et al.,2020; Radford et al., 2019). For example, they can perform
multiple choice tasks simply by conditioning on a question and selecting the
answer with the highest probability.
</p>
<p>However, ranking by string probability can be problematic due to surface form
competition-wherein different surface forms compete for probability mass, even
if they represent the same underlying concept, e.g. "computer" and "PC." Since
probability mass is finite, this lowers the probability of the correct answer,
due to competition from other strings that are valid answers (but not one of
the multiple choice options).
</p>
<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative
scoring function that directly compensates for surface form competition by
simply reweighing each option according to a term that is proportional to its a
priori likelihood within the context of the specific zero-shot task. It
achieves consistent gains in zero-shot performance over both calibrated (Zhao
et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models
over a variety of multiple choice datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explanation-Based Human Debugging of NLP Models: A Survey. (arXiv:2104.15135v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.15135">
<div class="article-summary-box-inner">
<span><p>Debugging a machine learning model is hard since the bug usually involves the
training data and the learning process. This becomes even harder for an opaque
deep learning model if we have no clue about how the model actually works. In
this survey, we review papers that exploit explanations to enable humans to
give feedback and debug NLP models. We call this problem explanation-based
human debugging (EBHD). In particular, we categorize and discuss existing work
along three dimensions of EBHD (the bug context, the workflow, and the
experimental setting), compile findings on how EBHD components affect the
feedback providers, and highlight open problems that could be future research
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04727">
<div class="article-summary-box-inner">
<span><p>We propose FEDENHANCE, an unsupervised federated learning (FL) approach for
speech enhancement and separation with non-IID distributed data across multiple
clients. We simulate a real-world scenario where each client only has access to
a few noisy recordings from a limited and disjoint number of speakers (hence
non-IID). Each client trains their model in isolation using mixture invariant
training while periodically providing updates to a central server. Our
experiments show that our approach achieves competitive enhancement performance
compared to IID training on a single device and that we can further facilitate
the convergence speed and the overall performance using transfer learning on
the server-side. Moreover, we show that we can effectively combine updates from
clients trained locally with supervised and unsupervised losses. We also
release a new dataset LibriFSD50K and its creation recipe in order to
facilitate FL research for source separation problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparsely Overlapped Speech Training in the Time Domain: Joint Learning of Target Speech Separation and Personal VAD Benefits. (arXiv:2106.14371v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14371">
<div class="article-summary-box-inner">
<span><p>Target speech separation is the process of filtering a certain speaker's
voice out of speech mixtures according to the additional speaker identity
information provided. Recent works have made considerable improvement by
processing signals in the time domain directly. The majority of them take fully
overlapped speech mixtures for training. However, since most real-life
conversations occur randomly and are sparsely overlapped, we argue that
training with different overlap ratio data benefits. To do so, an unavoidable
problem is that the popularly used SI-SNR loss has no definition for silent
sources. This paper proposes the weighted SI-SNR loss, together with the joint
learning of target speech separation and personal VAD. The weighted SI-SNR loss
imposes a weight factor that is proportional to the target speaker's duration
and returns zero when the target speaker is absent. Meanwhile, the personal VAD
generates masks and sets non-target speech to silence. Experiments show that
our proposed method outperforms the baseline by 1.73 dB in terms of SDR on
fully overlapped speech, as well as by 4.17 dB and 0.9 dB on sparsely
overlapped speech of clean and noisy conditions. Besides, with slight
degradation in performance, our model could reduce the time costs in inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03200">
<div class="article-summary-box-inner">
<span><p>The increasing use of social media sites in countries like India has given
rise to large volumes of code-mixed data. Sentiment analysis of this data can
provide integral insights into people's perspectives and opinions. Developing
robust explainability techniques which explain why models make their
predictions becomes essential. In this paper, we propose an adequate
methodology to integrate explainable approaches into code-mixed sentiment
analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Fuzzy Attention for Structured Sentiment Analysis. (arXiv:2109.06719v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06719">
<div class="article-summary-box-inner">
<span><p>Attention scorers have achieved success in parsing tasks like semantic and
syntactic dependency parsing. However, in tasks modeled into parsing, like
structured sentiment analysis, "dependency edges" are very sparse which hinders
parser performance. Thus we propose a sparse and fuzzy attention scorer with
pooling layers which improves parser performance and sets the new
state-of-the-art on structured sentiment analysis. We further explore the
parsing modeling on structured sentiment analysis with second-order parsing and
introduce a novel sparse second-order edge building procedure that leads to
significant improvement in parsing performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Entity-Centric Questions Challenge Dense Retrievers. (arXiv:2109.08535v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08535">
<div class="article-summary-box-inner">
<span><p>Open-domain question answering has exploded in popularity recently due to the
success of dense retrieval models, which have surpassed sparse models using
only a few supervised training examples. However, in this paper, we demonstrate
current dense models are not yet the holy grail of retrieval. We first
construct EntityQuestions, a set of simple, entity-rich questions based on
facts from Wikidata (e.g., "Where was Arve Furset born?"), and observe that
dense retrievers drastically underperform sparse methods. We investigate this
issue and uncover that dense retrievers can only generalize to common entities
unless the question pattern is explicitly observed during training. We discuss
two simple solutions towards addressing this critical problem. First, we
demonstrate that data augmentation is unable to fix the generalization problem.
Second, we argue a more robust passage encoder helps facilitate better question
adaptation using specialized question encoders. We hope our work can shed light
on the challenges in creating a robust, universal dense retriever that works
well across different input distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10282">
<div class="article-summary-box-inner">
<span><p>Text recognition is a long-standing research problem for document
digitalization. Existing approaches for text recognition are usually built
based on CNN for image understanding and RNN for char-level text generation. In
addition, another language model is usually needed to improve the overall
accuracy as a post-processing step. In this paper, we propose an end-to-end
text recognition approach with pre-trained image Transformer and text
Transformer models, namely TrOCR, which leverages the Transformer architecture
for both image understanding and wordpiece-level text generation. The TrOCR
model is simple but effective, and can be pre-trained with large-scale
synthetic data and fine-tuned with human-labeled datasets. Experiments show
that the TrOCR model outperforms the current state-of-the-art models on both
printed and handwritten text recognition tasks. The code and models will be
publicly available at https://aka.ms/TrOCR.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Robotic Vision for Space Mining. (arXiv:2109.12109v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12109">
<div class="article-summary-box-inner">
<span><p>Future Moon bases will likely be constructed using resources mined from the
surface of the Moon. The difficulty of maintaining a human workforce on the
Moon and communications lag with Earth means that mining will need to be
conducted using collaborative robots with a high degree of autonomy. In this
paper, we explore the utility of robotic vision towards addressing several
major challenges in autonomous mining in the lunar environment: lack of
satellite positioning systems, navigation in hazardous terrain, and delicate
robot interactions. Specifically, we describe and report the results of robotic
vision algorithms that we developed for Phase 2 of the NASA Space Robotics
Challenge, which was framed in the context of autonomous collaborative robots
for mining on the Moon. The competition provided a simulated lunar environment
that exhibits the complexities alluded to above. We show how machine
learning-enabled vision could help alleviate the challenges posed by the lunar
environment. A robust multi-robot coordinator was also developed to achieve
long-term operation and effective collaboration between robots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Women with Mammographically-Occult Breast Cancer Leveraging GAN-Simulated Mammograms. (arXiv:2109.12113v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12113">
<div class="article-summary-box-inner">
<span><p>Our objective is to show the feasibility of using simulated mammograms to
detect mammographically-occult (MO) cancer in women with dense breasts and a
normal screening mammogram who could be triaged for additional screening with
magnetic resonance imaging (MRI) or ultrasound. We developed a Conditional
Generative Adversarial Network (CGAN) to simulate a mammogram with normal
appearance using the opposite mammogram as the condition. We used a
Convolutional Neural Network (CNN) trained on Radon Cumulative Distribution
Transform (RCDT) processed mammograms to detect MO cancer. For training CGAN,
we used screening mammograms of 1366 women. For MO cancer detection, we used
screening mammograms of 333 women (97 MO cancer) with dense breasts. We
simulated the right mammogram for normal controls and the cancer side for MO
cancer cases. We created two RCDT images, one from a real mammogram pair and
another from a real-simulated mammogram pair. We finetuned a VGG16 on resulting
RCDT images to classify the women with MO cancer. We compared the
classification performance of the CNN trained on fused RCDT images, CNN_{Fused}
to that of trained only on real RCDT images, CNN_{Real}, and to that of trained
only on simulated RCDT images, CNN_{Simulated}. The test AUC for CNN_{Fused}
was 0.77 with a 95% confidence interval (95CI) of [0.71, 0.83], which was
statistically better (p-value &lt; 0.02) than the CNN_{Real} AUC of 0.70 with a
95CI of [0.64, 0.77] and CNN_{Simulated} AUC of 0.68 with a 95CI of [0.62,
0.75]. It showed that CGAN simulated mammograms can help MO cancer detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Use of the Deep Learning Approach to Measure Alveolar Bone Level. (arXiv:2109.12115v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12115">
<div class="article-summary-box-inner">
<span><p>Abstract:
</p>
<p>Aim: The goal was to use a Deep Convolutional Neural Network to measure the
radiographic alveolar bone level to aid periodontal diagnosis.
</p>
<p>Material and methods: A Deep Learning (DL) model was developed by integrating
three segmentation networks (bone area, tooth, cementoenamel junction) and
image analysis to measure the radiographic bone level and assign radiographic
bone loss (RBL) stages. The percentage of RBL was calculated to determine the
stage of RBL for each tooth. A provisional periodontal diagnosis was assigned
using the 2018 periodontitis classification. RBL percentage, staging, and
presumptive diagnosis were compared to the measurements and diagnoses made by
the independent examiners.
</p>
<p>Results: The average Dice Similarity Coefficient (DSC) for segmentation was
over 0.91. There was no significant difference in RBL percentage measurements
determined by DL and examiners (p=0.65). The Area Under the Receiver Operating
Characteristics Curve of RBL stage assignment for stage I, II and III was 0.89,
0.90 and 0.90, respectively. The accuracy of the case diagnosis was 0.85.
</p>
<p>Conclusion: The proposed DL model provides reliable RBL measurements and
image-based periodontal diagnosis using periapical radiographic images.
However, this model has to be further optimized and validated by a larger
number of images to facilitate its application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Map Update Using Dashcam Videos. (arXiv:2109.12131v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12131">
<div class="article-summary-box-inner">
<span><p>Autonomous driving requires 3D maps that provide accurate and up-to-date
information about semantic landmarks. Due to the wider availability and lower
cost of cameras compared with laser scanners, vision-based mapping has
attracted much attention from academia and industry. Among the existing
solutions, Structure-from-Motion (SfM) technology has proved to be feasible for
building 3D maps from crowdsourced data, since it allows unordered images as
input. Previous works on SfM have mainly focused on issues related to building
3D point clouds and calculating camera poses, leaving the issues of automatic
change detection and localization open.
</p>
<p>We propose in this paper an SfM-based solution for automatic map update, with
a focus on real-time change detection and localization. Our solution builds on
comparison of semantic map data (e.g. types and locations of traffic signs).
Through a novel design of the pixel-wise 3D localization algorithm, our system
can locate the objects detected from 2D images in a 3D space, utilizing sparse
SfM point clouds. Experiments with dashcam videos collected from two urban
areas prove that the system is able to locate visible traffic signs in front
along the driving direction with a median distance error of 1.52 meters.
Moreover, it can detect up to 80\% of the changes with a median distance error
of 2.21 meters. The result analysis also shows the potential of significantly
improving the system performance in the future by increasing the accuracy of
the background technology in use, including in particularly the object
detection and point cloud geo-registration algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive Contractive Flow: Improved Contractive Flows with Lipschitz-constrained Self-Attention. (arXiv:2109.12135v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12135">
<div class="article-summary-box-inner">
<span><p>Normalizing flows provide an elegant method for obtaining tractable density
estimates from distributions by using invertible transformations. The main
challenge is to improve the expressivity of the models while keeping the
invertibility constraints intact. We propose to do so via the incorporation of
localized self-attention. However, conventional self-attention mechanisms don't
satisfy the requirements to obtain invertible flows and can't be naively
incorporated into normalizing flows. To address this, we introduce a novel
approach called Attentive Contractive Flow (ACF) which utilizes a special
category of flow-based generative models - contractive flows. We demonstrate
that ACF can be introduced into a variety of state of the art flow models in a
plug-and-play manner. This is demonstrated to not only improve the
representation power of these models (improving on the bits per dim metric),
but also to results in significantly faster convergence in training them.
Qualitative results, including interpolations between test images, demonstrate
that samples are more realistic and capture local correlations in the data
well. We evaluate the results further by performing perturbation analysis using
AWGN demonstrating that ACF models (especially the dot-product variant) show
better and more consistent resilience to additive noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Neural Networks for Blind Image Quality Assessment: Addressing the Data Challenge. (arXiv:2109.12161v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12161">
<div class="article-summary-box-inner">
<span><p>The enormous space and diversity of natural images is usually represented by
a few small-scale human-rated image quality assessment (IQA) datasets. This
casts great challenges to deep neural network (DNN) based blind IQA (BIQA),
which requires large-scale training data that is representative of the natural
image distribution. It is extremely difficult to create human-rated IQA
datasets composed of millions of images due to constraints of subjective
testing. While a number of efforts have focused on design innovations to
enhance the performance of DNN based BIQA, attempts to address the scarcity of
labeled IQA data remain surprisingly missing. To address this data challenge,
we construct so far the largest IQA database, namely Waterloo Exploration-II,
which contains 3,570 pristine reference and around 3.45 million singly and
multiply distorted images. Since subjective testing for such a large dataset is
nearly impossible, we develop a novel mechanism that synthetically assigns
perceptual quality labels to the distorted images. We construct a DNN-based
BIQA model called EONSS, train it on Waterloo Exploration-II, and test it on
nine subject-rated IQA datasets, without any retraining or fine-tuning. The
results show that with a straightforward DNN architecture, EONSS is able to
outperform the very state-of-the-art in BIQA, both in terms of quality
prediction performance and execution speed. This study strongly supports the
view that the quantity and quality of meaningfully annotated training data,
rather than a sophisticated network architecture or training strategy, is the
dominating factor that determines the performance of DNN-based BIQA models.
(Note: Since this is an ongoing project, the final versions of Waterloo
Exploration-II database, quality annotations, and EONSS, will be made publicly
available in the future when it culminates.)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Cross-Modality Domain Adaptation for Segmenting Vestibular Schwannoma and Cochlea with Data Augmentation and Model Ensemble. (arXiv:2109.12169v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12169">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance images (MRIs) are widely used to quantify vestibular
schwannoma and the cochlea. Recently, deep learning methods have shown
state-of-the-art performance for segmenting these structures. However, training
segmentation models may require manual labels in target domain, which is
expensive and time-consuming. To overcome this problem, domain adaptation is an
effective way to leverage information from source domain to obtain accurate
segmentations without requiring manual labels in target domain. In this paper,
we propose an unsupervised learning framework to segment the VS and cochlea.
Our framework leverages information from contrast-enhanced T1-weighted (ceT1-w)
MRIs and its labels, and produces segmentations for T2-weighted MRIs without
any labels in the target domain. We first applied a generator to achieve
image-to-image translation. Next, we ensemble outputs from an ensemble of
different models to obtain final segmentations. To cope with MRIs from
different sites/scanners, we applied various 'online' augmentations during
training to better capture the geometric variability and the variability in
image appearance and quality. Our method is easy to build and produces
promising segmentations, with a mean Dice score of 0.7930 and 0.7432 for VS and
cochlea respectively in the validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLIM: Vision-and-Language Model Pre-training with Masked Language and Image Modeling. (arXiv:2109.12178v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12178">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language Pre-training (VLP) improves model performance for
downstream tasks that require image and text inputs. Current VLP approaches
differ on (i) model architecture (especially image embedders), (ii) loss
functions, and (iii) masking policies. Image embedders are either deep models
like ResNet or linear projections that directly feed image-pixels into the
transformer. Typically, in addition to the Masked Language Modeling (MLM) loss,
alignment-based objectives are used for cross-modality interaction, and RoI
feature regression and classification tasks for Masked Image-Region Modeling
(MIRM). Both alignment and MIRM objectives mostly do not have ground truth.
Alignment-based objectives require pairings of image and text and heuristic
objective functions. MIRM relies on object detectors. Masking policies either
do not take advantage of multi-modality or are strictly coupled with alignments
generated by other models. In this paper, we present Masked Language and Image
Modeling (MLIM) for VLP. MLIM uses two loss functions: Masked Language Modeling
(MLM) loss and image reconstruction (RECON) loss. We propose Modality Aware
Masking (MAM) to boost cross-modality interaction and take advantage of MLM and
RECON losses that separately capture text and image reconstruction quality.
Using MLM + RECON tasks coupled with MAM, we present a simplified VLP
methodology and show that it has better downstream task performance on a
proprietary e-commerce multi-modal dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NanoBatch DPSGD: Exploring Differentially Private learning on ImageNet with low batch sizes on the IPU. (arXiv:2109.12191v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12191">
<div class="article-summary-box-inner">
<span><p>Differentially private SGD (DPSGD) has recently shown promise in deep
learning. However, compared to non-private SGD, the DPSGD algorithm places
computational overheads that can undo the benefit of batching in GPUs.
Microbatching is a standard method to alleviate this and is fully supported in
the TensorFlow Privacy library (TFDP). However, this technique, while improving
training times also reduces the quality of the gradients and degrades the
classification accuracy. Recent works that for example use the JAX framework
show promise in also alleviating this but still show degradation in throughput
from non-private to private SGD on CNNs, and have not yet shown ImageNet
implementations. In our work, we argue that low batch sizes using group
normalization on ResNet-50 can yield high accuracy and privacy on Graphcore
IPUs. This enables DPSGD training of ResNet-50 on ImageNet in just 6 hours (100
epochs) on an IPU-POD16 system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog. (arXiv:2109.12212v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12212">
<div class="article-summary-box-inner">
<span><p>Online conversations include more than just text. Increasingly, image-based
responses such as memes and animated gifs serve as culturally recognized and
often humorous responses in conversation. However, while NLP has broadened to
multimodal models, conversational dialog systems have largely focused only on
generating text replies. Here, we introduce a new dataset of 1.56M text-gif
conversation turns and introduce a new multimodal conversational model Pepe the
King Prawn for selecting gif-based replies. We demonstrate that our model
produces relevant and high-quality gif responses and, in a large randomized
control trial of multiple models replying to real users, we show that our model
replies with gifs that are significantly better received by the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ground material classification and for UAV-based photogrammetric 3D data A 2D-3D Hybrid Approach. (arXiv:2109.12221v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12221">
<div class="article-summary-box-inner">
<span><p>In recent years, photogrammetry has been widely used in many areas to create
photorealistic 3D virtual data representing the physical environment. The
innovation of small unmanned aerial vehicles (sUAVs) has provided additional
high-resolution imaging capabilities with low cost for mapping a relatively
large area of interest. These cutting-edge technologies have caught the US Army
and Navy's attention for the purpose of rapid 3D battlefield reconstruction,
virtual training, and simulations. Our previous works have demonstrated the
importance of information extraction from the derived photogrammetric data to
create semantic-rich virtual environments (Chen et al., 2019). For example, an
increase of simulation realism and fidelity was achieved by segmenting and
replacing photogrammetric trees with game-ready tree models. In this work, we
further investigated the semantic information extraction problem and focused on
the ground material segmentation and object detection tasks. The main
innovation of this work was that we leveraged both the original 2D images and
the derived 3D photogrammetric data to overcome the challenges faced when using
each individual data source. For ground material segmentation, we utilized an
existing convolutional neural network architecture (i.e., 3DMV) which was
originally designed for segmenting RGB-D sensed indoor data. We improved its
performance for outdoor photogrammetric data by introducing a depth pooling
layer in the architecture to take into consideration the distance between the
source images and the reconstructed terrain model. To test the performance of
our improved 3DMV, a ground truth ground material database was created using
data from the One World Terrain (OWT) data repository. Finally, a workflow for
importing the segmented ground materials into a virtual simulation scene was
introduced, and visual results are reported in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bringing Generalization to Deep Multi-view Detection. (arXiv:2109.12227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12227">
<div class="article-summary-box-inner">
<span><p>Multi-view Detection (MVD) is highly effective for occlusion reasoning and is
a mainstream solution in various applications that require accurate top-view
occupancy maps. While recent works using deep learning have made significant
advances in the field, they have overlooked the generalization aspect, which
makes them \emph{impractical for real-world deployment}. The key novelty of our
work is to \emph{formalize} three critical forms of generalization and
\emph{propose experiments to investigate them}: i) generalization across a
varying number of cameras, ii) generalization with varying camera positions,
and finally, iii) generalization to new scenes. We find that existing \sota
models show poor generalization by overfitting to a single scene and camera
configuration. We propose modifications in terms of pre-training, pooling
strategy, regularization, and loss function to an existing state-of-the-art
framework, leading to successful generalization across new camera
configurations and new scenes. We perform a comprehensive set of experiments on
the \wildtrack and \multiviewx datasets to (a) motivate the necessity to
evaluate MVD methods on generalization abilities and (b) demonstrate the
efficacy of the proposed approach. The code is publicly available at
\url{https://github.com/jeetv/GMVD}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-Range Feature Propagating for Natural Image Matting. (arXiv:2109.12252v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12252">
<div class="article-summary-box-inner">
<span><p>Natural image matting estimates the alpha values of unknown regions in the
trimap. Recently, deep learning based methods propagate the alpha values from
the known regions to unknown regions according to the similarity between them.
However, we find that more than 50\% pixels in the unknown regions cannot be
correlated to pixels in known regions due to the limitation of small effective
reception fields of common convolutional neural networks, which leads to
inaccurate estimation when the pixels in the unknown regions cannot be inferred
only with pixels in the reception fields. To solve this problem, we propose
Long-Range Feature Propagating Network (LFPNet), which learns the long-range
context features outside the reception fields for alpha matte estimation.
Specifically, we first design the propagating module which extracts the context
features from the downsampled image. Then, we present Center-Surround Pyramid
Pooling (CSPP) that explicitly propagates the context features from the
surrounding context image patch to the inner center image patch. Finally, we
use the matting module which takes the image, trimap and context features to
estimate the alpha matte. Experimental results demonstrate that the proposed
method performs favorably against the state-of-the-art methods on the
AlphaMatting and Adobe Image Matting datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensor Full Feature Measure and Its Nonconvex Relaxation Applications to Tensor Recovery. (arXiv:2109.12257v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12257">
<div class="article-summary-box-inner">
<span><p>Tensor sparse modeling as a promising approach, in the whole of science and
engineering has been a huge success. As is known to all, various data in
practical application are often generated by multiple factors, so the use of
tensors to represent the data containing the internal structure of multiple
factors came into being. However, different from the matrix case, constructing
reasonable sparse measure of tensor is a relatively difficult and very
important task. Therefore, in this paper, we propose a new tensor sparsity
measure called Tensor Full Feature Measure (FFM). It can simultaneously
describe the feature information of each dimension of the tensor and the
related features between two dimensions, and connect the Tucker rank with the
tensor tube rank. This measurement method can describe the sparse features of
the tensor more comprehensively. On this basis, we establish its non-convex
relaxation, and apply FFM to low rank tensor completion (LRTC) and tensor
robust principal component analysis (TRPCA). LRTC and TRPCA models based on FFM
are proposed, and two efficient Alternating Direction Multiplier Method (ADMM)
algorithms are developed to solve the proposed model. A variety of real
numerical experiments substantiate the superiority of the proposed methods
beyond state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An embarrassingly simple comparison of machine learning algorithms for indoor scene classification. (arXiv:2109.12261v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12261">
<div class="article-summary-box-inner">
<span><p>With the emergence of autonomous indoor robots, the computer vision task of
indoor scene recognition has gained the spotlight. Indoor scene recognition is
a challenging problem in computer vision that relies on local and global
features in a scene. This study aims to compare the performance of five machine
learning algorithms on the task of indoor scene classification to identify the
pros and cons of each classifier. It also provides a comparison of low latency
feature extractors versus enormous feature extractors to understand the
performance effects. Finally, a simple MnasNet based indoor classification
system is proposed, which can achieve 72% accuracy at 23 ms latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data, Assemble: Leveraging Multiple Datasets with Heterogeneous and Partial Labels. (arXiv:2109.12265v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12265">
<div class="article-summary-box-inner">
<span><p>The success of deep learning relies heavily on large datasets with extensive
labels, but we often only have access to several small, heterogeneous datasets
associated with partial labels, particularly in the field of medical imaging.
When learning from multiple datasets, existing challenges include incomparable,
heterogeneous, or even conflicting labeling protocols across datasets. In this
paper, we propose a new initiative--"data, assemble"--which aims to unleash the
full potential of partially labeled data and enormous unlabeled data from an
assembly of datasets. To accommodate the supervised learning paradigm to
partial labels, we introduce a dynamic adapter that encodes multiple visual
tasks and aggregates image features in a question-and-answer manner.
Furthermore, we employ pseudo-labeling and consistency constraints to harness
images with missing labels and to mitigate the domain gap across datasets. From
proof-of-concept studies on three natural imaging datasets and rigorous
evaluations on two large-scale thorax X-ray benchmarks, we discover that
learning from "negative examples" facilitates both classification and
segmentation of classes of interest. This sheds new light on the computer-aided
diagnosis of rare diseases and emerging pandemics, wherein "positive examples"
are hard to collect, yet "negative examples" are relatively easier to assemble.
As a result, besides exceeding the prior art in the NIH ChestXray benchmark,
our model is particularly strong in identifying diseases of minority classes,
yielding over 3-point improvement on average. Remarkably, when using existing
partial labels, our model performance is on-par (p&gt;0.05) with that using a
fully curated dataset with exhaustive labels, eliminating the need for
additional 40% annotation costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Stereopsis from Geometric Synthesis for 6D Object Pose Estimation. (arXiv:2109.12266v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12266">
<div class="article-summary-box-inner">
<span><p>Current monocular-based 6D object pose estimation methods generally achieve
less competitive results than RGBD-based methods, mostly due to the lack of 3D
information. To make up this gap, this paper proposes a 3D geometric volume
based pose estimation method with a short baseline two-view setting. By
constructing a geometric volume in the 3D space, we combine the features from
two adjacent images to the same 3D space. Then a network is trained to learn
the distribution of the position of object keypoints in the volume, and a
robust soft RANSAC solver is deployed to solve the pose in closed form. To
balance accuracy and cost, we propose a coarse-to-fine framework to improve the
performance in an iterative way. The experiments show that our method
outperforms state-of-the-art monocular-based methods, and is robust in
different objects and scenes, especially in serious occlusion situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiTr-Unet: a CNN-Transformer Combined Network for MRI Brain Tumor Segmentation. (arXiv:2109.12271v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12271">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have recently achieved remarkable
success in automatically identifying organs or lesions on 3D medical images.
Meanwhile, vision transformer networks have exhibited exceptional performance
in 2D image classification tasks. Compared with CNNs, transformer networks have
an obvious advantage of extracting long-range features due to their
self-attention algorithm. Therefore, in this paper we present a CNN-Transformer
combined model called BiTr-Unet for brain tumor segmentation on multi-modal MRI
scans. The proposed BiTr-Unet achieves good performance on the BraTS 2021
validation dataset with mean Dice score 0.9076, 0.8392 and 0.8231, and mean
Hausdorff distance 4.5322, 13.4592 and 14.9963 for the whole tumor, tumor core,
and enhancing tumor, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Differentiable and Interpretable Model for VIO with 4 Trainable Parameters. (arXiv:2109.12292v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12292">
<div class="article-summary-box-inner">
<span><p>Monocular visual-inertial odometry (VIO) is a critical problem in robotics
and autonomous driving. Traditional methods solve this problem based on
filtering or optimization. While being fully interpretable, they rely on manual
interference and empirical parameter tuning. On the other hand, learning-based
approaches allow for end-to-end training but require a large number of training
data to learn millions of parameters. However, the non-interpretable and heavy
models hinder the generalization ability. In this paper, we propose a fully
differentiable, interpretable, and lightweight monocular VIO model that
contains only 4 trainable parameters. Specifically, we first adopt Unscented
Kalman Filter as a differentiable layer to predict the pitch and roll, where
the covariance matrices of noise are learned to filter out the noise of the IMU
raw data. Second, the refined pitch and roll are adopted to retrieve a
gravity-aligned BEV image of each frame using differentiable camera projection.
Finally, a differentiable pose estimator is utilized to estimate the remaining
4 DoF poses between the BEV frames. Our method allows for learning the
covariance matrices end-to-end supervised by the pose estimation loss,
demonstrating superior performance to empirical baselines. Experimental results
on synthetic and real-world datasets demonstrate that our simple approach is
competitive with state-of-the-art methods and generalizes well on unseen
scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Patch Convolutional Neural Network for View-based 3D Model Retrieval. (arXiv:2109.12299v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12299">
<div class="article-summary-box-inner">
<span><p>Recently, many view-based 3D model retrieval methods have been proposed and
have achieved state-of-the-art performance. Most of these methods focus on
extracting more discriminative view-level features and effectively aggregating
the multi-view images of a 3D model, but the latent relationship among these
multi-view images is not fully explored. Thus, we tackle this problem from the
perspective of exploiting the relationships between patch features to capture
long-range associations among multi-view images. To capture associations among
views, in this work, we propose a novel patch convolutional neural network
(PCNN) for view-based 3D model retrieval. Specifically, we first employ a CNN
to extract patch features of each view image separately. Secondly, a novel
neural network module named PatchConv is designed to exploit intrinsic
relationships between neighboring patches in the feature space to capture
long-range associations among multi-view images. Then, an adaptive weighted
view layer is further embedded into PCNN to automatically assign a weight to
each view according to the similarity between each view feature and the
view-pooling feature. Finally, a discrimination loss function is employed to
extract the discriminative 3D model feature, which consists of softmax loss
values generated by the fusion lassifier and the specific classifier. Extensive
experimental results on two public 3D model retrieval benchmarks, namely, the
ModelNet40, and ModelNet10, demonstrate that our proposed PCNN can outperform
state-of-the-art approaches, with mAP alues of 93.67%, and 96.23%,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Multi-Instance Learning for Retinal Disease Recognition. (arXiv:2109.12307v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12307">
<div class="article-summary-box-inner">
<span><p>This paper attacks an emerging challenge of multi-modal retinal disease
recognition. Given a multi-modal case consisting of a color fundus photo (CFP)
and an array of OCT B-scan images acquired during an eye examination, we aim to
build a deep neural network that recognizes multiple vision-threatening
diseases for the given case. As the diagnostic efficacy of CFP and OCT is
disease-dependent, the network's ability of being both selective and
interpretable is important. Moreover, as both data acquisition and manual
labeling are extremely expensive in the medical domain, the network has to be
relatively lightweight for learning from a limited set of labeled multi-modal
samples. Prior art on retinal disease recognition focuses either on a single
disease or on a single modality, leaving multi-modal fusion largely
underexplored. We propose in this paper Multi-Modal Multi-Instance Learning
(MM-MIL) for selectively fusing CFP and OCT modalities. Its lightweight
architecture (as compared to current multi-head attention modules) makes it
suited for learning from relatively small-sized datasets. For an effective use
of MM-MIL, we propose to generate a pseudo sequence of CFPs by over sampling a
given CFP. The benefits of this tactic include well balancing instances across
modalities, increasing the resolution of the CFP input, and finding out regions
of the CFP most relevant with respect to the final diagnosis. Extensive
experiments on a real-world dataset consisting of 1,206 multi-modal cases from
1,193 eyes of 836 subjects demonstrate the viability of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hard-sample Guided Hybrid Contrast Learning for Unsupervised Person Re-Identification. (arXiv:2109.12333v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12333">
<div class="article-summary-box-inner">
<span><p>Unsupervised person re-identification (Re-ID) is a promising and very
challenging research problem in computer vision. Learning robust and
discriminative features with unlabeled data is of central importance to Re-ID.
Recently, more attention has been paid to unsupervised Re-ID algorithms based
on clustered pseudo-label. However, the previous approaches did not fully
exploit information of hard samples, simply using cluster centroid or all
instances for contrastive learning. In this paper, we propose a Hard-sample
Guided Hybrid Contrast Learning (HHCL) approach combining cluster-level loss
with instance-level loss for unsupervised person Re-ID. Our approach applies
cluster centroid contrastive loss to ensure that the network is updated in a
more stable way. Meanwhile, introduction of a hard instance contrastive loss
further mines the discriminative information. Extensive experiments on two
popular large-scale Re-ID benchmarks demonstrate that our HHCL outperforms
previous state-of-the-art methods and significantly improves the performance of
unsupervised person Re-ID. The code of our work is available soon at
https://github.com/bupt-ai-cz/HHCL-ReID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting survival of glioblastoma from automatic whole-brain and tumor segmentation of MR images. (arXiv:2109.12334v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12334">
<div class="article-summary-box-inner">
<span><p>Survival prediction models can potentially be used to guide treatment of
glioblastoma patients. However, currently available MR imaging biomarkers
holding prognostic information are often challenging to interpret, have
difficulties generalizing across data acquisitions, or are only applicable to
pre-operative MR data. In this paper we aim to address these issues by
introducing novel imaging features that can be automatically computed from MR
images and fed into machine learning models to predict patient survival. The
features we propose have a direct biological interpretation: They measure the
deformation caused by the tumor on the surrounding brain structures, comparing
the shape of various structures in the patient's brain to their expected shape
in healthy individuals. To obtain the required segmentations, we use an
automatic method that is contrast-adaptive and robust to missing modalities,
making the features generalizable across scanners and imaging protocols. Since
the features we propose do not depend on characteristics of the tumor region
itself, they are also applicable to post-operative images, which have been much
less studied in the context of survival prediction. Using experiments involving
both pre- and post-operative data, we show that the proposed features carry
prognostic value in terms of overall- and progression-free survival, over and
above that of conventional non-imaging features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distribution-sensitive Information Retention for Accurate Binary Neural Network. (arXiv:2109.12338v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12338">
<div class="article-summary-box-inner">
<span><p>Model binarization is an effective method of compressing neural networks and
accelerating their inference process, which enables state-of-the-art models to
run on resource-limited devices. However, a significant performance gap still
exists between the 1-bit model and the 32-bit one. The empirical study shows
that binarization causes a great loss of information in the forward and
backward propagation which harms the performance of binary neural networks
(BNNs), and the limited information representation ability of binarized
parameter is one of the bottlenecks of BNN performance. We present a novel
Distribution-sensitive Information Retention Network (DIR-Net) to retain the
information of the forward activations and backward gradients, which improves
BNNs by distribution-sensitive optimization without increasing the overhead in
the inference process. The DIR-Net mainly relies on two technical
contributions: (1) Information Maximized Binarization (IMB): minimizing the
information loss and the quantization error of weights/activations
simultaneously by balancing and standardizing the weight distribution in the
forward propagation; (2) Distribution-sensitive Two-stage Estimator (DTE):
minimizing the information loss of gradients by gradual distribution-sensitive
approximation of the sign function in the backward propagation, jointly
considering the updating capability and accurate gradient. The DIR-Net
investigates both forward and backward processes of BNNs from the unified
information perspective, thereby provides new insight into the mechanism of
network binarization. Comprehensive experiments on CIFAR-10 and ImageNet
datasets show our DIR-Net consistently outperforms the SOTA binarization
approaches under mainstream and compact architectures. Additionally, we conduct
our DIR-Net on real-world resource-limited devices which achieves 11.1 times
storage saving and 5.4 times speedup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of MGMT Methylation Status of Glioblastoma using Radiomics and Latent Space Shape Features. (arXiv:2109.12339v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12339">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a method for predicting the status of MGMT promoter
methylation in high-grade gliomas. From the available MR images, we segment the
tumor using deep convolutional neural networks and extract both radiomic
features and shape features learned by a variational autoencoder. We
implemented a standard machine learning workflow to obtain predictions,
consisting of feature selection followed by training of a random forest
classification model. We trained and evaluated our method on the
RSNA-ASNR-MICCAI BraTS 2021 challenge dataset and submitted our predictions to
the challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TreeNet: A lightweight One-Shot Aggregation Convolutional Network. (arXiv:2109.12342v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12342">
<div class="article-summary-box-inner">
<span><p>The architecture of deep convolutional networks (CNNs) has evolved for years,
becoming more accurate and faster. However, it is still challenging to design
reasonable network structures that aim at obtaining the best accuracy under a
limited computational budget. In this paper, we propose a Tree block, named
after its appearance, which extends the One-Shot Aggregation (OSA) module while
being more lightweight and flexible. Specifically, the Tree block replaces each
of the $3\times3$ Conv layers in OSA into a stack of shallow residual block
(SRB) and $1\times1$ Conv layer. The $1\times1$ Conv layer is responsible for
dimension increasing and the SRB is fed into the next step. By doing this, when
aggregating the same number of subsequent feature maps, the Tree block has a
deeper network structure while having less model complexity. In addition,
residual connection and efficient channel attention(ECA) is added to the Tree
block to further improve the performance of the network. Based on the Tree
block, we build efficient backbone models calling TreeNets. TreeNet has a
similar network architecture to ResNet, making it flexible to replace ResNet in
various computer vision frameworks. We comprehensively evaluate TreeNet on
common-used benchmarks, including ImageNet-1k for classification, MS COCO for
object detection, and instance segmentation. Experimental results demonstrate
that TreeNet is more efficient and performs favorably against the current
state-of-the-art backbone methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Principled Approach to Failure Analysis and Model Repairment: Demonstration in Medical Imaging. (arXiv:2109.12347v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12347">
<div class="article-summary-box-inner">
<span><p>Machine learning models commonly exhibit unexpected failures post-deployment
due to either data shifts or uncommon situations in the training environment.
Domain experts typically go through the tedious process of inspecting the
failure cases manually, identifying failure modes and then attempting to fix
the model. In this work, we aim to standardise and bring principles to this
process through answering two critical questions: (i) how do we know that we
have identified meaningful and distinct failure types?; (ii) how can we
validate that a model has, indeed, been repaired? We suggest that the quality
of the identified failure types can be validated through measuring the intra-
and inter-type generalisation after fine-tuning and introduce metrics to
compare different subtyping methods. Furthermore, we argue that a model can be
considered repaired if it achieves high accuracy on the failure types while
retaining performance on the previously correct data. We combine these two
ideas into a principled framework for evaluating the quality of both the
identified failure subtypes and model repairment. We evaluate its utility on a
classification and an object detection tasks. Our code is available at
https://github.com/Rokken-lab6/Failure-Analysis-and-Model-Repairment
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning for Mitochondria Segmentation. (arXiv:2109.12363v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12363">
<div class="article-summary-box-inner">
<span><p>Mitochondria segmentation in electron microscopy images is essential in
neuroscience. However, due to the image degradation during the imaging process,
the large variety of mitochondrial structures, as well as the presence of
noise, artifacts and other sub-cellular structures, mitochondria segmentation
is very challenging. In this paper, we propose a novel and effective
contrastive learning framework to learn a better feature representation from
hard examples to improve segmentation. Specifically, we adopt a point sampling
strategy to pick out representative pixels from hard examples in the training
phase. Based on these sampled pixels, we introduce a pixel-wise label-based
contrastive loss which consists of a similarity loss term and a consistency
loss term. The similarity term can increase the similarity of pixels from the
same class and the separability of pixels from different classes in feature
space, while the consistency term is able to enhance the sensitivity of the 3D
model to changes in image content from frame to frame. We demonstrate the
effectiveness of our method on MitoEM dataset as well as FIB-SEM dataset and
show better or on par with state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Compositional Feature Embedding and Similarity Metric for Ultra-Fine-Grained Visual Categorization. (arXiv:2109.12380v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12380">
<div class="article-summary-box-inner">
<span><p>Fine-grained visual categorization (FGVC), which aims at classifying objects
with small inter-class variances, has been significantly advanced in recent
years. However, ultra-fine-grained visual categorization (ultra-FGVC), which
targets at identifying subclasses with extremely similar patterns, has not
received much attention. In ultra-FGVC datasets, the samples per category are
always scarce as the granularity moves down, which will lead to overfitting
problems. Moreover, the difference among different categories is too subtle to
distinguish even for professional experts. Motivated by these issues, this
paper proposes a novel compositional feature embedding and similarity metric
(CECS). Specifically, in the compositional feature embedding module, we
randomly select patches in the original input image, and these patches are then
replaced by patches from the images of different categories or masked out. Then
the replaced and masked images are used to augment the original input images,
which can provide more diverse samples and thus largely alleviate overfitting
problem resulted from limited training samples. Besides, learning with diverse
samples forces the model to learn not only the most discriminative features but
also other informative features in remaining regions, enhancing the
generalization and robustness of the model. In the compositional similarity
metric module, a new similarity metric is developed to improve the
classification performance by narrowing the intra-category distance and
enlarging the inter-category distance. Experimental results on two ultra-FGVC
datasets and one FGVC dataset with recent benchmark methods consistently
demonstrate that the proposed CECS method achieves the state of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Progressive and Coarse-to-fine Registration of Brain MRI via Deformation Field Integration and Non-Rigid Feature Fusion. (arXiv:2109.12384v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12384">
<div class="article-summary-box-inner">
<span><p>Registration of brain MRI images requires to solve a deformation field, which
is extremely difficult in aligning intricate brain tissues, e.g., subcortical
nuclei, etc. Existing efforts resort to decomposing the target deformation
field into intermediate sub-fields with either tiny motions, i.e., progressive
registration stage by stage, or lower resolutions, i.e., coarse-to-fine
estimation of the full-size deformation field. In this paper, we argue that
those efforts are not mutually exclusive, and propose a unified framework for
robust brain MRI registration in both progressive and coarse-to-fine manners
simultaneously. Specifically, building on a dual-encoder U-Net, the
fixed-moving MRI pair is encoded and decoded into multi-scale deformation
sub-fields from coarse to fine. Each decoding block contains two proposed novel
modules: i) in Deformation Field Integration (DFI), a single integrated
sub-field is calculated, warping by which is equivalent to warping
progressively by sub-fields from all previous decoding blocks, and ii) in
Non-rigid Feature Fusion (NFF), features of the fixed-moving pair are aligned
by DFI-integrated sub-field, and then fused to predict a finer sub-field.
Leveraging both DFI and NFF, the target deformation field is factorized into
multi-scale sub-fields, where the coarser fields alleviate the estimate of a
finer one and the finer field learns to make up those misalignments insolvable
by previous coarser ones. The extensive and comprehensive experimental results
on both private and public datasets demonstrate a superior registration
performance of brain MRI images over progressive registration only and
coarse-to-fine estimation only, with an increase by at most 10% in the average
Dice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-source Few-shot Domain Adaptation. (arXiv:2109.12391v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12391">
<div class="article-summary-box-inner">
<span><p>Multi-source Domain Adaptation (MDA) aims to transfer predictive models from
multiple, fully-labeled source domains to an unlabeled target domain. However,
in many applications, relevant labeled source datasets may not be available,
and collecting source labels can be as expensive as labeling the target data
itself. In this paper, we investigate Multi-source Few-shot Domain Adaptation
(MFDA): a new domain adaptation scenario with limited multi-source labels and
unlabeled target data. As we show, existing methods often fail to learn
discriminative features for both source and target domains in the MFDA setting.
Therefore, we propose a novel framework, termed Multi-Source Few-shot
Adaptation Network (MSFAN), which can be trained end-to-end in a
non-adversarial manner. MSFAN operates by first using a type of prototypical,
multi-domain, self-supervised learning to learn features that are not only
domain-invariant but also class-discriminative. Second, MSFAN uses a small,
labeled support set to enforce feature consistency and domain invariance across
domains. Finally, prototypes from multiple sources are leveraged to learn
better classifiers. Compared with state-of-the-art MDA methods, MSFAN improves
the mean classification accuracy over different domain pairs on MFDA by 20.2%,
9.4%, and 16.2% on Office, Office-Home, and DomainNet, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vehicle Detection and Tracking From Surveillance Cameras in Urban Scenes. (arXiv:2109.12414v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12414">
<div class="article-summary-box-inner">
<span><p>Detecting and tracking vehicles in urban scenes is a crucial step in many
traffic-related applications as it helps to improve road user safety among
other benefits. Various challenges remain unresolved in multi-object tracking
(MOT) including target information description, long-term occlusions and fast
motion. We propose a multi-vehicle detection and tracking system following the
tracking-by-detection paradigm that tackles the previously mentioned
challenges. Our MOT method extends an Intersection-over-Union (IOU)-based
tracker with vehicle re-identification features. This allows us to utilize
appearance information to better match objects after long occlusion phases
and/or when object location is significantly shifted due to fast motion. We
outperform our baseline MOT method on the UA-DETRAC benchmark while maintaining
a total processing speed suitable for online use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L$^{2}$NAS: Learning to Optimize Neural Architectures via Continuous-Action Reinforcement Learning. (arXiv:2109.12425v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12425">
<div class="article-summary-box-inner">
<span><p>Neural architecture search (NAS) has achieved remarkable results in deep
neural network design. Differentiable architecture search converts the search
over discrete architectures into a hyperparameter optimization problem which
can be solved by gradient descent. However, questions have been raised
regarding the effectiveness and generalizability of gradient methods for
solving non-convex architecture hyperparameter optimization problems. In this
paper, we propose L$^{2}$NAS, which learns to intelligently optimize and update
architecture hyperparameters via an actor neural network based on the
distribution of high-performing architectures in the search history. We
introduce a quantile-driven training procedure which efficiently trains
L$^{2}$NAS in an actor-critic framework via continuous-action reinforcement
learning. Experiments show that L$^{2}$NAS achieves state-of-the-art results on
NAS-Bench-201 benchmark as well as DARTS search space and Once-for-All
MobileNetV3 search space. We also show that search policies generated by
L$^{2}$NAS are generalizable and transferable across different training
datasets with minimal fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Profiling Neural Blocks and Design Spaces for Mobile Neural Architecture Search. (arXiv:2109.12426v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12426">
<div class="article-summary-box-inner">
<span><p>Neural architecture search automates neural network design and has achieved
state-of-the-art results in many deep learning applications. While recent
literature has focused on designing networks to maximize accuracy, little work
has been conducted to understand the compatibility of architecture design
spaces to varying hardware. In this paper, we analyze the neural blocks used to
build Once-for-All (MobileNetV3), ProxylessNAS and ResNet families, in order to
understand their predictive power and inference latency on various devices,
including Huawei Kirin 9000 NPU, RTX 2080 Ti, AMD Threadripper 2990WX, and
Samsung Note10. We introduce a methodology to quantify the friendliness of
neural blocks to hardware and the impact of their placement in a macro network
on overall network performance via only end-to-end measurements. Based on
extensive profiling results, we derive design insights and apply them to
hardware-specific search space reduction. We show that searching in the reduced
search space generates better accuracy-latency Pareto frontiers than searching
in the original search spaces, customizing architecture search according to the
hardware. Moreover, insights derived from measurements lead to notably higher
ImageNet top-1 scores on all search spaces investigated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Unpaired Translation using Focal Loss for Patch Classification. (arXiv:2109.12431v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12431">
<div class="article-summary-box-inner">
<span><p>Image-to-image translation models transfer images from input domain to output
domain in an endeavor to retain the original content of the image. Contrastive
Unpaired Translation is one of the existing methods for solving such problems.
Significant advantage of this method, compared to competitors, is the ability
to train and perform well in cases where both input and output domains are only
a single image. Another key thing that differentiates this method from its
predecessors is the usage of image patches rather than the whole images. It
also turns out that sampling negatives (patches required to calculate the loss)
from the same image achieves better results than a scenario where the negatives
are sampled from other images in the dataset. This type of approach encourages
mapping of corresponding patches to the same location in relation to other
patches (negatives) while at the same time improves the output image quality
and significantly decreases memory usage as well as the time required to train
the model compared to CycleGAN method used as a baseline. Through a series of
experiments we show that using focal loss in place of cross-entropy loss within
the PatchNCE loss can improve on the model's performance and even surpass the
current state-of-the-art model for image-to-image translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReCal-Net: Joint Region-Channel-Wise Calibrated Network for Semantic Segmentation in Cataract Surgery Videos. (arXiv:2109.12448v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12448">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation in surgical videos is a prerequisite for a broad range
of applications towards improving surgical outcomes and surgical video
analysis. However, semantic segmentation in surgical videos involves many
challenges. In particular, in cataract surgery, various features of the
relevant objects such as blunt edges, color and context variation, reflection,
transparency, and motion blur pose a challenge for semantic segmentation. In
this paper, we propose a novel convolutional module termed as \textit{ReCal}
module, which can calibrate the feature maps by employing region
intra-and-inter-dependencies and channel-region cross-dependencies. This
calibration strategy can effectively enhance semantic representation by
correlating different representations of the same semantic label, considering a
multi-angle local view centering around each pixel. Thus the proposed module
can deal with distant visual characteristics of unique objects as well as
cross-similarities in the visual characteristics of different objects.
Moreover, we propose a novel network architecture based on the proposed module
termed as ReCal-Net. Experimental results confirm the superiority of ReCal-Net
compared to rival state-of-the-art approaches for all relevant objects in
cataract surgery. Moreover, ablation studies reveal the effectiveness of the
ReCal module in boosting semantic segmentation accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of COVID-19 from CXR Images in a 15-class Scenario: an Attempt to Avoid Bias in the System. (arXiv:2109.12453v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12453">
<div class="article-summary-box-inner">
<span><p>As of June 2021, the World Health Organization (WHO) has reported 171.7
million confirmed cases including 3,698,621 deaths from COVID-19. Detecting
COVID-19 and other lung diseases from Chest X-Ray (CXR) images can be very
effective for emergency diagnosis and treatment as CXR is fast and cheap. The
objective of this study is to develop a system capable of detecting COVID-19
along with 14 other lung diseases from CXRs in a fair and unbiased manner. The
proposed system consists of a CXR image selection technique and a deep learning
based model to classify 15 diseases including COVID-19. The proposed CXR
selection technique aims to retain the maximum variation uniformly and
eliminate poor quality CXRs with the goal of reducing the training dataset size
without compromising classifier accuracy. More importantly, it reduces the
often hidden bias and unfairness in decision making. The proposed solution
exhibits a promising COVID-19 detection scheme in a more realistic situation
than most existing studies as it deals with 15 lung diseases together. We hope
the proposed method will have wider adoption in medical image classification
and other related fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auditing AI models for Verified Deployment under Semantic Specifications. (arXiv:2109.12456v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12456">
<div class="article-summary-box-inner">
<span><p>Auditing trained deep learning (DL) models prior to deployment is vital in
preventing unintended consequences. One of the biggest challenges in auditing
is in understanding how we can obtain human-interpretable specifications that
are directly useful to the end-user. We address this challenge through a
sequence of semantically-aligned unit tests, where each unit test verifies
whether a predefined specification (e.g., accuracy over 95%) is satisfied with
respect to controlled and semantically aligned variations in the input space
(e.g., in face recognition, the angle relative to the camera). We perform these
unit tests by directly verifying the semantically aligned variations in an
interpretable latent space of a generative model. Our framework, AuditAI,
bridges the gap between interpretable formal verification and scalability. With
evaluations on four different datasets, covering images of towers, chest
X-rays, human faces, and ImageNet classes, we show how AuditAI allows us to
obtain controlled variations for verification and certified training while
addressing the limitations of verifying using only pixel-space perturbations. A
blog post accompanying the paper is at this link
https://developer.nvidia.com/blog/nvidia-research-auditing-ai-models-for-verified-deployment-under-semantic-specifications
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two Souls in an Adversarial Image: Towards Universal Adversarial Example Detection using Multi-view Inconsistency. (arXiv:2109.12459v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12459">
<div class="article-summary-box-inner">
<span><p>In the evasion attacks against deep neural networks (DNN), the attacker
generates adversarial instances that are visually indistinguishable from benign
samples and sends them to the target DNN to trigger misclassifications. In this
paper, we propose a novel multi-view adversarial image detector, namely Argos,
based on a novel observation. That is, there exist two "souls" in an
adversarial instance, i.e., the visually unchanged content, which corresponds
to the true label, and the added invisible perturbation, which corresponds to
the misclassified label. Such inconsistencies could be further amplified
through an autoregressive generative approach that generates images with seed
pixels selected from the original image, a selected label, and pixel
distributions learned from the training data. The generated images (i.e., the
"views") will deviate significantly from the original one if the label is
adversarial, demonstrating inconsistencies that Argos expects to detect. To
this end, Argos first amplifies the discrepancies between the visual content of
an image and its misclassified label induced by the attack using a set of
regeneration mechanisms and then identifies an image as adversarial if the
reproduced views deviate to a preset degree. Our experimental results show that
Argos significantly outperforms two representative adversarial detectors in
both detection accuracy and robustness against six well-known adversarial
attacks. Code is available at:
https://github.com/sohaib730/Argos-Adversarial_Detection
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EllipseNet: Anchor-Free Ellipse Detection for Automatic Cardiac Biometrics in Fetal Echocardiography. (arXiv:2109.12474v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12474">
<div class="article-summary-box-inner">
<span><p>As an important scan plane, four chamber view is routinely performed in both
second trimester perinatal screening and fetal echocardiographic examinations.
The biometrics in this plane including cardio-thoracic ratio (CTR) and cardiac
axis are usually measured by sonographers for diagnosing congenital heart
disease. However, due to the commonly existing artifacts like acoustic
shadowing, the traditional manual measurements not only suffer from the low
efficiency, but also with the inconsistent results depending on the operators'
skills. In this paper, we present an anchor-free ellipse detection network,
namely EllipseNet, which detects the cardiac and thoracic regions in ellipse
and automatically calculates the CTR and cardiac axis for fetal cardiac
biometrics in 4-chamber view. In particular, we formulate the network that
detects the center of each object as points and regresses the ellipses'
parameters simultaneously. We define an intersection-over-union loss to further
regulate the regression procedure. We evaluate EllipseNet on clinical
echocardiogram dataset with more than 2000 subjects. Experimental results show
that the proposed framework outperforms several state-of-the-art methods.
Source code will be available at https://git.openi.org.cn/capepoint/EllipseNet .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation. (arXiv:2109.12484v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12484">
<div class="article-summary-box-inner">
<span><p>Self-supervised methods play an increasingly important role in monocular
depth estimation due to their great potential and low annotation cost. To close
the gap with supervised methods, recent works take advantage of extra
constraints, e.g., semantic segmentation. However, these methods will
inevitably increase the burden on the model. In this paper, we show theoretical
and empirical evidence that the potential capacity of self-supervised monocular
depth estimation can be excavated without increasing this cost. In particular,
we propose (1) a novel data augmentation approach called data grafting, which
forces the model to explore more cues to infer depth besides the vertical image
position, (2) an exploratory self-distillation loss, which is supervised by the
self-distillation label generated by our new post-processing method - selective
post-processing, and (3) the full-scale network, designed to endow the encoder
with the specialization of depth estimation task and enhance the
representational power of the model. Extensive experiments show that our
contributions can bring significant performance improvement to the baseline
with even less computational overhead, and our model, named EPCDepth, surpasses
the previous state-of-the-art methods even those supervised by additional
constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image Translation. (arXiv:2109.12492v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12492">
<div class="article-summary-box-inner">
<span><p>Recently, there has been an increasing interest in image editing methods that
employ pre-trained unconditional image generators (e.g., StyleGAN). However,
applying these methods to translate images to multiple visual domains remains
challenging. Existing works do not often preserve the domain-invariant part of
the image (e.g., the identity in human face translations), they do not usually
handle multiple domains, or do not allow for multi-modal translations. This
work proposes an implicit style function (ISF) to straightforwardly achieve
multi-modal and multi-domain image-to-image translation from pre-trained
unconditional generators. The ISF manipulates the semantics of an input latent
code to make the image generated from it lying in the desired visual domain.
Our results in human face and animal manipulations show significantly improved
results over the baselines. Our model enables cost-effective multi-modal
unsupervised image-to-image translations at high resolution using pre-trained
unconditional GANs. The code and data are available at:
\url{https://github.com/yhlleo/stylegan-mmuit}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Video Representation Learning by Video Incoherence Detection. (arXiv:2109.12493v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12493">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel self-supervised method that leverages
incoherence detection for video representation learning. It roots from the
observation that visual systems of human beings can easily identify video
incoherence based on their comprehensive understanding of videos. Specifically,
the training sample, denoted as the incoherent clip, is constructed by multiple
sub-clips hierarchically sampled from the same raw video with various lengths
of incoherence between each other. The network is trained to learn high-level
representation by predicting the location and length of incoherence given the
incoherent clip as input. Additionally, intra-video contrastive learning is
introduced to maximize the mutual information between incoherent clips from the
same raw video. We evaluate our proposed method through extensive experiments
on action recognition and video retrieval utilizing various backbone networks.
Experiments show that our proposed method achieves state-of-the-art performance
across different backbone networks and different datasets compared with
previous coherence-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Short-Term Load Forecasting Using Time Pooling Deep Recurrent Neural Network. (arXiv:2109.12498v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12498">
<div class="article-summary-box-inner">
<span><p>Integration of renewable energy sources and emerging loads like electric
vehicles to smart grids brings more uncertainty to the distribution system
management. Demand Side Management (DSM) is one of the approaches to reduce the
uncertainty. Some applications like Nonintrusive Load Monitoring (NILM) can
support DSM, however they require accurate forecasting on high resolution data.
This is challenging when it comes to single loads like one residential
household due to its high volatility. In this paper, we review some of the
existing Deep Learning-based methods and present our solution using Time
Pooling Deep Recurrent Neural Network. The proposed method augments data using
time pooling strategy and can overcome overfitting problems and model
uncertainties of data more efficiently. Simulation and implementation results
show that our method outperforms the existing algorithms in terms of RMSE and
MAE metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PETA: Photo Albums Event Recognition using Transformers Attention. (arXiv:2109.12499v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12499">
<div class="article-summary-box-inner">
<span><p>In recent years the amounts of personal photos captured increased
significantly, giving rise to new challenges in multi-image understanding and
high-level image understanding. Event recognition in personal photo albums
presents one challenging scenario where life events are recognized from a
disordered collection of images, including both relevant and irrelevant images.
Event recognition in images also presents the challenge of high-level image
understanding, as opposed to low-level image object classification. In absence
of methods to analyze multiple inputs, previous methods adopted temporal
mechanisms, including various forms of recurrent neural networks. However,
their effective temporal window is local. In addition, they are not a natural
choice given the disordered characteristic of photo albums. We address this gap
with a tailor-made solution, combining the power of CNNs for image
representation and transformers for album representation to perform global
reasoning on image collection, offering a practical and efficient solution for
photo albums event recognition. Our solution reaches state-of-the-art results
on 3 prominent benchmarks, achieving above 90\% mAP on all datasets. We further
explore the related image-importance task in event recognition, demonstrating
how the learned attentions correlate with the human-annotated importance for
this subjective task, thus opening the door for new applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning for MRI Reconstruction with a Parallel Network Training Framework. (arXiv:2109.12502v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12502">
<div class="article-summary-box-inner">
<span><p>Image reconstruction from undersampled k-space data plays an important role
in accelerating the acquisition of MR data, and a lot of deep learning-based
methods have been exploited recently. Despite the achieved inspiring results,
the optimization of these methods commonly relies on the fully-sampled
reference data, which are time-consuming and difficult to collect. To address
this issue, we propose a novel self-supervised learning method. Specifically,
during model optimization, two subsets are constructed by randomly selecting
part of k-space data from the undersampled data and then fed into two parallel
reconstruction networks to perform information recovery. Two reconstruction
losses are defined on all the scanned data points to enhance the network's
capability of recovering the frequency information. Meanwhile, to constrain the
learned unscanned data points of the network, a difference loss is designed to
enforce consistency between the two parallel networks. In this way, the
reconstruction model can be properly trained with only the undersampled data.
During the model evaluation, the undersampled data are treated as the inputs
and either of the two trained networks is expected to reconstruct the
high-quality results. The proposed method is flexible and can be employed in
any existing deep learning-based method. The effectiveness of the method is
evaluated on an open brain MRI dataset. Experimental results demonstrate that
the proposed self-supervised method can achieve competitive reconstruction
performance compared to the corresponding supervised learning method at high
acceleration rates (4 and 8). The code is publicly available at
\url{https://github.com/chenhu96/Self-Supervised-MRI-Reconstruction}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Self-calibration Method for The Internal Time Synchronization of MEMS LiDAR. (arXiv:2109.12506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12506">
<div class="article-summary-box-inner">
<span><p>This paper proposes a simple self-calibration method for the internal time
synchronization of MEMS(Micro-electromechanical systems) LiDAR during research
and development. Firstly, we introduced the problem of internal time
misalignment in MEMS lidar. Then, a robust Minimum Vertical Gradient(MVG) prior
is proposed to calibrate the time difference between the laser and MEMS mirror,
which can be calculated automatically without any artificial participation or
specially designed cooperation target. Finally, actual experiments on MEMS
LiDARs are implemented to demonstrate the effectiveness of the proposed method.
It should be noted that the calibration can be implemented in a simple
laboratory environment without any ranging equipment and artificial
participation, which greatly accelerate the progress of research and
development in practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partial to Whole Knowledge Distillation: Progressive Distilling Decomposed Knowledge Boosts Student Better. (arXiv:2109.12507v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12507">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation field delicately designs various types of knowledge to
shrink the performance gap between compact student and large-scale teacher.
These existing distillation approaches simply focus on the improvement of
\textit{knowledge quality}, but ignore the significant influence of
\textit{knowledge quantity} on the distillation procedure. Opposed to the
conventional distillation approaches, which extract knowledge from a fixed
teacher computation graph, this paper explores a non-negligible research
direction from a novel perspective of \textit{knowledge quantity} to further
improve the efficacy of knowledge distillation. We introduce a new concept of
knowledge decomposition, and further put forward the \textbf{P}artial to
\textbf{W}hole \textbf{K}nowledge \textbf{D}istillation~(\textbf{PWKD})
paradigm. Specifically, we reconstruct teacher into weight-sharing sub-networks
with same depth but increasing channel width, and train sub-networks jointly to
obtain decomposed knowledge~(sub-networks with more channels represent more
knowledge). Then, student extract partial to whole knowledge from the
pre-trained teacher within multiple training stages where cyclic learning rate
is leveraged to accelerate convergence. Generally, \textbf{PWKD} can be
regarded as a plugin to be compatible with existing offline knowledge
distillation approaches. To verify the effectiveness of \textbf{PWKD}, we
conduct experiments on two benchmark datasets:~CIFAR-100 and ImageNet, and
comprehensive evaluation results reveal that \textbf{PWKD} consistently improve
existing knowledge distillation approaches without bells and whistles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized multiscale feature extraction for remaining useful life prediction of bearings with generative adversarial networks. (arXiv:2109.12513v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12513">
<div class="article-summary-box-inner">
<span><p>Bearing is a key component in industrial machinery and its failure may lead
to unwanted downtime and economic loss. Hence, it is necessary to predict the
remaining useful life (RUL) of bearings. Conventional data-driven approaches of
RUL prediction require expert domain knowledge for manual feature extraction
and may suffer from data distribution discrepancy between training and test
data. In this study, we propose a novel generalized multiscale feature
extraction method with generative adversarial networks. The adversarial
training learns the distribution of training data from different bearings and
is introduced for health stage division and RUL prediction. To capture the
sequence feature from a one-dimensional vibration signal, we adapt a U-Net
architecture that reconstructs features to process them with multiscale layers
in the generator of the adversarial network. To validate the proposed method,
comprehensive experiments on two rotating machinery datasets have been
conducted to predict the RUL. The experimental results show that the proposed
feature extraction method can effectively predict the RUL and outperforms the
conventional RUL prediction approaches based on deep neural networks. The
implementation code is available at https://github.com/opensuh/GMFE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure-Preserving Image Super-Resolution. (arXiv:2109.12530v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12530">
<div class="article-summary-box-inner">
<span><p>Structures matter in single image super-resolution (SISR). Benefiting from
generative adversarial networks (GANs), recent studies have promoted the
development of SISR by recovering photo-realistic images. However, there are
still undesired structural distortions in the recovered images. In this paper,
we propose a structure-preserving super-resolution (SPSR) method to alleviate
the above issue while maintaining the merits of GAN-based methods to generate
perceptual-pleasant details. Firstly, we propose SPSR with gradient guidance
(SPSR-G) by exploiting gradient maps of images to guide the recovery in two
aspects. On the one hand, we restore high-resolution gradient maps by a
gradient branch to provide additional structure priors for the SR process. On
the other hand, we propose a gradient loss to impose a second-order restriction
on the super-resolved images, which helps generative networks concentrate more
on geometric structures. Secondly, since the gradient maps are handcrafted and
may only be able to capture limited aspects of structural information, we
further extend SPSR-G by introducing a learnable neural structure extractor
(NSE) to unearth richer local structures and provide stronger supervision for
SR. We propose two self-supervised structure learning methods, contrastive
prediction and solving jigsaw puzzles, to train the NSEs. Our methods are
model-agnostic, which can be potentially used for off-the-shelf SR networks.
Experimental results on five benchmark datasets show that the proposed methods
outperform state-of-the-art perceptual-driven SR methods under LPIPS, PSNR, and
SSIM metrics. Visual results demonstrate the superiority of our methods in
restoring structures while generating natural SR images. Code is available at
https://github.com/Maclory/SPSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAMix: Density-Aware Data Augmentation for Unsupervised Domain Adaptation on Single Image Dehazing. (arXiv:2109.12544v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12544">
<div class="article-summary-box-inner">
<span><p>Learning-based methods have achieved great success on single image dehazing
in recent years. However, these methods are often subject to performance
degradation when domain shifts are confronted. Specifically, haze density gaps
exist among the existing datasets, often resulting in poor performance when
these methods are tested across datasets. To address this issue, we propose a
density-aware data augmentation method (DAMix) that generates synthetic hazy
samples according to the haze density level of the target domain. These samples
are generated by combining a hazy image with its corresponding ground truth by
a combination ratio sampled from a density-aware distribution. They not only
comply with the atmospheric scattering model but also bridge the haze density
gap between the source and target domains. DAMix ensures that the model learns
from examples featuring diverse haze densities. To better utilize the various
hazy samples generated by DAMix, we develop a dual-branch dehazing network
involving two branches that can adaptively remove haze according to the haze
density of the region. In addition, the dual-branch design enlarges the
learning capacity of the entire network; hence, our network can fully utilize
the DAMix-ed samples. We evaluate the effectiveness of DAMix by applying it to
the existing open-source dehazing methods. The experimental results demonstrate
that all methods show significant improvements after DAMix is applied.
Furthermore, by combining DAMix with our model, we can achieve state-of-the-art
(SOTA) performance in terms of domain adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Feature Representation for Few-shot Image Classification. (arXiv:2109.12548v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12548">
<div class="article-summary-box-inner">
<span><p>Learning the generalizable feature representation is critical for few-shot
image classification. While recent works exploited task-specific feature
embedding using meta-tasks for few-shot learning, they are limited in many
challenging tasks as being distracted by the excursive features such as the
background, domain and style of the image samples. In this work, we propose a
novel Disentangled Feature Representation framework, dubbed DFR, for few-shot
learning applications. DFR can adaptively decouple the discriminative features
that are modeled by the classification branch, from the class-irrelevant
component of the variation branch. In general, most of the popular deep
few-shot learning methods can be plugged in as the classification branch, thus
DFR can boost their performance on various few-shot tasks. Furthermore, we
propose a novel FS-DomainNet dataset based on DomainNet, for benchmarking the
few-shot domain generalization tasks. We conducted extensive experiments to
evaluate the proposed DFR on general and fine-grained few-shot classification,
as well as few-shot domain generalization, using the corresponding four
benchmarks, i.e., mini-ImageNet, tiered-ImageNet, CUB, as well as the proposed
FS-DomainNet. Thanks to the effective feature disentangling, the DFR-based
few-shot classifiers achieved the state-of-the-art results on all datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency Disentangled Residual Network. (arXiv:2109.12556v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12556">
<div class="article-summary-box-inner">
<span><p>Residual networks (ResNets) have been utilized for various computer vision
and image processing applications. The residual connection improves the
training of the network with better gradient flow. A residual block consists of
few convolutional layers having trainable parameters, which leads to
overfitting. Moreover, the present residual networks are not able to utilize
the high and low frequency information suitably, which also challenges the
generalization capability of the network. In this paper, a frequency
disentangled residual network (FDResNet) is proposed to tackle these issues.
Specifically, FDResNet includes separate connections in the residual block for
low and high frequency components, respectively. Basically, the proposed model
disentangles the low and high frequency components to increase the
generalization ability. Moreover, the computation of low and high frequency
components using fixed filters further avoids the overfitting. The proposed
model is tested on benchmark CIFAR10/100, Caltech and TinyImageNet datasets for
image classification. The performance of the proposed model is also tested in
image retrieval framework. It is noticed that the proposed model outperforms
its counterpart residual model. The effect of kernel size and standard
deviation is also evaluated. The impact of the frequency disentangling is also
analyzed using saliency map.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer Hashing for Image Retrieval. (arXiv:2109.12564v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12564">
<div class="article-summary-box-inner">
<span><p>Deep learning has shown a tremendous growth in hashing techniques for image
retrieval. Recently, Transformer has emerged as a new architecture by utilizing
self-attention without convolution. Transformer is also extended to Vision
Transformer (ViT) for the visual recognition with a promising performance on
ImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)
for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone
network and add the hashing head. The proposed VTS model is fine tuned for
hashing under six different image retrieval frameworks, including Deep
Supervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network
(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)
with their objective functions. We perform the extensive experiments on
CIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image
retrieval outperforms the recent state-of-the-art hashing techniques with a
great margin. We also find the proposed VTS model as the backbone network is
better than the existing networks, such as AlexNet and ResNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Video Summarization Method Using Temporal Interest Detection and Key Frame Prediction. (arXiv:2109.12581v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12581">
<div class="article-summary-box-inner">
<span><p>In this paper, a Video Summarization Method using Temporal Interest Detection
and Key Frame Prediction is proposed for supervised video summarization, where
video summarization is formulated as a combination of sequence labeling and
temporal interest detection problem. In our method, we firstly built a flexible
universal network frame to simultaneously predicts frame-level importance
scores and temporal interest segments, and then combine the two components with
different weights to achieve a more detailed video summarization. Extensive
experiments and analysis on two benchmark datasets prove the effectiveness of
our method. Specifically, compared with other state-of-the-art methods, its
performance is increased by at least 2.6% and 4.2% on TVSum and SumMe
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure-aware scale-adaptive networks for cancer segmentation in whole-slide images. (arXiv:2109.12617v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12617">
<div class="article-summary-box-inner">
<span><p>Cancer segmentation in whole-slide images is a fundamental step for viable
tumour burden estimation, which is of great value for cancer assessment.
However, factors like vague boundaries or small regions dissociated from viable
tumour areas make it a challenging task. Considering the usefulness of
multi-scale features in various vision-related tasks, we present a
structure-aware scale-adaptive feature selection method for efficient and
accurate cancer segmentation. Based on a segmentation network with a popular
encoder-decoder architecture, a scale-adaptive module is proposed for selecting
more robust features to represent the vague, non-rigid boundaries. Furthermore,
a structural similarity metric is proposed for better tissue structure
awareness to deal with small region segmentation. In addition, advanced designs
including several attention mechanisms and the selective-kernel convolutions
are applied to the baseline network for comparative study purposes. Extensive
experimental results show that the proposed structure-aware scale-adaptive
networks achieve outstanding performance on liver cancer segmentation when
compared to top ten submitted results in the challenge of PAIP 2019. Further
evaluation on colorectal cancer segmentation shows that the scale-adaptive
module improves the baseline network or outperforms the other excellent designs
of attention mechanisms when considering the tradeoff between efficiency and
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Soft Labels to Model Uncertainty in Medical Image Segmentation. (arXiv:2109.12622v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12622">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation is inherently uncertain. For a given image, there
may be multiple plausible segmentation hypotheses, and physicians will often
disagree on lesion and organ boundaries. To be suited to real-world
application, automatic segmentation systems must be able to capture this
uncertainty and variability. Thus far, this has been addressed by building deep
learning models that, through dropout, multiple heads, or variational
inference, can produce a set - infinite, in some cases - of plausible
segmentation hypotheses for any given image. However, in clinical practice, it
may not be practical to browse all hypotheses. Furthermore, recent work shows
that segmentation variability plateaus after a certain number of independent
annotations, suggesting that a large enough group of physicians may be able to
represent the whole space of possible segmentations. Inspired by this, we
propose a simple method to obtain soft labels from the annotations of multiple
physicians and train models that, for each image, produce a single
well-calibrated output that can be thresholded at multiple confidence levels,
according to each application's precision-recall requirements. We evaluated our
method on the MICCAI 2021 QUBIQ challenge, showing that it performs well across
multiple medical image segmentation tasks, produces well-calibrated
predictions, and, on average, performs better at matching physicians'
predictions than other physicians.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logo Generation Using Regional Features: A Faster R-CNN Approach to Generative Adversarial Networks. (arXiv:2109.12628v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12628">
<div class="article-summary-box-inner">
<span><p>In this paper we introduce the Local Logo Generative Adversarial Network
(LL-GAN) that uses regional features extracted from the Faster Regional
Convolutional Neural Network (Faster R-CNN) to generate logos. We demonstrate
the strength of this approach by training the framework on a small style-rich
dataset collected online to generate large impressive logos. Our approach beats
the state-of-the-art models (StyleGAN2, Self-Attention GANs) that suffer from
mode collapse due to the size of the data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Group Shift Pointwise Convolution for Volumetric Medical Image Segmentation. (arXiv:2109.12629v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12629">
<div class="article-summary-box-inner">
<span><p>Recent studies have witnessed the effectiveness of 3D convolutions on
segmenting volumetric medical images. Compared with the 2D counterparts, 3D
convolutions can capture the spatial context in three dimensions. Nevertheless,
models employing 3D convolutions introduce more trainable parameters and are
more computationally complex, which may lead easily to model overfitting
especially for medical applications with limited available training data. This
paper aims to improve the effectiveness and efficiency of 3D convolutions by
introducing a novel Group Shift Pointwise Convolution (GSP-Conv). GSP-Conv
simplifies 3D convolutions into pointwise ones with 1x1x1 kernels, which
dramatically reduces the number of model parameters and FLOPs (e.g. 27x fewer
than 3D convolutions with 3x3x3 kernels). Na\"ive pointwise convolutions with
limited receptive fields cannot make full use of the spatial image context. To
address this problem, we propose a parameter-free operation, Group Shift (GS),
which shifts the feature maps along with different spatial directions in an
elegant way. With GS, pointwise convolutions can access features from different
spatial locations, and the limited receptive fields of pointwise convolutions
can be compensated. We evaluate the proposed methods on two datasets, PROMISE12
and BraTS18. Results show that our method, with substantially decreased model
complexity, achieves comparable or even better performance than models
employing 3D convolutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Hybrid Convolutional Neural Network for Accurate Organ Segmentation in 3D Head and Neck CT Images. (arXiv:2109.12634v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12634">
<div class="article-summary-box-inner">
<span><p>Radiation therapy (RT) is widely employed in the clinic for the treatment of
head and neck (HaN) cancers. An essential step of RT planning is the accurate
segmentation of various organs-at-risks (OARs) in HaN CT images. Nevertheless,
segmenting OARs manually is time-consuming, tedious, and error-prone
considering that typical HaN CT images contain tens to hundreds of slices.
Automated segmentation algorithms are urgently required. Recently,
convolutional neural networks (CNNs) have been extensively investigated on this
task. Particularly, 3D CNNs are frequently adopted to process 3D HaN CT images.
There are two issues with na\"ive 3D CNNs. First, the depth resolution of 3D CT
images is usually several times lower than the in-plane resolution. Direct
employment of 3D CNNs without distinguishing this difference can lead to the
extraction of distorted image features and influence the final segmentation
performance. Second, a severe class imbalance problem exists, and large organs
can be orders of times larger than small organs. It is difficult to
simultaneously achieve accurate segmentation for all the organs. To address
these issues, we propose a novel hybrid CNN that fuses 2D and 3D convolutions
to combat the different spatial resolutions and extract effective edge and
semantic features from 3D HaN CT images. To accommodate large and small organs,
our final model, named OrganNet2.5D, consists of only two instead of the
classic four downsampling operations, and hybrid dilated convolutions are
introduced to maintain the respective field. Experiments on the MICCAI 2015
challenge dataset demonstrate that OrganNet2.5D achieves promising performance
compared to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nesterov Accelerated ADMM for Fast Diffeomorphic Image Registration. (arXiv:2109.12688v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12688">
<div class="article-summary-box-inner">
<span><p>Deterministic approaches using iterative optimisation have been historically
successful in diffeomorphic image registration (DiffIR). Although these
approaches are highly accurate, they typically carry a significant
computational burden. Recent developments in stochastic approaches based on
deep learning have achieved sub-second runtimes for DiffIR with competitive
registration accuracy, offering a fast alternative to conventional iterative
methods. In this paper, we attempt to reduce this difference in speed whilst
retaining the performance advantage of iterative approaches in DiffIR. We first
propose a simple iterative scheme that functionally composes intermediate
non-stationary velocity fields to handle large deformations in images whilst
guaranteeing diffeomorphisms in the resultant deformation. We then propose a
convex optimisation model that uses a regularisation term of arbitrary order to
impose smoothness on these velocity fields and solve this model with a fast
algorithm that combines Nesterov gradient descent and the alternating direction
method of multipliers (ADMM). Finally, we leverage the computational power of
GPU to implement this accelerated ADMM solver on a 3D cardiac MRI dataset,
further reducing runtime to less than 2 seconds. In addition to producing
strictly diffeomorphic deformations, our methods outperform both
state-of-the-art deep learning-based and iterative DiffIR approaches in terms
of dice and Hausdorff scores, with speed approaching the inference time of deep
learning-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Multi-Process CTC Detection using Deep Learning. (arXiv:2109.12709v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12709">
<div class="article-summary-box-inner">
<span><p>Circulating Tumor Cells (CTCs) bear great promise as biomarkers in tumor
prognosis. However, the process of identification and later enumeration of CTCs
require manual labor, which is error-prone and time-consuming. The recent
developments in object detection via Deep Learning using Mask-RCNNs and wider
availability of pre-trained models have enabled sensitive tasks with limited
data of such to be tackled with unprecedented accuracy. In this report, we
present a novel 3-stage detection model for automated identification of
Circulating Tumor Cells in multi-channel darkfield microscopic images comprised
of: RetinaNet based identification of Cytokeratin (CK) stains, Mask-RCNN based
cell detection of DAPI cell nuclei and Otsu thresholding to detect CD-45s. The
training dataset is composed of 46 high variance data points, with 10 Negative
and 36 Positive data points. The test set is composed of 420 negative data
points. The final accuracy of the pipeline is 98.81%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cluster Analysis with Deep Embeddings and Contrastive Learning. (arXiv:2109.12714v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12714">
<div class="article-summary-box-inner">
<span><p>Unsupervised disentangled representation learning is a long-standing problem
in computer vision. This work proposes a novel framework for performing image
clustering from deep embeddings by combining instance-level contrastive
learning with a deep embedding based cluster center predictor. Our approach
jointly learns representations and predicts cluster centers in an end-to-end
manner. This is accomplished via a three-pronged approach that combines a
clustering loss, an instance-wise contrastive loss, and an anchor loss. Our
fundamental intuition is that using an ensemble loss that incorporates
instance-level features and a clustering procedure focusing on semantic
similarity reinforces learning better representations in the latent space. We
observe that our method performs exceptionally well on popular vision datasets
when evaluated using standard clustering metrics such as Normalized Mutual
Information (NMI), in addition to producing geometrically well-separated
cluster embeddings as defined by the Euclidean distance. Our framework performs
on par with widely accepted clustering methods and outperforms the
state-of-the-art contrastive learning method on the CIFAR-10 dataset with an
NMI score of 0.772, a 7-8% improvement on the strong baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Markerless Suture Needle 6D Pose Tracking with Robust Uncertainty Estimation for Autonomous Minimally Invasive Robotic Surgery. (arXiv:2109.12722v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12722">
<div class="article-summary-box-inner">
<span><p>Suture needle localization plays a crucial role towards autonomous suturing.
To track the 6D pose of a suture needle robustly, previous approaches usually
add markers on the needle or perform complex operations for feature extraction,
making these methods difficult to be applicable to real-world environments.
Therefore in this work, we present a novel approach for markerless suture
needle pose tracking using Bayesian filters. A data-efficient feature point
detector is trained to extract the feature points on the needle. Then based on
these detections, we propose a novel observation model that measures the
overlap between the detections and the expected projection of the needle, which
can be calculated efficiently. In addition, for the proposed method, we derive
the approximation for the covariance of the observation noise, making this
model more robust to the uncertainty in the detections. The experimental
results in simulation show that the proposed observation model achieves low
tracking errors of approximately 1.5mm in position in space and 1 degree in
orientation. We also demonstrate the qualitative results of our trained
markerless feature detector combined with the proposed observation model in
real-world environments. The results show high consistency between the
projection of the tracked pose and that of the real pose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Research on facial expression recognition based on Multimodal data fusion and neural network. (arXiv:2109.12724v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12724">
<div class="article-summary-box-inner">
<span><p>Facial expression recognition is a challenging task when neural network is
applied to pattern recognition. Most of the current recognition research is
based on single source facial data, which generally has the disadvantages of
low accuracy and low robustness. In this paper, a neural network algorithm of
facial expression recognition based on multimodal data fusion is proposed. The
algorithm is based on the multimodal data, and it takes the facial image, the
histogram of oriented gradient of the image and the facial landmarks as the
input, and establishes CNN, LNN and HNN three sub neural networks to extract
data features, using multimodal data feature fusion mechanism to improve the
accuracy of facial expression recognition. Experimental results show that,
benefiting by the complementarity of multimodal data, the algorithm has a great
improvement in accuracy, robustness and detection speed compared with the
traditional facial expression recognition algorithm. Especially in the case of
partial occlusion, illumination and head posture transformation, the algorithm
also shows a high confidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel network training approach for open set image recognition. (arXiv:2109.12756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12756">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) are commonly designed for closed set
arrangements, where test instances only belong to some "Known Known" (KK)
classes used in training. As such, they predict a class label for a test sample
based on the distribution of the KK classes. However, when used under the Open
Set Recognition (OSR) setup (where an input may belong to an "Unknown Unknown"
or UU class), such a network will always classify a test instance as one of the
KK classes even if it is from a UU class. As a solution, recently, data
augmentation based on Generative Adversarial Networks(GAN) has been used. In
this work, we propose a novel approach for mining a "Known UnknownTrainer" or
KUT set and design a deep OSR Network (OSRNet) to harness this dataset. The
goal isto teach OSRNet the essence of the UUs through KUT set, which is
effectively a collection of mined "hard Known Unknown negatives". Once trained,
OSRNet can detect the UUs while maintaining high classification accuracy on
KKs. We evaluate OSRNet on six benchmark datasets and demonstrate it
outperforms contemporary OSR methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Thermal Infrared Monitoring of Volcanoes: A Deep Learning Approach for Intermittent Image Series. (arXiv:2109.12767v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12767">
<div class="article-summary-box-inner">
<span><p>Active volcanoes are globally distributed and pose societal risks at multiple
geographic scales, ranging from local hazards to regional/international
disruptions. Many volcanoes do not have continuous ground monitoring networks;
meaning that satellite observations provide the only record of volcanic
behavior and unrest. Among these remote sensing observations, thermal imagery
is inspected daily by volcanic observatories for examining the early signs,
onset, and evolution of eruptive activity. However, thermal scenes are often
obstructed by clouds, meaning that forecasts must be made off image sequences
whose scenes are only usable intermittently through time. Here, we explore
forecasting this thermal data stream from a deep learning perspective using
existing architectures that model sequences with varying spatiotemporal
considerations. Additionally, we propose and evaluate new architectures that
explicitly model intermittent image sequences. Using ASTER Kinetic Surface
Temperature data for $9$ volcanoes between $1999$ and $2020$, we found that a
proposed architecture (ConvLSTM + Time-LSTM + U-Net) forecasts volcanic
temperature imagery with the lowest RMSE ($4.164^{\circ}$C, other methods:
$4.217-5.291^{\circ}$C). Additionally, we examined performance on multiple time
series derived from the thermal imagery and the effect of training with data
from singular volcanoes. Ultimately, we found that models with the lowest RMSE
on forecasting imagery did not possess the lowest RMSE on recreating time
series derived from that imagery and that training with individual volcanoes
generally worsened performance relative to a multi-volcano data set. This work
highlights the potential of data-driven deep learning models for volcanic
unrest forecasting while revealing the need for carefully constructed
optimization targets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Multimedia Event Extraction from Video and Article. (arXiv:2109.12776v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12776">
<div class="article-summary-box-inner">
<span><p>Visual and textual modalities contribute complementary information about
events described in multimedia documents. Videos contain rich dynamics and
detailed unfoldings of events, while text describes more high-level and
abstract concepts. However, existing event extraction methods either do not
handle video or solely target video while ignoring other modalities. In
contrast, we propose the first approach to jointly extract events from video
and text articles. We introduce the new task of Video MultiMedia Event
Extraction (Video M2E2) and propose two novel components to build the first
system towards this task. First, we propose the first self-supervised
multimodal event coreference model that can determine coreference between video
events and text events without any manually annotated pairs. Second, we
introduce the first multimodal transformer which extracts structured event
information jointly from both videos and text documents. We also construct and
will publicly release a new benchmark of video-article pairs, consisting of 860
video-article pairs with extensive annotations for evaluating methods on this
task. Our experimental results demonstrate the effectiveness of our proposed
method on our new benchmark dataset. We achieve 6.0% and 5.8% absolute F-score
gain on multimodal event coreference resolution and multimedia event
extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Multiple CNNs for Triaging Medical Workflow. (arXiv:2109.12783v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12783">
<div class="article-summary-box-inner">
<span><p>High hospitalization rates due to the global spread of Covid-19 bring about a
need for improvements to classical triaging workflows. To this end,
convolutional neural networks (CNNs) can effectively differentiate critical
from non-critical images so that critical cases may be addressed quickly, so
long as there exists some representative image for the illness. Presented is a
conglomerate neural network system consisting of multiple VGG16 CNNs; the
system trains on weighted skin disease images re-labelled as critical or
non-critical, to then attach to input images a critical index between 0 and 10.
A critical index offers a more comprehensive rating system compared to binary
critical/non-critical labels. Results for batches of input images run through
the trained network are promising. A batch is shown being re-ordered by the
proposed architecture from most critical to least critical roughly accurately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High Frame Rate Video Quality Assessment using VMAF and Entropic Differences. (arXiv:2109.12785v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12785">
<div class="article-summary-box-inner">
<span><p>The popularity of streaming videos with live, high-action content has led to
an increased interest in High Frame Rate (HFR) videos. In this work we address
the problem of frame rate dependent Video Quality Assessment (VQA) when the
videos to be compared have different frame rate and compression factor. The
current VQA models such as VMAF have superior correlation with perceptual
judgments when videos to be compared have same frame rates and contain
conventional distortions such as compression, scaling etc. However this
framework requires additional pre-processing step when videos with different
frame rates need to be compared, which can potentially limit its overall
performance. Recently, Generalized Entropic Difference (GREED) VQA model was
proposed to account for artifacts that arise due to changes in frame rate, and
showed superior performance on the LIVE-YT-HFR database which contains frame
rate dependent artifacts such as judder, strobing etc. In this paper we propose
a simple extension, where the features from VMAF and GREED are fused in order
to exploit the advantages of both models. We show through various experiments
that the proposed fusion framework results in more efficient features for
predicting frame rate dependent video quality. We also evaluate the fused
feature set on standard non-HFR VQA databases and obtain superior performance
than both GREED and VMAF, indicating the combined feature set captures
complimentary perceptual quality information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning based Medical Image Deepfake Detection: A Comparative Study. (arXiv:2109.12800v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12800">
<div class="article-summary-box-inner">
<span><p>Deep generative networks in recent years have reinforced the need for caution
while consuming various modalities of digital information. One avenue of
deepfake creation is aligned with injection and removal of tumors from medical
scans. Failure to detect medical deepfakes can lead to large setbacks on
hospital resources or even loss of life. This paper attempts to address the
detection of such attacks with a structured case study. We evaluate different
machine learning algorithms and pretrained convolutional neural networks on
distinguishing between tampered and untampered data. The findings of this work
show near perfect accuracy in detecting instances of tumor injections and
removals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect Of Personalized Calibration On Gaze Estimation Using Deep-Learning. (arXiv:2109.12801v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12801">
<div class="article-summary-box-inner">
<span><p>With the increase in computation power and the development of new
state-of-the-art deep learning algorithms, appearance-based gaze estimation is
becoming more and more popular. It is believed to work well with curated
laboratory data sets, however it faces several challenges when deployed in real
world scenario. One such challenge is to estimate the gaze of a person about
which the Deep Learning model trained for gaze estimation has no knowledge
about. To analyse the performance in such scenarios we have tried to simulate a
calibration mechanism. In this work we use the MPIIGaze data set. We trained a
multi modal convolutional neural network and analysed its performance with and
without calibration and this evaluation provides clear insights on how
calibration improved the performance of the Deep Learning model in estimating
gaze in the wild.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N-shot Palm Vein Verification Using Siamese Networks. (arXiv:2109.12808v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12808">
<div class="article-summary-box-inner">
<span><p>The use of deep learning methods to extract vascular biometric patterns from
the palm surface has been of interest among researchers in recent years. In
many biometric recognition tasks, there is a limit in the number of training
samples. This is because of limited vein biometric databases being available
for research. This restricts the application of deep learning methods to design
algorithms that can effectively identify or authenticate people for vein
recognition. This paper proposes an architecture using Siamese neural network
structure for few shot palm vein identification. The proposed network uses
images from both the palms and consists of two sub-nets that share weights to
identify a person. The architecture performance was tested on the HK PolyU
multi spectral palm vein database with limited samples. The results suggest
that the method is effective since it has 91.9% precision, 91.1% recall, 92.2%
specificity, 91.5%, F1-Score, and 90.5% accuracy values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An optimised deep spiking neural network architecture without gradients. (arXiv:2109.12813v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12813">
<div class="article-summary-box-inner">
<span><p>We present an end-to-end trainable modular event-driven neural architecture
that uses local synaptic and threshold adaptation rules to perform
transformations between arbitrary spatio-temporal spike patterns. The
architecture represents a highly abstracted model of existing Spiking Neural
Network (SNN) architectures. The proposed Optimized Deep Event-driven Spiking
neural network Architecture (ODESA) can simultaneously learn hierarchical
spatio-temporal features at multiple arbitrary time scales. ODESA performs
online learning without the use of error back-propagation or the calculation of
gradients. Through the use of simple local adaptive selection thresholds at
each node, the network rapidly learns to appropriately allocate its neuronal
resources at each layer for any given problem without using a real-valued error
measure. These adaptive selection thresholds are the central feature of ODESA,
ensuring network stability and remarkable robustness to noise as well as to the
selection of initial system parameters. Network activations are inherently
sparse due to a hard Winner-Take-All (WTA) constraint at each layer. We
evaluate the architecture on existing spatio-temporal datasets, including the
spike-encoded IRIS and TIDIGITS datasets, as well as a novel set of tasks based
on International Morse Code that we created. These tests demonstrate the
hierarchical spatio-temporal learning capabilities of ODESA. Through these
tests, we demonstrate ODESA can optimally solve practical and highly
challenging hierarchical spatio-temporal learning tasks with the minimum
possible number of computing nodes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPRInvNet: Deep Learning-Based Ground Penetrating Radar Data Inversion for Tunnel Lining. (arXiv:1912.05759v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.05759">
<div class="article-summary-box-inner">
<span><p>A DNN architecture referred to as GPRInvNet was proposed to tackle the
challenges of mapping the ground-penetrating radar (GPR) B-Scan data to complex
permittivity maps of subsurface structures. The GPRInvNet consisted of a
trace-to-trace encoder and a decoder. It was specially designed to take into
account the characteristics of GPR inversion when faced with complex GPR B-Scan
data, as well as addressing the spatial alignment issues between time-series
B-Scan data and spatial permittivity maps. It displayed the ability to fuse
features from several adjacent traces on the B-Scan data to enhance each trace,
and then further condense the features of each trace separately. As a result,
the sensitive zones on the permittivity maps spatially aligned to the enhanced
trace could be reconstructed accurately. The GPRInvNet has been utilized to
reconstruct the permittivity map of tunnel linings. A diverse range of
dielectric models of tunnel linings containing complex defects has been
reconstructed using GPRInvNet. The results have demonstrated that the GPRInvNet
is capable of effectively reconstructing complex tunnel lining defects with
clear boundaries. Comparative results with existing baseline methods also
demonstrated the superiority of the GPRInvNet. For the purpose of generalizing
the GPRInvNet to real GPR data, some background noise patches recorded from
practical model testing were integrated into the synthetic GPR data to retrain
the GPRInvNet. The model testing has been conducted for validation, and
experimental results revealed that the GPRInvNet had also achieved satisfactory
results with regard to the real data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Long Tail Visual Relationship Recognition with Large Vocabulary. (arXiv:2004.00436v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.00436">
<div class="article-summary-box-inner">
<span><p>Several approaches have been proposed in recent literature to alleviate the
long-tail problem, mainly in object classification tasks. In this paper, we
make the first large-scale study concerning the task of Long-Tail Visual
Relationship Recognition (LTVRR). LTVRR aims at improving the learning of
structured visual relationships that come from the long-tail (e.g., "rabbit
grazing on grass"). In this setup, the subject, relation, and object classes
each follow a long-tail distribution. To begin our study and make a future
benchmark for the community, we introduce two LTVRR-related benchmarks, dubbed
VG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets.
We use these benchmarks to study the performance of several state-of-the-art
long-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic
hubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR
setup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top
of existing models and despite being simple, our results show that they can
remarkably improve the performance, especially on tail classes. Benchmarks,
code, and models have been made available at:
https://github.com/Vision-CAIR/LTVRR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Convolutional Online Tracking. (arXiv:2004.07109v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.07109">
<div class="article-summary-box-inner">
<span><p>Online learning has turned out to be effective for improving tracking
performance. However, it could be simply applied for classification branch, but
still remains challenging to adapt to regression branch due to its complex
design and intrinsic requirement for high-quality online samples. To tackle
this issue, we present the fully convolutional online tracking framework,
coined as FCOT, and focus on enabling online learning for both classification
and regression branches by using a target filter based tracking paradigm. Our
key contribution is to introduce an online regression model generator (RMG) for
initializing weights of the target filter with online samples and then
optimizing this target filter weights based on the groundtruth samples at the
first frame. Based on the online RGM, we devise a simple anchor-free tracker
(FCOT), composed of a feature backbone, an up-sampling decoder, a multi-scale
classification branch, and a multi-scale regression branch. Thanks to the
unique design of RMG, our FCOT can not only be more effective in handling
target variation along temporal dimension thus generating more precise results,
but also overcome the issue of error accumulation during the tracking
procedure. In addition, due to its simplicity in design, our FCOT could be
trained and deployed in a fully convolutional manner with a real-time running
speed. The proposed FCOT achieves the state-of-the-art performance on seven
benchmarks, including VOT2018, LaSOT, TrackingNet, GOT-10k, OTB100, UAV123, and
NFS. Code and models of our FCOT have been released at:
\url{https://github.com/MCG-NJU/FCOT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Multi-Dimension Pruning via Numerical Gradient Update. (arXiv:2005.08931v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.08931">
<div class="article-summary-box-inner">
<span><p>We present joint multi-dimension pruning (abbreviated as JointPruning), an
effective method of pruning a network on three crucial aspects: spatial, depth
and channel simultaneously. To tackle these three naturally different
dimensions, we proposed a general framework by defining pruning as seeking the
best pruning vector (i.e., the numerical value of layer-wise channel number,
spacial size, depth) and construct a unique mapping from the pruning vector to
the pruned network structures. Then we optimize the pruning vector with
gradient update and model joint pruning as a numerical gradient optimization
process. To overcome the challenge that there is no explicit function between
the loss and the pruning vectors, we proposed self-adapted stochastic gradient
estimation to construct a gradient path through network loss to pruning vectors
and enable efficient gradient update. We show that the joint strategy discovers
a better status than previous studies that focused on individual dimensions
solely, as our method is optimized collaboratively across the three dimensions
in a single end-to-end training and it is more efficient than the previous
exhaustive methods. Extensive experiments on large-scale ImageNet dataset
across a variety of network architectures MobileNet V1&amp;V2&amp;V3 and ResNet
demonstrate the effectiveness of our proposed method. For instance, we achieve
significant margins of 2.5% and 2.6% improvement over the state-of-the-art
approach on the already compact MobileNet V1&amp;V2 under an extremely large
compression ratio.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VirTex: Learning Visual Representations from Textual Annotations. (arXiv:2006.06666v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.06666">
<div class="article-summary-box-inner">
<span><p>The de-facto approach to many vision tasks is to start from pretrained visual
representations, typically learned via supervised training on ImageNet. Recent
methods have explored unsupervised pretraining to scale to vast quantities of
unlabeled images. In contrast, we aim to learn high-quality visual
representations from fewer images. To this end, we revisit supervised
pretraining, and seek data-efficient alternatives to classification-based
pretraining. We propose VirTex -- a pretraining approach using semantically
dense captions to learn visual representations. We train convolutional networks
from scratch on COCO Captions, and transfer them to downstream recognition
tasks including image classification, object detection, and instance
segmentation. On all tasks, VirTex yields features that match or exceed those
learned on ImageNet -- supervised or unsupervised -- despite using up to ten
times fewer images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subjective and Objective Quality Assessment of High Frame Rate Videos. (arXiv:2007.11634v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.11634">
<div class="article-summary-box-inner">
<span><p>High frame rate (HFR) videos are becoming increasingly common with the
tremendous popularity of live, high-action streaming content such as sports.
Although HFR contents are generally of very high quality, high bandwidth
requirements make them challenging to deliver efficiently, while simultaneously
maintaining their quality. To optimize trade-offs between bandwidth
requirements and video quality, in terms of frame rate adaptation, it is
imperative to understand the intricate relationship between frame rate and
perceptual video quality. Towards advancing progression in this direction we
designed a new subjective resource, called the LIVE-YouTube-HFR (LIVE-YT-HFR)
dataset, which is comprised of 480 videos having 6 different frame rates,
obtained from 16 diverse contents. In order to understand the combined effects
of compression and frame rate adjustment, we also processed videos at 5
compression levels at each frame rate. To obtain subjective labels on the
videos, we conducted a human study yielding 19,000 human quality ratings
obtained from a pool of 85 human subjects. We also conducted a holistic
evaluation of existing state-of-the-art Full and No-Reference video quality
algorithms, and statistically benchmarked their performance on the new
database. The LIVE-YT-HFR database has been made available online for public
use and evaluation purposes, with hopes that it will help advance research in
this exciting video technology direction. It may be obtained at
\url{https://live.ece.utexas.edu/research/LIVE_YT_HFR/LIVE_YT_HFR/index.html}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenRooms: An End-to-End Open Framework for Photorealistic Indoor Scene Datasets. (arXiv:2007.12868v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.12868">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework for creating large-scale photorealistic datasets
of indoor scenes, with ground truth geometry, material, lighting and semantics.
Our goal is to make the dataset creation process widely accessible,
transforming scans into photorealistic datasets with high-quality ground truth
for appearance, layout, semantic labels, high quality spatially-varying BRDF
and complex lighting, including direct, indirect and visibility components.
This enables important applications in inverse rendering, scene understanding
and robotics. We show that deep networks trained on the proposed dataset
achieve competitive performance for shape, material and lighting estimation on
real images, enabling photorealistic augmented reality applications, such as
object insertion and material editing. We also show our semantic labels may be
used for segmentation and multi-task learning. Finally, we demonstrate that our
framework may also be integrated with physics engines, to create virtual
robotics environments with unique ground truth such as friction coefficients
and correspondence to real scenes. The dataset and all the tools to create such
datasets will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonnegative Low Rank Tensor Approximation and its Application to Multi-dimensional Images. (arXiv:2007.14137v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.14137">
<div class="article-summary-box-inner">
<span><p>The main aim of this paper is to develop a new algorithm for computing
nonnegative low rank tensor approximation for nonnegative tensors that arise in
many multi-dimensional imaging applications. Nonnegativity is one of the
important property as each pixel value refers to nonzero light intensity in
image data acquisition. Our approach is different from classical nonnegative
tensor factorization (NTF) which requires each factorized matrix and/or tensor
to be nonnegative. In this paper, we determine a nonnegative low Tucker rank
tensor to approximate a given nonnegative tensor. We propose an alternating
projections algorithm for computing such nonnegative low rank tensor
approximation, which is referred to as NLRT. The convergence of the proposed
manifold projection method is established. Experimental results for synthetic
data and multi-dimensional images are presented to demonstrate the performance
of NLRT is better than state-of-the-art NTF methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmark for Anonymous Video Analytics. (arXiv:2009.14684v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14684">
<div class="article-summary-box-inner">
<span><p>Out-of-home audience measurement aims to count and characterize the people
exposed to advertising content in the physical world. While audience
measurement solutions based on computer vision are of increasing interest, no
commonly accepted benchmark exists to evaluate and compare their performance.
In this paper, we propose the first benchmark for digital out-of-home audience
measurement that evaluates the vision-based tasks of audience localization and
counting, and audience demographics. The benchmark is composed of a novel,
dataset captured at multiple locations and a set of performance measures. Using
the benchmark, we present an in-depth comparison of eight open-source
algorithms on four hardware platforms with GPU and CPU-optimized inferences and
of two commercial off-the-shelf solutions for localization, count, age, and
gender estimation. This benchmark and related open-source codes are available
at <a href="http://ava.eecs.qmul.ac.uk.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Once Quantization-Aware Training: High Performance Extremely Low-bit Architecture Search. (arXiv:2010.04354v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04354">
<div class="article-summary-box-inner">
<span><p>Quantization Neural Networks (QNN) have attracted a lot of attention due to
their high efficiency. To enhance the quantization accuracy, prior works mainly
focus on designing advanced quantization algorithms but still fail to achieve
satisfactory results under the extremely low-bit case. In this work, we take an
architecture perspective to investigate the potential of high-performance QNN.
Therefore, we propose to combine Network Architecture Search methods with
quantization to enjoy the merits of the two sides. However, a naive combination
inevitably faces unacceptable time consumption or unstable training problem. To
alleviate these problems, we first propose the joint training of architecture
and quantization with a shared step size to acquire a large number of quantized
models. Then a bit-inheritance scheme is introduced to transfer the quantized
models to the lower bit, which further reduces the time cost and meanwhile
improves the quantization accuracy. Equipped with this overall framework,
dubbed as Once Quantization-Aware Training~(OQAT), our searched model family,
OQATNets, achieves a new state-of-the-art compared with various architectures
under different bit-widths. In particular, OQAT-2bit-M achieves 61.6% ImageNet
Top-1 accuracy, outperforming 2-bit counterpart MobileNetV3 by a large margin
of 9% with 10% less computation cost. A series of quantization-friendly
architectures are identified easily and extensive analysis can be made to
summarize the interaction between quantization and neural architectures. Codes
and models are released at https://github.com/LaVieEnRoseSMZ/OQA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Predictive Visual Analytics System for Studying Neurodegenerative Disease based on DTI Fiber Tracts. (arXiv:2010.07047v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07047">
<div class="article-summary-box-inner">
<span><p>Diffusion tensor imaging (DTI) has been used to study the effects of
neurodegenerative diseases on neural pathways, which may lead to more reliable
and early diagnosis of these diseases as well as a better understanding of how
they affect the brain. We introduce an intelligent visual analytics system for
studying patient groups based on their labeled DTI fiber tract data and
corresponding statistics. The system's AI-augmented interface guides the user
through an organized and holistic analysis space, including the statistical
feature space, the physical space, and the space of patients over different
groups. We use a custom machine learning pipeline to help narrow down this
large analysis space, and then explore it pragmatically through a range of
linked visualizations. We conduct several case studies using real data from the
research database of Parkinson's Progression Markers Initiative.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate Dependent Video Quality Prediction. (arXiv:2010.13715v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13715">
<div class="article-summary-box-inner">
<span><p>We consider the problem of conducting frame rate dependent video quality
assessment (VQA) on videos of diverse frame rates, including high frame rate
(HFR) videos. More generally, we study how perceptual quality is affected by
frame rate, and how frame rate and compression combine to affect perceived
quality. We devise an objective VQA model called Space-Time GeneRalized
Entropic Difference (GREED) which analyzes the statistics of spatial and
temporal band-pass video coefficients. A generalized Gaussian distribution
(GGD) is used to model band-pass responses, while entropy variations between
reference and distorted videos under the GGD model are used to capture video
quality variations arising from frame rate changes. The entropic differences
are calculated across multiple temporal and spatial subbands, and merged using
a learned regressor. We show through extensive experiments that GREED achieves
state-of-the-art performance on the LIVE-YT-HFR Database when compared with
existing VQA models. The features used in GREED are highly generalizable and
obtain competitive performance even on standard, non-HFR VQA databases. The
implementation of GREED has been made available online:
https://github.com/pavancm/GREED
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale-covariant and scale-invariant Gaussian derivative networks. (arXiv:2011.14759v10 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14759">
<div class="article-summary-box-inner">
<span><p>This paper presents a hybrid approach between scale-space theory and deep
learning, where a deep learning architecture is constructed by coupling
parameterized scale-space operations in cascade. By sharing the learnt
parameters between multiple scale channels, and by using the transformation
properties of the scale-space primitives under scaling transformations, the
resulting network becomes provably scale covariant. By in addition performing
max pooling over the multiple scale channels, a resulting network architecture
for image classification also becomes provably scale invariant. We investigate
the performance of such networks on the MNISTLargeScale dataset, which contains
rescaled images from original MNIST over a factor of 4 concerning training data
and over a factor of 16 concerning testing data. It is demonstrated that the
resulting approach allows for scale generalization, enabling good performance
for classifying patterns at scales not present in the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v9 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01338">
<div class="article-summary-box-inner">
<span><p>Training deep learning models in technical domains is often accompanied by
the challenge that although the task is clear, insufficient data for training
is available. In this work, we propose a novel approach based on the
combination of Siamese networks and radial basis function networks to perform
data-efficient classification without pretraining by measuring the distance
between images in semantic space in a data-efficient manner. We develop the
models using three technical datasets, the NEU dataset, the BSD dataset, and
the TEX dataset. In addition to the technical domain, we show the general
applicability to classical datasets (cifar10 and MNIST) as well. The approach
is tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise
reduction of the number of samples available for training. The authors show
that the proposed approach outperforms the state-of-the-art models in the low
data regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Transformer. (arXiv:2012.09164v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09164">
<div class="article-summary-box-inner">
<span><p>Self-attention networks have revolutionized natural language processing and
are making impressive strides in image analysis tasks such as image
classification and object detection. Inspired by this success, we investigate
the application of self-attention networks to 3D point cloud processing. We
design self-attention layers for point clouds and use these to construct
self-attention networks for tasks such as semantic scene segmentation, object
part segmentation, and object classification. Our Point Transformer design
improves upon prior work across domains and tasks. For example, on the
challenging S3DIS dataset for large-scale semantic scene segmentation, the
Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the
strongest prior model by 3.3 absolute percentage points and crossing the 70%
mIoU threshold for the first time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Persistent Homology Topological Features to Characterize Medical Images: Case Studies on Lung and Brain Cancers. (arXiv:2012.12102v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.12102">
<div class="article-summary-box-inner">
<span><p>Tumor shape is a key factor that affects tumor growth and metastasis. This
paper proposes a topological feature computed by persistent homology to
characterize tumor progression from digital pathology and radiology images and
examines its effect on the time-to-event data. The proposed topological
features are invariant to scale-preserving transformation and can summarize
various tumor shape patterns. The topological features are represented in
functional space and used as functional predictors in a functional Cox
proportional hazards model. The proposed model enables interpretable inference
about the association between topological shape features and survival risks.
Two case studies are conducted using consecutive 143 lung cancer and 77 brain
tumor patients. The results of both studies show that the topological features
predict survival prognosis after adjusting clinical variables, and the
predicted high-risk groups have significantly (at the level of 0.001) worse
survival outcomes than the low-risk groups. Also, the topological shape
features found to be positively associated with survival hazards are irregular
and heterogeneous shape patterns, which are known to be related to tumor
progression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Memory Attention for Fast Video Semantic Segmentation. (arXiv:2101.01715v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.01715">
<div class="article-summary-box-inner">
<span><p>We propose a novel neural network module that transforms an existing
single-frame semantic segmentation model into a video semantic segmentation
pipeline. In contrast to prior works, we strive towards a simple, fast, and
general module that can be integrated into virtually any single-frame
architecture. Our approach aggregates a rich representation of the semantic
information in past frames into a memory module. Information stored in the
memory is then accessed through an attention mechanism. In contrast to previous
memory-based approaches, we propose a fast local attention layer, providing
temporal appearance cues in the local region of prior frames. We further fuse
these cues with an encoding of the current frame through a second
attention-based module. The segmentation decoder processes the fused
representation to predict the final semantic segmentation. We integrate our
approach into two popular semantic segmentation networks: ERFNet and PSPNet. We
observe an improvement in segmentation performance on Cityscapes by 1.7% and
2.1% in mIoU respectively, while increasing inference time of ERFNet by only
1.5ms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Summarization Using Deep Neural Networks: A Survey. (arXiv:2101.06072v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06072">
<div class="article-summary-box-inner">
<span><p>Video summarization technologies aim to create a concise and complete
synopsis by selecting the most informative parts of the video content. Several
approaches have been developed over the last couple of decades and the current
state of the art is represented by methods that rely on modern deep neural
network architectures. This work focuses on the recent advances in the area and
provides a comprehensive survey of the existing deep-learning-based methods for
generic video summarization. After presenting the motivation behind the
development of technologies for video summarization, we formulate the video
summarization task and discuss the main characteristics of a typical
deep-learning-based analysis pipeline. Then, we suggest a taxonomy of the
existing algorithms and provide a systematic review of the relevant literature
that shows the evolution of the deep-learning-based video summarization
technologies and leads to suggestions for future developments. We then report
on protocols for the objective evaluation of video summarization algorithms and
we compare the performance of several deep-learning-based approaches. Based on
the outcomes of these comparisons, as well as some documented considerations
about the amount of annotated data and the suitability of evaluation protocols,
we indicate potential future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Variable Models for Visual Question Answering. (arXiv:2101.06399v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06399">
<div class="article-summary-box-inner">
<span><p>Current work on Visual Question Answering (VQA) explore deterministic
approaches conditioned on various types of image and question features. We
posit that, in addition to image and question pairs, other modalities are
useful for teaching machine to carry out question answering. Hence in this
paper, we propose latent variable models for VQA where extra information (e.g.
captions and answer categories) are incorporated as latent variables, which are
observed during training but in turn benefit question-answering performance at
test time. Experiments on the VQA v2.0 benchmarking dataset demonstrate the
effectiveness of our proposed models: they improve over strong baselines,
especially those that do not rely on extensive language-vision pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Better Explanations of Class Activation Mapping. (arXiv:2102.05228v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05228">
<div class="article-summary-box-inner">
<span><p>Increasing demands for understanding the internal behavior of convolutional
neural networks (CNNs) have led to remarkable improvements in explanation
methods. Particularly, several class activation mapping (CAM) based methods,
which generate visual explanation maps by a linear combination of activation
maps from CNNs, have been proposed. However, the majority of the methods lack a
clear theoretical basis on how they assign the coefficients of the linear
combination. In this paper, we revisit the intrinsic linearity of CAM with
respect to the activation maps; we construct an explanation model of CNN as a
linear function of binary variables that denote the existence of the
corresponding activation maps. With this approach, the explanation model can be
determined by additive feature attribution methods in an analytic manner. We
then demonstrate the adequacy of SHAP values, which is a unique solution for
the explanation model with a set of desirable properties, as the coefficients
of CAM. Since the exact SHAP values are unattainable, we introduce an efficient
approximation method, LIFT-CAM, based on DeepLIFT. Our proposed LIFT-CAM can
estimate the SHAP values of the activation maps with high speed and accuracy.
Furthermore, it greatly outperforms other previous CAM-based methods in both
qualitative and quantitative aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness in Compressed Neural Networks for Object Detection. (arXiv:2102.05509v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05509">
<div class="article-summary-box-inner">
<span><p>Model compression techniques allow to significantly reduce the computational
cost associated with data processing by deep neural networks with only a minor
decrease in average accuracy. Simultaneously, reducing the model size may have
a large effect on noisy cases or objects belonging to less frequent classes. It
is a crucial problem from the perspective of the models' safety, especially for
object detection in the autonomous driving setting, which is considered in this
work. It was shown in the paper that the sensitivity of compressed models to
different distortion types is nuanced, and some of the corruptions are heavily
impacted by the compression methods (i.e., additive noise), while others (blur
effect) are only slightly affected. A common way to improve the robustness of
models is to use data augmentation, which was confirmed to positively affect
models' robustness, also for highly compressed models. It was further shown
that while data imbalance methods brought only a slight increase in accuracy
for the baseline model (without compression), the impact was more striking at
higher compression rates for the structured pruning. Finally, methods for
handling data imbalance brought a significant improvement of the pruned models'
worst-detected class accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReRankMatch: Semi-Supervised Learning with Semantics-Oriented Similarity Representation. (arXiv:2102.06328v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06328">
<div class="article-summary-box-inner">
<span><p>This paper proposes integrating semantics-oriented similarity representation
into RankingMatch, a recently proposed semi-supervised learning method. Our
method, dubbed ReRankMatch, aims to deal with the case in which labeled and
unlabeled data share non-overlapping categories. ReRankMatch encourages the
model to produce the similar image representations for the samples likely
belonging to the same class. We evaluate our method on various datasets such as
CIFAR-10, CIFAR-100, SVHN, STL-10, and Tiny ImageNet. We obtain promising
results (4.21% error rate on CIFAR-10 with 4000 labels, 22.32% error rate on
CIFAR-100 with 10000 labels, and 2.19% error rate on SVHN with 1000 labels)
when the amount of labeled data is sufficient to learn semantics-oriented
similarity representation. The code is made publicly available at
https://github.com/tqtrunghnvn/ReRankMatch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equivariant Filters for Efficient Tracking in 3D Imaging. (arXiv:2103.10255v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10255">
<div class="article-summary-box-inner">
<span><p>We demonstrate an object tracking method for 3D images with fixed
computational cost and state-of-the-art performance. Previous methods predicted
transformation parameters from convolutional layers. We instead propose an
architecture that does not include either flattening of convolutional features
or fully connected layers, but instead relies on equivariant filters to
preserve transformations between inputs and outputs (e.g. rot./trans. of inputs
rotate/translate outputs). The transformation is then derived in closed form
from the outputs of the filters. This method is useful for applications
requiring low latency, such as real-time tracking. We demonstrate our model on
synthetically augmented adult brain MRI, as well as fetal brain MRI, which is
the intended use-case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining Latent Classes for Few-shot Segmentation. (arXiv:2103.15402v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15402">
<div class="article-summary-box-inner">
<span><p>Few-shot segmentation (FSS) aims to segment unseen classes given only a few
annotated samples. Existing methods suffer the problem of feature undermining,
i.e. potential novel classes are treated as background during training phase.
Our method aims to alleviate this problem and enhance the feature embedding on
latent novel classes. In our work, we propose a novel joint-training framework.
Based on conventional episodic training on support-query pairs, we add an
additional mining branch that exploits latent novel classes via transferable
sub-clusters, and a new rectification technique on both background and
foreground categories to enforce more stable prototypes. Over and above that,
our transferable sub-cluster has the ability to leverage extra unlabeled data
for further feature enhancement. Extensive experiments on two FSS benchmarks
demonstrate that our method outperforms previous state-of-the-art by a large
margin of 3.7% mIOU on PASCAL-5i and 7.0% mIOU on COCO-20i at the cost of 74%
fewer parameters and 2.5x faster inference speed. The source code is available
at https://github.com/LiheYoung/MiningFSS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIMstack: A Generative Shape and Instance Model for Unordered Object Stacks. (arXiv:2103.16442v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16442">
<div class="article-summary-box-inner">
<span><p>By estimating 3D shape and instances from a single view, we can capture
information about an environment quickly, without the need for comprehensive
scanning and multi-view fusion. Solving this task for composite scenes (such as
object stacks) is challenging: occluded areas are not only ambiguous in shape
but also in instance segmentation; multiple decompositions could be valid. We
observe that physics constrains decomposition as well as shape in occluded
regions and hypothesise that a latent space learned from scenes built under
physics simulation can serve as a prior to better predict shape and instances
in occluded regions. To this end we propose SIMstack, a depth-conditioned
Variational Auto-Encoder (VAE), trained on a dataset of objects stacked under
physics simulation. We formulate instance segmentation as a centre voting task
which allows for class-agnostic detection and doesn't require setting the
maximum number of objects in the scene. At test time, our model can generate 3D
shape and instance segmentation from a single depth view, probabilistically
sampling proposals for the occluded region from the learned latent space. Our
method has practical applications in providing robots some of the ability
humans have to make rapid intuitive inferences of partially observed scenes. We
demonstrate an application for precise (non-disruptive) object grasping of
unknown objects from a single depth view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Misclassification-Aware Gaussian Smoothing and Mixed Augmentations improves Robustness against Domain Shifts. (arXiv:2104.01231v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01231">
<div class="article-summary-box-inner">
<span><p>Deep neural networks achieve high prediction accuracy when the train and test
distributions coincide. In practice though, various types of corruptions can
deviate from this setup and cause severe performance degradations. Few methods
have been proposed to address generalization in the presence of unforeseen
domain shifts. In this paper, we propose a misclassification-aware Gaussian
smoothing approach, coupled with mixed data augmentations, for improving
robustness of image classifiers against a variety of corruptions while still
maintaining high clean accuracy. We show that our method improves upon the
state-of-the-art in robustness and uncertainty calibration on several image
classification benchmarks and network architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hypothesis-driven Stream Learning with Augmented Memory. (arXiv:2104.02206v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02206">
<div class="article-summary-box-inner">
<span><p>Stream learning refers to the ability to acquire and transfer knowledge
across a continuous stream of data without forgetting and without repeated
passes over the data. A common way to avoid catastrophic forgetting is to
intersperse new examples with replays of old examples stored as image pixels or
reproduced by generative models. Here, we consider stream learning in image
classification tasks and propose a novel hypotheses-driven Augmented Memory
Network, which efficiently consolidates previous knowledge with a limited
number of hypotheses in the augmented memory and replays relevant hypotheses to
avoid catastrophic forgetting. The advantages of hypothesis-driven replay over
pixel-level replay and generative replay are two-fold. First, hypothesis-based
knowledge consolidation avoids redundant information in the image pixel space
and makes memory usage more efficient. Second, hypotheses in the augmented
memory can be re-used for learning new tasks, improving generalization and
transfer learning ability. We evaluated our method on three stream learning
object recognition datasets. Our method performs comparably well or better than
state-of-the-art methods, while offering more efficient memory usage. All
source code and data are publicly available
https://github.com/kreimanlab/AugMem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval. (arXiv:2104.08271v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08271">
<div class="article-summary-box-inner">
<span><p>In recent years, considerable progress on the task of text-video retrieval
has been achieved by leveraging large-scale pretraining on visual and audio
datasets to construct powerful video encoders. By contrast, despite the natural
symmetry, the design of effective algorithms for exploiting large-scale
language pretraining remains under-explored. In this work, we are the first to
investigate the design of such algorithms and propose a novel generalized
distillation method, TeachText, which leverages complementary cues from
multiple text encoders to provide an enhanced supervisory signal to the
retrieval model. Moreover, we extend our method to video side modalities and
show that we can effectively reduce the number of used modalities at test time
without compromising performance. Our approach advances the state of the art on
several video retrieval benchmarks by a significant margin and adds no
computational overhead at test time. Last but not least, we show an effective
application of our method for eliminating noise from retrieval datasets. Code
and data can be found at https://www.robots.ox.ac.uk/~vgg/research/teachtext/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?. (arXiv:2104.09425v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09425">
<div class="article-summary-box-inner">
<span><p>While additional training data improves the robustness of deep neural
networks against adversarial examples, it presents the challenge of curating a
large number of specific real-world samples. We circumvent this challenge by
using additional data from proxy distributions learned by state-of-the-art
generative models. We first seek to formally understand the transfer of
robustness from classifiers trained on proxy distributions to the real data
distribution. We prove that the difference between the robustness of a
classifier on the two distributions is upper bounded by the conditional
Wasserstein distance between them. Motivated by our result, we next ask how to
empirically select an appropriate generative model? We find that existing
distance metrics, such as FID, fail to correctly determine the robustness
transfer from proxy distributions. We propose a robust discrimination approach,
which measures the distinguishability of synthetic and real samples under
adversarial perturbations. Our approach accurately predicts the robustness
transfer from different proxy distributions. After choosing a proxy
distribution, the next question is which samples are most beneficial? We
successfully optimize this selection by estimating the importance of each
sample in robustness transfer. Finally, using our selection criterion for proxy
distribution and individual samples, we curate a set of ten million most
beneficial synthetic samples for robust training on the CIFAR-10 dataset. Using
this set we improve robust accuracy by up to 7.5% and 6.7% in $\ell_{\infty}$
and $\ell_2$ threat model, and certified robust accuracy by 7.6% in $\ell_2$
threat model over baselines not using proxy distributions on the CIFAR-10
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anchor-based Plain Net for Mobile Image Super-Resolution. (arXiv:2105.09750v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09750">
<div class="article-summary-box-inner">
<span><p>Along with the rapid development of real-world applications, higher
requirements on the accuracy and efficiency of image super-resolution (SR) are
brought forward. Though existing methods have achieved remarkable success, the
majority of them demand plenty of computational resources and large amount of
RAM, and thus they can not be well applied to mobile device. In this paper, we
aim at designing efficient architecture for 8-bit quantization and deploy it on
mobile device. First, we conduct an experiment about meta-node latency by
decomposing lightweight SR architectures, which determines the portable
operations we can utilize. Then, we dig deeper into what kind of architecture
is beneficial to 8-bit quantization and propose anchor-based plain net (ABPN).
Finally, we adopt quantization-aware training strategy to further boost the
performance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in
terms of PSNR, while satisfying realistic needs at the same time. Code is
avaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Safety Metrics for Semantic Segmentation in Autonomous Driving. (arXiv:2105.10142v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10142">
<div class="article-summary-box-inner">
<span><p>Within the context of autonomous driving, safety-related metrics for deep
neural networks have been widely studied for image classification and object
detection. In this paper, we further consider safety-aware correctness and
robustness metrics specialized for semantic segmentation. The novelty of our
proposal is to move beyond pixel-level metrics: Given two images with each
having N pixels being class-flipped, the designed metrics should, depending on
the clustering of pixels being class-flipped or the location of occurrence,
reflect a different level of safety criticality. The result evaluated on an
autonomous driving dataset demonstrates the validity and practicality of our
proposed methodology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EDDA: Explanation-driven Data Augmentation to Improve Explanation Faithfulness. (arXiv:2105.14162v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14162">
<div class="article-summary-box-inner">
<span><p>Recent years have seen the introduction of a range of methods for post-hoc
explainability of image classifier predictions. However, these post-hoc
explanations may not always be faithful to classifier predictions, which poses
a significant challenge when attempting to debug models based on such
explanations. To this end, we seek a methodology that can improve the
faithfulness of an explanation method with respect to model predictions which
does not require ground truth explanations. We achieve this through a novel
explanation-driven data augmentation (EDDA) technique that augments the
training data with occlusions inferred from model explanations; this is based
on the simple motivating principle that \emph{if} the explainer is faithful to
the model \emph{then} occluding salient regions for the model prediction should
decrease the model confidence in the prediction, while occluding non-salient
regions should not change the prediction. To verify that the proposed
augmentation method has the potential to improve faithfulness, we evaluate EDDA
using a variety of datasets and classification models. We demonstrate
empirically that our approach leads to a significant increase of faithfulness,
which can facilitate better debugging and successful deployment of image
classification models in real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Closer Look at the Uncertainty Estimation in Semantic Segmentation under Distributional Shift. (arXiv:2106.00076v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00076">
<div class="article-summary-box-inner">
<span><p>While recent computer vision algorithms achieve impressive performance on
many benchmarks, they lack robustness - presented with an image from a
different distribution, (e.g. weather or lighting conditions not considered
during training), they may produce an erroneous prediction. Therefore, it is
desired that such a model will be able to reliably predict its confidence
measure. In this work, uncertainty estimation for the task of semantic
segmentation is evaluated under a varying level of domain shift: in a
cross-dataset setting and when adapting a model trained on data from the
simulation. It was shown that simple color transformations already provide a
strong baseline, comparable to using more sophisticated style-transfer data
augmentation. Further, by constructing an ensemble consisting of models using
different backbones and/or augmentation methods, it was possible to improve
significantly model performance in terms of overall accuracy and uncertainty
estimation under the domain shift setting. The Expected Calibration Error (ECE)
on challenging GTA to Cityscapes adaptation was reduced from 4.05 to the
competitive value of 1.1. Further, an ensemble of models was utilized in the
self-training setting to improve the pseudo-labels generation, which resulted
in a significant gain in the final model accuracy, compared to the standard
fine-tuning (without ensemble).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAR Image Change Detection Based on Multiscale Capsule Network. (arXiv:2106.06896v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06896">
<div class="article-summary-box-inner">
<span><p>Traditional change detection methods based on convolutional neural networks
(CNNs) face the challenges of speckle noise and deformation sensitivity for
synthetic aperture radar images. To mitigate these issues, we proposed a
Multiscale Capsule Network (Ms-CapsNet) to extract the discriminative
information between the changed and unchanged pixels. On the one hand, the
capsule module is employed to exploit the spatial relationship of features.
Therefore, equivariant properties can be achieved by aggregating the features
from different positions. On the other hand, an adaptive fusion convolution
(AFC) module is designed for the proposed Ms-CapsNet. Higher semantic features
can be captured for the primary capsules. Feature extracted by the AFC module
significantly improves the robustness to speckle noise. The effectiveness of
the proposed Ms-CapsNet is verified on three real SAR datasets. The comparison
experiments with four state-of-the-art methods demonstrated the efficiency of
the proposed method. Our codes are available at
https://github.com/summitgao/SAR_CD_MS_CapsNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Siamese Network Training Using Artificial Triplets By Sampling and Image Transformation. (arXiv:2106.07015v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07015">
<div class="article-summary-box-inner">
<span><p>The device used in this work detects the objects over the surface of the
water using two thermal cameras which aid the users to detect and avoid the
objects in scenarios where the human eyes cannot (night, fog, etc.). To avoid
the obstacle collision autonomously, it is required to track the objects in
real-time and assign a specific identity to each object to determine its
dynamics (trajectory, velocity, etc.) for making estimated collision
predictions. In the following work, a Machine Learning (ML) approach for
Computer Vision (CV) called Convolutional Neural Network (CNN) was used using
TensorFlow as the high-level programming environment in Python. To validate the
algorithm a test set was generated using an annotation tool that was created
during the work for proper evaluation. Once validated, the algorithm was
deployed on the platform and tested with the sequence generated by the test
boat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastAno: Fast Anomaly Detection via Spatio-temporal Patch Transformation. (arXiv:2106.08613v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08613">
<div class="article-summary-box-inner">
<span><p>Video anomaly detection has gained significant attention due to the
increasing requirements of automatic monitoring for surveillance videos.
Especially, the prediction based approach is one of the most studied methods to
detect anomalies by predicting frames that include abnormal events in the test
set after learning with the normal frames of the training set. However, a lot
of prediction networks are computationally expensive owing to the use of
pre-trained optical flow networks, or fail to detect abnormal situations
because of their strong generative ability to predict even the anomalies. To
address these shortcomings, we propose spatial rotation transformation (SRT)
and temporal mixing transformation (TMT) to generate irregular patch cuboids
within normal frame cuboids in order to enhance the learning of normal
features. Additionally, the proposed patch transformation is used only during
the training phase, allowing our model to detect abnormal frames at fast speed
during inference. Our model is evaluated on three anomaly detection benchmarks,
achieving competitive accuracy and surpassing all the previous works in terms
of speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Visual Context for Weakly Supervised Person Search. (arXiv:2106.10506v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10506">
<div class="article-summary-box-inner">
<span><p>Person search has recently emerged as a challenging task that jointly
addresses pedestrian detection and person re-identification. Existing
approaches follow a fully supervised setting where both bounding box and
identity annotations are available. However, annotating identities is
labor-intensive, limiting the practicability and scalability of current
frameworks. This paper inventively considers weakly supervised person search
with only bounding box annotations. We proposed to address this novel task by
investigating three levels of context clues (i.e., detection, memory and scene)
in unconstrained natural images. The first two are employed to promote local
and global discriminative capabilities, while the latter enhances clustering
accuracy. Despite its simple design, our CGPS achieves 80.0% in mAP on
CUHK-SYSU, boosting the baseline model by 8.8%. Surprisingly, it even achieves
comparable performance with several supervised person search models. Our code
is available at https://github.com/ljpadam/CGPS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13301">
<div class="article-summary-box-inner">
<span><p>Physics perception very often faces the problem that only limited data or
partial measurements on the scene are available. In this work, we propose a
strategy to learn the full state of sloshing liquids from measurements of the
free surface. Our approach is based on recurrent neural networks (RNN) that
project the limited information available to a reduced-order manifold so as to
not only reconstruct the unknown information, but also to be capable of
performing fluid reasoning about future scenarios in real time. To obtain
physically consistent predictions, we train deep neural networks on the
reduced-order manifold that, through the employ of inductive biases, ensure the
fulfillment of the principles of thermodynamics. RNNs learn from history the
required hidden information to correlate the limited information with the
latent space where the simulation occurs. Finally, a decoder returns data back
to the high-dimensional manifold, so as to provide the user with insightful
information in the form of augmented reality. This algorithm is connected to a
computer vision system to test the performance of the proposed methodology with
real information, resulting in a system capable of understanding and predicting
future states of the observed fluid in real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DenseTNT: Waymo Open Dataset Motion Prediction Challenge 1st Place Solution. (arXiv:2106.14160v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14160">
<div class="article-summary-box-inner">
<span><p>In autonomous driving, goal-based multi-trajectory prediction methods are
proved to be effective recently, where they first score goal candidates, then
select a final set of goals, and finally complete trajectories based on the
selected goals. However, these methods usually involve goal predictions based
on sparse predefined anchors. In this work, we propose an anchor-free model,
named DenseTNT, which performs dense goal probability estimation for trajectory
prediction. Our model achieves state-of-the-art performance, and ranks 1st on
the Waymo Open Dataset Motion Prediction Challenge. Project page is at
https://github.com/Tsinghua-MARS-Lab/DenseTNT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?. (arXiv:2106.15475v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15475">
<div class="article-summary-box-inner">
<span><p>Incorrectly labeled examples, or label noise, is common in real-world
computer vision datasets. While the impact of label noise on learning in deep
neural networks has been studied in prior work, these studies have exclusively
focused on homogeneous label noise, i.e., the degree of label noise is the same
across all categories. However, in the real-world, label noise is often
heterogeneous, with some categories being affected to a greater extent than
others. Here, we address this gap in the literature. We hypothesized that
heterogeneous label noise would only affect the classes that had label noise
unless there was transfer from those classes to the classes without label
noise. To test this hypothesis, we designed a series of computer vision studies
using MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous
label noise during the training of multi-class, multi-task, and multi-label
systems. Our results provide evidence in support of our hypothesis: label noise
only affects the class affected by it unless there is transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Source domain adaptation via supervised contrastive learning and confident consistency regularization. (arXiv:2106.16093v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.16093">
<div class="article-summary-box-inner">
<span><p>Multi-Source Unsupervised Domain Adaptation (multi-source UDA) aims to learn
a model from several labeled source domains while performing well on a
different target domain where only unlabeled data are available at training
time. To align source and target features distributions, several recent works
use source and target explicit statistics matching such as features moments or
class centroids. Yet, these approaches do not guarantee class conditional
distributions alignment across domains. In this work, we propose a new
framework called Contrastive Multi-Source Domain Adaptation (CMSDA) for
multi-source UDA that addresses this limitation. Discriminative features are
learned from interpolated source examples via cross entropy minimization and
from target examples via consistency regularization and hard pseudo-labeling.
Simultaneously, interpolated source examples are leveraged to align source
class conditional distributions through an interpolated version of the
supervised contrastive loss. This alignment leads to more general and
transferable features which further improves the generalization on the target
domain. Extensive experiments have been carried out on three standard
multi-source UDA datasets where our method reports state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Graph-Based Deep Learning for Computational Histopathology. (arXiv:2107.00272v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00272">
<div class="article-summary-box-inner">
<span><p>With the remarkable success of representation learning for prediction
problems, we have witnessed a rapid expansion of the use of machine learning
and deep learning for the analysis of digital pathology and biopsy image
patches. However, learning over patch-wise features using convolutional neural
networks limits the ability of the model to capture global contextual
information and comprehensively model tissue composition. The phenotypical and
topological distribution of constituent histological entities play a critical
role in tissue diagnosis. As such, graph data representations and deep learning
have attracted significant attention for encoding tissue representations, and
capturing intra- and inter- entity level interactions. In this review, we
provide a conceptual grounding for graph analytics in digital pathology,
including entity-graph construction and graph architectures, and present their
current success for tumor localization and classification, tumor invasion and
staging, image retrieval, and survival prediction. We provide an overview of
these methods in a systematic manner organized by the graph representation of
the input image, scale, and organ on which they operate. We also outline the
limitations of existing techniques, and suggest potential future research
directions in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise Labels. (arXiv:2107.01002v2 [cs.NI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01002">
<div class="article-summary-box-inner">
<span><p>We introduce WiCluster, a new machine learning (ML) approach for passive
indoor positioning using radio frequency (RF) channel state information (CSI).
WiCluster can predict both a zone-level position and a precise 2D or 3D
position, without using any precise position labels during training. Prior
CSI-based indoor positioning work has relied on non-parametric approaches using
digital signal-processing (DSP) and, more recently, parametric approaches
(e.g., fully supervised ML methods). However these do not handle the complexity
of real-world environments well and do not meet requirements for large-scale
commercial deployments: the accuracy of DSP-based method deteriorates
significantly in non-line-of-sight conditions, while supervised ML methods need
large amounts of hard-to-acquire centimeter accuracy position labels. In
contrast, WiCluster is precise, requires weaker label-information that can be
easily collected, and works well in non-line-of-sight conditions. Our first
contribution is a novel dimensionality reduction method for charting. It
combines a triplet-loss with a multi-scale clustering-loss to map the
high-dimensional CSI representation to a 2D/3D latent space. Our second
contribution is two weakly supervised losses that map this latent space into a
Cartesian map, resulting in meter-accuracy position results. These losses only
require simple to acquire priors: a sketch of the floorplan, approximate
access-point locations and a few CSI packets that are labelled with the
corresponding zone in the floorplan. Thirdly, we report results and a
robustness study for 2D positioning in two single-floor office buildings and 3D
positioning in a two-story home.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Network for Significant Stenosis Detection in CCTA of Coronary Arteries. (arXiv:2107.03035v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03035">
<div class="article-summary-box-inner">
<span><p>Coronary artery disease (CAD) has posed a leading threat to the lives of
cardiovascular disease patients worldwide for a long time. Therefore, automated
diagnosis of CAD has indispensable significance in clinical medicine. However,
the complexity of coronary artery plaques that cause CAD makes the automatic
detection of coronary artery stenosis in Coronary CT angiography (CCTA) a
difficult task. In this paper, we propose a Transformer network (TR-Net) for
the automatic detection of significant stenosis (i.e. luminal narrowing &gt; 50%)
while practically completing the computer-assisted diagnosis of CAD. The
proposed TR-Net introduces a novel Transformer, and tightly combines
convolutional layers and Transformer encoders, allowing their advantages to be
demonstrated in the task. By analyzing semantic information sequences, TR-Net
can fully understand the relationship between image information in each
position of a multiplanar reformatted (MPR) image, and accurately detect
significant stenosis based on both local and global information. We evaluate
our TR-Net on a dataset of 76 patients from different patients annotated by
experienced radiologists. Experimental results illustrate that our TR-Net has
achieved better results in ACC (0.92), Spec (0.96), PPV (0.84), F1 (0.79) and
MCC (0.74) indicators compared with the state-of-the-art methods. The source
code is publicly available from the link (https://github.com/XinghuaMa/TR-Net).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical Inspection of the Silicon Micro-strip Sensors for the CBM Experiment employing Artificial Intelligence. (arXiv:2107.07714v2 [physics.ins-det] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07714">
<div class="article-summary-box-inner">
<span><p>Optical inspection of 1191 silicon micro-strip sensors was performed using a
custom made optical inspection setup, employing a machine-learning based
approach for the defect analysis and subsequent quality assurance. Furthermore,
metrological control of the sensor's surface was performed. In this manuscript,
we present the analysis of various sensor surface defects. Among these are
implant breaks, p-stop breaks, aluminium strip opens, aluminium strip shorts,
surface scratches, double metallization layer defects, passivation layer
defects, bias resistor defects as well as dust particle identification. The
defect detection was done using the application of Convolutional Deep Neural
Networks (CDNNs). From this, defective strips and defect clusters were
identified, as well as a 2D map of the defects using their geometrical
positions on the sensor was performed. Based on the total number of defects
found on the sensor's surface, a method for the estimation of sensor's overall
quality grade and quality score was proposed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S2Looking: A Satellite Side-Looking Dataset for Building Change Detection. (arXiv:2107.09244v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09244">
<div class="article-summary-box-inner">
<span><p>Building change detection underpins many important applications, especially
in the military and crisis management domains. Recent methods used for change
detection have shifted towards deep-learning, which depends on the quality of
its training data. The assembly of large-scale annotated satellite imagery
datasets is therefore essential for global building change surveillance.
Existing datasets almost exclusively offer near-nadir viewing angles. This
limits the range of changes that can be detected. By offering larger
observation ranges, the scroll imaging mode of optical satellites presents an
opportunity to overcome this restriction. This paper therefore introduces
S2Looking, a building change detection dataset that contains large-scale
side-looking satellite images captured at various off-nadir angles. The dataset
consists of 5000 bitemporal image pairs of rural areas and more than 65,920
annotated instances of changes throughout the world. The dataset can be used to
train deep-learning-based change detection algorithms. It expands upon existing
datasets by providing: 1) larger viewing angles; 2) large illumination
variances; and 3) the added complexity of rural images. To facilitate use of
the dataset, a benchmark task has been established and preliminary tests
suggest deep-learning algorithms find the dataset significantly more
challenging than the closest competing near-nadir dataset, LEVIR-CD+. S2Looking
may therefore promote important advances in existing building change detection
algorithms. The dataset is available at https://github.com/S2Looking/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transductive Maximum Margin Classifier for Few-Shot Learning. (arXiv:2107.11975v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11975">
<div class="article-summary-box-inner">
<span><p>Few-shot learning aims to train a classifier that can generalize well when
just a small number of labeled examples per class are given. We introduce a
transductive maximum margin classifier for few-shot learning (FS-TMMC). The
basic idea of the classical maximum margin classifier is to solve an optimal
prediction function so that the training data can be correctly classified by
the resulting classifer with the largest geometric margin. In few-shot
learning, it is challenging to find such classifiers with good generalization
ability due to the insufficiency of training data in the support set. FS-TMMC
leverages the unlabeled query examples to adjust the separating hyperplane of
the maximum margin classifier such that the prediction function is optimal on
both the support and query sets. Furthermore, we use an efficient and effective
quasi-Newton algorithm, the L-BFGS method for optimization. Experimental
results on three standard few-shot learning benchmarks including miniImagenet,
tieredImagenet and CUB show that our method achieves state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting. (arXiv:2107.12674v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12674">
<div class="article-summary-box-inner">
<span><p>Autonomous driving gained huge traction in recent years, due to its potential
to change the way we commute. Much effort has been put into trying to estimate
the state of a vehicle. Meanwhile, learning to forecast the state of a vehicle
ahead introduces new capabilities, such as predicting dangerous situations.
Moreover, forecasting brings new supervision opportunities by learning to
predict richer a context, expressed by multiple horizons. Intuitively, a video
stream originated from a front-facing camera is necessary because it encodes
information about the upcoming road. Besides, historical traces of the
vehicle's states give more context. In this paper, we tackle multi-horizon
forecasting of vehicle states by fusing the two modalities. We design and
experiment with 3 end-to-end architectures that exploit 3D convolutions for
visual features extraction and 1D convolutions for features extraction from
speed and steering angle traces. To demonstrate the effectiveness of our
method, we perform extensive experiments on two publicly available real-world
datasets, Comma2k19 and the Udacity challenge. We show that we are able to
forecast a vehicle's state to various horizons, while outperforming the current
state-of-the-art results on the related task of driving state estimation. We
examine the contribution of vision features, and find that a model fed with
vision features achieves an error that is 56.6% and 66.9% of the error of a
model that doesn't use those features, on the Udacity and Comma2k19 datasets
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation. (arXiv:2108.02425v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02425">
<div class="article-summary-box-inner">
<span><p>Grasping in cluttered scenes has always been a great challenge for robots,
due to the requirement of the ability to well understand the scene and object
information. Previous works usually assume that the geometry information of the
objects is available, or utilize a step-wise, multi-stage strategy to predict
the feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF
grasp pose estimation as a simultaneous multi-task learning problem. In a
unified framework, we jointly predict the feasible 6-DoF grasp poses, instance
semantic segmentation, and collision information. The whole framework is
jointly optimized and end-to-end differentiable. Our model is evaluated on
large-scale benchmarks as well as the real robot system. On the public dataset,
our method outperforms prior state-of-the-art methods by a large margin (+4.08
AP). We also demonstrate the implementation of our model on a real robotic
platform and show that the robot can accurately grasp target objects in
cluttered scenarios with a high success rate. Project link:
https://openbyterobotics.github.io/sscl
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRAEM -- A discriminatively trained reconstruction embedding for surface anomaly detection. (arXiv:2108.07610v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07610">
<div class="article-summary-box-inner">
<span><p>Visual surface anomaly detection aims to detect local image regions that
significantly deviate from normal appearance. Recent surface anomaly detection
methods rely on generative models to accurately reconstruct the normal areas
and to fail on anomalies. These methods are trained only on anomaly-free
images, and often require hand-crafted post-processing steps to localize the
anomalies, which prohibits optimizing the feature extraction for maximal
detection capability. In addition to reconstructive approach, we cast surface
anomaly detection primarily as a discriminative problem and propose a
discriminatively trained reconstruction anomaly embedding model (DRAEM). The
proposed method learns a joint representation of an anomalous image and its
anomaly-free reconstruction, while simultaneously learning a decision boundary
between normal and anomalous examples. The method enables direct anomaly
localization without the need for additional complicated post-processing of the
network output and can be trained using simple and general anomaly simulations.
On the challenging MVTec anomaly detection dataset, DRAEM outperforms the
current state-of-the-art unsupervised methods by a large margin and even
delivers detection performance close to the fully-supervised methods on the
widely used DAGM surface-defect detection dataset, while substantially
outperforming them in localization accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Journey from SDRTV to HDRTV. (arXiv:2108.07978v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07978">
<div class="article-summary-box-inner">
<span><p>Nowadays modern displays are capable to render video content with high
dynamic range (HDR) and wide color gamut (WCG). However, most available
resources are still in standard dynamic range (SDR). Therefore, there is an
urgent demand to transform existing SDR-TV contents into their HDR-TV versions.
In this paper, we conduct an analysis of SDRTV-to-HDRTV task by modeling the
formation of SDRTV/HDRTV content. Base on the analysis, we propose a three-step
solution pipeline including adaptive global color mapping, local enhancement
and highlight generation. Moreover, the above analysis inspires us to present a
lightweight network that utilizes global statistics as guidance to conduct
image-adaptive color mapping. In addition, we construct a dataset using HDR
videos in HDR10 standard, named HDRTV1K, and select five metrics to evaluate
the results of SDRTV-to-HDRTV algorithms. Furthermore, our final results
achieve state-of-the-art performance in quantitative comparisons and visual
quality. The code and dataset are available at
https://github.com/chxy95/HDRTVNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PW-MAD: Pixel-wise Supervision for Generalized Face Morphing Attack Detection. (arXiv:2108.10291v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10291">
<div class="article-summary-box-inner">
<span><p>A face morphing attack image can be verified to multiple identities, making
this attack a major vulnerability to processes based on identity verification,
such as border checks. Various methods have been proposed to detect face
morphing attacks, however, with low generalizability to unexpected
post-morphing processes. A major post-morphing process is the print and scan
operation performed in many countries when issuing a passport or identity
document. In this work, we address this generalization problem by adapting a
pixel-wise supervision approach where we train a network to classify each pixel
of the image into an attack or not, rather than only having one label for the
whole image. Our pixel-wise morphing attack detection (PW-MAD) solution proved
to perform more accurately than a set of established baselines. More
importantly, PW-MAD shows high generalizability in comparison to related works,
when evaluated on unknown re-digitized attacks. Additionally to our PW-MAD
approach, we create a new face morphing attack dataset with digital and
re-digitized samples, namely the LMA-DRD dataset that is publicly available for
research purposes upon request.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised domain adaptation for clinician pose estimation and instance segmentation in the OR. (arXiv:2108.11801v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11801">
<div class="article-summary-box-inner">
<span><p>The fine-grained localization of clinicians in the operating room (OR) is a
key component to design the new generation of OR support systems. Computer
vision models for person pixel-based segmentation and body-keypoints detection
are needed to better understand the clinical activities and the spatial layout
of the OR. This is challenging, not only because OR images are very different
from traditional vision datasets, but also because data and annotations are
hard to collect and generate in the OR due to privacy concerns. To address
these concerns, we first study how joint person pose estimation and instance
segmentation can be performed on low resolutions images from 1x to 12x. Second,
to address the domain shift and the lack of annotations, we propose a novel
unsupervised domain adaptation method, called \emph{AdaptOR}, to adapt a model
from an \emph{in-the-wild} labeled source domain to a statistically different
unlabeled target domain. We propose to exploit explicit geometric constraints
on the different augmentations of the unlabeled target domain image to generate
accurate pseudo labels, and using these pseudo labels to train the model on
high- and low-resolution OR images in a \emph{self-training} framework.
Furthermore, we propose \emph{disentangled feature normalization} to handle the
statistically different source and target domain data. Extensive experimental
results with detailed ablation studies on the two OR datasets \emph{MVOR+} and
\emph{TUM-OR-test} show the effectiveness of our approach against strongly
constructed baselines, especially on the low-resolution privacy-preserving OR
images. Finally, we show the generality of our method as a semi-supervised
learning (SSL) method on the large-scale \emph{COCO} dataset, where we achieve
comparable results with as few as \textbf{1\%} of labeled supervision against a
model trained with 100\% labeled supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rotation Invariance and Extensive Data Augmentation: a strategy for the Mitosis Domain Generalization (MIDOG) Challenge. (arXiv:2109.00823v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00823">
<div class="article-summary-box-inner">
<span><p>Automated detection of mitotic figures in histopathology images is a
challenging task: here, we present the different steps that describe the
strategy we applied to participate in the MIDOG 2021 competition. The purpose
of the competition was to evaluate the generalization of solutions to images
acquired with unseen target scanners (hidden for the participants) under the
constraint of using training data from a limited set of four independent source
scanners. Given this goal and constraints, we joined the challenge by proposing
a straight-forward solution based on a combination of state-of-the-art deep
learning methods with the aim of yielding robustness to possible
scanner-related distributional shifts at inference time. Our solution combines
methods that were previously shown to be efficient for mitosis detection: hard
negative mining, extensive data augmentation, rotation-invariant convolutional
networks.
</p>
<p>We trained five models with different splits of the provided dataset. The
subsequent classifiers produced F1-scores with a mean and standard deviation of
0.747+/-0.032 on the test splits. The resulting ensemble constitutes our
candidate algorithm: its automated evaluation on the preliminary test set of
the challenge returned a F1-score of 0.6828.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing domain adaptation techniques for mitosis detection in multi-scanner breast cancer histopathology images. (arXiv:2109.00869v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00869">
<div class="article-summary-box-inner">
<span><p>Breast cancer is the most prevalent cancer worldwide and is increasing in
incidence, with over two million new cases now diagnosed each year. As part of
diagnostic tumour grading, histopathologists manually count the number of
dividing cells (mitotic figures) in a sample. Since the process is subjective
and time-consuming, artificial intelligence (AI) methods have been developed to
automate the process, however these methods often perform poorly when applied
to data from outside of the original (training) domain, i.e. they do not
generalise well to variations in histological background, staining protocols,
or scanner types. Style transfer, a form of domain adaptation, provides the
means to transform images from different domains to a shared visual appearance
and have been adopted in various applications to mitigate the issue of domain
shift. In this paper we train two mitosis detection models and two style
transfer methods and evaluate the usefulness of the latter for improving
mitosis detection performance in images digitised using different scanners. We
found that the best of these models, U-Net without style transfer, achieved an
F1-score of 0.693 on the MIDOG 2021 preliminary test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cascade RCNN for MIDOG Challenge. (arXiv:2109.01085v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01085">
<div class="article-summary-box-inner">
<span><p>Mitotic counts are one of the key indicators of breast cancer prognosis.
However, accurate mitotic cell counting is still a difficult problem and is
labourious. Automated methods have been proposed for this task, but are usually
dependent on the training images and show poor performance on unseen domains.
In this work, we present a multi-stage mitosis detection method based on a
Cascade RCNN developed to be sequentially more selective against false
positives. On the preliminary test set, the algorithm scores an F1-score of
0.7492.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges and Solutions in DeepFakes. (arXiv:2109.05397v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05397">
<div class="article-summary-box-inner">
<span><p>Deep learning has been successfully appertained to solve various complex
problems in the area of big data analytics to computer vision. A deep
learning-powered application recently emerged is Deep Fake. It helps to create
fake images and videos that human cannot distinguish them from the real ones
and are recent off-shelf manipulation technique that allows swapping two
identities in a single video. Technology is a controversial technology with
many wide-reaching issues impacting society. So, to counter this emerging
problem, we introduce a dataset of 140k real and fake faces which contain 70k
real faces from the Flickr dataset collected by Nvidia, as well as 70k fake
faces sampled from 1 million fake faces generated by style GAN. We will train
our model in the dataset so that our model can identify real or fake faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Joint Source-Channel Coding for Multi-Task Network. (arXiv:2109.05779v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05779">
<div class="article-summary-box-inner">
<span><p>Multi-task learning (MTL) is an efficient way to improve the performance of
related tasks by sharing knowledge. However, most existing MTL networks run on
a single end and are not suitable for collaborative intelligence (CI)
scenarios. In this work, we propose an MTL network with a deep joint
source-channel coding (JSCC) framework, which allows operating under CI
scenarios. We first propose a feature fusion based MTL network (FFMNet) for
joint object detection and semantic segmentation. Compared with other MTL
networks, FFMNet gets higher performance with fewer parameters. Then FFMNet is
split into two parts, which run on a mobile device and an edge server
respectively. The feature generated by the mobile device is transmitted through
the wireless channel to the edge server. To reduce the transmission overhead of
the intermediate feature, a deep JSCC network is designed. By combining two
networks together, the whole model achieves 512x compression for the
intermediate feature and a performance loss within 2% on both tasks. At last,
by training with noise, the FFMNet with JSCC is robust to various channel
conditions and outperforms the separate source and channel coding scheme.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Task Cross-Task Learning Architecture for Ad-hoc Uncertainty Estimation in 3D Cardiac MRI Image Segmentation. (arXiv:2109.07702v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07702">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation has significantly benefitted thanks to deep
learning architectures. Furthermore, semi-supervised learning (SSL) has
recently been a growing trend for improving a model's overall performance by
leveraging abundant unlabeled data. Moreover, learning multiple tasks within
the same model further improves model generalizability. To generate smoother
and accurate segmentation masks from 3D cardiac MR images, we present a
Multi-task Cross-task learning consistency approach to enforce the correlation
between the pixel-level (segmentation) and the geometric-level (distance map)
tasks. Our extensive experimentation with varied quantities of labeled data in
the training sets justifies the effectiveness of our model for the segmentation
of the left atrial cavity from Gadolinium-enhanced magnetic resonance (GE-MR)
images. With the incorporation of uncertainty estimates to detect failures in
the segmentation masks generated by CNNs, our study further showcases the
potential of our model to flag low-quality segmentation from a given model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised learning methods and applications in medical imaging analysis: A survey. (arXiv:2109.08685v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08685">
<div class="article-summary-box-inner">
<span><p>The availability of high quality annotated medical imaging datasets is a
major problem that collides with machine learning applications in the field of
medical imaging analysis and impedes its advancement. Self-supervised learning
is a recent training paradigm that enables learning robust representations
without the need for human annotation which can be considered as an effective
solution for the scarcity in annotated medical data. This article reviews the
state-of-the-art research directions in self-supervised learning approaches for
image data with concentration on their applications in the field of medical
imaging analysis. The article covers a set of the most recent self-supervised
learning methods from the computer vision field as they are applicable to the
medical imaging analysis and categorize them as predictive, generative and
contrastive approaches. Moreover, the article covers (40) of the most recent
researches in the field of self-supervised learning in medical imaging analysis
aiming at shedding the light on the recent innovation in the field. Ultimately,
the article concludes with possible future research directions in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HPTQ: Hardware-Friendly Post Training Quantization. (arXiv:2109.09113v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09113">
<div class="article-summary-box-inner">
<span><p>Neural network quantization enables the deployment of models on edge devices.
An essential requirement for their hardware efficiency is that the quantizers
are hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To
the best of our knowledge, current post-training quantization methods do not
support all of these constraints simultaneously. In this work, we introduce a
hardware-friendly post training quantization (HPTQ) framework, which addresses
this problem by synergistically combining several known quantization methods.
We perform a large-scale study on four tasks: classification, object detection,
semantic segmentation and pose estimation over a wide variety of network
architectures. Our extensive experiments show that competitive results can be
obtained under hardware-friendly constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Autism Spectrum Disorder Based on Individual-Aware Down-Sampling and Multi-Modal Learning. (arXiv:2109.09129v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09129">
<div class="article-summary-box-inner">
<span><p>Autism Spectrum Disorder(ASD) is a set of neurodevelopmental conditions that
affect patients' social abilities. In recent years, many studies have employed
deep learning to diagnose this brain dysfunction through functional MRI (fMRI).
However, existing approaches solely focused on the abnormal brain functional
connections but ignored the impact of regional activities. Due to this biased
prior knowledge, previous diagnosis models suffered from inter-site
heterogeneity and inter-individual phenotypic differences. To address this
issue, we propose a novel feature extraction method for fMRI that can learn a
personalized lower-resolution representation of the entire brain networking
regarding both the functional connections and regional activities.
Specifically, we abstract the brain imaging as a graph structure and
straightforwardly downsample it to sparse substructures by hierarchical graph
pooling. The down-scaled feature vectors are embedded into a population graph
where the hidden inter-subject heterogeneity and homogeneity are explicitly
expressed as inter- and intra-community connectivity differences. Subsequently,
we fuse the imaging and non-imaging information by graph convolutional networks
(GCN), which recalibrates features to node embeddings under phenotypic
statistics. By these means, our framework can extract features directly and
efficiently from the entire fMRI and be aware of implicit inter-individual
variance. We have evaluated our framework on the ABIDE-I dataset with 10-fold
cross-validation. The present model has achieved a mean classification accuracy
of 85.95\% and a mean AUC of 0.92, better than the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Automated Framework for COVID-19 Disease Identification from a Multicenter Dataset of Chest CT Scans. (arXiv:2109.09241v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09241">
<div class="article-summary-box-inner">
<span><p>The objective of this study is to develop a robust deep learning-based
framework to distinguish COVID-19, Community-Acquired Pneumonia (CAP), and
Normal cases based on chest CT scans acquired in different imaging centers
using various protocols, and radiation doses. We showed that while our proposed
model is trained on a relatively small dataset acquired from only one imaging
center using a specific scanning protocol, the model performs well on
heterogeneous test sets obtained by multiple scanners using different technical
parameters. We also showed that the model can be updated via an unsupervised
approach to cope with the data shift between the train and test sets and
enhance the robustness of the model upon receiving a new external dataset from
a different center. We adopted an ensemble architecture to aggregate the
predictions from multiple versions of the model. For initial training and
development purposes, an in-house dataset of 171 COVID-19, 60 CAP, and 76
Normal cases was used, which contained volumetric CT scans acquired from one
imaging center using a constant standard radiation dose scanning protocol. To
evaluate the model, we collected four different test sets retrospectively to
investigate the effects of the shifts in the data characteristics on the
model's performance. Among the test cases, there were CT scans with similar
characteristics as the train set as well as noisy low-dose and ultra-low dose
CT scans. In addition, some test CT scans were obtained from patients with a
history of cardiovascular diseases or surgeries. The entire test dataset used
in this study contained 51 COVID-19, 28 CAP, and 51 Normal cases. Experimental
results indicate that our proposed framework performs well on all test sets
achieving total accuracy of 96.15% (95%CI: [91.25-98.74]), COVID-19 sensitivity
of 96.08% (95%CI: [86.54-99.5]), CAP sensitivity of 92.86% (95%CI:
[76.50-99.19]).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Source Video Domain Adaptation with Temporal Attentive Moment Alignment. (arXiv:2109.09964v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09964">
<div class="article-summary-box-inner">
<span><p>Multi-Source Domain Adaptation (MSDA) is a more practical domain adaptation
scenario in real-world scenarios. It relaxes the assumption in conventional
Unsupervised Domain Adaptation (UDA) that source data are sampled from a single
domain and match a uniform data distribution. MSDA is more difficult due to the
existence of different domain shifts between distinct domain pairs. When
considering videos, the negative transfer would be provoked by spatial-temporal
features and can be formulated into a more challenging Multi-Source Video
Domain Adaptation (MSVDA) problem. In this paper, we address the MSVDA problem
by proposing a novel Temporal Attentive Moment Alignment Network (TAMAN) which
aims for effective feature transfer by dynamically aligning both spatial and
temporal feature moments. TAMAN further constructs robust global temporal
features by attending to dominant domain-invariant local temporal features with
high local classification confidence and low disparity between global and local
feature discrepancies. To facilitate future research on the MSVDA problem, we
introduce comprehensive benchmarks, covering extensive MSVDA scenarios.
Empirical results demonstrate a superior performance of the proposed TAMAN
across multiple MSVDA benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations Using Deep Spatio-Temporal Graph CNNs. (arXiv:2109.10257v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10257">
<div class="article-summary-box-inner">
<span><p>Several applications such as autonomous driving, augmented reality and
virtual reality require a precise prediction of the 3D human pose. Recently, a
new problem was introduced in the field to predict the 3D human poses from
observed 2D poses. We propose Skeleton-Graph, a deep spatio-temporal graph CNN
model that predicts the future 3D skeleton poses in a single pass from the 2D
ones. Unlike prior works, Skeleton-Graph focuses on modeling the interaction
between the skeleton joints by exploiting their spatial configuration. This is
being achieved by formulating the problem as a graph structure while learning a
suitable graph adjacency kernel. By the design, Skeleton-Graph predicts the
future 3D poses without divergence in the long-term, unlike prior works. We
also introduce a new metric that measures the divergence of predictions in the
long term. Our results show an FDE improvement of at least 27% and an ADE of 4%
on both the GTA-IM and PROX datasets respectively in comparison with prior
works. Also, we are 88% and 93% less divergence on the long-term motion
prediction in comparison with prior works on both GTA-IM and PROX datasets.
Code is available at https://github.com/abduallahmohamed/Skeleton-Graph.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10282">
<div class="article-summary-box-inner">
<span><p>Text recognition is a long-standing research problem for document
digitalization. Existing approaches for text recognition are usually built
based on CNN for image understanding and RNN for char-level text generation. In
addition, another language model is usually needed to improve the overall
accuracy as a post-processing step. In this paper, we propose an end-to-end
text recognition approach with pre-trained image Transformer and text
Transformer models, namely TrOCR, which leverages the Transformer architecture
for both image understanding and wordpiece-level text generation. The TrOCR
model is simple but effective, and can be pre-trained with large-scale
synthetic data and fine-tuned with human-labeled datasets. Experiments show
that the TrOCR model outperforms the current state-of-the-art models on both
printed and handwritten text recognition tasks. The code and models will be
publicly available at https://aka.ms/TrOCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Transfer Attacks With Unknown Data and Class Overlap. (arXiv:2109.11125v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11125">
<div class="article-summary-box-inner">
<span><p>The ability to transfer adversarial attacks from one model (the surrogate) to
another model (the victim) has been an issue of concern within the machine
learning (ML) community. The ability to successfully evade unseen models
represents an uncomfortable level of ease toward implementing attacks. In this
work we note that as studied, current transfer attack research has an
unrealistic advantage for the attacker: the attacker has the exact same
training data as the victim. We present the first study of transferring
adversarial attacks focusing on the data available to attacker and victim under
imperfect settings without querying the victim, where there is some variable
level of overlap in the exact data used or in the classes learned by each
model. This threat model is relevant to applications in medicine, malware, and
others. Under this new threat model attack success rate is not correlated with
data or class overlap in the way one would expect, and varies with dataset.
This makes it difficult for attacker and defender to reason about each other
and contributes to the broader study of model robustness and security. We
remedy this by developing a masked version of Projected Gradient Descent that
simulates class disparity, which enables the attacker to reliably estimate a
lower-bound on their attack's success.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How much "human-like" visual experience do current self-supervised learning algorithms need to achieve human-level object recognition?. (arXiv:2109.11523v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11523">
<div class="article-summary-box-inner">
<span><p>This paper addresses a fundamental question: how good are our current
self-supervised visual representation learning algorithms relative to humans?
More concretely, how much "human-like", natural visual experience would these
algorithms need in order to reach human-level performance in a complex,
realistic visual object recognition task such as ImageNet? Using a scaling
experiment, here we estimate that the answer is on the order of a million years
of natural visual experience, in other words several orders of magnitude longer
than a human lifetime. However, this estimate is quite sensitive to some
underlying assumptions, underscoring the need to run carefully controlled human
experiments. We discuss the main caveats surrounding our estimate and the
implications of this rather surprising result.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From images in the wild to video-informed image classification. (arXiv:2109.12040v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12040">
<div class="article-summary-box-inner">
<span><p>Image classifiers work effectively when applied on structured images, yet
they often fail when applied on images with very high visual complexity. This
paper describes experiments applying state-of-the-art object classifiers toward
a unique set of images in the wild with high visual complexity collected on the
island of Bali. The text describes differences between actual images in the
wild and images from Imagenet, and then discusses a novel approach combining
informational cues particular to video with an ensemble of imperfect
classifiers in order to improve classification results on video sourced images
of plants in the wild.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-28 23:02:12.591309672 UTC">2021-09-28 23:02:12 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>