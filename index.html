<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-05T01:30:00Z">07-05</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Learning from Natural Language and Demonstrations using Signal Temporal Logic. (arXiv:2207.00627v1 [cs.FL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00627">
<div class="article-summary-box-inner">
<span><p>Natural language is an intuitive way for humans to communicate tasks to a
robot. While natural language (NL) is ambiguous, real world tasks and their
safety requirements need to be communicated unambiguously. Signal Temporal
Logic (STL) is a formal logic that can serve as a versatile, expressive, and
unambiguous formal language to describe robotic tasks. On one hand, existing
work in using STL for the robotics domain typically requires end-users to
express task specifications in STL, a challenge for non-expert users.
</p>
<p>On the other, translating from NL to STL specifications is currently
restricted to specific fragments. In this work, we propose DIALOGUESTL, an
interactive approach for learning correct and concise STL formulas from (often)
ambiguous NL descriptions. We use a combination of semantic parsing,
pre-trained transformer-based language models, and user-in-the-loop
clarifications aided by a small number of user demonstrations to predict the
best STL formula to encode NL task descriptions. An advantage of mapping NL to
STL is that there has been considerable recent work on the use of reinforcement
learning (RL) to identify control policies for robots. We show we can use Deep
Q-Learning techniques to learn optimal policies from the learned STL
specifications. We demonstrate that DIALOGUESTL is efficient, scalable, and
robust, and has high accuracy in predicting the correct STL formula with a few
number of demonstrations and a few interactions with an oracle user.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Low-Resource Speech Recognition with Pretrained Speech Models: Continued Pretraining vs. Semi-Supervised Training. (arXiv:2207.00659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00659">
<div class="article-summary-box-inner">
<span><p>Self-supervised Transformer based models, such as wav2vec 2.0 and HuBERT,
have produced significant improvements over existing approaches to automatic
speech recognition (ASR). This is evident in the performance of the wav2vec 2.0
based pretrained XLSR-53 model across many languages when fine-tuned with
available labeled data. However, the performance from finetuning these models
can be dependent on the amount of in-language or similar-to-in-language data
included in the pretraining dataset. In this paper we investigate continued
pretraining (CoPT) with unlabeled in-language audio data on the XLSR-53
pretrained model in several low-resource languages. CoPT is more
computationally efficient than semi-supervised training (SST), the standard
approach of utilizing unlabeled data in ASR, since it omits the need for
pseudo-labeling of the unlabeled data. We show CoPT results in word error rates
(WERs), equal to or slightly better than using SST. In addition, we show that
using the CoPT model for pseudo-labeling, and using these labels in SST,
results in further improvements in WER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building African Voices. (arXiv:2207.00688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00688">
<div class="article-summary-box-inner">
<span><p>Modern speech synthesis techniques can produce natural-sounding speech given
sufficient high-quality data and compute resources. However, such data is not
readily available for many languages. This paper focuses on speech synthesis
for low-resourced African languages, from corpus creation to sharing and
deploying the Text-to-Speech (TTS) systems. We first create a set of
general-purpose instructions on building speech synthesis systems with minimum
technological resources and subject-matter expertise. Next, we create new
datasets and curate datasets from "found" data (existing recordings) through a
participatory approach while considering accessibility, quality, and breadth.
We demonstrate that we can develop synthesizers that generate intelligible
speech with 25 minutes of created speech, even when recorded in suboptimal
environments. Finally, we release the speech data, code, and trained voices for
12 African languages to support researchers and developers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">American == White in Multimodal Language-and-Image AI. (arXiv:2207.00691v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00691">
<div class="article-summary-box-inner">
<span><p>Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP,
are evaluated for evidence of a bias previously observed in social and
experimental psychology: equating American identity with being White. Embedding
association tests (EATs) using standardized images of self-identified Asian,
Black, Latina/o, and White individuals from the Chicago Face Database (CFD)
reveal that White individuals are more associated with collective in-group
words than are Asian, Black, or Latina/o individuals. In assessments of three
core aspects of American identity reported by social psychologists,
single-category EATs reveal that images of White individuals are more
associated with patriotism and with being born in America, but that, consistent
with prior findings in psychology, White individuals are associated with being
less likely to treat people of all races and backgrounds equally. Three
downstream machine learning tasks demonstrate biases associating American with
White. In a visual question answering task using BLIP, 97% of White individuals
are identified as American, compared to only 3% of Asian individuals. When
asked in what state the individual depicted lives in, the model responds China
53% of the time for Asian individuals, but always with an American state for
White individuals. In an image captioning task, BLIP remarks upon the race of
Asian individuals as much as 36% of the time, but never remarks upon race for
White individuals. Finally, provided with an initialization image from the CFD
and the text "an American person," a synthetic image generator (VQGAN) using
the text-based guidance of CLIP lightens the skin tone of individuals of all
races (by 35% for Black individuals, based on pixel brightness). The results
indicate that biases equating American identity with being White are learned by
language-and-image AI, and propagate to downstream applications of such models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UserLibri: A Dataset for ASR Personalization Using Only Text. (arXiv:2207.00706v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00706">
<div class="article-summary-box-inner">
<span><p>Personalization of speech models on mobile devices (on-device
personalization) is an active area of research, but more often than not, mobile
devices have more text-only data than paired audio-text data. We explore
training a personalized language model on text-only data, used during inference
to improve speech recognition performance for that user. We experiment on a
user-clustered LibriSpeech corpus, supplemented with personalized text-only
data for each user from Project Gutenberg. We release this User-Specific
LibriSpeech (UserLibri) dataset to aid future personalization research.
LibriSpeech audio-transcript pairs are grouped into 55 users from the
test-clean dataset and 52 users from test-other. We are able to lower the
average word error rate per user across both sets in streaming and nonstreaming
models, including an improvement of 2.5 for the harder set of test-other users
when streaming.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language statistics at different spatial, temporal, and grammatical scales. (arXiv:2207.00709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00709">
<div class="article-summary-box-inner">
<span><p>Statistical linguistics has advanced considerably in recent decades as data
has become available. This has allowed researchers to study how statistical
properties of languages change over time. In this work, we use data from
Twitter to explore English and Spanish considering the rank diversity at
different scales: temporal (from 3 to 96 hour intervals), spatial (from 3km to
3000+km radii), and grammatical (from monograms to pentagrams). We find that
all three scales are relevant. However, the greatest changes come from
variations in the grammatical scale. At the lowest grammatical scale
(monograms), the rank diversity curves are most similar, independently on the
values of other scales, languages, and countries. As the grammatical scale
grows, the rank diversity curves vary more depending on the temporal and
spatial scales, as well as on the language and country. We also study the
statistics of Twitter-specific tokens: emojis, hashtags, and user mentions.
These particular type of tokens show a sigmoid kind of behaviour as a rank
diversity function. Our results are helpful to quantify aspects of language
statistics that seem universal and what may lead to variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Log-Precision Transformers are Constant-Depth Uniform Threshold Circuits. (arXiv:2207.00729v1 [cs.CC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00729">
<div class="article-summary-box-inner">
<span><p>We prove that transformer neural networks with logarithmic precision in the
input length (and where the feedforward subnetworks are computable using linear
space in their input length) can be simulated by constant-depth uniform
threshold circuits. Thus, such transformers only recognize formal languages in
$\mathsf{TC}^0$, the class of languages defined by constant-depth, poly-size
threshold circuits. This demonstrates a connection between a practical claim in
NLP and a theoretical conjecture in computational complexity theory: "attention
is all you need" (Vaswani et al., 2017), i.e., transformers are capable of all
efficient computation, only if all efficiently computable problems can be
solved with log space, i.e., $\mathsf L = \mathsf P$. We also construct a
transformer that can evaluate any constant-depth threshold circuit on any
input, proving that transformers can follow instructions that are representable
in $\mathsf{TC}^0$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk. (arXiv:2207.00735v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00735">
<div class="article-summary-box-inner">
<span><p>Language is the principal tool for human communication, in which humor is one
of the most attractive parts. Producing natural language like humans using
computers, a.k.a, Natural Language Generation (NLG), has been widely used for
dialogue systems, chatbots, machine translation, as well as computer-aid
creation e.g., idea generations, scriptwriting. However, the humor aspect of
natural language is relatively under-investigated, especially in the age of
pre-trained language models. In this work, we aim to preliminarily test whether
NLG can generate humor as humans do. We build a new dataset consisting of
numerous digitized Chinese Comical Crosstalk scripts (called C$^3$ in short),
which is for a popular Chinese performing art called `Xiangsheng' since 1800s.
(For convenience for non-Chinese speakers, we called `crosstalk' for
`Xiangsheng' in this paper.) We benchmark various generation approaches
including training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and
large-scale PLMs (with and without fine-tuning). Moreover, we also conduct a
human assessment, showing that 1) large-scale pretraining largely improves
crosstalk generation quality; and 2) even the scripts generated from the best
PLM is far from what we expect, with only 65% quality of human-created
crosstalk. We conclude, humor generation could be largely improved using
large-scaled PLMs, but it is still in its infancy.
</p>
<p>The data and benchmarking code is publicly available in
\url{https://github.com/anonNo2/crosstalk-generation}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">INSCIT: Information-Seeking Conversations with Mixed-Initiative Interactions. (arXiv:2207.00746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00746">
<div class="article-summary-box-inner">
<span><p>In an information-seeking conversation, a user converses with an agent to ask
a series of questions that can often be under- or over-specified. An ideal
agent would first identify that they were in such a situation by searching
through their underlying knowledge source and then appropriately interacting
with a user to resolve it. However, most existing studies either fail to or
artificially incorporate such agent-side initiatives. In this work, we present
INSCIT (pronounced Insight), a dataset for information-seeking conversations
with mixed-initiative interactions. It contains a total of 4.7K user-agent
turns from 805 human-human conversations where the agent searches over
Wikipedia and either asks for clarification or provides relevant information to
address user queries. We define two subtasks, namely evidence passage
identification and response generation, as well as a new human evaluation
protocol to assess the model performance. We report results of two strong
baselines based on state-of-the-art models of conversational knowledge
identification and open-domain question answering. Both models significantly
underperform humans and fail to generate coherent and informative responses,
suggesting ample room for improvement in future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rationale-Augmented Ensembles in Language Models. (arXiv:2207.00747v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00747">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that rationales, or step-by-step chains of thought,
can be used to improve performance in multi-step reasoning tasks. We reconsider
rationale-augmented prompting for few-shot in-context learning, where (input -&gt;
output) prompts are expanded to (input, rationale -&gt; output) prompts. For
rationale-augmented prompting we demonstrate how existing approaches, which
rely on manual prompt engineering, are subject to sub-optimal rationales that
may harm performance. To mitigate this brittleness, we propose a unified
framework of rationale-augmented ensembles, where we identify rationale
sampling in the output space as the key component to robustly improve
performance. This framework is general and can easily be extended to common
natural language processing tasks, even those that do not traditionally
leverage intermediate steps, such as question answering, word sense
disambiguation, and sentiment analysis. We demonstrate that rationale-augmented
ensembles achieve more accurate and interpretable results than existing
prompting approaches--including standard prompting without rationales and
rationale-based chain-of-thought prompting--while simultaneously improving
interpretability of model predictions through the associated rationales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-aware multimodal page classification of Brazilian legal documents. (arXiv:2207.00748v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00748">
<div class="article-summary-box-inner">
<span><p>The Brazilian Supreme Court receives tens of thousands of cases each
semester. Court employees spend thousands of hours to execute the initial
analysis and classification of those cases -- which takes effort away from
posterior, more complex stages of the case management workflow. In this paper,
we explore multimodal classification of documents from Brazil's Supreme Court.
We train and evaluate our methods on a novel multimodal dataset of 6,510
lawsuits (339,478 pages) with manual annotation assigning each page to one of
six classes. Each lawsuit is an ordered sequence of pages, which are stored
both as an image and as a corresponding text extracted through optical
character recognition. We first train two unimodal classifiers: a ResNet
pre-trained on ImageNet is fine-tuned on the images, and a convolutional
network with filters of multiple kernel sizes is trained from scratch on
document texts. We use them as extractors of visual and textual features, which
are then combined through our proposed Fusion Module. Our Fusion Module can
handle missing textual or visual input by using learned embeddings for missing
data. Moreover, we experiment with bi-directional Long Short-Term Memory
(biLSTM) networks and linear-chain conditional random fields to model the
sequential nature of the pages. The multimodal approaches outperform both
textual and visual classifiers, especially when leveraging the sequential
nature of the pages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An End-to-End Set Transformer for User-Level Classification of Depression and Gambling Disorder. (arXiv:2207.00753v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00753">
<div class="article-summary-box-inner">
<span><p>This work proposes a transformer architecture for user-level classification
of gambling addiction and depression that is trainable end-to-end. As opposed
to other methods that operate at the post level, we process a set of social
media posts from a particular individual, to make use of the interactions
between posts and eliminate label noise at the post level. We exploit the fact
that, by not injecting positional encodings, multi-head attention is
permutation invariant and we process randomly sampled sets of texts from a user
after being encoded with a modern pretrained sentence encoder (RoBERTa /
MiniLM). Moreover, our architecture is interpretable with modern feature
attribution methods and allows for automatic dataset creation by identifying
discriminating posts in a user's text-set. We perform ablation studies on
hyper-parameters and evaluate our method for the eRisk 2022 Lab on early
detection of signs of pathological gambling and early risk detection of
depression. The method proposed by our team BLUE obtained the best ERDE5 score
of 0.015, and the second-best ERDE50 score of 0.009 for pathological gambling
detection. For the early detection of depression, we obtained the second-best
ERDE50 of 0.027.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages. (arXiv:2207.00758v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00758">
<div class="article-summary-box-inner">
<span><p>We present the results of the Workshop on Multilingual Information Access
(MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question
answering (QA) systems in 16 typologically diverse languages. In this task, we
adapted two large-scale cross-lingual open-retrieval QA datasets in 14
typologically diverse languages, and newly annotated open-retrieval QA data in
2 underrepresented languages: Tagalog and Tamil. Four teams submitted their
systems. The best system leveraging iteratively mined diverse negative examples
and larger pretrained models achieves 32.2 F1, outperforming our baseline by
4.5 points. The second best system uses entity-aware contextualized
representations for document retrieval, and achieves significant improvements
in Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FRAME: Evaluating Simulatability Metrics for Free-Text Rationales. (arXiv:2207.00779v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00779">
<div class="article-summary-box-inner">
<span><p>Free-text rationales aim to explain neural language model (LM) behavior more
flexibly and intuitively via natural language. To ensure rationale quality, it
is important to have metrics for measuring rationales' faithfulness (reflects
LM's actual behavior) and plausibility (convincing to humans). All existing
free-text rationale metrics are based on simulatability (association between
rationale and LM's predicted label), but there is no protocol for assessing
such metrics' reliability. To investigate this, we propose FRAME, a framework
for evaluating free-text rationale simulatability metrics. FRAME is based on
three axioms: (1) good metrics should yield highest scores for reference
rationales, which maximize rationale-label association by construction; (2)
good metrics should be appropriately sensitive to semantic perturbation of
rationales; and (3) good metrics should be robust to variation in the LM's task
performance. Across three text classification datasets, we show that existing
simulatability metrics cannot satisfy all three FRAME axioms, since they are
implemented via model pretraining which muddles the metric's signal. We
introduce a non-pretraining simulatability variant that improves performance on
(1) and (3) by an average of 41.7% and 42.9%, respectively, while performing
competitively on (2).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANEC: An Amharic Named Entity Corpus and Transformer Based Recognizer. (arXiv:2207.00785v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00785">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition is an information extraction task that serves as a
preprocessing step for other natural language processing tasks, such as machine
translation, information retrieval, and question answering. Named entity
recognition enables the identification of proper names as well as temporal and
numeric expressions in an open domain text. For Semitic languages such as
Arabic, Amharic, and Hebrew, the named entity recognition task is more
challenging due to the heavily inflected structure of these languages. In this
paper, we present an Amharic named entity recognition system based on
bidirectional long short-term memory with a conditional random fields layer. We
annotate a new Amharic named entity recognition dataset (8,070 sentences, which
has 182,691 tokens) and apply Synthetic Minority Over-sampling Technique to our
dataset to mitigate the imbalanced classification problem. Our named entity
recognition system achieves an F_1 score of 93%, which is the new
state-of-the-art result for Amharic named entity recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Task BERT Model for Schema-Guided Dialogue State Tracking. (arXiv:2207.00828v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00828">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems often employ a Dialogue State Tracker (DST) to
successfully complete conversations. Recent state-of-the-art DST
implementations rely on schemata of diverse services to improve model
robustness and handle zero-shot generalization to new domains [1], however such
methods [2, 3] typically require multiple large scale transformer models and
long input sequences to perform well. We propose a single multi-task BERT-based
model that jointly solves the three DST tasks of intent prediction, requested
slot prediction and slot filling. Moreover, we propose an efficient and
parsimonious encoding of the dialogue history and service schemata that is
shown to further improve performance. Evaluation on the SGD dataset shows that
our approach outperforms the baseline SGP-DST by a large margin and performs
well compared to the state-of-the-art, while being significantly more
computationally efficient. Extensive ablation studies are performed to examine
the contributing factors to the success of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree-constrained Pointer Generator with Graph Neural Network Encodings for Contextual Speech Recognition. (arXiv:2207.00857v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00857">
<div class="article-summary-box-inner">
<span><p>Incorporating biasing words obtained as contextual knowledge is critical for
many automatic speech recognition (ASR) applications. This paper proposes the
use of graph neural network (GNN) encodings in a tree-constrained pointer
generator (TCPGen) component for end-to-end contextual ASR. By encoding the
biasing words in the prefix-tree with a tree-based GNN, lookahead for future
wordpieces in end-to-end ASR decoding is achieved at each tree node by
incorporating information about all wordpieces on the tree branches rooted from
it, which allows a more accurate prediction of the generation probability of
the biasing words. Systems were evaluated on the Librispeech corpus using
simulated biasing tasks, and on the AMI corpus by proposing a novel
visual-grounded contextual ASR pipeline that extracts biasing words from slides
alongside each meeting. Results showed that TCPGen with GNN encodings achieved
about a further 15% relative WER reduction on the biasing words compared to the
original TCPGen, with a negligible increase in the computation cost for
decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Linguistic Blind Spot of Value-Aligned Agency, Natural and Artificial. (arXiv:2207.00868v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00868">
<div class="article-summary-box-inner">
<span><p>The value-alignment problem for artificial intelligence (AI) asks how we can
ensure that the 'values' (i.e., objective functions) of artificial systems are
aligned with the values of humanity. In this paper, I argue that linguistic
communication (natural language) is a necessary condition for robust value
alignment. I discuss the consequences that the truth of this claim would have
for research programmes that attempt to ensure value alignment for AI systems;
or, more loftily, designing robustly beneficial or ethical artificial agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Biomedical Pipeline to Detect Clinical and Non-Clinical Named Entities. (arXiv:2207.00876v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00876">
<div class="article-summary-box-inner">
<span><p>There are a few challenges related to the task of biomedical named entity
recognition, which are: the existing methods consider a fewer number of
biomedical entities (e.g., disease, symptom, proteins, genes); and these
methods do not consider the social determinants of health (age, gender,
employment, race), which are the non-medical factors related to patients'
health. We propose a machine learning pipeline that improves on previous
efforts in the following ways: first, it recognizes many biomedical entity
types other than the standard ones; second, it considers non-clinical factors
related to patient's health. This pipeline also consists of stages, such as
preprocessing, tokenization, mapping embedding lookup and named entity
recognition task to extract biomedical named entities from the free texts. We
present a new dataset that we prepare by curating the COVID-19 case reports.
The proposed approach outperforms the baseline methods on five benchmark
datasets with macro-and micro-average F1 scores around 90, as well as our
dataset with a macro-and micro-average F1 score of 95.25 and 93.18
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism. (arXiv:2207.00883v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00883">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have demonstrated their effectiveness in automatic
speech recognition (ASR) tasks and even shown superior performance over the
conventional hybrid framework. The main idea of Transformers is to capture the
long-range global context within an utterance by self-attention layers.
However, for scenarios like conversational speech, such utterance-level
modeling will neglect contextual dependencies that span across utterances. In
this paper, we propose to explicitly model the inter-sentential information in
a Transformer based end-to-end architecture for conversational speech
recognition. Specifically, for the encoder network, we capture the contexts of
previous speech and incorporate such historic information into current input by
a context-aware residual attention mechanism. For the decoder, the prediction
of current utterance is also conditioned on the historic linguistic information
through a conditional decoder framework. We show the effectiveness of our
proposed method on several open-source dialogue corpora and the proposed method
consistently improved the performance from the utterance-level
Transformer-based ASR models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Repetitions with Appropriate Repeated Words. (arXiv:2207.00929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00929">
<div class="article-summary-box-inner">
<span><p>A repetition is a response that repeats words in the previous speaker's
utterance in a dialogue. Repetitions are essential in communication to build
trust with others, as investigated in linguistic studies. In this work, we
focus on repetition generation. To the best of our knowledge, this is the first
neural approach to address repetition generation. We propose Weighted Label
Smoothing, a smoothing method for explicitly learning which words to repeat
during fine-tuning, and a repetition scoring method that can output more
appropriate repetitions during decoding. We conducted automatic and human
evaluations involving applying these methods to the pre-trained language model
T5 for generating repetitions. The experimental results indicate that our
methods outperformed baselines in both evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics. (arXiv:2207.00939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00939">
<div class="article-summary-box-inner">
<span><p>Long documents such as academic articles and business reports have been the
standard format to detail out important issues and complicated subjects that
require extra attention. An automatic summarization system that can effectively
condense long documents into short and concise texts to encapsulate the most
important information would thus be significant in aiding the reader's
comprehension. Recently, with the advent of neural architectures, significant
research efforts have been made to advance automatic text summarization
systems, and numerous studies on the challenges of extending these systems to
the long document domain have emerged. In this survey, we provide a
comprehensive overview of the research on long document summarization and a
systematic evaluation across the three principal components of its research
setting: benchmark datasets, summarization models, and evaluation metrics. For
each component, we organize the literature within the context of long document
summarization and conduct an empirical analysis to broaden the perspective on
current research progress. The empirical analysis includes a study on the
intrinsic characteristics of benchmark datasets, a multi-dimensional analysis
of summarization models, and a review of the summarization evaluation metrics.
Based on the overall findings, we conclude by proposing possible directions for
future exploration in this rapidly growing field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation. (arXiv:2207.00952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00952">
<div class="article-summary-box-inner">
<span><p>End-to-end speech-to-text translation models are often initialized with
pre-trained speech encoder and pre-trained text decoder. This leads to a
significant training gap between pre-training and fine-tuning, largely due to
the modality differences between speech outputs from the encoder and text
inputs to the decoder. In this work, we aim to bridge the modality gap between
speech and text to improve translation quality. We propose M-Adapter, a novel
Transformer-based module, to adapt speech representations to text. While
shrinking the speech sequence, M-Adapter produces features desired for
speech-to-text translation via modelling global and local dependencies of a
speech sequence. Our experimental results show that our model outperforms a
strong baseline by up to 1 BLEU score on the Must-C En$\rightarrow$DE
dataset.\footnote{Our code is available at
https://github.com/mingzi151/w2v2-st.}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Tieq Viet with Deep Learning Models. (arXiv:2207.00975v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00975">
<div class="article-summary-box-inner">
<span><p>Deep learning is a powerful approach in recovering lost information as well
as harder inverse function computation problems. When applied in natural
language processing, this approach is essentially making use of context as a
mean to recover information through likelihood maximization. Not long ago, a
linguistic study called Tieq Viet was controversial among both researchers and
society. We find this a great example to demonstrate the ability of deep
learning models to recover lost information. In the proposal of Tieq Viet, some
consonants in the standard Vietnamese are replaced. A sentence written in this
proposal can be interpreted into multiple sentences in the standard version,
with different meanings. The hypothesis that we want to test is whether a deep
learning model can recover the lost information if we translate the text from
Vietnamese to Tieq Viet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mental Illness Classification on Social Media Texts using Deep Learning and Transfer Learning. (arXiv:2207.01012v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01012">
<div class="article-summary-box-inner">
<span><p>Given the current social distance restrictions across the world, most
individuals now use social media as their major medium of communication.
Millions of people suffering from mental diseases have been isolated due to
this, and they are unable to get help in person. They have become more reliant
on online venues to express themselves and seek advice on dealing with their
mental disorders. According to the World health organization (WHO),
approximately 450 million people are affected. Mental illnesses, such as
depression, anxiety, etc., are immensely common and have affected an
individuals' physical health. Recently Artificial Intelligence (AI) methods
have been presented to help mental health providers, including psychiatrists
and psychologists, in decision making based on patients' authentic information
(e.g., medical records, behavioral data, social media utilization, etc.). AI
innovations have demonstrated predominant execution in numerous real-world
applications broadening from computer vision to healthcare. This study analyzes
unstructured user data on the Reddit platform and classifies five common mental
illnesses: depression, anxiety, bipolar disorder, ADHD, and PTSD. We trained
traditional machine learning, deep learning, and transfer learning multi-class
models to detect mental disorders of individuals. This effort will benefit the
public health system by automating the detection process and informing
appropriate authorities about people who require emergency assistance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Acoustic Contextual Representation by Audio-textual Cross-modal Learning for Conversational ASR. (arXiv:2207.01039v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01039">
<div class="article-summary-box-inner">
<span><p>Leveraging context information is an intuitive idea to improve performance on
conversational automatic speech recognition(ASR). Previous works usually adopt
recognized hypotheses of historical utterances as preceding context, which may
bias the current recognized hypothesis due to the inevitable
historicalrecognition errors. To avoid this problem, we propose an
audio-textual cross-modal representation extractor to learn contextual
representations directly from preceding speech. Specifically, it consists of
two modal-related encoders, extracting high-level latent features from speech
and the corresponding text, and a cross-modal encoder, which aims to learn the
correlation between speech and text. We randomly mask some input tokens and
input sequences of each modality. Then a token-missing or modal-missing
prediction with a modal-level CTC loss on the cross-modal encoder is performed.
Thus, the model captures not only the bi-directional context dependencies in a
specific modality but also relationships between different modalities. Then,
during the training of the conversational ASR system, the extractor will be
frozen to extract the textual representation of preceding speech, while such
representation is used as context fed to the ASR decoder through attention
mechanism. The effectiveness of the proposed approach is validated on several
Mandarin conversation corpora and the highest character error rate (CER)
reduction up to 16% is achieved on the MagicData dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-aspect Multilingual and Cross-lingual Parliamentary Speech Analysis. (arXiv:2207.01054v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01054">
<div class="article-summary-box-inner">
<span><p>Parliamentary and legislative debate transcripts provide an exciting insight
into elected politicians' opinions, positions, and policy preferences. They are
interesting for political and social sciences as well as linguistics and
natural language processing (NLP). Exiting research covers discussions within
individual parliaments. In contrast, we apply advanced NLP methods to a joint
and comparative analysis of six national parliaments (Bulgarian, Czech, French,
Slovene, Spanish, and United Kingdom) between 2017 and 2020, whose transcripts
are a part of the ParlaMint dataset collection. Using a uniform methodology, we
analyze topics discussed, emotions, and sentiment. We assess if the age,
gender, and political orientation of speakers can be detected from speeches.
The results show some commonalities and many surprising differences among the
analyzed countries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech. (arXiv:2207.01063v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01063">
<div class="article-summary-box-inner">
<span><p>The majority of current TTS datasets, which are collections of individual
utterances, contain few conversational aspects in terms of both style and
metadata. In this paper, we introduce DailyTalk, a high-quality conversational
speech dataset designed for Text-to-Speech. We sampled, modified, and recorded
2,541 dialogues from the open-domain dialogue dataset DailyDialog which are
adequately long to represent context of each dialogue. During the data
construction step, we maintained attributes distribution originally annotated
in DailyDialog to support diverse dialogue in DailyTalk. On top of our dataset,
we extend prior work as our baseline, where a non-autoregressive TTS is
conditioned on historical information in a dialog. We gather metadata so that a
TTS model can learn historical dialog information, the key to generating
context-aware speech. From the baseline experiment results, we show that
DailyTalk can be used to train neural text-to-speech models, and our baseline
can represent contextual information. The DailyTalk dataset and baseline code
are freely available for academic use with CC-BY-SA 4.0 license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Understand Depth?. (arXiv:2207.01077v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01077">
<div class="article-summary-box-inner">
<span><p>Besides image classification, Contrastive Language-Image Pre-training (CLIP)
has accomplished extraordinary success for a wide range of vision tasks,
including object-level and 3D space understanding. However, it's still
challenging to transfer semantic knowledge learned from CLIP into more
intricate tasks of quantified targets, such as depth estimation with geometric
information. In this paper, we propose to apply CLIP for zero-shot monocular
depth estimation, named DepthCLIP. We found that the patches of the input image
could respond to a certain semantic distance token and then be projected to a
quantified depth bin for coarse estimation. Without any training, our DepthCLIP
surpasses existing unsupervised methods and even approaches the early
fully-supervised networks. To our best knowledge, we are the first to conduct
zero-shot adaptation from the semantic language knowledge to quantified
downstream tasks and perform zero-shot monocular depth estimation. We hope our
work could cast a light on future research. The code is available at
https://github.com/Adonis-galaxy/DepthCLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Material Science Articles. (arXiv:2207.01079v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01079">
<div class="article-summary-box-inner">
<span><p>A crucial component in the curation of KB for a scientific domain is
information extraction from tables in the domain's published articles -- tables
carry important information (often numeric), which must be adequately extracted
for a comprehensive machine understanding of an article. Existing table
extractors assume prior knowledge of table structure and format, which may not
be known in scientific tables. We study a specific and challenging table
extraction problem: extracting compositions of materials (e.g., glasses,
alloys). We first observe that material science researchers organize similar
compositions in a wide variety of table styles, necessitating an intelligent
model for table understanding and composition extraction. Consequently, we
define this novel task as a challenge for the ML community and create a
training dataset comprising 4,408 distantly supervised tables, along with 1,475
manually annotated dev and test tables. We also present DiSCoMaT, a strong
baseline geared towards this specific task, which combines multiple graph
neural networks with several task-specific regular expressions, features, and
constraints. We show that DiSCoMaT outperforms recent table processing
architectures by significant margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProoFVer: Natural Logic Theorem Proving for Fact Verification. (arXiv:2108.11357v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11357">
<div class="article-summary-box-inner">
<span><p>Fact verification systems typically rely on neural network classifiers for
veracity prediction which lack explainability. This paper proposes ProoFVer,
which uses a seq2seq model to generate natural logic-based inferences as
proofs. These proofs consist of lexical mutations between spans in the claim
and the evidence retrieved, each marked with a natural logic operator. Claim
veracity is determined solely based on the sequence of these operators. Hence,
these proofs are faithful explanations, and this makes ProoFVer faithful by
construction. Currently, ProoFVer has the highest label accuracy and the
second-best Score in the FEVER leaderboard. Furthermore, it improves by 13.21%
points over the next best model on a dataset with counterfactual instances,
demonstrating its robustness. As explanations, the proofs show better overlap
with human rationales than attention-based highlights and the proofs help
humans predict model decisions correctly more often than using the evidence
directly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution. (arXiv:2108.13530v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13530">
<div class="article-summary-box-inner">
<span><p>We consider the task of document-level entity linking (EL), where it is
important to make consistent decisions for entity mentions over the full
document jointly. We aim to leverage explicit "connections" among mentions
within the document itself: we propose to join the EL task with that of
coreference resolution (coref). This is complementary to related works that
exploit either (i) implicit document information (e.g., latent relations among
entity mentions, or general language models) or (ii) connections between the
candidate links (e.g, as inferred from the external knowledge base).
Specifically, we cluster mentions that are linked via coreference, and enforce
a single EL for all of the clustered mentions together. The latter constraint
has the added benefit of increased coverage by joining EL candidate lists for
the thus clustered mentions. We formulate the coref+EL problem as a structured
prediction task over directed trees and use a globally normalized model to
solve it. Experimental results on two datasets show a boost of up to +5%
F1-score on both coref and EL tasks, compared to their standalone counterparts.
For a subset of hard cases, with individual mentions lacking the correct EL in
their candidate entity list, we obtain a +50% increase in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing in-and-for Design Research. (arXiv:2111.13827v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13827">
<div class="article-summary-box-inner">
<span><p>We review the scholarly contributions that utilise Natural Language
Processing (NLP) techniques to support the design process. Using a heuristic
approach, we gathered 223 articles that are published in 32 journals within the
period 1991-present. We present state-of-the-art NLP in-and-for design research
by reviewing these articles according to the type of natural language text
sources: internal reports, design concepts, discourse transcripts, technical
publications, consumer opinions, and others. Upon summarizing and identifying
the gaps in these contributions, we utilise an existing design innovation
framework to identify the applications that are currently being supported by
NLP. We then propose a few methodological and theoretical directions for future
NLP in-and-for design research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PM-MMUT: Boosted Phone-Mask Data Augmentation using Multi-Modeling Unit Training for Phonetic-Reduction-Robust E2E Speech Recognition. (arXiv:2112.06721v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06721">
<div class="article-summary-box-inner">
<span><p>Consonant and vowel reduction are often encountered in speech, which might
cause performance degradation in automatic speech recognition (ASR). Our
recently proposed learning strategy based on masking, Phone Masking Training
(PMT), alleviates the impact of such phenomenon in Uyghur ASR. Although PMT
achieves remarkably improvements, there still exists room for further gains due
to the granularity mismatch between the masking unit of PMT (phoneme) and the
modeling unit (word-piece). To boost the performance of PMT, we propose
multi-modeling unit training (MMUT) architecture fusion with PMT (PM-MMUT). The
idea of MMUT framework is to split the Encoder into two parts including
acoustic feature sequences to phoneme-level representation (AF-to-PLR) and
phoneme-level representation to word-piece-level representation (PLR-to-WPLR).
It allows AF-to-PLR to be optimized by an intermediate phoneme-based CTC loss
to learn the rich phoneme-level context information brought by PMT.
Experimental results on Uyghur ASR show that the proposed approaches outperform
obviously the pure PMT. We also conduct experiments on the 960-hour Librispeech
benchmark using ESPnet1, which achieves about 10% relative WER reduction on all
the test set without LM fusion comparing with the latest official ESPnet1
pre-trained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis. (arXiv:2201.03969v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03969">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis (MSA) is a fundamental complex research problem
due to the heterogeneity gap between different modalities and the ambiguity of
human emotional expression. Although there have been many successful attempts
to construct multimodal representations for MSA, there are still two challenges
to be addressed: 1) A more robust multimodal representation needs to be
constructed to bridge the heterogeneity gap and cope with the complex
multimodal interactions, and 2) the contextual dynamics must be modeled
effectively throughout the information flow. In this work, we propose a
multimodal representation model based on Mutual information Maximization and
Minimization and Identity Embedding (MMMIE). We combine mutual information
maximization between modal pairs, and mutual information minimization between
input data and corresponding features to mine the modal-invariant and
task-related information. Furthermore, Identity Embedding is proposed to prompt
the downstream network to perceive the contextual information. Experimental
results on two public datasets demonstrate the effectiveness of the proposed
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping global dynamics of benchmark creation and saturation in artificial intelligence. (arXiv:2203.04592v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04592">
<div class="article-summary-box-inner">
<span><p>Benchmarks are crucial to measuring and steering progress in artificial
intelligence (AI). However, recent studies raised concerns over the state of AI
benchmarking, reporting issues such as benchmark overfitting, benchmark
saturation and increasing centralization of benchmark dataset creation. To
facilitate monitoring of the health of the AI benchmarking ecosystem, we
introduce methodologies for creating condensed maps of the global dynamics of
benchmark creation and saturation. We curated data for 1688 benchmarks covering
the entire domains of computer vision and natural language processing, and show
that a large fraction of benchmarks quickly trended towards near-saturation,
that many benchmarks fail to find widespread utilization, and that benchmark
performance gains for different AI tasks were prone to unforeseen bursts. We
analyze attributes associated with benchmark popularity, and conclude that
future benchmarks should emphasize versatility, breadth and real-world utility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05482">
<div class="article-summary-box-inner">
<span><p>The conventional recipe for maximizing model accuracy is to (1) train
multiple models with various hyperparameters and (2) pick the individual model
which performs best on a held-out validation set, discarding the remainder. In
this paper, we revisit the second step of this procedure in the context of
fine-tuning large pre-trained models, where fine-tuned models often appear to
lie in a single low error basin. We show that averaging the weights of multiple
models fine-tuned with different hyperparameter configurations often improves
accuracy and robustness. Unlike a conventional ensemble, we may average many
models without incurring any additional inference or memory costs -- we call
the results "model soups." When fine-tuning large pre-trained models such as
CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides
significant improvements over the best model in a hyperparameter sweep on
ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on
ImageNet, achieved a new state of the art. Furthermore, we show that the model
soup approach extends to multiple image classification and natural language
processing tasks, improves out-of-distribution performance, and improves
zero-shot performance on new downstream tasks. Finally, we analytically relate
the performance similarity of weight-averaging and logit-ensembling to flatness
of the loss and confidence of the predictions, and validate this relation
empirically. Code is available at https://github.com/mlfoundations/model-soups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo Label Is Better Than Human Label. (arXiv:2203.12668v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12668">
<div class="article-summary-box-inner">
<span><p>State-of-the-art automatic speech recognition (ASR) systems are trained with
tens of thousands of hours of labeled speech data. Human transcription is
expensive and time consuming. Factors such as the quality and consistency of
the transcription can greatly affect the performance of the ASR models trained
with these data. In this paper, we show that we can train a strong teacher
model to produce high quality pseudo labels by utilizing recent self-supervised
and semi-supervised learning techniques. Specifically, we use JUST (Joint
Unsupervised/Supervised Training) and iterative noisy student teacher training
to train a 600 million parameter bi-directional teacher model. This model
achieved 4.0% word error rate (WER) on a voice search task, 11.1% relatively
better than a baseline. We further show that by using this strong teacher model
to generate high-quality pseudo labels for training, we can achieve 13.6%
relative WER reduction (5.9% to 5.1%) for a streaming model compared to using
human labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12886">
<div class="article-summary-box-inner">
<span><p>Preschool evaluation is crucial because it gives teachers and parents crucial
knowledge about a children's growth and development. The coronavirus pandemic
has highlighted the necessity for preschool children to be assessed online.
This online testing requires a variety of technologies, from web application
development to various artificial intelligence models in diverse criteria such
as speech recognition. Because of the acoustic fluctuations and differences in
voice frequencies between children and adults, employing Automatic Speech
Recognition(ASR) systems is difficult because they are pre-trained on adults'
voices. In addition, training a new model requires a large amount of data. To
solve this issue, we constructed an ASR for our cognitive test system using the
Wav2Vec 2.0 model with a new pre-training objective, called Random Frequency
Pitch(RFP), and our new dataset, which was tested on Meaningless Words(MW) and
Rapid Automatic Naming(RAN) tests. Due to the peculiarities of these two tests,
we explored numerous models, including Convolutional Neural Network(CNN) and
Wav2Vec 2.0 models. Our new approach, reaches Word Error Rate(WER) of 6.45 on
the Persian section of CommonVoice dataset. Furthermore our novel methodology
produces positive outcomes in zero- and few-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Filler Word Detection and Classification: A Dataset and Benchmark. (arXiv:2203.15135v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15135">
<div class="article-summary-box-inner">
<span><p>Filler words such as `uh' or `um' are sounds or words people use to signal
they are pausing to think. Finding and removing filler words from recordings is
a common and tedious task in media editing. Automatically detecting and
classifying filler words could greatly aid in this task, but few studies have
been published on this problem to date. A key reason is the absence of a
dataset with annotated filler words for model training and evaluation. In this
work, we present a novel speech dataset, PodcastFillers, with 35K annotated
filler words and 50K annotations of other sounds that commonly occur in
podcasts such as breaths, laughter, and word repetitions. We propose a pipeline
that leverages VAD and ASR to detect filler candidates and a classifier to
distinguish between filler word types. We evaluate our proposed pipeline on
PodcastFillers, compare to several baselines, and present a detailed ablation
study. In particular, we evaluate the importance of using ASR and how it
compares to a transcription-free approach resembling keyword spotting. We show
that our pipeline obtains state-of-the-art results, and that leveraging ASR
strongly outperforms a keyword spotting approach. We make PodcastFillers
publicly available, in the hope that our work serves as a benchmark for future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deliberation Model for On-Device Spoken Language Understanding. (arXiv:2204.01893v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01893">
<div class="article-summary-box-inner">
<span><p>We propose a novel deliberation-based approach to end-to-end (E2E) spoken
language understanding (SLU), where a streaming automatic speech recognition
(ASR) model produces the first-pass hypothesis and a second-pass natural
language understanding (NLU) component generates the semantic parse by
conditioning on both ASR's text and audio embeddings. By formulating E2E SLU as
a generalized decoder, our system is able to support complex compositional
semantic structures. Furthermore, the sharing of parameters between ASR and NLU
makes the system especially suitable for resource-constrained (on-device)
environments; our proposed approach consistently outperforms strong pipeline
NLU baselines by 0.60% to 0.65% on the spoken version of the TOPv2 dataset
(STOP). We demonstrate that the fusion of text and audio features, coupled with
the system's ability to rewrite the first-pass hypothesis, makes our approach
more robust to ASR errors. Finally, we show that our approach can significantly
reduce the degradation when moving from natural speech to synthetic speech
training, but more work is required to make text-to-speech (TTS) a viable
solution for scaling up E2E SLU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Algebraic Approach to Learning and Grounding. (arXiv:2204.02813v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02813">
<div class="article-summary-box-inner">
<span><p>We consider the problem of learning the semantics of composite algebraic
expressions from examples. The outcome is a versatile framework for studying
learning tasks that can be put into the following abstract form: The input is a
partial algebra $\alg$ and a finite set of examples $(\varphi_1, O_1),
(\varphi_2, O_2), \ldots$, each consisting of an algebraic term $\varphi_i$ and
a set of objects~$O_i$. The objective is to simultaneously fill in the missing
algebraic operations in $\alg$ and ground the variables of every $\varphi_i$ in
$O_i$, so that the combined value of the terms is optimised. We demonstrate the
applicability of this framework through case studies in grammatical inference,
picture-language learning, and the grounding of logic scene descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ByT5 model for massively multilingual grapheme-to-phoneme conversion. (arXiv:2204.03067v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03067">
<div class="article-summary-box-inner">
<span><p>In this study, we tackle massively multilingual grapheme-to-phoneme
conversion through implementing G2P models based on ByT5. We have curated a G2P
dataset from various sources that covers around 100 languages and trained
large-scale multilingual G2P models based on ByT5. We found that ByT5 operating
on byte-level inputs significantly outperformed the token-based mT5 model in
terms of multilingual G2P. Pairwise comparison with monolingual models in these
languages suggests that multilingual ByT5 models generally lower the phone
error rate by jointly learning from a variety of languages. The pretrained
model can further benefit low resource G2P through zero-shot prediction on
unseen languages or provides pretrained weights for finetuning, which helps the
model converge to a lower phone error rate than randomly initialized weights.
To facilitate future research on multilingual G2P, we make available our code
and pretrained multilingual G2P models at:
https://github.com/lingjzhu/CharsiuG2P.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAESTRO: Matched Speech Text Representations through Modality Matching. (arXiv:2204.03409v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03409">
<div class="article-summary-box-inner">
<span><p>We present Maestro, a self-supervised training method to unify
representations learnt from speech and text modalities. Self-supervised
learning from speech signals aims to learn the latent structure inherent in the
signal, while self-supervised learning from text attempts to capture lexical
information. Learning aligned representations from unpaired speech and text
sequences is a challenging task. Previous work either implicitly enforced the
representations learnt from these two modalities to be aligned in the latent
space through multitasking and parameter sharing or explicitly through
conversion of modalities via speech synthesis. While the former suffers from
interference between the two modalities, the latter introduces additional
complexity. In this paper, we propose Maestro, a novel algorithm to learn
unified representations from both these modalities simultaneously that can
transfer to diverse downstream tasks such as Automated Speech Recognition (ASR)
and Speech Translation (ST). Maestro learns unified representations through
sequence alignment, duration prediction and matching embeddings in the learned
space through an aligned masked-language model loss. We establish a new
state-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 8% relative
reduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative)
and 21 languages to English multilingual ST on CoVoST 2 with an improvement of
2.8 BLEU averaged over 21 languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Ordering of Coordinate Compounds and Elaborate Expressions in Hmong, Lahu, and Chinese. (arXiv:2204.04080v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04080">
<div class="article-summary-box-inner">
<span><p>Coordinate compounds (CCs) and elaborate expressions (EEs) are coordinate
constructions common in languages of East and Southeast Asia. Mortensen (2006)
claims that (1) the linear ordering of EEs and CCs in Hmong, Lahu, and Chinese
can be predicted via phonological hierarchies and (2) these phonological
hierarchies lack a clear phonetic rationale. These claims are significant
because morphosyntax has often been seen as in a feed-forward relationship with
phonology, and phonological generalizations have often been assumed to be
phonetically "natural". We investigate whether the ordering of CCs and EEs can
be learned empirically and whether computational models (classifiers and
sequence labeling models) learn unnatural hierarchies similar to those posited
by Mortensen (2006). We find that decision trees and SVMs learn to predict the
order of CCs/EEs on the basis of phonology, with DTs learning hierarchies
strikingly similar to those proposed by Mortensen. However, we also find that a
neural sequence labeling model is able to learn the ordering of elaborate
expressions in Hmong very effectively without using any phonological
information. We argue that EE ordering can be learned through two independent
routes: phonology and lexical distribution, presenting a more nuanced picture
than previous work. [ISO 639-3:hmn, lhu, cmn]
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Streaming End-to-End Speech Translation with Neural Transducers. (arXiv:2204.05352v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05352">
<div class="article-summary-box-inner">
<span><p>Neural transducers have been widely used in automatic speech recognition
(ASR). In this paper, we introduce it to streaming end-to-end speech
translation (ST), which aims to convert audio signals to texts in other
languages directly. Compared with cascaded ST that performs ASR followed by
text-based machine translation (MT), the proposed Transformer transducer
(TT)-based ST model drastically reduces inference latency, exploits speech
information, and avoids error propagation from ASR to MT. To improve the
modeling capacity, we propose attention pooling for the joint network in TT. In
addition, we extend TT-based ST to multilingual ST, which generates texts of
multiple languages at the same time. Experimental results on a large-scale 50
thousand (K) hours pseudo-labeled training set show that TT-based ST not only
significantly reduces inference time but also outperforms non-streaming
cascaded ST for English-German translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Learning for Natural Language Processing: A Survey. (arXiv:2205.01500v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01500">
<div class="article-summary-box-inner">
<span><p>Deep learning has been the mainstream technique in natural language
processing (NLP) area. However, the techniques require many labeled data and
are less generalizable across domains. Meta-learning is an arising field in
machine learning studying approaches to learn better learning algorithms.
Approaches aim at improving algorithms in various aspects, including data
efficiency and generalizability. Efficacy of approaches has been shown in many
NLP tasks, but there is no systematic survey of these approaches in NLP, which
hinders more researchers from joining the field. Our goal with this survey
paper is to offer researchers pointers to relevant meta-learning works in NLP
and attract more attention from the NLP community to drive future innovation.
This paper first introduces the general concepts of meta-learning and the
common approaches. Then we summarize task construction settings and application
of meta-learning for various NLP problems and review the development of
meta-learning in NLP community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language. (arXiv:2205.06111v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06111">
<div class="article-summary-box-inner">
<span><p>To solve difficult tasks, humans ask questions to acquire knowledge from
external sources. In contrast, classical reinforcement learning agents lack
such an ability and often resort to exploratory behavior. This is exacerbated
as few present-day environments support querying for knowledge. In order to
study how agents can be taught to query external knowledge via language, we
first introduce two new environments: the grid-world-based Q-BabyAI and the
text-based Q-TextWorld. In addition to physical interactions, an agent can
query an external knowledge source specialized for these environments to gather
information. Second, we propose the "Asking for Knowledge" (AFK) agent, which
learns to generate language commands to query for meaningful knowledge that
helps solve the tasks. AFK leverages a non-parametric memory, a pointer
mechanism and an episodic exploration bonus to tackle (1) irrelevant
information, (2) a large query language space, (3) delayed reward for making
meaningful queries. Extensive experiments demonstrate that the AFK agent
outperforms recent baselines on the challenging Q-BabyAI and Q-TextWorld
environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Crossword Solving. (arXiv:2205.09665v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09665">
<div class="article-summary-box-inner">
<span><p>We present the Berkeley Crossword Solver, a state-of-the-art approach for
automatically solving crossword puzzles. Our system works by generating answer
candidates for each crossword clue using neural question answering models and
then combines loopy belief propagation with local search to find full puzzle
solutions. Compared to existing approaches, our system improves exact puzzle
accuracy from 71% to 82% on crosswords from The New York Times and obtains
99.9% letter accuracy on themeless puzzles. Additionally, in 2021, a hybrid of
our system and the existing Dr.Fill system outperformed all human competitors
for the first time at the American Crossword Puzzle Tournament. To facilitate
research on question answering and crossword solving, we analyze our system's
remaining errors and release a dataset of over six million question-answer
pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">hmBERT: Historical Multilingual Language Models for Named Entity Recognition. (arXiv:2205.15575v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15575">
<div class="article-summary-box-inner">
<span><p>Compared to standard Named Entity Recognition (NER), identifying persons,
locations, and organizations in historical texts constitutes a big challenge.
To obtain machine-readable corpora, the historical text is usually scanned and
Optical Character Recognition (OCR) needs to be performed. As a result, the
historical corpora contain errors. Also, entities like location or organization
can change over time, which poses another challenge. Overall, historical texts
come with several peculiarities that differ greatly from modern texts and large
labeled corpora for training a neural tagger are hardly available for this
domain. In this work, we tackle NER for historical German, English, French,
Swedish, and Finnish by training large historical language models. We
circumvent the need for large amounts of labeled data by using unlabeled data
for pretraining a language model. We propose hmBERT, a historical multilingual
BERT-based language model, and release the model in several versions of
different sizes. Furthermore, we evaluate the capability of hmBERT by solving
downstream NER as part of this year's HIPE-2022 shared task and provide
detailed analysis and insights. For the Multilingual Classical Commentary
coarse-grained NER challenge, our tagger HISTeria outperforms the other teams'
models for two out of three languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Key Event Detection from Massive Text Corpora. (arXiv:2206.04153v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04153">
<div class="article-summary-box-inner">
<span><p>Automated event detection from news corpora is a crucial task towards mining
fast-evolving structured knowledge. As real-world events have different
granularities, from the top-level themes to key events and then to event
mentions corresponding to concrete actions, there are generally two lines of
research: (1) theme detection identifies from a news corpus major themes (e.g.,
"2019 Hong Kong Protests" vs. "2020 U.S. Presidential Election") that have very
distinct semantics; and (2) action extraction extracts from one document
mention-level actions (e.g., "the police hit the left arm of the protester")
that are too fine-grained for comprehending the event. In this paper, we
propose a new task, key event detection at the intermediate level, aiming to
detect from a news corpus key events (e.g., "HK Airport Protest on Aug.
12-14"), each happening at a particular time/location and focusing on the same
topic. This task can bridge event understanding and structuring and is
inherently challenging because of the thematic and temporal closeness of key
events and the scarcity of labeled data due to the fast-evolving nature of news
articles. To address these challenges, we develop an unsupervised key event
detection framework, EvMine, that (1) extracts temporally frequent peak phrases
using a novel ttf-itf score, (2) merges peak phrases into event-indicative
feature sets by detecting communities from our designed peak phrase graph that
captures document co-occurrences, semantic similarities, and temporal closeness
signals, and (3) iteratively retrieves documents related to each key event by
training a classifier with automatically generated pseudo labels from the
event-indicative feature sets and refining the detected key events using the
retrieved documents. Extensive experiments and case studies show EvMine
outperforms all the baseline methods and its ablations on two real-world news
corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unveiling Transformers with LEGO: a synthetic reasoning task. (arXiv:2206.04301v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04301">
<div class="article-summary-box-inner">
<span><p>We propose a synthetic task, LEGO (Learning Equality and Group Operations),
that encapsulates the problem of following a chain of reasoning, and we study
how the transformer architectures learn this task. We pay special attention to
data effects such as pretraining (on seemingly unrelated NLP tasks) and dataset
composition (e.g., differing chain length at training and test time), as well
as architectural variants such as weight-tied layers or adding convolutional
components. We study how the trained models eventually succeed at the task, and
in particular, we are able to understand (to some extent) some of the attention
heads as well as how the information flows in the network. Based on these
observations we propose a hypothesis that here pretraining helps merely due to
being a smart initialization rather than some deep knowledge stored in the
network. We also observe that in some data regime the trained transformer finds
"shortcut" solutions to follow the chain of reasoning, which impedes the
model's ability to generalize to simple variants of the main task, and moreover
we find that one can prevent such shortcut with appropriate architecture
modification or careful data preparation. Motivated by our findings, we begin
to explore the task of learning to execute C programs, where a convolutional
modification to transformers, namely adding convolutional structures in the
key/query/value maps, shows an encouraging edge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Diffusion Energy-Based Model for Interpretable Text Modeling. (arXiv:2206.05895v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05895">
<div class="article-summary-box-inner">
<span><p>Latent space Energy-Based Models (EBMs), also known as energy-based priors,
have drawn growing interests in generative modeling. Fueled by its flexibility
in the formulation and strong modeling power of the latent space, recent works
built upon it have made interesting attempts aiming at the interpretability of
text modeling. However, latent space EBMs also inherit some flaws from EBMs in
data space; the degenerate MCMC sampling quality in practice can lead to poor
generation quality and instability in training, especially on data with complex
latent structures. Inspired by the recent efforts that leverage diffusion
recovery likelihood learning as a cure for the sampling issue, we introduce a
novel symbiosis between the diffusion models and latent space EBMs in a
variational learning framework, coined as the latent diffusion energy-based
model. We develop a geometric clustering-based regularization jointly with the
information bottleneck to further improve the quality of the learned latent
space. Experiments on several challenging tasks demonstrate the superior
performance of our model on interpretable text modeling over strong
counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing informativeness of an NLG chatbot vs graphical app in diet-information domain. (arXiv:2206.13435v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13435">
<div class="article-summary-box-inner">
<span><p>Visual representation of data like charts and tables can be challenging to
understand for readers. Previous work showed that combining visualisations with
text can improve the communication of insights in static contexts, but little
is known about interactive ones. In this work we present an NLG chatbot that
processes natural language queries and provides insights through a combination
of charts and text. We apply it to nutrition, a domain communication quality is
critical. Through crowd-sourced evaluation we compare the informativeness of
our chatbot against traditional, static diet-apps. We find that the
conversational context significantly improved users' understanding of dietary
data in various tasks, and that users considered the chatbot as more useful and
quick to use than traditional apps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long Range Language Modeling via Gated State Spaces. (arXiv:2206.13947v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13947">
<div class="article-summary-box-inner">
<span><p>State space models have shown to be effective at modeling long range
dependencies, specially on sequence classification tasks. In this work we focus
on autoregressive sequence modeling over English books, Github source code and
ArXiv mathematics articles. Based on recent developments around the
effectiveness of gated activation functions, we propose a new layer named Gated
State Space (GSS) and show that it trains significantly faster than the
diagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several
well-tuned Transformer-based baselines and exhibits zero-shot generalization to
longer inputs while being straightforward to implement. Finally, we show that
leveraging self-attention to model local dependencies improves the performance
of GSS even further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is it possible not to cheat on the Turing Test_Exploring the potential and challenges for true natural language 'understanding' by computers. (arXiv:2206.14672v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14672">
<div class="article-summary-box-inner">
<span><p>The increasing sophistication of NLP models has renewed optimism regarding
machines achieving a full human-like command of natural language. Whilst work
in NLP/NLU may have made great strides in that direction, the lack of
conceptual clarity in how 'understanding' is used in this and other disciplines
have made it difficult to discern how close we actually are. A critical,
interdisciplinary review of current approaches and remaining challenges is yet
to be carried out. Beyond linguistic knowledge, this requires considering our
species-specific capabilities to categorize, memorize, label and communicate
our (sufficiently similar) embodied and situated experiences. Moreover, gauging
the practical constraints requires critically analyzing the technical
capabilities of current models, as well as deeper philosophical reflection on
theoretical possibilities and limitations. In this paper, I unite all of these
perspectives -- the philosophical, cognitive-linguistic, and technical -- to
unpack the challenges involved in approaching true (human-like) language
understanding. By unpacking the theoretical assumptions inherent in current
approaches, I hope to illustrate how far we actually are from achieving this
goal, if indeed it is the goal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the Effects of Hyperparameters on Knowledge Graph Embedding Quality. (arXiv:2207.00473v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00473">
<div class="article-summary-box-inner">
<span><p>Embedding knowledge graphs into low-dimensional spaces is a popular method
for applying approaches, such as link prediction or node classification, to
these databases. This embedding process is very costly in terms of both
computational time and space. Part of the reason for this is the optimisation
of hyperparameters, which involves repeatedly sampling, by random, guided, or
brute-force selection, from a large hyperparameter space and testing the
resulting embeddings for their quality. However, not all hyperparameters in
this search space will be equally important. In fact, with prior knowledge of
the relative importance of the hyperparameters, some could be eliminated from
the search altogether without significantly impacting the overall quality of
the outputted embeddings. To this end, we ran a Sobol sensitivity analysis to
evaluate the effects of tuning different hyperparameters on the variance of
embedding quality. This was achieved by performing thousands of embedding
trials, each time measuring the quality of embeddings produced by different
hyperparameter configurations. We regressed the embedding quality on those
hyperparameter configurations, using this model to generate Sobol sensitivity
indices for each of the hyperparameters. By evaluating the correlation between
Sobol indices, we find substantial variability in the hyperparameter
sensitivities between knowledge graphs, with differing dataset characteristics
being the probable cause of these inconsistencies. As an additional
contribution of this work we identify several relations in the UMLS knowledge
graph that may cause data leakage via inverse relations, and derive and present
UMLS-43, a leakage-robust variant of that graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt. (arXiv:2206.07137v2 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07137">
<div class="article-summary-box-inner">
<span><p>Training on web-scale data can take months. But most computation and time is
wasted on redundant and noisy points that are already learnt or not learnable.
To accelerate training, we introduce Reducible Holdout Loss Selection
(RHO-LOSS), a simple but principled technique which selects approximately those
points for training that most reduce the model's generalization loss. As a
result, RHO-LOSS mitigates the weaknesses of existing data selection methods:
techniques from the optimization literature typically select 'hard' (e.g. high
loss) points, but such points are often noisy (not learnable) or less
task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but
such points need not be trained on once learned. In contrast, RHO-LOSS selects
points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains
in far fewer steps than prior art, improves accuracy, and speeds up training on
a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and
BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in
18x fewer steps and reaches 2% higher final accuracy than uniform data
shuffling.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-selected Graph Spatial Attention Network for Addictive Brain-Networks Identification. (arXiv:2207.00583v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00583">
<div class="article-summary-box-inner">
<span><p>Functional alterations in the relevant neural circuits occur from drug
addiction over a certain period. And these significant alterations are also
revealed by analyzing fMRI. However, because of fMRI's high dimensionality and
poor signal-to-noise ratio, it is challenging to encode efficient and robust
brain regional embeddings for both graph-level identification and region-level
biomarkers detection tasks between nicotine addiction (NA) and healthy control
(HC) groups. In this work, we represent the fMRI of the rat brain as a graph
with biological attributes and propose a novel feature-selected graph spatial
attention network(FGSAN) to extract the biomarkers of addiction and identify
from these brain networks. Specially, a graph spatial attention encoder is
employed to capture the features of spatiotemporal brain networks with spatial
information. The method simultaneously adopts a Bayesian feature selection
strategy to optimize the model and improve classification task by constraining
features. Experiments on an addiction-related neural imaging dataset show that
the proposed model can obtain superior performance and detect interpretable
biomarkers associated with addiction-relevant neural circuits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PrUE: Distilling Knowledge from Sparse Teacher Networks. (arXiv:2207.00586v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00586">
<div class="article-summary-box-inner">
<span><p>Although deep neural networks have enjoyed remarkable success across a wide
variety of tasks, their ever-increasing size also imposes significant overhead
on deployment. To compress these models, knowledge distillation was proposed to
transfer knowledge from a cumbersome (teacher) network into a lightweight
(student) network. However, guidance from a teacher does not always improve the
generalization of students, especially when the size gap between student and
teacher is large. Previous works argued that it was due to the high certainty
of the teacher, resulting in harder labels that were difficult to fit. To
soften these labels, we present a pruning method termed Prediction Uncertainty
Enlargement (PrUE) to simplify the teacher. Specifically, our method aims to
decrease the teacher's certainty about data, thereby generating soft
predictions for students. We empirically investigate the effectiveness of the
proposed method with experiments on CIFAR-10/100, Tiny-ImageNet, and ImageNet.
Results indicate that student networks trained with sparse teachers achieve
better performance. Besides, our method allows researchers to distill knowledge
from deeper networks to improve students further. Our code is made public at:
\url{https://github.com/wangshaopu/prue}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pair-Relationship Modeling for Latent Fingerprint Recognition. (arXiv:2207.00587v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00587">
<div class="article-summary-box-inner">
<span><p>Latent fingerprints are important for identifying criminal suspects. However,
recognizing a latent fingerprint in a collection of reference fingerprints
remains a challenge. Most, if not all, of existing methods would extract
representation features of each fingerprint independently and then compare the
similarity of these representation features for recognition in a different
process. Without the supervision of similarity for the feature extraction
process, the extracted representation features are hard to optimally reflect
the similarity of the two compared fingerprints which is the base for matching
decision making. In this paper, we propose a new scheme that can model the
pair-relationship of two fingerprints directly as the similarity feature for
recognition. The pair-relationship is modeled by a hybrid deep network which
can handle the difficulties of random sizes and corrupted areas of latent
fingerprints. Experimental results on two databases show that the proposed
method outperforms the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoVA: Exploiting Compressed-Domain Analysis to Accelerate Video Analytics. (arXiv:2207.00588v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00588">
<div class="article-summary-box-inner">
<span><p>Modern retrospective analytics systems leverage cascade architecture to
mitigate bottleneck for computing deep neural networks (DNNs). However, the
existing cascades suffer two limitations: (1) decoding bottleneck is either
neglected or circumvented, paying significant compute and storage cost for
pre-processing; and (2) the systems are specialized for temporal queries and
lack spatial query support. This paper presents CoVA, a novel cascade
architecture that splits the cascade computation between compressed domain and
pixel domain to address the decoding bottleneck, supporting both temporal and
spatial queries. CoVA cascades analysis into three major stages where the first
two stages are performed in compressed domain while the last one in pixel
domain. First, CoVA detects occurrences of moving objects (called blobs) over a
set of compressed frames (called tracks). Then, using the track results, CoVA
prudently selects a minimal set of frames to obtain the label information and
only decode them to compute the full DNNs, alleviating the decoding bottleneck.
Lastly, CoVA associates tracks with labels to produce the final analysis
results on which users can process both temporal and spatial queries. Our
experiments demonstrate that CoVA offers 4.8x throughput improvement over
modern cascade systems, while imposing modest accuracy loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSD-Faster Net: A Hybrid Network for Industrial Defect Inspection. (arXiv:2207.00589v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00589">
<div class="article-summary-box-inner">
<span><p>The quality of industrial components is critical to the production of special
equipment such as robots. Defect inspection of these components is an efficient
way to ensure quality. In this paper, we propose a hybrid network, SSD-Faster
Net, for industrial defect inspection of rails, insulators, commutators etc.
SSD-Faster Net is a two-stage network, including SSD for quickly locating
defective blocks, and an improved Faster R-CNN for defect segmentation. For the
former, we propose a novel slice localization mechanism to help SSD scan
quickly. The second stage is based on improved Faster R-CNN, using FPN,
deformable kernel(DK) to enhance representation ability. It fuses multi-scale
information, and self-adapts the receptive field. We also propose a novel loss
function and use ROI Align to improve accuracy. Experiments show that our
SSD-Faster Net achieves an average accuracy of 84.03%, which is 13.42% higher
than the nearest competitor based on Faster R-CNN, 4.14% better than GAN-based
methods, more than 10% higher than that of DNN-based detectors. And the
computing speed is improved by nearly 7%, which proves its robustness and
superior performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViRel: Unsupervised Visual Relations Discovery with Graph-level Analogy. (arXiv:2207.00590v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00590">
<div class="article-summary-box-inner">
<span><p>Visual relations form the basis of understanding our compositional world, as
relationships between visual objects capture key information in a scene. It is
then advantageous to learn relations automatically from the data, as learning
with predefined labels cannot capture all possible relations. However, current
relation learning methods typically require supervision, and are not designed
to generalize to scenes with more complicated relational structures than those
seen during training. Here, we introduce ViRel, a method for unsupervised
discovery and learning of Visual Relations with graph-level analogy. In a
setting where scenes within a task share the same underlying relational
subgraph structure, our learning method of contrasting isomorphic and
non-isomorphic graphs discovers the relations across tasks in an unsupervised
manner. Once the relations are learned, ViRel can then retrieve the shared
relational graph structure for each task by parsing the predicted relational
structure. Using a dataset based on grid-world and the Abstract Reasoning
Corpus, we show that our method achieves above 95% accuracy in relation
classification, discovers the relation graph structure for most tasks, and
further generalizes to unseen tasks with more complicated relational
structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identification of Binary Neutron Star Mergers in Gravitational-Wave Data Using YOLO One-Shot Object Detection. (arXiv:2207.00591v1 [astro-ph.IM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00591">
<div class="article-summary-box-inner">
<span><p>We demonstrate the application of the YOLOv5 model, a general purpose
convolution-based single-shot object detection model, in the task of detecting
binary neutron star (BNS) coalescence events from gravitational-wave data of
current generation interferometer detectors. We also present a thorough
explanation of the synthetic data generation and preparation tasks based on
approximant waveform models used for the model training, validation and testing
steps. Using this approach, we achieve mean average precision
($\text{mAP}_{[0.50]}$) values of 0.945 for a single class validation dataset
and as high as 0.978 for test datasets. Moreover, the trained model is
successful in identifying the GW170817 event in the LIGO H1 detector data. The
identification of this event is also possible for the LIGO L1 detector data
with an additional pre-processing step, without the need of removing the large
glitch in the final stages of the inspiral. The detection of the GW190425 event
is less successful, which attests to performance degradation with the
signal-to-noise ratio. Our study indicates that the YOLOv5 model is an
interesting approach for first-stage detection alarm pipelines and, when
integrated in more complex pipelines, for real-time inference of physical
source parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRESS: Dynamic REal-time Sparse Subnets. (arXiv:2207.00670v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00670">
<div class="article-summary-box-inner">
<span><p>The limited and dynamically varied resources on edge devices motivate us to
deploy an optimized deep neural network that can adapt its sub-networks to fit
in different resource constraints. However, existing works often build
sub-networks through searching different network architectures in a
hand-crafted sampling space, which not only can result in a subpar performance
but also may cause on-device re-configuration overhead. In this paper, we
propose a novel training algorithm, Dynamic REal-time Sparse Subnets (DRESS).
DRESS samples multiple sub-networks from the same backbone network through
row-based unstructured sparsity, and jointly trains these sub-networks in
parallel with weighted loss. DRESS also exploits strategies including parameter
reusing and row-based fine-grained sampling for efficient storage consumption
and efficient on-device adaptation. Extensive experiments on public vision
datasets show that DRESS yields significantly higher accuracy than
state-of-the-art sub-networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">American == White in Multimodal Language-and-Image AI. (arXiv:2207.00691v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00691">
<div class="article-summary-box-inner">
<span><p>Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP,
are evaluated for evidence of a bias previously observed in social and
experimental psychology: equating American identity with being White. Embedding
association tests (EATs) using standardized images of self-identified Asian,
Black, Latina/o, and White individuals from the Chicago Face Database (CFD)
reveal that White individuals are more associated with collective in-group
words than are Asian, Black, or Latina/o individuals. In assessments of three
core aspects of American identity reported by social psychologists,
single-category EATs reveal that images of White individuals are more
associated with patriotism and with being born in America, but that, consistent
with prior findings in psychology, White individuals are associated with being
less likely to treat people of all races and backgrounds equally. Three
downstream machine learning tasks demonstrate biases associating American with
White. In a visual question answering task using BLIP, 97% of White individuals
are identified as American, compared to only 3% of Asian individuals. When
asked in what state the individual depicted lives in, the model responds China
53% of the time for Asian individuals, but always with an American state for
White individuals. In an image captioning task, BLIP remarks upon the race of
Asian individuals as much as 36% of the time, but never remarks upon race for
White individuals. Finally, provided with an initialization image from the CFD
and the text "an American person," a synthetic image generator (VQGAN) using
the text-based guidance of CLIP lightens the skin tone of individuals of all
races (by 35% for Black individuals, based on pixel brightness). The results
indicate that biases equating American identity with being White are learned by
language-and-image AI, and propagate to downstream applications of such models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot incremental learning in the context of solar cell quality inspection. (arXiv:2207.00693v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00693">
<div class="article-summary-box-inner">
<span><p>In industry, Deep Neural Networks have shown high defect detection rates
surpassing other more traditional manual feature engineering based proposals.
This has been achieved mainly through supervised training where a great amount
of data is required in order to learn good classification models. However, such
amount of data is sometimes hard to obtain in industrial scenarios, as few
defective pieces are produced normally. In addition, certain kinds of defects
are very rare and usually just appear from time to time, which makes the
generation of a proper dataset for training a classification model even harder.
Moreover, the lack of available data limits the adaptation of inspection models
to new defect types that appear in production as it might require a model
retraining in order to incorporate the detects and detect them. In this work,
we have explored the technique of weight imprinting in the context of solar
cell quality inspection where we have trained a network on three base defect
classes, and then we have incorporated new defect classes using few samples.
The results have shown that this technique allows the network to extend its
knowledge with regard to defect classes with few samples, which can be
interesting for industrial practitioners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Turning to a Teacher for Timestamp Supervised Temporal Action Segmentation. (arXiv:2207.00712v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00712">
<div class="article-summary-box-inner">
<span><p>Temporal action segmentation in videos has drawn much attention recently.
Timestamp supervision is a cost-effective way for this task. To obtain more
information to optimize the model, the existing method generated pseudo
frame-wise labels iteratively based on the output of a segmentation model and
the timestamp annotations. However, this practice may introduce noise and
oscillation during the training, and lead to performance degeneration. To
address this problem, we propose a new framework for timestamp supervised
temporal action segmentation by introducing a teacher model parallel to the
segmentation model to help stabilize the process of model optimization. The
teacher model can be seen as an ensemble of the segmentation model, which helps
to suppress the noise and to improve the stability of pseudo labels. We further
introduce a segmentally smoothing loss, which is more focused and cohesive, to
enforce the smooth transition of the predicted probabilities within action
instances. The experiments on three datasets show that our method outperforms
the state-of-the-art method and performs comparably against the
fully-supervised methods at a much lower annotation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noise and Edge Based Dual Branch Image Manipulation Detection. (arXiv:2207.00724v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00724">
<div class="article-summary-box-inner">
<span><p>Unlike ordinary computer vision tasks that focus more on the semantic content
of images, the image manipulation detection task pays more attention to the
subtle information of image manipulation. In this paper, the noise image
extracted by the improved constrained convolution is used as the input of the
model instead of the original image to obtain more subtle traces of
manipulation. Meanwhile, the dual-branch network, consisting of a
high-resolution branch and a context branch, is used to capture the traces of
artifacts as much as possible. In general, most manipulation leaves
manipulation artifacts on the manipulation edge. A specially designed
manipulation edge detection module is constructed based on the dual-branch
network to identify these artifacts better. The correlation between pixels in
an image is closely related to their distance. The farther the two pixels are,
the weaker the correlation. We add a distance factor to the self-attention
module to better describe the correlation between pixels. Experimental results
on four publicly available image manipulation datasets demonstrate the
effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale Attentive Image De-raining Networks via Neural Architecture Search. (arXiv:2207.00728v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00728">
<div class="article-summary-box-inner">
<span><p>Multi-scale architectures and attention modules have shown effectiveness in
many deep learning-based image de-raining methods. However, manually designing
and integrating these two components into a neural network requires a bulk of
labor and extensive expertise. In this article, a high-performance multi-scale
attentive neural architecture search (MANAS) framework is technically developed
for image deraining. The proposed method formulates a new multi-scale attention
search space with multiple flexible modules that are favorite to the image
de-raining task. Under the search space, multi-scale attentive cells are built,
which are further used to construct a powerful image de-raining network. The
internal multiscale attentive architecture of the de-raining network is
searched automatically through a gradient-based search algorithm, which avoids
the daunting procedure of the manual design to some extent. Moreover, in order
to obtain a robust image de-raining model, a practical and effective
multi-to-one training strategy is also presented to allow the de-raining
network to get sufficient background information from multiple rainy images
with the same background scene, and meanwhile, multiple loss functions
including external loss, internal loss, architecture regularization loss, and
model complexity loss are jointly optimized to achieve robust de-raining
performance and controllable model complexity. Extensive experimental results
on both synthetic and realistic rainy images, as well as the down-stream vision
applications (i.e., objection detection and segmentation) consistently
demonstrate the superiority of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SketchCleanNet -- A deep learning approach to the enhancement and correction of query sketches for a 3D CAD model retrieval system. (arXiv:2207.00732v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00732">
<div class="article-summary-box-inner">
<span><p>Search and retrieval remains a major research topic in several domains,
including computer graphics, computer vision, engineering design, etc. A search
engine requires primarily an input search query and a database of items to
search from. In engineering, which is the primary context of this paper, the
database consists of 3D CAD models, such as washers, pistons, connecting rods,
etc. A query from a user is typically in the form of a sketch, which attempts
to capture the details of a 3D model. However, sketches have certain typical
defects such as gaps, over-drawn portions (multi-strokes), etc. Since the
retrieved results are only as good as the input query, sketches need
cleaning-up and enhancement for better retrieval results.
</p>
<p>In this paper, a deep learning approach is proposed to improve or clean the
query sketches. Initially, sketches from various categories are analysed in
order to understand the many possible defects that may occur. A dataset of
cleaned-up or enhanced query sketches is then created based on an understanding
of these defects. Consequently, an end-to-end training of a deep neural network
is carried out in order to provide a mapping between the defective and the
clean sketches. This network takes the defective query sketch as the input and
generates a clean or an enhanced query sketch. Qualitative and quantitative
comparisons of the proposed approach with other state-of-the-art techniques
show that the proposed approach is effective. The results of the search engine
are reported using both the defective and enhanced query sketches, and it is
shown that using the enhanced query sketches from the developed approach yields
improved search results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Cross-Modal Knowledge Sharing Pre-training for Vision-Language Representation Learning and Retrieval. (arXiv:2207.00733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00733">
<div class="article-summary-box-inner">
<span><p>Recently, the cross-modal pre-training task has been a hotspot because of its
wide application in various down-streaming researches including retrieval,
captioning, question answering and so on. However, exiting methods adopt a
one-stream pre-training model to explore the united vision-language
representation for conducting cross-modal retrieval, which easily suffer from
the calculation explosion. Moreover, although the conventional double-stream
structures are quite efficient, they still lack the vital cross-modal
interactions, resulting in low performances. Motivated by these challenges, we
put forward a Contrastive Cross-Modal Knowledge Sharing Pre-training (COOKIE)
to grasp the joint text-image representations. Structurally, COOKIE adopts the
traditional double-stream structure because of the acceptable time consumption.
To overcome the inherent defects of double-stream structure as mentioned above,
we elaborately design two effective modules. Concretely, the first module is a
weight-sharing transformer that builds on the head of the visual and textual
encoders, aiming to semantically align text and image. This design enables
visual and textual paths focus on the same semantics. The other one is three
specially designed contrastive learning, aiming to share knowledge between
different models. The shared cross-modal knowledge develops the study of
unimodal representation greatly, promoting the single-modal retrieval tasks.
Extensive experimental results on multi-modal matching researches that includes
cross-modal retrieval, text matching, and image retrieval reveal the superiors
in calculation efficiency and statistical indicators of our pre-training model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Golfer: Trajectory Prediction with Masked Goal Conditioning MnM Network. (arXiv:2207.00738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00738">
<div class="article-summary-box-inner">
<span><p>Transformers have enabled breakthroughs in NLP and computer vision, and have
recently began to show promising performance in trajectory prediction for
Autonomous Vehicle (AV). How to efficiently model the interactive relationships
between the ego agent and other road and dynamic objects remains challenging
for the standard attention module. In this work we propose a general
Transformer-like architectural module MnM network equipped with novel masked
goal conditioning training procedures for AV trajectory prediction. The
resulted model, named golfer, achieves state-of-the-art performance, winning
the 2nd place in the 2022 Waymo Open Dataset Motion Prediction Challenge and
ranked 1st place according to minADE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gaussian Kernel-based Cross Modal Network for Spatio-Temporal Video Grounding. (arXiv:2207.00744v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00744">
<div class="article-summary-box-inner">
<span><p>Spatial-Temporal Video Grounding (STVG) is a challenging task which aims to
localize the spatio-temporal tube of the interested object semantically
according to a natural language query. Most previous works not only severely
rely on the anchor boxes extracted by Faster R-CNN, but also simply regard the
video as a series of individual frames, thus lacking their temporal modeling.
Instead, in this paper, we are the first to propose an anchor-free framework
for STVG, called Gaussian Kernel-based Cross Modal Network (GKCMN).
Specifically, we utilize the learned Gaussian Kernel-based heatmaps of each
video frame to locate the query-related object. A mixed serial and parallel
connection network is further developed to leverage both spatial and temporal
relations among frames for better grounding. Experimental results on VidSTG
dataset demonstrate the effectiveness of our proposed GKCMN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PhotoScene: Photorealistic Material and Lighting Transfer for Indoor Scenes. (arXiv:2207.00757v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00757">
<div class="article-summary-box-inner">
<span><p>Most indoor 3D scene reconstruction methods focus on recovering 3D geometry
and scene layout. In this work, we go beyond this to propose PhotoScene, a
framework that takes input image(s) of a scene along with approximately aligned
CAD geometry (either reconstructed automatically or manually specified) and
builds a photorealistic digital twin with high-quality materials and similar
lighting. We model scene materials using procedural material graphs; such
graphs represent photorealistic and resolution-independent materials. We
optimize the parameters of these graphs and their texture scale and rotation,
as well as the scene lighting to best match the input image via a
differentiable rendering layer. We evaluate our technique on objects and layout
reconstructions from ScanNet, SUN RGB-D and stock photographs, and demonstrate
that our method reconstructs high-quality, fully relightable 3D scenes that can
be re-rendered under arbitrary viewpoints, zooms and lighting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Attack is A Devil in Federated GAN-based Medical Image Synthesis. (arXiv:2207.00762v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00762">
<div class="article-summary-box-inner">
<span><p>Deep Learning-based image synthesis techniques have been applied in
healthcare research for generating medical images to support open research.
Training generative adversarial neural networks (GAN) usually requires large
amounts of training data. Federated learning (FL) provides a way of training a
central model using distributed data from different medical institutions while
keeping raw data locally. However, FL is vulnerable to backdoor attack, an
adversarial by poisoning training data, given the central server cannot access
the original data directly. Most backdoor attack strategies focus on
classification models and centralized domains. In this study, we propose a way
of attacking federated GAN (FedGAN) by treating the discriminator with a
commonly used data poisoning strategy in backdoor attack classification models.
We demonstrate that adding a small trigger with size less than 0.5 percent of
the original image size can corrupt the FL-GAN model. Based on the proposed
attack, we provide two effective defense strategies: global malicious detection
and local training regularization. We show that combining the two defense
strategies yields a robust medical image generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift. (arXiv:2207.00769v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00769">
<div class="article-summary-box-inner">
<span><p>Class distribution plays an important role in learning deep classifiers. When
the proportion of each class in the test set differs from the training set, the
performance of classification nets usually degrades. Such a label distribution
shift problem is common in medical diagnosis since the prevalence of disease
vary over location and time. In this paper, we propose the first method to
tackle label shift for medical image classification, which effectively adapt
the model learned from a single training label distribution to arbitrary
unknown test label distribution. Our approach innovates distribution
calibration to learn multiple representative classifiers, which are capable of
handling different one-dominating-class distributions. When given a test image,
the diverse classifiers are dynamically aggregated via the consistency-driven
test-time adaptation, to deal with the unknown test label distribution. We
validate our method on two important medical image classification tasks
including liver fibrosis staging and COVID-19 severity prediction. Our
experiments clearly show the decreased model performance under label shift.
With our method, model performance significantly improves on all the test
datasets with different label shifts for both medical image diagnosis tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Cross-Image Object Semantic Relation in Transformer for Few-Shot Fine-Grained Image Classification. (arXiv:2207.00784v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00784">
<div class="article-summary-box-inner">
<span><p>Few-shot fine-grained learning aims to classify a query image into one of a
set of support categories with fine-grained differences. Although learning
different objects' local differences via Deep Neural Networks has achieved
success, how to exploit the query-support cross-image object semantic relations
in Transformer-based architecture remains under-explored in the few-shot
fine-grained scenario. In this work, we propose a Transformer-based
double-helix model, namely HelixFormer, to achieve the cross-image object
semantic relation mining in a bidirectional and symmetrical manner. The
HelixFormer consists of two steps: 1) Relation Mining Process (RMP) across
different branches, and 2) Representation Enhancement Process (REP) within each
individual branch. By the designed RMP, each branch can extract fine-grained
object-level Cross-image Semantic Relation Maps (CSRMs) using information from
the other branch, ensuring better cross-image interaction in semantically
related local object regions. Further, with the aid of CSRMs, the developed REP
can strengthen the extracted features for those discovered semantically-related
local regions in each branch, boosting the model's ability to distinguish
subtle feature differences of fine-grained objects. Extensive experiments
conducted on five public fine-grained benchmarks demonstrate that HelixFormer
can effectively enhance the cross-image object semantic relation matching for
recognizing fine-grained objects, achieving much better performance over most
state-of-the-art methods under 1-shot and 5-shot scenarios. Our code is
available at: https://github.com/JiakangYuan/HelixFormer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Representations as Fixed Points: Training Iterative Refinement Algorithms with Implicit Differentiation. (arXiv:2207.00787v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00787">
<div class="article-summary-box-inner">
<span><p>Iterative refinement -- start with a random guess, then iteratively improve
the guess -- is a useful paradigm for representation learning because it offers
a way to break symmetries among equally plausible explanations for the data.
This property enables the application of such methods to infer representations
of sets of entities, such as objects in physical scenes, structurally
resembling clustering algorithms in latent space. However, most prior works
differentiate through the unrolled refinement process, which can make
optimization challenging. We observe that such methods can be made
differentiable by means of the implicit function theorem, and develop an
implicit differentiation approach that improves the stability and tractability
of training by decoupling the forward and backward passes. This connection
enables us to apply advances in optimizing implicit layers to not only improve
the optimization of the slot attention module in SLATE, a state-of-the-art
method for learning entity representations, but do so with constant space and
time complexity in backpropagation and only one additional line of code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boundary-Guided Camouflaged Object Detection. (arXiv:2207.00794v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00794">
<div class="article-summary-box-inner">
<span><p>Camouflaged object detection (COD), segmenting objects that are elegantly
blended into their surroundings, is a valuable yet challenging task. Existing
deep-learning methods often fall into the difficulty of accurately identifying
the camouflaged object with complete and fine object structure. To this end, in
this paper, we propose a novel boundary-guided network (BGNet) for camouflaged
object detection. Our method explores valuable and extra object-related edge
semantics to guide representation learning of COD, which forces the model to
generate features that highlight object structure, thereby promoting
camouflaged object detection of accurate boundary localization. Extensive
experiments on three challenging benchmark datasets demonstrate that our BGNet
significantly outperforms the existing 18 state-of-the-art methods under four
widely-used evaluation metrics. Our code is publicly available at:
https://github.com/thograce/BGNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarks for Industrial Inspection Based on Structured Light. (arXiv:2207.00796v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00796">
<div class="article-summary-box-inner">
<span><p>Robustness and accuracy are two critical metrics for industrial inspection.
In this paper, we propose benchmarks that can evaluate the structured light
method's performance. Our evaluation metric was learning from a lot of
inspection tasks from the factories. The metric we proposed consists of four
detailed criteria such as flatness, length, height and sphericity. Then we can
judge whether the structured light method/device can be applied to a specified
inspection task by our evaluation metric quickly. A structured light device
built for TypeC pin needles inspection performance is evaluated via our metrics
in the final experimental section.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: Adaptive Curriculum Learning for Thyroid Nodule Diagnosis. (arXiv:2207.00807v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00807">
<div class="article-summary-box-inner">
<span><p>Thyroid nodule classification aims at determining whether the nodule is
benign or malignant based on a given ultrasound image. However, the label
obtained by the cytological biopsy which is the golden standard in clinical
medicine is not always consistent with the ultrasound imaging TI-RADS criteria.
The information difference between the two causes the existing deep
learning-based classification methods to be indecisive. To solve the
Inconsistent Label problem, we propose an Adaptive Curriculum Learning (ACL)
framework, which adaptively discovers and discards the samples with
inconsistent labels. Specifically, ACL takes both hard sample and model
certainty into account, and could accurately determine the threshold to
distinguish the samples with Inconsistent Label. Moreover, we contribute TNCD:
a Thyroid Nodule Classification Dataset to facilitate future related research
on the thyroid nodules. Extensive experimental results on TNCD based on three
different backbone networks not only demonstrate the superiority of our method
but also prove that the less-is-more principle which strategically discards the
samples with Inconsistent Label could yield performance gains. Source code and
data are available at https://github.com/chenghui-666/ACL/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulating reaction time for Eureka effect in visual object recognition using artificial neural network. (arXiv:2207.00815v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00815">
<div class="article-summary-box-inner">
<span><p>The human brain can recognize objects hidden in even severely degraded images
after observing them for a while, which is known as a type of Eureka effect,
possibly associated with human creativity. A previous psychological study
suggests that the basis of this "Eureka recognition" is neural processes of
coincidence of multiple stochastic activities. Here we constructed an
artificial-neural-network-based model that simulated the characteristics of the
human Eureka recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImLoveNet: Misaligned Image-supported Registration Network for Low-overlap Point Cloud Pairs. (arXiv:2207.00826v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00826">
<div class="article-summary-box-inner">
<span><p>Low-overlap regions between paired point clouds make the captured features
very low-confidence, leading cutting edge models to point cloud registration
with poor quality. Beyond the traditional wisdom, we raise an intriguing
question: Is it possible to exploit an intermediate yet misaligned image
between two low-overlap point clouds to enhance the performance of cutting-edge
registration models? To answer it, we propose a misaligned image supported
registration network for low-overlap point cloud pairs, dubbed ImLoveNet.
ImLoveNet first learns triple deep features across different modalities and
then exports these features to a two-stage classifier, for progressively
obtaining the high-confidence overlap region between the two point clouds.
Therefore, soft correspondences are well established on the predicted overlap
region, resulting in accurate rigid transformations for registration. ImLoveNet
is simple to implement yet effective, since 1) the misaligned image provides
clearer overlap information for the two low-overlap point clouds to better
locate overlap parts; 2) it contains certain geometry knowledge to extract
better deep features; and 3) it does not require the extrinsic parameters of
the imaging device with respect to the reference frame of the 3D point cloud.
Extensive qualitative and quantitative evaluations on different kinds of
benchmarks demonstrate the effectiveness and superiority of our ImLoveNet over
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UTD-Yolov5: A Real-time Underwater Targets Detection Method based on Attention Improved YOLOv5. (arXiv:2207.00837v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00837">
<div class="article-summary-box-inner">
<span><p>As the treasure house of nature, the ocean contains abundant resources. But
the coral reefs, which are crucial to the sustainable development of marine
life, are facing a huge crisis because of the existence of COTS and other
organisms. The protection of society through manual labor is limited and
inefficient. The unpredictable nature of the marine environment also makes
manual operations risky. The use of robots for underwater operations has become
a trend. However, the underwater image acquisition has defects such as weak
light, low resolution, and many interferences, while the existing target
detection algorithms are not effective. Based on this, we propose an underwater
target detection algorithm based on Attention Improved YOLOv5, called
UTD-Yolov5. It can quickly and efficiently detect COTS, which in turn provides
a prerequisite for complex underwater operations. We adjusted the original
network architecture of YOLOv5 in multiple stages, including: replacing the
original Backbone with a two-stage cascaded CSP (CSP2); introducing the visual
channel attention mechanism module SE; designing random anchor box similarity
calculation method etc. These operations enable UTD-Yolov5 to detect more
flexibly and capture features more accurately. In order to make the network
more efficient, we also propose optimization methods such as WBF and iterative
refinement mechanism. This paper conducts a lot of experiments based on the
CSIRO dataset [1]. The results show that the average accuracy of our UTD-Yolov5
reaches 78.54%, which is a great improvement compared to the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised Approach. (arXiv:2207.00844v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00844">
<div class="article-summary-box-inner">
<span><p>Medical image synthesis has attracted increasing attention because it could
generate missing image data, improving diagnosis and benefits many downstream
tasks. However, so far the developed synthesis model is not adaptive to unseen
data distribution that presents domain shift, limiting its applicability in
clinical routine. This work focuses on exploring domain adaptation (DA) of 3D
image-to-image synthesis models. First, we highlight the technical difference
in DA between classification, segmentation and synthesis models. Second, we
present a novel efficient adaptation approach based on 2D variational
autoencoder which approximates 3D distributions. Third, we present empirical
studies on the effect of the amount of adaptation data and the key
hyper-parameters. Our results show that the proposed approach can significantly
improve the synthesis accuracy on unseen domains in a 3D setting. The code is
publicly available at
https://github.com/WinstonHuTiger/2D_VAE_UDA_for_3D_sythesis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less Is More: A Comparison of Active Learning Strategies for 3D Medical Image Segmentation. (arXiv:2207.00845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00845">
<div class="article-summary-box-inner">
<span><p>Since labeling medical image data is a costly and labor-intensive process,
active learning has gained much popularity in the medical image segmentation
domain in recent years. A variety of active learning strategies have been
proposed in the literature, but their effectiveness is highly dependent on the
dataset and training scenario. To facilitate the comparison of existing
strategies and provide a baseline for evaluating novel strategies, we evaluate
the performance of several well-known active learning strategies on three
datasets from the Medical Segmentation Decathlon. Additionally, we consider a
strided sampling strategy specifically tailored to 3D image data. We
demonstrate that both random and strided sampling act as strong baselines and
discuss the advantages and disadvantages of the studied methods. To allow other
researchers to compare their work to our results, we provide an open-source
framework for benchmarking active learning strategies on a variety of medical
segmentation datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hardware architecture for high throughput event visual data filtering with matrix of IIR filters algorithm. (arXiv:2207.00860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00860">
<div class="article-summary-box-inner">
<span><p>Neuromorphic vision is a rapidly growing field with numerous applications in
the perception systems of autonomous vehicles. Unfortunately, due to the
sensors working principle, there is a significant amount of noise in the event
stream. In this paper we present a novel algorithm based on an IIR filter
matrix for filtering this type of noise and a hardware architecture that allows
its acceleration using an SoC FPGA. Our method has a very good filtering
efficiency for uncorrelated noise - over 99% of noisy events are removed. It
has been tested for several event data sets with added random noise. We
designed the hardware architecture in such a way as to reduce the utilisation
of the FPGA's internal BRAM resources. This enabled a very low latency and a
throughput of up to 385.8 MEPS million events per second.The proposed hardware
architecture was verified in simulation and in hardware on the Xilinx Zynq
Ultrascale+ MPSoC chip on the Mercury+ XU9 module with the Mercury+ ST1 base
board.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ORA3D: Overlap Region Aware Multi-view 3D Object Detection. (arXiv:2207.00865v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00865">
<div class="article-summary-box-inner">
<span><p>In multi-view 3D object detection tasks, disparity supervision over
overlapping image regions substantially improves the overall detection
performance. However, current multi-view 3D object detection methods often fail
to detect objects in the overlap region properly, and the network's
understanding of the scene is often limited to that of a monocular detection
network. To mitigate this issue, we advocate for applying the traditional
stereo disparity estimation method to obtain reliable disparity information for
the overlap region. Given the disparity estimates as a supervision, we propose
to regularize the network to fully utilize the geometric potential of binocular
images, and improve the overall detection accuracy. Moreover, we propose to use
an adversarial overlap region discriminator, which is trained to minimize the
representational gap between non-overlap regions and overlapping regions where
objects are often largely occluded or suffer from deformation due to camera
distortion, causing a domain shift. We demonstrate the effectiveness of the
proposed method with the large-scale multi-view 3D object detection benchmark,
called nuScenes. Our experiment shows that our proposed method outperforms the
current state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Video Object Segmentation with Adaptive Object Calibration. (arXiv:2207.00887v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00887">
<div class="article-summary-box-inner">
<span><p>In the booming video era, video segmentation attracts increasing research
attention in the multimedia community. Semi-supervised video object
segmentation (VOS) aims at segmenting objects in all target frames of a video,
given annotated object masks of reference frames. Most existing methods build
pixel-wise reference-target correlations and then perform pixel-wise tracking
to obtain target masks. Due to neglecting object-level cues, pixel-level
approaches make the tracking vulnerable to perturbations, and even
indiscriminate among similar objects. Towards robust VOS, the key insight is to
calibrate the representation and mask of each specific object to be expressive
and discriminative. Accordingly, we propose a new deep network, which can
adaptively construct object representations and calibrate object masks to
achieve stronger robustness. First, we construct the object representations by
applying an adaptive object proxy (AOP) aggregation method, where the proxies
represent arbitrary-shaped segments at multi-levels for reference. Then,
prototype masks are initially generated from the reference-target correlations
based on AOP. Afterwards, such proto-masks are further calibrated through
network modulation, conditioning on the object proxy representations. We
consolidate this conditional mask calibration process in a progressive manner,
where the object representations and proto-masks evolve to be discriminative
iteratively. Extensive experiments are conducted on the standard VOS
benchmarks, YouTube-VOS-18/19 and DAVIS-17. Our model achieves the
state-of-the-art performance among existing published works, and also exhibits
superior robustness against perturbations. Our project repo is at
https://github.com/JerryX1110/Robust-Video-Object-Segmentation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Morphing Attack Detection Using Privacy-Aware Training Data. (arXiv:2207.00899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00899">
<div class="article-summary-box-inner">
<span><p>Images of morphed faces pose a serious threat to face recognition--based
security systems, as they can be used to illegally verify the identity of
multiple people with a single morphed image. Modern detection algorithms learn
to identify such morphing attacks using authentic images of real individuals.
This approach raises various privacy concerns and limits the amount of publicly
available training data. In this paper, we explore the efficacy of detection
algorithms that are trained only on faces of non--existing people and their
respective morphs. To this end, two dedicated algorithms are trained with
synthetic data and then evaluated on three real-world datasets, i.e.:
FRLL-Morphs, FERET-Morphs and FRGC-Morphs. Our results show that synthetic
facial images can be successfully employed for the training process of the
detection algorithms and generalize well to real-world scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drift Reduction for Monocular Visual Odometry of Intelligent Vehicles using Feedforward Neural Networks. (arXiv:2207.00909v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00909">
<div class="article-summary-box-inner">
<span><p>In this paper, an approach for reducing the drift in monocular visual
odometry algorithms is proposed based on a feedforward neural network. A visual
odometry algorithm computes the incremental motion of the vehicle between the
successive camera frames, then integrates these increments to determine the
pose of the vehicle. The proposed neural network reduces the errors in the pose
estimation of the vehicle which results from the inaccuracies in features
detection and matching, camera intrinsic parameters, and so on. These
inaccuracies are propagated to the motion estimation of the vehicle causing
larger amounts of estimation errors. The drift reducing neural network
identifies such errors based on the motion of features in the successive camera
frames leading to more accurate incremental motion estimates. The proposed
drift reducing neural network is trained and validated using the KITTI dataset
and the results show the efficacy of the proposed approach in reducing the
errors in the incremental orientation estimation, thus reducing the overall
error in the pose estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SKIPP'D: a SKy Images and Photovoltaic Power Generation Dataset for Short-term Solar Forecasting. (arXiv:2207.00913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00913">
<div class="article-summary-box-inner">
<span><p>Large-scale integration of photovoltaics (PV) into electricity grids is
challenged by the intermittent nature of solar power. Sky-image-based solar
forecasting using deep learning has been recognized as a promising approach to
predicting the short-term fluctuations. However, there are few publicly
available standardized benchmark datasets for image-based solar forecasting,
which limits the comparison of different forecasting models and the exploration
of forecasting methods. To fill these gaps, we introduce SKIPP'D -- a SKy
Images and Photovoltaic Power Generation Dataset. The dataset contains three
years (2017-2019) of quality-controlled down-sampled sky images and PV power
generation data that is ready-to-use for short-term solar forecasting using
deep learning. In addition, to support the flexibility in research, we provide
the high resolution, high frequency sky images and PV power generation data as
well as the concurrent sky video footage. We also include a code base
containing data processing scripts and baseline model implementations for
researchers to reproduce our previous work and accelerate their research in
solar forecasting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Sign Language Recognition via Temporal Super-Resolution Network. (arXiv:2207.00928v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00928">
<div class="article-summary-box-inner">
<span><p>Aiming at the problem that the spatial-temporal hierarchical continuous sign
language recognition model based on deep learning has a large amount of
computation, which limits the real-time application of the model, this paper
proposes a temporal super-resolution network(TSRNet). The data is reconstructed
into a dense feature sequence to reduce the overall model computation while
keeping the final recognition accuracy loss to a minimum. The continuous sign
language recognition model(CSLR) via TSRNet mainly consists of three parts:
frame-level feature extraction, time series feature extraction and TSRNet,
where TSRNet is located between frame-level feature extraction and time-series
feature extraction, which mainly includes two branches: detail descriptor and
rough descriptor. The sparse frame-level features are fused through the
features obtained by the two designed branches as the reconstructed dense
frame-level feature sequence, and the connectionist temporal
classification(CTC) loss is used for training and optimization after the
time-series feature extraction part. To better recover semantic-level
information, the overall model is trained with the self-generating adversarial
training method proposed in this paper to reduce the model error rate. The
training method regards the TSRNet as the generator, and the frame-level
processing part and the temporal processing part as the discriminator. In
addition, in order to unify the evaluation criteria of model accuracy loss
under different benchmarks, this paper proposes word error rate
deviation(WERD), which takes the error rate between the estimated word error
rate (WER) and the reference WER obtained by the reconstructed frame-level
feature sequence and the complete original frame-level feature sequence as the
WERD. Experiments on two large-scale sign language datasets demonstrate the
effectiveness of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable by Design: Learning Predictors by Composing Interpretable Queries. (arXiv:2207.00938v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00938">
<div class="article-summary-box-inner">
<span><p>There is a growing concern about typically opaque decision-making with
high-performance machine learning algorithms. Providing an explanation of the
reasoning process in domain-specific terms can be crucial for adoption in
risk-sensitive domains such as healthcare. We argue that machine learning
algorithms should be interpretable by design and that the language in which
these interpretations are expressed should be domain- and task-dependent.
Consequently, we base our model's prediction on a family of user-defined and
task-specific binary functions of the data, each having a clear interpretation
to the end-user. We then minimize the expected number of queries needed for
accurate prediction on any given input. As the solution is generally
intractable, following prior work, we choose the queries sequentially based on
information gain. However, in contrast to previous work, we need not assume the
queries are conditionally independent. Instead, we leverage a stochastic
generative model (VAE) and an MCMC algorithm (Unadjusted Langevin) to select
the most informative query about the input based on previous query-answers.
This enables the online determination of a query chain of whatever depth is
required to resolve prediction ambiguities. Finally, experiments on vision and
NLP tasks demonstrate the efficacy of our approach and its superiority over
post-hoc explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Degradation-Guided Meta-Restoration Network for Blind Super-Resolution. (arXiv:2207.00943v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00943">
<div class="article-summary-box-inner">
<span><p>Blind super-resolution (SR) aims to recover high-quality visual textures from
a low-resolution (LR) image, which is usually degraded by down-sampling blur
kernels and additive noises. This task is extremely difficult due to the
challenges of complicated image degradations in the real-world. Existing SR
approaches either assume a predefined blur kernel or a fixed noise, which
limits these approaches in challenging cases. In this paper, we propose a
Degradation-guided Meta-restoration network for blind Super-Resolution (DMSR)
that facilitates image restoration for real cases. DMSR consists of a
degradation extractor and meta-restoration modules. The extractor estimates the
degradations in LR inputs and guides the meta-restoration modules to predict
restoration parameters for different degradations on-the-fly. DMSR is jointly
optimized by a novel degradation consistency loss and reconstruction losses.
Through such an optimization, DMSR outperforms SOTA by a large margin on three
widely-used benchmarks. A user study including 16 subjects further validates
the superiority of DMSR in real-world blind SR tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PS$^2$F: Polarized Spiral Point Spread Function for Single-Shot 3D Sensing. (arXiv:2207.00945v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00945">
<div class="article-summary-box-inner">
<span><p>We propose a compact snapshot monocular depth estimation technique that
relies on an engineered point spread function (PSF). Traditional approaches
used in microscopic super-resolution imaging, such as the Double-Helix PSF
(DHPSF), are ill-suited for scenes that are more complex than a sparse set of
point light sources. We show, using the Cram\'er-Rao lower bound (CRLB), that
separating the two lobes of the DHPSF and thereby capturing two separate images
leads to a dramatic increase in depth accuracy. A unique property of the phase
mask used for generating the DHPSF is that a separation of the phase mask into
two halves leads to a spatial separation of the two lobes. We leverage this
property to build a compact polarization-based optical setup, where we place
two orthogonal linear polarizers on each half of the DHPSF phase mask and then
capture the resulting image with a polarization sensitive camera. Results from
simulations and a lab prototype demonstrate that our technique achieves up to
$50\%$ lower depth error compared to state-of-the-art designs including the
DHPSF, and the Tetrapod PSF, with little to no loss in spatial resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WaferSegClassNet -- A Light-weight Network for Classification and Segmentation of Semiconductor Wafer Defects. (arXiv:2207.00960v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00960">
<div class="article-summary-box-inner">
<span><p>As the integration density and design intricacy of semiconductor wafers
increase, the magnitude and complexity of defects in them are also on the rise.
Since the manual inspection of wafer defects is costly, an automated artificial
intelligence (AI) based computer-vision approach is highly desired. The
previous works on defect analysis have several limitations, such as low
accuracy and the need for separate models for classification and segmentation.
For analyzing mixed-type defects, some previous works require separately
training one model for each defect type, which is non-scalable. In this paper,
we present WaferSegClassNet (WSCN), a novel network based on encoder-decoder
architecture. WSCN performs simultaneous classification and segmentation of
both single and mixed-type wafer defects. WSCN uses a "shared encoder" for
classification, and segmentation, which allows training WSCN end-to-end. We use
N-pair contrastive loss to first pretrain the encoder and then use BCE-Dice
loss for segmentation, and categorical cross-entropy loss for classification.
Use of N-pair contrastive loss helps in better embedding representation in the
latent dimension of wafer maps. WSCN has a model size of only 0.51MB and
performs only 0.2M FLOPS. Thus, it is much lighter than other state-of-the-art
models. Also, it requires only 150 epochs for convergence, compared to 4,000
epochs needed by a previous work. We evaluate our model on the MixedWM38
dataset, which has 38,015 images. WSCN achieves an average classification
accuracy of 98.2% and a dice coefficient of 0.9999. We are the first to show
segmentation results on the MixedWM38 dataset. The source code can be obtained
from https://github.com/ckmvigil/WaferSegClassNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cycle-Interactive Generative Adversarial Network for Robust Unsupervised Low-Light Enhancement. (arXiv:2207.00965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00965">
<div class="article-summary-box-inner">
<span><p>Getting rid of the fundamental limitations in fitting to the paired training
data, recent unsupervised low-light enhancement methods excel in adjusting
illumination and contrast of images. However, for unsupervised low light
enhancement, the remaining noise suppression issue due to the lacking of
supervision of detailed signal largely impedes the wide deployment of these
methods in real-world applications. Herein, we propose a novel
Cycle-Interactive Generative Adversarial Network (CIGAN) for unsupervised
low-light image enhancement, which is capable of not only better transferring
illumination distributions between low/normal-light images but also
manipulating detailed signals between two domains, e.g.,
suppressing/synthesizing realistic noise in the cyclic enhancement/degradation
process. In particular, the proposed low-light guided transformation
feed-forwards the features of low-light images from the generator of
enhancement GAN (eGAN) into the generator of degradation GAN (dGAN). With the
learned information of real low-light images, dGAN can synthesize more
realistic diverse illumination and contrast in low-light images. Moreover, the
feature randomized perturbation module in dGAN learns to increase the feature
randomness to produce diverse feature distributions, persuading the synthesized
low-light images to contain realistic noise. Extensive experiments demonstrate
both the superiority of the proposed method and the effectiveness of each
module in CIGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Features of a Splashing Drop on a Solid Surface and the Temporal Evolution extracted through Image-Sequence Classification using an Interpretable Feedforward Neural Network. (arXiv:2207.00971v1 [physics.flu-dyn])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00971">
<div class="article-summary-box-inner">
<span><p>This paper reports the features of a splashing drop on a solid surface and
the temporal evolution, which are extracted through image-sequence
classification using a highly interpretable feedforward neural network (FNN)
with zero hidden layer. The image sequences used for training-validation and
testing of the FNN show the early-stage deformation of milli-sized ethanol
drops that impact a hydrophilic glass substrate with the Weber number ranges
between 31-474 (splashing threshold about 173). Specific videographing
conditions and digital image processing are performed to ensure the high
similarity among the image sequences. As a result, the trained FNNs achieved a
test accuracy higher than 96%. Remarkably, the feature extraction shows that
the trained FNN identifies the temporal evolution of the ejected secondary
droplets around the aerodynamically lifted lamella and the relatively high
contour of the main body as the features of a splashing drop, while the
relatively short and thick lamella as the feature of a nonsplashing drop. The
physical interpretation for these features and their respective temporal
evolution have been identified except for the difference in contour height of
the main body between splashing and nonsplashing drops. The observation
reported in this study is important for the development of a data-driven
simulation for modeling the deformation of a splashing drop during the impact
on a solid surface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trichomonas Vaginalis Segmentation in Microscope Images. (arXiv:2207.00973v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00973">
<div class="article-summary-box-inner">
<span><p>Trichomoniasis is a common infectious disease with high incidence caused by
the parasite Trichomonas vaginalis, increasing the risk of getting HIV in
humans if left untreated. Automated detection of Trichomonas vaginalis from
microscopic images can provide vital information for the diagnosis of
trichomoniasis. However, accurate Trichomonas vaginalis segmentation (TVS) is a
challenging task due to the high appearance similarity between the Trichomonas
and other cells (e.g., leukocyte), the large appearance variation caused by
their motility, and, most importantly, the lack of large-scale annotated data
for deep model training. To address these challenges, we elaborately collected
the first large-scale Microscopic Image dataset of Trichomonas Vaginalis, named
TVMI3K, which consists of 3,158 images covering Trichomonas of various
appearances in diverse backgrounds, with high-quality annotations including
object-level mask labels, object boundaries, and challenging attributes.
Besides, we propose a simple yet effective baseline, termed TVNet, to
automatically segment Trichomonas from microscopic images, including
high-resolution fusion and foreground-background attention modules. Extensive
experiments demonstrate that our model achieves superior segmentation
performance and outperforms various cutting-edge object detection models both
quantitatively and qualitatively, making it a promising framework to promote
future research in TVS tasks. The dataset and results will be publicly
available at: https://github.com/CellRecog/cellRecog.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NARRATE: A Normal Assisted Free-View Portrait Stylizer. (arXiv:2207.00974v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00974">
<div class="article-summary-box-inner">
<span><p>In this work, we propose NARRATE, a novel pipeline that enables
simultaneously editing portrait lighting and perspective in a photorealistic
manner. As a hybrid neural-physical face model, NARRATE leverages complementary
benefits of geometry-aware generative approaches and normal-assisted physical
face models. In a nutshell, NARRATE first inverts the input portrait to a
coarse geometry and employs neural rendering to generate images resembling the
input, as well as producing convincing pose changes. However, inversion step
introduces mismatch, bringing low-quality images with less facial details. As
such, we further estimate portrait normal to enhance the coarse geometry,
creating a high-fidelity physical face model. In particular, we fuse the neural
and physical renderings to compensate for the imperfect inversion, resulting in
both realistic and view-consistent novel perspective images. In relighting
stage, previous works focus on single view portrait relighting but ignoring
consistency between different perspectives as well, leading unstable and
inconsistent lighting effects for view changes. We extend Total Relighting to
fix this problem by unifying its multi-view input normal maps with the physical
face model. NARRATE conducts relighting with consistent normal maps, imposing
cross-view constraints and exhibiting stable and coherent illumination effects.
We experimentally demonstrate that NARRATE achieves more photorealistic,
reliable results over prior works. We further bridge NARRATE with animation and
style transfer tools, supporting pose change, light change, facial animation,
and style transfer, either separately or in combination, all at a photographic
quality. We showcase vivid free-view facial animations as well as 3D-aware
relightable stylization, which help facilitate various AR/VR applications like
virtual cinematography, 3D video conferencing, and post-production.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stabilizing Off-Policy Deep Reinforcement Learning from Pixels. (arXiv:2207.00986v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00986">
<div class="article-summary-box-inner">
<span><p>Off-policy reinforcement learning (RL) from pixel observations is notoriously
unstable. As a result, many successful algorithms must combine different
domain-specific practices and auxiliary losses to learn meaningful behaviors in
complex environments. In this work, we provide novel analysis demonstrating
that these instabilities arise from performing temporal-difference learning
with a convolutional encoder and low-magnitude rewards. We show that this new
visual deadly triad causes unstable training and premature convergence to
degenerate solutions, a phenomenon we name catastrophic self-overfitting. Based
on our analysis, we propose A-LIX, a method providing adaptive regularization
to the encoder's gradients that explicitly prevents the occurrence of
catastrophic self-overfitting using a dual objective. By applying A-LIX, we
significantly outperform the prior state-of-the-art on the DeepMind Control and
Atari 100k benchmarks without any data augmentation or auxiliary losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic boxes fusion strategy in object detection. (arXiv:2207.00997v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00997">
<div class="article-summary-box-inner">
<span><p>Object detection on microscopic scenarios is a popular task. As microscopes
always have variable magnifications, the object can vary substantially in
scale, which burdens the optimization of detectors. Moreover, different
situations of camera focusing bring in the blurry images, which leads to great
challenge of distinguishing the boundaries between objects and background. To
solve the two issues mentioned above, we provide bags of useful training
strategies and extensive experiments on Chula-ParasiteEgg-11 dataset, bring
non-negligible results on ICIP 2022 Challenge: Parasitic Egg Detection and
Classification in Microscopic Images, further more, we propose a new box
selection strategy and an improved boxes fusion method for multi-model
ensemble, as a result our method wins 1st place(mIoU 95.28%, mF1Score 99.62%),
which is also the state-of-the-art method on Chula-ParasiteEgg-11 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised learning for improving the accuracy of robot-mounted 3D camera applied to human gait analysis. (arXiv:2207.01002v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01002">
<div class="article-summary-box-inner">
<span><p>The use of 3D cameras for gait analysis has been highly questioned due to the
low accuracy they have demonstrated in the past. The objective of the study
presented in this paper is to improve the accuracy of the estimations made by
robot-mounted 3D cameras in human gait analysis by applying a supervised
learning stage. The 3D camera was mounted in a mobile robot to obtain a longer
walking distance. This study shows an improvement in detection of kinematic
gait signals and gait descriptors by post-processing the raw estimations of the
camera using artificial neural networks trained with the data obtained from a
certified Vicon system. To achieve this, 37 healthy participants were recruited
and data of 207 gait sequences were collected using an Orbbec Astra 3D camera.
There are two basic possible approaches for training: using kinematic gait
signals and using gait descriptors. The former seeks to improve the waveforms
of kinematic gait signals by reducing the error and increasing the correlation
with respect to the Vicon system. The second is a more direct approach,
focusing on training the artificial neural networks using gait descriptors
directly. The accuracy of the 3D camera was measured before and after training.
In both training approaches, an improvement was observed. Kinematic gait
signals showed lower errors and higher correlations with respect to the ground
truth. The accuracy of the system to detect gait descriptors also showed a
substantial improvement, mostly for kinematic descriptors rather than
spatio-temporal. When comparing both training approaches, it was not possible
to define which was the absolute best. Therefore, we believe that the selection
of the training approach will depend on the purpose of the study to be
conducted. This study reveals the great potential of 3D cameras and encourages
the research community to continue exploring their use in gait analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lasers to Events: Automatic Extrinsic Calibration of Lidars and Event Cameras. (arXiv:2207.01009v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01009">
<div class="article-summary-box-inner">
<span><p>Despite significant academic and corporate efforts, autonomous driving under
adverse visual conditions still proves challenging. As neuromorphic technology
has matured, its application to robotics and autonomous vehicle systems has
become an area of active research. Low-light and latency-demanding situations
can benefit. To enable event cameras to operate alongside staple sensors like
lidar in perception tasks, we propose a direct, temporally-decoupled
calibration method between event cameras and lidars. The high dynamic range and
low-light operation of event cameras are exploited to directly register lidar
laser returns, allowing information-based correlation methods to optimize for
the 6-DoF extrinsic calibration between the two sensors. This paper presents
the first direct calibration method between event cameras and lidars, removing
dependencies on frame-based camera intermediaries and/or highly-accurate hand
measurements. Code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial Image Reconstruction from Functional Magnetic Resonance Imaging via GAN Inversion with Improved Attribute Consistency. (arXiv:2207.01011v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01011">
<div class="article-summary-box-inner">
<span><p>Neuroscience studies have revealed that the brain encodes visual content and
embeds information in neural activity. Recently, deep learning techniques have
facilitated attempts to address visual reconstructions by mapping brain
activity to image stimuli using generative adversarial networks (GANs).
However, none of these studies have considered the semantic meaning of latent
code in image space. Omitting semantic information could potentially limit the
performance. In this study, we propose a new framework to reconstruct facial
images from functional Magnetic Resonance Imaging (fMRI) data. With this
framework, the GAN inversion is first applied to train an image encoder to
extract latent codes in image space, which are then bridged to fMRI data using
linear transformation. Following the attributes identified from fMRI data using
an attribute classifier, the direction in which to manipulate attributes is
decided and the attribute manipulator adjusts the latent code to improve the
consistency between the seen image and the reconstructed image. Our
experimental results suggest that the proposed framework accomplishes two
goals: (1) reconstructing clear facial images from fMRI data and (2)
maintaining the consistency of semantic characteristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Single-Frame 3D Object Detection by Simulating Multi-Frame Point Clouds. (arXiv:2207.01030v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01030">
<div class="article-summary-box-inner">
<span><p>To boost a detector for single-frame 3D object detection, we present a new
approach to train it to simulate features and responses following a detector
trained on multi-frame point clouds. Our approach needs multi-frame point
clouds only when training the single-frame detector, and once trained, it can
detect objects with only single-frame point clouds as inputs during the
inference. We design a novel Simulated Multi-Frame Single-Stage object Detector
(SMF-SSD) framework to realize the approach: multi-view dense object fusion to
densify ground-truth objects to generate a multi-frame point cloud;
self-attention voxel distillation to facilitate one-to-many knowledge transfer
from multi- to single-frame voxels; multi-scale BEV feature distillation to
transfer knowledge in low-level spatial and high-level semantic BEV features;
and adaptive response distillation to activate single-frame responses of high
confidence and accurate localization. Experimental results on the Waymo test
set show that our SMF-SSD consistently outperforms all state-of-the-art
single-frame 3D object detectors for all object classes of difficulty levels 1
and 2 in terms of both mAP and mAPH.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Based Label-Text Tuning for Few-Shot Class-Incremental Learning. (arXiv:2207.01036v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01036">
<div class="article-summary-box-inner">
<span><p>Few-shot class-incremental learning(FSCIL) focuses on designing learning
algorithms that can continually learn a sequence of new tasks from a few
samples without forgetting old ones. The difficulties are that training on a
sequence of limited data from new tasks leads to severe overfitting issues and
causes the well-known catastrophic forgetting problem. Existing researches
mainly utilize the image information, such as storing the image knowledge of
previous tasks or limiting classifiers updating. However, they ignore analyzing
the informative and less noisy text information of class labels. In this work,
we propose leveraging the label-text information by adopting the memory prompt.
The memory prompt can learn new data sequentially, and meanwhile store the
previous knowledge. Furthermore, to optimize the memory prompt without
undermining the stored knowledge, we propose a stimulation-based training
strategy. It optimizes the memory prompt depending on the image embedding
stimulation, which is the distribution of the image embedding elements.
Experiments show that our proposed method outperforms all prior
state-of-the-art approaches, significantly mitigating the catastrophic
forgetting and overfitting problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Context Information for Generic Event Boundary Captioning. (arXiv:2207.01050v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01050">
<div class="article-summary-box-inner">
<span><p>Generic Event Boundary Captioning (GEBC) aims to generate three sentences
describing the status change for a given time boundary. Previous methods only
process the information of a single boundary at a time, which lacks utilization
of video context information. To tackle this issue, we design a model that
directly takes the whole video as input and generates captions for all
boundaries parallelly. The model could learn the context information for each
time boundary by modeling the boundary-boundary interactions. Experiments
demonstrate the effectiveness of context information. The proposed method
achieved a 72.84 score on the test set, and we reached the $2^{nd}$ place in
this challenge. Our code is available at:
\url{https://github.com/zjr2000/Context-GEBC}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models. (arXiv:2207.01056v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01056">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pre-training (VLP) models have achieved state-of-the-art
performance in numerous cross-modal tasks. Since they are optimized to capture
the statistical properties of intra- and inter-modality, there remains risk to
learn social biases presented in the data as well. In this work, we (1)
introduce a counterfactual-based bias measurement \emph{CounterBias} to
quantify the social bias in VLP models by comparing the [MASK]ed prediction
probabilities of factual and counterfactual samples; (2) construct a novel
VL-Bias dataset including 24K image-text pairs for measuring gender bias in VLP
models, from which we observed that significant gender bias is prevalent in VLP
models; and (3) propose a VLP debiasing method \emph{FairVLP} to minimize the
difference in the [MASK]ed prediction probabilities between factual and
counterfactual image-text pairs for VLP debiasing. Although CounterBias and
FairVLP focus on social bias, they are generalizable to serve as tools and
provide new insights to probe and regularize more knowledge in VLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chat-to-Design: AI Assisted Personalized Fashion Design. (arXiv:2207.01058v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01058">
<div class="article-summary-box-inner">
<span><p>In this demo, we present Chat-to-Design, a new multimodal interaction system
for personalized fashion design. Compared to classic systems that recommend
apparel based on keywords, Chat-to-Design enables users to design clothes in
two steps: 1) coarse-grained selection via conversation and 2) fine-grained
editing via an interactive interface. It encompasses three sub-systems to
deliver an immersive user experience: A conversation system empowered by
natural language understanding to accept users' requests and manages dialogs; A
multimodal fashion retrieval system empowered by a large-scale pretrained
language-image network to retrieve requested apparel; A fashion design system
empowered by emerging generative techniques to edit attributes of retrieved
clothes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NP-Match: When Neural Processes meet Semi-Supervised Learning. (arXiv:2207.01066v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01066">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) has been widely explored in recent years, and
it is an effective way of leveraging unlabeled data to reduce the reliance on
labeled data. In this work, we adjust neural processes (NPs) to the
semi-supervised image classification task, resulting in a new method named
NP-Match. NP-Match is suited to this task for two reasons. Firstly, NP-Match
implicitly compares data points when making predictions, and as a result, the
prediction of each unlabeled data point is affected by the labeled data points
that are similar to it, which improves the quality of pseudo-labels. Secondly,
NP-Match is able to estimate uncertainty that can be used as a tool for
selecting unlabeled samples with reliable pseudo-labels. Compared with
uncertainty-based SSL methods implemented with Monte Carlo (MC) dropout,
NP-Match estimates uncertainty with much less computational overhead, which can
save time at both the training and the testing phases. We conducted extensive
experiments on four public datasets, and NP-Match outperforms state-of-the-art
(SOTA) results or achieves competitive results on them, which shows the
effectiveness of NP-Match and its potential for SSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Need One Detector: Unified Object Detector for Different Modalities based on Vision Transformers. (arXiv:2207.01071v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01071">
<div class="article-summary-box-inner">
<span><p>Most systems use different models for different modalities, such as one model
for processing RGB images and one for depth images. Meanwhile, some recent
works discovered that an identical model for one modality can be used for
another modality with the help of cross modality transfer learning. In this
article, we further find out that by using a vision transformer together with
cross/inter modality transfer learning, a unified detector can achieve better
performances when using different modalities as inputs. The unified model is
useful as we don't need to maintain separate models or weights for robotics,
hence, it is more efficient. One application scenario of our unified system for
robotics can be: without any model architecture and model weights updating,
robotics can switch smoothly on using RGB camera or both RGB and Depth Sensor
during the day time and Depth sensor during the night time .
</p>
<p>Experiments on SUN RGB-D dataset show: Our unified model is not only
efficient, but also has a similar or better performance in terms of mAP50 based
on SUNRGBD16 category: compare with the RGB only one, ours is slightly worse
(52.3 $\to$ 51.9). compare with the point cloud only one, we have similar
performance (52.7 $\to$ 52.8); When using the novel inter modality mixing
method proposed in this work, our model can achieve a significantly better
performance with 3.1 (52.7 $\to$ 55.8) absolute improvement comparing with the
previous best result. Code (including training/inference logs and model
checkpoints) is available: \url{https://github.com/liketheflower/YONOD.git}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sub-cluster-aware Network for Few-shot Skin Disease Classification. (arXiv:2207.01072v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01072">
<div class="article-summary-box-inner">
<span><p>This paper studies the few-shot skin disease classification problem. Based on
a crucial observation that skin disease images often exist multiple
sub-clusters within a class (i.e., the appearances of images within one class
of disease vary and form multiple distinct sub-groups), we design a novel
Sub-Cluster-Aware Network, namely SCAN, for rare skin disease diagnosis with
enhanced accuracy. As the performance of few-shot learning highly depends on
the quality of the learned feature encoder, the main principle guiding the
design of SCAN is the intrinsic sub-clustered representation learning for each
class so as to better describe feature distributions. Specifically, SCAN
follows a dual-branch framework, where the first branch is to learn class-wise
features to distinguish different skin diseases, and the second one aims to
learn features which can effectively partition each class into several groups
so as to preserve the sub-clustered structure within each class. To achieve the
objective of the second branch, we present a cluster loss to learn image
similarities via unsupervised clustering. To ensure that the samples in each
sub-cluster are from the same class, we further design a purity loss to refine
the unsupervised clustering results. We evaluate the proposed approach on two
public datasets for few-shot skin disease classification. The experimental
results validate that our framework outperforms the other state-of-the-art
methods by around 2% to 4% on the SD-198 and Derm7pt datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Deep Image Restoration. (arXiv:2207.01074v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01074">
<div class="article-summary-box-inner">
<span><p>This paper presents a new variational inference framework for image
restoration and a convolutional neural network (CNN) structure that can solve
the restoration problems described by the proposed framework. Earlier CNN-based
image restoration methods primarily focused on network architecture design or
training strategy with non-blind scenarios where the degradation models are
known or assumed. For a step closer to real-world applications, CNNs are also
blindly trained with the whole dataset, including diverse degradations.
However, the conditional distribution of a high-quality image given a diversely
degraded one is too complicated to be learned by a single CNN. Therefore, there
have also been some methods that provide additional prior information to train
a CNN. Unlike previous approaches, we focus more on the objective of
restoration based on the Bayesian perspective and how to reformulate the
objective. Specifically, our method relaxes the original posterior inference
problem to better manageable sub-problems and thus behaves like a
divide-and-conquer scheme. As a result, the proposed framework boosts the
performance of several restoration problems compared to the previous ones.
Specifically, our method delivers state-of-the-art performance on Gaussian
denoising, real-world noise reduction, blind image super-resolution, and JPEG
compression artifacts reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Patch Analysis and Mining Skills for Image Restoration Deep Neural Networks. (arXiv:2207.01075v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01075">
<div class="article-summary-box-inner">
<span><p>There have been numerous image restoration methods based on deep
convolutional neural networks (CNNs). However, most of the literature on this
topic focused on the network architecture and loss functions, while less
detailed on the training methods. Hence, some of the works are not easily
reproducible because it is required to know the hidden training skills to
obtain the same results. To be specific with the training dataset, few works
discussed how to prepare and order the training image patches. Moreover, it
requires a high cost to capture new datasets to train a restoration network for
the real-world scene. Hence, we believe it is necessary to study the
preparation and selection of training data. In this regard, we present an
analysis of the training patches and explore the consequences of different
patch extraction methods. Eventually, we propose a guideline for the patch
extraction from given training images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Divert More Attention to Vision-Language Tracking. (arXiv:2207.01076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01076">
<div class="article-summary-box-inner">
<span><p>Relying on Transformer for complex visual feature learning, object tracking
has witnessed the new standard for state-of-the-arts (SOTAs). However, this
advancement accompanies by larger training data and longer training period,
making tracking increasingly expensive. In this paper, we demonstrate that the
Transformer-reliance is not necessary and the pure ConvNets are still
competitive and even better yet more economical and friendly in achieving SOTA
tracking. Our solution is to unleash the power of multimodal vision-language
(VL) tracking, simply using ConvNets. The essence lies in learning novel
unified-adaptive VL representations with our modality mixer (ModaMixer) and
asymmetrical ConvNet search. We show that our unified-adaptive VL
representation, learned purely with the ConvNets, is a simple yet strong
alternative to Transformer visual features, by unbelievably improving a
CNN-based Siamese tracker by 14.5% in SUC on challenging LaSOT (50.7% &gt; 65.2%),
even outperforming several Transformer-based SOTA trackers. Besides empirical
results, we theoretically analyze our approach to evidence its effectiveness.
By revealing the potential of VL representation, we expect the community to
divert more attention to VL tracking and hope to open more possibilities for
future tracking beyond Transformer. Code and models will be released at
https://github.com/JudasDie/SOTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Understand Depth?. (arXiv:2207.01077v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01077">
<div class="article-summary-box-inner">
<span><p>Besides image classification, Contrastive Language-Image Pre-training (CLIP)
has accomplished extraordinary success for a wide range of vision tasks,
including object-level and 3D space understanding. However, it's still
challenging to transfer semantic knowledge learned from CLIP into more
intricate tasks of quantified targets, such as depth estimation with geometric
information. In this paper, we propose to apply CLIP for zero-shot monocular
depth estimation, named DepthCLIP. We found that the patches of the input image
could respond to a certain semantic distance token and then be projected to a
quantified depth bin for coarse estimation. Without any training, our DepthCLIP
surpasses existing unsupervised methods and even approaches the early
fully-supervised networks. To our best knowledge, we are the first to conduct
zero-shot adaptation from the semantic language knowledge to quantified
downstream tasks and perform zero-shot monocular depth estimation. We hope our
work could cast a light on future research. The code is available at
https://github.com/Adonis-galaxy/DepthCLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patient-specific modelling, simulation and real time processing for constrictive respiratory diseases. (arXiv:2207.01082v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01082">
<div class="article-summary-box-inner">
<span><p>Asthma is a common chronic disease of the respiratory system causing
significant disability and societal burden. It affects over 500 million people
worldwide and generates costs exceeding $USD 56 billion in 2011 in the United
States. Managing asthma involves controlling symptoms, preventing
exacerbations, and maintaining lung function. Improving asthma control affects
the daily life of patients and is associated with a reduced risk of
exacerbations and lung function impairment, reduces the cost of asthma care and
indirect costs associated with reduced productivity. Understanding the complex
dynamics of the pulmonary system and the lung's response to disease, injury,
and treatment is fundamental to the advancement of Asthma treatment.
Computational models of the respiratory system seek to provide a theoretical
framework to understand the interaction between structure and function. Their
application can improve pulmonary medicine by a patient-specific approach to
medicinal methodologies optimizing the delivery given the personalized geometry
and personalized ventilation patterns while introducing a patient-specific
technique that maximizes drug delivery. A three-fold objective addressed within
this dissertation becomes prominent at this point. The first part refers to the
comprehension of pulmonary pathophysiology and the mechanics of Asthma and
subsequently of constrictive pulmonary conditions in general. The second part
refers to the design and implementation of tools that facilitate personalized
medicine to improve delivery and effectiveness. Finally, the third part refers
to the self-management of the condition, meaning that medical personnel and
patients have access to tools and methods that allow the first party to easily
track the course of the condition and the second party, i.e. the patient to
easily self-manage it alleviating the significant burden from the health
system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Gesture Authoring Space: Authoring Customised Hand Gestures for Grasping Virtual Objects in Immersive Virtual Environments. (arXiv:2207.01092v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01092">
<div class="article-summary-box-inner">
<span><p>Natural user interfaces are on the rise. Manufacturers for Augmented,
Virtual, and Mixed Reality head mounted displays are increasingly integrating
new sensors into their consumer grade products, allowing gesture recognition
without additional hardware. This offers new possibilities for bare handed
interaction within virtual environments. This work proposes a hand gesture
authoring tool for object specific grab gestures allowing virtual objects to be
grabbed as in the real world. The presented solution uses template matching for
gesture recognition and requires no technical knowledge to design and create
custom tailored hand gestures. In a user study, the proposed approach is
compared with the pinch gesture and the controller for grasping virtual
objects. The different grasping techniques are compared in terms of accuracy,
task completion time, usability, and naturalness. The study showed that
gestures created with the proposed approach are perceived by users as a more
natural input modality than the others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection with Adversarially Learned Perturbations of Latent Space. (arXiv:2207.01106v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01106">
<div class="article-summary-box-inner">
<span><p>Anomaly detection is to identify samples that do not conform to the
distribution of the normal data. Due to the unavailability of anomalous data,
training a supervised deep neural network is a cumbersome task. As such,
unsupervised methods are preferred as a common approach to solve this task.
Deep autoencoders have been broadly adopted as a base of many unsupervised
anomaly detection methods. However, a notable shortcoming of deep autoencoders
is that they provide insufficient representations for anomaly detection by
generalizing to reconstruct outliers. In this work, we have designed an
adversarial framework consisting of two competing components, an Adversarial
Distorter, and an Autoencoder. The Adversarial Distorter is a convolutional
encoder that learns to produce effective perturbations and the autoencoder is a
deep convolutional neural network that aims to reconstruct the images from the
perturbed latent feature space. The networks are trained with opposing goals in
which the Adversarial Distorter produces perturbations that are applied to the
encoder's latent feature space to maximize the reconstruction error and the
autoencoder tries to neutralize the effect of these perturbations to minimize
it. When applied to anomaly detection, the proposed method learns semantically
richer representations due to applying perturbations to the feature space. The
proposed method outperforms the existing state-of-the-art methods in anomaly
detection on image and video datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augment to Detect Anomalies with Continuous Labelling. (arXiv:2207.01112v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01112">
<div class="article-summary-box-inner">
<span><p>Anomaly detection is to recognize samples that differ in some respect from
the training observations. These samples which do not conform to the
distribution of normal data are called outliers or anomalies. In real-world
anomaly detection problems, the outliers are absent, not well defined, or have
a very limited number of instances. Recent state-of-the-art deep learning-based
anomaly detection methods suffer from high computational cost, complexity,
unstable training procedures, and non-trivial implementation, making them
difficult to deploy in real-world applications. To combat this problem, we
leverage a simple learning procedure that trains a lightweight convolutional
neural network, reaching state-of-the-art performance in anomaly detection. In
this paper, we propose to solve anomaly detection as a supervised regression
problem. We label normal and anomalous data using two separable distributions
of continuous values. To compensate for the unavailability of anomalous samples
during training time, we utilize straightforward image augmentation techniques
to create a distinct set of samples as anomalies. The distribution of the
augmented set is similar but slightly deviated from the normal data, whereas
real anomalies are expected to have an even further distribution. Therefore,
training a regressor on these augmented samples will result in more separable
distributions of labels for normal and real anomalous data points. Anomaly
detection experiments on image and video datasets show the superiority of the
proposed method over the state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are 3D Face Shapes Expressive Enough for Recognising Continuous Emotions and Action Unit Intensities?. (arXiv:2207.01113v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01113">
<div class="article-summary-box-inner">
<span><p>Recognising continuous emotions and action unit (AU) intensities from face
videos requires a spatial and temporal understanding of expression dynamics.
Existing works primarily rely on 2D face appearances to extract such dynamics.
This work focuses on a promising alternative based on parametric 3D face shape
alignment models, which disentangle different factors of variation, including
expression-induced shape variations. We aim to understand how expressive 3D
face shapes are in estimating valence-arousal and AU intensities compared to
the state-of-the-art 2D appearance-based models. We benchmark four recent 3D
face alignment models: ExpNet, 3DDFA-V2, DECA, and EMOCA. In valence-arousal
estimation, expression features of 3D face models consistently surpassed
previous works and yielded an average concordance correlation of .739 and .574
on SEWA and AVEC 2019 CES corpora, respectively. We also study how 3D face
shapes performed on AU intensity estimation on BP4D and DISFA datasets, and
report that 3D face features were on par with 2D appearance features in AUs 4,
6, 10, 12, and 25, but not the entire set of AUs. To understand this
discrepancy, we conduct a correspondence analysis between valence-arousal and
AUs, which points out that accurate prediction of valence-arousal may require
the knowledge of only a few AUs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DecisioNet -- A Binary-Tree Structured Neural Network. (arXiv:2207.01127v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01127">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) and decision trees (DTs) are both
state-of-the-art classifiers. DNNs perform well due to their representational
learning capabilities, while DTs are computationally efficient as they perform
inference along one route (root-to-leaf) that is dependent on the input data.
In this paper, we present DecisioNet (DN), a binary-tree structured neural
network. We propose a systematic way to convert an existing DNN into a DN to
create a lightweight version of the original model. DecisioNet takes the best
of both worlds - it uses neural modules to perform representational learning
and utilizes its tree structure to perform only a portion of the computations.
We evaluate various DN architectures, along with their corresponding baseline
models on the FashionMNIST, CIFAR10, and CIFAR100 datasets. We show that the DN
variants achieve similar accuracy while significantly reducing the
computational cost of the original network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Visual Field of View: Perceiving 3D Environment with Echoes and Vision. (arXiv:2207.01136v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01136">
<div class="article-summary-box-inner">
<span><p>This paper focuses on perceiving and navigating 3D environments using echoes
and RGB image. In particular, we perform depth estimation by fusing RGB image
with echoes, received from multiple orientations. Unlike previous works, we go
beyond the field of view of the RGB and estimate dense depth maps for
substantially larger parts of the environment. We show that the echoes provide
holistic and in-expensive information about the 3D structures complementing the
RGB image. Moreover, we study how echoes and the wide field-of-view depth maps
can be utilised in robot navigation. We compare the proposed methods against
recent baselines using two sets of challenging realistic 3D environments:
Replica and Matterport3D. The implementation and pre-trained models will be
made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ABAW: Learning from Synthetic Data & Multi-Task Learning Challenges. (arXiv:2207.01138v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01138">
<div class="article-summary-box-inner">
<span><p>This paper describes the fourth Affective Behavior Analysis in-the-wild
(ABAW) Competition, held in conjunction with European Conference on Computer
Vision (ECCV), 2022. The 4th ABAW Competition is a continuation of the
Competitions held at IEEE CVPR 2022, ICCV 2021, IEEE FG 2020 and IEEE CVPR 2017
Conferences, and aims at automatically analyzing affect. In the previous runs
of this Competition, the Challenges targeted Valence-Arousal Estimation,
Expression Classification and Action Unit Detection. This year the Competition
encompasses two different Challenges: i) a Multi-Task-Learning one in which the
goal is to learn at the same time (i.e., in a multi-task learning setting) all
the three above mentioned tasks; and ii) a Learning from Synthetic Data one in
which the goal is to learn to recognise the basic expressions from artificially
generated data and generalise to real data. The Aff-Wild2 database is a large
scale in-the-wild database and the first one that contains annotations for
valence and arousal, expressions and action units. This database is the basis
for the above Challenges. In more detail: i) s-Aff-Wild2 -- a static version of
Aff-Wild2 database -- has been constructed and utilized for the purposes of the
Multi-Task-Learning Challenge; and ii) some specific frames-images from the
Aff-Wild2 database have been used in an expression manipulation manner for
creating the synthetic dataset, which is the basis for the Learning from
Synthetic Data Challenge. In this paper, at first we present the two
Challenges, along with the utilized corpora, then we outline the evaluation
metrics and finally present the baseline systems per Challenge, as well as
their derived results. More information regarding the Competition can be found
in the competition's website:
https://ibug.doc.ic.ac.uk/resources/eccv-2023-4th-abaw/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Train a CAT: Learning Canonical Appearance Transformations for Direct Visual Localization Under Illumination Change. (arXiv:1709.03009v6 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1709.03009">
<div class="article-summary-box-inner">
<span><p>Direct visual localization has recently enjoyed a resurgence in popularity
with the increasing availability of cheap mobile computing power. The
competitive accuracy and robustness of these algorithms compared to
state-of-the-art feature-based methods, as well as their natural ability to
yield dense maps, makes them an appealing choice for a variety of mobile
robotics applications. However, direct methods remain brittle in the face of
appearance change due to their underlying assumption of photometric
consistency, which is commonly violated in practice. In this paper, we propose
to mitigate this problem by training deep convolutional encoder-decoder models
to transform images of a scene such that they correspond to a previously-seen
canonical appearance. We validate our method in multiple environments and
illumination conditions using high-fidelity synthetic RGB-D datasets, and
integrate the trained models into a direct visual localization pipeline,
yielding improvements in visual odometry (VO) accuracy through time-varying
illumination conditions, as well as improved metric relocalization performance
under illumination change, where conventional methods normally fail. We further
provide a preliminary investigation of transfer learning from synthetic to real
environments in a localization context. An open-source implementation of our
method using PyTorch is available at https://github.com/utiasSTARS/cat-net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SiamVGG: Visual Tracking using Deeper Siamese Networks. (arXiv:1902.02804v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1902.02804">
<div class="article-summary-box-inner">
<span><p>Recently, we have seen a rapid development of Deep Neural Network (DNN) based
visual tracking solutions. Some trackers combine the DNN-based solutions with
Discriminative Correlation Filters (DCF) to extract semantic features and
successfully deliver the state-of-the-art tracking accuracy. However, these
solutions are highly compute-intensive, which require long processing time,
resulting unsecured real-time performance. To deliver both high accuracy and
reliable real-time performance, we propose a novel tracker called
SiamVGG\footnote{https://github.com/leeyeehoo/SiamVGG}. It combines a
Convolutional Neural Network (CNN) backbone and a cross-correlation operator,
and takes advantage of the features from exemplary images for more accurate
object tracking. The architecture of SiamVGG is customized from VGG-16 with the
parameters shared by both exemplary images and desired input video frames. We
demonstrate the proposed SiamVGG on OTB-2013/50/100 and VOT 2015/2016/2017
datasets with the state-of-the-art accuracy while maintaining a decent
real-time performance of 50 FPS running on a GTX 1080Ti. Our design can achieve
2% higher Expected Average Overlap (EAO) compared to the ECO and C-COT in
VOT2017 Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Seeing to Moving: A Survey on Learning for Visual Indoor Navigation (VIN). (arXiv:2002.11310v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.11310">
<div class="article-summary-box-inner">
<span><p>Visual Indoor Navigation (VIN) task has drawn increasing attention from the
data-driven machine learning communities especially with the recently reported
success from learning-based methods. Due to the innate complexity of this task,
researchers have tried approaching the problem from a variety of different
angles, the full scope of which has not yet been captured within an overarching
report. This survey first summarizes the representative work of learning-based
approaches for the VIN task and then identifies and discusses lingering issues
impeding the VIN performance, as well as motivates future research in these key
areas worth exploring for the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural Network Synthesis. (arXiv:2009.13008v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13008">
<div class="article-summary-box-inner">
<span><p>Recent advancements in the area of deep learning have shown the effectiveness
of very large neural networks in several applications. However, as these deep
neural networks continue to grow in size, it becomes more and more difficult to
configure their many parameters to obtain good results. Presently, analysts
must experiment with many different configurations and parameter settings,
which is labor-intensive and time-consuming. On the other hand, the capacity of
fully automated techniques for neural network architecture search is limited
without the domain knowledge of human experts. To deal with the problem, we
formulate the task of neural network architecture optimization as a graph space
exploration, based on the one-shot architecture search technique. In this
approach, a super-graph of all candidate architectures is trained in one-shot
and the optimal neural network is identified as a sub-graph. In this paper, we
present a framework that allows analysts to effectively build the solution
sub-graph space and guide the network search by injecting their domain
knowledge. Starting with the network architecture space composed of basic
neural network components, analysts are empowered to effectively select the
most promising components via our one-shot search scheme. Applying this
technique in an iterative manner allows analysts to converge to the best
performing neural network architecture for a given application. During the
exploration, analysts can use their domain knowledge aided by cues provided
from a scatterplot visualization of the search space to edit different
components and guide the search for faster convergence. We designed our
interface in collaboration with several deep learning researchers and its final
effectiveness is evaluated with a user study and two case studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi Scale Identity-Preserving Image-to-Image Translation Network for Low-Resolution Face Recognition. (arXiv:2010.12249v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12249">
<div class="article-summary-box-inner">
<span><p>State-of-the-art deep neural network models have reached near perfect face
recognition accuracy rates on controlled high-resolution face images. However,
their performance is drastically degraded when they are tested with very
low-resolution face images. This is particularly critical in surveillance
systems, where a low-resolution probe image is to be matched with
high-resolution gallery images. super-resolution techniques aim at producing
high-resolution face images from low-resolution counterparts. While they are
capable of reconstructing images that are visually appealing, the
identity-related information is not preserved. Here, we propose an
identity-preserving end-to-end image-to-image translation deep neural network
which is capable of super-resolving very low-resolution faces to their
high-resolution counterparts while preserving identity-related information. We
achieved this by training a very deep convolutional encoder-decoder network
with a symmetric contracting path between corresponding layers. This network
was trained with a combination of a reconstruction and an identity-preserving
loss, on multi-scale low-resolution conditions. Extensive quantitative
evaluations of our proposed model demonstrated that it outperforms competing
super-resolution and low-resolution face recognition methods on natural and
artificial low-resolution face data sets and even unseen identities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SegGroup: Seg-Level Supervision for 3D Instance and Semantic Segmentation. (arXiv:2012.10217v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10217">
<div class="article-summary-box-inner">
<span><p>Most existing point cloud instance and semantic segmentation methods rely
heavily on strong supervision signals, which require point-level labels for
every point in the scene. However, such strong supervision suffers from large
annotation costs, arousing the need to study efficient annotating. In this
paper, we discover that the locations of instances matter for both instance and
semantic 3D scene segmentation. By fully taking advantage of locations, we
design a weakly-supervised point cloud segmentation method that only requires
clicking on one point per instance to indicate its location for annotation.
With over-segmentation for pre-processing, we extend these location annotations
into segments as seg-level labels. We further design a segment grouping network
(SegGroup) to generate point-level pseudo labels under seg-level labels by
hierarchically grouping the unlabeled segments into the relevant nearby labeled
segments, so that existing point-level supervised segmentation models can
directly consume these pseudo labels for training. Experimental results show
that our seg-level supervised method (SegGroup) achieves comparable results
with the fully annotated point-level supervised methods. Moreover, it
outperforms the recent weakly-supervised methods given a fixed annotation
budget. Code is available at https://github.com/AnTao97/SegGroup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Medical Image Registration via Appearance Adjustment Networks. (arXiv:2103.05213v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05213">
<div class="article-summary-box-inner">
<span><p>Deformable image registration is fundamental for many medical image analyses.
A key obstacle for accurate image registration lies in image appearance
variations such as the variations in texture, intensities, and noise. These
variations are readily apparent in medical images, especially in brain images
where registration is frequently used. Recently, deep learning-based
registration methods (DLRs), using deep neural networks, have shown
computational efficiency that is several orders of magnitude faster than
traditional optimization-based registration methods (ORs). DLRs rely on a
globally optimized network that is trained with a set of training samples to
achieve faster registration. DLRs tend, however, to disregard the
target-pair-specific optimization inherent in ORs and thus have degraded
adaptability to variations in testing samples. This limitation is severe for
registering medical images with large appearance variations, especially since
few existing DLRs explicitly take into account appearance variations. In this
study, we propose an Appearance Adjustment Network (AAN) to enhance the
adaptability of DLRs to appearance variations. Our AAN, when integrated into a
DLR, provides appearance transformations to reduce the appearance variations
during registration. In addition, we propose an anatomy-constrained loss
function through which our AAN generates anatomy-preserving transformations.
Our AAN has been purposely designed to be readily inserted into a wide range of
DLRs and can be trained cooperatively in an unsupervised and end-to-end manner.
We evaluated our AAN with three state-of-the-art DLRs on three well-established
public datasets of 3D brain magnetic resonance imaging (MRI). The results show
that our AAN consistently improved existing DLRs and outperformed
state-of-the-art ORs on registration accuracy, while adding a fractional
computational load to existing DLRs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of 5-year Progression-Free Survival in Advanced Nasopharyngeal Carcinoma with Pretreatment PET/CT using Multi-Modality Deep Learning-based Radiomics. (arXiv:2103.05220v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05220">
<div class="article-summary-box-inner">
<span><p>Objective: Deep Learning-based Radiomics (DLR) has achieved great success in
medical image analysis and has been considered a replacement for conventional
radiomics that relies on handcrafted features. In this study, we aimed to
explore the capability of DLR for the prediction of 5-year Progression-Free
Survival (PFS) in Nasopharyngeal Carcinoma (NPC) using pretreatment PET/CT.
Methods: A total of 257 patients (170/87 in internal/external cohorts) with
advanced NPC (TNM stage III or IVa) were enrolled. We developed an end-to-end
multi-modality DLR model, in which a 3D convolutional neural network was
optimized to extract deep features from pretreatment PET/CT images and predict
the probability of 5-year PFS. TNM stage, as a high-level clinical feature,
could be integrated into our DLR model to further improve the prognostic
performance. To compare conventional radiomics and DLR, 1456 handcrafted
features were extracted, and optimal conventional radiomics methods were
selected from 54 cross-combinations of 6 feature selection methods and 9
classification methods. In addition, risk group stratification was performed
with clinical signature, conventional radiomics signature, and DLR signature.
Results: Our multi-modality DLR model using both PET and CT achieved higher
prognostic performance than the optimal conventional radiomics method.
Furthermore, the multi-modality DLR model outperformed single-modality DLR
models using only PET or only CT. For risk group stratification, the
conventional radiomics signature and DLR signature enabled significant
differences between the high- and low-risk patient groups in both internal and
external cohorts, while the clinical signature failed in the external cohort.
Conclusion: Our study identified potential prognostic tools for survival
prediction in advanced NPC, suggesting that DLR could provide complementary
values to the current TNM staging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual Reality. (arXiv:2104.04794v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04794">
<div class="article-summary-box-inner">
<span><p>Social presence, the feeling of being there with a real person, will fuel the
next generation of communication systems driven by digital humans in virtual
reality (VR). The best 3D video-realistic VR avatars that minimize the uncanny
effect rely on person-specific (PS) models. However, these PS models are
time-consuming to build and are typically trained with limited data
variability, which results in poor generalization and robustness. Major sources
of variability that affects the accuracy of facial expression transfer
algorithms include using different VR headsets (e.g., camera configuration,
slop of the headset), facial appearance changes over time (e.g., beard,
make-up), and environmental factors (e.g., lighting, backgrounds). This is a
major drawback for the scalability of these models in VR. This paper makes
progress in overcoming these limitations by proposing an end-to-end
multi-identity architecture (MIA) trained with specialized augmentation
strategies. MIA drives the shape component of the avatar from three cameras in
the VR headset (two eyes, one mouth), in untrained subjects, using minimal
personalized information (i.e., neutral 3D mesh shape). Similarly, if the PS
texture decoder is available, MIA is able to drive the full avatar
(shape+texture) robustly outperforming PS models in challenging scenarios. Our
key contribution to improve robustness and generalization, is that our method
implicitly decouples, in an unsupervised manner, the facial expression from
nuisance factors (e.g., headset, environment, facial appearance). We
demonstrate the superior performance and robustness of the proposed method
versus state-of-the-art PS approaches in a variety of experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09124">
<div class="article-summary-box-inner">
<span><p>While self-supervised representation learning (SSL) has received widespread
attention from the community, recent research argue that its performance will
suffer a cliff fall when the model size decreases. The current method mainly
relies on contrastive learning to train the network and in this work, we
propose a simple yet effective Distilled Contrastive Learning (DisCo) to ease
the issue by a large margin. Specifically, we find the final embedding obtained
by the mainstream SSL methods contains the most fruitful information, and
propose to distill the final embedding to maximally transmit a teacher's
knowledge to a lightweight model by constraining the last embedding of the
student to be consistent with that of the teacher. In addition, in the
experiment, we find that there exists a phenomenon termed Distilling BottleNeck
and present to enlarge the embedding dimension to alleviate this problem. Our
method does not introduce any extra parameter to lightweight models during
deployment. Experimental results demonstrate that our method achieves the
state-of-the-art on all lightweight models. Particularly, when
ResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear
result of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,
but the number of parameters of EfficientNet-B0 is only 9.4\%/16.3\% of
ResNet-101/ResNet-50. Code is available at https://github.
com/Yuting-Gao/DisCo-pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Topology In Pancreatic Tubular Networks From Live Imaging 3D Microscopy. (arXiv:2105.09737v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09737">
<div class="article-summary-box-inner">
<span><p>Motivated by the challenging segmentation task of pancreatic tubular
networks, this paper tackles two commonly encountered problems in biomedical
imaging: Topological consistency of the segmentation, and expensive or
difficult annotation. Our contributions are the following: a) We propose a
topological score which measures both topological and geometric consistency
between the predicted and ground truth segmentations, applied to model
selection and validation. b) We provide a full deep-learning methodology for
this difficult noisy task on time-series image data. In our method, we first
use a semisupervised U-net architecture, applicable to generic segmentation
tasks, which jointly trains an autoencoder and a segmentation network. We then
use tracking of loops over time to further improve the predicted topology. This
semi-supervised approach allows us to utilize unannotated data to learn feature
representations that generalize to test data with high variability, in spite of
our annotated training data having very limited variation. Our contributions
are validated on a challenging segmentation task, locating tubular structures
in the fetal pancreas from noisy live imaging confocal microscopy. We show that
our semi-supervised model outperforms not only fully supervised and pre-trained
models but also an approach which takes topological consistency into account
during training. Further, our approach achieves a mean loop score of 0.808 for
detecting loops in the fetal pancreas, compared to a U-net trained with clDice
with mean loop score 0.762.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I Don't Need u: Identifiable Non-Linear ICA Without Side Information. (arXiv:2106.05238v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05238">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the algorithmic stability of unsupervised
representation learning with deep generative models, as a function of repeated
re-training on the same input data. Algorithms for learning low dimensional
linear representations -- for example principal components analysis (PCA), or
linear independent components analysis (ICA) -- come with guarantees that they
will always reveal the same latent representations (perhaps up to an arbitrary
rotation or permutation). Unfortunately, for non-linear representation
learning, such as in a variational auto-encoder (VAE) model trained by
stochastic gradient descent, we have no such guarantees. Recent work on
identifiability in non-linear ICA have introduced a family of deep generative
models that have identifiable latent representations, achieved by conditioning
on side information (e.g. informative labels). We empirically evaluate the
stability of these models under repeated re-estimation of parameters, and
compare them to both standard VAEs and deep generative models which learn to
cluster in their latent space. Surprisingly, we discover side information is
not necessary for algorithmic stability: using standard quantitative measures
of identifiability, we find deep generative models with latent clusterings are
empirically identifiable to the same degree as models which rely on auxiliary
labels. We relate these results to the possibility of identifiable non-linear
ICA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold. (arXiv:2106.05965v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05965">
<div class="article-summary-box-inner">
<span><p>Single image pose estimation is a fundamental problem in many vision and
robotics tasks, and existing deep learning approaches suffer by not completely
modeling and handling: i) uncertainty about the predictions, and ii) symmetric
objects with multiple (sometimes infinite) correct poses. To this end, we
introduce a method to estimate arbitrary, non-parametric distributions on
SO(3). Our key idea is to represent the distributions implicitly, with a neural
network that estimates the probability given the input image and a candidate
pose. Grid sampling or gradient ascent can be used to find the most likely
pose, but it is also possible to evaluate the probability at any pose, enabling
reasoning about symmetries and uncertainty. This is the most general way of
representing distributions on manifolds, and to showcase the rich expressive
power, we introduce a dataset of challenging symmetric and nearly-symmetric
objects. We require no supervision on pose uncertainty -- the model trains only
with a single pose per example. Nonetheless, our implicit model is highly
expressive to handle complex distributions over 3D poses, while still obtaining
accurate pose estimation on standard non-ambiguous environments, achieving
state-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-Cycle Pruning: Pruning ConvNets Under a Tight Training Budget. (arXiv:2107.02086v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02086">
<div class="article-summary-box-inner">
<span><p>Introducing sparsity in a neural network has been an efficient way to reduce
its complexity while keeping its performance almost intact. Most of the time,
sparsity is introduced using a three-stage pipeline: 1) train the model to
convergence, 2) prune the model according to some criterion, 3) fine-tune the
pruned model to recover performance. The last two steps are often performed
iteratively, leading to reasonable results but also to a time-consuming and
complex process. In our work, we propose to get rid of the first step of the
pipeline and to combine the two other steps in a single pruning-training cycle,
allowing the model to jointly learn for the optimal weights while being pruned.
We do this by introducing a novel pruning schedule, named One-Cycle Pruning,
which starts pruning from the beginning of the training, and until its very
end. Adopting such a schedule not only leads to better performing pruned models
but also drastically reduces the training budget required to prune a model.
Experiments are conducted on a variety of architectures (VGG-16 and ResNet-18)
and datasets (CIFAR-10, CIFAR-100 and Caltech-101), and for relatively high
sparsity values (80%, 90%, 95% of weights removed). Our results show that
One-Cycle Pruning consistently outperforms commonly used pruning schedules such
as One-Shot Pruning, Iterative Pruning and Automated Gradual Pruning, on a
fixed training budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: Lighter and Faster Deep Neural Architecture for Tomato Leaf Disease Classification. (arXiv:2109.02394v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02394">
<div class="article-summary-box-inner">
<span><p>To ensure global food security and the overall profit of stakeholders, the
importance of correctly detecting and classifying plant diseases is paramount.
In this connection, the emergence of deep learning-based image classification
has introduced a substantial number of solutions. However, the applicability of
these solutions in low-end devices requires fast, accurate, and computationally
inexpensive systems. This work proposes a lightweight transfer learning-based
approach for detecting diseases from tomato leaves. It utilizes an effective
preprocessing method to enhance the leaf images with illumination correction
for improved classification. Our system extracts features using a combined
model consisting of a pretrained MobileNetV2 architecture and a classifier
network for effective prediction. Traditional augmentation approaches are
replaced by runtime augmentation to avoid data leakage and address the class
imbalance issue. Evaluation on tomato leaf images from the PlantVillage dataset
shows that the proposed architecture achieves 99.30% accuracy with a model size
of 9.60MB and 4.87M floating-point operations, making it a suitable choice for
real-life applications in low-end devices. Our codes and models are available
at https://github.com/redwankarimsony/project-tomato.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Traffic-Net: 3D Traffic Monitoring Using a Single Camera. (arXiv:2109.09165v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09165">
<div class="article-summary-box-inner">
<span><p>Computer Vision has played a major role in Intelligent Transportation Systems
(ITS) and traffic surveillance. Along with the rapidly growing automated
vehicles and crowded cities, the automated and advanced traffic management
systems (ATMS) using video surveillance infrastructures have been evolved by
the implementation of Deep Neural Networks. In this research, we provide a
practical platform for real-time traffic monitoring, including 3D
vehicle/pedestrian detection, speed detection, trajectory estimation,
congestion detection, as well as monitoring the interaction of vehicles and
pedestrians, all using a single CCTV traffic camera. We adapt a custom YOLOv5
deep neural network model for vehicle/pedestrian detection and an enhanced SORT
tracking algorithm. For the first time, a hybrid satellite-ground based inverse
perspective mapping (SG-IPM) method for camera auto-calibration is also
developed which leads to an accurate 3D object detection and visualisation. We
also develop a hierarchical traffic modelling solution based on short- and
long-term temporal video data stream to understand the traffic flow,
bottlenecks, and risky spots for vulnerable road users. Several experiments on
real-world scenarios and comparisons with state-of-the-art are conducted using
various traffic monitoring datasets, including MIO-TCD, UA-DETRAC and GRAM-RTM
collected from highways, intersections, and urban areas under different
lighting and weather conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2109.09960v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09960">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel mutual consistency network (MC-Net+) to
effectively exploit the unlabeled data for semi-supervised medical image
segmentation. The MC-Net+ model is motivated by the observation that deep
models trained with limited annotations are prone to output highly uncertain
and easily mis-classified predictions in the ambiguous regions (e.g., adhesive
edges or thin branches) for medical image segmentation. Leveraging these
challenging samples can make the semi-supervised segmentation model training
more effective. Therefore, our proposed MC-Net+ model consists of two new
designs. First, the model contains one shared encoder and multiple slightly
different decoders (i.e., using different up-sampling strategies). The
statistical discrepancy of multiple decoders' outputs is computed to denote the
model's uncertainty, which indicates the unlabeled hard regions. Second, we
apply a novel mutual consistency constraint between one decoder's probability
output and other decoders' soft pseudo labels. In this way, we minimize the
discrepancy of multiple outputs (i.e., the model uncertainty) during training
and force the model to generate invariant results in such challenging regions,
aiming at regularizing the model training. We compared the segmentation results
of our MC-Net+ model with five state-of-the-art semi-supervised approaches on
three public medical datasets. Extension experiments with two standard
semi-supervised settings demonstrate the superior performance of our model over
other methods, which sets a new state of the art for semi-supervised medical
image segmentation. Our code is released publicly at
https://github.com/ycwu1997/MC-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Stacking Ensemble Approach for Supervised Video Summarization. (arXiv:2109.12581v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12581">
<div class="article-summary-box-inner">
<span><p>Video summarization methods are usually classified into shot-level or
frame-level methods, which are individually used in a general way. This paper
investigates the underlying complementarity between the frame-level and
shot-level methods, and a stacking ensemble approach is proposed for supervised
video summarization. Firstly, we build up a stacking model to predict both the
key frame probabilities and the temporal interest segments simultaneously. The
two components are then combined via soft decision fusion to obtain the final
scores of each frame in the video. A joint loss function is proposed for the
model training. The ablation experimental results show that the proposed method
outperforms both the two corresponding individual method. Furthermore,
extensive experimental results on two benchmark datasets shows its superior
performance in comparison with the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Multi-view and Temporal Fusing Transformer for 3D Human Pose Estimation. (arXiv:2110.05092v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05092">
<div class="article-summary-box-inner">
<span><p>This paper proposes a unified framework dubbed Multi-view and Temporal Fusing
Transformer (MTF-Transformer) to adaptively handle varying view numbers and
video length without camera calibration in 3D Human Pose Estimation (HPE). It
consists of Feature Extractor, Multi-view Fusing Transformer (MFT), and
Temporal Fusing Transformer (TFT). Feature Extractor estimates 2D pose from
each image and fuses the prediction according to the confidence. It provides
pose-focused feature embedding and makes subsequent modules computationally
lightweight. MFT fuses the features of a varying number of views with a novel
Relative-Attention block. It adaptively measures the implicit relative
relationship between each pair of views and reconstructs more informative
features. TFT aggregates the features of the whole sequence and predicts 3D
pose via a transformer. It adaptively deals with the video of arbitrary length
and fully unitizes the temporal information. The migration of transformers
enables our model to learn spatial geometry better and preserve robustness for
varying application scenarios. We report quantitative and qualitative results
on the Human3.6M, TotalCapture, and KTH Multiview Football II. Compared with
state-of-the-art methods with camera parameters, MTF-Transformer obtains
competitive results and generalizes well to dynamic capture with an arbitrary
number of unseen views.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lidar with Velocity: Correcting Moving Objects Point Cloud Distortion from Oscillating Scanning Lidars by Fusion with Camera. (arXiv:2111.09497v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09497">
<div class="article-summary-box-inner">
<span><p>Lidar point cloud distortion from moving object is an important problem in
autonomous driving, and recently becomes even more demanding with the emerging
of newer lidars, which feature back-and-forth scanning patterns. Accurately
estimating moving object velocity would not only provide a tracking capability
but also correct the point cloud distortion with more accurate description of
the moving object. Since lidar measures the time-of-flight distance but with a
sparse angular resolution, the measurement is precise in the radial measurement
but lacks angularly. Camera on the other hand provides a dense angular
resolution. In this paper, Gaussian-based lidar and camera fusion is proposed
to estimate the full velocity and correct the lidar distortion. A probabilistic
Kalman-filter framework is provided to track the moving objects, estimate their
velocities and simultaneously correct the point clouds distortions. The
framework is evaluated on real road data and the fusion method outperforms the
traditional ICP-based and point-cloud only method. The complete working
framework is open-sourced
(https://github.com/ISEE-Technology/lidar-with-velocity) to accelerate the
adoption of the emerging lidars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaFormer Is Actually What You Need for Vision. (arXiv:2111.11418v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11418">
<div class="article-summary-box-inner">
<span><p>Transformers have shown great potential in computer vision tasks. A common
belief is their attention-based token mixer module contributes most to their
competence. However, recent works show the attention-based module in
Transformers can be replaced by spatial MLPs and the resulted models still
perform quite well. Based on this observation, we hypothesize that the general
architecture of the Transformers, instead of the specific token mixer module,
is more essential to the model's performance. To verify this, we deliberately
replace the attention module in Transformers with an embarrassingly simple
spatial pooling operator to conduct only basic token mixing. Surprisingly, we
observe that the derived model, termed as PoolFormer, achieves competitive
performance on multiple computer vision tasks. For example, on ImageNet-1K,
PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned Vision
Transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with
35%/52% fewer parameters and 50%/62% fewer MACs. The effectiveness of
PoolFormer verifies our hypothesis and urges us to initiate the concept of
"MetaFormer", a general architecture abstracted from Transformers without
specifying the token mixer. Based on the extensive experiments, we argue that
MetaFormer is the key player in achieving superior results for recent
Transformer and MLP-like models on vision tasks. This work calls for more
future research dedicated to improving MetaFormer instead of focusing on the
token mixer modules. Additionally, our proposed PoolFormer could serve as a
starting baseline for future MetaFormer architecture design. Code is available
at https://github.com/sail-sg/poolformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Adaptation for Implicit Object Tracking and Shape Reconstruction in the Wild. (arXiv:2111.12728v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12728">
<div class="article-summary-box-inner">
<span><p>Tracking and reconstructing 3D objects from cluttered scenes are the key
components for computer vision, robotics and autonomous driving systems. While
recent progress in implicit function has shown encouraging results on
high-quality 3D shape reconstruction, it is still very challenging to
generalize to cluttered and partially observable LiDAR data. In this paper, we
propose to leverage the continuity in video data. We introduce a novel and
unified framework which utilizes a neural implicit function to simultaneously
track and reconstruct 3D objects in the wild. Our approach adapts the DeepSDF
model (i.e., an instantiation of the implicit function) in the video online,
iteratively improving the shape reconstruction while in return improving the
tracking, and vice versa. We experiment with both Waymo and KITTI datasets and
show significant improvements over state-of-the-art methods for both tracking
and shape reconstruction tasks. Our project page is at
https://jianglongye.com/implicit-tracking .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACNet: Approaching-and-Centralizing Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2111.12757v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12757">
<div class="article-summary-box-inner">
<span><p>The huge domain gap between sketches and photos and the highly abstract
sketch representations pose challenges for sketch-based image retrieval
(\underline{SBIR}). The zero-shot sketch-based image retrieval
(\underline{ZS-SBIR}) is more generic and practical but poses an even greater
challenge because of the additional knowledge gap between the seen and unseen
categories. To simultaneously mitigate both gaps, we propose an
\textbf{A}pproaching-and-\textbf{C}entralizing \textbf{Net}work (termed
"\textbf{ACNet}") to jointly optimize sketch-to-photo synthesis and the image
retrieval. The retrieval module guides the synthesis module to generate large
amounts of diverse photo-like images which gradually approach the photo domain,
and thus better serve the retrieval module than ever to learn domain-agnostic
representations and category-agnostic common knowledge for generalizing to
unseen categories. These diverse images generated with retrieval guidance can
effectively alleviate the overfitting problem troubling concrete
category-specific training samples with high gradients. We also discover the
use of proxy-based NormSoftmax loss is effective in the zero-shot setting
because its centralizing effect can stabilize our joint training and promote
the generalization ability to unseen categories. Our approach is simple yet
effective, which achieves state-of-the-art performance on two widely used
ZS-SBIR datasets and surpasses previous methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Channel Encoding Transformer for Point Cloud Analysis. (arXiv:2112.02507v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02507">
<div class="article-summary-box-inner">
<span><p>Transformer plays an increasingly important role in various computer vision
areas and remarkable achievements have also been made in point cloud analysis.
Since they mainly focus on point-wise transformer, an adaptive channel encoding
transformer is proposed in this paper. Specifically, a channel convolution
called Transformer-Conv is designed to encode the channel. It can encode
feature channels by capturing the potential relationship between coordinates
and features. Compared with simply assigning attention weight to each channel,
our method aims to encode the channel adaptively. In addition, our network
adopts the neighborhood search method of low-level and high-level dual semantic
receptive fields to improve the performance. Extensive experiments show that
our method is superior to state-of-the-art point cloud classification and
segmentation methods on three benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions. (arXiv:2112.08088v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08088">
<div class="article-summary-box-inner">
<span><p>Though deep learning-based object detection methods have achieved promising
results on the conventional datasets, it is still challenging to locate objects
from the low-quality images captured in adverse weather conditions. The
existing methods either have difficulties in balancing the tasks of image
enhancement and object detection, or often ignore the latent information
beneficial for detection. To alleviate this problem, we propose a novel
Image-Adaptive YOLO (IA-YOLO) framework, where each image can be adaptively
enhanced for better detection performance. Specifically, a differentiable image
processing (DIP) module is presented to take into account the adverse weather
conditions for YOLO detector, whose parameters are predicted by a small
convolutional neural net-work (CNN-PP). We learn CNN-PP and YOLOv3 jointly in
an end-to-end fashion, which ensures that CNN-PP can learn an appropriate DIP
to enhance the image for detection in a weakly supervised manner. Our proposed
IA-YOLO approach can adaptively process images in both normal and adverse
weather conditions. The experimental results are very encouraging,
demonstrating the effectiveness of our proposed IA-YOLO method in both foggy
and low-light scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of a face mask detection pipeline for mask-wearing monitoring in the era of the COVID-19 pandemic: A modular approach. (arXiv:2112.15031v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15031">
<div class="article-summary-box-inner">
<span><p>During the SARS-Cov-2 pandemic, mask-wearing became an effective tool to
prevent spreading and contracting the virus. The ability to monitor the
mask-wearing rate in the population would be useful for determining public
health strategies against the virus. However, artificial intelligence
technologies for detecting face masks have not been deployed at a large scale
in real-life to measure the mask-wearing rate in public. In this paper, we
present a two-step face mask detection approach consisting of two separate
modules: 1) face detection and alignment and 2) face mask classification. This
approach allowed us to experiment with different combinations of face detection
and face mask classification modules. More specifically, we experimented with
PyramidKey and RetinaFace as face detectors while maintaining a lightweight
backbone for the face mask classification module. Moreover, we also provide a
relabeled annotation of the test set of the AIZOO dataset, where we rectified
the incorrect labels for some face images. The evaluation results on the AIZOO
and Moxa 3K datasets showed that the proposed face mask detection pipeline
surpassed the state-of-the-art methods. The proposed pipeline also yielded a
higher mAP on the relabeled test set of the AIZOO dataset than the original
test set. Since we trained the proposed model using in-the-wild face images, we
can successfully deploy our model to monitor the mask-wearing rate using public
CCTV images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis. (arXiv:2201.03969v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03969">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis (MSA) is a fundamental complex research problem
due to the heterogeneity gap between different modalities and the ambiguity of
human emotional expression. Although there have been many successful attempts
to construct multimodal representations for MSA, there are still two challenges
to be addressed: 1) A more robust multimodal representation needs to be
constructed to bridge the heterogeneity gap and cope with the complex
multimodal interactions, and 2) the contextual dynamics must be modeled
effectively throughout the information flow. In this work, we propose a
multimodal representation model based on Mutual information Maximization and
Minimization and Identity Embedding (MMMIE). We combine mutual information
maximization between modal pairs, and mutual information minimization between
input data and corresponding features to mine the modal-invariant and
task-related information. Furthermore, Identity Embedding is proposed to prompt
the downstream network to perceive the contextual information. Experimental
results on two public datasets demonstrate the effectiveness of the proposed
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrated Multiscale Domain Adaptive YOLO. (arXiv:2202.03527v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03527">
<div class="article-summary-box-inner">
<span><p>The area of domain adaptation has been instrumental in addressing the domain
shift problem encountered by many applications. This problem arises due to the
difference between the distributions of source data used for training in
comparison with target data used during realistic testing scenarios. In this
paper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)
framework that employs multiple domain adaptation paths and corresponding
domain classifiers at different scales of the recently introduced YOLOv4 object
detector. Building on our baseline multiscale DAYOLO framework, we introduce
three novel deep learning architectures for a Domain Adaptation Network (DAN)
that generates domain-invariant features. In particular, we propose a
Progressive Feature Reduction (PFR), a Unified Classifier (UC), and an
Integrated architecture. We train and test our proposed DAN architectures in
conjunction with YOLOv4 using popular datasets. Our experiments show
significant improvements in object detection performance when training YOLOv4
using the proposed MS-DAYOLO architectures and when tested on target data for
autonomous driving applications. Moreover, MS-DAYOLO framework achieves an
order of magnitude real-time speed improvement relative to Faster R-CNN
solutions while providing comparable object detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attack and Defense of YOLO Detectors in Autonomous Driving Scenarios. (arXiv:2202.04781v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04781">
<div class="article-summary-box-inner">
<span><p>Visual detection is a key task in autonomous driving, and it serves as a
crucial foundation for self-driving planning and control. Deep neural networks
have achieved promising results in various visual tasks, but they are known to
be vulnerable to adversarial attacks. A comprehensive understanding of deep
visual detectors' vulnerability is required before people can improve their
robustness. However, only a few adversarial attack/defense works have focused
on object detection, and most of them employed only classification and/or
localization losses, ignoring the objectness aspect. In this paper, we identify
a serious objectness-related adversarial vulnerability in YOLO detectors and
present an effective attack strategy targeting the objectness aspect of visual
detection in autonomous vehicles. Furthermore, to address such vulnerability,
we propose a new objectness-aware adversarial training approach for visual
detection. Experiments show that the proposed attack targeting the objectness
aspect is 45.17% and 43.50% more effective than those generated from
classification and/or localization losses on the KITTI and COCO traffic
datasets, respectively. Also, the proposed adversarial defense approach can
improve the detectors' robustness against objectness-oriented attacks by up to
21% and 12% mAP on KITTI and COCO traffic, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable COVID-19 Infections Identification and Delineation Using Calibrated Pseudo Labels. (arXiv:2202.07422v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07422">
<div class="article-summary-box-inner">
<span><p>The upheaval brought by the arrival of the COVID-19 pandemic has continued to
bring fresh challenges over the past two years. During this COVID-19 pandemic,
there has been a need for rapid identification of infected patients and
specific delineation of infection areas in computed tomography (CT) images.
Although deep supervised learning methods have been established quickly, the
scarcity of both image-level and pixel-level labels as well as the lack of
explainable transparency still hinder the applicability of AI. Can we identify
infected patients and delineate the infections with extreme minimal
supervision? Semi-supervised learning has demonstrated promising performance
under limited labelled data and sufficient unlabelled data. Inspired by
semi-supervised learning, we propose a model-agnostic calibrated
pseudo-labelling strategy and apply it under a consistency regularization
framework to generate explainable identification and delineation results. We
demonstrate the effectiveness of our model with the combination of limited
labelled data and sufficient unlabelled data or weakly-labelled data. Extensive
experiments have shown that our model can efficiently utilize limited labelled
data and provide explainable classification and segmentation results for
decision-making in clinical routine. The code is available at
https://github.com/ayanglab/XAI COVID-19.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-World Blind Super-Resolution via Feature Matching with Implicit High-Resolution Priors. (arXiv:2202.13142v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13142">
<div class="article-summary-box-inner">
<span><p>A key challenge of real-world image super-resolution (SR) is to recover the
missing details in low-resolution (LR) images with complex unknown degradations
(e.g., downsampling, noise and compression). Most previous works restore such
missing details in the image space. To cope with the high diversity of natural
images, they either rely on the unstable GANs that are difficult to train and
prone to artifacts, or resort to explicit references from high-resolution (HR)
images that are usually unavailable. In this work, we propose Feature Matching
SR (FeMaSR), which restores realistic HR images in a much more compact feature
space. Unlike image-space methods, our FeMaSR restores HR images by matching
distorted LR image {\it features} to their distortion-free HR counterparts in
our pretrained HR priors, and decoding the matched features to obtain realistic
HR images. Specifically, our HR priors contain a discrete feature codebook and
its associated decoder, which are pretrained on HR images with a Vector
Quantized Generative Adversarial Network (VQGAN). Notably, we incorporate a
novel semantic regularization in VQGAN to improve the quality of reconstructed
images. For the feature matching, we first extract LR features with an LR
encoder consisting of several Swin Transformer blocks and then follow a simple
nearest neighbour strategy to match them with the pretrained codebook. In
particular, we equip the LR encoder with residual shortcut connections to the
decoder, which is critical to the optimization of feature matching loss and
also helps to complement the possible feature matching errors. Experimental
results show that our approach produces more realistic HR images than previous
methods. Codes are released at \url{https://github.com/chaofengc/FeMaSR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Scene Flow Estimation with 4-D Automotive Radar. (arXiv:2203.01137v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01137">
<div class="article-summary-box-inner">
<span><p>Scene flow allows autonomous vehicles to reason about the arbitrary motion of
multiple independent objects which is the key to long-term mobile autonomy.
While estimating the scene flow from LiDAR has progressed recently, it remains
largely unknown how to estimate the scene flow from a 4-D radar - an
increasingly popular automotive sensor for its robustness against adverse
weather and lighting conditions. Compared with the LiDAR point clouds, radar
data are drastically sparser, noisier and in much lower resolution. Annotated
datasets for radar scene flow are also in absence and costly to acquire in the
real world. These factors jointly pose the radar scene flow estimation as a
challenging problem. This work aims to address the above challenges and
estimate scene flow from 4-D radar point clouds by leveraging self-supervised
learning. A robust scene flow estimation architecture and three novel losses
are bespoken designed to cope with intractable radar data. Real-world
experimental results validate that our method is able to robustly estimate the
radar scene flow in the wild and effectively supports the downstream task of
motion segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks. (arXiv:2203.03844v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03844">
<div class="article-summary-box-inner">
<span><p>Light-weight super-resolution (SR) models have received considerable
attention for their serviceability in mobile devices. Many efforts employ
network quantization to compress SR models. However, these methods suffer from
severe performance degradation when quantizing the SR models to ultra-low
precision (e.g., 2-bit and 3-bit) with the low-cost layer-wise quantizer. In
this paper, we identify that the performance drop comes from the contradiction
between the layer-wise symmetric quantizer and the highly asymmetric activation
distribution in SR models. This discrepancy leads to either a waste on the
quantization levels or detail loss in reconstructed images. Therefore, we
propose a novel activation quantizer, referred to as Dynamic Dual Trainable
Bounds (DDTB), to accommodate the asymmetry of the activations. Specifically,
DDTB innovates in: 1) A layer-wise quantizer with trainable upper and lower
bounds to tackle the highly asymmetric activations. 2) A dynamic gate
controller to adaptively adjust the upper and lower bounds at runtime to
overcome the drastically varying activation ranges over different samples.To
reduce the extra overhead, the dynamic gate controller is quantized to 2-bit
and applied to only part of the SR networks according to the introduced dynamic
intensity. Extensive experiments demonstrate that our DDTB exhibits significant
performance improvements in ultra-low precision. For example, our DDTB achieves
a 0.70dB PSNR increase on Urban100 benchmark when quantizing EDSR to 2-bit and
scaling up output images to x4. Code is at
\url{https://github.com/zysxmu/DDTB}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Temporal Consistency for Source-Free Video Domain Adaptation. (arXiv:2203.04559v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04559">
<div class="article-summary-box-inner">
<span><p>Video-based Unsupervised Domain Adaptation (VUDA) methods improve the
robustness of video models, enabling them to be applied to action recognition
tasks across different environments. However, these methods require constant
access to source data during the adaptation process. Yet in many real-world
applications, subjects and scenes in the source video domain should be
irrelevant to those in the target video domain. With the increasing emphasis on
data privacy, such methods that require source data access would raise serious
privacy issues. Therefore, to cope with such concern, a more practical domain
adaptation scenario is formulated as the Source-Free Video-based Domain
Adaptation (SFVDA). Though there are a few methods for Source-Free Domain
Adaptation (SFDA) on image data, these methods yield degenerating performance
in SFVDA due to the multi-modality nature of videos, with the existence of
additional temporal features. In this paper, we propose a novel Attentive
Temporal Consistent Network (ATCoN) to address SFVDA by learning temporal
consistency, guaranteed by two novel consistency objectives, namely feature
consistency and source prediction consistency, performed across local temporal
features. ATCoN further constructs effective overall temporal features by
attending to local temporal features based on prediction confidence. Empirical
results demonstrate the state-of-the-art performance of ATCoN across various
cross-domain action recognition benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping global dynamics of benchmark creation and saturation in artificial intelligence. (arXiv:2203.04592v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04592">
<div class="article-summary-box-inner">
<span><p>Benchmarks are crucial to measuring and steering progress in artificial
intelligence (AI). However, recent studies raised concerns over the state of AI
benchmarking, reporting issues such as benchmark overfitting, benchmark
saturation and increasing centralization of benchmark dataset creation. To
facilitate monitoring of the health of the AI benchmarking ecosystem, we
introduce methodologies for creating condensed maps of the global dynamics of
benchmark creation and saturation. We curated data for 1688 benchmarks covering
the entire domains of computer vision and natural language processing, and show
that a large fraction of benchmarks quickly trended towards near-saturation,
that many benchmarks fail to find widespread utilization, and that benchmark
performance gains for different AI tasks were prone to unforeseen bursts. We
analyze attributes associated with benchmark popularity, and conclude that
future benchmarks should emphasize versatility, breadth and real-world utility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ModDrop++: A Dynamic Filter Network with Intra-subject Co-training for Multiple Sclerosis Lesion Segmentation with Missing Modalities. (arXiv:2203.04959v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04959">
<div class="article-summary-box-inner">
<span><p>Multiple Sclerosis (MS) is a chronic neuroinflammatory disease and
multi-modality MRIs are routinely used to monitor MS lesions. Many automatic MS
lesion segmentation models have been developed and have reached human-level
performance. However, most established methods assume the MRI modalities used
during training are also available during testing, which is not guaranteed in
clinical practice. Previously, a training strategy termed Modality Dropout
(ModDrop) has been applied to MS lesion segmentation to achieve the
state-of-the-art performance with missing modality. In this paper, we present a
novel method dubbed ModDrop++ to train a unified network adaptive to an
arbitrary number of input MRI sequences. ModDrop++ upgrades the main idea of
ModDrop in two key ways. First, we devise a plug-and-play dynamic head and
adopt a filter scaling strategy to improve the expressiveness of the network.
Second, we design a co-training strategy to leverage the intra-subject relation
between full modality and missing modality. Specifically, the intra-subject
co-training strategy aims to guide the dynamic head to generate similar feature
representations between the full- and missing-modality data from the same
subject. We use two public MS datasets to show the superiority of ModDrop++.
Source code and trained models are available at
https://github.com/han-liu/ModDropPlusPlus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05482">
<div class="article-summary-box-inner">
<span><p>The conventional recipe for maximizing model accuracy is to (1) train
multiple models with various hyperparameters and (2) pick the individual model
which performs best on a held-out validation set, discarding the remainder. In
this paper, we revisit the second step of this procedure in the context of
fine-tuning large pre-trained models, where fine-tuned models often appear to
lie in a single low error basin. We show that averaging the weights of multiple
models fine-tuned with different hyperparameter configurations often improves
accuracy and robustness. Unlike a conventional ensemble, we may average many
models without incurring any additional inference or memory costs -- we call
the results "model soups." When fine-tuning large pre-trained models such as
CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides
significant improvements over the best model in a hyperparameter sweep on
ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on
ImageNet, achieved a new state of the art. Furthermore, we show that the model
soup approach extends to multiple image classification and natural language
processing tasks, improves out-of-distribution performance, and improves
zero-shot performance on new downstream tasks. Finally, we analytically relate
the performance similarity of weight-averaging and logit-ensembling to flatness
of the loss and confidence of the predictions, and validate this relation
empirically. Code is available at https://github.com/mlfoundations/model-soups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion. (arXiv:2203.09780v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09780">
<div class="article-summary-box-inner">
<span><p>Current LiDAR-only 3D detection methods inevitably suffer from the sparsity
of point clouds. Many multi-modal methods are proposed to alleviate this issue,
while different representations of images and point clouds make it difficult to
fuse them, resulting in suboptimal performance. In this paper, we present a
novel multi-modal framework SFD (Sparse Fuse Dense), which utilizes pseudo
point clouds generated from depth completion to tackle the issues mentioned
above. Different from prior works, we propose a new RoI fusion strategy 3D-GAF
(3D Grid-wise Attentive Fusion) to make fuller use of information from
different types of point clouds. Specifically, 3D-GAF fuses 3D RoI features
from the couple of point clouds in a grid-wise attentive way, which is more
fine-grained and more precise. In addition, we propose a SynAugment
(Synchronized Augmentation) to enable our multi-modal framework to utilize all
data augmentation approaches tailored to LiDAR-only methods. Lastly, we
customize an effective and efficient feature extractor CPConv (Color Point
Convolution) for pseudo point clouds. It can explore 2D image features and 3D
geometric features of pseudo point clouds simultaneously. Our method holds the
highest entry on the KITTI car 3D object detection leaderboard, demonstrating
the effectiveness of our SFD. Codes are available at
https://github.com/LittlePey/SFD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Point Cloud Representation Learning with Occlusion Auto-Encoder. (arXiv:2203.14084v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14084">
<div class="article-summary-box-inner">
<span><p>Learning representations for point clouds is an important task in 3D computer
vision, especially without manually annotated supervision. Previous methods
usually take the common aid from auto-encoders to establish the
self-supervision by reconstructing the input itself. However, the existing
self-reconstruction based auto-encoders merely focus on the global shapes, and
ignore the hierarchical context between the local and global geometries, which
is a crucial supervision for 3D representation learning. To resolve this issue,
we present a novel self-supervised point cloud representation learning
framework, named 3D Occlusion Auto-Encoder (3D-OAE). Our key idea is to
randomly occlude some local patches of the input point cloud and establish the
supervision via recovering the occluded patches using the remaining visible
ones. Specifically, we design an encoder for learning the features of visible
local patches, and a decoder for leveraging these features to predict the
occluded patches. In contrast with previous methods, our 3D-OAE can remove a
large proportion of patches and predict them only with a small number of
visible patches, which enable us to significantly accelerate training and yield
a nontrivial self-supervisory performance. The trained encoder can be further
transferred to various downstream tasks. We demonstrate our superior
performances over the state-of-the-art methods in different discriminant and
generative applications under widely used benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Polyp Segmentation: A Deep Learning Perspective. (arXiv:2203.14291v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14291">
<div class="article-summary-box-inner">
<span><p>We present the first comprehensive video polyp segmentation (VPS) study in
the deep learning era. Over the years, developments in VPS are not moving
forward with ease due to the lack of large-scale fine-grained segmentation
annotations. To address this issue, we first introduce a high-quality
frame-by-frame annotated VPS dataset, named SUN-SEG, which contains 158,690
frames from the well-known SUN-database. We provide additional annotations with
diverse types, i.e., attribute, object mask, boundary, scribble, and polygon.
Second, we design a simple but efficient baseline, dubbed PNS+, consisting of a
global encoder, a local encoder, and normalized self-attention (NS) blocks. The
global and local encoders receive an anchor frame and multiple successive
frames to extract long-term and short-term spatial-temporal representations,
which are then progressively updated by two NS blocks. Extensive experiments
show that PNS+ achieves the best performance and real-time inference speed
(170fps), making it a promising solution for the VPS task. Third, we
extensively evaluate 13 representative polyp/object segmentation models on our
SUN-SEG dataset and provide attribute-based comparisons. Finally, we discuss
several open issues and suggest possible research directions for the VPS
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrating Class Weights with Multi-Modal Information for Partial Video Domain Adaptation. (arXiv:2204.06187v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06187">
<div class="article-summary-box-inner">
<span><p>Assuming the source label space subsumes the target one, Partial Video Domain
Adaptation (PVDA) is a more general and practical scenario for cross-domain
video classification problems. The key challenge of PVDA is to mitigate the
negative transfer caused by the source-only outlier classes. To tackle this
challenge, a crucial step is to aggregate target predictions to assign class
weights by up-weighing target classes and down-weighing outlier classes.
However, the incorrect predictions of class weights can mislead the network and
lead to negative transfer. Previous works improve the class weight accuracy by
utilizing temporal features and attention mechanisms, but these methods may
fall short when trying to generate accurate class weight when domain shifts are
significant, as in most real-world scenarios. To deal with these challenges, we
propose the Multi-modality Cluster-calibrated partial Adversarial Network
(MCAN). MCAN enhances video feature extraction with multi-modal features from
multiple temporal scales to form more robust overall features. It utilizes a
novel class weight calibration method to alleviate the negative transfer caused
by incorrect class weights. The calibration method tries to identify and weigh
correct and incorrect predictions using distributional information implied by
unsupervised clustering. Extensive experiments are conducted on prevailing PVDA
benchmarks, and the proposed MCAN achieves significant improvements when
compared to state-of-the-art PVDA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIFS: Neural Implicit Function for General Shape Representation. (arXiv:2204.07126v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07126">
<div class="article-summary-box-inner">
<span><p>Recent development of neural implicit function has shown tremendous success
on high-quality 3D shape reconstruction. However, most works divide the space
into inside and outside of the shape, which limits their representing power to
single-layer and watertight shapes. This limitation leads to tedious data
processing (converting non-watertight raw data to watertight) as well as the
incapability of representing general object shapes in the real world. In this
work, we propose a novel method to represent general shapes including
non-watertight shapes and shapes with multi-layer surfaces. We introduce
General Implicit Function for 3D Shape (GIFS), which models the relationships
between every two points instead of the relationships between points and
surfaces. Instead of dividing 3D space into predefined inside-outside regions,
GIFS encodes whether two points are separated by any surface. Experiments on
ShapeNet show that GIFS outperforms previous state-of-the-art methods in terms
of reconstruction quality, rendering efficiency, and visual fidelity. Project
page is available at https://jianglongye.com/gifs .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A case for using rotation invariant features in state of the art feature matchers. (arXiv:2204.10144v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10144">
<div class="article-summary-box-inner">
<span><p>The aim of this paper is to demonstrate that a state of the art feature
matcher (LoFTR) can be made more robust to rotations by simply replacing the
backbone CNN with a steerable CNN which is equivariant to translations and
image rotations. It is experimentally shown that this boost is obtained without
reducing performance on ordinary illumination and viewpoint matching sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. (arXiv:2204.10965v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10965">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose CLIP-Dissect, a new technique to automatically
describe the function of individual hidden neurons inside vision networks.
CLIP-Dissect leverages recent advances in multimodal vision/language models to
label internal neurons with open-ended concepts without the need for any
labeled data or human examples, which are required for existing tools to
succeed. We show that CLIP-Dissect provides more accurate descriptions than
existing methods for neurons where the ground-truth is available as well as
qualitatively good descriptions for hidden layer neurons. In addition, our
method is very flexible: it is model agnostic, can easily handle new concepts
and can be extended to take advantage of better multimodal models in the
future. Finally CLIP-Dissect is computationally efficient and labels all
neurons of a layer in a large vision model in tens of minutes.
</p>
<p>In this paper, we propose CLIP-Dissect, a new technique to automatically
describe the function of individual hidden neurons inside vision networks.
CLIP-Dissect leverages recent advances in multimodal vision/language models to
label internal neurons with open-ended concepts without the need for any
labeled data or human examples, which are required for existing tools to
succeed. We show that CLIP-Dissect provides more accurate descriptions than
existing methods for last layer neurons where the ground-truth is available as
well as qualitatively good descriptions for hidden layer neurons. In addition,
our method is very flexible: it is model agnostic, can easily handle new
concepts and can be extended to take advantage of better multimodal models in
the future. Finally CLIP-Dissect is computationally efficient and can label all
neurons from five layers of ResNet-50 in just four minutes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Urban Change Detection Using a Dual-Task Siamese Network and Semi-Supervised Learning. (arXiv:2204.12202v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12202">
<div class="article-summary-box-inner">
<span><p>In this study, a Semi-Supervised Learning (SSL) method for improving urban
change detection from bi-temporal image pairs was presented. The proposed
method adapted a Dual-Task Siamese Difference network that not only predicts
changes with the difference decoder, but also segments buildings for both
images with a semantics decoder. First, the architecture was modified to
produce a second change prediction derived from the semantics predictions.
Second, SSL was adopted to improve supervised change detection. For unlabeled
data, we introduced a loss that encourages the network to predict consistent
changes across the two change outputs. The proposed method was tested on urban
change detection using the SpaceNet7 dataset. SSL achieved improved results
compared to three fully supervised benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Traffic Context Aware Data Augmentation for Rare Object Detection in Autonomous Driving. (arXiv:2205.00376v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00376">
<div class="article-summary-box-inner">
<span><p>Detection of rare objects (e.g., traffic cones, traffic barrels and traffic
warning triangles) is an important perception task to improve the safety of
autonomous driving. Training of such models typically requires a large number
of annotated data which is expensive and time consuming to obtain. To address
the above problem, an emerging approach is to apply data augmentation to
automatically generate cost-free training samples. In this work, we propose a
systematic study on simple Copy-Paste data augmentation for rare object
detection in autonomous driving. Specifically, local adaptive instance-level
image transformation is introduced to generate realistic rare object masks from
source domain to the target domain. Moreover, traffic scene context is utilized
to guide the placement of masks of rare objects. To this end, our data
augmentation generates training data with high quality and realistic
characteristics by leveraging both local and global consistency. In addition,
we build a new dataset, Rare Object Dataset (ROD), consisting 10k training
images, 4k validation images and the corresponding labels with a diverse range
of scenarios in autonomous driving. Experiments on ROD show that our method
achieves promising results on rare object detection. We also present a thorough
study to illustrate the effectiveness of our local-adaptive and global
constraints based Copy-Paste data augmentation for rare object detection. The
data, development kit and more information of ROD are available online at:
\url{https://nullmax-vision.github.io}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation. (arXiv:2205.01271v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01271">
<div class="article-summary-box-inner">
<span><p>Pose estimation plays a critical role in human-centered vision applications.
However, it is difficult to deploy state-of-the-art HRNet-based pose estimation
models on resource-constrained edge devices due to the high computational cost
(more than 150 GMACs per frame). In this paper, we study efficient architecture
design for real-time multi-person pose estimation on edge. We reveal that
HRNet's high-resolution branches are redundant for models at the
low-computation region via our gradual shrinking experiments. Removing them
improves both efficiency and performance. Inspired by this finding, we design
LitePose, an efficient single-branch architecture for pose estimation, and
introduce two simple approaches to enhance the capacity of LitePose, including
Fusion Deconv Head and Large Kernel Convs. Fusion Deconv Head removes the
redundancy in high-resolution branches, allowing scale-aware feature fusion
with low overhead. Large Kernel Convs significantly improve the model's
capacity and receptive field while maintaining a low computational cost. With
only 25% computation increment, 7x7 kernels achieve +14.0 mAP better than 3x3
kernels on the CrowdPose dataset. On mobile platforms, LitePose reduces the
latency by up to 5.0x without sacrificing performance, compared with prior
state-of-the-art efficient pose estimation models, pushing the frontier of
real-time multi-person pose estimation on edge. Our code and pre-trained models
are released at https://github.com/mit-han-lab/litepose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Detection of Unknown Objects on Roads for Autonomous Driving. (arXiv:2205.01414v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01414">
<div class="article-summary-box-inner">
<span><p>Tremendous progress in deep learning over the last years has led towards a
future with autonomous vehicles on our roads. Nevertheless, the performance of
their perception systems is strongly dependent on the quality of the utilized
training data. As these usually only cover a fraction of all object classes an
autonomous driving system will face, such systems struggle with handling the
unexpected. In order to safely operate on public roads, the identification of
objects from unknown classes remains a crucial task. In this paper, we propose
a novel pipeline to detect unknown objects. Instead of focusing on a single
sensor modality, we make use of lidar and camera data by combining state-of-the
art detection models in a sequential manner. We evaluate our approach on the
Waymo Open Perception Dataset and point out current research gaps in anomaly
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust and Efficient Medical Imaging with Self-Supervision. (arXiv:2205.09723v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09723">
<div class="article-summary-box-inner">
<span><p>Recent progress in Medical Artificial Intelligence (AI) has delivered systems
that can reach clinical expert level performance. However, such systems tend to
demonstrate sub-optimal "out-of-distribution" performance when evaluated in
clinical settings different from the training environment. A common mitigation
strategy is to develop separate systems for each clinical setting using
site-specific data [1]. However, this quickly becomes impractical as medical
data is time-consuming to acquire and expensive to annotate [2]. Thus, the
problem of "data-efficient generalization" presents an ongoing difficulty for
Medical AI development. Although progress in representation learning shows
promise, their benefits have not been rigorously studied, specifically for
out-of-distribution settings. To meet these challenges, we present REMEDIS, a
unified representation learning strategy to improve robustness and
data-efficiency of medical imaging AI. REMEDIS uses a generic combination of
large-scale supervised transfer learning with self-supervised learning and
requires little task-specific customization. We study a diverse range of
medical imaging tasks and simulate three realistic application scenarios using
retrospective data. REMEDIS exhibits significantly improved in-distribution
performance with up to 11.5% relative improvement in diagnostic accuracy over a
strong supervised baseline. More importantly, our strategy leads to strong
data-efficient generalization of medical imaging AI, matching strong supervised
baselines using between 1% to 33% of retraining data across tasks. These
results suggest that REMEDIS can significantly accelerate the life-cycle of
medical imaging AI development thereby presenting an important step forward for
medical imaging AI to deliver broad impact.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Preserving Image Registration. (arXiv:2205.10120v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10120">
<div class="article-summary-box-inner">
<span><p>Image registration is a key task in medical imaging applications, allowing to
represent medical images in a common spatial reference frame. Current
literature on image registration is generally based on the assumption that
images are usually accessible to the researcher, from which the spatial
transformation is subsequently estimated. This common assumption may not be met
in current practical applications, since the sensitive nature of medical images
may ultimately require their analysis under privacy constraints, preventing to
share the image content in clear form. In this work, we formulate the problem
of image registration under a privacy preserving regime, where images are
assumed to be confidential and cannot be disclosed in clear. We derive our
privacy preserving image registration framework by extending classical
registration paradigms to account for advanced cryptographic tools, such as
secure multi-party computation and homomorphic encryption, that enable the
execution of operations without leaking the underlying data. To overcome the
problem of performance and scalability of cryptographic tools in high
dimensions, we first propose to optimize the underlying image registration
operations using gradient approximations. We further revisit the use of
homomorphic encryption and use a packing method to allow the encryption and
multiplication of large matrices more efficiently. We demonstrate our privacy
preserving framework in linear and non-linear registration problems, evaluating
its accuracy and scalability with respect to standard image registration. Our
results show that privacy preserving image registration is feasible and can be
adopted in sensitive medical imaging applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Optimizing Color Rendition and In-Camera Backgrounds in an RGB Virtual Production Stage. (arXiv:2205.12403v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12403">
<div class="article-summary-box-inner">
<span><p>While the LED panels used in virtual production systems can display vibrant
imagery with a wide color gamut, they produce problematic color shifts when
used as lighting due to their peaky spectral output from narrow-band red,
green, and blue LEDs. In this work, we present an improved color calibration
process for virtual production stages which ameliorates this color rendition
problem while also passing through accurate in-camera background colors. We do
this by optimizing linear color correction transformations for 1) the LED panel
pixels visible in the field of view of the camera, 2) the pixels outside the
field of view of the camera illuminating the subjects, and, as a post-process,
3) the pixel values recorded by the camera. The result is that footage shot in
an RGB LED panel virtual production stage can exhibit more accurate skin tones
and costume colors while still reproducing the desired colors of the in-camera
background.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cervical Glandular Cell Detection from Whole Slide Image with Out-Of-Distribution Data. (arXiv:2205.14625v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14625">
<div class="article-summary-box-inner">
<span><p>Cervical glandular cell (GC) detection is a key step in computer-aided
diagnosis for cervical adenocarcinomas screening. It is challenging to
accurately recognize GCs in cervical smears in which squamous cells are the
major. Widely existing Out-Of-Distribution (OOD) data in the entire smear leads
decreasing reliability of machine learning system for GC detection. Although,
the State-Of-The-Art (SOTA) deep learning model can outperform pathologists in
preselected regions of interest, the mass False Positive (FP) prediction with
high probability is still unsolved when facing such gigapixel whole slide
image. This paper proposed a novel PolarNet based on the morphological prior
knowledge of GC trying to solve the FP problem via a self-attention mechanism
in eight-neighbor. It estimates the polar orientation of nucleus of GC. As a
plugin module, PolarNet can guide the deep feature and predicted confidence of
general object detection models. In experiments, we discovered that general
models based on four different frameworks can reject FP in small image set and
increase the mean of average precision (mAP) by $\text{0.007}\sim\text{0.015}$
in average, where the highest exceeds the recent cervical cell detection model
0.037. By plugging PolarNet, the deployed C++ program improved by 8.8\% on
accuracy of top-20 GC detection from external WSIs, while sacrificing 14.4 s of
computational time. Code is available in
https://github.com/Chrisa142857/PolarNet-GCdet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating Building Energy Efficiency From Street View Imagery, Aerial Imagery, and Land Surface Temperature Data. (arXiv:2206.02270v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02270">
<div class="article-summary-box-inner">
<span><p>Decarbonizing the building sector by improving the energy efficiency of the
existing building stock through retrofits in a targeted and efficient way
remains challenging. This is because, as of now, the energy efficiency of
buildings is generally determined by on-site visits of certified energy
auditors which makes the process slow, costly, and geographically incomplete.
In order to accelerate the identification of promising retrofit targets on a
large scale, we propose to estimate building energy efficiency from remotely
sensed data sources only. To do so, we collect street view, aerial view,
footprint, and satellite-borne land surface temperature (LST) data for almost
40,000 buildings across four diverse geographies in the United Kingdom. After
training multiple end-to-end deep learning models on the fused input data in
order to classify buildings as energy efficient (EU rating A-D) or inefficient
(EU rating E-G), we analyze the best performing models quantitatively as well
as qualitatively. Lastly, we extend our analysis by studying the predictive
power of each data source in an ablation study. We find that the best
end-to-end deep learning model achieves a macro-averaged F1-score of 62.06% and
outperforms the k-NN and SVM-based baseline models by 5.62 to 11.47 percentage
points, respectively. As such, this work shows the potential and complementary
nature of remotely sensed data in predicting energy efficiency and opens up new
opportunities for future work to integrate additional data sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Layers are Equivariant to Discrete Shifts But Not Continuous Translations. (arXiv:2206.04979v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04979">
<div class="article-summary-box-inner">
<span><p>The purpose of this short and simple note is to clarify a common
misconception about convolutional neural networks (CNNs). CNNs are made up of
convolutional layers which are shift equivariant due to weight sharing.
However, convolutional layers are not translation equivariant, even when
boundary effects are ignored and when pooling and subsampling are absent. This
is because shift equivariance is a discrete symmetry while translation
equivariance is a continuous symmetry. This fact is well known among
researchers in equivariant machine learning, but is usually overlooked among
non-experts. To minimize confusion, we suggest using the term `shift
equivariance' to refer to discrete shifts in pixels and `translation
equivariance' to refer to continuous translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-purpose Real Haze Benchmark with Quantifiable Haze Levels and Ground Truth. (arXiv:2206.06427v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06427">
<div class="article-summary-box-inner">
<span><p>Imagery collected from outdoor visual environments is often degraded due to
the presence of dense smoke or haze. A key challenge for research in scene
understanding in these degraded visual environments (DVE) is the lack of
representative benchmark datasets. These datasets are required to evaluate
state-of-the-art object recognition and other computer vision algorithms in
degraded settings. In this paper, we address some of these limitations by
introducing the first paired real image benchmark dataset with hazy and
haze-free images, and in-situ haze density measurements. This dataset was
produced in a controlled environment with professional smoke generating
machines that covered the entire scene, and consists of images captured from
the perspective of both an unmanned aerial vehicle (UAV) and an unmanned ground
vehicle (UGV). We also evaluate a set of representative state-of-the-art
dehazing approaches as well as object detectors on the dataset. The full
dataset presented in this paper, including the ground truth object
classification bounding boxes and haze density measurements, is provided for
the community to evaluate their algorithms at: https://a2i2-archangel.vision. A
subset of this dataset has been used for the Object Detection in Haze Track of
CVPR UG2 2022 challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantitative Imaging Principles Improves Medical Image Learning. (arXiv:2206.06663v2 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06663">
<div class="article-summary-box-inner">
<span><p>Fundamental differences between natural and medical images have recently
favored the use of self-supervised learning (SSL) over ImageNet transfer
learning for medical image applications. Differences between image types are
primarily due to the imaging modality and medical images utilize a wide range
of physics based techniques while natural images are captured using only
visible light. While many have demonstrated that SSL on medical images has
resulted in better downstream task performance, our work suggests that more
performance can be gained. The scientific principles which are used to acquire
medical images are not often considered when constructing learning problems.
For this reason, we propose incorporating quantitative imaging principles
during generative SSL to improve image quality and quantitative biological
accuracy. We show that this training schema results in better starting states
for downstream supervised training on limited data. Our model also generates
images that validate on clinical quantitative analysis software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient-Based Adversarial and Out-of-Distribution Detection. (arXiv:2206.08255v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08255">
<div class="article-summary-box-inner">
<span><p>We propose to utilize gradients for detecting adversarial and
out-of-distribution samples. We introduce confounding labels -- labels that
differ from normal labels seen during training -- in gradient generation to
probe the effective expressivity of neural networks. Gradients depict the
amount of change required for a model to properly represent given inputs,
providing insight into the representational power of the model established by
network architectural properties as well as training data. By introducing a
label of different design, we remove the dependency on ground truth labels for
gradient generation during inference. We show that our gradient-based approach
allows for capturing the anomaly in inputs based on the effective expressivity
of the models with no hyperparameter tuning or additional processing, and
outperforms state-of-the-art methods for adversarial and out-of-distribution
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Long-Tailed Bird Audio Recognition. (arXiv:2206.11260v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11260">
<div class="article-summary-box-inner">
<span><p>It is easier to hear birds than see them. However, they still play an
essential role in nature and are excellent indicators of deteriorating
environmental quality and pollution. Recent advances in Deep Neural Networks
allow us to process audio data to detect and classify birds. This technology
can assist researchers in monitoring bird populations and biodiversity. We
propose a sound detection and classification pipeline to analyze complex
soundscape recordings and identify birdcalls in the background. Our method
learns from weak labels and few data and acoustically recognizes the bird
species. Our solution achieved 18th place of 807 teams at the BirdCLEF 2022
Challenge hosted on Kaggle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CV 3315 Is All You Need : Semantic Segmentation Competition. (arXiv:2206.12571v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12571">
<div class="article-summary-box-inner">
<span><p>This competition focus on Urban-Sense Segmentation based on the vehicle
camera view. Class highly unbalanced Urban-Sense images dataset challenge the
existing solutions and further studies. Deep Conventional neural network-based
semantic segmentation methods such as encoder-decoder architecture and
multi-scale and pyramid-based approaches become flexible solutions applicable
to real-world applications. In this competition, we mainly review the
literature and conduct experiments on transformer-driven methods especially
SegFormer, to achieve an optimal trade-off between performance and efficiency.
For example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G,
and the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple
factors, including individual case failure analysis, individual class
performance, training pressure and efficiency estimation, the final candidate
model for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU
evaluated on the testing set. Checkout our code implementation at
https://vmv.re/cv3315.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-view Feature Augmentation with Adaptive Class Activation Mapping. (arXiv:2206.12943v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12943">
<div class="article-summary-box-inner">
<span><p>We propose an end-to-end-trainable feature augmentation module built for
image classification that extracts and exploits multi-view local features to
boost model performance. Different from using global average pooling (GAP) to
extract vectorized features from only the global view, we propose to sample and
ensemble diverse multi-view local features to improve model robustness. To
sample class-representative local features, we incorporate a simple auxiliary
classifier head (comprising only one 1$\times$1 convolutional layer) which
efficiently and adaptively attends to class-discriminative local regions of
feature maps via our proposed AdaCAM (Adaptive Class Activation Mapping).
Extensive experiments demonstrate consistent and noticeable performance gains
achieved by our multi-view feature augmentation module.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Strategy Optimized Pix2pix Approach for SAR-to-Optical Image Translation Task. (arXiv:2206.13042v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13042">
<div class="article-summary-box-inner">
<span><p>This technical report summarizes the analysis and approach on the
image-to-image translation task in the Multimodal Learning for Earth and
Environment Challenge (MultiEarth 2022). In terms of strategy optimization,
cloud classification is utilized to filter optical images with dense cloud
coverage to aid the supervised learning alike approach. The commonly used
pix2pix framework with a few optimizations is applied to build the model. A
weighted combination of mean squared error and mean absolute error is
incorporated in the loss function. As for evaluation, peak to signal ratio and
structural similarity were both considered in our preliminary analysis. Lastly,
our method achieved the second place with a final error score of 0.0412. The
results indicate great potential towards SAR-to-optical translation in remote
sensing tasks, specifically for the support of long-term environmental
monitoring and protection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SearchMorph:Multi-scale Correlation Iterative Network for Deformable Registration. (arXiv:2206.13076v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13076">
<div class="article-summary-box-inner">
<span><p>Deformable image registration provides dynamic information about the image
and is essential in medical image analysis. However, due to the different
characteristics of single-temporal brain MR images and multi-temporal
echocardiograms, it is difficult to accurately register them using the same
algorithm or model. We propose an unsupervised multi-scale correlation
iterative registration network (SearchMorph), and the model has three
highlights. (1)We introduced cost volumes to strengthen feature correlations
and constructed correlation pyramids to complement multi-scale correlation
information. (2) We designed the search module to search for the registration
of features in multi-scale pyramids. (3) We use the GRU module for iterative
refinement of the deformation field. The proposed network in this paper shows
leadership in common single-temporal registration tasks and solves
multi-temporal motion estimation tasks. The experimental results show that our
proposed method achieves higher registration accuracy and a lower folding point
ratio than the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Medical Image Fusion Method based on MDLatLRRv2. (arXiv:2206.15179v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15179">
<div class="article-summary-box-inner">
<span><p>Since MDLatLRR only considers detailed parts (salient features) of input
images extracted by latent low-rank representation (LatLRR), it doesn't use
base parts (principal features) extracted by LatLRR effectively. Therefore, we
proposed an improved multi-level decomposition method called MDLatLRRv2 which
effectively analyzes and utilizes all the image features obtained by LatLRR.
Then we apply MDLatLRRv2 to medical image fusion. The base parts are fused by
average strategy and the detail parts are fused by nuclear-norm operation. The
comparison with the existing methods demonstrates that the proposed method can
achieve state-of-the-art fusion performance in objective and subjective
assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class Impression for Data-free Incremental Learning. (arXiv:2207.00005v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00005">
<div class="article-summary-box-inner">
<span><p>Standard deep learning-based classification approaches require collecting all
samples from all classes in advance and are trained offline. This paradigm may
not be practical in real-world clinical applications, where new classes are
incrementally introduced through the addition of new data. Class incremental
learning is a strategy allowing learning from such data. However, a major
challenge is catastrophic forgetting, i.e., performance degradation on previous
classes when adapting a trained model to new data. Prior methodologies to
alleviate this challenge save a portion of training data require perpetual
storage of such data that may introduce privacy issues. Here, we propose a
novel data-free class incremental learning framework that first synthesizes
data from the model trained on previous classes to generate a \ours.
Subsequently, it updates the model by combining the synthesized data with new
class data. Furthermore, we incorporate a cosine normalized Cross-entropy loss
to mitigate the adverse effects of the imbalance, a margin loss to increase
separation among previous classes and new ones, and an intra-domain contrastive
loss to generalize the model trained on the synthesized data to real data. We
compare our proposed framework with state-of-the-art methods in class
incremental learning, where we demonstrate improvement in accuracy for the
classification of 11,062 echocardiography cine series of patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Query-Key Pairwise Interactions in Vision Transformers. (arXiv:2207.00188v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00188">
<div class="article-summary-box-inner">
<span><p>Vision Transformers have achieved state-of-the-art performance in many visual
tasks. Due to the quadratic computational and memory complexities of
self-attention, recent works either apply attention only to low-resolution
inputs or restrict the receptive field to a small local region. To overcome
these limitations, we propose key-only attention, which excludes query-key
pairwise interactions and uses a compute-efficient saliency-gate to obtain
attention weights, modeling local-global interactions in all stages. Key-only
attention has linear computational and memory complexities w.r.t input size. We
use alternate layout to hybridize convolution and attention layers instead of
grafting which is suggested by previous works, so that all stages can benefit
from both spatial attentions and convolutions. We leverage these improvements
to develop a new self-attention model family, LinGlos, which reach
state-of-the-art accuracies on the parameter-limited setting of ImageNet
classification benchmark, and outperform baselines significantly in downstream
tasks, e.g., COCO object detection and ADE20K semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Covid-19 Detection Using transfer Learning Approach from Computed Temography Images. (arXiv:2207.00259v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00259">
<div class="article-summary-box-inner">
<span><p>Our main goal in this study is to propose a transfer learning based method
for COVID-19 detection from Computed Tomography (CT) images. The transfer
learning model used for the task is a pretrained Xception model. Both model
architecture and pre-trained weights on ImageNet were used. The resulting
modified model was trained with 128 batch size and 224x224, 3 channeled input
images, converted from original 512x512, grayscale images. The dataset used is
a the COV19-CT-DB. Labels in the dataset include COVID-19 cases and
Non-COVID-19 cases for COVID-1919 detection. Firstly, a accuracy and loss on
the validation partition of the dataset as well as precision recall and macro
F1 score were used to measure the performance of the proposed method. The
resulting Macro F1 score on the validation set exceeded the baseline model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label. (arXiv:2207.00278v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00278">
<div class="article-summary-box-inner">
<span><p>Due to its powerful feature learning capability and high efficiency, deep
hashing has achieved great success in large-scale image retrieval. Meanwhile,
extensive works have demonstrated that deep neural networks (DNNs) are
susceptible to adversarial examples, and exploring adversarial attack against
deep hashing has attracted many research efforts. Nevertheless, backdoor
attack, another famous threat to DNNs, has not been studied for deep hashing
yet. Although various backdoor attacks have been proposed in the field of image
classification, existing approaches failed to realize a truly imperceptive
backdoor attack that enjoys invisible triggers and clean label setting
simultaneously, and they also cannot meet the intrinsic demand of image
retrieval backdoor.
</p>
<p>In this paper, we propose BadHash, the first generative-based imperceptible
backdoor attack against deep hashing, which can effectively generate invisible
and input-specific poisoned images with clean label. Specifically, we first
propose a new conditional generative adversarial network (cGAN) pipeline to
effectively generate poisoned samples. For any given benign image, it seeks to
generate a natural-looking poisoned counterpart with a unique invisible
trigger. In order to improve the attack effectiveness, we introduce a
label-based contrastive learning network LabCLN to exploit the semantic
characteristics of different labels, which are subsequently used for confusing
and misleading the target model to learn the embedded trigger. We finally
explore the mechanism of backdoor attacks on image retrieval in the hash space.
Extensive experiments on multiple benchmark datasets verify that BadHash can
generate imperceptible poisoned samples with strong attack ability and
transferability over state-of-the-art deep hashing schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt. (arXiv:2206.07137v2 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07137">
<div class="article-summary-box-inner">
<span><p>Training on web-scale data can take months. But most computation and time is
wasted on redundant and noisy points that are already learnt or not learnable.
To accelerate training, we introduce Reducible Holdout Loss Selection
(RHO-LOSS), a simple but principled technique which selects approximately those
points for training that most reduce the model's generalization loss. As a
result, RHO-LOSS mitigates the weaknesses of existing data selection methods:
techniques from the optimization literature typically select 'hard' (e.g. high
loss) points, but such points are often noisy (not learnable) or less
task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but
such points need not be trained on once learned. In contrast, RHO-LOSS selects
points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains
in far fewer steps than prior art, improves accuracy, and speeds up training on
a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and
BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in
18x fewer steps and reaches 2% higher final accuracy than uniform data
shuffling.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-05 23:08:58.876991981 UTC">2022-07-05 23:08:58 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>