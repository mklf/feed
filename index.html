<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-14T01:30:00Z">07-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Highly Expressive Machine Learning Models of Non-Melanoma Skin Cancer. (arXiv:2207.05749v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05749">
<div class="article-summary-box-inner">
<span><p>Pathologists have a rich vocabulary with which they can describe all the
nuances of cellular morphology. In their world, there is a natural pairing of
images and words. Recent advances demonstrate that machine learning models can
now be trained to learn high-quality image features and represent them as
discrete units of information. This enables natural language, which is also
discrete, to be jointly modelled alongside the imaging, resulting in a
description of the contents of the imaging. Here we present experiments in
applying discrete modelling techniques to the problem domain of non-melanoma
skin cancer, specifically, histological images of Intraepidermal Carcinoma
(IEC). Implementing a VQ-GAN model to reconstruct high-resolution (256x256)
images of IEC images, we trained a sequence-to-sequence transformer to generate
natural language descriptions using pathologist terminology. Combined with the
idea of interactive concept vectors available by using continuous generative
methods, we demonstrate an additional angle of interpretability. The result is
a promising means of working towards highly expressive machine learning systems
which are not only useful as predictive/classification tools, but also means to
further our scientific understanding of disease.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CQC: A Crosstalk-Aware Quantum Program Compilation Framework. (arXiv:2207.05751v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05751">
<div class="article-summary-box-inner">
<span><p>Near-term quantum systems are noisy. Crosstalk noise has been identified as
one of the major sources of noises in superconducting Noisy Intermediate-Scale
Quantum (NISQ) devices. Crosstalk arises from the concurrent execution of
two-qubit gates, such as \texttt{CX}, on nearby qubits. It may significantly
increase the error rate of gates compared to running them individually.
Crosstalk can be mitigated through scheduling or hardware tuning. Prior
studies, however, handle crosstalk at a very late stage in the compilation
later, typically after hardware mapping is done. It might miss great
opportunities of optimizing algorithm logic, routing, and crosstalk at the same
time. In this paper, we push the envelope by considering all these factors
simultaneously at the very early compilation stage. We propose a
crosstalk-aware quantum program compilation framework called CQC that can
enhance crosstalk-mitigation while achieving satisfactory circuit depth.
Moreover, we identify opportunities for translation from intermediate
representation to the circuit for application-specific crosstalk mitigation,
for instance, the \texttt{CX} ladder construction in variational quantum
eigensolvers (VQE). Evaluations through simulation and on real IBM-Q devices
show that our framework can significantly reduce the error rate by up to
6$\times$, with only $\sim$60\% circuit depth compared to state-of-the-art gate
scheduling approaches. In particular for VQE, we demonstrate 49\% circuit depth
reduction with 9.6\% fidelity improvement over prior art on the H4 molecule
using IBMQ Guadalupe. Our CQC framework will be released on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSLAT: Open Set Label Attention Transformer for Medical Entity Span Extraction. (arXiv:2207.05817v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05817">
<div class="article-summary-box-inner">
<span><p>Identifying spans in medical texts that correspond to medical entities is one
of the core steps for many healthcare NLP tasks such as ICD coding, medical
finding extraction, medical note contextualization, to name a few. Existing
entity extraction methods rely on a fixed and limited vocabulary of medical
entities and have difficulty with extracting entities represented by disjoint
spans. In this paper, we present a new transformer-based architecture called
OSLAT, Open Set Label Attention Transformer, that addresses many of the
limitations of the previous methods. Our approach uses the label-attention
mechanism to implicitly learn spans associated with entities of interest. These
entities can be provided as free text, including entities not seen during
OSLAT's training, and the model can extract spans even when they are disjoint.
To test the generalizability of our method, we train two separate models on two
different datasets, which have very low entity overlap: (1) a public discharge
notes dataset from hNLP, and (2) a much more challenging proprietary patient
text dataset "Reasons for Encounter" (RFE). We find that OSLAT models trained
on either dataset outperform rule-based and fuzzy string matching baselines
when applied to the RFE dataset as well as to the portion of hNLP dataset where
entities are represented by disjoint spans. Our code can be found at
https://github.com/curai/curai-research/tree/main/OSLAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Social Graph Networks for Emotion Prediction. (arXiv:2207.05820v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05820">
<div class="article-summary-box-inner">
<span><p>Emotion prediction plays an essential role in mental health and emotion-aware
computing. The complex nature of emotion resulting from its dependency on a
person's physiological health, mental state, and his surroundings makes its
prediction a challenging task. In this work, we utilize mobile sensing data to
predict happiness and stress. In addition to a person's physiological features,
we also incorporate the environment's impact through weather and social
network. To this end, we leverage phone data to construct social networks and
develop a machine learning architecture that aggregates information from
multiple users of the graph network and integrates it with the temporal
dynamics of data to predict emotion for all the users. The construction of
social networks does not incur additional cost in terms of EMAs or data
collection from users and doesn't raise privacy concerns. We propose an
architecture that automates the integration of a user's social network affect
prediction, is capable of dealing with the dynamic distribution of real-life
social networks, making it scalable to large-scale networks. Our extensive
evaluation highlights the improvement provided by the integration of social
networks. We further investigate the impact of graph topology on model's
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sockeye 3: Fast Neural Machine Translation with PyTorch. (arXiv:2207.05851v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05851">
<div class="article-summary-box-inner">
<span><p>Sockeye 3 is the latest version of the Sockeye toolkit for Neural Machine
Translation (NMT). Now based on PyTorch, Sockeye 3 provides faster model
implementations and more advanced features with a further streamlined codebase.
This enables broader experimentation with faster iteration, efficient training
of stronger and faster models, and the flexibility to move new ideas quickly
from research to production. When running comparable models, Sockeye 3 is up to
126% faster than other PyTorch implementations on GPUs and up to 292% faster on
CPUs. Sockeye 3 is open source software released under the Apache 2.0 license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel DeBERTa-based Model for Financial Question Answering Task. (arXiv:2207.05875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05875">
<div class="article-summary-box-inner">
<span><p>As a rising star in the field of natural language processing, question
answering systems (Q&amp;A Systems) are widely used in all walks of life. Compared
with other scenarios, the applicationin financial scenario has strong
requirements in the traceability and interpretability of the Q&amp;A systems. In
addition, since the demand for artificial intelligence technology has gradually
shifted from the initial computational intelligence to cognitive intelligence,
this research mainly focuses on the financial numerical reasoning dataset -
FinQA. In the shared task, the objective is to generate the reasoning program
and the final answer according to the given financial report containing text
and tables. We use the method based on DeBERTa pre-trained language model, with
additional optimization methods including multi-model fusion, training set
combination on this basis. We finally obtain an execution accuracy of 68.99 and
a program accuracy of 64.53, ranking No. 4 in the 2022 FinQA Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained Models. (arXiv:2207.05928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05928">
<div class="article-summary-box-inner">
<span><p>Most of the Chinese pre-trained models adopt characters as basic units for
downstream tasks. However, these models ignore the information carried by words
and thus lead to the loss of some important semantics. In this paper, we
propose a new method to exploit word structure and integrate lexical semantics
into character representations of pre-trained models. Specifically, we project
a word's embedding into its internal characters' embeddings according to the
similarity weight. To strengthen the word boundary information, we mix the
representations of the internal characters within a word. After that, we apply
a word-to-character alignment attention mechanism to emphasize important
characters by masking unimportant ones. Moreover, in order to reduce the error
propagation caused by word segmentation, we present an ensemble approach to
combine segmentation results given by different tokenizers. The experimental
results show that our approach achieves superior performance over the basic
pre-trained models BERT, BERT-wwm and ERNIE on different Chinese NLP tasks:
sentiment classification, sentence pair matching, natural language inference
and machine reading comprehension. We make further analysis to prove the
effectiveness of each component of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A General Contextualized Rewriting Framework for Text Summarization. (arXiv:2207.05948v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05948">
<div class="article-summary-box-inner">
<span><p>The rewriting method for text summarization combines extractive and
abstractive approaches, improving the conciseness and readability of extractive
summaries using an abstractive model. Exiting rewriting systems take each
extractive sentence as the only input, which is relatively focused but can lose
necessary background knowledge and discourse context. In this paper, we
investigate contextualized rewriting, which consumes the entire document and
considers the summary context. We formalize contextualized rewriting as a
seq2seq with group-tag alignments, introducing group-tag as a solution to model
the alignments, identifying extractive sentences through content-based
addressing. Results show that our approach significantly outperforms
non-contextualized rewriting systems without requiring reinforcement learning,
achieving strong improvements on ROUGE scores upon multiple extractors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Developing a Component Comment Extractor from Product Reviews on E-Commerce Sites. (arXiv:2207.05979v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05979">
<div class="article-summary-box-inner">
<span><p>Consumers often read product reviews to inform their buying decision, as some
consumers want to know a specific component of a product. However, because
typical sentences on product reviews contain various details, users must
identify sentences about components they want to know amongst the many reviews.
Therefore, we aimed to develop a system that identifies and collects component
and aspect information of products in sentences. Our BERT-based classifiers
assign labels referring to components and aspects to sentences in reviews and
extract sentences with comments on specific components and aspects. We
determined proper labels based for the words identified through pattern
matching from product reviews to create the training data. Because we could not
use the words as labels, we carefully created labels covering the meanings of
the words. However, the training data was imbalanced on component and aspect
pairs. We introduced a data augmentation method using WordNet to reduce the
bias. Our evaluation demonstrates that the system can determine labels for road
bikes using pattern matching, covering more than 88\% of the indicators of
components and aspects on e-commerce sites. Moreover, our data augmentation
method can improve the-F1-measure on insufficient data from 0.66 to 0.76.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocCoder: Generating Code by Retrieving and Reading Docs. (arXiv:2207.05987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05987">
<div class="article-summary-box-inner">
<span><p>Natural-language-to-code models learn to generate a code snippet given a
natural language (NL) intent. However, the rapid growth of both publicly
available and proprietary libraries and functions makes it impossible to cover
all APIs using training examples, as new libraries and functions are introduced
daily. Thus, existing models inherently cannot generalize to using unseen
functions and libraries merely through incorporating them into the training
data. In contrast, when human programmers write programs, they frequently refer
to textual resources such as code manuals, documentation, and tutorials, to
explore and understand available library functionality. Inspired by this
observation, we introduce DocCoder: an approach that explicitly leverages code
manuals and documentation by (1) retrieving the relevant documentation given
the NL intent, and (2) generating the code based on the NL intent and the
retrieved documentation. Our approach is general, can be applied to any
programming language, and is agnostic to the underlying neural model. We
demonstrate that DocCoder consistently improves NL-to-code models: DocCoder
achieves 11x higher exact match accuracy than strong baselines on a new Bash
dataset tldr; on the popular Python CoNaLa benchmark, DocCoder improves over
strong baselines by 1.65 BLEU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS. (arXiv:2207.06000v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06000">
<div class="article-summary-box-inner">
<span><p>Expressive text-to-speech has shown improved performance in recent years.
However, the style control of synthetic speech is often restricted to discrete
emotion categories and requires training data recorded by the target speaker in
the target style. In many practical situations, users may not have reference
speech recorded in target emotion but still be interested in controlling speech
style just by typing text description of desired emotional style. In this
paper, we propose a text-based interface for emotional style control and
cross-speaker style transfer in multi-speaker TTS. We propose the bi-modal
style encoder which models the semantic relationship between text description
embedding and speech style embedding with a pretrained language model. To
further improve cross-speaker style transfer on disjoint, multi-style datasets,
we propose the novel style loss. The experimental results show that our model
can generate high-quality expressive speech even in unseen style.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation. (arXiv:2207.06130v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06130">
<div class="article-summary-box-inner">
<span><p>The past several years have witnessed Variational Auto-Encoder's superiority
in various text generation tasks. However, due to the sequential nature of the
text, auto-regressive decoders tend to ignore latent variables and then reduce
to simple language models, known as the KL vanishing problem, which would
further deteriorate when VAE is combined with Transformer-based structures. To
ameliorate this problem, we propose DELLA, a novel variational Transformer
framework. DELLA learns a series of layer-wise latent variables with each
inferred from those of lower layers and tightly coupled with the hidden states
by low-rank tensor product. In this way, DELLA forces these posterior latent
variables to be fused deeply with the whole computation path and hence
incorporate more information. We theoretically demonstrate that our method can
be regarded as entangling latent variables to avoid posterior information
decrease through layers, enabling DELLA to get higher non-zero KL values even
without any annealing or thresholding tricks. Experiments on four unconditional
and three conditional generation tasks show that DELLA could better alleviate
KL vanishing and improve both quality and diversity compared to several strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Hashtags to Analyze Purpose and Technology Application of Open-Source Project Related to COVID-19. (arXiv:2207.06219v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06219">
<div class="article-summary-box-inner">
<span><p>COVID-19 has had a profound impact on the lives of all human beings. Emerging
technologies have made significant contributions to the fight against the
pandemic. An extensive review of the application of technology will help
facilitate future research and technology development to provide better
solutions for future pandemics. In contrast to the extensive surveys of
academic communities that have already been conducted, this study explores the
IT community of practice. Using GitHub as the study target, we analyzed the
main functionalities of the projects submitted during the pandemic. This study
examines trends in projects with different functionalities and the relationship
between functionalities and technologies. The study results show an imbalance
in the number of projects with varying functionalities in the GitHub community,
i.e., applications account for more than half of the projects. In contrast,
other data analysis and AI projects account for a smaller share. This differs
significantly from the survey of the academic community, where the findings
focus more on cutting-edge technologies while projects in the community of
practice use more mature technologies. The spontaneous behavior of developers
may lack organization and make it challenging to target needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building a Relation Extraction Baseline for Gene-Disease Associations: A Reproducibility Study. (arXiv:2207.06226v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06226">
<div class="article-summary-box-inner">
<span><p>Reproducibility is an important task in scientific research. It is crucial
for researchers to compare newly developed systems with the state-of-the-art to
assess whether they made a breakthrough. However previous works may not be
immediately reproducible, for example due to the lack of source code. In this
work we reproduce DEXTER, a system to automatically extract Gene-Disease
Associations (GDAs) from biomedical abstracts. The goal is to provide a
benchmark for future works regarding Relation Extraction (RE), enabling
researchers to test and compare their results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transfer Learning Based Model for Text Readability Assessment in German. (arXiv:2207.06265v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06265">
<div class="article-summary-box-inner">
<span><p>Text readability assessment has a wide range of applications for different
target people, from language learners to people with disabilities. The fast
pace of textual content production on the web makes it impossible to measure
text complexity without the benefit of machine learning and natural language
processing techniques. Although various research addressed the readability
assessment of English text in recent years, there is still room for improvement
of the models for other languages. In this paper, we proposed a new model for
text complexity assessment for German text based on transfer learning. Our
results show that the model outperforms more classical solutions based on
linguistic features extraction from input text. The best model is based on the
BERT pre-trained language model achieved the Root Mean Square Error (RMSE) of
0.483.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re2G: Retrieve, Rerank, Generate. (arXiv:2207.06300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06300">
<div class="article-summary-box-inner">
<span><p>As demonstrated by GPT-3 and T5, transformers grow in capability as parameter
spaces become larger and larger. However, for tasks that require a large amount
of knowledge, non-parametric memory allows models to grow dramatically with a
sub-linear increase in computational cost and GPU memory requirements. Recent
models such as RAG and REALM have introduced retrieval into conditional
generation. These models incorporate neural initial retrieval from a corpus of
passages. We build on this line of research, proposing Re2G, which combines
both neural initial retrieval and reranking into a BART-based
sequence-to-sequence generation. Our reranking approach also permits merging
retrieval results from sources with incomparable scores, enabling an ensemble
of BM25 and neural initial retrieval. To train our system end-to-end, we
introduce a novel variation of knowledge distillation to train the initial
retrieval, reranker, and generation using only ground truth on the target
sequence output. We find large gains in four diverse tasks: zero-shot slot
filling, question answering, fact-checking, and dialog, with relative gains of
9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make
our code available as open source at
https://github.com/IBM/kgi-slot-filling/tree/re2g.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N-Grammer: Augmenting Transformers with latent n-grams. (arXiv:2207.06366v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06366">
<div class="article-summary-box-inner">
<span><p>Transformer models have recently emerged as one of the foundational models in
natural language processing, and as a byproduct, there is significant recent
interest and investment in scaling these models. However, the training and
inference costs of these large Transformer language models are prohibitive,
thus necessitating more research in identifying more efficient variants. In
this work, we propose a simple yet effective modification to the Transformer
architecture inspired by the literature in statistical language modeling, by
augmenting the model with n-grams that are constructed from a discrete latent
representation of the text sequence. We evaluate our model, the N-Grammer on
language modeling on the C4 data-set as well as text classification on the
SuperGLUE data-set, and find that it outperforms several strong baselines such
as the Transformer and the Primer. We open-source our model for reproducibility
purposes in Jax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Concept Grounding on Neural Fields. (arXiv:2207.06403v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06403">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the challenging problem of 3D concept grounding
(i.e. segmenting and learning visual concepts) by looking at RGBD images and
reasoning about paired questions and answers. Existing visual reasoning
approaches typically utilize supervised methods to extract 2D segmentation
masks on which concepts are grounded. In contrast, humans are capable of
grounding concepts on the underlying 3D representation of images. However,
traditionally inferred 3D representations (e.g., point clouds, voxelgrids, and
meshes) cannot capture continuous 3D features flexibly, thus making it
challenging to ground concepts to 3D regions based on the language description
of the object being referred to. To address both issues, we propose to leverage
the continuous, differentiable nature of neural fields to segment and learn
concepts. Specifically, each 3D coordinate in a scene is represented as a
high-dimensional descriptor. Concept grounding can then be performed by
computing the similarity between the descriptor vector of a 3D coordinate and
the vector embedding of a language concept, which enables segmentations and
concept learning to be jointly learned on neural fields in a differentiable
fashion. As a result, both 3D semantic and instance segmentations can emerge
directly from question answering supervision using a set of defined neural
operators on top of neural fields (e.g., filtering and counting). Experimental
results show that our proposed framework outperforms
unsupervised/language-mediated segmentation models on semantic and instance
segmentation tasks, as well as outperforms existing models on the challenging
3D aware visual reasoning tasks. Furthermore, our framework can generalize well
to unseen shape categories and real scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic modeling of rational communication with conditionals. (arXiv:2105.05502v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05502">
<div class="article-summary-box-inner">
<span><p>While a large body of work has scrutinized the meaning of conditional
sentences, considerably less attention has been paid to formal models of their
pragmatic use and interpretation. Here, we take a probabilistic approach to
pragmatic reasoning about indicative conditionals which flexibly integrates
gradient beliefs about richly structured world states. We model listeners'
update of their prior beliefs about the causal structure of the world and the
joint probabilities of the consequent and antecedent based on assumptions about
the speaker's utterance production protocol. We show that, when supplied with
natural contextual assumptions, our model uniformly explains a number of
inferences attested in the literature, including epistemic inferences,
conditional perfection and the dependency between antecedent and consequent of
a conditional. We argue that this approach also helps explain three puzzles
introduced by Douven (2012) about updating with conditionals: depending on the
utterance context, the listener's belief in the antecedent may increase,
decrease or remain unchanged.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Machines Learn Morality? The Delphi Experiment. (arXiv:2110.07574v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07574">
<div class="article-summary-box-inner">
<span><p>As AI systems become increasingly powerful and pervasive, there are growing
concerns about machines' morality or a lack thereof. Yet, teaching morality to
machines is a formidable task, as morality remains among the most intensely
debated questions in humanity, let alone for AI. Existing AI systems deployed
to millions of users, however, are already making decisions loaded with moral
implications, which poses a seemingly impossible challenge: teaching machines
moral sense, while humanity continues to grapple with it.
</p>
<p>To explore this challenge, we introduce Delphi, an experimental framework
based on deep neural networks trained directly to reason about descriptive
ethical judgments, e.g., "helping a friend" is generally good, while "helping a
friend spread fake news" is not. Empirical results shed novel insights on the
promises and limits of machine ethics; Delphi demonstrates strong
generalization capabilities in the face of novel ethical situations, while
off-the-shelf neural network models exhibit markedly poor judgment including
unjust biases, confirming the need for explicitly teaching machines moral
sense.
</p>
<p>Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and
inconsistencies. Despite that, we demonstrate positive use cases of imperfect
Delphi, including using it as a component model within other imperfect AI
systems. Importantly, we interpret the operationalization of Delphi in light of
prominent ethical theories, which leads us to important future research
questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks. (arXiv:2112.03227v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03227">
<div class="article-summary-box-inner">
<span><p>General-purpose robots coexisting with humans in their environment must learn
to relate human language to their perceptions and actions to be useful in a
range of daily tasks. Moreover, they need to acquire a diverse repertoire of
general-purpose skills that allow composing long-horizon tasks by following
unconstrained language instructions. In this paper, we present CALVIN
(Composing Actions from Language and Vision), an open-source simulated
benchmark to learn long-horizon language-conditioned tasks. Our aim is to make
it possible to develop agents that can solve many robotic manipulation tasks
over a long horizon, from onboard sensors, and specified only via human
language. CALVIN tasks are more complex in terms of sequence length, action
space, and language than existing vision-and-language task datasets and
supports flexible specification of sensor suites. We evaluate the agents in
zero-shot to novel language instructions and to novel environments and objects.
We show that a baseline model based on multi-context imitation learning
performs poorly on CALVIN, suggesting that there is significant room for
developing innovative agents that learn to relate human language to their world
models with this benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FRUIT: Faithfully Reflecting Updated Information in Text. (arXiv:2112.08634v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08634">
<div class="article-summary-box-inner">
<span><p>Textual knowledge bases such as Wikipedia require considerable effort to keep
up to date and consistent. While automated writing assistants could potentially
ease this burden, the problem of suggesting edits grounded in external
knowledge has been under-explored. In this paper, we introduce the novel
generation task of *faithfully reflecting updated information in text* (FRUIT)
where the goal is to update an existing article given new evidence. We release
the FRUIT-WIKI dataset, a collection of over 170K distantly supervised data
produced from pairs of Wikipedia snapshots, along with our data generation
pipeline and a gold evaluation set of 914 instances whose edits are guaranteed
to be supported by the evidence. We provide benchmark results for popular
generation systems as well as EDIT5 -- a T5-based approach tailored to editing
we introduce that establishes the state of the art. Our analysis shows that
developing models that can update articles faithfully requires new capabilities
for neural generation models, and opens doors to many new applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Degendering Resumes for Fair Algorithmic Resume Screening. (arXiv:2112.08910v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08910">
<div class="article-summary-box-inner">
<span><p>We investigate whether it is feasible to remove gendered information from
resumes to mitigate potential bias in algorithmic resume screening. Using a
corpus of 709k resumes from IT firms, we first train a series of models to
classify the self-reported gender of the applicant, thereby measuring the
extent and nature of gendered information encoded in resumes. We then conduct a
series of gender obfuscation experiments, where we iteratively remove gendered
information from resumes. Finally, we train a resume screening algorithm and
investigate the trade-off between gender obfuscation and screening algorithm
performance. Results show: (1) There is a significant amount of gendered
information in resumes. (2) Lexicon-based gender obfuscation method (i.e.
removing tokens that are predictive of gender) can reduce the amount of
gendered information to a large extent. However, after a certain point, the
performance of the resume screening algorithm starts suffering. (3)
General-purpose gender debiasing methods for NLP models such as removing gender
subspace from embeddings are not effective in obfuscating gender.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation. (arXiv:2203.15479v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15479">
<div class="article-summary-box-inner">
<span><p>Speech segmentation, which splits long speech into short segments, is
essential for speech translation (ST). Popular VAD tools like WebRTC VAD have
generally relied on pause-based segmentation. Unfortunately, pauses in speech
do not necessarily match sentence boundaries, and sentences can be connected by
a very short pause that is difficult to detect by VAD. In this study, we
propose a speech segmentation method using a binary classification model
trained using a segmented bilingual speech corpus. We also propose a hybrid
method that combines VAD and the above speech segmentation method. Experimental
results revealed that the proposed method is more suitable for cascade and
end-to-end ST systems than conventional segmentation methods. The hybrid
approach further improved the translation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedNST: Federated Noisy Student Training for Automatic Speech Recognition. (arXiv:2206.02797v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02797">
<div class="article-summary-box-inner">
<span><p>Federated Learning (FL) enables training state-of-the-art Automatic Speech
Recognition (ASR) models on user devices (clients) in distributed systems,
hence preventing transmission of raw user data to a central server. A key
challenge facing practical adoption of FL for ASR is obtaining ground-truth
labels on the clients. Existing approaches rely on clients to manually
transcribe their speech, which is impractical for obtaining large training
corpora. A promising alternative is using semi-/self-supervised learning
approaches to leverage unlabelled user data. To this end, we propose FedNST, a
novel method for training distributed ASR models using private and unlabelled
user data. We explore various facets of FedNST, such as training models with
different proportions of labelled and unlabelled data, and evaluate the
proposed approach on 1173 simulated clients. Evaluating FedNST on LibriSpeech,
where 960 hours of speech data is split equally into server (labelled) and
client (unlabelled) data, showed a 22.5% relative word error rate reduction}
(WERR) over a supervised baseline trained only on server data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Robust Natural Language Understanding is a Challenge. (arXiv:2206.14575v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14575">
<div class="article-summary-box-inner">
<span><p>With the proliferation of Deep Machine Learning into real-life applications,
a particular property of this technology has been brought to attention:
robustness Neural Networks notoriously present low robustness and can be highly
sensitive to small input perturbations. Recently, many methods for verifying
networks' general properties of robustness have been proposed, but they are
mostly applied in Computer Vision. In this paper we propose a Verification
specification for Natural Language Understanding classification based on larger
regions of interest, and we discuss the challenges of such task. We observe
that, although the data is almost linearly separable, the verifier struggles to
output positive results and we explain the problems and implications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Translation with Compiler Representations. (arXiv:2207.03578v2 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03578">
<div class="article-summary-box-inner">
<span><p>In this paper, we leverage low-level compiler intermediate representations
(IR) to improve code translation. Traditional transpilers rely on syntactic
information and handcrafted rules, which limits their applicability and
produces unnatural-looking code. Applying neural machine translation (NMT)
approaches to code has successfully broadened the set of programs on which one
can get a natural-looking translation. However, they treat the code as
sequences of text tokens, and still do not differentiate well enough between
similar pieces of code which have different semantics in different languages.
The consequence is low quality translation, reducing the practicality of NMT,
and stressing the need for an approach significantly increasing its accuracy.
Here we propose to augment code translation with IRs, specifically LLVM IR,
with results on the C++, Java, Rust, and Go languages. Our method improves upon
the state of the art for unsupervised code translation, increasing the number
of correct translations by 11% on average, and up to 79% for the Java - Rust
pair. We extend previous test sets for code translation, by adding hundreds of
Go and Rust functions. Additionally, we train models with high performance on
the problem of IR decompilation, generating programming source code from IR,
and study using IRs as intermediary pivot for translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis. (arXiv:2207.03800v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03800">
<div class="article-summary-box-inner">
<span><p>Unconstrained lip-to-speech synthesis aims to generate corresponding speeches
from silent videos of talking faces with no restriction on head poses or
vocabulary. Current works mainly use sequence-to-sequence models to solve this
problem, either in an autoregressive architecture or a flow-based
non-autoregressive architecture. However, these models suffer from several
drawbacks: 1) Instead of directly generating audios, they use a two-stage
pipeline that first generates mel-spectrograms and then reconstructs audios
from the spectrograms. This causes cumbersome deployment and degradation of
speech quality due to error propagation; 2) The audio reconstruction algorithm
used by these models limits the inference speed and audio quality, while neural
vocoders are not available for these models since their output spectrograms are
not accurate enough; 3) The autoregressive model suffers from high inference
latency, while the flow-based model has high memory occupancy: neither of them
is efficient enough in both time and memory usage. To tackle these problems, we
propose FastLTS, a non-autoregressive end-to-end model which can directly
synthesize high-quality speech audios from unconstrained talking videos with
low latency, and has a relatively small model size. Besides, different from the
widely used 3D-CNN visual frontend for lip movement encoding, we for the first
time propose a transformer-based visual frontend for this task. Experiments
show that our model achieves $19.76\times$ speedup for audio waveform
generation compared with the current autoregressive model on input sequences of
3 seconds, and obtains superior audio quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A description of Turkish Discourse Bank 1.2 and an examination of common dependencies in Turkish discourse. (arXiv:2207.05008v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05008">
<div class="article-summary-box-inner">
<span><p>We describe Turkish Discourse Bank 1.2, the latest version of a discourse
corpus annotated for explicitly or implicitly conveyed discourse relations,
their constitutive units, and senses in the Penn Discourse Treebank style. We
present an evaluation of the recently added tokens and examine three commonly
occurring dependency patterns that hold among the constitutive units of a pair
of adjacent discourse relations, namely, shared arguments, full embedding and
partial containment of a discourse relation. We present three major findings:
(a) implicitly conveyed relations occur more often than explicitly conveyed
relations in the data; (b) it is much more common for two adjacent implicit
discourse relations to share an argument than for two adjacent explicit
relations to do so; (c) both full embedding and partial containment of
discourse relations are pervasive in the corpus, which can be partly due to
subordinator connectives whose preposed subordinate clause tends to be selected
together with the matrix clause rather than being selected alone. Finally, we
briefly discuss the implications of our findings for Turkish discourse parsing.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Highly Expressive Machine Learning Models of Non-Melanoma Skin Cancer. (arXiv:2207.05749v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05749">
<div class="article-summary-box-inner">
<span><p>Pathologists have a rich vocabulary with which they can describe all the
nuances of cellular morphology. In their world, there is a natural pairing of
images and words. Recent advances demonstrate that machine learning models can
now be trained to learn high-quality image features and represent them as
discrete units of information. This enables natural language, which is also
discrete, to be jointly modelled alongside the imaging, resulting in a
description of the contents of the imaging. Here we present experiments in
applying discrete modelling techniques to the problem domain of non-melanoma
skin cancer, specifically, histological images of Intraepidermal Carcinoma
(IEC). Implementing a VQ-GAN model to reconstruct high-resolution (256x256)
images of IEC images, we trained a sequence-to-sequence transformer to generate
natural language descriptions using pathologist terminology. Combined with the
idea of interactive concept vectors available by using continuous generative
methods, we demonstrate an additional angle of interpretability. The result is
a promising means of working towards highly expressive machine learning systems
which are not only useful as predictive/classification tools, but also means to
further our scientific understanding of disease.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust and efficient computation of retinal fractal dimension through deep approximation. (arXiv:2207.05757v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05757">
<div class="article-summary-box-inner">
<span><p>A retinal trait, or phenotype, summarises a specific aspect of a retinal
image in a single number. This can then be used for further analyses, e.g. with
statistical methods. However, reducing an aspect of a complex image to a
single, meaningful number is challenging. Thus, methods for calculating retinal
traits tend to be complex, multi-step pipelines that can only be applied to
high quality images. This means that researchers often have to discard
substantial portions of the available data. We hypothesise that such pipelines
can be approximated with a single, simpler step that can be made robust to
common quality issues. We propose Deep Approximation of Retinal Traits (DART)
where a deep neural network is used predict the output of an existing pipeline
on high quality images from synthetically degraded versions of these images. We
demonstrate DART on retinal Fractal Dimension (FD) calculated by VAMPIRE, using
retinal images from UK Biobank that previous work identified as high quality.
Our method shows very high agreement with FD VAMPIRE on unseen test images
(Pearson r=0.9572). Even when those images are severely degraded, DART can
still recover an FD estimate that shows good agreement with FD VAMPIRE obtained
from the original images (Pearson r=0.8817). This suggests that our method
could enable researchers to discard fewer images in the future. Our method can
compute FD for over 1,000img/s using a single GPU. We consider these to be very
encouraging initial results and hope to develop this approach into a useful
tool for retinal analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Gap Estimation for Source Free Unsupervised Domain Adaptation with Many Classifiers. (arXiv:2207.05785v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05785">
<div class="article-summary-box-inner">
<span><p>In theory, the success of unsupervised domain adaptation (UDA) largely relies
on domain gap estimation. However, for source free UDA, the source domain data
can not be accessed during adaptation, which poses great challenge of measuring
the domain gap. In this paper, we propose to use many classifiers to learn the
source domain decision boundaries, which provides a tighter upper bound of the
domain gap, even if both of the domain data can not be simultaneously accessed.
The source model is trained to push away each pair of classifiers whilst
ensuring the correctness of the decision boundaries. In this sense, our many
classifiers model separates the source different categories as far as possible
which induces the maximum disagreement of many classifiers in the target
domain, thus the transferable source domain knowledge is maximized. For
adaptation, the source model is adapted to maximize the agreement among pairs
of the classifiers. Thus the target features are pushed away from the decision
boundaries. Experiments on several datasets of UDA show that our approach
achieves state of the art performance among source free UDA approaches and can
even compete to source available UDA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape-Aware Masking for Inpainting in Medical Imaging. (arXiv:2207.05787v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05787">
<div class="article-summary-box-inner">
<span><p>Inpainting has recently been proposed as a successful deep learning technique
for unsupervised medical image model discovery. The masks used for inpainting
are generally independent of the dataset and are not tailored to perform on
different given classes of anatomy. In this work, we introduce a method for
generating shape-aware masks for inpainting, which aims at learning the
statistical shape prior. We hypothesize that although the variation of masks
improves the generalizability of inpainting models, the shape of the masks
should follow the topology of the organs of interest. Hence, we propose an
unsupervised guided masking approach based on an off-the-shelf inpainting model
and a superpixel over-segmentation algorithm to generate a wide range of
shape-dependent masks. Experimental results on abdominal MR image
reconstruction show the superiority of our proposed masking method over
standard methods using square-shaped or dataset of irregular shape masks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dam reservoir extraction from remote sensing imagery using tailored metric learning strategies. (arXiv:2207.05807v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05807">
<div class="article-summary-box-inner">
<span><p>Dam reservoirs play an important role in meeting sustainable development
goals and global climate targets. However, particularly for small dam
reservoirs, there is a lack of consistent data on their geographical location.
To address this data gap, a promising approach is to perform automated dam
reservoir extraction based on globally available remote sensing imagery. It can
be considered as a fine-grained task of water body extraction, which involves
extracting water areas in images and then separating dam reservoirs from
natural water bodies. We propose a novel deep neural network (DNN) based
pipeline that decomposes dam reservoir extraction into water body segmentation
and dam reservoir recognition. Water bodies are firstly separated from
background lands in a segmentation model and each individual water body is then
predicted as either dam reservoir or natural water body in a classification
model. For the former step, point-level metric learning with triplets across
images is injected into the segmentation model to address contour ambiguities
between water areas and land regions. For the latter step, prior-guided metric
learning with triplets from clusters is injected into the classification model
to optimize the image embedding space in a fine-grained level based on
reservoir clusters. To facilitate future research, we establish a benchmark
dataset with earth imagery data and human labelled reservoirs from river basins
in West Africa and India. Extensive experiments were conducted on this
benchmark in the water body segmentation task, dam reservoir recognition task,
and the joint dam reservoir extraction task. Superior performance has been
observed in the respective tasks when comparing our method with state of the
art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look-ups are not (yet) all you need for deep learning inference. (arXiv:2207.05808v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05808">
<div class="article-summary-box-inner">
<span><p>Fast approximations to matrix multiplication have the potential to
dramatically reduce the cost of neural network inference. Recent work on
approximate matrix multiplication proposed to replace costly multiplications
with table-lookups by fitting a fast hash function from training data. In this
work, we propose improvements to this previous work, targeted to the deep
learning inference setting, where one has access to both training data and
fixed (already learned) model weight matrices. We further propose a fine-tuning
procedure for accelerating entire neural networks while minimizing loss in
accuracy. Finally, we analyze the proposed method on a simple image
classification task. While we show improvements to prior work, overall
classification accuracy remains substantially diminished compared to exact
matrix multiplication. Our work, despite this negative result, points the way
towards future efforts to accelerate inner products with fast nonlinear hashing
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Earthformer: Exploring Space-Time Transformers for Earth System Forecasting. (arXiv:2207.05833v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05833">
<div class="article-summary-box-inner">
<span><p>Conventionally, Earth system (e.g., weather and climate) forecasting relies
on numerical simulation with complex physical models and are hence both
expensive in computation and demanding on domain expertise. With the explosive
growth of the spatiotemporal Earth observation data in the past decade,
data-driven models that apply Deep Learning (DL) are demonstrating impressive
potential for various Earth system forecasting tasks. The Transformer as an
emerging DL architecture, despite its broad success in other domains, has
limited adoption in this area. In this paper, we propose Earthformer, a
space-time Transformer for Earth system forecasting. Earthformer is based on a
generic, flexible and efficient space-time attention block, named Cuboid
Attention. The idea is to decompose the data into cuboids and apply
cuboid-level self-attention in parallel. These cuboids are further connected
with a collection of global vectors. We conduct experiments on the MovingMNIST
dataset and a newly proposed chaotic N-body MNIST dataset to verify the
effectiveness of cuboid attention and figure out the best design of
Earthformer. Experiments on two real-world benchmarks about precipitation
nowcasting and El Nino/Southern Oscillation (ENSO) forecasting show Earthformer
achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REZCR: A Zero-shot Character Recognition Method via Radical Extraction. (arXiv:2207.05842v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05842">
<div class="article-summary-box-inner">
<span><p>The long-tail effect is a common issue that limits the performance of deep
learning models on real-world datasets. Character image dataset development is
also affected by such unbalanced data distribution due to differences in
character usage frequency. Thus, current character recognition methods are
limited when applying to real-world datasets, in particular to the character
categories in the tail which are lacking training samples, e.g., uncommon
characters, or characters from historical documents. In this paper, we propose
a zero-shot character recognition framework via radical extraction, i.e.,
REZCR, to improve the recognition performance of few-sample character
categories, in which we exploit information on radicals, the graphical units of
characters, by decomposing and reconstructing characters following orthography.
REZCR consists of an attention-based radical information extractor (RIE) and a
knowledge graph-based character reasoner (KGR). The RIE aims to recognize
candidate radicals and their possible structural relations from character
images. The results will be fed into KGR to recognize the target character by
reasoning with a pre-designed character knowledge graph. We validate our method
on multiple datasets, REZCR shows promising experimental results, especially
for few-sample character datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wayformer: Motion Forecasting via Simple & Efficient Attention Networks. (arXiv:2207.05844v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05844">
<div class="article-summary-box-inner">
<span><p>Motion forecasting for autonomous driving is a challenging task because
complex driving scenarios result in a heterogeneous mix of static and dynamic
inputs. It is an open problem how best to represent and fuse information about
road geometry, lane connectivity, time-varying traffic light state, and history
of a dynamic set of agents and their interactions into an effective encoding.
To model this diverse set of input features, many approaches proposed to design
an equally complex system with a diverse set of modality specific modules. This
results in systems that are difficult to scale, extend, or tune in rigorous
ways to trade off quality and efficiency. In this paper, we present Wayformer,
a family of attention based architectures for motion forecasting that are
simple and homogeneous. Wayformer offers a compact model description consisting
of an attention based scene encoder and a decoder. In the scene encoder we
study the choice of early, late and hierarchical fusion of the input
modalities. For each fusion type we explore strategies to tradeoff efficiency
and quality via factorized attention or latent query attention. We show that
early fusion, despite its simplicity of construction, is not only modality
agnostic but also achieves state-of-the-art results on both Waymo Open
MotionDataset (WOMD) and Argoverse leaderboards, demonstrating the
effectiveness of our design philosophy
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Estimate External Forces of Human Motion in Video. (arXiv:2207.05845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05845">
<div class="article-summary-box-inner">
<span><p>Analyzing sports performance or preventing injuries requires capturing ground
reaction forces (GRFs) exerted by the human body during certain movements.
Standard practice uses physical markers paired with force plates in a
controlled environment, but this is marred by high costs, lengthy
implementation time, and variance in repeat experiments; hence, we propose GRF
inference from video. While recent work has used LSTMs to estimate GRFs from 2D
viewpoints, these can be limited in their modeling and representation capacity.
First, we propose using a transformer architecture to tackle the GRF from video
task, being the first to do so. Then we introduce a new loss to minimize high
impact peaks in regressed curves. We also show that pre-training and multi-task
learning on 2D-to-3D human pose estimation improves generalization to unseen
motions. And pre-training on this different task provides good initial weights
when finetuning on smaller (rarer) GRF datasets. We evaluate on LAAS Parkour
and a newly collected ForcePose dataset; we show up to 19% decrease in error
compared to prior approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpOT: Spatiotemporal Modeling for 3D Object Tracking. (arXiv:2207.05856v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05856">
<div class="article-summary-box-inner">
<span><p>3D multi-object tracking aims to uniquely and consistently identify all
mobile entities through time. Despite the rich spatiotemporal information
available in this setting, current 3D tracking methods primarily rely on
abstracted information and limited history, e.g. single-frame object bounding
boxes. In this work, we develop a holistic representation of traffic scenes
that leverages both spatial and temporal information of the actors in the
scene. Specifically, we reformulate tracking as a spatiotemporal problem by
representing tracked objects as sequences of time-stamped points and bounding
boxes over a long temporal history. At each timestamp, we improve the location
and motion estimates of our tracked objects through learned refinement over the
full sequence of object history. By considering time and space jointly, our
representation naturally encodes fundamental physical priors such as object
permanence and consistency across time. Our spatiotemporal tracking framework
achieves state-of-the-art performance on the Waymo and nuScenes benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Diffusion Priors for Accelerated MRI Reconstruction. (arXiv:2207.05876v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05876">
<div class="article-summary-box-inner">
<span><p>Deep MRI reconstruction is commonly performed with conditional models that
map undersampled data as input onto fully-sampled data as output. Conditional
models perform de-aliasing under knowledge of the accelerated imaging operator,
so they poorly generalize under domain shifts in the operator. Unconditional
models are a powerful alternative that instead learn generative image priors to
improve reliability against domain shifts. Recent diffusion models are
particularly promising given their high representational diversity and sample
quality. Nevertheless, projections through a static image prior can lead to
suboptimal performance. Here we propose a novel MRI reconstruction, AdaDiff,
based on an adaptive diffusion prior. To enable efficient image sampling, an
adversarial mapper is introduced that enables use of large diffusion steps. A
two-phase reconstruction is performed with the trained prior: a rapid-diffusion
phase that produces an initial reconstruction, and an adaptation phase where
the diffusion prior is updated to minimize reconstruction loss on acquired
k-space data. Demonstrations on multi-contrast brain MRI clearly indicate that
AdaDiff achieves superior performance to competing models in cross-domain
tasks, and superior or on par performance in within-domain tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Near Sensor Edge Computing System for Point Cloud Semantic Segmentation. (arXiv:2207.05888v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05888">
<div class="article-summary-box-inner">
<span><p>Point cloud semantic segmentation has attracted attentions due to its
robustness to light condition. This makes it an ideal semantic solution for
autonomous driving. However, considering the large computation burden and
bandwidth demanding of neural networks, putting all the computing into vehicle
Electronic Control Unit (ECU) is not efficient or practical. In this paper, we
proposed a light weighted point cloud semantic segmentation network based on
range view. Due to its simple pre-processing and standard convolution, it is
efficient when running on deep learning accelerator like DPU. Furthermore, a
near sensor computing system is built for autonomous vehicles. In this system,
a FPGA-based deep learning accelerator core (DPU) is placed next to the LiDAR
sensor, to perform point cloud pre-processing and segmentation neural network.
By leaving only the post-processing step to ECU, this solution heavily
alleviate the computation burden of ECU and consequently shortens the decision
making and vehicles reaction latency. Our semantic segmentation network
achieved 10 frame per second (fps) on Xilinx DPU with computation efficiency
42.5 GOP/W.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Spatial-Temporal Entropy Modelling for Neural Video Compression. (arXiv:2207.05894v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05894">
<div class="article-summary-box-inner">
<span><p>For neural video codec, it is critical, yet challenging, to design an
efficient entropy model which can accurately predict the probability
distribution of the quantized latent representation. However, most existing
video codecs directly use the ready-made entropy model from image codec to
encode the residual or motion, and do not fully leverage the spatial-temporal
characteristics in video. To this end, this paper proposes a powerful entropy
model which efficiently captures both spatial and temporal dependencies. In
particular, we introduce the latent prior which exploits the correlation among
the latent representation to squeeze the temporal redundancy. Meanwhile, the
dual spatial prior is proposed to reduce the spatial redundancy in a
parallel-friendly manner. In addition, our entropy model is also versatile.
Besides estimating the probability distribution, our entropy model also
generates the quantization step at spatial-channel-wise. This content-adaptive
quantization mechanism not only helps our codec achieve the smooth rate
adjustment in single model but also improves the final rate-distortion
performance by dynamic bit allocation. Experimental results show that, powered
by the proposed entropy model, our neural codec can achieve 18.2% bitrate
saving on UVG dataset when compared with H.266 (VTM) using the highest
compression ratio configuration. It makes a new milestone in the development of
neural video codec. The codes are at https://github.com/microsoft/DCVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D-CBRS: Accounting For Intra-Class Diversity in Continual Learning. (arXiv:2207.05897v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05897">
<div class="article-summary-box-inner">
<span><p>Continual learning -- accumulating knowledge from a sequence of learning
experiences -- is an important yet challenging problem. In this paradigm, the
model's performance for previously encountered instances may substantially drop
as additional data are seen. When dealing with class-imbalanced data,
forgetting is further exacerbated. Prior work has proposed replay-based
approaches which aim at reducing forgetting by intelligently storing instances
for future replay. Although Class-Balancing Reservoir Sampling (CBRS) has been
successful in dealing with imbalanced data, the intra-class diversity has not
been accounted for, implicitly assuming that each instance of a class is
equally informative. We present Diverse-CBRS (D-CBRS), an algorithm that allows
us to consider within class diversity when storing instances in the memory. Our
results show that D-CBRS outperforms state-of-the-art memory management
continual learning algorithms on data sets with considerable intra-class
diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Verifying Attention Robustness of Deep Neural Networks against Semantic Perturbations. (arXiv:2207.05902v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05902">
<div class="article-summary-box-inner">
<span><p>It is known that deep neural networks (DNNs) classify an input image by
paying particular attention to certain specific pixels; a graphical
representation of the magnitude of attention to each pixel is called a
saliency-map. Saliency-maps are used to check the validity of the
classification decision basis, e.g., it is not a valid basis for classification
if a DNN pays more attention to the background rather than the subject of an
image. Semantic perturbations can significantly change the saliency-map. In
this work, we propose the first verification method for attention robustness,
i.e., the local robustness of the changes in the saliency-map against
combinations of semantic perturbations. Specifically, our method determines the
range of the perturbation parameters (e.g., the brightness change) that
maintains the difference between the actual saliency-map change and the
expected saliency-map change below a given threshold value. Our method is based
on activation region traversals, focusing on the outermost robust boundary for
scalability on larger DNNs. Experimental results demonstrate that our method
can show the extent to which DNNs can classify with the same basis regardless
of semantic perturbations and report on performance and performance factors of
activation region traversals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Dance Synthesis via Keyframes with Transformer Controllers. (arXiv:2207.05906v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05906">
<div class="article-summary-box-inner">
<span><p>Existing keyframe-based motion synthesis mainly focuses on the generation of
cyclic actions or short-term motion, such as walking, running, and transitions
between close postures. However, these methods will significantly degrade the
naturalness and diversity of the synthesized motion when dealing with complex
and impromptu movements, e.g., dance performance and martial arts. In addition,
current research lacks fine-grained control over the generated motion, which is
essential for intelligent human-computer interaction and animation creation. In
this paper, we propose a novel keyframe-based motion generation network based
on multiple constraints, which can achieve diverse dance synthesis via learned
knowledge. Specifically, the algorithm is mainly formulated based on the
recurrent neural network (RNN) and the Transformer architecture. The backbone
of our network is a hierarchical RNN module composed of two long short-term
memory (LSTM) units, in which the first LSTM is utilized to embed the posture
information of the historical frames into a latent space, and the second one is
employed to predict the human posture for the next frame. Moreover, our
framework contains two Transformer-based controllers, which are used to model
the constraints of the root trajectory and the velocity factor respectively, so
as to better utilize the temporal context of the frames and achieve
fine-grained motion control. We verify the proposed approach on a dance dataset
containing a wide range of contemporary dance. The results of three
quantitative analyses validate the superiority of our algorithm. The video and
qualitative experimental results demonstrate that the complex motion sequences
generated by our algorithm can achieve diverse and smooth motion transitions
between keyframes, even for long-term synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Appearance-guided Attentive Self-Paced Learning for Unsupervised Salient Object Detection. (arXiv:2207.05921v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05921">
<div class="article-summary-box-inner">
<span><p>Existing Deep-Learning-based (DL-based) Unsupervised Salient Object Detection
(USOD) methods learn saliency information in images based on the prior
knowledge of traditional saliency methods and pretrained deep networks.
However, these methods employ a simple learning strategy to train deep networks
and therefore cannot properly incorporate the "hidden" information of the
training samples into the learning process. Moreover, appearance information,
which is crucial for segmenting objects, is only used as post-process after the
network training process. To address these two issues, we propose a novel
appearance-guided attentive self-paced learning framework for unsupervised
salient object detection. The proposed framework integrates both self-paced
learning (SPL) and appearance guidance into a unified learning framework.
Specifically, for the first issue, we propose an Attentive Self-Paced Learning
(ASPL) paradigm that organizes the training samples in a meaningful order to
excavate gradually more detailed saliency information. Our ASPL facilitates our
framework capable of automatically producing soft attention weights that
measure the learning difficulty of training samples in a purely self-learning
way. For the second issue, we propose an Appearance Guidance Module (AGM),
which formulates the local appearance contrast of each pixel as the probability
of saliency boundary and finds the potential boundary of the target objects by
maximizing the probability. Furthermore, we further extend our framework to
other multi-modality SOD tasks by aggregating the appearance vectors of other
modality data, such as depth map, thermal image or optical flow. Extensive
experiments on RGB, RGB-D, RGB-T and video SOD benchmarks prove that our
framework achieves state-of-the-art performance against existing USOD methods
and is comparable to the latest supervised SOD methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rapid Person Re-Identification via Sub-space Consistency Regularization. (arXiv:2207.05933v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05933">
<div class="article-summary-box-inner">
<span><p>Person Re-Identification (ReID) matches pedestrians across disjoint cameras.
Existing ReID methods adopting real-value feature descriptors have achieved
high accuracy, but they are low in efficiency due to the slow Euclidean
distance computation as well as complex quick-sort algorithms. Recently, some
works propose to yield binary encoded person descriptors which instead only
require fast Hamming distance computation and simple counting-sort algorithms.
However, the performances of such binary encoded descriptors, especially with
short code (e.g., 32 and 64 bits), are hardly satisfactory given the sparse
binary space. To strike a balance between the model accuracy and efficiency, we
propose a novel Sub-space Consistency Regularization (SCR) algorithm that can
speed up the ReID procedure by $0.25$ times than real-value features under the
same dimensions whilst maintaining a competitive accuracy, especially under
short codes. SCR transforms real-value features vector (e.g., 2048 float32)
with short binary codes (e.g., 64 bits) by first dividing real-value features
vector into $M$ sub-spaces, each with $C$ clustered centroids. Thus the
distance between two samples can be expressed as the summation of the
respective distance to the centroids, which can be sped up by offline
calculation and maintained via a look-up table. On the other side, these
real-value centroids help to achieve significantly higher accuracy than using
binary code. Lastly, we convert the distance look-up table to be integer and
apply the counting-sort algorithm to speed up the ranking stage.
</p>
<p>We also propose a novel consistency regularization with an iterative
framework. Experimental results on Market-1501 and DukeMTMC-reID show promising
and exciting results. Under short code, our proposed SCR enjoys
Real-value-level accuracy and Hashing-level speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of the motion of chest internal points using a recurrent neural network trained with real-time recurrent learning for latency compensation in lung cancer radiotherapy. (arXiv:2207.05951v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05951">
<div class="article-summary-box-inner">
<span><p>During the radiotherapy treatment of patients with lung cancer, the radiation
delivered to healthy tissue around the tumor needs to be minimized, which is
difficult because of respiratory motion and the latency of linear accelerator
systems. In the proposed study, we first use the Lucas-Kanade pyramidal optical
flow algorithm to perform deformable image registration of chest computed
tomography scan images of four patients with lung cancer. We then track three
internal points close to the lung tumor based on the previously computed
deformation field and predict their position with a recurrent neural network
(RNN) trained using real-time recurrent learning (RTRL) and gradient clipping.
The breathing data is quite regular, sampled at approximately 2.5Hz, and
includes artificial drift in the spine direction. The amplitude of the motion
of the tracked points ranged from 12.0mm to 22.7mm. Finally, we propose a
simple method for recovering and predicting 3D tumor images from the tracked
points and the initial tumor image based on a linear correspondence model and
Nadaraya-Watson non-linear regression. The root-mean-square error, maximum
error, and jitter corresponding to the RNN prediction on the test set were
smaller than the same performance measures obtained with linear prediction and
least mean squares (LMS). In particular, the maximum prediction error
associated with the RNN, equal to 1.51mm, is respectively 16.1% and 5.0% lower
than the maximum error associated with linear prediction and LMS. The average
prediction time per time step with RTRL is equal to 119ms, which is less than
the 400ms marker position sampling time. The tumor position in the predicted
images appears visually correct, which is confirmed by the high mean
cross-correlation between the original and predicted images, equal to 0.955.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Orthogonal-Coding-Based Feature Generation for Transductive Open-Set Recognition via Dual-Space Consistent Sampling. (arXiv:2207.05957v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05957">
<div class="article-summary-box-inner">
<span><p>Open-set recognition (OSR) aims to simultaneously detect unknown-class
samples and classify known-class samples. Most of the existing OSR methods are
inductive methods, which generally suffer from the domain shift problem that
the learned model from the known-class domain might be unsuitable for the
unknown-class domain. Addressing this problem, inspired by the success of
transductive learning for alleviating the domain shift problem in many other
visual tasks, we propose an Iterative Transductive OSR framework, called
IT-OSR, which implements three explored modules iteratively, including a
reliability sampling module, a feature generation module, and a baseline update
module. Specifically, at each iteration, a dual-space consistent sampling
approach is presented in the explored reliability sampling module for selecting
some relatively more reliable ones from the test samples according to their
pseudo labels assigned by a baseline method, which could be an arbitrary
inductive OSR method. Then, a conditional dual-adversarial generative network
under an orthogonal coding condition is designed in the feature generation
module to generate discriminative sample features of both known and unknown
classes according to the selected test samples with their pseudo labels.
Finally, the baseline method is updated for sample re-prediction in the
baseline update module by jointly utilizing the generated features, the
selected test samples with pseudo labels, and the training samples. Extensive
experimental results on both the standard-dataset and the cross-dataset
settings demonstrate that the derived transductive methods, by introducing two
typical inductive OSR methods into the proposed IT-OSR framework, achieve
better performances than 15 state-of-the-art methods in most cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A new database of Houma Alliance Book ancient handwritten characters and its baseline algorithm. (arXiv:2207.05993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05993">
<div class="article-summary-box-inner">
<span><p>The Houma Alliance Book is one of the national treasures of the Museum in
Shanxi Museum Town in China. It has great historical significance in
researching ancient history. To date, the research on the Houma Alliance Book
has been staying in the identification of paper documents, which is inefficient
to identify and difficult to display, study and publicize. Therefore, the
digitization of the recognized ancient characters of Houma League can
effectively improve the efficiency of recognizing ancient characters and
provide more reliable technical support and text data. This paper proposes a
new database of Houma Alliance Book ancient handwritten characters and a
multi-modal fusion method to recognize ancient handwritten characters. In the
database, 297 classes and 3,547 samples of Houma Alliance ancient handwritten
characters are collected from the original book collection and by human
imitative writing. Furthermore, the decision-level classifier fusion strategy
is applied to fuse three well-known deep neural network architectures for
ancient handwritten character recognition. Experiments are performed on our new
database. The experimental results first provide the baseline result of the new
database to the research community and then demonstrate the efficiency of our
proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition. (arXiv:2207.06020v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06020">
<div class="article-summary-box-inner">
<span><p>This paper focuses on designing a noise-robust end-to-end Audio-Visual Speech
Recognition (AVSR) system. To this end, we propose Visual Context-driven Audio
Feature Enhancement module (V-CAFE) to enhance the input noisy audio speech
with a help of audio-visual correspondence. The proposed V-CAFE is designed to
capture the transition of lip movements, namely visual context and to generate
a noise reduction mask by considering the obtained visual context. Through
context-dependent modeling, the ambiguity in viseme-to-phoneme mapping can be
refined for mask generation. The noisy representations are masked out with the
noise reduction mask resulting in enhanced audio features. The enhanced audio
features are fused with the visual features and taken to an encoder-decoder
model composed of Conformer and Transformer for speech recognition. We show the
proposed end-to-end AVSR with the V-CAFE can further improve the
noise-robustness of AVSR. The effectiveness of the proposed method is evaluated
in noisy speech recognition and overlapped speech recognition experiments using
the two largest audio-visual datasets, LRS2 and LRS3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perturbation Inactivation Based Adversarial Defense for Face Recognition. (arXiv:2207.06035v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06035">
<div class="article-summary-box-inner">
<span><p>Deep learning-based face recognition models are vulnerable to adversarial
attacks. To curb these attacks, most defense methods aim to improve the
robustness of recognition models against adversarial perturbations. However,
the generalization capacities of these methods are quite limited. In practice,
they are still vulnerable to unseen adversarial attacks. Deep learning models
are fairly robust to general perturbations, such as Gaussian noises. A
straightforward approach is to inactivate the adversarial perturbations so that
they can be easily handled as general perturbations. In this paper, a
plug-and-play adversarial defense method, named perturbation inactivation
(PIN), is proposed to inactivate adversarial perturbations for adversarial
defense. We discover that the perturbations in different subspaces have
different influences on the recognition model. There should be a subspace,
called the immune space, in which the perturbations have fewer adverse impacts
on the recognition model than in other subspaces. Hence, our method estimates
the immune space and inactivates the adversarial perturbations by restricting
them to this subspace. The proposed method can be generalized to unseen
adversarial perturbations since it does not rely on a specific kind of
adversarial attack method. This approach not only outperforms several
state-of-the-art adversarial defense methods but also demonstrates a superior
generalization capacity through exhaustive experiments. Moreover, the proposed
method can be successfully applied to four commercial APIs without additional
training, indicating that it can be easily generalized to existing face
recognition systems. The source code is available at
https://github.com/RenMin1991/Perturbation-Inactivate
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experiments on Anomaly Detection in Autonomous Driving by Forward-Backward Style Transfers. (arXiv:2207.06055v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06055">
<div class="article-summary-box-inner">
<span><p>Great progress has been achieved in the community of autonomous driving in
the past few years. As a safety-critical problem, however, anomaly detection is
a huge hurdle towards a large-scale deployment of autonomous vehicles in the
real world. While many approaches, such as uncertainty estimation or
segmentation-based image resynthesis, are extremely promising, there is more to
be explored. Especially inspired by works on anomaly detection based on image
resynthesis, we propose a novel approach for anomaly detection through style
transfer. We leverage generative models to map an image from its original style
domain of road traffic to an arbitrary one and back to generate pixelwise
anomaly scores. However, our experiments have proven our hypothesis wrong, and
we were unable to produce significant results. Nevertheless, we want to share
our findings, so that others can learn from our experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras. (arXiv:2207.06058v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06058">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates a visual SLAM system that utilizes point and line
cloud for robust camera localization, simultaneously, with an embedded
piece-wise planar reconstruction (PPR) module which in all provides a
structural map. To build a scale consistent map in parallel with tracking, such
as employing a single camera brings the challenge of reconstructing geometric
primitives with scale ambiguity, and further introduces the difficulty in graph
optimization of bundle adjustment (BA). We address these problems by proposing
several run-time optimizations on the reconstructed lines and planes. The
system is then extended with depth and stereo sensors based on the design of
the monocular framework. The results show that our proposed SLAM tightly
incorporates the semantic features to boost both frontend tracking as well as
backend optimization. We evaluate our system exhaustively on various datasets,
and open-source our code for the community
(https://github.com/PeterFWS/Structure-PLP-SLAM).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pyramid Transformer for Traffic Sign Detection. (arXiv:2207.06067v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06067">
<div class="article-summary-box-inner">
<span><p>Traffic sign detection is a vital task in the visual system of self-driving
cars and the automated driving system. Recently, novel Transformer-based models
have achieved encouraging results for various computer vision tasks. We still
observed that vanilla ViT could not yield satisfactory results in traffic sign
detection because the overall size of the datasets is very small and the class
distribution of traffic signs is extremely unbalanced. To overcome this
problem, a novel Pyramid Transformer with locality mechanisms is proposed in
this paper. Specifically, Pyramid Transformer has several spatial pyramid
reduction layers to shrink and embed the input image into tokens with rich
multi-scale context by using atrous convolutions. Moreover, it inherits an
intrinsic scale invariance inductive bias and is able to learn local feature
representation for objects at various scales, thereby enhancing the network
robustness against the size discrepancy of traffic signs. The experiments are
conducted on the German Traffic Sign Detection Benchmark (GTSDB). The results
demonstrate the superiority of the proposed model in the traffic sign detection
tasks. More specifically, Pyramid Transformer achieves 75.6% mAP in GTSDB when
applied to the Cascade RCNN as the backbone and surpassing most well-known and
widely used SOTAs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSPNet: Towards Slimmable Pretrained Networks based on Discriminative Self-supervised Learning. (arXiv:2207.06075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06075">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) has achieved promising downstream performance.
However, when facing various resource budgets in real-world applications, it
costs a huge computation burden to pretrain multiple networks of various sizes
one by one. In this paper, we propose Discriminative-SSL-based Slimmable
Pretrained Networks (DSPNet), which can be trained at once and then slimmed to
multiple sub-networks of various sizes, each of which faithfully learns good
representation and can serve as good initialization for downstream tasks with
various resource budgets. Specifically, we extend the idea of slimmable
networks to a discriminative SSL paradigm, by integrating SSL and knowledge
distillation gracefully. We show comparable or improved performance of DSPNet
on ImageNet to the networks individually pretrained one by one under the linear
evaluation and semi-supervised evaluation protocols, while reducing large
training cost. The pretrained models also generalize well on downstream
detection and segmentation tasks. Code will be made public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiStream: A Simple and Fast Multiple Cameras Visual Monitor and Directly Streaming. (arXiv:2207.06078v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06078">
<div class="article-summary-box-inner">
<span><p>Monitoring and streaming is one of the most important applications for the
real time cameras. The research of this has provided a novel design idea that
uses the FFmpeg and Tkinter, combining with the libraries: OpenCV and PIL to
develop a simple but fast streaming toolkit MultiSteam that can achieve the
function of visible monitoring streaming for multiple simultaneously.
MultiStream is able to automatically arrange the layout of the displays of
multiple camera windows and intelligently analyze the input streaming URL to
select the correct corresponding streaming communication protocol. Multiple
cameras can be streamed with different communication protocols or the same
protocol. Besides, the paper has tested the different streaming speeds for
different protocols in camera streaming. MultiStream is able to gain the
information of media equipment on the computer. The configuration information
for media-id selection and multiple cameras streaming can be saved as json
files.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teachers in concordance for pseudo-labeling of 3D sequential data. (arXiv:2207.06079v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06079">
<div class="article-summary-box-inner">
<span><p>Automatic pseudo-labeling is a powerful tool to tap into large amounts of
sequential unlabeled data. It is especially appealing in safety-critical
applications of autonomous driving where performance requirements are extreme,
datasets large, and manual labeling is very challenging. We propose to leverage
the sequentiality of the captures to boost the pseudo-labeling technique in a
teacher-student setup via training multiple teachers, each with access to
different temporal information. This set of teachers, dubbed Concordance,
provides higher quality pseudo-labels for the student training than standard
methods. The output of multiple teachers is combined via a novel pseudo-label
confidence-guided criterion. Our experimental evaluation focuses on the 3D
point cloud domain in urban driving scenarios. We show the performance of our
method applied to multiple model architectures with tasks of 3D semantic
segmentation and 3D object detection on two benchmark datasets. Our method,
using only 20% of manual labels, outperforms some of the fully supervised
methods. Special performance boost is achieved for classes rarely appearing in
the training data, e.g., bicycles and pedestrians. The implementation of our
approach is publicly available at https://github.com/ctu-vras/T-Concord3D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Ranking for Object Image Blur Assessment. (arXiv:2207.06085v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06085">
<div class="article-summary-box-inner">
<span><p>Assessing the blurriness of an object image is fundamentally important to
improve the performance for object recognition and retrieval. The main
challenge lies in the lack of abundant images with reliable labels and
effective learning strategies. Current datasets are labeled with limited and
confused quality levels. To overcome this limitation, we propose to label the
rank relationships between pairwise images rather their quality levels, since
it is much easier for humans to label, and establish a large-scale realistic
face image blur assessment dataset with reliable labels. Based on this dataset,
we propose a method to obtain the blur scores only with the pairwise rank
labels as supervision. Moreover, to further improve the performance, we propose
a self-supervised method based on quadruplet ranking consistency to leverage
the unlabeled data more effectively. The supervised and self-supervised methods
constitute a final semi-supervised learning framework, which can be trained
end-to-end. Experimental results demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliminating Gradient Conflict in Reference-based Line-art Colorization. (arXiv:2207.06095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06095">
<div class="article-summary-box-inner">
<span><p>Reference-based line-art colorization is a challenging task in computer
vision. The color, texture, and shading are rendered based on an abstract
sketch, which heavily relies on the precise long-range dependency modeling
between the sketch and reference. Popular techniques to bridge the cross-modal
information and model the long-range dependency employ the attention mechanism.
However, in the context of reference-based line-art colorization, several
techniques would intensify the existing training difficulty of attention, for
instance, self-supervised training protocol and GAN-based losses. To understand
the instability in training, we detect the gradient flow of attention and
observe gradient conflict among attention branches. This phenomenon motivates
us to alleviate the gradient issue by preserving the dominant gradient branch
while removing the conflict ones. We propose a novel attention mechanism using
this training strategy, Stop-Gradient Attention (SGA), outperforming the
attention baseline by a large margin with better training stability. Compared
with state-of-the-art modules in line-art colorization, our approach
demonstrates significant improvements in Fr\'echet Inception Distance (FID, up
to 27.21%) and structural similarity index measure (SSIM, up to 25.67%) on
several benchmarks. The code of SGA is available at
https://github.com/kunkun0w0/SGA .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global-local Motion Transformer for Unsupervised Skeleton-based Action Learning. (arXiv:2207.06101v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06101">
<div class="article-summary-box-inner">
<span><p>We propose a new transformer model for the task of unsupervised learning of
skeleton motion sequences. The existing transformer model utilized for
unsupervised skeleton-based action learning is learned the instantaneous
velocity of each joint from adjacent frames without global motion information.
Thus, the model has difficulties in learning the attention globally over
whole-body motions and temporally distant joints. In addition, person-to-person
interactions have not been considered in the model. To tackle the learning of
whole-body motion, long-range temporal dynamics, and person-to-person
interactions, we design a global and local attention mechanism, where, global
body motions and local joint motions pay attention to each other. In addition,
we propose a novel pretraining strategy, multi-interval pose displacement
prediction, to learn both global and local attention in diverse time ranges.
The proposed model successfully learns local dynamics of the joints and
captures global context from the motion sequences. Our model outperforms
state-of-the-art models by notable margins in the representative benchmarks.
Codes are available at https://github.com/Boeun-Kim/GL-Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnability Enhancement for Low-light Raw Denoising: Where Paired Real Data Meets Noise Modeling. (arXiv:2207.06103v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06103">
<div class="article-summary-box-inner">
<span><p>Low-light raw denoising is an important and valuable task in computational
photography where learning-based methods trained with paired real data are
mainstream. However, the limited data volume and complicated noise distribution
have constituted a learnability bottleneck for paired real data, which limits
the denoising performance of learning-based methods. To address this issue, we
present a learnability enhancement strategy to reform paired real data
according to noise modeling. Our strategy consists of two efficient techniques:
shot noise augmentation (SNA) and dark shading correction (DSC). Through noise
model decoupling, SNA improves the precision of data mapping by increasing the
data volume and DSC reduces the complexity of data mapping by reducing the
noise complexity. Extensive results on the public datasets and real imaging
scenarios collectively demonstrate the state-of-the-art performance of our
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Detection of Label Errors in Semantic Segmentation Datasets via Deep Learning and Uncertainty Quantification. (arXiv:2207.06104v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06104">
<div class="article-summary-box-inner">
<span><p>In this work, we for the first time present a method for detecting label
errors in image datasets with semantic segmentation, i.e., pixel-wise class
labels. Annotation acquisition for semantic segmentation datasets is
time-consuming and requires plenty of human labor. In particular, review
processes are time consuming and label errors can easily be overlooked by
humans. The consequences are biased benchmarks and in extreme cases also
performance degradation of deep neural networks (DNNs) trained on such
datasets. DNNs for semantic segmentation yield pixel-wise predictions, which
makes detection of label errors via uncertainty quantification a complex task.
Uncertainty is particularly pronounced at the transitions between connected
components of the prediction. By lifting the consideration of uncertainty to
the level of predicted components, we enable the usage of DNNs together with
component-level uncertainty quantification for the detection of label errors.
We present a principled approach to benchmarking the task of label error
detection by dropping labels from the Cityscapes dataset as well from a dataset
extracted from the CARLA driving simulator, where in the latter case we have
the labels under control. Our experiments show that our approach is able to
detect the vast majority of label errors while controlling the number of false
label error detections. Furthermore, we apply our method to semantic
segmentation datasets frequently used by the computer vision community and
present a collection of label errors along with sample statistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation. (arXiv:2207.06124v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06124">
<div class="article-summary-box-inner">
<span><p>One key challenge of exemplar-guided image generation lies in establishing
fine-grained correspondences between input and guided images. Prior approaches,
despite the promising results, have relied on either estimating dense attention
to compute per-point matching, which is limited to only coarse scales due to
the quadratic memory cost, or fixing the number of correspondences to achieve
linear complexity, which lacks flexibility. In this paper, we propose a dynamic
sparse attention based Transformer model, termed Dynamic Sparse Transformer
(DynaST), to achieve fine-level matching with favorable efficiency. The heart
of our approach is a novel dynamic-attention unit, dedicated to covering the
variation on the optimal number of tokens one position should focus on.
Specifically, DynaST leverages the multi-layer nature of Transformer structure,
and performs the dynamic attention scheme in a cascaded manner to refine
matching results and synthesize visually-pleasing outputs. In addition, we
introduce a unified training objective for DynaST, making it a versatile
reference-based image translation framework for both supervised and
unsupervised scenarios. Extensive experiments on three applications,
pose-guided person image generation, edge-based face synthesis, and undistorted
image style transfer, demonstrate that DynaST achieves superior performance in
local details, outperforming the state of the art while reducing the
computational cost significantly. Our code is available at
https://github.com/Huage001/DynaST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust and accurate depth estimation by fusing LiDAR and Stereo. (arXiv:2207.06139v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06139">
<div class="article-summary-box-inner">
<span><p>Depth estimation is one of the key technologies in some fields such as
autonomous driving and robot navigation. However, the traditional method of
using a single sensor is inevitably limited by the performance of the sensor.
Therefore, a precision and robust method for fusing the LiDAR and stereo
cameras is proposed. This method fully combines the advantages of the LiDAR and
stereo camera, which can retain the advantages of the high precision of the
LiDAR and the high resolution of images respectively. Compared with the
traditional stereo matching method, the texture of the object and lighting
conditions have less influence on the algorithm. Firstly, the depth of the
LiDAR data is converted to the disparity of the stereo camera. Because the
density of the LiDAR data is relatively sparse on the y-axis, the converted
disparity map is up-sampled using the interpolation method. Secondly, in order
to make full use of the precise disparity map, the disparity map and stereo
matching are fused to propagate the accurate disparity. Finally, the disparity
map is converted to the depth map. Moreover, the converted disparity map can
also increase the speed of the algorithm. We evaluate the proposed pipeline on
the KITTI benchmark. The experiment demonstrates that our algorithm has higher
accuracy than several classic methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating the Power Consumption of Heterogeneous Devices when performing AI Inference. (arXiv:2207.06150v1 [cs.AR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06150">
<div class="article-summary-box-inner">
<span><p>Modern-day life is driven by electronic devices connected to the internet.
The emerging research field of the Internet-of-Things (IoT) has become popular,
just as there has been a steady increase in the number of connected devices -
now over 50 billion. Since many of these devices are utilised to perform
\gls*{cv} tasks, it is essential to understand their power consumption against
performance. We report the power consumption profile and analysis of the NVIDIA
Jetson Nano board while performing object classification. The authors present
an extensive analysis regarding power consumption per frame and the output in
frames per second (FPS) using YOLOv5 models. The results show that the YOLOv5n
outperforms other YOLOV5 variants in terms of throughput (i.e. 12.34 fps) and
low power consumption (i.e. 0.154 mWh/frame).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comparison between PMBM Bayesian track initiation and labelled RFS adaptive birth. (arXiv:2207.06156v1 [stat.AP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06156">
<div class="article-summary-box-inner">
<span><p>This paper provides a comparative analysis between the adaptive birth model
used in the labelled random finite set literature and the track initiation in
the Poisson multi-Bernoulli mixture (PMBM) filter, with point-target models.
The PMBM track initiation is obtained via Bayes' rule applied on the predicted
PMBM density, and creates one Bernoulli component for each received
measurement, representing that this measurement may be clutter or a detection
from a new target. Adaptive birth mimics this procedure by creating a Bernoulli
component for each measurement using a different rule to determine the
probability of existence and a user-defined single-target density. This paper
first provides an analysis of the differences that arise in track initiation
based on isolated measurements. Then, it shows that adaptive birth
underestimates the number of objects present in the surveillance area under
common modelling assumptions. Finally, we provide numerical simulations to
further illustrate the differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Visual Representation Learning by Synchronous Momentum Grouping. (arXiv:2207.06167v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06167">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a genuine group-level contrastive visual
representation learning method whose linear evaluation performance on ImageNet
surpasses the vanilla supervised learning. Two mainstream unsupervised learning
schemes are the instance-level contrastive framework and clustering-based
schemes. The former adopts the extremely fine-grained instance-level
discrimination whose supervisory signal is not efficient due to the false
negatives. Though the latter solves this, they commonly come with some
restrictions affecting the performance. To integrate their advantages, we
design the SMoG method. SMoG follows the framework of contrastive learning but
replaces the contrastive unit from instance to group, mimicking
clustering-based methods. To achieve this, we propose the momentum grouping
scheme which synchronously conducts feature grouping with representation
learning. In this way, SMoG solves the problem of supervisory signal hysteresis
which the clustering-based method usually faces, and reduces the false
negatives of instance contrastive methods. We conduct exhaustive experiments to
show that SMoG works well on both CNN and Transformer backbones. Results prove
that SMoG has surpassed the current SOTA unsupervised representation learning
methods. Moreover, its linear evaluation results surpass the performances
obtained by vanilla supervised learning and the representation can be well
transferred to downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MRF-UNets: Searching UNet with Markov Random Fields. (arXiv:2207.06168v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06168">
<div class="article-summary-box-inner">
<span><p>UNet [27] is widely used in semantic segmentation due to its simplicity and
effectiveness. However, its manually-designed architecture is applied to a
large number of problem settings, either with no architecture optimizations, or
with manual tuning, which is time consuming and can be sub-optimal. In this
work, firstly, we propose Markov Random Field Neural Architecture Search
(MRF-NAS) that extends and improves the recent Adaptive and Optimal Network
Width Search (AOWS) method [4] with (i) a more general MRF framework (ii)
diverse M-best loopy inference (iii) differentiable parameter learning. This
provides the necessary NAS framework to efficiently explore network
architectures that induce loopy inference graphs, including loops that arise
from skip connections. With UNet as the backbone, we find an architecture,
MRF-UNet, that shows several interesting characteristics. Secondly, through the
lens of these characteristics, we identify the sub-optimality of the original
UNet architecture and further improve our results with MRF-UNetV2. Experiments
show that our MRF-UNets significantly outperform several benchmarks on three
aerial image datasets and two medical image datasets while maintaining low
computational costs. The code is available at:
https://github.com/zifuwanggg/MRF-UNets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RTN: Reinforced Transformer Network for Coronary CT Angiography Vessel-level Image Quality Assessment. (arXiv:2207.06177v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06177">
<div class="article-summary-box-inner">
<span><p>Coronary CT Angiography (CCTA) is susceptible to various distortions (e.g.,
artifacts and noise), which severely compromise the exact diagnosis of
cardiovascular diseases. The appropriate CCTA Vessel-level Image Quality
Assessment (CCTA VIQA) algorithm can be used to reduce the risk of error
diagnosis. The primary challenges of CCTA VIQA are that the local part of
coronary that determines final quality is hard to locate. To tackle the
challenge, we formulate CCTA VIQA as a multiple-instance learning (MIL)
problem, and exploit Transformer-based MIL backbone (termed as T-MIL) to
aggregate the multiple instances along the coronary centerline into the final
quality. However, not all instances are informative for final quality. There
are some quality-irrelevant/negative instances intervening the exact quality
assessment(e.g., instances covering only background or the coronary in
instances is not identifiable). Therefore, we propose a Progressive
Reinforcement learning based Instance Discarding module (termed as PRID) to
progressively remove quality-irrelevant/negative instances for CCTA VIQA. Based
on the above two modules, we propose a Reinforced Transformer Network (RTN) for
automatic CCTA VIQA based on end-to-end optimization. Extensive experimental
results demonstrate that our proposed method achieves the state-of-the-art
performance on the real-world CCTA dataset, exceeding previous MIL methods by a
large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Depression Estimation based on Sub-attentional Fusion. (arXiv:2207.06180v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06180">
<div class="article-summary-box-inner">
<span><p>Failure to timely diagnose and effectively treat depression leads to over 280
million people suffering from this psychological disorder worldwide. The
information cues of depression can be harvested from diverse heterogeneous
resources, e.g., audio, visual, and textual data, raising demand for new
effective multi-modal fusion approaches for its automatic estimation. In this
work, we tackle the task of automatically identifying depression from
multi-modal data and introduce a sub-attention mechanism for linking
heterogeneous information while leveraging Convolutional Bidirectional LSTM as
our backbone. To validate this idea, we conduct extensive experiments on the
public DAIC-WOZ benchmark for depression assessment featuring different
evaluation modes and taking gender-specific biases into account. The proposed
model yields effective results with 0.89 precision and 0.70 F1-score in
detecting major depression and 4.92 MAE in estimating the severity. Our
attention-based fusion module consistently outperforms conventional late fusion
approaches and achieves a competitive performance compared to the previously
published depression estimation frameworks, while learning to diagnose the
disorder end-to-end and relying on far less preprocessing steps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Quantization Embeddings for Intra-Subject Prostate MR Image Registration. (arXiv:2207.06189v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06189">
<div class="article-summary-box-inner">
<span><p>Image registration is useful for quantifying morphological changes in
longitudinal MR images from prostate cancer patients. This paper describes a
development in improving the learning-based registration algorithms, for this
challenging clinical application often with highly variable yet limited
training data. First, we report that the latent space can be clustered into a
much lower dimensional space than that commonly found as bottleneck features at
the deep layer of a trained registration network. Based on this observation, we
propose a hierarchical quantization method, discretizing the learned feature
vectors using a jointly-trained dictionary with a constrained size, in order to
improve the generalisation of the registration networks. Furthermore, a novel
collaborative dictionary is independently optimised to incorporate additional
prior information, such as the segmentation of the gland or other regions of
interest, in the latent quantized space. Based on 216 real clinical images from
86 prostate cancer patients, we show the efficacy of both the designed
components. Improved registration accuracy was obtained with statistical
significance, in terms of both Dice on gland and target registration error on
corresponding landmarks, the latter of which achieved 5.46 mm, an improvement
of 28.7\% from the baseline without quantization. Experimental results also
show that the difference in performance was indeed minimised between training
and testing data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain adaptation strategies for cancer-independent detection of lymph node metastases. (arXiv:2207.06193v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06193">
<div class="article-summary-box-inner">
<span><p>Recently, large, high-quality public datasets have led to the development of
convolutional neural networks that can detect lymph node metastases of breast
cancer at the level of expert pathologists. Many cancers, regardless of the
site of origin, can metastasize to lymph nodes. However, collecting and
annotating high-volume, high-quality datasets for every cancer type is
challenging. In this paper we investigate how to leverage existing high-quality
datasets most efficiently in multi-task settings for closely related tasks.
Specifically, we will explore different training and domain adaptation
strategies, including prevention of catastrophic forgetting, for colon and
head-and-neck cancer metastasis detection in lymph nodes.
</p>
<p>Our results show state-of-the-art performance on both cancer metastasis
detection tasks. Furthermore, we show the effectiveness of repeated adaptation
of networks from one cancer type to another to obtain multi-task metastasis
detection networks. Last, we show that leveraging existing high-quality
datasets can significantly boost performance on new target tasks and that
catastrophic forgetting can be effectively mitigated using regularization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarially-Aware Robust Object Detector. (arXiv:2207.06202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06202">
<div class="article-summary-box-inner">
<span><p>Object detection, as a fundamental computer vision task, has achieved a
remarkable progress with the emergence of deep neural networks. Nevertheless,
few works explore the adversarial robustness of object detectors to resist
adversarial attacks for practical applications in various real-world scenarios.
Detectors have been greatly challenged by unnoticeable perturbation, with sharp
performance drop on clean images and extremely poor performance on adversarial
images. In this work, we empirically explore the model training for adversarial
robustness in object detection, which greatly attributes to the conflict
between learning clean images and adversarial images. To mitigate this issue,
we propose a Robust Detector (RobustDet) based on adversarially-aware
convolution to disentangle gradients for model learning on clean and
adversarial images. RobustDet also employs the Adversarial Image Discriminator
(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable
robustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that
our model effectively disentangles gradients and significantly enhances the
detection robustness with maintaining the detection ability on clean images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trans4Map: Revisiting Holistic Top-down Mapping from Egocentric Images to Allocentric Semantics with Vision Transformers. (arXiv:2207.06205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06205">
<div class="article-summary-box-inner">
<span><p>Humans have an innate ability to sense their surroundings, as they can
extract the spatial representation from the egocentric perception and form an
allocentric semantic map via spatial transformation and memory updating.
However, endowing mobile agents with such a spatial sensing ability is still a
challenge, due to two difficulties: (1) the previous convolutional models are
limited by the local receptive field, thus, struggling to capture holistic
long-range dependencies during observation; (2) the excessive computational
budgets required for success, often lead to a separation of the mapping
pipeline into stages, resulting the entire mapping process inefficient. To
address these issues, we propose an end-to-end one-stage Transformer-based
framework for Mapping, termed Trans4Map. Our egocentric-to-allocentric mapping
process includes three steps: (1) the efficient transformer extracts the
contextual features from a batch of egocentric images; (2) the proposed
Bidirectional Allocentric Memory (BAM) module projects egocentric features into
the allocentric memory; (3) the map decoder parses the accumulated memory and
predicts the top-down semantic segmentation map. In contrast, Trans4Map
achieves state-of-the-art results, reducing 67.2% parameters, yet gaining a
+3.25% mIoU and a +4.09% mBF1 improvements on the Matterport3D dataset. Code
will be made publicly available at https://github.com/jamycheung/Trans4Map.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sample-dependent Adaptive Temperature Scaling for Improved Calibration. (arXiv:2207.06211v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06211">
<div class="article-summary-box-inner">
<span><p>It is now well known that neural networks can be wrong with high confidence
in their predictions, leading to poor calibration. The most common post-hoc
approach to compensate for this is to perform temperature scaling, which
adjusts the confidences of the predictions on any input by scaling the logits
by a fixed value. Whilst this approach typically improves the average
calibration across the whole test dataset, this improvement typically reduces
the individual confidences of the predictions irrespective of whether the
classification of a given input is correct or incorrect. With this insight, we
base our method on the observation that different samples contribute to the
calibration error by varying amounts, with some needing to increase their
confidence and others needing to decrease it. Therefore, for each input, we
propose to predict a different temperature value, allowing us to adjust the
mismatch between confidence and accuracy at a finer granularity. Furthermore,
we observe improved results on OOD detection and can also extract a notion of
hardness for the data-points. Our method is applied post-hoc, consequently
using very little computation time and with a negligible memory footprint and
is applied to off-the-shelf pre-trained classifiers. We test our method on the
ResNet50 and WideResNet28-10 architectures using the CIFAR10/100 and
Tiny-ImageNet datasets, showing that producing per-data-point temperatures is
beneficial also for the expected calibration error across the whole test set.
Code is available at: https://github.com/thwjoy/adats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is one annotation enough? A data-centric image classification benchmark for noisy and ambiguous label estimation. (arXiv:2207.06214v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06214">
<div class="article-summary-box-inner">
<span><p>High-quality data is necessary for modern machine learning. However, the
acquisition of such data is difficult due to noisy and ambiguous annotations of
humans. The aggregation of such annotations to determine the label of an image
leads to a lower data quality. We propose a data-centric image classification
benchmark with nine real-world datasets and multiple annotations per image to
investigate and quantify the impact of such data quality issues. We focus on a
data-centric perspective by asking how we could improve the data quality.
Across thousands of experiments, we show that multiple annotations allow a
better approximation of the real underlying class distribution. We identify
that hard labels can not capture the ambiguity of the data and this might lead
to the common issue of overconfident models. Based on the presented datasets,
benchmark baselines, and analysis, we create multiple research opportunities
for the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLO2U-Net: Detection-Guided 3D Instance Segmentation for Microscopy. (arXiv:2207.06215v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06215">
<div class="article-summary-box-inner">
<span><p>Microscopy imaging techniques are instrumental for characterization and
analysis of biological structures. As these techniques typically render 3D
visualization of cells by stacking 2D projections, issues such as out-of-plane
excitation and low resolution in the $z$-axis may pose challenges (even for
human experts) to detect individual cells in 3D volumes as these
non-overlapping cells may appear as overlapping. In this work, we introduce a
comprehensive method for accurate 3D instance segmentation of cells in the
brain tissue. The proposed method combines the 2D YOLO detection method with a
multi-view fusion algorithm to construct a 3D localization of the cells. Next,
the 3D bounding boxes along with the data volume are input to a 3D U-Net
network that is designed to segment the primary cell in each 3D bounding box,
and in turn, to carry out instance segmentation of cells in the entire volume.
The promising performance of the proposed method is shown in comparison with
some current deep learning-based 3D instance segmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Hard Labels: Investigating data label distributions. (arXiv:2207.06224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06224">
<div class="article-summary-box-inner">
<span><p>High-quality data is a key aspect of modern machine learning. However, labels
generated by humans suffer from issues like label noise and class ambiguities.
We raise the question of whether hard labels are sufficient to represent the
underlying ground truth distribution in the presence of these inherent
imprecision. Therefore, we compare the disparity of learning with hard and soft
labels quantitatively and qualitatively for a synthetic and a real-world
dataset. We show that the application of soft labels leads to improved
performance and yields a more regular structure of the internal feature space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entry-Flipped Transformer for Inference and Prediction of Participant Behavior. (arXiv:2207.06235v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06235">
<div class="article-summary-box-inner">
<span><p>Some group activities, such as team sports and choreographed dances, involve
closely coupled interaction between participants. Here we investigate the tasks
of inferring and predicting participant behavior, in terms of motion paths and
actions, under such conditions. We narrow the problem to that of estimating how
a set target participants react to the behavior of other observed participants.
Our key idea is to model the spatio-temporal relations among participants in a
manner that is robust to error accumulation during frame-wise inference and
prediction. We propose a novel Entry-Flipped Transformer (EF-Transformer),
which models the relations of participants by attention mechanisms on both
spatial and temporal domains. Unlike typical transformers, we tackle the
problem of error accumulation by flipping the order of query, key, and value
entries, to increase the importance and fidelity of observed features in the
current frame. Comparative experiments show that our EF-Transformer achieves
the best performance on a newly-collected tennis doubles dataset, a Ceilidh
dance dataset, and two pedestrian datasets. Furthermore, it is also
demonstrated that our EF-Transformer is better at limiting accumulated errors
and recovering from wrong estimations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SlimSeg: Slimmable Semantic Segmentation with Boundary Supervision. (arXiv:2207.06242v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06242">
<div class="article-summary-box-inner">
<span><p>Accurate semantic segmentation models typically require significant
computational resources, inhibiting their use in practical applications. Recent
works rely on well-crafted lightweight models to achieve fast inference.
However, these models cannot flexibly adapt to varying accuracy and efficiency
requirements. In this paper, we propose a simple but effective slimmable
semantic segmentation (SlimSeg) method, which can be executed at different
capacities during inference depending on the desired accuracy-efficiency
tradeoff. More specifically, we employ parametrized channel slimming by
stepwise downward knowledge distillation during training. Motivated by the
observation that the differences between segmentation results of each submodel
are mainly near the semantic borders, we introduce an additional boundary
guided semantic segmentation loss to further improve the performance of each
submodel. We show that our proposed SlimSeg with various mainstream networks
can produce flexible models that provide dynamic adjustment of computational
cost and better performance than independent models. Extensive experiments on
semantic segmentation benchmarks, Cityscapes and CamVid, demonstrate the
generalization ability of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Consistent Semantic Image Editing with Style-Preserved Modulation. (arXiv:2207.06252v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06252">
<div class="article-summary-box-inner">
<span><p>Semantic image editing utilizes local semantic label maps to generate the
desired content in the edited region. A recent work borrows SPADE block to
achieve semantic image editing. However, it cannot produce pleasing results due
to style discrepancy between the edited region and surrounding pixels. We
attribute this to the fact that SPADE only uses an image-independent local
semantic layout but ignores the image-specific styles included in the known
pixels. To address this issue, we propose a style-preserved modulation (SPM)
comprising two modulations processes: The first modulation incorporates the
contextual style and semantic layout, and then generates two fused modulation
parameters. The second modulation employs the fused parameters to modulate
feature maps. By using such two modulations, SPM can inject the given semantic
layout while preserving the image-specific context style. Moreover, we design a
progressive architecture for generating the edited content in a coarse-to-fine
manner. The proposed method can obtain context-consistent results and
significantly alleviate the unpleasant boundary between the generated regions
and the known pixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image warp preserving content intensity. (arXiv:2207.06256v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06256">
<div class="article-summary-box-inner">
<span><p>An accurate method for warping images is presented. Differently from most
commonly used techniques, this method guarantees the conservation of the
intensity of the transformed image, evaluated as the sum of its pixel values
over the whole image or over corresponding transformed subregions of it. Such
property is mandatory for quantitative analysis, as, for instance, when
deformed images are used to assess radiances, to measure optical fluxes from
light sources, or to characterize material optical densities. The proposed
method enforces area resampling by decomposing each rectangular pixel in two
triangles, and projecting the pixel intensity onto half pixels of the
transformed image, with weights proportional to the area of overlap of the
triangular half-pixels. The result is quantitatively exact, as long as the
original pixel value is assumed to represent a constant image density within
the pixel area, and as long as the coordinate transformation is diffeomorphic.
Implementation details and possible variations of the method are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Appearance Free Action Recognition Possible?. (arXiv:2207.06261v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06261">
<div class="article-summary-box-inner">
<span><p>Intuition might suggest that motion and dynamic information are key to
video-based action recognition. In contrast, there is evidence that
state-of-the-art deep-learning video understanding architectures are biased
toward static information available in single frames. Presently, a methodology
and corresponding dataset to isolate the effects of dynamic information in
video are missing. Their absence makes it difficult to understand how well
contemporary architectures capitalize on dynamic vs. static information. We
respond with a novel Appearance Free Dataset (AFD) for action recognition. AFD
is devoid of static information relevant to action recognition in a single
frame. Modeling of the dynamics is necessary for solving the task, as the
action is only apparent through consideration of the temporal dimension. We
evaluated 11 contemporary action recognition architectures on AFD as well as
its related RGB video. Our results show a notable decrease in performance for
all architectures on AFD compared to RGB. We also conducted a complimentary
study with humans that shows their recognition accuracy on AFD and RGB is very
similar and much better than the evaluated architectures on AFD. Our results
motivate a novel architecture that revives explicit recovery of optical flow,
within a contemporary design for best performance on AFD and RGB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Organic Priors in Non-Rigid Structure from Motion. (arXiv:2207.06262v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06262">
<div class="article-summary-box-inner">
<span><p>This paper advocates the use of organic priors in classical non-rigid
structure from motion (NRSfM). By organic priors, we mean invaluable
intermediate prior information intrinsic to the NRSfM matrix factorization
theory. It is shown that such priors reside in the factorized matrices, and
quite surprisingly, existing methods generally disregard them. The paper's main
contribution is to put forward a simple, methodical, and practical method that
can effectively exploit such organic priors to solve NRSfM. The proposed method
does not make assumptions other than the popular one on the low-rank shape and
offers a reliable solution to NRSfM under orthographic projection. Our work
reveals that the accessibility of organic priors is independent of the camera
motion and shape deformation type. Besides that, the paper provides insights
into the NRSfM factorization -- both in terms of shape, motion -- and is the
first approach to show the benefit of single rotation averaging for NRSfM.
Furthermore, we outline how to effectively recover motion and non-rigid 3D
shape using the proposed organic prior based approach and demonstrate results
that outperform prior-free NRSfM performance by a significant margin. Finally,
we present the benefits of our method via extensive experiments and evaluations
on several benchmark dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Agnostic Representation Consolidation: a Self-supervised based Continual Learning Approach. (arXiv:2207.06267v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06267">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) over non-stationary data streams remains one of the
long-standing challenges in deep neural networks (DNNs) as they are prone to
catastrophic forgetting. CL models can benefit from self-supervised
pre-training as it enables learning more generalizable task-agnostic features.
However, the effect of self-supervised pre-training diminishes as the length of
task sequences increases. Furthermore, the domain shift between pre-training
data distribution and the task distribution reduces the generalizability of the
learned representations. To address these limitations, we propose Task Agnostic
Representation Consolidation (TARC), a two-stage training paradigm for CL that
intertwines task-agnostic and task-specific learning whereby self-supervised
training is followed by supervised learning for each task. To further restrict
the deviation from the learned representations in the self-supervised stage, we
employ a task-agnostic auxiliary loss during the supervised stage. We show that
our training paradigm can be easily added to memory- or regularization-based
approaches and provides consistent performance gain across more challenging CL
settings. We further show that it leads to more robust and well-calibrated
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACLNet: An Attention and Clustering-based Cloud Segmentation Network. (arXiv:2207.06277v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06277">
<div class="article-summary-box-inner">
<span><p>We propose a novel deep learning model named ACLNet, for cloud segmentation
from ground images. ACLNet uses both deep neural network and machine learning
(ML) algorithm to extract complementary features. Specifically, it uses
EfficientNet-B0 as the backbone, "`a trous spatial pyramid pooling" (ASPP) to
learn at multiple receptive fields, and "global attention module" (GAM) to
extract finegrained details from the image. ACLNet also uses k-means clustering
to extract cloud boundaries more precisely. ACLNet is effective for both
daytime and nighttime images. It provides lower error rate, higher recall and
higher F1-score than state-of-art cloud segmentation models. The source-code of
ACLNet is available here: https://github.com/ckmvigil/ACLNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Neural Representations for Generative Modeling of Living Cell Shapes. (arXiv:2207.06283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06283">
<div class="article-summary-box-inner">
<span><p>Methods allowing the synthesis of realistic cell shapes could help generate
training data sets to improve cell tracking and segmentation in biomedical
images. Deep generative models for cell shape synthesis require a light-weight
and flexible representation of the cell shape. However, commonly used
voxel-based representations are unsuitable for high-resolution shape synthesis,
and polygon meshes have limitations when modeling topology changes such as cell
growth or mitosis. In this work, we propose to use level sets of signed
distance functions (SDFs) to represent cell shapes. We optimize a neural
network as an implicit neural representation of the SDF value at any point in a
3D+time domain. The model is conditioned on a latent code, thus allowing the
synthesis of new and unseen shape sequences. We validate our approach
quantitatively and qualitatively on C. elegans cells that grow and divide, and
lung cancer cells with growing complex filopodial protrusions. Our results show
that shape descriptors of synthetic cells resemble those of real cells, and
that our model is able to generate topologically plausible sequences of complex
cell shapes in 3D+time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PointNorm: Normalization is All You Need for Point Cloud Analysis. (arXiv:2207.06324v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06324">
<div class="article-summary-box-inner">
<span><p>Point cloud analysis is challenging due to the irregularity of the point
cloud data structure. Existing works typically employ the ad-hoc
sampling-grouping operation of PointNet++, followed by sophisticated local
and/or global feature extractors for leveraging the 3D geometry of the point
cloud. Unfortunately, those intricate hand-crafted model designs have led to
poor inference latency and performance saturation in the last few years. In
this paper, we point out that the classical sampling-grouping operations on the
irregular point cloud cause learning difficulty for the subsequent MLP layers.
To reduce the irregularity of the point cloud, we introduce a DualNorm module
after the sampling-grouping operation. The DualNorm module consists of Point
Normalization, which normalizes the grouped points to the sampled points, and
Reverse Point Normalization, which normalizes the sampled points to the grouped
points. The proposed PointNorm utilizes local mean and global standard
deviation to benefit from both local and global features while maintaining a
faithful inference speed. Experiments on point cloud classification show that
we achieved state-of-the-art accuracy on ModelNet40 and ScanObjectNN datasets.
We also generalize our model to point cloud part segmentation and demonstrate
competitive performance on the ShapeNetPart dataset. Code is available at
https://github.com/ShenZheng2000/PointNorm-for-Point-Cloud-Analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Left Ventricle Contouring of Apical Three-Chamber Views on 2D Echocardiography. (arXiv:2207.06330v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06330">
<div class="article-summary-box-inner">
<span><p>We propose a new method to automatically contour the left ventricle on 2D
echocardiographic images. Unlike most existing segmentation methods, which are
based on predicting segmentation masks, we focus at predicting the endocardial
contour and the key landmark points within this contour (basal points and
apex). This provides a representation that is closer to how experts perform
manual annotations and hence produce results that are physiologically more
plausible.
</p>
<p>Our proposed method uses a two-headed network based on the U-Net
architecture. One head predicts the 7 contour points, and the other head
predicts a distance map to the contour. This approach was compared to the U-Net
and to a point based approach, achieving performance gains of up to 30\% in
terms of landmark localisation (&lt;4.5mm) and distance to the ground truth
contour (&lt;3.5mm).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symmetry-Aware Transformer-based Mirror Detection. (arXiv:2207.06332v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06332">
<div class="article-summary-box-inner">
<span><p>Mirror detection aims to identify the mirror regions in the given input
image. Existing works mainly focus on integrating the semantic features and
structural features to mine the similarity and discontinuity between mirror and
non-mirror regions, or introducing depth information to help analyze the
existence of mirrors. In this work, we observe that a real object typically
forms a loose symmetry relationship with its corresponding reflection in the
mirror, which is beneficial in distinguishing mirrors from real objects. Based
on this observation, we propose a dual-path Symmetry-Aware Transformer-based
mirror detection Network (SATNet), which includes two novel modules:
Symmetry-Aware Attention Module (SAAM) and Contrast and Fusion Decoder Module
(CFDM). Specifically, we first introduce the transformer backbone to model
global information aggregation in images, extracting multi-scale features in
two paths. We then feed the high-level dual-path features to SAAMs to capture
the symmetry relations. Finally, we fuse the dual-path features and refine our
prediction maps progressively with CFDMs to obtain the final mirror mask.
Experimental results show that SATNet outperforms both RGB and RGB-D mirror
detection methods on all available mirror detection datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">6D Camera Relocalization in Visually Ambiguous Extreme Environments. (arXiv:2207.06333v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06333">
<div class="article-summary-box-inner">
<span><p>We propose a novel method to reliably estimate the pose of a camera given a
sequence of images acquired in extreme environments such as deep seas or
extraterrestrial terrains. Data acquired under these challenging conditions are
corrupted by textureless surfaces, image degradation, and presence of
repetitive and highly ambiguous structures. When naively deployed, the
state-of-the-art methods can fail in those scenarios as confirmed by our
empirical analysis. In this paper, we attempt to make camera relocalization
work in these extreme situations. To this end, we propose: (i) a hierarchical
localization system, where we leverage temporal information and (ii) a novel
environment-aware image enhancement method to boost the robustness and
accuracy. Our extensive experimental results demonstrate superior performance
in favor of our method under two extreme settings: localizing an autonomous
underwater vehicle and localizing a planetary rover in a Mars-like desert. In
addition, our method achieves comparable performance with state-of-the-art
methods on the indoor benchmark (7-Scenes dataset) using only 20% training
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Align Once: Bidirectional Interaction for Spatial-Temporal Video Super-Resolution. (arXiv:2207.06345v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06345">
<div class="article-summary-box-inner">
<span><p>Spatial-Temporal Video Super-Resolution (ST-VSR) technology generates
high-quality videos with higher resolution and higher frame rates. Existing
advanced methods accomplish ST-VSR tasks through the association of Spatial and
Temporal video super-resolution (S-VSR and T-VSR). These methods require two
alignments and fusions in S-VSR and T-VSR, which is obviously redundant and
fails to sufficiently explore the information flow of consecutive spatial LR
frames. Although bidirectional learning (future-to-past and past-to-future) was
introduced to cover all input frames, the direct fusion of final predictions
fails to sufficiently exploit intrinsic correlations of bidirectional motion
learning and spatial information from all frames. We propose an effective yet
efficient recurrent network with bidirectional interaction for ST-VSR, where
only one alignment and fusion is needed. Specifically, it first performs
backward inference from future to past, and then follows forward inference to
super-resolve intermediate frames. The backward and forward inferences are
assigned to learn structures and details to simplify the learning task with
joint optimizations. Furthermore, a Hybrid Fusion Module (HFM) is designed to
aggregate and distill information to refine spatial information and reconstruct
high-quality video frames. Extensive experiments on two public datasets
demonstrate that our method outperforms state-of-the-art methods in efficiency,
and reduces calculation cost by about 22%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Prediction of Monocular Depth and Structure using Planar and Parallax Geometry. (arXiv:2207.06351v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06351">
<div class="article-summary-box-inner">
<span><p>Supervised learning depth estimation methods can achieve good performance
when trained on high-quality ground-truth, like LiDAR data. However, LiDAR can
only generate sparse 3D maps which causes losing information. Obtaining
high-quality ground-truth depth data per pixel is difficult to acquire. In
order to overcome this limitation, we propose a novel approach combining
structure information from a promising Plane and Parallax geometry pipeline
with depth information into a U-Net supervised learning network, which results
in quantitative and qualitative improvement compared to existing popular
learning-based methods. In particular, the model is evaluated on two
large-scale and challenging datasets: KITTI Vision Benchmark and Cityscapes
dataset and achieve the best performance in terms of relative error. Compared
with pure depth supervision models, our model has impressive performance on
depth prediction of thin objects and edges, and compared to structure
prediction baseline, our model performs more robustly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radar Image Reconstruction from Raw ADC Data using Parametric Variational Autoencoder with Domain Adaptation. (arXiv:2207.06379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06379">
<div class="article-summary-box-inner">
<span><p>This paper presents a parametric variational autoencoder-based human target
detection and localization framework working directly with the raw
analog-to-digital converter data from the frequency modulated continous wave
radar. We propose a parametrically constrained variational autoencoder, with
residual and skip connections, capable of generating the clustered and
localized target detections on the range-angle image. Furthermore, to
circumvent the problem of training the proposed neural network on all possible
scenarios using real radar data, we propose domain adaptation strategies
whereby we first train the neural network using ray tracing based model data
and then adapt the network to work on real sensor data. This strategy ensures
better generalization and scalability of the proposed neural network even
though it is trained with limited radar data. We demonstrate the superior
detection and localization performance of our proposed solution compared to the
conventional signal processing pipeline and earlier state-of-art deep U-Net
architecture with range-doppler images as inputs
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A General Framework for Partial to Full Image Registration. (arXiv:2207.06387v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06387">
<div class="article-summary-box-inner">
<span><p>Image registration is a research field in which images must be compared and
aligned independently of the point of view or camera characteristics. In some
applications (such as forensic biometrics, satellite photography or outdoor
scene identification) classical image registration systems fail due to one of
the images compared represents a tiny piece of the other image. For instance,
in forensics palmprint recognition, it is usual to find only a small piece of
the palmprint, but in the database, the whole palmprint has been enrolled. The
main reason of the poor behaviour of classical image registration methods is
the gap between the amounts of salient points of both images, which is related
to the number of points to be considered as outliers. Usually, the difficulty
of finding a good match increases when the image that represents the tiny part
of the scene has been drastically rotated. Again, in the case of palmprint
forensics, it is difficult to decide a priori the orientation of the found tiny
palmprint image. We present a rotation invariant registration method that
explicitly considers that the image to be matched is a small piece of a larger
image. We have experimentally validated our method in two different scenarios;
palmprint identification and outdoor image registration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Water Surface Patch Classification Using Mixture Augmentation for River Scum Index. (arXiv:2207.06388v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06388">
<div class="article-summary-box-inner">
<span><p>Urban rivers provide a water environment that influences residential living.
River surface monitoring has become crucial for making decisions about where to
prioritize cleaning and when to automatically start the cleaning treatment. We
focus on the organic mud, or "scum" that accumulates on the river's surface and
gives it its peculiar odor and external economic effects on the landscape.
Because of its feature of a sparsely distributed and unstable pattern of
organic shape, automating the monitoring has proved difficult. We propose a
patch classification pipeline to detect scum features on the river surface
using mixture image augmentation to increase the diversity between the scum
floating on the river and the entangled background on the river surface
reflected by nearby structures like buildings, bridges, poles, and barriers.
Furthermore, we propose a scum index covered on rivers to help monitor worse
grade online, collecting floating scum and deciding on chemical treatment
policies. Finally, we show how to use our method on a time series dataset with
frames every ten minutes recording river scum events over several days. We
discuss the value of our pipeline and its experimental findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular Images. (arXiv:2207.06400v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06400">
<div class="article-summary-box-inner">
<span><p>Regression-based methods can estimate body, hand, and even full-body models
from monocular images by directly mapping raw pixels to the model parameters in
a feed-forward manner. However, minor deviation in parameters may lead to
noticeable misalignment between the estimated meshes and input images,
especially in the context of full-body mesh recovery. To address this issue, we
propose a Pyramidal Mesh Alignment Feedback (PyMAF) loop in our regression
network for well-aligned human mesh recovery and extend it to PyMAF-X for the
recovery of expressive full-body models. The core idea of PyMAF is to leverage
a feature pyramid and rectify the predicted parameters explicitly based on the
mesh-image alignment status. Specifically, given the currently predicted
parameters, mesh-aligned evidences will be extracted from finer-resolution
features accordingly and fed back for parameter rectification. To enhance the
alignment perception, an auxiliary dense supervision is employed to provide
mesh-image correspondence guidance while a spatial alignment attention is
introduced to enable the awareness of the global contexts for our network. When
extending PyMAF for full-body mesh recovery, an adaptive integration strategy
is proposed in PyMAF-X to adjust the elbow-twist rotations, which produces
natural wrist poses while maintaining the well-aligned performance of the
part-specific estimations. The efficacy of our approach is validated on several
benchmark datasets for body-only and full-body mesh recovery, where PyMAF and
PyMAF-X effectively improve the mesh-image alignment and achieve new
state-of-the-art results. The project page with code and video results can be
found at https://www.liuyebin.com/pymaf-x.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Concept Grounding on Neural Fields. (arXiv:2207.06403v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06403">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the challenging problem of 3D concept grounding
(i.e. segmenting and learning visual concepts) by looking at RGBD images and
reasoning about paired questions and answers. Existing visual reasoning
approaches typically utilize supervised methods to extract 2D segmentation
masks on which concepts are grounded. In contrast, humans are capable of
grounding concepts on the underlying 3D representation of images. However,
traditionally inferred 3D representations (e.g., point clouds, voxelgrids, and
meshes) cannot capture continuous 3D features flexibly, thus making it
challenging to ground concepts to 3D regions based on the language description
of the object being referred to. To address both issues, we propose to leverage
the continuous, differentiable nature of neural fields to segment and learn
concepts. Specifically, each 3D coordinate in a scene is represented as a
high-dimensional descriptor. Concept grounding can then be performed by
computing the similarity between the descriptor vector of a 3D coordinate and
the vector embedding of a language concept, which enables segmentations and
concept learning to be jointly learned on neural fields in a differentiable
fashion. As a result, both 3D semantic and instance segmentations can emerge
directly from question answering supervision using a set of defined neural
operators on top of neural fields (e.g., filtering and counting). Experimental
results show that our proposed framework outperforms
unsupervised/language-mediated segmentation models on semantic and instance
segmentation tasks, as well as outperforms existing models on the challenging
3D aware visual reasoning tasks. Furthermore, our framework can generalize well
to unseen shape categories and real scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open set learning with augmented category by exploiting unlabelled data (open-LACU). (arXiv:2002.01368v4 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.01368">
<div class="article-summary-box-inner">
<span><p>Considering the nature of unlabelled data, it is common for partially
labelled training datasets to contain samples that belong to novel categories.
Although these so-called observed novel categories exist in the training data,
they do not belong to any of the training labels. In contrast, open-sets define
novel categories as those unobserved during during training, but present during
testing. This research is the first to generalize between observed and
unobserved novel categories within a new learning policy called open-set
learning with augmented category by exploiting unlabeled data or open-LACU.
This study conducts a high-level review on novelty detection so to
differentiate between research fields that concern observed novel categories,
and the research fields that concern unobserved novel categories. Open-LACU is
then introduced as a synthesis of the relevant fields to maintain the
advantages of each within a single learning policy. Currently, we are
finalising the first open-LACU network which will be combined with this
pre-print to be sent for publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Interpretable Microscopic Features of Tumor by Multi-task Adversarial CNNs Improves Generalization. (arXiv:2008.01478v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01478">
<div class="article-summary-box-inner">
<span><p>Adopting Convolutional Neural Networks (CNNs) in the daily routine of primary
diagnosis requires not only near-perfect precision, but also a sufficient
degree of generalization to data acquisition shifts and transparency. Existing
CNN models act as black boxes, not ensuring to the physicians that important
diagnostic features are used by the model. Building on top of successfully
existing techniques such as multi-task learning, domain adversarial training
and concept-based interpretability, this paper addresses the challenge of
introducing diagnostic factors in the training objectives. Here we show that
our architecture, by learning end-to-end an uncertainty-based weighting
combination of multi-task and adversarial losses, is encouraged to focus on
pathology features such as density and pleomorphism of nuclei, e.g. variations
in size and appearance, while discarding misleading features such as staining
differences. Our results on breast lymph node tissue show significantly
improved generalization in the detection of tumorous tissue, with best average
AUC 0.89 (0.01) against the baseline AUC 0.86 (0.005). By applying the
interpretability technique of linearly probing intermediate representations, we
also demonstrate that interpretable pathology features such as nuclei density
are learned by the proposed CNN architecture, confirming the increased
transparency of this model. This result is a starting point towards building
interpretable multi-task architectures that are robust to data heterogeneity.
Our code is available at https://bit.ly/356yQ2u.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Intermediate Flow Estimation for Video Frame Interpolation. (arXiv:2011.06294v12 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06294">
<div class="article-summary-box-inner">
<span><p>Real-time video frame interpolation (VFI) is very useful in video processing,
media players, and display devices. We propose RIFE, a Real-time Intermediate
Flow Estimation algorithm for VFI. To realize a high-quality flow-based VFI
method, RIFE uses a neural network named IFNet that can estimate the
intermediate flows end-to-end with much faster speed. A privileged distillation
scheme is designed for stable IFNet training and improve the overall
performance. RIFE does not rely on pre-trained optical flow models and can
support arbitrary-timestep frame interpolation with the temporal encoding
input. Experiments demonstrate that RIFE achieves state-of-the-art performance
on several public benchmarks. Compared with the popular SuperSlomo and DAIN
methods, RIFE is 4--27 times faster and produces better results. Furthermore,
RIFE can be extended to wider applications thanks to temporal encoding. The
code is available at https://github.com/megvii-research/ECCV2022-RIFE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Generalized Zero-Shot Learning Methods. (arXiv:2011.08641v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08641">
<div class="article-summary-box-inner">
<span><p>Generalized zero-shot learning (GZSL) aims to train a model for classifying
data samples under the condition that some output classes are unknown during
supervised learning. To address this challenging task, GZSL leverages semantic
information of the seen (source) and unseen (target) classes to bridge the gap
between both seen and unseen classes. Since its introduction, many GZSL models
have been formulated. In this review paper, we present a comprehensive review
on GZSL. Firstly, we provide an overview of GZSL including the problems and
challenges. Then, we introduce a hierarchical categorization for the GZSL
methods and discuss the representative methods in each category. In addition,
we discuss the available benchmark data sets and applications of GZSL, along
with a discussion on the research gaps and directions for future
investigations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Label Noise Robust Collaborative Learning Method for Remote Sensing Image Classification. (arXiv:2012.10715v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10715">
<div class="article-summary-box-inner">
<span><p>The development of accurate methods for multi-label classification (MLC) of
remote sensing (RS) images is one of the most important research topics in RS.
The MLC methods based on Convolutional Neural Networks (CNNs) have shown strong
performance gains in RS. However, they usually require a high number of
reliable training images annotated with multiple land-cover class labels.
Collecting such data is time-consuming and costly. To address this problem, the
publicly available thematic products, which can include noisy labels, can be
used to annotate RS images with zero-labeling cost. However, multi-label noise
(which can be associated with wrong and missing label annotations) can distort
the learning process of the MLC methods. To address this problem, we propose a
novel multi-label noise robust collaborative learning (RCML) method to
alleviate the negative effects of multi-label noise during the training phase
of a CNN model. RCML identifies, ranks and excludes noisy multi-labels in RS
images based on three main modules: 1) the discrepancy module; 2) the group
lasso module; and 3) the swap module. The discrepancy module ensures that the
two networks learn diverse features, while producing the same predictions. The
task of the group lasso module is to detect the potentially noisy labels
assigned to the multi-labeled training images, while the swap module is devoted
to exchange the ranking information between two networks. Unlike existing
methods that make assumptions about the noise distribution, our proposed RCML
does not make any prior assumption about the type of noise in the training set.
The experiments conducted on two multi-label RS image archives confirm the
robustness of the proposed RCML under extreme multi-label noise rates. Our code
is publicly available at: <a href="http://www.noisy-labels-in-rs.org">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vote from the Center: 6 DoF Pose Estimation in RGB-D Images by Radial Keypoint Voting. (arXiv:2104.02527v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02527">
<div class="article-summary-box-inner">
<span><p>We propose a novel keypoint voting scheme based on intersecting spheres, that
is more accurate than existing schemes and allows for fewer, more disperse
keypoints. The scheme is based upon the distance between points, which as a 1D
quantity can be regressed more accurately than the 2D and 3D vector and offset
quantities regressed in previous work, yielding more accurate keypoint
localization. The scheme forms the basis of the proposed RCVPose method for 6
DoF pose estimation of 3D objects in RGB-D data, which is particularly
effective at handling occlusions. A CNN is trained to estimate the distance
between the 3D point corresponding to the depth mode of each RGB pixel, and a
set of 3 disperse keypoints defined in the object frame. At inference, a sphere
centered at each 3D point is generated, of radius equal to this estimated
distance. The surfaces of these spheres vote to increment a 3D accumulator
space, the peaks of which indicate keypoint locations. The proposed radial
voting scheme is more accurate than previous vector or offset schemes, and is
robust to disperse keypoints. Experiments demonstrate RCVPose to be highly
accurate and competitive, achieving state-of-the-art results on the LINEMOD
99.7% and YCB-Video 97.2% datasets, notably scoring +4.9% higher 71.1% than
previous methods on the challenging Occlusion LINEMOD dataset, and on average
outperforming all other published results from the BOP benchmark for these 3
datasets. Our code is available at <a href="http://www.github.com/aaronwool/rcvpose.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13266">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the problem of high performance and computationally
efficient content-based video retrieval in large-scale datasets. Current
methods typically propose either: (i) fine-grained approaches employing
spatio-temporal representations and similarity calculations, achieving high
performance at a high computational cost or (ii) coarse-grained approaches
representing/indexing videos as global vectors, where the spatio-temporal
structure is lost, providing low performance but also having low computational
cost. In this work, we propose a Knowledge Distillation framework, called
Distill-and-Select (DnS), that starting from a well-performing fine-grained
Teacher Network learns: a) Student Networks at different retrieval performance
and computational efficiency trade-offs and b) a Selector Network that at test
time rapidly directs samples to the appropriate student to maintain both high
retrieval performance and high computational efficiency. We train several
students with different architectures and arrive at different trade-offs of
performance and efficiency, i.e., speed and storage requirements, including
fine-grained students that store/index videos using binary representations.
Importantly, the proposed scheme allows Knowledge Distillation in large,
unlabelled datasets -- this leads to good students. We evaluate DnS on five
public datasets on three different video retrieval tasks and demonstrate a)
that our students achieve state-of-the-art performance in several cases and b)
that the DnS framework provides an excellent trade-off between retrieval
performance, computational speed, and storage space. In specific
configurations, the proposed method achieves similar mAP with the teacher but
is 20 times faster and requires 240 times less storage space. The collected
dataset and implementation are publicly available:
https://github.com/mever-team/distill-and-select.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A survey on computational spectral reconstruction methods from RGB to hyperspectral imaging. (arXiv:2106.15944v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15944">
<div class="article-summary-box-inner">
<span><p>Hyperspectral imaging enables versatile applications due to its competence in
capturing abundant spatial and spectral information, which are crucial for
identifying substances. However, the devices for acquiring hyperspectral images
are expensive and complicated. Therefore, many alternative spectral imaging
methods have been proposed by directly reconstructing the hyperspectral
information from lower-cost, more available RGB images. We present a thorough
investigation of these state-of-the-art spectral reconstruction methods from
the widespread RGB images. A systematic study and comparison of more than 25
methods has revealed that most of the data-driven deep learning methods are
superior to prior-based methods in terms of reconstruction accuracy and quality
despite lower speeds. This comprehensive review can serve as a fruitful
reference source for peer researchers, thus further inspiring future
development directions in related domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A data-centric approach for improving ambiguous labels with combined semi-supervised classification and clustering. (arXiv:2106.16209v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.16209">
<div class="article-summary-box-inner">
<span><p>Consistently high data quality is essential for the development of novel loss
functions and architectures in the field of deep learning. The existence of
such data and labels is usually presumed, while acquiring high-quality datasets
is still a major issue in many cases. In real-world datasets we often encounter
ambiguous labels due to subjective annotations by annotators. In our
data-centric approach, we propose a method to relabel such ambiguous labels
instead of implementing the handling of this issue in a neural network. A hard
classification is by definition not enough to capture the real-world ambiguity
of the data. Therefore, we propose our method "Data-Centric Classification &amp;
Clustering (DC3)" which combines semi-supervised classification and clustering.
It automatically estimates the ambiguity of an image and performs a
classification or clustering depending on that ambiguity. DC3 is general in
nature so that it can be used in addition to many Semi-Supervised Learning
(SSL) algorithms. On average, this results in a 7.6% better F1-Score for
classifications and 7.9% lower inner distance of clusters across multiple
evaluated SSL algorithms and datasets. Most importantly, we give a
proof-of-concept that the classifications and clusterings from DC3 are
beneficial as proposals for the manual refinement of such ambiguous labels.
Overall, a combination of SSL with our method DC3 can lead to better handling
of ambiguous labels during the annotation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12636">
<div class="article-summary-box-inner">
<span><p>Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: https://github.com/encounter1997/SFA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RigNet: Repetitive Image Guided Network for Depth Completion. (arXiv:2107.13802v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13802">
<div class="article-summary-box-inner">
<span><p>Depth completion deals with the problem of recovering dense depth maps from
sparse ones, where color images are often used to facilitate this task. Recent
approaches mainly focus on image guided learning frameworks to predict dense
depth. However, blurry guidance in the image and unclear structure in the depth
still impede the performance of the image guided frameworks. To tackle these
problems, we explore a repetitive design in our image guided network to
gradually and sufficiently recover depth values. Specifically, the repetition
is embodied in both the image guidance branch and depth generation branch. In
the former branch, we design a repetitive hourglass network to extract
discriminative image features of complex environments, which can provide
powerful contextual instruction for depth prediction. In the latter branch, we
introduce a repetitive guidance module based on dynamic convolution, in which
an efficient convolution factorization is proposed to simultaneously reduce its
complexity and progressively model high-frequency structures. Extensive
experiments show that our method achieves superior or competitive results on
KITTI benchmark and NYUv2 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">View Vertically: A Hierarchical Network for Trajectory Prediction via Fourier Spectrums. (arXiv:2110.07288v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07288">
<div class="article-summary-box-inner">
<span><p>Understanding and forecasting future trajectories of agents are critical for
behavior analysis, robot navigation, autonomous cars, and other related
applications. Previous methods mostly treat trajectory prediction as time
sequence generation. Different from them, this work studies agents'
trajectories in a "vertical" view, i.e., modeling and forecasting trajectories
from the spectral domain. Different frequency bands in the trajectory spectrums
could hierarchically reflect agents' motion preferences at different scales.
The low-frequency and high-frequency portions could represent their coarse
motion trends and fine motion variations, respectively. Accordingly, we propose
a hierarchical network V$^2$-Net, which contains two sub-networks, to
hierarchically model and predict agents' trajectories with trajectory
spectrums. The coarse-level keypoints estimation sub-network first predicts the
"minimal" spectrums of agents' trajectories on several "key" frequency
portions. Then the fine-level spectrum interpolation sub-network interpolates
the spectrums to reconstruct the final predictions. Experimental results
display the competitiveness and superiority of V$^2$-Net on both ETH-UCY
benchmark and the Stanford Drone Dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Vocabulary Object Detection with Pseudo Bounding-Box Labels. (arXiv:2111.09452v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09452">
<div class="article-summary-box-inner">
<span><p>Despite great progress in object detection, most existing methods work only
on a limited set of object categories, due to the tremendous human effort
needed for bounding-box annotations of training data. To alleviate the problem,
recent open vocabulary and zero-shot detection methods attempt to detect novel
object categories beyond those seen during training. They achieve this goal by
training on a pre-defined base categories to induce generalization to novel
objects. However, their potential is still constrained by the small set of base
categories available for training. To enlarge the set of base classes, we
propose a method to automatically generate pseudo bounding-box annotations of
diverse objects from large-scale image-caption pairs. Our method leverages the
localization ability of pre-trained vision-language models to generate pseudo
bounding-box labels and then directly uses them for training object detectors.
Experimental results show that our method outperforms the state-of-the-art open
vocabulary detector by 8% AP on COCO novel categories, by 6.3% AP on PASCAL
VOC, by 2.3% AP on Objects365 and by 2.8% AP on LVIS. Code is available at
https://github.com/salesforce/PB-OVD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Clustering Learning for Large-scale Unsupervised Person Re-identification. (arXiv:2111.10032v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10032">
<div class="article-summary-box-inner">
<span><p>Unsupervised Person Re-identification (U-ReID) with pseudo labeling recently
reaches a competitive performance compared to fully-supervised ReID methods
based on modern clustering algorithms. However, such clustering-based scheme
becomes computationally prohibitive for large-scale datasets. How to
efficiently leverage endless unlabeled data with limited computing resources
for better U-ReID is under-explored. In this paper, we make the first attempt
to the large-scale U-ReID and propose a "small data for big task" paradigm
dubbed Meta Clustering Learning (MCL). MCL only pseudo-labels a subset of the
entire unlabeled data via clustering to save computing for the first-phase
training. After that, the learned cluster centroids, termed as meta-prototypes
in our MCL, are regarded as a proxy annotator to softly annotate the rest
unlabeled data for further polishing the model. To alleviate the potential
noisy labeling issue in the polishment phase, we enforce two well-designed loss
constraints to promise intra-identity consistency and inter-identity strong
correlation. For multiple widely-used U-ReID benchmarks, our method
significantly saves computational cost while achieving a comparable or even
better performance compared to prior works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AVA-AVD: Audio-Visual Speaker Diarization in the Wild. (arXiv:2111.14448v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14448">
<div class="article-summary-box-inner">
<span><p>Audio-visual speaker diarization aims at detecting "who spoke when" using
both auditory and visual signals. Existing audio-visual diarization datasets
are mainly focused on indoor environments like meeting rooms or news studios,
which are quite different from in-the-wild videos in many scenarios such as
movies, documentaries, and audience sitcoms. To develop diarization methods for
these challenging videos, we create the AVA Audio-Visual Diarization (AVA-AVD)
dataset. Our experiments demonstrate that adding AVA-AVD into training set can
produce significantly better diarization models for in-the-wild videos despite
that the data is relatively small. Moreover, this benchmark is challenging due
to the diverse scenes, complicated acoustic conditions, and completely
off-screen speakers. As a first step towards addressing the challenges, we
design the Audio-Visual Relation Network (AVR-Net) which introduces a simple
yet effective modality mask to capture discriminative information based on face
visibility. Experiments show that our method not only can outperform
state-of-the-art methods but is more robust as varying the ratio of off-screen
speakers. Our data and code has been made publicly available at
https://github.com/showlab/AVA-AVD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks. (arXiv:2112.03227v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03227">
<div class="article-summary-box-inner">
<span><p>General-purpose robots coexisting with humans in their environment must learn
to relate human language to their perceptions and actions to be useful in a
range of daily tasks. Moreover, they need to acquire a diverse repertoire of
general-purpose skills that allow composing long-horizon tasks by following
unconstrained language instructions. In this paper, we present CALVIN
(Composing Actions from Language and Vision), an open-source simulated
benchmark to learn long-horizon language-conditioned tasks. Our aim is to make
it possible to develop agents that can solve many robotic manipulation tasks
over a long horizon, from onboard sensors, and specified only via human
language. CALVIN tasks are more complex in terms of sequence length, action
space, and language than existing vision-and-language task datasets and
supports flexible specification of sensor suites. We evaluate the agents in
zero-shot to novel language instructions and to novel environments and objects.
We show that a baseline model based on multi-context imitation learning
performs poorly on CALVIN, suggesting that there is significant room for
developing innovative agents that learn to relate human language to their world
models with this benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Instance and Task-Aware Dynamic Kernels for Few Shot Learning. (arXiv:2112.03494v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03494">
<div class="article-summary-box-inner">
<span><p>Learning and generalizing to novel concepts with few samples (Few-Shot
Learning) is still an essential challenge to real-world applications. A
principle way of achieving few-shot learning is to realize a model that can
rapidly adapt to the context of a given task. Dynamic networks have been shown
capable of learning content-adaptive parameters efficiently, making them
suitable for few-shot learning. In this paper, we propose to learn the dynamic
kernels of a convolution network as a function of the task at hand, enabling
faster generalization. To this end, we obtain our dynamic kernels based on the
entire task and each sample and develop a mechanism further conditioning on
each individual channel and position independently. This results in dynamic
kernels that simultaneously attend to the global information whilst also
considering minuscule details available. We empirically show that our model
improves performance on few-shot classification and detection tasks, achieving
a tangible improvement over several baseline models. This includes
state-of-the-art results on 4 few-shot classification benchmarks:
mini-ImageNet, tiered-ImageNet, CUB and FC100 and competitive results on a
few-shot detection dataset: MS COCO-PASCAL-VOC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polarimetric Pose Prediction. (arXiv:2112.03810v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03810">
<div class="article-summary-box-inner">
<span><p>Light has many properties that vision sensors can passively measure.
Colour-band separated wavelength and intensity are arguably the most commonly
used for monocular 6D object pose estimation. This paper explores how
complementary polarisation information, i.e. the orientation of light wave
oscillations, influences the accuracy of pose predictions. A hybrid model that
leverages physical priors jointly with a data-driven learning strategy is
designed and carefully tested on objects with different levels of photometric
complexity. Our design significantly improves the pose accuracy compared to
state-of-the-art photometric approaches and enables object pose estimation for
highly reflective and transparent objects. A new multi-modal instance-level 6D
object pose dataset with highly accurate pose annotations for multiple objects
with varying photometric complexity is introduced as a benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Hash Distillation for Image Retrieval. (arXiv:2112.08816v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08816">
<div class="article-summary-box-inner">
<span><p>In hash-based image retrieval systems, degraded or transformed inputs usually
generate different codes from the original, deteriorating the retrieval
accuracy. To mitigate this issue, data augmentation can be applied during
training. However, even if augmented samples of an image are similar in real
feature space, the quantization can scatter them far away in Hamming space.
This results in representation discrepancies that can impede training and
degrade performance. In this work, we propose a novel self-distilled hashing
scheme to minimize the discrepancy while exploiting the potential of augmented
data. By transferring the hash knowledge of the weakly-transformed samples to
the strong ones, we make the hash code insensitive to various transformations.
We also introduce hash proxy-based similarity learning and binary cross
entropy-based quantization loss to provide fine quality hash codes. Ultimately,
we construct a deep hashing framework that not only improves the existing deep
hashing approaches, but also achieves the state-of-the-art retrieval results.
Extensive experiments are conducted and confirm the effectiveness of our work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Navigating to Objects in Unseen Environments by Distance Prediction. (arXiv:2202.03735v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03735">
<div class="article-summary-box-inner">
<span><p>Object Goal Navigation (ObjectNav) task is to navigate an agent to an object
category in unseen environments without a pre-built map. In this paper, we
solve this task by predicting the distance to the target using
semantically-related objects as cues. Based on the estimated distance to the
target object, our method directly choose optimal mid-term goals that are more
likely to have a shorter path to the target. Specifically, based on the learned
knowledge, our model takes a bird's-eye view semantic map as input, and
estimates the path length from the frontier map cells to the target object.
With the estimated distance map, the agent could simultaneously explore the
environment and navigate to the target objects based on a simple human-designed
strategy. Empirical results in visually realistic simulation environments show
that the proposed method outperforms a wide range of baselines on success rate
and efficiency. Real-robot experiment also demonstrates that our method
generalizes well to the real world. Video at
https://www.youtube.com/watch?v=R79pWVGFKS4
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound. (arXiv:2202.06599v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06599">
<div class="article-summary-box-inner">
<span><p>Segmentation and spatial alignment of ultrasound (US) imaging data acquired
in the in first trimester are crucial for monitoring human embryonic growth and
development throughout this crucial period of life. Current approaches are
either manual or semi-automatic and are therefore very time-consuming and prone
to errors. To automate these tasks, we propose a multi-atlas framework for
automatic segmentation and spatial alignment of the embryo using deep learning
with minimal supervision. Our framework learns to register the embryo to an
atlas, which consists of the US images acquired at a range of gestational age
(GA), segmented and spatially aligned to a predefined standard orientation.
From this, we can derive the segmentation of the embryo and put the embryo in
standard orientation. US images acquired at 8+0 till 12+6 weeks GA were used
and eight subjects were selected as atlas. We evaluated different fusion
strategies to incorporate multiple atlases: 1) training the framework using
atlas images from a single subject, 2) training the framework with data of all
available atlases and 3) ensembling of the frameworks trained per subject. To
evaluate the performance, we calculated the Dice score over the test set. We
found that training the framework using all available atlases outperformed
ensembling and gave similar results compared to the best of all frameworks
trained on a single subject. Furthermore, we found that selecting images from
the four atlases closest in GA out of all available atlases, regardless of the
individual quality, gave the best results with a median Dice score of 0.72. We
conclude that our framework can accurately segment and spatially align the
embryo in first trimester 3D US images and is robust for the variation in
quality that existed in the available atlases. Our code is publicly available
at: https://github.com/wapbastiaansen/multi-atlas-seg-reg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection. (arXiv:2202.06934v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06934">
<div class="article-summary-box-inner">
<span><p>Detection of small objects and objects far away in the scene is a major
challenge in surveillance applications. Such objects are represented by small
number of pixels in the image and lack sufficient details, making them
difficult to detect using conventional detectors. In this work, an open-source
framework called Slicing Aided Hyper Inference (SAHI) is proposed that provides
a generic slicing aided inference and fine-tuning pipeline for small object
detection. The proposed technique is generic in the sense that it can be
applied on top of any available object detector without any fine-tuning.
Experimental evaluations, using object detection baselines on the Visdrone and
xView aerial object detection datasets show that the proposed inference method
can increase object detection AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and
TOOD detectors, respectively. Moreover, the detection accuracy can be further
increased with a slicing aided fine-tuning, resulting in a cumulative increase
of 12.7%, 13.4% and 14.5% AP in the same order. Proposed technique has been
integrated with Detectron2, MMDetection and YOLOv5 models and it is publicly
available at https://github.com/obss/sahi.git .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale Hybrid Vision Transformer for Learning Gastric Cancer Histology. (arXiv:2202.08510v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08510">
<div class="article-summary-box-inner">
<span><p>Gastric endoscopic screening is an effective way to decide appropriate
gastric cancer (GC) treatment at an early stage, reducing GC-associated
mortality rate. Although artificial intelligence (AI) has brought a great
promise to assist pathologist to screen digitalized whole slide images,
existing AI systems are limited in fine-grained cancer subclassifications and
have little usability in planning cancer treatment. We propose a practical AI
system that enables five subclassifications of GC pathology, which can be
directly matched to general GC treatment guidance. The AI system is designed to
efficiently differentiate multi-classes of GC through multi-scale
self-attention mechanism using 2-stage hybrid Vision Transformer (ViT)
networks, by mimicking the way how human pathologists understand histology. The
AI system demonstrates reliable diagnostic performance by achieving
class-average sensitivity of above 0.85 on a total of 1,212 slides from
multicentric cohort. Furthermore, AI-assisted pathologists show significantly
improved diagnostic sensitivity by 12% in addition to 18% reduced screening
time compared to human pathologists. Our results demonstrate that AI-assisted
gastric endoscopic screening has a great potential for providing presumptive
pathologic opinion and appropriate cancer treatment of gastric cancer in
practical clinical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving Deep into One-Shot Skeleton-based Action Recognition with Diverse Occlusions. (arXiv:2202.11423v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11423">
<div class="article-summary-box-inner">
<span><p>Occlusions are universal disruptions constantly present in the real world.
Especially for sparse representations, such as human skeletons, a few occluded
points might destroy the geometrical and temporal continuity critically
affecting the results. Yet, the research of data-scarce recognition from
skeleton sequences, such as one-shot action recognition, does not explicitly
consider occlusions despite their everyday pervasiveness. In this work, we
explicitly tackle body occlusions for Skeleton-based One-shot Action
Recognition (SOAR). We mainly consider two occlusion variants: 1) random
occlusions and 2) more realistic occlusions caused by diverse everyday objects,
which we generate by projecting the existing IKEA 3D furniture models into the
camera coordinate system of the 3D skeletons. We leverage the proposed pipeline
to blend out portions of skeleton sequences of the three popular action
recognition datasets (NTU-120, NTU-60 and Toyota Smart Home) and formalize the
first benchmark for SOAR from partially occluded body poses. This is the first
benchmark which considers occlusions for data-scarce action recognition.
Another key property of our benchmark are the more realistic occlusions
generated by everyday objects, as even in standard recognition from 3D
skeletons, only randomly missing joints were considered. We re-evaluate
state-of-the-art frameworks in the light of this new task and further introduce
Trans4SOAR, a new transformer-based model which leverages three data streams
and mixed attention fusion mechanism to alleviate the adverse effects caused by
occlusions. While our experiments demonstrate a clear decline in accuracy with
missing skeleton portions, this effect is smaller with Trans4SOAR, which
outperforms other architectures on all datasets. Trans4SOAR additionally yields
state-of-the-art in the standard SOAR, surpassing the best published approach
by 2.85% on NTU-120.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Adversarial Robustness of Adaptive Test-time Defenses. (arXiv:2202.13711v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13711">
<div class="article-summary-box-inner">
<span><p>Adaptive defenses, which optimize at test time, promise to improve
adversarial robustness. We categorize such adaptive test-time defenses, explain
their potential benefits and drawbacks, and evaluate a representative variety
of the latest adaptive defenses for image classification. Unfortunately, none
significantly improve upon static defenses when subjected to our careful case
study evaluation. Some even weaken the underlying static model while
simultaneously increasing inference computation. While these results are
disappointing, we still believe that adaptive test-time defenses are a
promising avenue of research and, as such, we provide recommendations for their
thorough evaluation. We extend the checklist of Carlini et al. (2019) by
providing concrete steps specific to adaptive defenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving. (arXiv:2203.00858v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00858">
<div class="article-summary-box-inner">
<span><p>Modern autonomous vehicles adopt state-of-the-art DNN models to interpret the
sensor data and perceive the environment. However, DNN models are vulnerable to
different types of adversarial attacks, which pose significant risks to the
security and safety of the vehicles and passengers. One prominent threat is the
backdoor attack, where the adversary can compromise the DNN model by poisoning
the training samples. Although lots of effort has been devoted to the
investigation of the backdoor attack to conventional computer vision tasks, its
practicality and applicability to the autonomous driving scenario is rarely
explored, especially in the physical world.
</p>
<p>In this paper, we target the lane detection system, which is an indispensable
module for many autonomous driving tasks, e.g., navigation, lane switching. We
design and realize the first physical backdoor attacks to such system. Our
attacks are comprehensively effective against different types of lane detection
algorithms. Specifically, we introduce two attack methodologies
(poison-annotation and clean-annotation) to generate poisoned samples. With
those samples, the trained lane detection model will be infected with the
backdoor, and can be activated by common objects (e.g., traffic cones) to make
wrong detections, leading the vehicle to drive off the road or onto the
opposite lane. Extensive evaluations on public datasets and physical autonomous
vehicles demonstrate that our backdoor attacks are effective, stealthy and
robust against various defense solutions. Our codes and experimental videos can
be found in https://sites.google.com/view/lane-detection-attack/lda.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering. (arXiv:2203.03949v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03949">
<div class="article-summary-box-inner">
<span><p>Finding accurate correspondences among different views is the Achilles' heel
of unsupervised Multi-View Stereo (MVS). Existing methods are built upon the
assumption that corresponding pixels share similar photometric features.
However, multi-view images in real scenarios observe non-Lambertian surfaces
and experience occlusions. In this work, we propose a novel approach with
neural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences
among views. Specifically, we impose a depth rendering consistency loss to
constrain the geometry features close to the object surface to alleviate
occlusions. Concurrently, we introduce a reference view synthesis loss to
generate consistent supervision, even for non-Lambertian surfaces. Extensive
experiments on DTU and Tanks\&amp;Temples benchmarks demonstrate that our RC-MVSNet
approach achieves state-of-the-art performance over unsupervised MVS frameworks
and competitive performance to many supervised methods.The code is released at
https://github.com/Boese0601/RC-MVSNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Detection as Probabilistic Set Prediction. (arXiv:2203.07980v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07980">
<div class="article-summary-box-inner">
<span><p>Accurate uncertainty estimates are essential for deploying deep object
detectors in safety-critical systems. The development and evaluation of
probabilistic object detectors have been hindered by shortcomings in existing
performance measures, which tend to involve arbitrary thresholds or limit the
detector's choice of distributions. In this work, we propose to view object
detection as a set prediction task where detectors predict the distribution
over the set of objects. Using the negative log-likelihood for random finite
sets, we present a proper scoring rule for evaluating and training
probabilistic object detectors. The proposed method can be applied to existing
probabilistic detectors, is free from thresholds, and enables fair comparison
between architectures. Three different types of detectors are evaluated on the
COCO dataset. Our results indicate that the training of existing detectors is
optimized toward non-probabilistic metrics. We hope to encourage the
development of new object detectors that can accurately estimate their own
uncertainty. Code available at https://github.com/georghess/pmb-nll.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers. (arXiv:2203.17270v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17270">
<div class="article-summary-box-inner">
<span><p>3D visual perception tasks, including 3D detection and map segmentation based
on multi-camera images, are essential for autonomous driving systems. In this
work, we present a new framework termed BEVFormer, which learns unified BEV
representations with spatiotemporal transformers to support multiple autonomous
driving perception tasks. In a nutshell, BEVFormer exploits both spatial and
temporal information by interacting with spatial and temporal space through
predefined grid-shaped BEV queries. To aggregate spatial information, we design
spatial cross-attention that each BEV query extracts the spatial features from
the regions of interest across camera views. For temporal information, we
propose temporal self-attention to recurrently fuse the history BEV
information. Our approach achieves the new state-of-the-art 56.9\% in terms of
NDS metric on the nuScenes \texttt{test} set, which is 9.0 points higher than
previous best arts and on par with the performance of LiDAR-based baselines. We
further show that BEVFormer remarkably improves the accuracy of velocity
estimation and recall of objects under low visibility conditions. The code is
available at \url{https://github.com/zhiqi-li/BEVFormer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dress Code: High-Resolution Multi-Category Virtual Try-On. (arXiv:2204.08532v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08532">
<div class="article-summary-box-inner">
<span><p>Image-based virtual try-on strives to transfer the appearance of a clothing
item onto the image of a target person. Prior work focuses mainly on upper-body
clothes (e.g. t-shirts, shirts, and tops) and neglects full-body or lower-body
items. This shortcoming arises from a main factor: current publicly available
datasets for image-based virtual try-on do not account for this variety, thus
limiting progress in the field. To address this deficiency, we introduce Dress
Code, which contains images of multi-category clothes. Dress Code is more than
3x larger than publicly available datasets for image-based virtual try-on and
features high-resolution paired images (1024x768) with front-view, full-body
reference models. To generate HD try-on images with high visual quality and
rich in details, we propose to learn fine-grained discriminating features.
Specifically, we leverage a semantic-aware discriminator that makes predictions
at pixel-level instead of image- or patch-level. Extensive experimental
evaluation demonstrates that the proposed approach surpasses the baselines and
state-of-the-art competitors in terms of visual quality and quantitative
results. The Dress Code dataset is publicly available at
https://github.com/aimagelab/dress-code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Negatives in Contrastive Learning for Unpaired Image-to-Image Translation. (arXiv:2204.11018v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11018">
<div class="article-summary-box-inner">
<span><p>Unpaired image-to-image translation aims to find a mapping between the source
domain and the target domain. To alleviate the problem of the lack of
supervised labels for the source images, cycle-consistency based methods have
been proposed for image structure preservation by assuming a reversible
relationship between unpaired images. However, this assumption only uses
limited correspondence between image pairs. Recently, contrastive learning (CL)
has been used to further investigate the image correspondence in unpaired image
translation by using patch-based positive/negative learning. Patch-based
contrastive routines obtain the positives by self-similarity computation and
recognize the rest patches as negatives. This flexible learning paradigm
obtains auxiliary contextualized information at a low cost. As the negatives
own an impressive sample number, with curiosity, we make an investigation based
on a question: are all negatives necessary for feature contrastive learning?
Unlike previous CL approaches that use negatives as much as possible, in this
paper, we study the negatives from an information-theoretic perspective and
introduce a new negative Pruning technology for Unpaired image-to-image
Translation (PUT) by sparsifying and ranking the patches. The proposed
algorithm is efficient, flexible and enables the model to learn essential
information between corresponding patches stably. By putting quality over
quantity, only a few negative patches are required to achieve better results.
Lastly, we validate the superiority, stability, and versatility of our model
through comparative experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PointInst3D: Segmenting 3D Instances by Points. (arXiv:2204.11402v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11402">
<div class="article-summary-box-inner">
<span><p>The current state-of-the-art methods in 3D instance segmentation typically
involve a clustering step, despite the tendency towards heuristics, greedy
algorithms, and a lack of robustness to the changes in data statistics. In
contrast, we propose a fully-convolutional 3D point cloud instance segmentation
method that works in a per-point prediction fashion. In doing so it avoids the
challenges that clustering-based methods face: introducing dependencies among
different tasks of the model. We find the key to its success is assigning a
suitable target to each sampled point. Instead of the commonly used static or
distance-based assignment strategies, we propose to use an Optimal Transport
approach to optimally assign target masks to the sampled points according to
the dynamic matching costs. Our approach achieves promising results on both
ScanNet and S3DIS benchmarks. The proposed approach removes intertask
dependencies and thus represents a simpler and more flexible 3D instance
segmentation framework than other competing methods, while achieving improved
segmentation accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Branch Classifiers of Multi-exit Architectures. (arXiv:2204.13347v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13347">
<div class="article-summary-box-inner">
<span><p>Multi-exit architectures consist of a backbone and branch classifiers that
offer shortened inference pathways to reduce the run-time of deep neural
networks. In this paper, we analyze different branching patterns that vary in
their allocation of computational complexity for the branch classifiers.
Constant-complexity branching keeps all branches the same, while
complexity-increasing and complexity-decreasing branching place more complex
branches later or earlier in the backbone respectively. Through extensive
experimentation on multiple backbones and datasets, we find that
complexity-decreasing branches are more effective than constant-complexity or
complexity-increasing branches, which achieve the best accuracy-cost trade-off.
We investigate a cause by using knowledge consistency to probe the effect of
adding branches onto a backbone. Our findings show that complexity-decreasing
branching yields the least disruption to the feature abstraction hierarchy of
the backbone, which explains the effectiveness of the branching patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn to Understand Negation in Video Retrieval. (arXiv:2205.00132v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00132">
<div class="article-summary-box-inner">
<span><p>Negation is a common linguistic skill that allows human to express what we do
NOT want. Naturally, one might expect video retrieval to support
natural-language queries with negation, e.g., finding shots of kids sitting on
the floor and not playing with a dog. However, the state-of-the-art deep
learning based video retrieval models lack such ability, as they are typically
trained on video description datasets such as MSR-VTT and VATEX that lack
negated descriptions. Their retrieved results basically ignore the negator in
the sample query, incorrectly returning videos showing kids playing with dog.
This paper presents the first study on learning to understand negation in video
retrieval and make contributions as follows. By re-purposing two existing
datasets (MSR-VTT and VATEX), we propose a new evaluation protocol for video
retrieval with negation. We propose a learning based method for training a
negation-aware video retrieval model. The key idea is to first construct a soft
negative caption for a specific training video by partially negating its
original caption, and then compute a bidirectionally constrained loss on the
triplet. This auxiliary loss is weightedly added to a standard retrieval loss.
Experiments on the re-purposed benchmarks show that re-training the CLIP
(Contrastive Language-Image Pre-Training) model by the proposed method clearly
improves its ability to handle queries with negation. In addition, the model
performance on the original benchmarks is also improved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Composition-aware Graphic Layout GAN for Visual-textual Presentation Designs. (arXiv:2205.00303v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00303">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the graphic layout generation problem of producing
high-quality visual-textual presentation designs for given images. We note that
image compositions, which contain not only global semantics but also spatial
information, would largely affect layout results. Hence, we propose a deep
generative model, dubbed as composition-aware graphic layout GAN (CGL-GAN), to
synthesize layouts based on the global and spatial visual contents of input
images. To obtain training images from images that already contain manually
designed graphic layout data, previous work suggests masking design elements
(e.g., texts and embellishments) as model inputs, which inevitably leaves hint
of the ground truth. We study the misalignment between the training inputs
(with hint masks) and test inputs (without masks), and design a novel domain
alignment module (DAM) to narrow this gap. For training, we built a large-scale
layout dataset which consists of 60,548 advertising posters with annotated
layout information. To evaluate the generated layouts, we propose three novel
metrics according to aesthetic intuitions. Through both quantitative and
qualitative evaluations, we demonstrate that the proposed model can synthesize
high-quality graphic layouts according to image compositions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-Aware Self-supervised Multi-Label Learning. (arXiv:2205.07028v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07028">
<div class="article-summary-box-inner">
<span><p>Multi-label Learning on Image data has been widely exploited with deep
learning models. However, supervised training on deep CNN models often cannot
discover sufficient discriminative features for classification. As a result,
numerous self-supervision methods are proposed to learn more robust image
representations. However, most self-supervised approaches focus on
single-instance single-label data and fall short on more complex images with
multiple objects. Therefore, we propose an Object-Aware Self-Supervision (OASS)
method to obtain more fine-grained representations for multi-label learning,
dynamically generating auxiliary tasks based on object locations. Secondly, the
robust representation learned by OASS can be leveraged to efficiently generate
Class-Specific Instances (CSI) in a proposal-free fashion to better guide
multi-label supervision signal transfer to instances. Extensive experiments on
the VOC2012 dataset for multi-label classification demonstrate the
effectiveness of the proposed method against the state-of-the-art counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Preserving Image Registration. (arXiv:2205.10120v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10120">
<div class="article-summary-box-inner">
<span><p>Image registration is a key task in medical imaging applications, allowing to
represent medical images in a common spatial reference frame. Current
literature on image registration is generally based on the assumption that
images are usually accessible to the researcher, from which the spatial
transformation is subsequently estimated. This common assumption may not be met
in current practical applications, since the sensitive nature of medical images
may ultimately require their analysis under privacy constraints, preventing to
share the image content in clear form. In this work, we formulate the problem
of image registration under a privacy preserving regime, where images are
assumed to be confidential and cannot be disclosed in clear. We derive our
privacy preserving image registration framework by extending classical
registration paradigms to account for advanced cryptographic tools, such as
secure multi-party computation and homomorphic encryption, that enable the
execution of operations without leaking the underlying data. To overcome the
problem of performance and scalability of cryptographic tools in high
dimensions, we first propose to optimize the underlying image registration
operations using gradient approximations. We further revisit the use of
homomorphic encryption and use a packing method to allow the encryption and
multiplication of large matrices more efficiently. We demonstrate our privacy
preserving framework in linear and non-linear registration problems, evaluating
its accuracy and scalability with respect to standard image registration. Our
results show that privacy preserving image registration is feasible and can be
adopted in sensitive medical imaging applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DH-GAN: A Physics-driven Untrained Generative Adversarial Network for 3D Microscopic Imaging using Digital Holography. (arXiv:2205.12920v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12920">
<div class="article-summary-box-inner">
<span><p>Digital holography is a 3D imaging technique by emitting a laser beam with a
plane wavefront to an object and measuring the intensity of the diffracted
waveform, called holograms. The object's 3D shape can be obtained by numerical
analysis of the captured holograms and recovering the incurred phase. Recently,
deep learning (DL) methods have been used for more accurate holographic
processing. However, most supervised methods require large datasets to train
the model, which is rarely available in most DH applications due to the
scarcity of samples or privacy concerns. A few one-shot DL-based recovery
methods exist with no reliance on large datasets of paired images. Still, most
of these methods often neglect the underlying physics law that governs wave
propagation. These methods offer a black-box operation, which is not
explainable, generalizable, and transferrable to other samples and
applications. In this work, we propose a new DL architecture based on
generative adversarial networks that uses a discriminative network for
realizing a semantic measure for reconstruction quality while using a
generative network as a function approximator to model the inverse of hologram
formation. We impose smoothness on the background part of the recovered image
using a progressive masking module powered by simulated annealing to enhance
the reconstruction quality. The proposed method is one of its kind that
exhibits high transferability to similar samples, which facilitates its fast
deployment in time-sensitive applications without the need for retraining the
network. The results show a considerable improvement to competitor methods in
reconstruction quality (about 5 dB PSNR gain) and robustness to noise (about
50% reduction in PSNR vs noise increase rate).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedNST: Federated Noisy Student Training for Automatic Speech Recognition. (arXiv:2206.02797v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02797">
<div class="article-summary-box-inner">
<span><p>Federated Learning (FL) enables training state-of-the-art Automatic Speech
Recognition (ASR) models on user devices (clients) in distributed systems,
hence preventing transmission of raw user data to a central server. A key
challenge facing practical adoption of FL for ASR is obtaining ground-truth
labels on the clients. Existing approaches rely on clients to manually
transcribe their speech, which is impractical for obtaining large training
corpora. A promising alternative is using semi-/self-supervised learning
approaches to leverage unlabelled user data. To this end, we propose FedNST, a
novel method for training distributed ASR models using private and unlabelled
user data. We explore various facets of FedNST, such as training models with
different proportions of labelled and unlabelled data, and evaluate the
proposed approach on 1173 simulated clients. Evaluating FedNST on LibriSpeech,
where 960 hours of speech data is split equally into server (labelled) and
client (unlabelled) data, showed a 22.5% relative word error rate reduction}
(WERR) over a supervised baseline trained only on server data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Adversarial Attacks and Defenses in Vision Transformers trained with DINO. (arXiv:2206.06761v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06761">
<div class="article-summary-box-inner">
<span><p>This work conducts the first analysis on the robustness against adversarial
attacks on self-supervised Vision Transformers trained using DINO. First, we
evaluate whether features learned through self-supervision are more robust to
adversarial attacks than those emerging from supervised learning. Then, we
present properties arising for attacks in the latent space. Finally, we
evaluate whether three well-known defense strategies can increase adversarial
robustness in downstream tasks by only fine-tuning the classification head to
provide robustness even in view of limited compute resources. These defense
strategies are: Adversarial Training, Ensemble Adversarial Training and
Ensemble of Specialized Networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label. (arXiv:2207.00278v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00278">
<div class="article-summary-box-inner">
<span><p>Due to its powerful feature learning capability and high efficiency, deep
hashing has achieved great success in large-scale image retrieval. Meanwhile,
extensive works have demonstrated that deep neural networks (DNNs) are
susceptible to adversarial examples, and exploring adversarial attack against
deep hashing has attracted many research efforts. Nevertheless, backdoor
attack, another famous threat to DNNs, has not been studied for deep hashing
yet. Although various backdoor attacks have been proposed in the field of image
classification, existing approaches failed to realize a truly imperceptive
backdoor attack that enjoys invisible triggers and clean label setting
simultaneously, and they also cannot meet the intrinsic demand of image
retrieval backdoor. In this paper, we propose BadHash, the first
generative-based imperceptible backdoor attack against deep hashing, which can
effectively generate invisible and input-specific poisoned images with clean
label. Specifically, we first propose a new conditional generative adversarial
network (cGAN) pipeline to effectively generate poisoned samples. For any given
benign image, it seeks to generate a natural-looking poisoned counterpart with
a unique invisible trigger. In order to improve the attack effectiveness, we
introduce a label-based contrastive learning network LabCLN to exploit the
semantic characteristics of different labels, which are subsequently used for
confusing and misleading the target model to learn the embedded trigger. We
finally explore the mechanism of backdoor attacks on image retrieval in the
hash space. Extensive experiments on multiple benchmark datasets verify that
BadHash can generate imperceptible poisoned samples with strong attack ability
and transferability over state-of-the-art deep hashing schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patient-specific modelling, simulation and real time processing for respiratory diseases. (arXiv:2207.01082v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01082">
<div class="article-summary-box-inner">
<span><p>Asthma is a common chronic disease of the respiratory system causing
significant disability and societal burden. It affects over 500 million people
worldwide and generates costs exceeding $USD 56 billion in 2011 in the United
States. Managing asthma involves controlling symptoms, preventing
exacerbations, and maintaining lung function. Improving asthma control affects
the daily life of patients and is associated with a reduced risk of
exacerbations and lung function impairment, reduces the cost of asthma care and
indirect costs associated with reduced productivity. Understanding the complex
dynamics of the pulmonary system and the lung's response to disease, injury,
and treatment is fundamental to the advancement of Asthma treatment.
Computational models of the respiratory system seek to provide a theoretical
framework to understand the interaction between structure and function. Their
application can improve pulmonary medicine by a patient-specific approach to
medicinal methodologies optimizing the delivery given the personalized geometry
and personalized ventilation patterns while introducing a patient-specific
technique that maximizes drug delivery. A three-fold objective addressed within
this dissertation becomes prominent at this point. The first part refers to the
comprehension of pulmonary pathophysiology and the mechanics of Asthma and
subsequently of constrictive pulmonary conditions in general. The second part
refers to the design and implementation of tools that facilitate personalized
medicine to improve delivery and effectiveness. Finally, the third part refers
to the self-management of the condition, meaning that medical personnel and
patients have access to tools and methods that allow the first party to easily
track the course of the condition and the second party, i.e. the patient to
easily self-manage it alleviating the significant burden from the health
system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Vehicle Detection and Tracking on Fisheye Traffic Monitoring Video using CNN and Bounding Box Propagation. (arXiv:2207.01183v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01183">
<div class="article-summary-box-inner">
<span><p>We design a fast car detection and tracking algorithm for traffic monitoring
fisheye video mounted on crossroads. We use ICIP 2020 VIP Cup dataset and adopt
YOLOv5 as the object detection base model. The nighttime video of this dataset
is very challenging, and the detection accuracy (AP50) of the base model is
about 54%. We design a reliable car detection and tracking algorithm based on
the concept of bounding box propagation among frames, which provides 17.9
percentage points (pp) and 6.2 pp. accuracy improvement over the base model for
the nighttime and daytime videos, respectively. To speed up, the grayscale
frame difference is used for the intermediate frames in a segment, which can
double the processing speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Much More Data Do I Need? Estimating Requirements for Downstream Tasks. (arXiv:2207.01725v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01725">
<div class="article-summary-box-inner">
<span><p>Given a small training data set and a learning algorithm, how much more data
is necessary to reach a target validation or test performance? This question is
of critical importance in applications such as autonomous driving or medical
imaging where collecting data is expensive and time-consuming. Overestimating
or underestimating data requirements incurs substantial costs that could be
avoided with an adequate budget. Prior work on neural scaling laws suggest that
the power-law function can fit the validation performance curve and extrapolate
it to larger data set sizes. We find that this does not immediately translate
to the more difficult downstream task of estimating the required data set size
to meet a target performance. In this work, we consider a broad class of
computer vision tasks and systematically investigate a family of functions that
generalize the power-law function to allow for better estimation of data
requirements. Finally, we show that incorporating a tuned correction factor and
collecting over multiple rounds significantly improves the performance of the
data estimators. Using our guidelines, practitioners can accurately estimate
data requirements of machine learning systems to gain savings in both
development time and data acquisition costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch. (arXiv:2207.01795v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01795">
<div class="article-summary-box-inner">
<span><p>Adversarial patch attacks mislead neural networks by injecting adversarial
pixels within a local region. Patch attacks can be highly effective in a
variety of tasks and physically realizable via attachment (e.g. a sticker) to
the real-world objects. Despite the diversity in attack patterns, adversarial
patches tend to be highly textured and different in appearance from natural
images. We exploit this property and present PatchZero, a general defense
pipeline against white-box adversarial patches without retraining the
downstream classifier or detector. Specifically, our defense detects
adversaries at the pixel-level and "zeros out" the patch region by repainting
with mean pixel values. We further design a two-stage adversarial training
scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA
defense performance on the image classification (ImageNet, RESISC45), object
detection (PASCAL VOC), and video classification (UCF101) tasks with little
degradation in benign performance. In addition, PatchZero transfers to
different patch shapes and attack types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RepMix: Representation Mixing for Robust Attribution of Synthesized Images. (arXiv:2207.02063v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02063">
<div class="article-summary-box-inner">
<span><p>Rapid advances in Generative Adversarial Networks (GANs) raise new challenges
for image attribution; detecting whether an image is synthetic and, if so,
determining which GAN architecture created it. Uniquely, we present a solution
to this task capable of 1) matching images invariant to their semantic content;
2) robust to benign transformations (changes in quality, resolution, shape,
etc.) commonly encountered as images are re-shared online. In order to
formalize our research, a challenging benchmark, Attribution88, is collected
for robust and practical image attribution. We then propose RepMix, our GAN
fingerprinting technique based on representation mixing and a novel loss. We
validate its capability of tracing the provenance of GAN-generated images
invariant to the semantic content of the image and also robust to
perturbations. We show our approach improves significantly from existing GAN
fingerprinting works on both semantic generalization and robustness. Data and
code are available at https://github.com/TuBui/image_attribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling. (arXiv:2207.02196v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02196">
<div class="article-summary-box-inner">
<span><p>Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GaitTAKE: Gait Recognition by Temporal Attention and Keypoint-guided Embedding. (arXiv:2207.03608v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03608">
<div class="article-summary-box-inner">
<span><p>Gait recognition, which refers to the recognition or identification of a
person based on their body shape and walking styles, derived from video data
captured from a distance, is widely used in crime prevention, forensic
identification, and social security. However, to the best of our knowledge,
most of the existing methods use appearance, posture and temporal feautures
without considering a learned temporal attention mechanism for global and local
information fusion. In this paper, we propose a novel gait recognition
framework, called Temporal Attention and Keypoint-guided Embedding (GaitTAKE),
which effectively fuses temporal-attention-based global and local appearance
feature and temporal aggregated human pose feature. Experimental results show
that our proposed method achieves a new SOTA in gait recognition with rank-1
accuracy of 98.0% (normal), 97.5% (bag) and 92.2% (coat) on the CASIA-B gait
dataset; 90.4% accuracy on the OU-MVLP gait dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis. (arXiv:2207.03800v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03800">
<div class="article-summary-box-inner">
<span><p>Unconstrained lip-to-speech synthesis aims to generate corresponding speeches
from silent videos of talking faces with no restriction on head poses or
vocabulary. Current works mainly use sequence-to-sequence models to solve this
problem, either in an autoregressive architecture or a flow-based
non-autoregressive architecture. However, these models suffer from several
drawbacks: 1) Instead of directly generating audios, they use a two-stage
pipeline that first generates mel-spectrograms and then reconstructs audios
from the spectrograms. This causes cumbersome deployment and degradation of
speech quality due to error propagation; 2) The audio reconstruction algorithm
used by these models limits the inference speed and audio quality, while neural
vocoders are not available for these models since their output spectrograms are
not accurate enough; 3) The autoregressive model suffers from high inference
latency, while the flow-based model has high memory occupancy: neither of them
is efficient enough in both time and memory usage. To tackle these problems, we
propose FastLTS, a non-autoregressive end-to-end model which can directly
synthesize high-quality speech audios from unconstrained talking videos with
low latency, and has a relatively small model size. Besides, different from the
widely used 3D-CNN visual frontend for lip movement encoding, we for the first
time propose a transformer-based visual frontend for this task. Experiments
show that our model achieves $19.76\times$ speedup for audio waveform
generation compared with the current autoregressive model on input sequences of
3 seconds, and obtains superior audio quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet. (arXiv:2207.04320v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04320">
<div class="article-summary-box-inner">
<span><p>Multi-person pose understanding from RGB videos includes three complex tasks:
pose estimation, tracking and motion forecasting. Among these three tasks, pose
estimation and tracking are correlated, and tracking is crucial to motion
forecasting. Most existing works either focus on a single task or employ
cascaded methods to solve each individual task separately. In this paper, we
propose Snipper, a framework to perform multi-person 3D pose estimation,
tracking and motion forecasting simultaneously in a single inference.
Specifically, we first propose a deformable attention mechanism to aggregate
spatiotemporal information from video snippets. Building upon this deformable
attention, a visual transformer is learned to encode the spatiotemporal
features from multi-frame images and to decode informative pose features to
update multi-person pose queries. Last, these queries are regressed to predict
multi-person pose trajectories and future motions in one forward pass. In the
experiments, we show the effectiveness of Snipper on three challenging public
datasets where a generic model rivals specialized state-of-art baselines for
pose estimation, tracking, and forecasting. Code is available at
https://github.com/JimmyZou/Snipper
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Coding Using Learned Latent GAN Compression. (arXiv:2207.04324v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04324">
<div class="article-summary-box-inner">
<span><p>We propose in this paper a new paradigm for facial video compression. We
leverage the generative capacity of GANs such as StyleGAN to represent and
compress a video, including intra and inter compression. Each frame is inverted
in the latent space of StyleGAN, from which the optimal compression is learned.
To do so, a diffeomorphic latent representation is learned using a normalizing
flows model, where an entropy model can be optimized for image coding. In
addition, we propose a new perceptual loss that is more efficient than other
counterparts. Finally, an entropy model for video inter coding with residual is
also learned in the previously constructed latent representation. Our method
(SGANC) is simple, faster to train, and achieves better results for image and
video coding compared to state-of-the-art codecs such as VTM, AV1, and recent
deep learning techniques. In particular, it drastically minimizes perceptual
distortion at low bit rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DCCF: Deep Comprehensible Color Filter Learning Framework for High-Resolution Image Harmonization. (arXiv:2207.04788v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04788">
<div class="article-summary-box-inner">
<span><p>Image color harmonization algorithm aims to automatically match the color
distribution of foreground and background images captured in different
conditions. Previous deep learning based models neglect two issues that are
critical for practical applications, namely high resolution (HR) image
processing and model comprehensibility. In this paper, we propose a novel Deep
Comprehensible Color Filter (DCCF) learning framework for high-resolution image
harmonization. Specifically, DCCF first downsamples the original input image to
its low-resolution (LR) counter-part, then learns four human comprehensible
neural filters (i.e. hue, saturation, value and attentive rendering filters) in
an end-to-end manner, finally applies these filters to the original input image
to get the harmonized result. Benefiting from the comprehensible neural
filters, we could provide a simple yet efficient handler for users to cooperate
with deep model to get the desired results with very little effort when
necessary. Extensive experiments demonstrate the effectiveness of DCCF learning
framework and it outperforms state-of-the-art post-processing method on
iHarmony4 dataset on images' full-resolutions by achieving 7.63% and 1.69%
relative improvements on MSE and PSNR respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer. (arXiv:2207.04808v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04808">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to devise a universally versatile style transfer method
capable of performing artistic, photo-realistic, and video style transfer
jointly, without seeing videos during training. Previous single-frame methods
assume a strong constraint on the whole image to maintain temporal consistency,
which could be violated in many cases. Instead, we make a mild and reasonable
assumption that global inconsistency is dominated by local inconsistencies and
devise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local
patches. CCPL can preserve the coherence of the content source during style
transfer without degrading stylization. Moreover, it owns a neighbor-regulating
mechanism, resulting in a vast reduction of local distortions and considerable
visual quality improvement. Aside from its superior performance on versatile
style transfer, it can be easily extended to other tasks, such as
image-to-image translation. Besides, to better fuse content and style features,
we propose Simple Covariance Transformation (SCT) to effectively align
second-order statistics of the content feature with the style feature.
Experiments demonstrate the effectiveness of the resulting model for versatile
style transfer, when armed with CCPL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Invariances in Self-supervised Pre-training for 3D Vision. (arXiv:2207.04997v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04997">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-training for 3D vision has drawn increasing research
interest in recent years. In order to learn informative representations, a lot
of previous works exploit invariances of 3D features, e.g.,
perspective-invariance between views of the same scene, modality-invariance
between depth and RGB images, format-invariance between point clouds and
voxels. Although they have achieved promising results, previous researches lack
a systematic and fair comparison of these invariances. To address this issue,
our work, for the first time, introduces a unified framework, under which
various pre-training methods can be investigated. We conduct extensive
experiments and provide a closer look at the contributions of different
invariances in 3D pre-training. Also, we propose a simple but effective method
that jointly pre-trains a 3D encoder and a depth map encoder using contrastive
learning. Models pre-trained with our method gain significant performance boost
in downstream tasks. For instance, a pre-trained VoteNet outperforms previous
methods on SUN RGB-D and ScanNet object detection benchmarks with a clear
margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intra-Modal Constraint Loss For Image-Text Retrieval. (arXiv:2207.05024v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05024">
<div class="article-summary-box-inner">
<span><p>Cross-modal retrieval has drawn much attention in both computer vision and
natural language processing domains. With the development of convolutional and
recurrent neural networks, the bottleneck of retrieval across image-text
modalities is no longer the extraction of image and text features but an
efficient loss function learning in embedding space. Many loss functions try to
closer pairwise features from heterogeneous modalities. This paper proposes a
method for learning joint embedding of images and texts using an intra-modal
constraint loss function to reduce the violation of negative pairs from the
same homogeneous modality. Experimental results show that our approach
outperforms state-of-the-art bi-directional image-text retrieval methods on
Flickr30K and Microsoft COCO datasets. Our code is publicly available:
https://github.com/CanonChen/IMC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Susceptibility of Continual Learning Against Adversarial Attacks. (arXiv:2207.05225v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05225">
<div class="article-summary-box-inner">
<span><p>The recent advances in continual (incremental or lifelong) learning have
concentrated on the prevention of forgetting that can lead to catastrophic
consequences, but there are two outstanding challenges that must be addressed.
The first is the evaluation of the robustness of the proposed methods. The
second is ensuring the security of learned tasks remains largely unexplored.
This paper presents a comprehensive study of the susceptibility of the
continually learned tasks (including both current and previously learned tasks)
that are vulnerable to forgetting. Such vulnerability of tasks against
adversarial attacks raises profound issues in data integrity and privacy. We
consider the task incremental learning (Task-IL) scenario and explore three
regularization-based experiments, three replay-based experiments, and one
hybrid technique based on the reply and exemplar approach. We examine the
robustness of these methods. In particular, we consider cases where we
demonstrate that any class belonging to the current or previously learned tasks
is prone to misclassification. Our observations highlight the potential
limitations of existing Task-IL approaches. Our empirical study recommends that
the research community consider the robustness of the proposed continual
learning approaches and invest extensive efforts in mitigating catastrophic
forgetting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Human Vision Inspired Action Recognition using Adaptive Spatiotemporal Sampling. (arXiv:2207.05249v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05249">
<div class="article-summary-box-inner">
<span><p>Adaptive sampling that exploits the spatiotemporal redundancy in videos is
critical for always-on action recognition on wearable devices with limited
computing and battery resources. The commonly used fixed sampling strategy is
not context-aware and may under-sample the visual content, and thus adversely
impacts both computation efficiency and accuracy. Inspired by the concepts of
foveal vision and pre-attentive processing from the human visual perception
mechanism, we introduce a novel adaptive spatiotemporal sampling scheme for
efficient action recognition. Our system pre-scans the global scene context at
low-resolution and decides to skip or request high-resolution features at
salient regions for further processing. We validate the system on EPIC-KITCHENS
and UCF-101 datasets for action recognition, and show that our proposed
approach can greatly speed up inference with a tolerable loss of accuracy
compared with those from state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios. (arXiv:2207.05501v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05501">
<div class="article-summary-box-inner">
<span><p>Due to the complex attention mechanisms and model design, most existing
vision Transformers (ViTs) can not perform as efficiently as convolutional
neural networks (CNNs) in realistic industrial deployment scenarios, e.g.
TensorRT and CoreML. This poses a distinct challenge: Can a visual neural
network be designed to infer as fast as CNNs and perform as powerful as ViTs?
Recent works have tried to design CNN-Transformer hybrid architectures to
address this issue, yet the overall performance of these works is far away from
satisfactory. To end these, we propose a next generation vision Transformer for
efficient deployment in realistic industrial scenarios, namely Next-ViT, which
dominates both CNNs and ViTs from the perspective of latency/accuracy
trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer
Block (NTB) are respectively developed to capture local and global information
with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is
designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts
performance in various downstream tasks. Extensive experiments show that
Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer
hybrid architectures with respect to the latency/accuracy trade-off across
various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.4 mAP (from
40.4 to 45.8) on COCO detection and 8.2% mIoU (from 38.8% to 47.0%) on ADE20K
segmentation under similar latency. Meanwhile, it achieves comparable
performance with CSWin, while the inference speed is accelerated by 3.6x. On
CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on
COCO detection and 3.5% mIoU (from 45.2% to 48.7%) on ADE20K segmentation under
similar latency. Code will be released recently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LudVision -- Remote Detection of Exotic Invasive Aquatic Floral Species using Drone-Mounted Multispectral Data. (arXiv:2207.05620v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05620">
<div class="article-summary-box-inner">
<span><p>Remote sensing is the process of detecting and monitoring the physical
characteristics of an area by measuring its reflected and emitted radiation at
a distance. It is being broadly used to monitor ecosystems, mainly for their
preservation. Ever-growing reports of invasive species have affected the
natural balance of ecosystems. Exotic invasive species have a critical impact
when introduced into new ecosystems and may lead to the extinction of native
species. In this study, we focus on Ludwigia peploides, considered by the
European Union as an aquatic invasive species. Its presence can negatively
impact the surrounding ecosystem and human activities such as agriculture,
fishing, and navigation. Our goal was to develop a method to identify the
presence of the species. We used images collected by a drone-mounted
multispectral sensor to achieve this, creating our LudVision data set. To
identify the targeted species on the collected images, we propose a new method
for detecting Ludwigia p. in multispectral images. The method is based on
existing state-of-the-art semantic segmentation methods modified to handle
multispectral data. The proposed method achieved a producer's accuracy of 79.9%
and a user's accuracy of 95.5%.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-14 23:09:43.552561599 UTC">2022-07-14 23:09:43 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>