<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-12-30T01:30:00Z">12-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Social Ontological Knowledge Representations be Measured Using Machine Learning?. (arXiv:2112.13870v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13870">
<div class="article-summary-box-inner">
<span><p>Personal Social Ontology (PSO), it is proposed, is how an individual
perceives the ontological properties of terms. For example, an absolute
fatalist would arguably use terms that remove any form of agency from a person.
Such fatalism has the impact of ontologically defining acts such as winning,
victory and success, for example, in a manner that is contrary to how a
non-fatalist would ontologically define them. While both a fatalist and
non-fatalist would agree on the dictionary definition of these terms, they
would differ on what and how they can be caused. This difference between the
two individuals, it is argued, can be induced from the co-occurrence of terms
used by each individual. That such co-occurrence carries an implied social
ontology, one that is specific to that person. The use of principal social
perceptions -as evidenced by the social psychology and social neuroscience
literature, is put forward as a viable method to feature engineer such texts.
With the natural language characterisation of these features, they are then
usable in machine learning pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?. (arXiv:2112.13906v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13906">
<div class="article-summary-box-inner">
<span><p>Contrastive Language--Image Pre-training (CLIP) has shown remarkable success
in learning with cross-modal supervision from extensive amounts of image--text
pairs collected online. Thus far, the effectiveness of CLIP has been
investigated primarily in general-domain multimodal problems. This work
evaluates the effectiveness of CLIP for the task of Medical Visual Question
Answering (MedVQA). To this end, we present PubMedCLIP, a fine-tuned version of
CLIP for the medical domain based on PubMed articles. Our experiments are
conducted on two MedVQA benchmark datasets and investigate two MedVQA methods,
MEVF (Mixture of Enhanced Visual Features) and QCR (Question answering via
Conditional Reasoning). For each of these, we assess the merits of visual
representation learning using PubMedCLIP, the original CLIP, and
state-of-the-art MAML (Model-Agnostic Meta-Learning) networks pre-trained only
on visual data. We open source the code for our MedVQA pipeline and
pre-training PubMedCLIP. CLIP and PubMedCLIP achieve improvements in comparison
to MAML's visual encoder. PubMedCLIP achieves the best results with gains in
the overall accuracy of up to 3%. Individual examples illustrate the strengths
of PubMedCLIP in comparison to the previously widely used MAML networks. Visual
representation learning with language supervision in PubMedCLIP leads to
noticeable improvements for MedVQA. Our experiments reveal distributional
differences in the two MedVQA benchmark datasets that have not been imparted in
previous work and cause different back-end visual encoders in PubMedCLIP to
exhibit different behavior on these datasets. Moreover, we witness fundamental
performance differences of VQA in general versus medical domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Persuasion in COVID-19 Social Media Content: A Multi-Modal Characterization. (arXiv:2112.13910v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13910">
<div class="article-summary-box-inner">
<span><p>Social media content routinely incorporates multi-modal design to covey
information and shape meanings, and sway interpretations toward desirable
implications, but the choices and outcomes of using both texts and visual
images have not been sufficiently studied. This work proposes a computational
approach to analyze the outcome of persuasive information in multi-modal
content, focusing on two aspects, popularity and reliability, in
COVID-19-related news articles shared on Twitter. The two aspects are
intertwined in the spread of misinformation: for example, an unreliable article
that aims to misinform has to attain some popularity. This work has several
contributions. First, we propose a multi-modal (image and text) approach to
effectively identify popularity and reliability of information sources
simultaneously. Second, we identify textual and visual elements that are
predictive to information popularity and reliability. Third, by modeling
cross-modal relations and similarity, we are able to uncover how unreliable
articles construct multi-modal meaning in a distorted, biased fashion. Our work
demonstrates how to use multi-modal analysis for understanding influential
content and has implications to social media literacy and engagement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The University of Texas at Dallas HLTRI's Participation in EPIC-QA: Searching for Entailed Questions Revealing Novel Answer Nuggets. (arXiv:2112.13946v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13946">
<div class="article-summary-box-inner">
<span><p>The Epidemic Question Answering (EPIC-QA) track at the Text Analysis
Conference (TAC) is an evaluation of methodologies for answering ad-hoc
questions about the COVID-19 disease. This paper describes our participation in
both tasks of EPIC-QA, targeting: (1) Expert QA and (2) Consumer QA. Our
methods used a multi-phase neural Information Retrieval (IR) system based on
combining BM25, BERT, and T5 as well as the idea of considering entailment
relations between the original question and questions automatically generated
from answer candidate sentences. Moreover, because entailment relations were
also considered between all generated questions, we were able to re-rank the
answer sentences based on the number of novel answer nuggets they contained, as
indicated by the processing of a question entailment graph. Our system, called
SEaRching for Entailed QUestions revealing NOVel nuggets of Answers
(SER4EQUNOVA), produced promising results in both EPIC-QA tasks, excelling in
the Expert QA task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Preordered RNN Layer Boosts Neural Machine Translation in Low Resource Settings. (arXiv:2112.13960v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13960">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation (NMT) models are strong enough to convey semantic
and syntactic information from the source language to the target language.
However, these models are suffering from the need for a large amount of data to
learn the parameters. As a result, for languages with scarce data, these models
are at risk of underperforming. We propose to augment attention based neural
network with reordering information to alleviate the lack of data. This
augmentation improves the translation quality for both English to Persian and
Persian to English by up to 6% BLEU absolute over the baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LINDA: Unsupervised Learning to Interpolate in Natural Language Processing. (arXiv:2112.13969v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13969">
<div class="article-summary-box-inner">
<span><p>Despite the success of mixup in data augmentation, its applicability to
natural language processing (NLP) tasks has been limited due to the discrete
and variable-length nature of natural languages. Recent studies have thus
relied on domain-specific heuristics and manually crafted resources, such as
dictionaries, in order to apply mixup in NLP. In this paper, we instead propose
an unsupervised learning approach to text interpolation for the purpose of data
augmentation, to which we refer as "Learning to INterpolate for Data
Augmentation" (LINDA), that does not require any heuristics nor manually
crafted resources but learns to interpolate between any pair of natural
language sentences over a natural language manifold. After empirically
demonstrating the LINDA's interpolation capability, we show that LINDA indeed
allows us to seamlessly apply mixup in NLP and leads to better generalization
in text classification both in-domain and out-of-domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Processing M.A. Castr\'en's Materials: Multilingual Typed and Handwritten Manuscripts. (arXiv:2112.14153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14153">
<div class="article-summary-box-inner">
<span><p>The study forms a technical report of various tasks that have been performed
on the materials collected and published by Finnish ethnographer and linguist,
Matthias Alexander Castr\'en (1813-1852). The Finno-Ugrian Society is
publishing Castr\'en's manuscripts as new critical and digital editions, and at
the same time different research groups have also paid attention to these
materials. We discuss the workflows and technical infrastructure used, and
consider how datasets that benefit different computational tasks could be
created to further improve the usability of these materials, and also to aid
the further processing of similar archived collections. We specifically focus
on the parts of the collections that are processed in a way that improves their
usability in more technical applications, complementing the earlier work on the
cultural and linguistic aspects of these materials. Most of these datasets are
openly available in Zenodo. The study points to specific areas where further
research is needed, and provides benchmarks for text recognition tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Gender Bias in Natural Language Processing. (arXiv:2112.14168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14168">
<div class="article-summary-box-inner">
<span><p>Language can be used as a means of reproducing and enforcing harmful
stereotypes and biases and has been analysed as such in numerous research. In
this paper, we present a survey of 304 papers on gender bias in natural
language processing. We analyse definitions of gender and its categories within
social sciences and connect them to formal definitions of gender bias in NLP
research. We survey lexica and datasets applied in research on gender bias and
then compare and contrast approaches to detecting and mitigating gender bias.
We find that research on gender bias suffers from four core limitations. 1)
Most research treats gender as a binary variable neglecting its fluidity and
continuity. 2) Most of the work has been conducted in monolingual setups for
English or other high-resource languages. 3) Despite a myriad of papers on
gender bias in NLP methods, we find that most of the newly developed algorithms
do not test their models for bias and disregard possible ethical considerations
of their work. 4) Finally, methodologies developed in this line of research are
fundamentally flawed covering very limited definitions of gender bias and
lacking evaluation baselines and pipelines. We suggest recommendations towards
overcoming these limitations as a guide for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Security Analysis Based on Random Geometry Theory for Satellite-Terrestrial-Vehicle Network. (arXiv:2112.14192v1 [cs.IT])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14192">
<div class="article-summary-box-inner">
<span><p>Driven by B5G and 6G technologies, multi-network fusion is an indispensable
tendency for future communications. In this paper, we focus on and analyze the
\emph{security performance} (SP) of the \emph{satellite-terrestrial downlink
transmission} (STDT). Here, the STDT is composed of a satellite network and a
vehicular network with a legitimate mobile receiver and an mobile eavesdropper
distributing. To theoretically analyze the SP of this system from the
perspective of mobile terminals better, the random geometry theory is adopted,
which assumes that both terrestrial vehicles are distributed stochastically in
one beam of the satellite. Furthermore, based on this theory, the closed-form
analytical expressions for two crucial and specific indicators in the STDT are
derived, respectively, the secrecy outage probability and the ergodic secrecy
capacity. Additionally, several related variables restricting the SP of the
STDT are discussed, and specific schemes are presented to enhance the SP. Then,
the asymptotic property is investigated in the high signal-to-noise ratio
scenario, and accurate and asymptotic closed-form expressions are given.
Finally, simulation results show that, under the precondition of guaranteeing
the reliability of the STDT, the asymptotic solutions outperform the
corresponding accurate results significantly in the effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mirror Matching: Document Matching Approach in Seed-driven Document Ranking for Medical Systematic Reviews. (arXiv:2112.14318v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14318">
<div class="article-summary-box-inner">
<span><p>When medical researchers conduct a systematic review (SR), screening studies
is the most time-consuming process: researchers read several thousands of
medical literature and manually label them relevant or irrelevant. Screening
prioritization (ie., document ranking) is an approach for assisting researchers
by providing document rankings where relevant documents are ranked higher than
irrelevant ones. Seed-driven document ranking (SDR) uses a known relevant
document (ie., seed) as a query and generates such rankings. Previous work on
SDR seeks ways to identify different term weights in a query document and
utilizes them in a retrieval model to compute ranking scores. Alternatively, we
formulate the SDR task as finding similar documents to a query document and
produce rankings based on similarity scores. We propose a document matching
measure named Mirror Matching, which calculates matching scores between medical
abstract texts by incorporating common writing patterns, such as background,
method, result, and conclusion in order. We conduct experiments on CLEF 2019
eHealth Task 2 TAR dataset, and the empirical results show this simple approach
achieves the higher performance than traditional and neural retrieval models on
Average Precision and Precision-focused metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora. (arXiv:2112.14330v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14330">
<div class="article-summary-box-inner">
<span><p>The problem of comparing two bodies of text and searching for words that
differ in their usage between them arises often in digital humanities and
computational social science. This is commonly approached by training word
embeddings on each corpus, aligning the vector spaces, and looking for words
whose cosine distance in the aligned space is large. However, these methods
often require extensive filtering of the vocabulary to perform well, and - as
we show in this work - result in unstable, and hence less reliable, results. We
propose an alternative approach that does not use vector space alignment, and
instead considers the neighbors of each word. The method is simple,
interpretable and stable. We demonstrate its effectiveness in 9 different
setups, considering different corpus splitting criteria (age, gender and
profession of tweet authors, time of tweet) and different languages (English,
French and Hebrew).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fake or Genuine? Contextualised Text Representation for Fake Review Detection. (arXiv:2112.14343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14343">
<div class="article-summary-box-inner">
<span><p>Online reviews have a significant influence on customers' purchasing
decisions for any products or services. However, fake reviews can mislead both
consumers and companies. Several models have been developed to detect fake
reviews using machine learning approaches. Many of these models have some
limitations resulting in low accuracy in distinguishing between fake and
genuine reviews. These models focused only on linguistic features to detect
fake reviews and failed to capture the semantic meaning of the reviews. To deal
with this, this paper proposes a new ensemble model that employs transformer
architecture to discover the hidden patterns in a sequence of fake reviews and
detect them precisely. The proposed approach combines three transformer models
to improve the robustness of fake and genuine behaviour profiling and modelling
to detect fake reviews. The experimental results using semi-real benchmark
datasets showed the superiority of the proposed model over state-of-the-art
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Learning for the Inverted Beta-Liouville Mixture Model and Its Application to Text Categorization. (arXiv:2112.14375v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14375">
<div class="article-summary-box-inner">
<span><p>The finite invert Beta-Liouville mixture model (IBLMM) has recently gained
some attention due to its positive data modeling capability. Under the
conventional variational inference (VI) framework, the analytically tractable
solution to the optimization of the variational posterior distribution cannot
be obtained, since the variational object function involves evaluation of
intractable moments. With the recently proposed extended variational inference
(EVI) framework, a new function is proposed to replace the original variational
object function in order to avoid intractable moment computation, so that the
analytically tractable solution of the IBLMM can be derived in an elegant way.
The good performance of the proposed approach is demonstrated by experiments
with both synthesized data and a real-world application namely text
categorization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency-Aware Contrastive Learning for Neural Machine Translation. (arXiv:2112.14484v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14484">
<div class="article-summary-box-inner">
<span><p>Low-frequency word prediction remains a challenge in modern neural machine
translation (NMT) systems. Recent adaptive training methods promote the output
of infrequent words by emphasizing their weights in the overall training
objectives. Despite the improved recall of low-frequency words, their
prediction precision is unexpectedly hindered by the adaptive objectives.
Inspired by the observation that low-frequency words form a more compact
embedding space, we tackle this challenge from a representation learning
perspective. Specifically, we propose a frequency-aware token-level contrastive
learning method, in which the hidden state of each decoding step is pushed away
from the counterparts of other target words, in a soft contrastive way based on
the corresponding word frequencies. We conduct experiments on widely used NIST
Chinese-English and WMT14 English-German translation tasks. Empirical results
show that our proposed methods can not only significantly improve the
translation quality but also enhance lexical diversity and optimize word
representation space. Further investigation reveals that, comparing with
related adaptive training strategies, the superiority of our method on
low-frequency word prediction lies in the robustness of token-level recall
across different frequencies without sacrificing precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuning Transformers: Vocabulary Transfer. (arXiv:2112.14569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14569">
<div class="article-summary-box-inner">
<span><p>Transformers are responsible for the vast majority of recent advances in
natural language processing. The majority of practical natural language
processing applications of these models is typically enabled through transfer
learning. This paper studies if corpus-specific tokenization used for
fine-tuning improves the resulting performance of the model. Through a series
of experiments, we demonstrate that such tokenization combined with the
initialization and fine-tuning strategy for the vocabulary tokens speeds up the
transfer and boosts the performance of the fine-tuned model. We call this
aspect of transfer facilitation vocabulary transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LeSICiN: A Heterogeneous Graph-based Approach for Automatic Legal Statute Identification from Indian Legal Documents. (arXiv:2112.14731v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14731">
<div class="article-summary-box-inner">
<span><p>The task of Legal Statute Identification (LSI) aims to identify the legal
statutes that are relevant to a given description of Facts or evidence of a
legal case. Existing methods only utilize the textual content of Facts and
legal articles to guide such a task. However, the citation network among case
documents and legal statutes is a rich source of additional information, which
is not considered by existing models. In this work, we take the first step
towards utilising both the text and the legal citation network for the LSI
task. We curate a large novel dataset for this task, including Facts of cases
from several major Indian Courts of Law, and statutes from the Indian Penal
Code (IPC). Modeling the statutes and training documents as a heterogeneous
graph, our proposed model LeSICiN can learn rich textual and graphical
features, and can also tune itself to correlate these features. Thereafter, the
model can be used to inductively predict links between test documents (new
nodes whose graphical features are not available to the model) and statutes
(existing nodes). Extensive experiments on the dataset show that our model
comfortably outperforms several state-of-the-art baselines, by exploiting the
graphical structure along with textual features. The dataset and our codes are
available at https://github.com/Law-AI/LeSICiN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On learning an interpreted language with recurrent models. (arXiv:1809.04128v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1809.04128">
<div class="article-summary-box-inner">
<span><p>Can recurrent neural nets, inspired by human sequential data processing,
learn to understand language? We construct simplified datasets reflecting core
properties of natural language as modeled in formal syntax and semantics:
recursive syntactic structure and compositionality. We find LSTM and GRU
networks to generalise to compositional interpretation well, but only in the
most favorable learning settings, with a well-paced curriculum, extensive
training data, and left-to-right (but not right-to-left) composition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Empathetic Responses by Looking Ahead the User's Sentiment. (arXiv:1906.08487v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.08487">
<div class="article-summary-box-inner">
<span><p>An important aspect of human conversation difficult for machines is
conversing with empathy, which is to understand the user's emotion and respond
appropriately. Recent neural conversation models that attempted to generate
empathetic responses either focused on conditioning the output to a given
emotion, or incorporating the current user emotional state. However, these
approaches do not factor in how the user would feel towards the generated
response. Hence, in this paper, we propose Sentiment Look-ahead, which is a
novel perspective for empathy that models the future user emotional state. In
short, Sentiment Look-ahead is a reward function under a reinforcement learning
framework that provides a higher reward to the generative model when the
generated utterance improves the user's sentiment. We implement and evaluate
three different possible implementations of sentiment look-ahead and
empirically show that our proposed approach can generate significantly more
empathetic, relevant, and fluent responses than other competitive baselines
such as multitask learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Bridging for Empathetic Dialogue Generation. (arXiv:2009.09708v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09708">
<div class="article-summary-box-inner">
<span><p>Lack of external knowledge makes empathetic dialogue systems difficult to
perceive implicit emotions and learn emotional interactions from limited
dialogue history. To address the above problems, we propose to leverage
external knowledge, including commonsense knowledge and emotional lexical
knowledge, to explicitly understand and express emotions in empathetic dialogue
generation. We first enrich the dialogue history by jointly interacting with
external knowledge and construct an emotional context graph. Then we learn
emotional context representations from the knowledge-enriched emotional context
graph and distill emotional signals, which are the prerequisites to predicate
emotions expressed in responses. Finally, to generate the empathetic response,
we propose an emotional cross-attention mechanism to learn the emotional
dependencies from the emotional context graph. Extensive experiments conducted
on a benchmark dataset verify the effectiveness of the proposed method. In
addition, we find the performance of our method can be further improved by
integrating with a pre-trained model that works orthogonally.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition. (arXiv:2012.05481v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05481">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel two-pass approach to unify streaming and
non-streaming end-to-end (E2E) speech recognition in a single model. Our model
adopts the hybrid CTC/attention architecture, in which the conformer layers in
the encoder are modified. We propose a dynamic chunk-based attention strategy
to allow arbitrary right context length. At inference time, the CTC decoder
generates n-best hypotheses in a streaming way. The inference latency could be
easily controlled by only changing the chunk size. The CTC hypotheses are then
rescored by the attention decoder to get the final result. This efficient
rescoring process causes very little sentence-level latency. Our experiments on
the open 170-hour AISHELL-1 dataset show that, the proposed method can unify
the streaming and non-streaming model simply and efficiently. On the AISHELL-1
test set, our unified model achieves 5.60% relative character error rate (CER)
reduction in non-streaming ASR compared to a standard non-streaming
transformer. The same model achieves 5.42% CER with 640ms latency in a
streaming ASR system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit. (arXiv:2102.01547v5 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01547">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an open source, production first, and production
ready speech recognition toolkit called WeNet in which a new two-pass approach
is implemented to unify streaming and non-streaming end-to-end (E2E) speech
recognition in a single model. The main motivation of WeNet is to close the gap
between the research and the production of E2E speechrecognition models. WeNet
provides an efficient way to ship ASR applications in several real-world
scenarios, which is the main difference and advantage to other open source E2E
speech recognition toolkits. In our toolkit, a new two-pass method is
implemented. Our method propose a dynamic chunk-based attention strategy of the
the transformer layers to allow arbitrary right context length modifies in
hybrid CTC/attention architecture. The inference latency could be easily
controlled by only changing the chunk size. The CTC hypotheses are then
rescored by the attention decoder to get the final result. Our experiments on
the AISHELL-1 dataset using WeNet show that, our model achieves 5.03\% relative
character error rate (CER) reduction in non-streaming ASR compared to a
standard non-streaming transformer. After model quantification, our model
perform reasonable RTF and latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending Multi-Sense Word Embedding to Phrases and Sentences for Unsupervised Semantic Applications. (arXiv:2103.15330v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15330">
<div class="article-summary-box-inner">
<span><p>Most unsupervised NLP models represent each word with a single point or
single region in semantic space, while the existing multi-sense word embeddings
cannot represent longer word sequences like phrases or sentences. We propose a
novel embedding method for a text sequence (a phrase or a sentence) where each
sequence is represented by a distinct set of multi-mode codebook embeddings to
capture different semantic facets of its meaning. The codebook embeddings can
be viewed as the cluster centers which summarize the distribution of possibly
co-occurring words in a pre-trained word embedding space. We introduce an
end-to-end trainable neural model that directly predicts the set of cluster
centers from the input text sequence during test time. Our experiments show
that the per-sentence codebook embeddings significantly improve the
performances in unsupervised sentence similarity and extractive summarization
benchmarks. In phrase similarity experiments, we discover that the multi-facet
embeddings provide an interpretable semantic representation but do not
outperform the single-facet baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Competency Problems: On Finding and Removing Artifacts in Language Data. (arXiv:2104.08646v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08646">
<div class="article-summary-box-inner">
<span><p>Much recent work in NLP has documented dataset artifacts, bias, and spurious
correlations between input features and output labels. However, how to tell
which features have "spurious" instead of legitimate correlations is typically
left unspecified. In this work we argue that for complex language understanding
tasks, all simple feature correlations are spurious, and we formalize this
notion into a class of problems which we call competency problems. For example,
the word "amazing" on its own should not give information about a sentiment
label independent of the context in which it appears, which could include
negation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of
creating data for competency problems when human bias is taken into account,
showing that realistic datasets will increasingly deviate from competency
problems as dataset size increases. This analysis gives us a simple statistical
test for dataset artifacts, which we use to show more subtle biases than were
described in prior work, including demonstrating that models are
inappropriately affected by these less extreme biases. Our theoretical
treatment of this problem also allows us to analyze proposed solutions, such as
making local edits to dataset instances, and to give recommendations for future
data collection and model design efforts that target competency problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Search Engines with Interactive Agents. (arXiv:2109.00527v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00527">
<div class="article-summary-box-inner">
<span><p>This paper presents first successful steps in designing agents that learn
meta-strategies for iterative query refinement. Our approach uses machine
reading to guide the selection of refinement terms from aggregated search
results. Agents are then empowered with simple but effective search operators
to exert fine-grained and transparent control over queries and search results.
We develop a novel way of generating synthetic search sessions, which leverages
the power of transformer-based language models through (self-)supervised
learning. We also present a reinforcement learning agent with dynamically
constrained actions that learns interactive search strategies from scratch. We
obtain retrieval and answer quality performance comparable to recent neural
methods using a traditional term-based BM25 ranking function. We provide an
in-depth analysis of the search policies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hocalarim: Mining Turkish Student Reviews. (arXiv:2109.02325v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02325">
<div class="article-summary-box-inner">
<span><p>We introduce Hocalarim (MyProfessors), the largest student review dataset
available for the Turkish language. It consists of over 5000 professor reviews
left online by students, with different aspects of education rated on a scale
of 1 to 5 stars. We investigate the properties of the dataset and present its
statistics. We examine the impact of students' institution type on their
ratings and the correlation of students' bias to give positive or negative
feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Categorical Semantics of Reversible Pattern-Matching. (arXiv:2109.05837v3 [cs.LO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05837">
<div class="article-summary-box-inner">
<span><p>This paper is concerned with categorical structures for reversible
computation. In particular, we focus on a typed, functional reversible language
based on Theseus. We discuss how join inverse rig categories do not in general
capture pattern-matching, the core construct Theseus uses to enforce
reversibility. We then derive a categorical structure to add to join inverse
rig categories in order to capture pattern-matching. We show how such a
structure makes an adequate model for reversible pattern-matching.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variance of Twitter Embeddings and Temporal Trends of COVID-19 cases. (arXiv:2110.00031v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00031">
<div class="article-summary-box-inner">
<span><p>The severity of the coronavirus pandemic necessitates the need of effective
administrative decisions. Over 4 lakh people in India succumbed to COVID-19,
with over 3 crore confirmed cases, and still counting. The threat of a
plausible third wave continues to haunt millions. In this ever changing dynamic
of the virus, predictive modeling methods can serve as an integral tool. The
pandemic has further triggered an unprecedented usage of social media. This
paper aims to propose a method for harnessing social media, specifically
Twitter, to predict the upcoming scenarios related to COVID-19 cases. In this
study, we seek to understand how the surges in COVID-19 related tweets can
indicate rise in the cases. This prospective analysis can be utilised to aid
administrators about timely resource allocation to lessen the severity of the
damage. Using word embeddings to capture the semantic meaning of tweets, we
identify Significant Dimensions (SDs).Our methodology predicts the rise in
cases with a lead time of 15 days and 30 days with R2 scores of 0.80 and 0.62
respectively. Finally, we explain the thematic utility of the SDs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEACh: Task-driven Embodied Agents that Chat. (arXiv:2110.00534v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00534">
<div class="article-summary-box-inner">
<span><p>Robots operating in human spaces must be able to engage in natural language
interaction with people, both understanding and executing instructions, and
using conversation to resolve ambiguity and recover from mistakes. To study
this, we introduce TEACh, a dataset of over 3,000 human--human, interactive
dialogues to complete household tasks in simulation. A Commander with access to
oracle information about a task communicates in natural language with a
Follower. The Follower navigates through and interacts with the environment to
complete tasks varying in complexity from "Make Coffee" to "Prepare Breakfast",
asking questions and getting additional information from the Commander. We
propose three benchmarks using TEACh to study embodied intelligence challenges,
and we evaluate initial models' abilities in dialogue understanding, language
grounding, and task execution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Proposed Conceptual Framework for a Representational Approach to Information Retrieval. (arXiv:2110.01529v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01529">
<div class="article-summary-box-inner">
<span><p>This paper outlines a conceptual framework for understanding recent
developments in information retrieval and natural language processing that
attempts to integrate dense and sparse retrieval methods. I propose a
representational approach that breaks the core text retrieval problem into a
logical scoring model and a physical retrieval model. The scoring model is
defined in terms of encoders, which map queries and documents into a
representational space, and a comparison function that computes query-document
scores. The physical retrieval model defines how a system produces the top-$k$
scoring documents from an arbitrarily large corpus with respect to a query. The
scoring model can be further analyzed along two dimensions: dense vs. sparse
representations and supervised (learned) vs. unsupervised approaches. I show
that many recently proposed retrieval methods, including multi-stage ranking
designs, can be seen as different parameterizations in this framework, and that
a unified view suggests a number of open research questions, providing a
roadmap for future work. As a bonus, this conceptual framework establishes
connections to sentence similarity tasks in natural language processing and
information access "technologies" prior to the dawn of computing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v4 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03370">
<div class="article-summary-box-inner">
<span><p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus
consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly
labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in
total. We collect the data from YouTube and Podcast, which covers a variety of
speaking styles, scenarios, domains, topics, and noisy conditions. An optical
character recognition (OCR) based method is introduced to generate the
audio/text segmentation candidates for the YouTube data on its corresponding
video captions, while a high-quality ASR transcription system is used to
generate audio/text pair candidates for the Podcast data. Then we propose a
novel end-to-end label error detection approach to further validate and filter
the candidates. We also provide three manually labelled high-quality test sets
along with WenetSpeech for evaluation -- Dev for cross-validation purpose in
training, Test_Net, collected from Internet for matched test, and
Test\_Meeting, recorded from real meetings for more challenging mismatched
test. Baseline systems trained with WenetSpeech are provided for three popular
speech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition
results on the three test sets are also provided as benchmarks. To the best of
our knowledge, WenetSpeech is the current largest open-sourced Mandarin speech
corpus with transcriptions, which benefits research on production-level speech
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MotifClass: Weakly Supervised Text Classification with Higher-order Metadata Information. (arXiv:2111.04022v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04022">
<div class="article-summary-box-inner">
<span><p>We study the problem of weakly supervised text classification, which aims to
classify text documents into a set of pre-defined categories with category
surface names only and without any annotated training document provided. Most
existing classifiers leverage textual information in each document. However, in
many domains, documents are accompanied by various types of metadata (e.g.,
authors, venue, and year of a research paper). These metadata and their
combinations may serve as strong category indicators in addition to textual
contents. In this paper, we explore the potential of using metadata to help
weakly supervised text classification. To be specific, we model the
relationships between documents and metadata via a heterogeneous information
network. To effectively capture higher-order structures in the network, we use
motifs to describe metadata combinations. We propose a novel framework, named
MotifClass, which (1) selects category-indicative motif instances, (2)
retrieves and generates pseudo-labeled training samples based on category names
and indicative motif instances, and (3) trains a text classifier using the
pseudo training data. Extensive experiments on real-world datasets demonstrate
the superior performance of MotifClass to existing weakly supervised text
classification approaches. Further analysis shows the benefit of considering
higher-order metadata information in our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pointer over Attention: An Improved Bangla Text Summarization Approach Using Hybrid Pointer Generator Network. (arXiv:2111.10269v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10269">
<div class="article-summary-box-inner">
<span><p>Despite the success of the neural sequence-to-sequence model for abstractive
text summarization, it has a few shortcomings, such as repeating inaccurate
factual details and tending to repeat themselves. We propose a hybrid pointer
generator network to solve the shortcomings of reproducing factual details
inadequately and phrase repetition. We augment the attention-based
sequence-to-sequence using a hybrid pointer generator network that can generate
Out-of-Vocabulary words and enhance accuracy in reproducing authentic details
and a coverage mechanism that discourages repetition. It produces a
reasonable-sized output text that preserves the conceptual integrity and
factual information of the input article. For evaluation, we primarily employed
"BANSData" - a highly adopted publicly available Bengali dataset. Additionally,
we prepared a large-scale dataset called "BANS-133" which consists of 133k
Bangla news articles associated with human-generated summaries. Experimenting
with the proposed model, we achieved ROUGE-1 and ROUGE-2 scores of 0.66, 0.41
for the "BANSData" dataset and 0.67, 0.42 for the BANS-133k" dataset,
respectively. We demonstrated that the proposed system surpasses previous
state-of-the-art Bengali abstractive summarization techniques and its stability
on a larger dataset. "BANS-133" datasets and code-base will be publicly
available for research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polite Emotional Dialogue Acts for Conversational Analysis in Daily Dialog Data. (arXiv:2112.13572v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13572">
<div class="article-summary-box-inner">
<span><p>Many socio-linguistic cues are used in the conversational analysis, such as
emotion, sentiment, and dialogue acts. One of the fundamental social cues is
politeness, which linguistically possesses properties useful in conversational
analysis. This short article presents some of the brief findings of polite
emotional dialogue acts, where we can correlate the relational bonds between
these socio-linguistics cues. We found that the utterances with emotion classes
Anger and Disgust are more likely to be impolite while Happiness and Sadness to
be polite. Similar phenomenon occurs with dialogue acts, Inform and Commissive
contain many polite utterances than Question and Directive. Finally, we will
conclude on the future work of these findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pedagogical Word Recommendation: A novel task and dataset on personalized vocabulary acquisition for L2 learners. (arXiv:2112.13808v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13808">
<div class="article-summary-box-inner">
<span><p>When learning a second language (L2), one of the most important but tedious
components that often demoralizes students with its ineffectiveness and
inefficiency is vocabulary acquisition, or more simply put, memorizing words.
In light of such, a personalized and educational vocabulary recommendation
system that traces a learner's vocabulary knowledge state would have an immense
learning impact as it could resolve both issues. Therefore, in this paper, we
propose and release data for a novel task called Pedagogical Word
Recommendation (PWR). The main goal of PWR is to predict whether a given
learner knows a given word based on other words the learner has already seen.
To elaborate, we collect this data via an Intelligent Tutoring System (ITS)
that is serviced to ~1M L2 learners who study for the standardized English
exam, TOEIC. As a feature of this ITS, students can directly indicate words
they do not know from the questions they solved to create wordbooks. Finally,
we report the evaluation results of a Neural Collaborative Filtering approach
along with an exploratory data analysis and discuss the impact and efficacy of
this dataset as a baseline for future studies on this task.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">BMPQ: Bit-Gradient Sensitivity Driven Mixed-Precision Quantization of DNNs from Scratch. (arXiv:2112.13843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13843">
<div class="article-summary-box-inner">
<span><p>Large DNNs with mixed-precision quantization can achieve ultra-high
compression while retaining high classification performance. However, because
of the challenges in finding an accurate metric that can guide the optimization
process, these methods either sacrifice significant performance compared to the
32-bit floating-point (FP-32) baseline or rely on a compute-expensive,
iterative training policy that requires the availability of a pre-trained
baseline. To address this issue, this paper presents BMPQ, a training method
that uses bit gradients to analyze layer sensitivities and yield
mixed-precision quantized models. BMPQ requires a single training iteration but
does not need a pre-trained baseline. It uses an integer linear program (ILP)
to dynamically adjust the precision of layers during training, subject to a
fixed hardware budget. To evaluate the efficacy of BMPQ, we conduct extensive
experiments with VGG16 and ResNet18 on CIFAR-10, CIFAR-100, and Tiny-ImageNet
datasets. Compared to the baseline FP-32 models, BMPQ can yield models that
have 15.4x fewer parameter bits with a negligible drop in accuracy. Compared to
the SOTA "during training", mixed-precision training scheme, our models are
2.1x, 2.2x, and 2.9x smaller, on CIFAR-10, CIFAR-100, and Tiny-ImageNet,
respectively, with an improved accuracy of up to 14.54%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Raw Produce Quality Detection with Shifted Window Self-Attention. (arXiv:2112.13845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13845">
<div class="article-summary-box-inner">
<span><p>Global food insecurity is expected to worsen in the coming decades with the
accelerated rate of climate change and the rapidly increasing population. In
this vein, it is important to remove inefficiencies at every level of food
production. The recent advances in deep learning can help reduce such
inefficiencies, yet their application has not yet become mainstream throughout
the industry, inducing economic costs at a massive scale. To this point, modern
techniques such as CNNs (Convolutional Neural Networks) have been applied to
RPQD (Raw Produce Quality Detection) tasks. On the other hand, Transformer's
successful debut in the vision among other modalities led us to expect a better
performance with these Transformer-based models in RPQD. In this work, we
exclusively investigate the recent state-of-the-art Swin (Shifted Windows)
Transformer which computes self-attention in both intra- and inter-window
fashion. We compare Swin Transformer against CNN models on four RPQD image
datasets, each containing different kinds of raw produce: fruits and
vegetables, fish, pork, and beef. We observe that Swin Transformer not only
achieves better or competitive performance but also is data- and
compute-efficient, making it ideal for actual deployment in real-world setting.
To the best of our knowledge, this is the first large-scale empirical study on
RPQD task, which we hope will gain more attention in future works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Algorithm for recognizing the contour of a honeycomb block. (arXiv:2112.13846v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13846">
<div class="article-summary-box-inner">
<span><p>The article discusses an algorithm for recognizing the contour of fragments
of a honeycomb block. The inapplicability of ready-made functions of the OpenCV
library is shown. Two proposed algorithms are considered. The direct scanning
algorithm finds the extreme white pixels in the binarized image, it works
adequately on convex shapes of products, but does not find a contour on concave
areas and in cavities of products. To solve this problem, a scanning algorithm
using a sliding matrix is proposed, which works correctly on products of any
shape.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using maps to predict economic activity. (arXiv:2112.13850v1 [econ.GN])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13850">
<div class="article-summary-box-inner">
<span><p>We introduce a novel machine learning approach to leverage historical and
contemporary maps to systematically predict economic statistics. Remote sensing
data have been used as reliable proxies for local economic activity. However,
they have only become available in recent years, thus limiting their
applicability for long-term analysis. Historical maps, on the other hand, date
back several decades. Our simple algorithm extracts meaningful features from
the maps based on their color compositions. The grid-level population
predictions by our approach outperform the conventional CNN-based predictions
using raw map images. It also predicts population better than other approaches
using night light satellite images or land cover classifications as the input
for predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Astronomical Image Colorization and upscaling with Generative Adversarial Networks. (arXiv:2112.13865v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13865">
<div class="article-summary-box-inner">
<span><p>Automatic colorization of images without human intervention has been a
subject of interest in the machine learning community for a brief period of
time. Assigning color to an image is a highly ill-posed problem because of its
innate nature of possessing very high degrees of freedom; given an image, there
is often no single color-combination that is correct. Besides colorization,
another problem in reconstruction of images is Single Image Super Resolution,
which aims at transforming low resolution images to a higher resolution. This
research aims to provide an automated approach for the problem by focusing on a
very specific domain of images, namely astronomical images, and process them
using Generative Adversarial Networks (GANs). We explore the usage of various
models in two different color spaces, RGB and L*a*b. We use transferred
learning owing to a small data set, using pre-trained ResNet-18 as a backbone,
i.e. encoder for the U-net and fine-tune it further. The model produces
visually appealing images which hallucinate high resolution, colorized data in
these results which does not exist in the original image. We present our
results by evaluating the GANs quantitatively using distance metrics such as L1
distance and L2 distance in each of the color spaces across all channels to
provide a comparative analysis. We use Frechet inception distance (FID) to
compare the distribution of the generated images with the distribution of the
real image to assess the model's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Fistful of Words: Learning Transferable Visual Models from Bag-of-Words Supervision. (arXiv:2112.13884v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13884">
<div class="article-summary-box-inner">
<span><p>Using natural language as a supervision for training visual recognition
models holds great promise. Recent works have shown that if such supervision is
used in the form of alignment between images and captions in large training
datasets, then the resulting aligned models perform well on zero-shot
classification as downstream tasks2. In this paper, we focus on teasing out
what parts of the language supervision are essential for training zero-shot
image classification models. Through extensive and careful experiments, we show
that: 1) A simple Bag-of-Words (BoW) caption could be used as a replacement for
most of the image captions in the dataset. Surprisingly, we observe that this
approach improves the zero-shot classification performance when combined with
word balancing. 2) Using a BoW pretrained model, we can obtain more training
data by generating pseudo-BoW captions on images that do not have a caption.
Models trained on images with real and pseudo-BoW captions achieve stronger
zero-shot performance. On ImageNet-1k zero-shot evaluation, our best model,
that uses only 3M image-caption pairs, performs on-par with a CLIP model
trained on 15M image-caption pairs (31.5% vs 31.3%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedShift: identifying shift data for medical dataset curation. (arXiv:2112.13885v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13885">
<div class="article-summary-box-inner">
<span><p>To curate a high-quality dataset, identifying data variance between the
internal and external sources is a fundamental and crucial step. However,
methods to detect shift or variance in data have not been significantly
researched. Challenges to this are the lack of effective approaches to learn
dense representation of a dataset and difficulties of sharing private data
across medical institutions. To overcome the problems, we propose a unified
pipeline called MedShift to detect the top-level shift samples and thus
facilitate the medical curation. Given an internal dataset A as the base
source, we first train anomaly detectors for each class of dataset A to learn
internal distributions in an unsupervised way. Second, without exchanging data
across sources, we run the trained anomaly detectors on an external dataset B
for each class. The data samples with high anomaly scores are identified as
shift data. To quantify the shiftness of the external dataset, we cluster B's
data into groups class-wise based on the obtained scores. We then train a
multi-class classifier on A and measure the shiftness with the classifier's
performance variance on B by gradually dropping the group with the largest
anomaly score for each class. Additionally, we adapt a dataset quality metric
to help inspect the distribution differences for multiple medical sources. We
verify the efficacy of MedShift with musculoskeletal radiographs (MURA) and
chest X-rays datasets from more than one external source. Experiments show our
proposed shift data detection pipeline can be beneficial for medical centers to
curate high-quality datasets more efficiently. An interface introduction video
to visualize our results is available at https://youtu.be/V3BF0P1sxQE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human View Synthesis using a Single Sparse RGB-D Input. (arXiv:2112.13889v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13889">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis for humans in motion is a challenging computer vision
problem that enables applications such as free-viewpoint video. Existing
methods typically use complex setups with multiple input views, 3D supervision,
or pre-trained models that do not generalize well to new identities. Aiming to
address these limitations, we present a novel view synthesis framework to
generate realistic renders from unseen views of any human captured from a
single-view sensor with sparse RGB-D, similar to a low-cost depth camera, and
without actor-specific models. We propose an architecture to learn dense
features in novel views obtained by sphere-based neural rendering, and create
complete renders using a global context inpainting model. Additionally, an
enhancer network leverages the overall fidelity, even in occluded areas from
the original view, producing crisp renders with fine details. We show our
method generates high-quality novel views of synthetic and real human actors
given a single sparse RGB-D input. It generalizes to unseen identities, new
poses and faithfully reconstructs facial expressions. Our approach outperforms
prior human view synthesis methods and is robust to different levels of input
sparsity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPViT: Enabling Faster Vision Transformers via Soft Token Pruning. (arXiv:2112.13890v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13890">
<div class="article-summary-box-inner">
<span><p>Recently, Vision Transformer (ViT) has continuously established new
milestones in the computer vision field, while the high computation and memory
cost makes its propagation in industrial production difficult. Pruning, a
traditional model compression paradigm for hardware efficiency, has been widely
applied in various DNN structures. Nevertheless, it stays ambiguous on how to
perform exclusive pruning on the ViT structure. Considering three key points:
the structural characteristics, the internal data pattern of ViTs, and the
related edge device deployment, we leverage the input token sparsity and
propose a computation-aware soft pruning framework, which can be set up on
vanilla Transformers of both flatten and CNN-type structures, such as
Pooling-based ViT (PiT). More concretely, we design a dynamic attention-based
multi-head token selector, which is a lightweight module for adaptive
instance-wise token selection. We further introduce a soft pruning technique,
which integrates the less informative tokens generated by the selector module
into a package token that will participate in subsequent calculations rather
than being completely discarded. Our framework is bound to the trade-off
between accuracy and computation constraints of specific edge devices through
our proposed computation-aware training strategy. Experimental results show
that our framework significantly reduces the computation cost of ViTs while
maintaining comparable performance on image classification. Moreover, our
framework can guarantee the identified model to meet resource specifications of
mobile devices and FPGA, and even achieve the real-time execution of DeiT-T on
mobile platforms. For example, our method reduces the latency of DeiT-T to 26
ms (26%$\sim $41% superior to existing works) on the mobile device with
0.25%$\sim $4% higher top-1 accuracy on ImageNet. Our code will be released
soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPU-accelerated Faster Mean Shift with euclidean distance metrics. (arXiv:2112.13891v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13891">
<div class="article-summary-box-inner">
<span><p>Handling clustering problems are important in data statistics, pattern
recognition and image processing. The mean-shift algorithm, a common
unsupervised algorithms, is widely used to solve clustering problems. However,
the mean-shift algorithm is restricted by its huge computational resource cost.
In previous research[10], we proposed a novel GPU-accelerated Faster Mean-shift
algorithm, which greatly speed up the cosine-embedding clustering problem. In
this study, we extend and improve the previous algorithm to handle Euclidean
distance metrics. Different from conventional GPU-based mean-shift algorithms,
our algorithm adopts novel Seed Selection &amp; Early Stopping approaches, which
greatly increase computing speed and reduce GPU memory consumption. In the
simulation testing, when processing a 200K points clustering problem, our
algorithm achieved around 3 times speedup compared to the state-of-the-art
GPU-based mean-shift algorithms with optimized GPU memory consumption.
Moreover, in this study, we implemented a plug-and-play model for faster
mean-shift algorithm, which can be easily deployed. (Plug-and-play model is
available: https://github.com/masqm/Faster-Mean-Shift-Euc)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Reference Quality Monitoring of Digital Images using Gradient Statistics and Feedforward Neural Networks. (arXiv:2112.13893v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13893">
<div class="article-summary-box-inner">
<span><p>Digital images contain a lot of redundancies, therefore, compressions are
applied to reduce the image size without the loss of reasonable image quality.
The same become more prominent in the case of videos that contains image
sequences and higher compression ratios are achieved in low throughput
networks. Assessment of the quality of images in such scenarios becomes of
particular interest. Subjective evaluation in most of the scenarios becomes
infeasible so objective evaluation is preferred. Among the three objective
quality measures, full-reference and reduced-reference methods require an
original image in some form to calculate the quality score which is not
feasible in scenarios such as broadcasting or IP video. Therefore, a
non-reference quality metric is proposed to assess the quality of digital
images which calculates luminance and multiscale gradient statistics along with
mean subtracted contrast normalized products as features to train a Feedforward
Neural Network with Scaled Conjugate Gradient. The trained network has provided
good regression and R2 measures and further testing on LIVE Image Quality
Assessment database release-2 has shown promising results. Pearson, Kendall,
and Spearman's correlation are calculated between predicted and actual quality
scores and their results are comparable to the state-of-the-art systems.
Moreover, the proposed metric is computationally faster than its counterparts
and can be used for the quality assessment of image sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?. (arXiv:2112.13906v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13906">
<div class="article-summary-box-inner">
<span><p>Contrastive Language--Image Pre-training (CLIP) has shown remarkable success
in learning with cross-modal supervision from extensive amounts of image--text
pairs collected online. Thus far, the effectiveness of CLIP has been
investigated primarily in general-domain multimodal problems. This work
evaluates the effectiveness of CLIP for the task of Medical Visual Question
Answering (MedVQA). To this end, we present PubMedCLIP, a fine-tuned version of
CLIP for the medical domain based on PubMed articles. Our experiments are
conducted on two MedVQA benchmark datasets and investigate two MedVQA methods,
MEVF (Mixture of Enhanced Visual Features) and QCR (Question answering via
Conditional Reasoning). For each of these, we assess the merits of visual
representation learning using PubMedCLIP, the original CLIP, and
state-of-the-art MAML (Model-Agnostic Meta-Learning) networks pre-trained only
on visual data. We open source the code for our MedVQA pipeline and
pre-training PubMedCLIP. CLIP and PubMedCLIP achieve improvements in comparison
to MAML's visual encoder. PubMedCLIP achieves the best results with gains in
the overall accuracy of up to 3%. Individual examples illustrate the strengths
of PubMedCLIP in comparison to the previously widely used MAML networks. Visual
representation learning with language supervision in PubMedCLIP leads to
noticeable improvements for MedVQA. Our experiments reveal distributional
differences in the two MedVQA benchmark datasets that have not been imparted in
previous work and cause different back-end visual encoders in PubMedCLIP to
exhibit different behavior on these datasets. Moreover, we witness fundamental
performance differences of VQA in general versus medical domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Persuasion in COVID-19 Social Media Content: A Multi-Modal Characterization. (arXiv:2112.13910v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13910">
<div class="article-summary-box-inner">
<span><p>Social media content routinely incorporates multi-modal design to covey
information and shape meanings, and sway interpretations toward desirable
implications, but the choices and outcomes of using both texts and visual
images have not been sufficiently studied. This work proposes a computational
approach to analyze the outcome of persuasive information in multi-modal
content, focusing on two aspects, popularity and reliability, in
COVID-19-related news articles shared on Twitter. The two aspects are
intertwined in the spread of misinformation: for example, an unreliable article
that aims to misinform has to attain some popularity. This work has several
contributions. First, we propose a multi-modal (image and text) approach to
effectively identify popularity and reliability of information sources
simultaneously. Second, we identify textual and visual elements that are
predictive to information popularity and reliability. Third, by modeling
cross-modal relations and similarity, we are able to uncover how unreliable
articles construct multi-modal meaning in a distorted, biased fashion. Our work
demonstrates how to use multi-modal analysis for understanding influential
content and has implications to social media literacy and engagement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Depth Estimation using Location Information. (arXiv:2112.13925v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13925">
<div class="article-summary-box-inner">
<span><p>The ability to accurately estimate depth information is crucial for many
autonomous applications to recognize the surrounded environment and predict the
depth of important objects. One of the most recently used techniques is
monocular depth estimation where the depth map is inferred from a single image.
This paper improves the self-supervised deep learning techniques to perform
accurate generalized monocular depth estimation. The main idea is to train the
deep model to take into account a sequence of the different frames, each frame
is geotagged with its location information. This makes the model able to
enhance depth estimation given area semantics. We demonstrate the effectiveness
of our model to improve depth estimation results. The model is trained in a
realistic environment and the results show improvements in the depth map after
adding the location data to the model training phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPIDER: Searching Personalized Neural Architecture for Federated Learning. (arXiv:2112.13939v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13939">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) is an efficient learning framework that assists
distributed machine learning when data cannot be shared with a centralized
server due to privacy and regulatory restrictions. Recent advancements in FL
use predefined architecture-based learning for all the clients. However, given
that clients' data are invisible to the server and data distributions are
non-identical across clients, a predefined architecture discovered in a
centralized setting may not be an optimal solution for all the clients in FL.
Motivated by this challenge, in this work, we introduce SPIDER, an algorithmic
framework that aims to Search Personalized neural architecture for federated
learning. SPIDER is designed based on two unique features: (1) alternately
optimizing one architecture-homogeneous global model (Supernet) in a generic FL
manner and one architecture-heterogeneous local model that is connected to the
global model by weight sharing-based regularization (2) achieving
architecture-heterogeneous local model by a novel neural architecture search
(NAS) method that can select optimal subnet progressively using operation-level
perturbation on the accuracy value as the criterion. Experimental results
demonstrate that SPIDER outperforms other state-of-the-art personalization
methods, and the searched personalized architectures are more inference
efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SurFit: Learning to Fit Surfaces Improves Few Shot Learning on Point Clouds. (arXiv:2112.13942v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13942">
<div class="article-summary-box-inner">
<span><p>We present SurFit, a simple approach for label efficient learning of 3D shape
segmentation networks. SurFit is based on a self-supervised task of decomposing
the surface of a 3D shape into geometric primitives. It can be readily applied
to existing network architectures for 3D shape segmentation and improves their
performance in the few-shot setting, as we demonstrate in the widely used
ShapeNet and PartNet benchmarks. SurFit outperforms the prior state-of-the-art
in this setting, suggesting that decomposability into primitives is a useful
prior for learning representations predictive of semantic parts. We present a
number of experiments varying the choice of geometric primitives and downstream
tasks to demonstrate the effectiveness of the method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source Feature Compression for Object Classification in Vision-Based Underwater Robotics. (arXiv:2112.13953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13953">
<div class="article-summary-box-inner">
<span><p>New efficient source feature compression solutions are proposed based on a
two-stage Walsh-Hadamard Transform (WHT) for Convolutional Neural Network
(CNN)-based object classification in underwater robotics. The object images are
firstly transformed by WHT following a two-stage process. The transform-domain
tensors have large values concentrated in the upper left corner of the matrices
in the RGB channels. By observing this property, the transform-domain matrix is
partitioned into inner and outer regions. Consequently, two novel partitioning
methods are proposed in this work: (i) fixing the size of inner and outer
regions; and (ii) adjusting the size of inner and outer regions adaptively per
image. The proposals are evaluated with an underwater object dataset captured
from the Raritan River in New Jersey, USA. It is demonstrated and verified that
the proposals reduce the training time effectively for learning-based
underwater object classification task and increase the accuracy compared with
the competing methods. The object classification is an essential part of a
vision-based underwater robot that can sense the environment and navigate
autonomously. Therefore, the proposed method is well-suited for efficient
computer vision-based tasks in underwater robotics applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Moment in the Sun: Solar Nowcasting from Multispectral Satellite Data using Self-Supervised Learning. (arXiv:2112.13974v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13974">
<div class="article-summary-box-inner">
<span><p>Solar energy is now the cheapest form of electricity in history.
Unfortunately, significantly increasing the grid's fraction of solar energy
remains challenging due to its variability, which makes balancing electricity's
supply and demand more difficult. While thermal generators' ramp rate -- the
maximum rate that they can change their output -- is finite, solar's ramp rate
is essentially infinite. Thus, accurate near-term solar forecasting, or
nowcasting, is important to provide advance warning to adjust thermal generator
output in response to solar variations to ensure a balanced supply and demand.
To address the problem, this paper develops a general model for solar
nowcasting from abundant and readily available multispectral satellite data
using self-supervised learning. Specifically, we develop deep auto-regressive
models using convolutional neural networks (CNN) and long short-term memory
networks (LSTM) that are globally trained across multiple locations to predict
raw future observations of the spatio-temporal data collected by the recently
launched GOES-R series of satellites. Our model estimates a location's future
solar irradiance based on satellite observations, which we feed to a regression
model trained on smaller site-specific solar data to provide near-term solar
photovoltaic (PV) forecasts that account for site-specific characteristics. We
evaluate our approach for different coverage areas and forecast horizons across
25 solar sites and show that our approach yields errors close to that of a
model using ground-truth observations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Fine-grained Face Forgery Clues via Progressive Enhancement Learning. (arXiv:2112.13977v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13977">
<div class="article-summary-box-inner">
<span><p>With the rapid development of facial forgery techniques, forgery detection
has attracted more and more attention due to security concerns. Existing
approaches attempt to use frequency information to mine subtle artifacts under
high-quality forged faces. However, the exploitation of frequency information
is coarse-grained, and more importantly, their vanilla learning process
struggles to extract fine-grained forgery traces. To address this issue, we
propose a progressive enhancement learning framework to exploit both the RGB
and fine-grained frequency clues. Specifically, we perform a fine-grained
decomposition of RGB images to completely decouple the real and fake traces in
the frequency space. Subsequently, we propose a progressive enhancement
learning framework based on a two-branch network, combined with
self-enhancement and mutual-enhancement modules. The self-enhancement module
captures the traces in different input spaces based on spatial noise
enhancement and channel attention. The Mutual-enhancement module concurrently
enhances RGB and frequency features by communicating in the shared spatial
dimension. The progressive enhancement process facilitates the learning of
discriminative features with fine-grained face forgery clues. Extensive
experiments on several datasets show that our method outperforms the
state-of-the-art face forgery detection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quaternion-based dynamic mode decomposition for background modeling in color videos. (arXiv:2112.13982v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13982">
<div class="article-summary-box-inner">
<span><p>Scene Background Initialization (SBI) is one of the challenging problems in
computer vision. Dynamic mode decomposition (DMD) is a recently proposed method
to robustly decompose a video sequence into the background model and the
corresponding foreground part. However, this method needs to convert the color
image into the grayscale image for processing, which leads to the neglect of
the coupling information between the three channels of the color image. In this
study, we propose a quaternion-based DMD (Q-DMD), which extends the DMD by
quaternion matrix analysis, so as to completely preserve the inherent color
structure of the color image and the color video. We exploit the standard
eigenvalues of the quaternion matrix to compute its spectral decomposition and
calculate the corresponding Q-DMD modes and eigenvalues. The results on the
publicly available benchmark datasets prove that our Q-DMD outperforms the
exact DMD method, and experiment results also demonstrate that the performance
of our approach is comparable to that of the state-of-the-art ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Siamese Network with Interactive Transformer for Video Object Segmentation. (arXiv:2112.13983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13983">
<div class="article-summary-box-inner">
<span><p>Semi-supervised video object segmentation (VOS) refers to segmenting the
target object in remaining frames given its annotation in the first frame,
which has been actively studied in recent years. The key challenge lies in
finding effective ways to exploit the spatio-temporal context of past frames to
help learn discriminative target representation of current frame. In this
paper, we propose a novel Siamese network with a specifically designed
interactive transformer, called SITVOS, to enable effective context propagation
from historical to current frames. Technically, we use the transformer encoder
and decoder to handle the past frames and current frame separately, i.e., the
encoder encodes robust spatio-temporal context of target object from the past
frames, while the decoder takes the feature embedding of current frame as the
query to retrieve the target from the encoder output. To further enhance the
target representation, a feature interaction module (FIM) is devised to promote
the information flow between the encoder and decoder. Moreover, we employ the
Siamese architecture to extract backbone features of both past and current
frames, which enables feature reuse and is more efficient than existing
methods. Experimental results on three challenging benchmarks validate the
superiority of SITVOS over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LatteGAN: Visually Guided Language Attention for Multi-Turn Text-Conditioned Image Manipulation. (arXiv:2112.13985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13985">
<div class="article-summary-box-inner">
<span><p>Text-guided image manipulation tasks have recently gained attention in the
vision-and-language community. While most of the prior studies focused on
single-turn manipulation, our goal in this paper is to address the more
challenging multi-turn image manipulation (MTIM) task. Previous models for this
task successfully generate images iteratively, given a sequence of instructions
and a previously generated image. However, this approach suffers from
under-generation and a lack of generated quality of the objects that are
described in the instructions, which consequently degrades the overall
performance. To overcome these problems, we present a novel architecture called
a Visually Guided Language Attention GAN (LatteGAN). Here, we address the
limitations of the previous approaches by introducing a Visually Guided
Language Attention (Latte) module, which extracts fine-grained text
representations for the generator, and a Text-Conditioned U-Net discriminator
architecture, which discriminates both the global and local representations of
fake or real images. Extensive experiments on two distinct MTIM datasets,
CoDraw and i-CLEVR, demonstrate the state-of-the-art performance of the
proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep-CNN based Robotic Multi-Class Under-Canopy Weed Control in Precision Farming. (arXiv:2112.13986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13986">
<div class="article-summary-box-inner">
<span><p>Smart weeding systems to perform plant-specific operations can contribute to
the sustainability of agriculture and the environment. Despite monumental
advances in autonomous robotic technologies for precision weed management in
recent years, work on under-canopy weeding in fields is yet to be realized. A
prerequisite of such systems is reliable detection and classification of weeds
to avoid mistakenly spraying and, thus, damaging the surrounding plants.
Real-time multi-class weed identification enables species-specific treatment of
weeds and significantly reduces the amount of herbicide use. Here, our first
contribution is the first adequately large realistic image dataset
\textit{AIWeeds} (one/multiple kinds of weeds in one image), a library of about
10,000 annotated images of flax, and the 14 most common weeds in fields and
gardens taken from 20 different locations in North Dakota, California, and
Central China. Second, we provide a full pipeline from model training with
maximum efficiency to deploying the TensorRT-optimized model onto a single
board computer. Based on \textit{AIWeeds} and the pipeline, we present a
baseline for classification performance using five benchmark CNN models. Among
them, MobileNetV2, with both the shortest inference time and lowest memory
consumption, is the qualified candidate for real-time applications. Finally, we
deploy MobileNetV2 onto our own compact autonomous robot \textit{SAMBot} for
real-time weed detection. The 90\% test accuracy realized in previously unseen
scenes in flax fields (with a row spacing of 0.2-0.3 m), with crops and weeds,
distortion, blur, and shadows, is a milestone towards precision weed control in
the real world. We have publicly released the dataset and code to generate the
results at
\url{https://github.com/StructuresComp/Multi-class-Weed-Classification}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Associative Adversarial Learning Based on Selective Attack. (arXiv:2112.13989v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13989">
<div class="article-summary-box-inner">
<span><p>A human's attention can intuitively adapt to corrupted areas of an image by
recalling a similar uncorrupted image they have previously seen. This
observation motivates us to improve the attention of adversarial images by
considering their clean counterparts. To accomplish this, we introduce
Associative Adversarial Learning (AAL) into adversarial learning to guide a
selective attack. We formulate the intrinsic relationship between attention and
attack (perturbation) as a coupling optimization problem to improve their
interaction. This leads to an attention backtracking algorithm that can
effectively enhance the attention's adversarial robustness. Our method is
generic and can be used to address a variety of tasks by simply choosing
different kernels for the associative attention that select other regions for a
specific attack. Experimental results show that the selective attack improves
the model's performance. We show that our method improves the recognition
accuracy of adversarial training on ImageNet by 8.32% compared with the
baseline. It also increases object detection mAP on PascalVOC by 2.02% and
recognition accuracy of few-shot learning on miniImageNet by 1.63%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention. (arXiv:2112.14000v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14000">
<div class="article-summary-box-inner">
<span><p>Recently, Transformers have shown promising performance in various vision
tasks. To reduce the quadratic computation complexity caused by the global
self-attention, various methods constrain the range of attention within a local
region to improve its efficiency. Consequently, their receptive fields in a
single attention layer are not large enough, resulting in insufficient context
modeling. To address this issue, we propose a Pale-Shaped self-Attention
(PS-Attention), which performs self-attention within a pale-shaped region.
Compared to the global self-attention, PS-Attention can reduce the computation
and memory costs significantly. Meanwhile, it can capture richer contextual
information under the similar computation complexity with previous local
self-attention mechanisms. Based on the PS-Attention, we develop a general
Vision Transformer backbone with a hierarchical architecture, named Pale
Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the
model size of 22M, 48M, and 85M respectively for 224 ImageNet-1K
classification, outperforming the previous Vision Transformer backbones. For
downstream tasks, our Pale Transformer backbone performs better than the recent
state-of-the-art CSWin Transformer by a large margin on ADE20K semantic
segmentation and COCO object detection &amp; instance segmentation. The code will
be released on https://github.com/BR-IDL/PaddleViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Band Wi-Fi Sensing with Matched Feature Granularity. (arXiv:2112.14006v1 [cs.NI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14006">
<div class="article-summary-box-inner">
<span><p>Complementary to the fine-grained channel state information (CSI) from the
physical layer and coarse-grained received signal strength indicator (RSSI)
measurements, the mid-grained spatial beam attributes (e.g., beam SNR) that are
available at millimeter-wave (mmWave) bands during the mandatory beam training
phase can be repurposed for Wi-Fi sensing applications. In this paper, we
propose a multi-band Wi-Fi fusion method for Wi-Fi sensing that hierarchically
fuses the features from both the fine-grained CSI at sub-6 GHz and the
mid-grained beam SNR at 60 GHz in a granularity matching framework. The
granularity matching is realized by pairing two feature maps from the CSI and
beam SNR at different granularity levels and linearly combining all paired
feature maps into a fused feature map with learnable weights.
</p>
<p>To further address the issue of limited labeled training data, we propose an
autoencoder-based multi-band Wi-Fi fusion network that can be pre-trained in an
unsupervised fashion. Once the autoencoder-based fusion network is pre-trained,
we detach the decoders and append multi-task sensing heads to the fused feature
map by fine-tuning the fusion block and re-training the multi-task heads from
the scratch. The multi-band Wi-Fi fusion framework is thoroughly validated by
in-house experimental Wi-Fi sensing datasets spanning three tasks: 1) pose
recognition; 2) occupancy sensing; and 3) indoor localization. Comparison to
four baseline methods (i.e., CSI-only, beam SNR-only, input fusion, and feature
fusion) demonstrates the granularity matching improves the multi-task sensing
performance. Quantitative performance is evaluated as a function of the number
of labeled training data, latent space dimension, and fine-tuning learning
rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GuidedMix-Net: Semi-supervised Semantic Segmentation by Using Labeled Images as Reference. (arXiv:2112.14015v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14015">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning is a challenging problem which aims to construct a
model by learning from limited labeled examples. Numerous methods for this task
focus on utilizing the predictions of unlabeled instances consistency alone to
regularize networks. However, treating labeled and unlabeled data separately
often leads to the discarding of mass prior knowledge learned from the labeled
examples. %, and failure to mine the feature interaction between the labeled
and unlabeled image pairs. In this paper, we propose a novel method for
semi-supervised semantic segmentation named GuidedMix-Net, by leveraging
labeled information to guide the learning of unlabeled instances. Specifically,
GuidedMix-Net employs three operations: 1) interpolation of similar
labeled-unlabeled image pairs; 2) transfer of mutual information; 3)
generalization of pseudo masks. It enables segmentation models can learning the
higher-quality pseudo masks of unlabeled data by transfer the knowledge from
labeled samples to unlabeled data. Along with supervised learning for labeled
data, the prediction of unlabeled data is jointly learned with the generated
pseudo masks from the mixed data. Extensive experiments on PASCAL VOC 2012, and
Cityscapes demonstrate the effectiveness of our GuidedMix-Net, which achieves
competitive segmentation accuracy and significantly improves the mIoU by +7$\%$
compared to previous approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking. (arXiv:2112.14016v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14016">
<div class="article-summary-box-inner">
<span><p>Tracking visual objects from a single initial exemplar in the testing phase
has been broadly cast as a one-/few-shot problem, i.e., one-shot learning for
initial adaptation and few-shot learning for online adaptation. The recent
few-shot online adaptation methods incorporate the prior knowledge from large
amounts of annotated training data via complex meta-learning optimization in
the offline phase. This helps the online deep trackers to achieve fast
adaptation and reduce overfitting risk in tracking. In this paper, we propose a
simple yet effective recursive least-squares estimator-aided online learning
approach for few-shot online adaptation without requiring offline training. It
allows an in-built memory retention mechanism for the model to remember the
knowledge about the object seen before, and thus the seen data can be safely
removed from training. This also bears certain similarities to the emerging
continual learning field in preventing catastrophic forgetting. This mechanism
enables us to unveil the power of modern online deep trackers without incurring
too much extra computational cost. We evaluate our approach based on two
networks in the online learning families for tracking, i.e., multi-layer
perceptrons in RT-MDNet and convolutional neural networks in DiMP. The
consistent improvements on several challenging tracking benchmarks demonstrate
its effectiveness and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Salient Object Detection with Effective Confidence Estimation. (arXiv:2112.14019v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14019">
<div class="article-summary-box-inner">
<span><p>The success of existing salient object detection models relies on a large
pixel-wise labeled training dataset. How-ever, collecting such a dataset is not
only time-consuming but also very expensive. To reduce the labeling burden, we
study semi-supervised salient object detection, and formulate it as an
unlabeled dataset pixel-level confidence estimation problem by identifying
pixels with less confident predictions. Specifically, we introduce a new latent
variable model with an energy-based prior for effective latent space
exploration, leading to more reliable confidence maps. With the proposed
strategy, the unlabelled images can effectively participate in model training.
Experimental results show that the proposed solution, using only 1/16 of the
annotations from the original training dataset, achieves competitive
performance compared with state-of-the-art fully supervised models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilayer Graph Contrastive Clustering Network. (arXiv:2112.14021v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14021">
<div class="article-summary-box-inner">
<span><p>Multilayer graph has garnered plenty of research attention in many areas due
to their high utility in modeling interdependent systems. However, clustering
of multilayer graph, which aims at dividing the graph nodes into categories or
communities, is still at a nascent stage. Existing methods are often limited to
exploiting the multiview attributes or multiple networks and ignoring more
complex and richer network frameworks. To this end, we propose a generic and
effective autoencoder framework for multilayer graph clustering named
Multilayer Graph Contrastive Clustering Network (MGCCN). MGCCN consists of
three modules: (1)Attention mechanism is applied to better capture the
relevance between nodes and neighbors for better node embeddings. (2)To better
explore the consistent information in different networks, a contrastive fusion
strategy is introduced. (3)MGCCN employs a self-supervised component that
iteratively strengthens the node embedding and clustering. Extensive
experiments on different types of real-world graph data indicate that our
proposed method outperforms state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Low Light Enhancement with RAW Images. (arXiv:2112.14022v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14022">
<div class="article-summary-box-inner">
<span><p>In this paper, we make the first benchmark effort to elaborate on the
superiority of using RAW images in the low light enhancement and develop a
novel alternative route to utilize RAW images in a more flexible and practical
way. Inspired by a full consideration on the typical image processing pipeline,
we are inspired to develop a new evaluation framework, Factorized Enhancement
Model (FEM), which decomposes the properties of RAW images into measurable
factors and provides a tool for exploring how properties of RAW images affect
the enhancement performance empirically. The empirical benchmark results show
that the Linearity of data and Exposure Time recorded in meta-data play the
most critical role, which brings distinct performance gains in various measures
over the approaches taking the sRGB images as input. With the insights obtained
from the benchmark results in mind, a RAW-guiding Exposure Enhancement Network
(REENet) is developed, which makes trade-offs between the advantages and
inaccessibility of RAW images in real applications in a way of using RAW images
only in the training phase. REENet projects sRGB images into linear RAW domains
to apply constraints with corresponding RAW images to reduce the difficulty of
modeling training. After that, in the testing phase, our REENet does not rely
on RAW images. Experimental results demonstrate not only the superiority of
REENet to state-of-the-art sRGB-based methods and but also the effectiveness of
the RAW guidance and all components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Devil is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection. (arXiv:2112.14023v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14023">
<div class="article-summary-box-inner">
<span><p>Low-cost monocular 3D object detection plays a fundamental role in autonomous
driving, whereas its accuracy is still far from satisfactory. In this paper, we
dig into the 3D object detection task and reformulate it as the sub-tasks of
object localization and appearance perception, which benefits to a deep
excavation of reciprocal information underlying the entire task. We introduce a
Dynamic Feature Reflecting Network, named DFR-Net, which contains two novel
standalone modules: (i) the Appearance-Localization Feature Reflecting module
(ALFR) that first separates taskspecific features and then self-mutually
reflects the reciprocal features; (ii) the Dynamic Intra-Trading module (DIT)
that adaptively realigns the training processes of various sub-tasks via a
self-learning manner. Extensive experiments on the challenging KITTI dataset
demonstrate the effectiveness and generalization of DFR-Net. We rank 1st among
all the monocular 3D object detectors in the KITTI test set (till March 16th,
2021). The proposed method is also easy to be plug-and-play in many
cutting-edge 3D detection frameworks at negligible cost to boost performance.
The code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving into Probabilistic Uncertainty for Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2112.14025v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14025">
<div class="article-summary-box-inner">
<span><p>Clustering-based unsupervised domain adaptive (UDA) person re-identification
(ReID) reduces exhaustive annotations. However, owing to unsatisfactory feature
embedding and imperfect clustering, pseudo labels for target domain data
inherently contain an unknown proportion of wrong ones, which would mislead
feature learning. In this paper, we propose an approach named probabilistic
uncertainty guided progressive label refinery (P$^2$LR) for domain adaptive
person re-identification. First, we propose to model the labeling uncertainty
with the probabilistic distance along with ideal single-peak distributions. A
quantitative criterion is established to measure the uncertainty of pseudo
labels and facilitate the network training. Second, we explore a progressive
strategy for refining pseudo labels. With the uncertainty-guided alternative
optimization, we balance between the exploration of target domain data and the
negative effects of noisy labeling. On top of a strong baseline, we obtain
significant improvements and achieve the state-of-the-art performance on four
UDA ReID benchmarks. Specifically, our method outperforms the baseline by 6.5%
mAP on the Duke2Market task, while surpassing the state-of-the-art method by
2.5% mAP on the Market2MSMT task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SECP-Net: SE-Connection Pyramid Network of Organ At Risk Segmentation for Nasopharyngeal Carcinoma. (arXiv:2112.14026v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14026">
<div class="article-summary-box-inner">
<span><p>Nasopharyngeal carcinoma (NPC) is a kind of malignant tumor. Accurate and
automatic segmentation of organs at risk (OAR) of computed tomography (CT)
images is clinically significant. In recent years, deep learning models
represented by U-Net have been widely applied in medical image segmentation
tasks, which can help doctors with reduction of workload and get accurate
results more quickly. In OAR segmentation of NPC, the sizes of OAR are
variable, especially, some of them are small. Traditional deep neural networks
underperform during segmentation due to the lack use of global and multi-size
information. This paper proposes a new SE-Connection Pyramid Network
(SECP-Net). SECP-Net extracts global and multi-size information flow with se
connection (SEC) modules and a pyramid structure of network for improving the
segmentation performance, especially that of small organs. SECP-Net also
designs an auto-context cascaded network to further improve the segmentation
performance. Comparative experiments are conducted between SECP-Net and other
recently methods on a dataset with CT images of head and neck. Five-fold cross
validation is used to evaluate the performance based on two metrics, i.e., Dice
and Jaccard similarity. Experimental results show that SECP-Net can achieve
SOTA performance in this challenging task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DetarNet: Decoupling Translation and Rotation by Siamese Network for Point Cloud Registration. (arXiv:2112.14059v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14059">
<div class="article-summary-box-inner">
<span><p>Point cloud registration is a fundamental step for many tasks. In this paper,
we propose a neural network named DetarNet to decouple the translation $t$ and
rotation $R$, so as to overcome the performance degradation due to their mutual
interference in point cloud registration. First, a Siamese Network based
Progressive and Coherent Feature Drift (PCFD) module is proposed to align the
source and target points in high-dimensional feature space, and accurately
recover translation from the alignment process. Then we propose a Consensus
Encoding Unit (CEU) to construct more distinguishable features for a set of
putative correspondences. After that, a Spatial and Channel Attention (SCA)
block is adopted to build a classification network for finding good
correspondences. Finally, the rotation is obtained by Singular Value
Decomposition (SVD). In this way, the proposed network decouples the estimation
of translation and rotation, resulting in better performance for both of them.
Experimental results demonstrate that the proposed DetarNet improves
registration performance on both indoor and outdoor scenes. Our code will be
available in \url{https://github.com/ZhiChen902/DetarNet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Shifts in GAN Output-Distributions. (arXiv:2112.14061v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14061">
<div class="article-summary-box-inner">
<span><p>A fundamental and still largely unsolved question in the context of
Generative Adversarial Networks is whether they are truly able to capture the
real data distribution and, consequently, to sample from it. In particular, the
multidimensional nature of image distributions leads to a complex evaluation of
the diversity of GAN distributions. Existing approaches provide only a partial
understanding of this issue, leaving the question unanswered. In this work, we
introduce a loop-training scheme for the systematic investigation of observable
shifts between the distributions of real training data and GAN generated data.
Additionally, we introduce several bounded measures for distribution shifts,
which are both easy to compute and to interpret. Overall, the combination of
these methods allows an explorative investigation of innate limitations of
current GAN algorithms. Our experiments on different data-sets and multiple
state-of-the-art GAN architectures show large shifts between input and output
distributions, showing that existing theoretical guarantees towards the
convergence of output distributions appear not to be holding in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embodied Learning for Lifelong Visual Perception. (arXiv:2112.14084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14084">
<div class="article-summary-box-inner">
<span><p>We study lifelong visual perception in an embodied setup, where we develop
new models and compare various agents that navigate in buildings and
occasionally request annotations which, in turn, are used to refine their
visual perception capabilities. The purpose of the agents is to recognize
objects and other semantic classes in the whole building at the end of a
process that combines exploration and active visual learning. As we study this
task in a lifelong learning context, the agents should use knowledge gained in
earlier visited environments in order to guide their exploration and active
learning strategy in successively visited buildings. We use the semantic
segmentation performance as a proxy for general visual perception and study
this novel task for several exploration and annotation methods, ranging from
frontier exploration baselines which use heuristic active learning, to a fully
learnable approach. For the latter, we introduce a deep reinforcement learning
(RL) based agent which jointly learns both navigation and active learning. A
point goal navigation formulation, coupled with a global planner which supplies
goals, is integrated into the RL model in order to provide further incentives
for systematic exploration of novel scenes. By performing extensive experiments
on the Matterport3D dataset, we show how the proposed agents can utilize
knowledge from previously explored scenes when exploring new ones, e.g. through
less granular exploration and less frequent requests for annotations. The
results also suggest that a learning-based agent is able to use its prior
visual knowledge more effectively than heuristic alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APRIL: Finding the Achilles' Heel on Privacy for Vision Transformers. (arXiv:2112.14087v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14087">
<div class="article-summary-box-inner">
<span><p>Federated learning frameworks typically require collaborators to share their
local gradient updates of a common model instead of sharing training data to
preserve privacy. However, prior works on Gradient Leakage Attacks showed that
private training data can be revealed from gradients. So far almost all
relevant works base their attacks on fully-connected or convolutional neural
networks. Given the recent overwhelmingly rising trend of adapting Transformers
to solve multifarious vision tasks, it is highly valuable to investigate the
privacy risk of vision transformers. In this paper, we analyse the gradient
leakage risk of self-attention based mechanism in both theoretical and
practical manners. Particularly, we propose APRIL - Attention PRIvacy Leakage,
which poses a strong threat to self-attention inspired models such as ViT.
Showing how vision Transformers are at the risk of privacy leakage via
gradients, we urge the significance of designing privacy-safer Transformer
models and defending schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synchronized Audio-Visual Frames with Fractional Positional Encoding for Transformers in Video-to-Text Translation. (arXiv:2112.14088v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14088">
<div class="article-summary-box-inner">
<span><p>Video-to-Text (VTT) is the task of automatically generating descriptions for
short audio-visual video clips, which can support visually impaired people to
understand scenes of a YouTube video for instance. Transformer architectures
have shown great performance in both machine translation and image captioning,
lacking a straightforward and reproducible application for VTT. However, there
is no comprehensive study on different strategies and advice for video
description generation including exploiting the accompanying audio with fully
self-attentive networks. Thus, we explore promising approaches from image
captioning and video processing and apply them to VTT by developing a
straightforward Transformer architecture. Additionally, we present a novel way
of synchronizing audio and video features in Transformers which we call
Fractional Positional Encoding (FPE). We run multiple experiments on the VATEX
dataset to determine a configuration applicable to unseen datasets that helps
describe short video clips in natural language and improved the CIDEr and
BLEU-4 scores by 37.13 and 12.83 points compared to a vanilla Transformer
network and achieve state-of-the-art results on the MSR-VTT and MSVD datasets.
Also, FPE helps increase the CIDEr score by a relative factor of 8.6%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extended Self-Critical Pipeline for Transforming Videos to Text (TRECVID-VTT Task 2021) -- Team: MMCUniAugsburg. (arXiv:2112.14100v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14100">
<div class="article-summary-box-inner">
<span><p>The Multimedia and Computer Vision Lab of the University of Augsburg
participated in the VTT task only. We use the VATEX and TRECVID-VTT datasets
for training our VTT models. We base our model on the Transformer approach for
both of our submitted runs. For our second model, we adapt the X-Linear
Attention Networks for Image Captioning which does not yield the desired bump
in scores. For both models, we train on the complete VATEX dataset and 90% of
the TRECVID-VTT dataset for pretraining while using the remaining 10% for
validation. We finetune both models with self-critical sequence training, which
boosts the validation performance significantly. Overall, we find that training
a Video-to-Text system on traditional Image Captioning pipelines delivers very
poor performance. When switching to a Transformer-based architecture our
results greatly improve and the generated captions match better with the
corresponding video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skin feature point tracking using deep feature encodings. (arXiv:2112.14159v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14159">
<div class="article-summary-box-inner">
<span><p>Facial feature tracking is a key component of imaging ballistocardiography
(BCG) where accurate quantification of the displacement of facial keypoints is
needed for good heart rate estimation. Skin feature tracking enables
video-based quantification of motor degradation in Parkinson's disease.
Traditional computer vision algorithms include Scale Invariant Feature
Transform (SIFT), Speeded-Up Robust Features (SURF), and Lucas-Kanade method
(LK). These have long represented the state-of-the-art in efficiency and
accuracy but fail when common deformations, like affine local transformations
or illumination changes, are present.
</p>
<p>Over the past five years, deep convolutional neural networks have
outperformed traditional methods for most computer vision tasks. We propose a
pipeline for feature tracking, that applies a convolutional stacked autoencoder
to identify the most similar crop in an image to a reference crop containing
the feature of interest. The autoencoder learns to represent image crops into
deep feature encodings specific to the object category it is trained on.
</p>
<p>We train the autoencoder on facial images and validate its ability to track
skin features in general using manually labeled face and hand videos. The
tracking errors of distinctive skin features (moles) are so small that we
cannot exclude that they stem from the manual labelling based on a
$\chi^2$-test. With a mean error of 0.6-4.2 pixels, our method outperformed the
other methods in all but one scenario. More importantly, our method was the
only one to not diverge.
</p>
<p>We conclude that our method creates better feature descriptors for feature
tracking, feature matching, and image registration than the traditional
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained Gradient Descent: A Powerful and Principled Evasion Attack Against Neural Networks. (arXiv:2112.14232v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14232">
<div class="article-summary-box-inner">
<span><p>Minimal adversarial perturbations added to inputs have been shown to be
effective at fooling deep neural networks. In this paper, we introduce several
innovations that make white-box targeted attacks follow the intuition of the
attacker's goal: to trick the model to assign a higher probability to the
target class than to any other, while staying within a specified distance from
the original input. First, we propose a new loss function that explicitly
captures the goal of targeted attacks, in particular, by using the logits of
all classes instead of just a subset, as is common. We show that Auto-PGD with
this loss function finds more adversarial examples than it does with other
commonly used loss functions. Second, we propose a new attack method that uses
a further developed version of our loss function capturing both the
misclassification objective and the $L_{\infty}$ distance limit $\epsilon$.
This new attack method is relatively 1.5--4.2% more successful on the CIFAR10
dataset and relatively 8.2--14.9% more successful on the ImageNet dataset, than
the next best state-of-the-art attack. We confirm using statistical tests that
our attack outperforms state-of-the-art attacks on different datasets and
values of $\epsilon$ and against different defenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition. (arXiv:2112.14238v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14238">
<div class="article-summary-box-inner">
<span><p>Recent works have shown that the computational efficiency of video
recognition can be significantly improved by reducing the spatial redundancy.
As a representative work, the adaptive focus method (AdaFocus) has achieved a
favorable trade-off between accuracy and inference speed by dynamically
identifying and attending to the informative regions in each video frame.
However, AdaFocus requires a complicated three-stage training pipeline
(involving reinforcement learning), leading to slow convergence and is
unfriendly to practitioners. This work reformulates the training of AdaFocus as
a simple one-stage algorithm by introducing a differentiable
interpolation-based patch selection operation, enabling efficient end-to-end
optimization. We further present an improved training scheme to address the
issues introduced by the one-stage formulation, including the lack of
supervision, input diversity and training stability. Moreover, a
conditional-exit technique is proposed to perform temporal adaptive computation
on top of AdaFocus without additional training. Extensive experiments on six
benchmark datasets (i.e., ActivityNet, FCVID, Mini-Kinetics,
Something-Something V1&amp;V2, and Jester) demonstrate that our model significantly
outperforms the original AdaFocus and other competitive baselines, while being
considerably more simple and efficient to train. Code is available at
https://github.com/LeapLabTHU/AdaFocusV2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAGPerson: A Target-Aware Generation Pipeline for Person Re-identification. (arXiv:2112.14239v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14239">
<div class="article-summary-box-inner">
<span><p>Nowadays, real data in person re-identification (ReID) task is facing privacy
issues, e.g., the banned dataset DukeMTMC-ReID. Thus it becomes much harder to
collect real data for ReID task. Meanwhile, the labor cost of labeling ReID
data is still very high and further hinders the development of the ReID
research. Therefore, many methods turn to generate synthetic images for ReID
algorithms as alternatives instead of real images. However, there is an
inevitable domain gap between synthetic and real images. In previous methods,
the generation process is based on virtual scenes, and their synthetic training
data can not be changed according to different target real scenes
automatically. To handle this problem, we propose a novel Target-Aware
Generation pipeline to produce synthetic person images, called TAGPerson.
Specifically, it involves a parameterized rendering method, where the
parameters are controllable and can be adjusted according to target scenes. In
TAGPerson, we extract information from target scenes and use them to control
our parameterized rendering process to generate target-aware synthetic images,
which would hold a smaller gap to the real images in the target domain. In our
experiments, our target-aware synthetic images can achieve a much higher
performance than the generalized synthetic images on MSMT17, i.e. 47.5% vs.
40.9% for rank-1 accuracy. We will release this toolkit\footnote{\noindent Code
is available at
\href{https://github.com/tagperson/tagperson-blender}{https://github.com/tagperson/tagperson-blender}}
for the ReID community to generate synthetic images at any desired taste.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal perception for dexterous manipulation. (arXiv:2112.14298v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14298">
<div class="article-summary-box-inner">
<span><p>Humans usually perceive the world in a multimodal way that vision, touch,
sound are utilised to understand surroundings from various dimensions. These
senses are combined together to achieve a synergistic effect where the learning
is more effectively than using each sense separately. For robotics, vision and
touch are two key senses for the dexterous manipulation. Vision usually gives
us apparent features like shape, color, and the touch provides local
information such as friction, texture, etc. Due to the complementary properties
between visual and tactile senses, it is desirable for us to combine vision and
touch for a synergistic perception and manipulation. Many researches have been
investigated about multimodal perception such as cross-modal learning, 3D
reconstruction, multimodal translation with vision and touch. Specifically, we
propose a cross-modal sensory data generation framework for the translation
between vision and touch, which is able to generate realistic pseudo data. By
using this cross-modal translation method, it is desirable for us to make up
inaccessible data, helping us to learn the object's properties from different
views. Recently, the attention mechanism becomes a popular method either in
visual perception or in tactile perception. We propose a spatio-temporal
attention model for tactile texture recognition, which takes both spatial
features and time dimension into consideration. Our proposed method not only
pays attention to the salient features in each spatial feature, but also models
the temporal correlation in the through the time. The obvious improvement
proves the efficiency of our selective attention mechanism. The spatio-temporal
attention method has potential in many applications such as grasping,
recognition, and multimodal perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepAdversaries: Examining the Robustness of Deep Learning Models for Galaxy Morphology Classification. (arXiv:2112.14299v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14299">
<div class="article-summary-box-inner">
<span><p>Data processing and analysis pipelines in cosmological survey experiments
introduce data perturbations that can significantly degrade the performance of
deep learning-based models. Given the increased adoption of supervised deep
learning methods for processing and analysis of cosmological survey data, the
assessment of data perturbation effects and the development of methods that
increase model robustness are increasingly important. In the context of
morphological classification of galaxies, we study the effects of perturbations
in imaging data. In particular, we examine the consequences of using neural
networks when training on baseline data and testing on perturbed data. We
consider perturbations associated with two primary sources: 1) increased
observational noise as represented by higher levels of Poisson noise and 2)
data processing noise incurred by steps such as image compression or telescope
errors as represented by one-pixel adversarial attacks. We also test the
efficacy of domain adaptation techniques in mitigating the perturbation-driven
errors. We use classification accuracy, latent space visualizations, and latent
space distance to assess model robustness. Without domain adaptation, we find
that processing pixel-level errors easily flip the classification into an
incorrect class and that higher observational noise makes the model trained on
low-noise data unable to classify galaxy morphologies. On the other hand, we
show that training with domain adaptation improves model robustness and
mitigates the effects of these perturbations, improving the classification
accuracy by 23% on data with higher observational noise. Domain adaptation also
increases by a factor of ~2.3 the latent space distance between the baseline
and the incorrectly classified one-pixel perturbed image, making the model more
robust to inadvertent perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FRIDA -- Generative Feature Replay for Incremental Domain Adaptation. (arXiv:2112.14316v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14316">
<div class="article-summary-box-inner">
<span><p>We tackle the novel problem of incremental unsupervised domain adaptation
(IDA) in this paper. We assume that a labeled source domain and different
unlabeled target domains are incrementally observed with the constraint that
data corresponding to the current domain is only available at a time. The goal
is to preserve the accuracies for all the past domains while generalizing well
for the current domain. The IDA setup suffers due to the abrupt differences
among the domains and the unavailability of past data including the source
domain. Inspired by the notion of generative feature replay, we propose a novel
framework called Feature Replay based Incremental Domain Adaptation (FRIDA)
which leverages a new incremental generative adversarial network (GAN) called
domain-generic auxiliary classification GAN (DGAC-GAN) for producing
domain-specific feature representations seamlessly. For domain alignment, we
propose a simple extension of the popular domain adversarial neural network
(DANN) called DANN-IB which encourages discriminative domain-invariant and
task-relevant feature learning. Experimental results on Office-Home,
Office-CalTech, and DomainNet datasets confirm that FRIDA maintains superior
stability-plasticity trade-off than the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brain Tumor Classification by Cascaded Multiscale Multitask Learning Framework Based on Feature Aggregation. (arXiv:2112.14320v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14320">
<div class="article-summary-box-inner">
<span><p>Brain tumor analysis in MRI images is a significant and challenging issue
because misdiagnosis can lead to death. Diagnosis and evaluation of brain
tumors in the early stages increase the probability of successful treatment.
However, the complexity and variety of tumors, shapes, and locations make their
segmentation and classification complex. In this regard, numerous researchers
have proposed brain tumor segmentation and classification methods. This paper
presents an approach that simultaneously segments and classifies brain tumors
in MRI images using a framework that contains MRI image enhancement and tumor
region detection. Eventually, a network based on a multitask learning approach
is proposed. Subjective and objective results indicate that the segmentation
and classification results based on evaluation metrics are better or comparable
to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Head Deep Metric Learning Using Global and Local Representations. (arXiv:2112.14327v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14327">
<div class="article-summary-box-inner">
<span><p>Deep Metric Learning (DML) models often require strong local and global
representations, however, effective integration of local and global features in
DML model training is a challenge. DML models are often trained with specific
loss functions, including pairwise-based and proxy-based losses. The
pairwise-based loss functions leverage rich semantic relations among data
points, however, they often suffer from slow convergence during DML model
training. On the other hand, the proxy-based loss functions often lead to
significant speedups in convergence during training, while the rich relations
among data points are often not fully explored by the proxy-based losses. In
this paper, we propose a novel DML approach to address these challenges. The
proposed DML approach makes use of a hybrid loss by integrating the
pairwise-based and the proxy-based loss functions to leverage rich data-to-data
relations as well as fast convergence. Furthermore, the proposed DML approach
utilizes both global and local features to obtain rich representations in DML
model training. Finally, we also use the second-order attention for feature
enhancement to improve accurate and efficient retrieval. In our experiments, we
extensively evaluated the proposed DML approach on four public benchmarks, and
the experimental results demonstrate that the proposed method achieved
state-of-the-art performance on all benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">360{\deg} Optical Flow using Tangent Images. (arXiv:2112.14331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14331">
<div class="article-summary-box-inner">
<span><p>Omnidirectional 360{\deg} images have found many promising and exciting
applications in computer vision, robotics and other fields, thanks to their
increasing affordability, portability and their 360{\deg} field of view. The
most common format for storing, processing and visualising 360{\deg} images is
equirectangular projection (ERP). However, the distortion introduced by the
nonlinear mapping from 360{\deg} image to ERP image is still a barrier that
holds back ERP images from being used as easily as conventional perspective
images. This is especially relevant when estimating 360{\deg} optical flow, as
the distortions need to be mitigated appropriately. In this paper, we propose a
360{\deg} optical flow method based on tangent images. Our method leverages
gnomonic projection to locally convert ERP images to perspective images, and
uniformly samples the ERP image by projection to a cubemap and regular
icosahedron vertices, to incrementally refine the estimated 360{\deg} flow
fields even in the presence of large rotations. Our experiments demonstrate the
benefits of our proposed method both quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Closer Look at the Transferability of Adversarial Examples: How They Fool Different Models Differently. (arXiv:2112.14337v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14337">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are vulnerable to adversarial examples (AEs), which have
adversarial transferability: AEs generated for the source model can mislead
another (target) model's predictions. However, the transferability has not been
understood from the perspective of to which class target model's predictions
were misled (i.e., class-aware transferability). In this paper, we
differentiate the cases in which a target model predicts the same wrong class
as the source model ("same mistake") or a different wrong class ("different
mistake") to analyze and provide an explanation of the mechanism. First, our
analysis shows (1) that same mistakes correlate with "non-targeted
transferability" and (2) that different mistakes occur between similar models
regardless of the perturbation size. Second, we present evidence that the
difference in same and different mistakes can be explained by non-robust
features, predictive but human-uninterpretable patterns: different mistakes
occur when non-robust features in AEs are used differently by models.
Non-robust features can thus provide consistent explanations for the
class-aware transferability of AEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Super-Efficient Super Resolution for Fast Adversarial Defense at the Edge. (arXiv:2112.14340v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14340">
<div class="article-summary-box-inner">
<span><p>Autonomous systems are highly vulnerable to a variety of adversarial attacks
on Deep Neural Networks (DNNs). Training-free model-agnostic defenses have
recently gained popularity due to their speed, ease of deployment, and ability
to work across many DNNs. To this end, a new technique has emerged for
mitigating attacks on image classification DNNs, namely, preprocessing
adversarial images using super resolution -- upscaling low-quality inputs into
high-resolution images. This defense requires running both image classifiers
and super resolution models on constrained autonomous systems. However, super
resolution incurs a heavy computational cost. Therefore, in this paper, we
investigate the following question: Does the robustness of image classifiers
suffer if we use tiny super resolution models? To answer this, we first review
a recent work called Super-Efficient Super Resolution (SESR) that achieves
similar or better image quality than prior art while requiring 2x to 330x fewer
Multiply-Accumulate (MAC) operations. We demonstrate that despite being orders
of magnitude smaller than existing models, SESR achieves the same level of
robustness as significantly larger networks. Finally, we estimate end-to-end
performance of super resolution-based defenses on a commercial Arm Ethos-U55
micro-NPU. Our findings show that SESR achieves nearly 3x higher FPS than a
baseline while achieving similar robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Background-aware Classification Activation Map for Weakly Supervised Object Localization. (arXiv:2112.14379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14379">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object localization (WSOL) relaxes the requirement of dense
annotations for object localization by using image-level classification masks
to supervise its learning process. However, current WSOL methods suffer from
excessive activation of background locations and need post-processing to obtain
the localization mask. This paper attributes these issues to the unawareness of
background cues, and propose the background-aware classification activation map
(B-CAM) to simultaneously learn localization scores of both object and
background with only image-level labels. In our B-CAM, two image-level
features, aggregated by pixel-level features of potential background and object
locations, are used to purify the object feature from the object-related
background and to represent the feature of the pure-background sample,
respectively. Then based on these two features, both the object classifier and
the background classifier are learned to determine the binary object
localization mask. Our B-CAM can be trained in end-to-end manner based on a
proposed stagger classification loss, which not only improves the objects
localization but also suppresses the background activation. Experiments show
that our B-CAM outperforms one-stage WSOL methods on the CUB-200, OpenImages
and VOC2012 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Empirical Risk Minimization for Unbiased Long-tailed Classification. (arXiv:2112.14380v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14380">
<div class="article-summary-box-inner">
<span><p>We address the overlooked unbiasedness in existing long-tailed classification
methods: we find that their overall improvement is mostly attributed to the
biased preference of tail over head, as the test distribution is assumed to be
balanced; however, when the test is as imbalanced as the long-tailed training
data -- let the test respect Zipf's law of nature -- the tail bias is no longer
beneficial overall because it hurts the head majorities. In this paper, we
propose Cross-Domain Empirical Risk Minimization (xERM) for training an
unbiased model to achieve strong performances on both test distributions, which
empirically demonstrates that xERM fundamentally improves the classification by
learning better feature representation rather than the head vs. tail game.
Based on causality, we further theoretically explain why xERM achieves
unbiasedness: the bias caused by the domain selection is removed by adjusting
the empirical risks on the imbalanced domain and the balanced but unseen
domain. Codes are available at https://github.com/BeierZhu/xERM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COTReg:Coupled Optimal Transport based Point Cloud Registration. (arXiv:2112.14381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14381">
<div class="article-summary-box-inner">
<span><p>Generating a set of high-quality correspondences or matches is one of the
most critical steps in point cloud registration. This paper proposes a learning
framework COTReg by jointly considering the pointwise and structural matchings
to predict correspondences of 3D point cloud registration. Specifically, we
transform the two matchings into a Wasserstein distance-based and a
Gromov-Wasserstein distance-based optimizations, respectively. Thus the task of
establishing the correspondences can be naturally reshaped to a coupled optimal
transport problem. Furthermore, we design a network to predict the confidence
score of being an inlier for each point of the point clouds, which provides the
overlap region information to generate correspondences. Our correspondence
prediction pipeline can be easily integrated into either learning-based
features like FCGF or traditional descriptors like FPFH. We conducted
comprehensive experiments on 3DMatch, KITTI, 3DCSR, and ModelNet40 benchmarks,
showing the state-of-art performance of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Robustifying Guidance for Monocular 3D Face Reconstruction. (arXiv:2112.14382v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14382">
<div class="article-summary-box-inner">
<span><p>Despite the recent developments in 3D Face Reconstruction from occluded and
noisy face images, the performance is still unsatisfactory. One of the main
challenges is to handle moderate to heavy occlusions in the face images. In
addition, the noise in the face images inhibits the correct capture of facial
attributes, thus needing to be reliably addressed. Moreover, most existing
methods rely on additional dependencies, posing numerous constraints over the
training procedure. Therefore, we propose a Self-Supervised RObustifying
GUidancE (ROGUE) framework to obtain robustness against occlusions and noise in
the face images. The proposed network contains 1) the Guidance Pipeline to
obtain the 3D face coefficients for the clean faces, and 2) the Robustification
Pipeline to acquire the consistency between the estimated coefficients for
occluded or noisy images and the clean counterpart. The proposed image- and
feature-level loss functions aid the ROGUE learning process without posing
additional dependencies. On the three variations of the test dataset of CelebA:
rational occlusions, delusional occlusions, and noisy face images, our method
outperforms the current state-of-the-art method by large margins (e.g., for the
shape-based 3D vertex errors, a reduction from 0.146 to 0.048 for rational
occlusions, from 0.292 to 0.061 for delusional occlusions and from 0.269 to
0.053 for the noise in the face images), demonstrating the effectiveness of the
proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overcoming Mode Collapse with Adaptive Multi Adversarial Training. (arXiv:2112.14406v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14406">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) are a class of generative models used
for various applications, but they have been known to suffer from the mode
collapse problem, in which some modes of the target distribution are ignored by
the generator. Investigative study using a new data generation procedure
indicates that the mode collapse of the generator is driven by the
discriminator's inability to maintain classification accuracy on previously
seen samples, a phenomenon called Catastrophic Forgetting in continual
learning. Motivated by this observation, we introduce a novel training
procedure that adaptively spawns additional discriminators to remember previous
modes of generation. On several datasets, we show that our training scheme can
be plugged-in to existing GAN frameworks to mitigate mode collapse and improve
standard metrics for GAN evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invertible Image Dataset Protection. (arXiv:2112.14420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14420">
<div class="article-summary-box-inner">
<span><p>Deep learning has achieved enormous success in various industrial
applications. Companies do not want their valuable data to be stolen by
malicious employees to train pirated models. Nor do they wish the data analyzed
by the competitors after using them online. We propose a novel solution for
dataset protection in this scenario by robustly and reversibly transform the
images into adversarial images. We develop a reversible adversarial example
generator (RAEG) that introduces slight changes to the images to fool
traditional classification models. Even though malicious attacks train pirated
models based on the defensed versions of the protected images, RAEG can
significantly weaken the functionality of these models. Meanwhile, the
reversibility of RAEG ensures the performance of authorized models. Extensive
experiments demonstrate that RAEG can better protect the data with slight
distortion against adversarial defense than previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Color Image Steganography Based on Frequency Sub-band Selection. (arXiv:2112.14437v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14437">
<div class="article-summary-box-inner">
<span><p>Color image steganography based on deep learning is the art of hiding
information in the color image. Among them, image hiding steganography(hiding
image with image) has attracted much attention in recent years because of its
great steganographic capacity. However, images generated by image hiding
steganography may show some obvious color distortion or artificial texture
traces. We propose a color image steganographic model based on frequency
sub-band selection to solve the above problems. Firstly, we discuss the
relationship between the characteristics of different color spaces/frequency
sub-bands and the generated image quality. Then, we select the B channel of the
RGB image as the embedding channel and the high-frequency sub-band as the
embedding domain. DWT(discrete wavelet transformation) transforms B channel
information and secret gray image into frequency domain information, and then
the secret image is embedded and extracted in the frequency domain.
Comprehensive experiments demonstrate that images generated by our model have
better image quality, and the imperceptibility is significantly increased.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACDNet: Adaptively Combined Dilated Convolution for Monocular Panorama Depth Estimation. (arXiv:2112.14440v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14440">
<div class="article-summary-box-inner">
<span><p>Depth estimation is a crucial step for 3D reconstruction with panorama images
in recent years. Panorama images maintain the complete spatial information but
introduce distortion with equirectangular projection. In this paper, we propose
an ACDNet based on the adaptively combined dilated convolution to predict the
dense depth map for a monocular panoramic image. Specifically, we combine the
convolution kernels with different dilations to extend the receptive field in
the equirectangular projection. Meanwhile, we introduce an adaptive
channel-wise fusion module to summarize the feature maps and get diverse
attention areas in the receptive field along the channels. Due to the
utilization of channel-wise attention in constructing the adaptive channel-wise
fusion module, the network can capture and leverage the cross-channel
contextual information efficiently. Finally, we conduct depth estimation
experiments on three datasets (both virtual and real-world) and the
experimental results demonstrate that our proposed ACDNet substantially
outperforms the current state-of-the-art (SOTA) methods. Our codes and model
parameters are accessed in https://github.com/zcq15/ACDNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Feature Extraction for Generalized Zero-shot Learning. (arXiv:2112.14478v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14478">
<div class="article-summary-box-inner">
<span><p>Generalized zero-shot learning (GZSL) is a technique to train a deep learning
model to identify unseen classes using the attribute. In this paper, we put
forth a new GZSL technique that improves the GZSL classification performance
greatly. Key idea of the proposed approach, henceforth referred to as semantic
feature extraction-based GZSL (SE-GZSL), is to use the semantic feature
containing only attribute-related information in learning the relationship
between the image and the attribute. In doing so, we can remove the
interference, if any, caused by the attribute-irrelevant information contained
in the image feature. To train a network extracting the semantic feature, we
present two novel loss functions, 1) mutual information-based loss to capture
all the attribute-related information in the image feature and 2)
similarity-based loss to remove unwanted attribute-irrelevant information. From
extensive experiments using various datasets, we show that the proposed SE-GZSL
technique outperforms conventional GZSL approaches by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-phase training mitigates class imbalance for camera trap image classification with CNNs. (arXiv:2112.14491v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14491">
<div class="article-summary-box-inner">
<span><p>By leveraging deep learning to automatically classify camera trap images,
ecologists can monitor biodiversity conservation efforts and the effects of
climate change on ecosystems more efficiently. Due to the imbalanced
class-distribution of camera trap datasets, current models are biased towards
the majority classes. As a result, they obtain good performance for a few
majority classes but poor performance for many minority classes. We used
two-phase training to increase the performance for these minority classes. We
trained, next to a baseline model, four models that implemented a different
versions of two-phase training on a subset of the highly imbalanced Snapshot
Serengeti dataset. Our results suggest that two-phase training can improve
performance for many minority classes, with limited loss in performance for the
other classes. We find that two-phase training based on majority undersampling
increases class-specific F1-scores up to 3.0%. We also find that two-phase
training outperforms using only oversampling or undersampling by 6.1% in
F1-score on average. Finally, we find that a combination of over- and
undersampling leads to a better performance than using them individually.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Distribution Patterns of Clownfish in Recirculating Aquaculture Systems. (arXiv:2112.14513v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14513">
<div class="article-summary-box-inner">
<span><p>Monitoring and detecting fish behaviors provide essential information on fish
welfare and contribute to achieving intelligent production in global
aquaculture. This work proposes an efficient approach to analyze the spatial
distribution status and motion patterns of juvenile clownfish (Amphiprion
bicinctus) maintained in aquaria at three stocking densities (1, 5, and 10
individuals/aquarium). The estimated displacement is the key factor in
assessing the dispersion and velocity to express the clownfish's spatial
distribution and movement behavior in a recirculating aquaculture system.
Indeed, we aim at computing the velocity, magnitude, and turning angle using an
optical flow method to assist aquaculturists in efficiently monitoring and
identifying fish behavior. We test the system design on a database containing
two days of video streams of juvenile clownfish maintained in aquaria. The
proposed displacement estimation reveals good performance in measuring
clownfish's motion and dispersion characteristics. Furthermore, we demonstrate
the effectiveness of the proposed technique for quantifying variation in
clownfish activity levels between recordings taken in the morning and
afternoon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Res2NetFuse: A Fusion Method for Infrared and Visible Images. (arXiv:2112.14540v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14540">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel Res2Net-based fusion framework for infrared and
visible images. The proposed fusion model has three parts: an encoder, a fusion
layer and a decoder, respectively. The Res2Net-based encoder is used to extract
multi-scale features of source images, the paper introducing a new training
strategy for training a Res2Net-based encoder that uses only a single image.
Then, a new fusion strategy is developed based on the attention model. Finally,
the fused image is reconstructed by the decoder. The proposed approach is also
analyzed in detail. Experiments show that our method achieves state-of-the-art
fusion performance in objective and subjective assessment by comparing with the
existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Onsite Non-Line-of-Sight Imaging via Online Calibrations. (arXiv:2112.14555v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14555">
<div class="article-summary-box-inner">
<span><p>There has been an increasing interest in deploying non-line-of-sight (NLOS)
imaging systems for recovering objects behind an obstacle. Existing solutions
generally pre-calibrate the system before scanning the hidden objects. Onsite
adjustments of the occluder, object and scanning pattern require
re-calibration. We present an online calibration technique that directly
decouples the acquired transients at onsite scanning into the LOS and hidden
components. We use the former to directly (re)calibrate the system upon changes
of scene/obstacle configurations, scanning regions, and scanning patterns
whereas the latter for hidden object recovery via spatial, frequency or
learning based techniques. Our technique avoids using auxiliary calibration
apparatus such as mirrors or checkerboards and supports both laboratory
validations and real-world deployments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HPRN: Holistic Prior-embedded Relation Network for Spectral Super-Resolution. (arXiv:2112.14608v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14608">
<div class="article-summary-box-inner">
<span><p>Spectral super-resolution (SSR) refers to the hyperspectral image (HSI)
recovery from an RGB counterpart. Due to the one-to-many nature of the SSR
problem, a single RGB image can be reprojected to many HSIs. The key to tackle
this illposed problem is to plug into multi-source prior information such as
the natural RGB spatial context-prior, deep feature-prior or inherent HSI
statistical-prior, etc., so as to improve the confidence and fidelity of
reconstructed spectra. However, most current approaches only consider the
general and limited priors in their designing the customized convolutional
neural networks (CNNs), which leads to the inability to effectively alleviate
the degree of ill-posedness. To address the problematic issues, we propose a
novel holistic prior-embedded relation network (HPRN) for SSR. Basically, the
core framework is delicately assembled by several multi-residual relation
blocks (MRBs) that fully facilitate the transmission and utilization of the
low-frequency content prior of RGB signals. Innovatively, the semantic prior of
RGB input is introduced to identify category attributes and a semantic-driven
spatial relation module (SSRM) is put forward to perform the feature
aggregation among the clustered similar characteristics using a
semantic-embedded relation matrix. Additionally, we develop a transformer-based
channel relation module (TCRM), which breaks the habit of employing scalars as
the descriptors of channel-wise relations in the previous deep feature-prior
and replaces them with certain vectors, together with Transformerstyle feature
interactions, supporting the representations to be more discriminative. In
order to maintain the mathematical correlation and spectral consistency between
hyperspectral bands, the second-order prior constraints (SOPC) are incorporated
into the loss function to guide the HSI reconstruction process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implementation of Convolutional Neural Network Architecture on 3D Multiparametric Magnetic Resonance Imaging for Prostate Cancer Diagnosis. (arXiv:2112.14644v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14644">
<div class="article-summary-box-inner">
<span><p>Prostate cancer is one of the most common causes of cancer deaths in men.
There is a growing demand for noninvasively and accurately diagnostic methods
that facilitate the current standard prostate cancer risk assessment in
clinical practice. Still, developing computer-aided classification tools in
prostate cancer diagnostics from multiparametric magnetic resonance images
continues to be a challenge. In this work, we propose a novel deep learning
approach for automatic classification of prostate lesions in the corresponding
magnetic resonance images by constructing a two-stage multimodal multi-stream
convolutional neural network (CNN)-based architecture framework. Without
implementing sophisticated image preprocessing steps or third-party software,
our framework achieved the classification performance with the area under a
Receiver Operating Characteristic (ROC) curve value of 0.87. The result
outperformed most of the submitted methods and shared the highest value
reported by the PROSTATEx Challenge organizer. Our proposed CNN-based framework
reflects the potential of assisting medical image interpretation in prostate
cancer and reducing unnecessary biopsies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Instability of Relative Pose Estimation and RANSAC's Role. (arXiv:2112.14651v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14651">
<div class="article-summary-box-inner">
<span><p>In this paper we study the numerical instabilities of the 5- and 7-point
problems for essential and fundamental matrix estimation in multiview geometry.
In both cases we characterize the ill-posed world scenes where the condition
number for epipolar estimation is infinite. We also characterize the ill-posed
instances in terms of the given image data. To arrive at these results, we
present a general framework for analyzing the conditioning of minimal problems
in multiview geometry, based on Riemannian manifolds. Experiments with
synthetic and real-world data then reveal a striking conclusion: that Random
Sample Consensus (RANSAC) in Structure-from-Motion (SfM) does not only serve to
filter out outliers, but RANSAC also selects for well-conditioned image data,
sufficiently separated from the ill-posed locus that our theory predicts. Our
findings suggest that, in future work, one could try to accelerate and increase
the success of RANSAC by testing only well-conditioned image data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gendered Differences in Face Recognition Accuracy Explained by Hairstyles, Makeup, and Facial Morphology. (arXiv:2112.14656v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14656">
<div class="article-summary-box-inner">
<span><p>Media reports have accused face recognition of being ''biased'', ''sexist''
and ''racist''. There is consensus in the research literature that face
recognition accuracy is lower for females, who often have both a higher false
match rate and a higher false non-match rate. However, there is little
published research aimed at identifying the cause of lower accuracy for
females. For instance, the 2019 Face Recognition Vendor Test that documents
lower female accuracy across a broad range of algorithms and datasets also
lists ''Analyze cause and effect'' under the heading ''What we did not do''. We
present the first experimental analysis to identify major causes of lower face
recognition accuracy for females on datasets where previous research has
observed this result. Controlling for equal amount of visible face in the test
images mitigates the apparent higher false non-match rate for females.
Additional analysis shows that makeup-balanced datasets further improves
females to achieve lower false non-match rates. Finally, a clustering
experiment suggests that images of two different females are inherently more
similar than of two different males, potentially accounting for a difference in
false match rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaGraspNet: A Large-Scale Benchmark Dataset forVision-driven Robotic Grasping via Physics-basedMetaverse Synthesis. (arXiv:2112.14663v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14663">
<div class="article-summary-box-inner">
<span><p>There has been increasing interest in smart factories powered by robotics
systems to tackle repetitive, laborious tasks. One impactful yet challenging
task in robotics-powered smart factory applications is robotic grasping: using
robotic arms to grasp objects autonomously in different settings. Robotic
grasping requires a variety of computer vision tasks such as object detection,
segmentation, grasp prediction, pick planning, etc. While significant progress
has been made in leveraging of machine learning for robotic grasping,
particularly with deep learning, a big challenge remains in the need for
large-scale, high-quality RGBD datasets that cover a wide diversity of
scenarios and permutations. To tackle this big, diverse data problem, we are
inspired by the recent rise in the concept of metaverse, which has greatly
closed the gap between virtual worlds and the physical world. Metaverses allow
us to create digital twins of real-world manufacturing scenarios and to
virtually create different scenarios from which large volumes of data can be
generated for training models. In this paper, we present MetaGraspNet: a
large-scale benchmark dataset for vision-driven robotic grasping via
physics-based metaverse synthesis. The proposed dataset contains 100,000 images
and 25 different object types and is split into 5 difficulties to evaluate
object detection and segmentation model performance in different grasping
scenarios. We also propose a new layout-weighted performance metric alongside
the dataset for evaluating object detection and segmentation performance in a
manner that is more appropriate for robotic grasp applications compared to
existing general-purpose performance metrics. Our benchmark dataset is
available open-source on Kaggle, with the first phase consisting of detailed
object detection, segmentation, layout annotations, and a layout-weighted
performance metric script.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2. (arXiv:2112.14683v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14683">
<div class="article-summary-box-inner">
<span><p>Videos show continuous events, yet most - if not all - video synthesis
frameworks treat them discretely in time. In this work, we think of videos of
what they should be - time-continuous signals, and extend the paradigm of
neural representations to build a continuous-time video generator. For this, we
first design continuous motion representations through the lens of positional
embeddings. Then, we explore the question of training on very sparse videos and
demonstrate that a good generator can be learned by using as few as 2 frames
per clip. After that, we rethink the traditional image and video discriminators
pair and propose to use a single hypernetwork-based one. This decreases the
training cost and provides richer learning signal to the generator, making it
possible to train directly on 1024$^2$ videos for the first time. We build our
model on top of StyleGAN2 and it is just 5% more expensive to train at the same
resolution while achieving almost the same image quality. Moreover, our latent
space features similar properties, enabling spatial manipulations that our
method can propagate in time. We can generate arbitrarily long videos at
arbitrary high frame rate, while prior work struggles to generate even 64
frames at a fixed rate. Our model achieves state-of-the-art results on four
modern 256$^2$ video synthesis benchmarks and one 1024$^2$ resolution one.
Videos and the source code are available at the project website:
https://universome.github.io/stylegan-v.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentanglement and Generalization Under Correlation Shifts. (arXiv:2112.14754v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14754">
<div class="article-summary-box-inner">
<span><p>Correlations between factors of variation are prevalent in real-world data.
Machine learning algorithms may benefit from exploiting such correlations, as
they can increase predictive performance on noisy data. However, often such
correlations are not robust (e.g., they may change between domains, datasets,
or applications) and we wish to avoid exploiting them. Disentanglement methods
aim to learn representations which capture different factors of variation in
latent subspaces. A common approach involves minimizing the mutual information
between latent subspaces, such that each encodes a single underlying attribute.
However, this fails when attributes are correlated. We solve this problem by
enforcing independence between subspaces conditioned on the available
attributes, which allows us to remove only dependencies that are not due to the
correlation structure present in the training data. We achieve this via an
adversarial approach to minimize the conditional mutual information (CMI)
between subspaces with respect to categorical variables. We first show
theoretically that CMI minimization is a good objective for robust
disentanglement on linear problems with Gaussian data. We then apply our method
on real-world datasets based on MNIST and CelebA, and show that it yields
models that are disentangled and robust under correlation shift, including in
weakly supervised settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Baseline for Zero-shot Semantic Segmentation with Pre-trained Vision-language Model. (arXiv:2112.14757v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14757">
<div class="article-summary-box-inner">
<span><p>Recently, zero-shot image classification by vision-language pre-training has
demonstrated incredible achievements, that the model can classify arbitrary
category without seeing additional annotated images of that category. However,
it is still unclear how to make the zero-shot recognition working well on
broader vision problems, such as object detection and semantic segmentation. In
this paper, we target for zero-shot semantic segmentation, by building it on an
off-the-shelf pre-trained vision-language model, i.e., CLIP. It is difficult
because semantic segmentation and the CLIP model perform on different visual
granularity, that semantic segmentation processes on pixels while CLIP performs
on images. To remedy the discrepancy on processing granularity, we refuse the
use of the prevalent one-stage FCN based framework, and advocate a two-stage
semantic segmentation framework, with the first stage extracting generalizable
mask proposals and the second stage leveraging an image based CLIP model to
perform zero-shot classification on the masked image crops which are generated
in the first stage. Our experimental results show that this simple framework
surpasses previous state-of-the-arts by a large margin: +29.5 hIoU on the
Pascal VOC 2012 dataset, and +8.9 hIoU on the COCO Stuff dataset. With its
simplicity and strong performance, we hope this framework to serve as a
baseline to facilitate the future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Oriented Convex Bilevel Optimization with Latent Feasibility. (arXiv:1907.03083v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.03083">
<div class="article-summary-box-inner">
<span><p>This paper firstly proposes a convex bilevel optimization paradigm to
formulate and optimize popular learning and vision problems in real-world
scenarios. Different from conventional approaches, which directly design their
iteration schemes based on given problem formulation, we introduce a
task-oriented energy as our latent constraint which integrates richer task
information. By explicitly re-characterizing the feasibility, we establish an
efficient and flexible algorithmic framework to tackle convex models with both
shrunken solution space and powerful auxiliary (based on domain knowledge and
data distribution of the task). In theory, we present the convergence analysis
of our latent feasibility re-characterization based numerical strategy. We also
analyze the stability of the theoretical convergence under computational error
perturbation. Extensive numerical experiments are conducted to verify our
theoretical findings and evaluate the practical performance of our method on
different applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Histogram Layers for Texture Analysis. (arXiv:2001.00215v12 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.00215">
<div class="article-summary-box-inner">
<span><p>An essential aspect of texture analysis is the extraction of features that
describe the distribution of values in local, spatial regions. We present a
localized histogram layer for artificial neural networks. Instead of computing
global histograms as done previously, the proposed histogram layer directly
computes the local, spatial distribution of features for texture analysis and
parameters for the layer are estimated during backpropagation. We compare our
method with state-of-the-art texture encoding methods such as the Deep Encoding
Network Pooling, Deep Texture Encoding Network, Fisher Vector convolutional
neural network, and Multi-level Texture Encoding and Representation on three
material/texture datasets: (1) the Describable Texture Dataset; (2) an
extension of the ground terrain in outdoor scenes; (3) and a subset of the
Materials in Context dataset. Results indicate that the inclusion of the
proposed histogram layer improves performance. The source code for the
histogram layer is publicly available:
https://github.com/GatorSense/Histogram_Layer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TPPO: A Novel Trajectory Predictor with Pseudo Oracle. (arXiv:2002.01852v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.01852">
<div class="article-summary-box-inner">
<span><p>Forecasting pedestrian trajectories in dynamic scenes remains a critical
problem in various applications, such as autonomous driving and socially aware
robots. Such forecasting is challenging due to human-human and human-object
interactions and future uncertainties caused by human randomness. Generative
model-based methods handle future uncertainties by sampling a latent variable.
However, few studies explored the generation of the latent variable. In this
work, we propose the Trajectory Predictor with Pseudo Oracle (TPPO), which is a
generative model-based trajectory predictor. The first pseudo oracle is
pedestrians' moving directions, and the second one is the latent variable
estimated from ground truth trajectories. A social attention module is used to
aggregate neighbors' interactions based on the correlation between pedestrians'
moving directions and future trajectories. This correlation is inspired by the
fact that pedestrians' future trajectories are often influenced by pedestrians
in front. A latent variable predictor is proposed to estimate latent variable
distributions from observed and ground-truth trajectories. Moreover, the gap
between these two distributions is minimized during training. Therefore, the
latent variable predictor can estimate the latent variable from observed
trajectories to approximate that estimated from ground-truth trajectories. We
compare the performance of TPPO with related methods on several public
datasets. Results demonstrate that TPPO outperforms state-of-the-art methods
with low average and final displacement errors. The ablation study shows that
the prediction performance will not dramatically decrease as sampling times
decline during tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Gradient based Adversarial Attacks for Quantized Networks. (arXiv:2003.13511v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.13511">
<div class="article-summary-box-inner">
<span><p>Neural network quantization has become increasingly popular due to efficient
memory consumption and faster computation resulting from bitwise operations on
the quantized networks. Even though they exhibit excellent generalization
capabilities, their robustness properties are not well-understood. In this
work, we systematically study the robustness of quantized networks against
gradient based adversarial attacks and demonstrate that these quantized models
suffer from gradient vanishing issues and show a fake sense of robustness. By
attributing gradient vanishing to poor forward-backward signal propagation in
the trained network, we introduce a simple temperature scaling approach to
mitigate this issue while preserving the decision boundary. Despite being a
simple modification to existing gradient based adversarial attacks, experiments
on multiple image classification datasets with multiple network architectures
demonstrate that our temperature scaled attacks obtain near-perfect success
rate on quantized networks while outperforming original attacks on
adversarially trained models as well as floating-point networks. Code is
available at https://github.com/kartikgupta-at-anu/attack-bnn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSSM: Deep State-Space Model for 3D Human Motion Prediction. (arXiv:2005.12155v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.12155">
<div class="article-summary-box-inner">
<span><p>Predicting future human motion plays a significant role in human-machine
interactions for various real-life applications. A unified formulation and
multi-order modeling are two critical perspectives for analyzing and
representing human motion. In contrast to prior works, we improve the
multi-order modeling ability of human motion systems for more accurate
predictions by building a deep state-space model (DeepSSM). The DeepSSM
utilizes the advantages of both the state-space theory and the deep network.
Specifically, we formulate the human motion system as the state-space model of
a dynamic system and model the motion system by the state-space theory,
offering a unified formulation for diverse human motion systems. Moreover, a
novel deep network is designed to parameterize this system, which jointly
models the state-state transition and state-observation transition processes.
In this way, the state of a system is updated by the multi-order information of
a time-varying human motion sequence. Multiple future poses are recursively
predicted via the state-observation transition. To further improve the model
ability of the system, a novel loss, WT-MPJPE (Weighted Temporal Mean Per Joint
Position Error), is introduced to optimize the model. The proposed loss
encourages the system to achieve more accurate predictions by increasing
weights to the early time steps. The experiments on two benchmark datasets
(i.e., Human3.6M and 3DPW) confirm that our method achieves state-of-the-art
performance with improved accuracy of at least 2.2mm per joint. The code will
be available at: \url{https://github.com/lily2lab/DeepSSM.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibration of Neural Networks using Splines. (arXiv:2006.12800v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12800">
<div class="article-summary-box-inner">
<span><p>Calibrating neural networks is of utmost importance when employing them in
safety-critical applications where the downstream decision making depends on
the predicted probabilities. Measuring calibration error amounts to comparing
two empirical distributions. In this work, we introduce a binning-free
calibration measure inspired by the classical Kolmogorov-Smirnov (KS)
statistical test in which the main idea is to compare the respective cumulative
probability distributions. From this, by approximating the empirical cumulative
distribution using a differentiable function via splines, we obtain a
recalibration function, which maps the network outputs to actual (calibrated)
class assignment probabilities. The spine-fitting is performed using a held-out
calibration set and the obtained recalibration function is evaluated on an
unseen test set. We tested our method against existing calibration approaches
on various image classification datasets and our spline-based recalibration
approach consistently outperforms existing methods on KS error as well as other
commonly used calibration measures. Our Code is available at
https://github.com/kartikgupta-at-anu/spline-calibration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation. (arXiv:2007.04954v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.04954">
<div class="article-summary-box-inner">
<span><p>We introduce ThreeDWorld (TDW), a platform for interactive multi-modal
physical simulation. TDW enables simulation of high-fidelity sensory data and
physical interactions between mobile agents and objects in rich 3D
environments. Unique properties include: real-time near-photo-realistic image
rendering; a library of objects and environments, and routines for their
customization; generative procedures for efficiently building classes of new
environments; high-fidelity audio rendering; realistic physical interactions
for a variety of material types, including cloths, liquid, and deformable
objects; customizable agents that embody AI agents; and support for human
interactions with VR devices. TDW's API enables multiple agents to interact
within a simulation and returns a range of sensor and physics data representing
the state of the world. We present initial experiments enabled by TDW in
emerging research directions in computer vision, machine learning, and
cognitive science, including multi-modal physical scene understanding, physical
dynamics predictions, multi-agent interactions, models that learn like a child,
and attention studies in humans and neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust and Reliable Point Cloud Recognition Network Under Rigid Transformation. (arXiv:2009.06903v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06903">
<div class="article-summary-box-inner">
<span><p>Point cloud recognition is an essential task in industrial robotics and
autonomous driving. Recently, several point cloud processing models have
achieved state-of-the-art performances. However, these methods lack rotation
robustness, and their performances degrade severely under random rotations,
failing to extend to real-world scenarios with varying orientations. To this
end, we propose a method named Self Contour-based Transformation (SCT), which
can be flexibly integrated into various existing point cloud recognition models
against arbitrary rotations. SCT provides efficient rotation and translation
invariance by introducing Contour-Aware Transformation (CAT), which linearly
transforms Cartesian coordinates of points to translation and
rotation-invariant representations. We prove that CAT is a rotation and
translation-invariant transformation based on the theoretical analysis.
Furthermore, the Frame Alignment module is proposed to enhance discriminative
feature extraction by capturing contours and transforming self contour-based
frames into intra-class frames. Extensive experimental results show that SCT
outperforms the state-of-the-art approaches under arbitrary rotations in
effectiveness and efficiency on synthetic and real-world benchmarks.
Furthermore, the robustness and generality evaluations indicate that SCT is
robust and is applicable to various point cloud processing models, which
highlights the superiority of SCT in industrial applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHS-Net: A Deep learning approach for hierarchical segmentation of COVID-19 infected CT images. (arXiv:2012.07079v7 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07079">
<div class="article-summary-box-inner">
<span><p>The pandemic of novel SARS-CoV-2 also known as COVID-19 has been spreading
worldwide, causing rampant loss of lives. Medical imaging such as CT, X-ray,
etc., plays a significant role in diagnosing the patients by presenting the
visual representation of the functioning of the organs. However, for any
radiologist analyzing such scans is a tedious and time-consuming task. The
emerging deep learning technologies have displayed its strength in analyzing
such scans to aid in the faster diagnosis of the diseases and viruses such as
COVID-19. In the present article, an automated deep learning based model,
COVID-19 hierarchical segmentation network (CHS-Net) is proposed that functions
as a semantic hierarchical segmenter to identify the COVID-19 infected regions
from lungs contour via CT medical imaging using two cascaded residual attention
inception U-Net (RAIU-Net) models. RAIU-Net comprises of a residual inception
U-Net model with spectral spatial and depth attention network (SSD) that is
developed with the contraction and expansion phases of depthwise separable
convolutions and hybrid pooling (max and spectral pooling) to efficiently
encode and decode the semantic and varying resolution information. The CHS-Net
is trained with the segmentation loss function that is the defined as the
average of binary cross entropy loss and dice loss to penalize false negative
and false positive predictions. The approach is compared with the recently
proposed approaches and evaluated using the standard metrics like accuracy,
precision, specificity, recall, dice coefficient and Jaccard similarity along
with the visualized interpretation of the model prediction with GradCam++ and
uncertainty maps. With extensive trials, it is observed that the proposed
approach outperformed the recently proposed approaches and effectively segments
the COVID-19 infected regions in the lungs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Adversarial Inconsistent Cognitive Sampling for Multi-view Progressive Subspace Clustering. (arXiv:2101.03783v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.03783">
<div class="article-summary-box-inner">
<span><p>Deep multi-view clustering methods have achieved remarkable performance.
However, all of them failed to consider the difficulty labels (uncertainty of
ground-truth for training samples) over multi-view samples, which may result
into a nonideal clustering network for getting stuck into poor local optima
during training process; worse still, the difficulty labels from multi-view
samples are always inconsistent, such fact makes it even more challenging to
handle. In this paper, we propose a novel Deep Adversarial Inconsistent
Cognitive Sampling (DAICS) method for multi-view progressive subspace
clustering. A multiview binary classification (easy or difficult) loss and a
feature similarity loss are proposed to jointly learn a binary classifier and a
deep consistent feature embedding network, throughout an adversarial minimax
game over difficulty labels of multiview consistent samples. We develop a
multi-view cognitive sampling strategy to select the input samples from easy to
difficult for multi-view clustering network training. However, the
distributions of easy and difficult samples are mixed together, hence not
trivial to achieve the goal. To resolve it, we define a sampling probability
with theoretical guarantee. Based on that, a golden section mechanism is
further designed to generate a sample set boundary to progressively select the
samples with varied difficulty labels via a gate unit, which is utilized to
jointly learn a multi-view common progressive subspace and clustering network
for more efficient clustering. Experimental results on four real-world datasets
demonstrate the superiority of DAICS over the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balanced Softmax Cross-Entropy for Incremental Learning. (arXiv:2103.12532v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12532">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are prone to catastrophic forgetting when incrementally
trained on new classes or new tasks as adaptation to the new data leads to a
drastic decrease of the performance on the old classes and tasks. By using a
small memory for rehearsal and knowledge distillation, recent methods have
proven to be effective to mitigate catastrophic forgetting. However due to the
limited size of the memory, large imbalance between the amount of data
available for the old and new classes still remains which results in a
deterioration of the overall accuracy of the model. To address this problem, we
propose the use of the Balanced Softmax Cross-Entropy loss and show that it can
be combined with exiting methods for incremental learning to improve their
performances while also decreasing the computational cost of the training
procedure in some cases. Experiments on the competitive ImageNet, subImageNet
and CIFAR100 datasets show states-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Best-Buddy GANs for Highly Detailed Image Super-Resolution. (arXiv:2103.15295v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15295">
<div class="article-summary-box-inner">
<span><p>We consider the single image super-resolution (SISR) problem, where a
high-resolution (HR) image is generated based on a low-resolution (LR) input.
Recently, generative adversarial networks (GANs) become popular to hallucinate
details. Most methods along this line rely on a predefined single-LR-single-HR
mapping, which is not flexible enough for the SISR task. Also, GAN-generated
fake details may often undermine the realism of the whole image. We address
these issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR.
Relaxing the immutable one-to-one constraint, we allow the estimated patches to
dynamically seek the best supervision during training, which is beneficial to
producing more reasonable details. Besides, we propose a region-aware
adversarial learning strategy that directs our model to focus on generating
details for textured areas adaptively. Extensive experiments justify the
effectiveness of our method. An ultra-high-resolution 4K dataset is also
constructed to facilitate future super-resolution research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking Shortcut: Exploring Fully Convolutional Cycle-Consistency for Video Correspondence Learning. (arXiv:2105.05838v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05838">
<div class="article-summary-box-inner">
<span><p>Previous cycle-consistency correspondence learning methods usually leverage
image patches for training. In this paper, we present a fully convolutional
method, which is simpler and more coherent to the inference process. While
directly applying fully convolutional training results in model collapse, we
study the underline reason behind this collapse phenomenon, indicating that the
absolute positions of pixels provide a shortcut to easily accomplish
cycle-consistence, which hinders the learning of meaningful visual
representations. To break this absolute position shortcut, we propose to apply
different crops for forward and backward frames, and adopt feature warping to
establish correspondence between two crops of a same frame. The former
technique enforces the corresponding pixels at forward and back tracks to have
different absolute positions, and the latter effectively blocks the shortcuts
going between forward and back tracks. In three label propagation benchmarks
for pose tracking, face landmark tracking and video object segmentation, our
method largely improves the results of vanilla fully convolutional
cycle-consistency method, achieving very competitive performance compared with
the self-supervised state-of-the-art approaches. Our trained model and code are
available at \url{https://github.com/Steve-Tod/STFC3}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Incremental Few-Shot Object Detection. (arXiv:2105.07637v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07637">
<div class="article-summary-box-inner">
<span><p>Conventional detection networks usually need abundant labeled training
samples, while humans can learn new concepts incrementally with just a few
examples. This paper focuses on a more challenging but realistic
class-incremental few-shot object detection problem (iFSD). It aims to
incrementally transfer the model for novel objects from only a few annotated
samples without catastrophically forgetting the previously learned ones. To
tackle this problem, we propose a novel method LEAST, which can transfer with
Less forgetting, fEwer training resources, And Stronger Transfer capability.
Specifically, we first present the transfer strategy to reduce unnecessary
weight adaptation and improve the transfer capability for iFSD. On this basis,
we then integrate the knowledge distillation technique using a less
resource-consuming approach to alleviate forgetting and propose a novel
clustering-based exemplar selection process to preserve more discriminative
features previously learned. Being a generic and effective method, LEAST can
largely improve the iFSD performance on various benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subdivision-Based Mesh Convolution Networks. (arXiv:2106.02285v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02285">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have made great breakthroughs in 2D
computer vision. However, their irregular structure makes it hard to harness
the potential of CNNs directly on meshes. A subdivision surface provides a
hierarchical multi-resolution structure, in which each face in a closed
2-manifold triangle mesh is exactly adjacent to three faces. Motivated by these
two observations, this paper presents SubdivNet, an innovative and versatile
CNN framework for 3D triangle meshes with Loop subdivision sequence
connectivity. Making an analogy between mesh faces and pixels in a 2D image
allows us to present a mesh convolution operator to aggregate local features
from nearby faces. By exploiting face neighborhoods, this convolution can
support standard 2D convolutional network concepts, e.g. variable kernel size,
stride, and dilation. Based on the multi-resolution hierarchy, we make use of
pooling layers which uniformly merge four faces into one and an upsampling
method which splits one face into four. Thereby, many popular 2D CNN
architectures can be easily adapted to process 3D meshes. Meshes with arbitrary
connectivity can be remeshed to have Loop subdivision sequence connectivity via
self-parameterization, making SubdivNet a general approach. Extensive
evaluation and various applications demonstrate SubdivNet's effectiveness and
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Transformer Networks for Semantic Image Segmentation. (arXiv:2106.04108v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04108">
<div class="article-summary-box-inner">
<span><p>Transformers have shown impressive performance in various natural language
processing and computer vision tasks, due to the capability of modeling
long-range dependencies. Recent progress has demonstrated that combining such
Transformers with CNN-based semantic image segmentation models is very
promising. However, it is not well studied yet on how well a pure Transformer
based approach can achieve for image segmentation. In this work, we explore a
novel framework for semantic image segmentation, which is encoder-decoder based
Fully Transformer Networks (FTN). Specifically, we first propose a Pyramid
Group Transformer (PGT) as the encoder for progressively learning hierarchical
features, meanwhile reducing the computation complexity of the standard Visual
Transformer (ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse
semantic-level and spatial-level information from multiple levels of the PGT
encoder for semantic image segmentation. Surprisingly, this simple baseline can
achieve better results on multiple challenging semantic segmentation and face
parsing benchmarks, including PASCAL Context, ADE20K, COCOStuff, and
CelebAMask-HQ. The source code will be released on
https://github.com/BR-IDL/PaddleViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topological Semantic Mapping by Consolidation of Deep Visual Features. (arXiv:2106.12709v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12709">
<div class="article-summary-box-inner">
<span><p>Many works in the recent literature introduce semantic mapping methods that
use CNNs (Convolutional Neural Networks) to recognize semantic properties in
images. The types of properties (eg.: room size, place category, and objects)
and their classes (eg.: kitchen and bathroom, for place category) are usually
predefined and restricted to a specific task. Thus, all the visual data
acquired and processed during the construction of the maps are lost and only
the recognized semantic properties remain on the maps. In contrast, this work
introduces a topological semantic mapping method that uses deep visual features
extracted by a CNN (GoogLeNet), from 2D images captured in multiple views of
the environment as the robot operates, to create, through averages,
consolidated representations of the visual features acquired in the regions
covered by each topological node. These representations allow flexible
recognition of semantic properties of the regions and use in other visual
tasks. Experiments with a real-world indoor dataset showed that the method is
able to consolidate the visual features of regions and use them to recognize
objects and place categories as semantic properties, and to indicate the
topological location of images, with very promising results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. (arXiv:2107.07933v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07933">
<div class="article-summary-box-inner">
<span><p>Unprecedented access to multi-temporal satellite imagery has opened new
perspectives for a variety of Earth observation tasks. Among them,
pixel-precise panoptic segmentation of agricultural parcels has major economic
and environmental implications. While researchers have explored this problem
for single images, we argue that the complex temporal patterns of crop
phenology are better addressed with temporal sequences of images. In this
paper, we present the first end-to-end, single-stage method for panoptic
segmentation of Satellite Image Time Series (SITS). This module can be combined
with our novel image sequence encoding network which relies on temporal
self-attention to extract rich and adaptive multi-scale spatio-temporal
features. We also introduce PASTIS, the first open-access SITS dataset with
panoptic annotations. We demonstrate the superiority of our encoder for
semantic segmentation against multiple competing architectures, and set up the
first state-of-the-art of panoptic segmentation of SITS. Our implementation and
PASTIS are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Explainability: A Tutorial on Gradient-Based Attribution Methods for Deep Neural Networks. (arXiv:2107.11400v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11400">
<div class="article-summary-box-inner">
<span><p>With the rise of deep neural networks, the challenge of explaining the
predictions of these networks has become increasingly recognized. While many
methods for explaining the decisions of deep neural networks exist, there is
currently no consensus on how to evaluate them. On the other hand, robustness
is a popular topic for deep learning research; however, it is hardly talked
about in explainability until very recently. In this tutorial paper, we start
by presenting gradient-based interpretability methods. These techniques use
gradient signals to assign the burden of the decision on the input features.
Later, we discuss how gradient-based methods can be evaluated for their
robustness and the role that adversarial robustness plays in having meaningful
explanations. We also discuss the limitations of gradient-based methods.
Finally, we present the best practices and attributes that should be examined
before choosing an explainability method. We conclude with the future
directions for research in the area at the convergence of robustness and
explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RCA-IUnet: A residual cross-spatial attention guided inception U-Net model for tumor segmentation in breast ultrasound imaging. (arXiv:2108.02508v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02508">
<div class="article-summary-box-inner">
<span><p>The advancements in deep learning technologies have produced immense
contributions to biomedical image analysis applications. With breast cancer
being the common deadliest disease among women, early detection is the key
means to improve survivability. Medical imaging like ultrasound presents an
excellent visual representation of the functioning of the organs; however, for
any radiologist analysing such scans is challenging and time consuming which
delays the diagnosis process. Although various deep learning based approaches
are proposed that achieved promising results, the present article introduces an
efficient residual cross-spatial attention guided inception U-Net (RCA-IUnet)
model with minimal training parameters for tumor segmentation using breast
ultrasound imaging to further improve the segmentation performance of varying
tumor sizes. The RCA-IUnet model follows U-Net topology with residual inception
depth-wise separable convolution and hybrid pooling (max pooling and spectral
pooling) layers. In addition, cross-spatial attention filters are added to
suppress the irrelevant features and focus on the target structure. The
segmentation performance of the proposed model is validated on two publicly
available datasets using standard segmentation evaluation metrics, where it
outperformed the other state-of-the-art segmentation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Real-Time Online Learning Framework for Joint 3D Reconstruction and Semantic Segmentation of Indoor Scenes. (arXiv:2108.05246v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05246">
<div class="article-summary-box-inner">
<span><p>This paper presents a real-time online vision framework to jointly recover an
indoor scene's 3D structure and semantic label. Given noisy depth maps, a
camera trajectory, and 2D semantic labels at train time, the proposed deep
neural network based approach learns to fuse the depth over frames with
suitable semantic labels in the scene space. Our approach exploits the joint
volumetric representation of the depth and semantics in the scene feature space
to solve this task. For a compelling online fusion of the semantic labels and
geometry in real-time, we introduce an efficient vortex pooling block while
dropping the use of routing network in online depth fusion to preserve
high-frequency surface details. We show that the context information provided
by the semantics of the scene helps the depth fusion network learn
noise-resistant features. Not only that, it helps overcome the shortcomings of
the current online depth fusion method in dealing with thin object structures,
thickening artifacts, and false surfaces. Experimental evaluation on the
Replica dataset shows that our approach can perform depth fusion at 37 and 10
frames per second with an average reconstruction F-score of 88% and 91%,
respectively, depending on the depth map resolution. Moreover, our model shows
an average IoU score of 0.515 on the ScanNet 3D semantic benchmark leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">New Pruning Method Based on DenseNet Network for Image Classification. (arXiv:2108.12604v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12604">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have made significant progress in the field of computer
vision. Recent studies have shown that depth, width and shortcut connections of
neural network architectures play a crucial role in their performance. One of
the most advanced neural network architectures, DenseNet, has achieved
excellent convergence rates through dense connections. However, it still has
obvious shortcomings in the usage of amount of memory. In this paper, we
introduce a new type of pruning tool, threshold, which refers to the principle
of the threshold voltage in MOSFET. This work employs this method to connect
blocks of different depths in different ways to reduce the usage of memory. It
is denoted as ThresholdNet. We evaluate ThresholdNet and other different
networks on datasets of CIFAR10. Experiments show that HarDNet is twice as fast
as DenseNet, and on this basis, ThresholdNet is 10% faster and 10% lower error
rate than HarDNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Attention Better Than Matrix Decomposition?. (arXiv:2109.04553v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04553">
<div class="article-summary-box-inner">
<span><p>As an essential ingredient of modern deep learning, attention mechanism,
especially self-attention, plays a vital role in the global correlation
discovery. However, is hand-crafted attention irreplaceable when modeling the
global context? Our intriguing finding is that self-attention is not better
than the matrix decomposition (MD) model developed 20 years ago regarding the
performance and computational cost for encoding the long-distance dependencies.
We model the global context issue as a low-rank recovery problem and show that
its optimization algorithms can help design global information blocks. This
paper then proposes a series of Hamburgers, in which we employ the optimization
algorithms for solving MDs to factorize the input representations into
sub-matrices and reconstruct a low-rank embedding. Hamburgers with different
MDs can perform favorably against the popular global context module
self-attention when carefully coping with gradients back-propagated through
MDs. Comprehensive experiments are conducted in the vision tasks where it is
crucial to learn the global context, including semantic segmentation and image
generation, demonstrating significant improvements over self-attention and its
variants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEACh: Task-driven Embodied Agents that Chat. (arXiv:2110.00534v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00534">
<div class="article-summary-box-inner">
<span><p>Robots operating in human spaces must be able to engage in natural language
interaction with people, both understanding and executing instructions, and
using conversation to resolve ambiguity and recover from mistakes. To study
this, we introduce TEACh, a dataset of over 3,000 human--human, interactive
dialogues to complete household tasks in simulation. A Commander with access to
oracle information about a task communicates in natural language with a
Follower. The Follower navigates through and interacts with the environment to
complete tasks varying in complexity from "Make Coffee" to "Prepare Breakfast",
asking questions and getting additional information from the Commander. We
propose three benchmarks using TEACh to study embodied intelligence challenges,
and we evaluate initial models' abilities in dialogue understanding, language
grounding, and task execution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CertainNet: Sampling-free Uncertainty Estimation for Object Detection. (arXiv:2110.01604v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01604">
<div class="article-summary-box-inner">
<span><p>Estimating the uncertainty of a neural network plays a fundamental role in
safety-critical settings. In perception for autonomous driving, measuring the
uncertainty means providing additional calibrated information to downstream
tasks, such as path planning, that can use it towards safe navigation. In this
work, we propose a novel sampling-free uncertainty estimation method for object
detection. We call it CertainNet, and it is the first to provide separate
uncertainties for each output signal: objectness, class, location and size. To
achieve this, we propose an uncertainty-aware heatmap, and exploit the
neighboring bounding boxes provided by the detector at inference time. We
evaluate the detection performance and the quality of the different uncertainty
estimates separately, also with challenging out-of-domain samples: BDD100K and
nuImages with models trained on KITTI. Additionally, we propose a new metric to
evaluate location and size uncertainties. When transferring to unseen datasets,
CertainNet generalizes substantially better than previous methods and an
ensemble, while being real-time and providing high quality and comprehensive
uncertainty estimates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Pedestrian Attribute Recognition Using Group Sparsity for Occlusion Videos. (arXiv:2110.08708v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08708">
<div class="article-summary-box-inner">
<span><p>Occlusion processing is a key issue in pedestrian attribute recognition
(PAR). Nevertheless, several existing video-based PAR methods have not yet
considered occlusion handling in depth. In this paper, we formulate finding
non-occluded frames as sparsity-based temporal attention of a crowded video. In
this manner, a model is guided not to pay attention to the occluded frame.
However, temporal sparsity cannot include a correlation between attributes when
occlusion occurs. For example, "boots" and "shoe color" cannot be recognized
when the foot is invisible. To solve the uncorrelated attention issue, we also
propose a novel group sparsity-based temporal attention module. Group sparsity
is applied across attention weights in correlated attributes. Thus, attention
weights in a group are forced to pay attention to the same frames. Experimental
results showed that the proposed method achieved a higher F1-score than the
state-of-the-art methods on two video-based PAR datasets and five occlusion
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. (arXiv:2110.08733v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08733">
<div class="article-summary-box-inner">
<span><p>Deep learning approaches have shown promising results in remote sensing high
spatial resolution (HSR) land-cover mapping. However, urban and rural scenes
can show completely different geographical landscapes, and the inadequate
generalizability of these algorithms hinders city-level or national-level
mapping. Most of the existing HSR land-cover datasets mainly promote the
research of learning semantic representation, thereby ignoring the model
transferability. In this paper, we introduce the Land-cOVEr Domain Adaptive
semantic segmentation (LoveDA) dataset to advance semantic and transferable
learning. The LoveDA dataset contains 5987 HSR images with 166768 annotated
objects from three different cities. Compared to the existing datasets, the
LoveDA dataset encompasses two domains (urban and rural), which brings
considerable challenges due to the: 1) multi-scale objects; 2) complex
background samples; and 3) inconsistent class distributions. The LoveDA dataset
is suitable for both land-cover semantic segmentation and unsupervised domain
adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on
eleven semantic segmentation methods and eight UDA methods. Some exploratory
studies including multi-scale architectures and strategies, additional
background supervision, and pseudo-label analysis were also carried out to
address these challenges. The code and data are available at
https://github.com/Junjue-Wang/LoveDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting the Transferability of Video Adversarial Examples via Temporal Translation. (arXiv:2110.09075v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09075">
<div class="article-summary-box-inner">
<span><p>Although deep-learning based video recognition models have achieved
remarkable success, they are vulnerable to adversarial examples that are
generated by adding human-imperceptible perturbations on clean video samples.
As indicated in recent studies, adversarial examples are transferable, which
makes it feasible for black-box attacks in real-world applications.
Nevertheless, most existing adversarial attack methods have poor
transferability when attacking other video models and transfer-based attacks on
video models are still unexplored. To this end, we propose to boost the
transferability of video adversarial examples for black-box attacks on video
recognition models. Through extensive analysis, we discover that different
video recognition models rely on different discriminative temporal patterns,
leading to the poor transferability of video adversarial examples. This
motivates us to introduce a temporal translation attack method, which optimizes
the adversarial perturbations over a set of temporal translated video clips. By
generating adversarial examples over translated videos, the resulting
adversarial examples are less sensitive to temporal patterns existed in the
white-box model being attacked and thus can be better transferred. Extensive
experiments on the Kinetics-400 dataset and the UCF-101 dataset demonstrate
that our method can significantly boost the transferability of video
adversarial examples. For transfer-based attack against video recognition
models, it achieves a 61.56% average attack success rate on the Kinetics-400
and 48.60% on the UCF-101. Code is available at
https://github.com/zhipeng-wei/TT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HM-Net: A Regression Network for Object Center Detection and Tracking on Wide Area Motion Imagery. (arXiv:2110.09881v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09881">
<div class="article-summary-box-inner">
<span><p>Wide Area Motion Imagery (WAMI) yields high-resolution images with a large
number of extremely small objects. Target objects have large spatial
displacements throughout consecutive frames. This nature of WAMI images makes
object tracking and detection challenging. In this paper, we present our deep
neural network-based combined object detection and tracking model, namely, Heat
Map Network (HM-Net). HM-Net is significantly faster than state-of-the-art
frame differencing and background subtraction-based methods, without
compromising detection and tracking performances. HM-Net follows the object
center-based joint detection and tracking paradigm. Simple heat map-based
predictions support an unlimited number of simultaneous detections. The
proposed method uses two consecutive frames and the object detection heat map
obtained from the previous frame as input, which helps HM-Net monitor
spatio-temporal changes between frames and keeps track of previously predicted
objects. Although reuse of prior object detection heat map acts as a vital
feedback-based memory element, it can lead to an unintended surge of
false-positive detections. To increase the robustness of the method against
false positives and to eliminate low confidence detections, HM-Net employs
novel feedback filters and advanced data augmentations. HM-Net outperforms
state-of-the-art WAMI moving object detection and tracking methods on the WPAFB
dataset with its 96.2% F1 and 94.4% mAP detection scores while achieving a
61.8% mAP tracking score on the same dataset. This performance corresponds to
an improvement of 2.1% for F1, 6.1% for mAP scores on detection, and 9.5% for
mAP score on tracking over the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QU-net++: Image Quality Detection Framework for Segmentation of 3D Medical Image Stacks. (arXiv:2110.14181v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14181">
<div class="article-summary-box-inner">
<span><p>Automated segmentation of pathological regions of interest aids medical image
diagnostics and follow-up care. However, accurate pathological segmentations
require high quality of annotated data that can be both cost and time intensive
to generate. In this work, we propose an automated two-step method that detects
a minimal image subset required to train segmentation models by evaluating the
quality of medical images from 3D image stacks using a U-net++ model. These
images that represent a lack of quality training can then be annotated and used
to fully train a U-net-based segmentation model. The proposed QU-net++ model
detects lack of quality training based on the disagreement in segmentations
produced from the final two output layers. The proposed model isolates around
10% of images per 3D stack and can scale across imaging modalities to segment
cysts in OCT images and ground glass opacity in Lung CT images with Dice scores
in the range 0.56-0.72. Thus, the proposed method can be applied for cost
effective multi-modal pathology segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Text Tracking With a Spatio-Temporal Complementary Model. (arXiv:2111.04987v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04987">
<div class="article-summary-box-inner">
<span><p>Text tracking is to track multiple texts in a video,and construct a
trajectory for each text. Existing methodstackle this task by utilizing the
tracking-by-detection frame-work, i.e., detecting the text instances in each
frame andassociating the corresponding text instances in consecutiveframes. We
argue that the tracking accuracy of this paradigmis severely limited in more
complex scenarios, e.g., owing tomotion blur, etc., the missed detection of
text instances causesthe break of the text trajectory. In addition, different
textinstances with similar appearance are easily confused, leadingto the
incorrect association of the text instances. To this end,a novel
spatio-temporal complementary text tracking model isproposed in this paper. We
leverage a Siamese ComplementaryModule to fully exploit the continuity
characteristic of the textinstances in the temporal dimension, which
effectively alleviatesthe missed detection of the text instances, and hence
ensuresthe completeness of each text trajectory. We further integratethe
semantic cues and the visual cues of the text instance intoa unified
representation via a text similarity learning network,which supplies a high
discriminative power in the presence oftext instances with similar appearance,
and thus avoids the mis-association between them. Our method achieves
state-of-the-art performance on several public benchmarks. The source codeis
available at https://github.com/lsabrinax/VideoTextSCM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion. (arXiv:2111.10332v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10332">
<div class="article-summary-box-inner">
<span><p>Point cloud processing is a challenging task due to its sparsity and
irregularity. Prior works introduce delicate designs on either local feature
aggregator or global geometric architecture, but few combine both advantages.
We propose Dual-Scale Point Cloud Recognition with High-frequency Fusion
(DSPoint) to extract local-global features by concurrently operating on voxels
and points. We reverse the conventional design of applying convolution on
voxels and attention to points. Specifically, we disentangle point features
through channel dimension for dual-scale processing: one by point-wise
convolution for fine-grained geometry parsing, the other by voxel-wise global
attention for long-range structural exploration. We design a co-attention
fusion module for feature alignment to blend local-global modalities, which
conducts inter-scale cross-modality interaction by communicating high-frequency
coordinates information. Experiments and ablations on widely-adopted
ModelNet40, ShapeNet, and S3DIS demonstrate the state-of-the-art performance of
our DSPoint.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransMorph: Transformer for unsupervised medical image registration. (arXiv:2111.10480v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10480">
<div class="article-summary-box-inner">
<span><p>In the last decade, convolutional neural networks (ConvNets) have been a
major focus of research in medical image analysis. However, the performances of
ConvNets may be limited by a lack of explicit consideration of the long-range
spatial relationships in an image. Recently Vision Transformer architectures
have been proposed to address the shortcomings of ConvNets and have produced
state-of-the-art performances in many medical imaging applications.
Transformers may be a strong candidate for image registration because their
unlimited receptive field enables a more precise comprehension of the spatial
correspondence between moving and fixed images. Here, we present TransMorph, a
hybrid Transformer-ConvNet model for volumetric medical image registration.
This paper also presents diffeomorphic and Bayesian variants of TransMorph: the
diffeomorphic variants ensure the topology-preserving deformations, and the
Bayesian variant produces a well-calibrated registration uncertainty estimate.
We extensively validated the proposed models using 3D medical images from three
applications: inter-patient and atlas-to-patient brain MRI registration and
phantom-to-CT registration. The proposed models are evaluated in comparison to
a variety of existing registration methods and Transformer architectures.
Qualitative and quantitative results demonstrate that the proposed
Transformer-based model leads to a substantial performance improvement over the
baseline methods, confirming the effectiveness of Transformers for medical
image registration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Path Guiding Using Spatio-Directional Mixture Models. (arXiv:2111.13094v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13094">
<div class="article-summary-box-inner">
<span><p>We propose a learning-based method for light-path construction in path
tracing algorithms, which iteratively optimizes and samples from what we refer
to as spatio-directional Gaussian mixture models (SDMMs). In particular, we
approximate incident radiance as an online-trained $5$D mixture that is
accelerated by a $k$D-tree. Using the same framework, we approximate BSDFs as
pre-trained $n$D mixtures, where $n$ is the number of BSDF parameters. Such an
approach addresses two major challenges in path-guiding models. First, the $5$D
radiance representation naturally captures correlation between the spatial and
directional dimensions. Such correlations are present in e.g. parallax and
caustics. Second, by using a tangent-space parameterization of Gaussians, our
spatio-directional mixtures can perform approximate product sampling with
arbitrarily oriented BSDFs. Existing models are only able to do this by either
foregoing anisotropy of the mixture components or by representing the radiance
field in local (normal aligned) coordinates, which both make the radiance field
more difficult to learn. An additional benefit of the tangent-space
parameterization is that each individual Gaussian is mapped to the solid sphere
with low distortion near its center of mass. Our method performs especially
well on scenes with small, localized luminaires that induce high
spatio-directional correlation in the incident radiance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExCon: Explanation-driven Supervised Contrastive Learning for Image Classification. (arXiv:2111.14271v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14271">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has led to substantial improvements in the quality of
learned embedding representations for tasks such as image classification.
However, a key drawback of existing contrastive augmentation methods is that
they may lead to the modification of the image content which can yield
undesired alterations of its semantics. This can affect the performance of the
model on downstream tasks. Hence, in this paper, we ask whether we can augment
image data in contrastive learning such that the task-relevant semantic content
of an image is preserved. For this purpose, we propose to leverage
saliency-based explanation methods to create content-preserving masked
augmentations for contrastive learning. Our novel explanation-driven supervised
contrastive learning (ExCon) methodology critically serves the dual goals of
encouraging nearby image embeddings to have similar content and explanation. To
quantify the impact of ExCon, we conduct experiments on the CIFAR-100 and the
Tiny ImageNet datasets. We demonstrate that ExCon outperforms vanilla
supervised contrastive learning in terms of classification, explanation
quality, adversarial robustness as well as calibration of probabilistic
predictions of the model in the context of distributional shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3rd Place: A Global and Local Dual Retrieval Solution to Facebook AI Image Similarity Challenge. (arXiv:2112.02373v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02373">
<div class="article-summary-box-inner">
<span><p>As a basic task of computer vision, image similarity retrieval is facing the
challenge of large-scale data and image copy attacks. This paper presents our
3rd place solution to the matching track of Image Similarity Challenge (ISC)
2021 organized by Facebook AI. We propose a multi-branch retrieval method of
combining global descriptors and local descriptors to cover all attack cases.
Specifically, we attempt many strategies to optimize global descriptors,
including abundant data augmentations, self-supervised learning with a single
Transformer model, overlay detection preprocessing. Moreover, we introduce the
robust SIFT feature and GPU Faiss for local retrieval which makes up for the
shortcomings of the global retrieval. Finally, KNN-matching algorithm is used
to judge the match and merge scores. We show some ablation experiments of our
method, which reveals the complementary advantages of global and local
features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deblurring via Stochastic Refinement. (arXiv:2112.02475v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02475">
<div class="article-summary-box-inner">
<span><p>Image deblurring is an ill-posed problem with multiple plausible solutions
for a given input image. However, most existing methods produce a deterministic
estimate of the clean image and are trained to minimize pixel-level distortion.
These metrics are known to be poorly correlated with human perception, and
often lead to unrealistic reconstructions. We present an alternative framework
for blind deblurring based on conditional diffusion models. Unlike existing
techniques, we train a stochastic sampler that refines the output of a
deterministic predictor and is capable of producing a diverse set of plausible
reconstructions for a given input. This leads to a significant improvement in
perceptual quality over existing state-of-the-art methods across multiple
standard benchmarks. Our predict-and-refine approach also enables much more
efficient sampling compared to typical diffusion models. Combined with a
carefully tuned network architecture and inference procedure, our method is
competitive in terms of distortion metrics such as PSNR. These results show
clear benefits of our diffusion-based method for deblurring and challenge the
widely used strategy of producing a single, deterministic reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceFormer: Speech-Driven 3D Facial Animation with Transformers. (arXiv:2112.05329v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05329">
<div class="article-summary-box-inner">
<span><p>Speech-driven 3D facial animation is challenging due to the complex geometry
of human faces and the limited availability of 3D audio-visual data. Prior
works typically focus on learning phoneme-level features of short audio windows
with limited context, occasionally resulting in inaccurate lip movements. To
tackle this limitation, we propose a Transformer-based autoregressive model,
FaceFormer, which encodes the long-term audio context and autoregressively
predicts a sequence of animated 3D face meshes. To cope with the data scarcity
issue, we integrate the self-supervised pre-trained speech representations.
Also, we devise two biased attention mechanisms well suited to this specific
task, including the biased cross-modal multi-head (MH) attention and the biased
causal MH self-attention with a periodic positional encoding strategy. The
former effectively aligns the audio-motion modalities, whereas the latter
offers abilities to generalize to longer audio sequences. Extensive experiments
and a perceptual user study show that our approach outperforms the existing
state-of-the-arts. We encourage watching the video. The code will be made
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Paced Deep Regression Forests with Consideration on Ranking Fairness. (arXiv:2112.06455v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06455">
<div class="article-summary-box-inner">
<span><p>Deep discriminative models (DDMs), such as deep regression forests, deep
neural decision forests, have been extensively studied recently to solve
problems like facial age estimation, head pose estimation, gaze estimation and
so forth. Such problems are challenging in part because a large amount of
effective training data without noise and bias is often not available. While
some progress has been achieved through learning more discriminative features,
or reweighting samples, we argue what is more desirable is to learn gradually
to discriminate like human beings. Then, we resort to self-paced learning
(SPL). But a natural question arises: can self-paced regime lead DDMs to
achieve more robust and less biased solutions? A serious problem with SPL,
which is firstly discussed by this work, is it tends to aggravate the bias of
solutions, especially for obvious imbalanced data. To this end, this paper
proposes a new self-paced paradigm for deep discriminative model, which
distinguishes noisy and underrepresented examples according to the output
likelihood and entropy associated with each example, and tackle the fundamental
ranking problem in SPL from a new perspective: fairness. This paradigm is
fundamental, and could be easily combined with a variety of DDMs. Extensive
experiments on three computer vision tasks, such as facial age estimation, head
pose estimation and gaze estimation, demonstrate the efficacy of our paradigm.
To the best of our knowledge, our work is the first paper in the literature of
SPL that considers ranking fairness for self-paced regime construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VirtualCube: An Immersive 3D Video Communication System. (arXiv:2112.06730v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06730">
<div class="article-summary-box-inner">
<span><p>The VirtualCube system is a 3D video conference system that attempts to
overcome some limitations of conventional technologies. The key ingredient is
VirtualCube, an abstract representation of a real-world cubicle instrumented
with RGBD cameras for capturing the 3D geometry and texture of a user. We
design VirtualCube so that the task of data capturing is standardized and
significantly simplified, and everything can be built using off-the-shelf
hardware. We use VirtualCubes as the basic building blocks of a virtual
conferencing environment, and we provide each VirtualCube user with a
surrounding display showing life-size videos of remote participants. To achieve
real-time rendering of remote participants, we develop the V-Cube View
algorithm, which uses multi-view stereo for more accurate depth estimation and
Lumi-Net rendering for better rendering quality. The VirtualCube system
correctly preserves the mutual eye gaze between participants, allowing them to
establish eye contact and be aware of who is visually paying attention to them.
The system also allows a participant to have side discussions with remote
participants as if they were in the same room. Finally, the system sheds lights
on how to support the shared space of work items (e.g., documents and
applications) and track the visual attention of participants to work items.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Depth Completion with Uncertainty-Driven Loss Functions. (arXiv:2112.07895v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07895">
<div class="article-summary-box-inner">
<span><p>Recovering a dense depth image from sparse LiDAR scans is a challenging task.
Despite the popularity of color-guided methods for sparse-to-dense depth
completion, they treated pixels equally during optimization, ignoring the
uneven distribution characteristics in the sparse depth map and the accumulated
outliers in the synthesized ground truth. In this work, we introduce
uncertainty-driven loss functions to improve the robustness of depth completion
and handle the uncertainty in depth completion. Specifically, we propose an
explicit uncertainty formulation for robust depth completion with Jeffrey's
prior. A parametric uncertain-driven loss is introduced and translated to new
loss functions that are robust to noisy or missing data. Meanwhile, we propose
a multiscale joint prediction model that can simultaneously predict depth and
uncertainty maps. The estimated uncertainty map is also used to perform
adaptive prediction on the pixels with high uncertainty, leading to a residual
map for refining the completion results. Our method has been tested on KITTI
Depth Completion Benchmark and achieved the state-of-the-art robustness
performance in terms of MAE, IMAE, and IRMSE metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Recognition as Classification via Visual Properties. (arXiv:2112.10531v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10531">
<div class="article-summary-box-inner">
<span><p>We base our work on the teleosemantic modelling of concepts as abilities
implementing the distinct functions of recognition and classification.
Accordingly, we model two types of concepts - substance concepts suited for
object recognition exploiting visual properties, and classification concepts
suited for classification of substance concepts exploiting linguistically
grounded properties. The goal in this paper is to demonstrate that object
recognition can be construed as classification via visual properties, as
distinct from work in mainstream computer vision. Towards that, we present an
object recognition process based on Ranganathan's four-phased faceted knowledge
organization process, grounded in the teleosemantic distinctions of substance
concept and classification concept. We also briefly introduce the ongoing
project MultiMedia UKC, whose aim is to build an object recognition resource
following our proposed process
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EyePAD++: A Distillation-based approach for joint Eye Authentication and Presentation Attack Detection using Periocular Images. (arXiv:2112.11610v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11610">
<div class="article-summary-box-inner">
<span><p>A practical eye authentication (EA) system targeted for edge devices needs to
perform authentication and be robust to presentation attacks, all while
remaining compute and latency efficient. However, existing eye-based frameworks
a) perform authentication and Presentation Attack Detection (PAD) independently
and b) involve significant pre-processing steps to extract the iris region.
Here, we introduce a joint framework for EA and PAD using periocular images.
While a deep Multitask Learning (MTL) network can perform both the tasks, MTL
suffers from the forgetting effect since the training datasets for EA and PAD
are disjoint. To overcome this, we propose Eye Authentication with PAD
(EyePAD), a distillation-based method that trains a single network for EA and
PAD while reducing the effect of forgetting. To further improve the EA
performance, we introduce a novel approach called EyePAD++ that includes
training an MTL network on both EA and PAD data, while distilling the
`versatility' of the EyePAD network through an additional distillation step.
Our proposed methods outperform the SOTA in PAD and obtain near-SOTA
performance in eye-to-eye verification, without any pre-processing. We also
demonstrate the efficacy of EyePAD and EyePAD++ in user-to-user verification
with PAD across network backbones and image quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning for brain metastasis detection and segmentation in longitudinal MRI data. (arXiv:2112.11833v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11833">
<div class="article-summary-box-inner">
<span><p>Brain metastases occur frequently in patients with metastatic cancer. Early
and accurate detection of brain metastases is very essential for treatment
planning and prognosis in radiation therapy. To improve brain metastasis
detection performance with deep learning, a custom detection loss called
volume-level sensitivity-specificity (VSS) is proposed, which rates individual
metastasis detection sensitivity and specificity in (sub-)volume levels. As
sensitivity and precision are always a trade-off in a metastasis level, either
a high sensitivity or a high precision can be achieved by adjusting the weights
in the VSS loss without decline in dice score coefficient for segmented
metastases. To reduce metastasis-like structures being detected as false
positive metastases, a temporal prior volume is proposed as an additional input
of the neural network. Our proposed VSS loss improves the sensitivity of brain
metastasis detection, increasing the sensitivity from 86.7% to 95.5%.
Alternatively, it improves the precision from 68.8% to 97.8%. With the
additional temporal prior volume, about 45% of the false positive metastases
are reduced in the high sensitivity model and the precision reaches 99.6% for
the high specificity model. The mean dice coefficient for all metastases is
about 0.81. With the ensemble of the high sensitivity and high specificity
models, on average only 1.5 false positive metastases per patient needs further
check, while the majority of true positive metastases are confirmed. The
ensemble learning is able to distinguish high confidence true positive
metastases from metastases candidates that require special expert review or
further follow-up, being particularly well-fit to the requirements of expert
support in real clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A formal approach to good practices in Pseudo-Labeling for Unsupervised Domain Adaptive Re-Identification. (arXiv:2112.12887v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12887">
<div class="article-summary-box-inner">
<span><p>The use of pseudo-labels prevails in order to tackle Unsupervised Domain
Adaptive (UDA) Re-Identification (re-ID) with the best performance. Indeed,
this family of approaches has given rise to several UDA re-ID specific
frameworks, which are effective. In these works, research directions to improve
Pseudo-Labeling UDA re-ID performance are varied and mostly based on intuition
and experiments: refining pseudo-labels, reducing the impact of errors in
pseudo-labels... It can be hard to deduce from them general good practices,
which can be implemented in any Pseudo-Labeling method, to consistently improve
its performance. To address this key question, a new theoretical view on
Pseudo-Labeling UDA re-ID is proposed. The contributions are threefold: (i) A
novel theoretical framework for Pseudo-Labeling UDA re-ID, formalized through a
new general learning upper-bound on the UDA re-ID performance. (ii) General
good practices for Pseudo-Labeling, directly deduced from the interpretation of
the proposed theoretical framework, in order to improve the target re-ID
performance. (iii) Extensive experiments on challenging person and vehicle
cross-dataset re-ID tasks, showing consistent performance improvements for
various state-of-the-art methods and various proposed implementations of good
practices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViR:the Vision Reservoir. (arXiv:2112.13545v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13545">
<div class="article-summary-box-inner">
<span><p>The most recent year has witnessed the success of applying the Vision
Transformer (ViT) for image classification. However, there are still evidences
indicating that ViT often suffers following two aspects, i) the high
computation and the memory burden from applying the multiple Transformer layers
for pre-training on a large-scale dataset, ii) the over-fitting when training
on small datasets from scratch. To address these problems, a novel method,
namely, Vision Reservoir computing (ViR), is proposed here for image
classification, as a parallel to ViT. By splitting each image into a sequence
of tokens with fixed length, the ViR constructs a pure reservoir with a nearly
fully connected topology to replace the Transformer module in ViT. Two kinds of
deep ViR models are subsequently proposed to enhance the network performance.
Comparative experiments between the ViR and the ViT are carried out on several
image classification benchmarks. Without any pre-training process, the ViR
outperforms the ViT in terms of both model and computational complexity.
Specifically, the number of parameters of the ViR is about 15% even 5% of the
ViT, and the memory footprint is about 20% to 40% of the ViT. The superiority
of the ViR performance is explained by Small-World characteristics, Lyapunov
exponents, and memory capacity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Robust and Lightweight Model through Separable Structured Transformations. (arXiv:2112.13551v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13551">
<div class="article-summary-box-inner">
<span><p>With the proliferation of mobile devices and the Internet of Things, deep
learning models are increasingly deployed on devices with limited computing
resources and memory, and are exposed to the threat of adversarial noise.
Learning deep models with both lightweight and robustness is necessary for
these equipments. However, current deep learning solutions are difficult to
learn a model that possesses these two properties without degrading one or the
other. As is well known, the fully-connected layers contribute most of the
parameters of convolutional neural networks. We perform a separable structural
transformation of the fully-connected layer to reduce the parameters, where the
large-scale weight matrix of the fully-connected layer is decoupled by the
tensor product of several separable small-sized matrices. Note that data, such
as images, no longer need to be flattened before being fed to the
fully-connected layer, retaining the valuable spatial geometric information of
the data. Moreover, in order to further enhance both lightweight and
robustness, we propose a joint constraint of sparsity and differentiable
condition number, which is imposed on these separable matrices. We evaluate the
proposed approach on MLP, VGG-16 and Vision Transformer. The experimental
results on datasets such as ImageNet, SVHN, CIFAR-100 and CIFAR10 show that we
successfully reduce the amount of network parameters by 90%, while the robust
accuracy loss is less than 1.5%, which is better than the SOTA methods based on
the original fully-connected layer. Interestingly, it can achieve an
overwhelming advantage even at a high compression rate, e.g., 200 times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Domain Balanced Sampling Improves Out-of-Distribution Generalization of Chest X-ray Pathology Prediction Models. (arXiv:2112.13734v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13734">
<div class="article-summary-box-inner">
<span><p>Learning models that generalize under different distribution shifts in
medical imaging has been a long-standing research challenge. There have been
several proposals for efficient and robust visual representation learning among
vision research practitioners, especially in the sensitive and critical
biomedical domain. In this paper, we propose an idea for out-of-distribution
generalization of chest X-ray pathologies that uses a simple balanced batch
sampling technique. We observed that balanced sampling between the multiple
training datasets improves the performance over baseline models trained without
balancing.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-12-30 23:07:30.361342113 UTC">2021-12-30 23:07:30 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>