<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-28T01:30:00Z">03-28</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for fingerspelled content in American Sign Language. (arXiv:2203.13291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13291">
<div class="article-summary-box-inner">
<span><p>Natural language processing for sign language video - including tasks like
recognition, translation, and search - is crucial for making artificial
intelligence technologies accessible to deaf individuals, and is gaining
research interest in recent years. In this paper, we address the problem of
searching for fingerspelled key-words or key phrases in raw sign language
videos. This is an important task since significant content in sign language is
often conveyed via fingerspelling, and to our knowledge the task has not been
studied before. We propose an end-to-end model for this task, FSS-Net, that
jointly detects fingerspelling and matches it to a text sequence. Our
experiments, done on a large public dataset of ASL fingerspelling in the wild,
show the importance of fingerspelling detection as a component of a search and
retrieval model. Our model significantly outperforms baseline methods adapted
from prior work on related tasks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mix and Match: Learning-free Controllable Text Generation using Energy Language Models. (arXiv:2203.13299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13299">
<div class="article-summary-box-inner">
<span><p>Recent work on controlled text generation has either required attribute-based
fine-tuning of the base language model (LM), or has restricted the
parameterization of the attribute discriminator to be compatible with the base
autoregressive LM. In this work, we propose Mix and Match LM, a global
score-based alternative for controllable text generation that combines
arbitrary pre-trained black-box models for achieving the desired attributes in
the generated text without involving any fine-tuning or structural assumptions
about the black-box models. We interpret the task of controllable generation as
drawing samples from an energy-based model whose energy values are a linear
combination of scores from black-box models that are separately responsible for
fluency, the control attribute, and faithfulness to any conditioning context.
We use a Metropolis-Hastings sampling scheme to sample from this energy-based
model using bidirectional context and global attribute features. We validate
the effectiveness of our approach on various controlled generation and
style-based text revision tasks by outperforming recently proposed methods that
involve extra training, fine-tuning, or restrictive assumptions over the form
of models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation. (arXiv:2203.13339v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13339">
<div class="article-summary-box-inner">
<span><p>End-to-end speech-to-speech translation (S2ST) without relying on
intermediate text representations is a rapidly emerging frontier of research.
Recent works have demonstrated that the performance of such direct S2ST systems
is approaching that of conventional cascade S2ST when trained on comparable
datasets. However, in practice, the performance of direct S2ST is bounded by
the availability of paired S2ST training data. In this work, we explore
multiple approaches for leveraging much more widely available unsupervised and
weakly-supervised speech and text data to improve the performance of direct
S2ST based on Translatotron 2. With our most effective approaches, the average
translation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is
improved by +13.6 BLEU (or +113% relatively), as compared to the previous
state-of-the-art trained without additional data. The improvements on
low-resource language are even more significant (+398% relatively on average).
Our comparative studies suggest future research directions for S2ST and speech
representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linking Emergent and Natural Languages via Corpus Transfer. (arXiv:2203.13344v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13344">
<div class="article-summary-box-inner">
<span><p>The study of language emergence aims to understand how human languages are
shaped by perceptual grounding and communicative intent. Computational
approaches to emergent communication (EC) predominantly consider referential
games in limited domains and analyze the learned protocol within the game
framework. As a result, it remains unclear how the emergent languages from
these settings connect to natural languages or provide benefits in real-world
language processing tasks, where statistical models trained on large text
corpora dominate. In this work, we propose a novel way to establish such a link
by corpus transfer, i.e. pretraining on a corpus of emergent language for
downstream natural language tasks, which is in contrast to prior work that
directly transfers speaker and listener parameters. Our approach showcases
non-trivial transfer benefits for two different tasks -- language modeling and
image captioning. For example, in a low-resource setup (modeling 2 million
natural language tokens), pre-training on an emergent language corpus with just
2 million tokens reduces model perplexity by $24.6\%$ on average across ten
natural languages. We also introduce a novel metric to predict the
transferability of an emergent language by translating emergent messages to
natural language captions grounded on the same images. We find that our
translation-based metric highly correlates with the downstream performance on
modeling natural languages (for instance $\rho=0.83$ on Hebrew), while
topographic similarity, a popular metric in previous work, shows surprisingly
low correlation ($\rho=0.003$), hinting that simple properties like attribute
disentanglement from synthetic domains might not capture the full complexities
of natural language. Our findings also indicate potential benefits of moving
language emergence forward with natural language resources and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does human speech follow Benford's Law?. (arXiv:2203.13352v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13352">
<div class="article-summary-box-inner">
<span><p>Researchers have observed that the frequencies of leading digits in many
man-made and naturally occurring datasets follow a logarithmic curve, with
digits that start with the number 1 accounting for $\sim 30\%$ of all numbers
in the dataset and digits that start with the number 9 accounting for $\sim
5\%$ of all numbers in the dataset. This phenomenon, known as Benford's Law, is
highly repeatable and appears in lists of numbers from electricity bills, stock
prices, tax returns, house prices, death rates, lengths of rivers, and
naturally occurring images. In this paper we demonstrate that human speech
spectra also follow Benford's Law. We use this observation to motivate a new
set of features that can be efficiently extracted from speech and demonstrate
that these features can be used to classify between human speech and synthetic
speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia. (arXiv:2203.13357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13357">
<div class="article-summary-box-inner">
<span><p>NLP research is impeded by a lack of resources and awareness of the
challenges presented by underrepresented languages and dialects. Focusing on
the languages spoken in Indonesia, the second most linguistically diverse and
the fourth most populous nation of the world, we provide an overview of the
current state of NLP research for Indonesia's 700+ languages. We highlight
challenges in Indonesian NLP and how these affect the performance of current
NLP systems. Finally, we provide general recommendations to help develop NLP
technology not only for languages of Indonesia but also other underrepresented
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13366">
<div class="article-summary-box-inner">
<span><p>For a long period, different recommendation tasks typically require designing
task-specific architectures and training objectives. As a result, it is hard to
transfer the learned knowledge and representations from one task to another,
thus restricting the generalization ability of existing recommendation
approaches, e.g., a sequential recommendation model can hardly be applied or
transferred to a review generation method. To deal with such issues,
considering that language grounding is a powerful medium to describe and
represent various problems or tasks, we present a flexible and unified
text-to-text paradigm called "Pretrain, Personalized Prompt, and Predict
Paradigm" (P5) for recommendation, which unifies various recommendation tasks
in a shared framework. In P5, all data such as user-item interactions, item
metadata, and user reviews are converted to a common format -- natural language
sequences. The rich information from natural language assist P5 to capture
deeper semantics for recommendation. P5 learns different tasks with the same
language modeling objective during pretraining. Thus, it possesses the
potential to serve as the foundation model for downstream recommendation tasks,
allows easy integration with other modalities, and enables instruction-based
recommendation, which will revolutionize the technical form of recommender
system towards unified recommendation engine. With adaptive personalized prompt
for different users, P5 is able to make predictions in a zero-shot or few-shot
manner and largely reduces the necessity for extensive fine-tuning. On several
recommendation benchmarks, we conduct experiments to show the effectiveness of
our generative approach. We will release our prompts and pretrained P5 language
model to help advance future research on Recommendation as Language Processing
(RLP) and Personalized Foundation Models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender and Racial Stereotype Detection in Legal Opinion Word Embeddings. (arXiv:2203.13369v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13369">
<div class="article-summary-box-inner">
<span><p>Studies have shown that some Natural Language Processing (NLP) systems encode
and replicate harmful biases with potential adverse ethical effects in our
society. In this article, we propose an approach for identifying gender and
racial stereotypes in word embeddings trained on judicial opinions from U.S.
case law. Embeddings containing stereotype information may cause harm when used
by downstream systems for classification, information extraction, question
answering, or other machine learning systems used to build legal research
tools. We first explain how previously proposed methods for identifying these
biases are not well suited for use with word embeddings trained on legal
opinion text. We then propose a domain adapted method for identifying gender
and racial biases in the legal domain. Our analyses using these methods suggest
that racial and gender biases are encoded into word embeddings trained on legal
opinions. These biases are not mitigated by exclusion of historical data, and
appear across multiple large topical areas of the law. Implications for
downstream systems that use legal opinion word embeddings and suggestions for
potential mitigation strategies based on our observations are also discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models. (arXiv:2203.13397v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13397">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) techniques involving fine-tuning large numbers of model
parameters have delivered impressive performance on the task of discriminating
between language produced by cognitively healthy individuals, and those with
Alzheimer's disease (AD). However, questions remain about their ability to
generalize beyond the small reference sets that are publicly available for
research. As an alternative to fitting model parameters directly, we propose a
novel method by which a Transformer DL model (GPT-2) pre-trained on general
English text is paired with an artificially degraded version of itself (GPT-D),
to compute the ratio between these two models' \textit{perplexities} on
language from cognitively healthy and impaired individuals. This technique
approaches state-of-the-art performance on text data from a widely used "Cookie
Theft" picture description task, and unlike established alternatives also
generalizes well to spontaneous conversations. Furthermore, GPT-D generates
text with characteristics known to be associated with AD, demonstrating the
induction of dementia-related linguistic anomalies. Our study is a step toward
better understanding of the relationships between the inner workings of
generative neural language models, the language that they produce, and the
deleterious effects of dementia on human speech and language characteristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Song Translation for Tonal Languages. (arXiv:2203.13420v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13420">
<div class="article-summary-box-inner">
<span><p>This paper develops automatic song translation (AST) for tonal languages and
addresses the unique challenge of aligning words' tones with melody of a song
in addition to conveying the original meaning. We propose three criteria for
effective AST -- preserving meaning, singability and intelligibility -- and
design metrics for these criteria. We develop a new benchmark for
English--Mandarin song translation and develop an unsupervised AST system,
Guided AliGnment for Automatic Song Translation (GagaST), which combines
pre-training with three decoding constraints. Both automatic and human
evaluations show GagaST successfully balances semantics and singability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plagiarism Detection in the Bengali Language: A Text Similarity-Based Approach. (arXiv:2203.13430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13430">
<div class="article-summary-box-inner">
<span><p>Plagiarism means taking another person's work and not giving any credit to
them for it. Plagiarism is one of the most serious problems in academia and
among researchers. Even though there are multiple tools available to detect
plagiarism in a document but most of them are domain-specific and designed to
work in English texts, but plagiarism is not limited to a single language only.
Bengali is the most widely spoken language of Bangladesh and the second most
spoken language in India with 300 million native speakers and 37 million
second-language speakers. Plagiarism detection requires a large corpus for
comparison. Bengali Literature has a history of 1300 years. Hence most Bengali
Literature books are not yet digitalized properly. As there was no such corpus
present for our purpose so we have collected Bengali Literature books from the
National Digital Library of India and with a comprehensive methodology
extracted texts from it and constructed our corpus. Our experimental results
find out average accuracy between 72.10 % - 79.89 % in text extraction using
OCR. Levenshtein Distance algorithm is used for determining Plagiarism. We have
built a web application for end-user and successfully tested it for Plagiarism
detection in Bengali texts. In future, we aim to construct a corpus with more
books for more accurate detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Conversational Paradigm for Program Synthesis. (arXiv:2203.13474v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13474">
<div class="article-summary-box-inner">
<span><p>Program synthesis strives to generate a computer program as a solution to a
given problem specification. We propose a conversational program synthesis
approach via large language models, which addresses the challenges of searching
over a vast program space and user intent specification faced in prior
approaches. Our new approach casts the process of writing a specification and
program as a multi-turn conversation between a user and a system. It treats
program synthesis as a sequence prediction problem, in which the specification
is expressed in natural language and the desired program is conditionally
sampled. We train a family of large language models, called CodeGen, on natural
language and programming language data. With weak supervision in the data and
the scaling up of data size and model size, conversational capacities emerge
from the simple autoregressive language modeling. To study the model behavior
on conversational program synthesis, we develop a multi-turn programming
benchmark (MTPB), where solving each problem requires multi-step synthesis via
multi-turn conversation between the user and the model. Our findings show the
emergence of conversational capabilities and the effectiveness of the proposed
conversational program synthesis paradigm. In addition, our model CodeGen (with
up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the
HumanEval benchmark. We plan to make the training library JaxFormer including
checkpoints available as open source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Striking a Balance: Alleviating Inconsistency in Pre-trained Models for Symmetric Classification Tasks. (arXiv:2203.13491v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13491">
<div class="article-summary-box-inner">
<span><p>While fine-tuning pre-trained models for downstream classification is the
conventional paradigm in NLP, often task-specific nuances may not get captured
in the resultant models. Specifically, for tasks that take two inputs and
require the output to be invariant of the order of the inputs, inconsistency is
often observed in the predicted labels or confidence scores. We highlight this
model shortcoming and apply a consistency loss function to alleviate
inconsistency in symmetric classification. Our results show an improved
consistency in predictions for three paraphrase detection datasets without a
significant drop in the accuracy scores. We examine the classification
performance of six datasets (both symmetric and non-symmetric) to showcase the
strengths and limitations of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition. (arXiv:2203.13504v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13504">
<div class="article-summary-box-inner">
<span><p>Emotion recognition in conversation (ERC) aims to analyze the speaker's state
and identify their emotion in the conversation. Recent works in ERC focus on
context modeling but ignore the representation of contextual emotional
tendency. In order to extract multi-modal information and the emotional
tendency of the utterance effectively, we propose a new structure named
Emoformer to extract multi-modal emotion vectors from different modalities and
fuse them with sentence vector to be an emotion capsule. Furthermore, we design
an end-to-end ERC model called EmoCaps, which extracts emotion vectors through
the Emoformer structure and obtain the emotion classification results from a
context analysis model. Through the experiments with two benchmark datasets,
our model shows better performance than the existing state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation. (arXiv:2203.13528v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13528">
<div class="article-summary-box-inner">
<span><p>Subword regularizations use multiple subword segmentations during training to
improve the robustness of neural machine translation models. In previous
subword regularizations, we use multiple segmentations in the training process
but use only one segmentation in the inference. In this study, we propose an
inference strategy to address this discrepancy. The proposed strategy
approximates the marginalized likelihood by using multiple segmentations
including the most plausible segmentation and several sampled segmentations.
Because the proposed strategy aggregates predictions from several
segmentations, we can regard it as a single model ensemble that does not
require any additional cost for training. Experimental results show that the
proposed strategy improves the performance of models trained with subword
regularization in low-resource machine translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Target-Side Morphology in Neural Machine Translation: A Comparison of Strategies. (arXiv:2203.13550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13550">
<div class="article-summary-box-inner">
<span><p>Morphologically rich languages pose difficulties to machine translation.
Machine translation engines that rely on statistical learning from parallel
training data, such as state-of-the-art neural systems, face challenges
especially with rich morphology on the output language side. Key challenges of
rich target-side morphology in data-driven machine translation include: (1) A
large amount of differently inflected word surface forms entails a larger
vocabulary and thus data sparsity. (2) Some inflected forms of infrequent terms
typically do not appear in the training corpus, which makes closed-vocabulary
systems unable to generate these unobserved variants. (3) Linguistic agreement
requires the system to correctly match the grammatical categories between
inflected word forms in the output sentence, both in terms of target-side
morpho-syntactic wellformedness and semantic adequacy with respect to the
input.
</p>
<p>In this paper, we re-investigate two target-side linguistic processing
techniques: a lemma-tag strategy and a linguistically informed word
segmentation strategy. Our experiments are conducted on a English-German
translation task under three training corpus conditions of different
magnitudes. We find that a stronger Transformer baseline leaves less room for
improvement than a shallow-RNN encoder-decoder model when translating
in-domain. However, we find that linguistic modeling of target-side morphology
does benefit the Transformer model when the same system is applied to
out-of-domain input text. We also successfully apply our approach to English to
Czech translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation. (arXiv:2203.13560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13560">
<div class="article-summary-box-inner">
<span><p>Applying existing methods to emotional support conversation -- which provides
valuable assistance to people who are in need -- has two major limitations: (a)
they generally employ a conversation-level emotion label, which is too
coarse-grained to capture user's instant mental state; (b) most of them focus
on expressing empathy in the response(s) rather than gradually reducing user's
distress. To address the problems, we propose a novel model \textbf{MISC},
which firstly infers the user's fine-grained emotional status, and then
responds skillfully using a mixture of strategy. Experimental results on the
benchmark dataset demonstrate the effectiveness of our method and reveal the
benefits of fine-grained emotion understanding as well as mixed-up strategy
modeling. Our code and data could be found in
\url{https://github.com/morecry/MISC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Dataset on Acoustic Models for Automatic Speech Recognition. (arXiv:2203.13590v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13590">
<div class="article-summary-box-inner">
<span><p>In Automatic Speech Recognition, GMM-HMM had been widely used for acoustic
modelling. With the current advancement of deep learning, the Gaussian Mixture
Model (GMM) from acoustic models has been replaced with Deep Neural Network,
namely DNN-HMM Acoustic Models. The GMM models are widely used to create the
alignments of the training data for the hybrid deep neural network model, thus
making it an important task to create accurate alignments. Many factors such as
training dataset size, training data augmentation, model hyperparameters, etc.,
affect the model learning. Traditionally in machine learning, larger datasets
tend to have better performance, while smaller datasets tend to trigger
over-fitting. The collection of speech data and their accurate transcriptions
is a significant challenge that varies over different languages, and in most
cases, it might be limited to big organizations. Moreover, in the case of
available large datasets, training a model using such data requires additional
time and computing resources, which may not be available. While the data about
the accuracy of state-of-the-art ASR models on open-source datasets are
published, the study about the impact of the size of a dataset on acoustic
models is not readily available. This work aims to investigate the impact of
dataset size variations on the performance of various GMM-HMM Acoustic Models
and their respective computational costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZS4IE: A toolkit for Zero-Shot Information Extraction with simple Verbalizations. (arXiv:2203.13602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13602">
<div class="article-summary-box-inner">
<span><p>The current workflow for Information Extraction (IE) analysts involves the
definition of the entities/relations of interest and a training corpus with
annotated examples. In this demonstration we introduce a new workflow where the
analyst directly verbalizes the entities/relations, which are then used by a
Textual Entailment model to perform zero-shot IE. We present the design and
implementation of a toolkit with a user interface, as well as experiments on
four IE tasks that show that the system achieves very good performance at
zero-shot learning using only 5--15 minutes per type of a user's effort. Our
demonstration system is open-sourced at https://github.com/BBN-E/ZS4IE . A
demonstration video is available at https://vimeo.<a href="/abs/com/6761383">com/6761383</a>40 .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Formality Style Transfer with Consistency Training. (arXiv:2203.13620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13620">
<div class="article-summary-box-inner">
<span><p>Formality style transfer (FST) is a task that involves paraphrasing an
informal sentence into a formal one without altering its meaning. To address
the data-scarcity problem of existing parallel datasets, previous studies tend
to adopt a cycle-reconstruction scheme to utilize additional unlabeled data,
where the FST model mainly benefits from target-side unlabeled sentences. In
this work, we propose a simple yet effective semi-supervised framework to
better utilize source-side unlabeled sentences based on consistency training.
Specifically, our approach augments pseudo-parallel data obtained from a
source-side informal sentence by enforcing the model to generate similar
outputs for its perturbed version. Moreover, we empirically examined the
effects of various data perturbation methods and propose effective data
filtering strategies to improve our framework. Experimental results on the
GYAFC benchmark demonstrate that our approach can achieve state-of-the-art
results, even with less than 40% of the parallel data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning. (arXiv:2203.13628v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13628">
<div class="article-summary-box-inner">
<span><p>Inspired by the recent progress in self-supervised learning for computer
vision, in this paper, through the DeLoRes learning framework, we introduce two
new general-purpose audio representation learning approaches, the DeLoRes-S and
DeLoRes-M. Our main objective is to make our network learn representations in a
resource-constrained setting (both data and compute), that can generalize well
across a diverse set of downstream tasks. Inspired from the Barlow Twins
objective function, we propose to learn embeddings that are invariant to
distortions of an input audio sample, while making sure that they contain
non-redundant information about the sample. To achieve this, we measure the
cross-correlation matrix between the outputs of two identical networks fed with
distorted versions of an audio segment sampled from an audio file and make it
as close to the identity matrix as possible. We call this the DeLoRes learning
framework, which we employ in different fashions with the DeLoRes-S and
DeLoRes-M. We use a combination of a small subset of the large-scale AudioSet
dataset and FSD50K for self-supervised learning and are able to learn with less
than half the parameters compared to state-of-the-art algorithms. For
evaluation, we transfer these learned representations to 11 downstream
classification tasks, including speech, music, and animal sounds, and achieve
state-of-the-art results on 7 out of 11 tasks on linear evaluation with
DeLoRes-M and show competitive results with DeLoRes-S, even when pre-trained
using only a fraction of the total data when compared to prior art. Our
transfer learning evaluation setup also shows extremely competitive results for
both DeLoRes-S and DeLoRes-M, with DeLoRes-M achieving state-of-the-art in 4
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-text Retrieval in Context. (arXiv:2203.13645v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13645">
<div class="article-summary-box-inner">
<span><p>Audio-text retrieval based on natural language descriptions is a challenging
task. It involves learning cross-modality alignments between long sequences
under inadequate data conditions. In this work, we investigate several audio
features as well as sequence aggregation methods for better audio-text
alignment. Moreover, through a qualitative analysis we observe that semantic
mapping is more important than temporal relations in contextual retrieval.
Using pre-trained audio features and a descriptor-based aggregation method, we
build our contextual audio-text retrieval system. Specifically, we utilize
PANNs features pre-trained on a large sound event dataset and NetRVLAD pooling,
which directly works with averaged descriptors. Experiments are conducted on
the AudioCaps and CLOTHO datasets, and results are compared with the previous
state-of-the-art system. With our proposed system, a significant improvement
has been achieved on bidirectional audio-text retrieval, on all metrics
including recall, median and mean rank.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Mediate Disparities Towards Pragmatic Communication. (arXiv:2203.13685v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13685">
<div class="article-summary-box-inner">
<span><p>Human communication is a collaborative process. Speakers, on top of conveying
their own intent, adjust the content and language expressions by taking the
listeners into account, including their knowledge background, personalities,
and physical capabilities. Towards building AI agents with similar abilities in
language communication, we propose Pragmatic Rational Speaker (PRS), a
framework extending Rational Speech Act (RSA). The PRS attempts to learn the
speaker-listener disparity and adjust the speech accordingly, by adding a
light-weighted disparity adjustment layer into working memory on top of
speaker's long-term memory system. By fixing the long-term memory, the PRS only
needs to update its working memory to learn and adapt to different types of
listeners. To validate our framework, we create a dataset that simulates
different types of speaker-listener disparities in the context of referential
games. Our empirical results demonstrate that the PRS is able to shift its
output towards the language that listener are able to understand, significantly
improve the collaborative task outcome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain-based Discriminative Autoencoders for Speech Recognition. (arXiv:2203.13687v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13687">
<div class="article-summary-box-inner">
<span><p>In our previous work, we proposed a discriminative autoencoder (DcAE) for
speech recognition. DcAE combines two training schemes into one. First, since
DcAE aims to learn encoder-decoder mappings, the squared error between the
reconstructed speech and the input speech is minimized. Second, in the code
layer, frame-based phonetic embeddings are obtained by minimizing the
categorical cross-entropy between ground truth labels and predicted
triphone-state scores. DcAE is developed based on the Kaldi toolkit by treating
various TDNN models as encoders. In this paper, we further propose three new
versions of DcAE. First, a new objective function that considers both
categorical cross-entropy and mutual information between ground truth and
predicted triphone-state sequences is used. The resulting DcAE is called a
chain-based DcAE (c-DcAE). For application to robust speech recognition, we
further extend c-DcAE to hierarchical and parallel structures, resulting in
hc-DcAE and pc-DcAE. In these two models, both the error between the
reconstructed noisy speech and the input noisy speech and the error between the
enhanced speech and the reference clean speech are taken into the objective
function. Experimental results on the WSJ and Aurora-4 corpora show that our
DcAE models outperform baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UKP-SQUARE: An Online Platform for Question Answering Research. (arXiv:2203.13693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13693">
<div class="article-summary-box-inner">
<span><p>Recent advances in NLP and information retrieval have given rise to a diverse
set of question answering tasks that are of different formats (e.g.,
extractive, abstractive), require different model architectures (e.g.,
generative, discriminative), and setups (e.g., with or without retrieval).
Despite having a large number of powerful, specialized QA pipelines (which we
refer to as Skills) that consider a single domain, model or setup, there exists
no framework where users can easily explore and compare such pipelines and can
extend them according to their needs. To address this issue, we present
UKP-SQUARE, an extensible online QA platform for researchers which allows users
to query and analyze a large collection of modern Skills via a user-friendly
web interface and integrated behavioural tests. In addition, QA researchers can
develop, manage, and share their custom Skills using our microservices that
support a wide range of models (Transformers, Adapters, ONNX), datastores and
retrieval techniques (e.g., sparse and dense). UKP-SQUARE is available on
https://square.ukp-lab.de.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech-enhanced and Noise-aware Networks for Robust Speech Recognition. (arXiv:2203.13696v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13696">
<div class="article-summary-box-inner">
<span><p>Compensation for channel mismatch and noise interference is essential for
robust automatic speech recognition. Enhanced speech has been introduced into
the multi-condition training of acoustic models to improve their generalization
ability. In this paper, a noise-aware training framework based on two cascaded
neural structures is proposed to jointly optimize speech enhancement and speech
recognition. The feature enhancement module is composed of a multi-task
autoencoder, where noisy speech is decomposed into clean speech and noise. By
concatenating its enhanced, noise-aware, and noisy features for each frame, the
acoustic-modeling module maps each feature-augmented frame into a triphone
state by optimizing the lattice-free maximum mutual information and cross
entropy between the predicted and actual state sequences. On top of the
factorized time delay neural network (TDNN-F) and its convolutional variant
(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error
rate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared
with the best existing systems that use bigram and trigram language models for
decoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction
of 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based
system also outperforms the baseline CNN-TDNNF system on the AMI task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Pre-Trained Language Models for Cross-Cultural Differences in Values. (arXiv:2203.13722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13722">
<div class="article-summary-box-inner">
<span><p>Language embeds information about social, cultural, and political values
people hold. Prior work has explored social and potentially harmful biases
encoded in Pre-Trained Language models (PTLMs). However, there has been no
systematic study investigating how values embedded in these models vary across
cultures. In this paper, we introduce probes to study which values across
cultures are embedded in these models, and whether they align with existing
theories and cross-cultural value surveys. We find that PTLMs capture
differences in values across cultures, but those only weakly align with
established value surveys. We discuss implications of using mis-aligned models
in cross-cultural settings, as well as ways of aligning PTLMs with value
surveys.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and BERT models. (arXiv:2203.13778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13778">
<div class="article-summary-box-inner">
<span><p>Social media platforms are used by a large number of people prominently to
express their thoughts and opinions. However, these platforms have contributed
to a substantial amount of hateful and abusive content as well. Therefore, it
is important to curb the spread of hate speech on these platforms. In India,
Marathi is one of the most popular languages used by a wide audience. In this
work, we present L3Cube-MahaHate, the first major Hate Speech Dataset in
Marathi. The dataset is curated from Twitter, annotated manually. Our dataset
consists of over 25000 distinct tweets labeled into four major classes i.e
hate, offensive, profane, and not. We present the approaches used for
collecting and annotating the data and the challenges faced during the process.
Finally, we present baseline classification results using deep learning models
based on CNN, LSTM, and Transformers. We explore mono-lingual and multi-lingual
variants of BERT like MahaBERT, IndicBERT, mBERT, and xlm-RoBERTa and show that
mono-lingual models perform better than their multi-lingual counterparts. The
MahaBERT model provides the best results on L3Cube-MahaHate Corpus. The data
and models are available at https://github.com/l3cube-pune/MarathiNLP .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallels of human language in the behavior of bottlenose dolphins. (arXiv:1605.01661v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1605.01661">
<div class="article-summary-box-inner">
<span><p>A short review of similarities between dolphins and humans with the help of
quantitative linguistics and information theory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Generation from Knowledge Graphs with Graph Transformers. (arXiv:1904.02342v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.02342">
<div class="article-summary-box-inner">
<span><p>Generating texts which express complex ideas spanning multiple sentences
requires a structured representation of their content (document plan), but
these representations are prohibitively expensive to manually produce. In this
work, we address the problem of generating coherent multi-sentence texts from
the output of an information extraction system, and in particular a knowledge
graph. Graphical knowledge representations are ubiquitous in computing, but
pose a significant challenge for text generation techniques due to their
non-hierarchical nature, collapsing of long-distance dependencies, and
structural variety. We introduce a novel graph transforming encoder which can
leverage the relational structure of such knowledge graphs without imposing
linearization or hierarchical constraints. Incorporated into an encoder-decoder
setup, we provide an end-to-end trainable system for graph-to-text generation
that we apply to the domain of scientific text. Automatic and human evaluations
show that our technique produces more informative texts which exhibit better
document structure than competitive encoder-decoder methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deduplicating Training Data Makes Language Models Better. (arXiv:2107.06499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06499">
<div class="article-summary-box-inner">
<span><p>We find that existing language modeling datasets contain many near-duplicate
examples and long repetitive substrings. As a result, over 1% of the unprompted
output of language models trained on these datasets is copied verbatim from the
training data. We develop two tools that allow us to deduplicate training
datasets -- for example removing from C4 a single 61 word English sentence that
is repeated over 60,000 times. Deduplication allows us to train models that
emit memorized text ten times less frequently and require fewer train steps to
achieve the same or better accuracy. We can also reduce train-test overlap,
which affects over 4% of the validation set of standard datasets, thus allowing
for more accurate evaluation. We release code for reproducing our work and
performing dataset deduplication at
https://github.com/google-research/deduplicate-text-datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\infty$-former: Infinite Memory Transformer. (arXiv:2109.00301v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00301">
<div class="article-summary-box-inner">
<span><p>Transformers are unable to model long-term memories effectively, since the
amount of computation they need to perform grows with the context length. While
variations of efficient transformers have been proposed, they all have a finite
memory capacity and are forced to drop old information. In this paper, we
propose the $\infty$-former, which extends the vanilla transformer with an
unbounded long-term memory. By making use of a continuous-space attention
mechanism to attend over the long-term memory, the $\infty$-former's attention
complexity becomes independent of the context length, trading off memory length
with precision. In order to control where precision is more important,
$\infty$-former maintains "sticky memories" being able to model arbitrarily
long contexts while keeping the computation budget fixed. Experiments on a
synthetic sorting task, language modeling, and document grounded dialogue
generation demonstrate the $\infty$-former's ability to retain information from
long sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models. (arXiv:2109.03892v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03892">
<div class="article-summary-box-inner">
<span><p>We investigate the use of multimodal information contained in images as an
effective method for enhancing the commonsense of Transformer models for text
generation. We perform experiments using BART and T5 on concept-to-text
generation, specifically the task of generative commonsense reasoning, or
CommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text
Generation. VisCTG involves captioning images representing appropriate everyday
scenarios, and using these captions to enrich and steer the generation process.
Comprehensive evaluation and analysis demonstrate that VisCTG noticeably
improves model performance while successfully addressing several issues of the
baseline generations, including poor commonsense, fluency, and specificity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Machine Translation Evaluation. (arXiv:2109.06352v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06352">
<div class="article-summary-box-inner">
<span><p>Several neural-based metrics have been recently proposed to evaluate machine
translation quality. However, all of them resort to point estimates, which
provide limited information at segment level. This is made worse as they are
trained on noisy, biased and scarce human judgements, often resulting in
unreliable quality predictions. In this paper, we introduce uncertainty-aware
MT evaluation and analyze the trustworthiness of the predicted quality. We
combine the COMET framework with two uncertainty estimation methods, Monte
Carlo dropout and deep ensembles, to obtain quality scores along with
confidence intervals. We compare the performance of our uncertainty-aware MT
evaluation methods across multiple language pairs from the QT21 dataset and the
WMT20 metrics task, augmented with MQM annotations. We experiment with varying
numbers of references and further discuss the usefulness of uncertainty-aware
quality estimation (without references) to flag possibly critical translation
mistakes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing as Quantifying Inductive Bias. (arXiv:2110.08388v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08388">
<div class="article-summary-box-inner">
<span><p>Pre-trained contextual representations have led to dramatic performance
improvements on a range of downstream tasks. Such performance improvements have
motivated researchers to quantify and understand the linguistic information
encoded in these representations. In general, researchers quantify the amount
of linguistic information through probing, an endeavor which consists of
training a supervised model to predict a linguistic property directly from the
contextual representations. Unfortunately, this definition of probing has been
subject to extensive criticism in the literature, and has been observed to lead
to paradoxical and counter-intuitive results. In the theoretical portion of
this paper, we take the position that the goal of probing ought to be measuring
the amount of inductive bias that the representations encode on a specific
task. We further describe a Bayesian framework that operationalizes this goal
and allows us to quantify the representations' inductive bias. In the empirical
portion of the paper, we apply our framework to a variety of NLP tasks. Our
results suggest that our proposed framework alleviates many previous problems
found in probing. Moreover, we are able to offer concrete evidence that -- for
some tasks -- fastText can offer a better inductive bias than BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimum Description Length Recurrent Neural Networks. (arXiv:2111.00600v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00600">
<div class="article-summary-box-inner">
<span><p>We train neural networks to optimize a Minimum Description Length score,
i.e., to balance between the complexity of the network and its accuracy at a
task. We show that networks optimizing this objective function master tasks
involving memory challenges and go beyond context-free languages. These
learners master languages such as $a^nb^n$, $a^nb^nc^n$, $a^nb^{2n}$,
$a^nb^mc^{n+m}$, and they perform addition. Moreover, they often do so with
100% accuracy. The networks are small, and their inner workings are
transparent. We thus provide formal proofs that their perfect accuracy holds
not only on a given test set, but for any input sequence. To our knowledge, no
other connectionist model has been shown to capture the underlying grammars for
these languages in full generality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02080">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) such as GPT-3 have the surprising ability to do
in-context learning, where the model learns to do a downstream task simply by
conditioning on a prompt consisting of input-output examples. The LM learns
from these examples without being explicitly pretrained to learn. Thus, it is
unclear what enables in-context learning. In this paper, we study how
in-context learning can emerge when pretraining documents have long-range
coherence. Here, the LM must infer a latent document-level concept to generate
coherent next tokens during pretraining. At test time, in-context learning
occurs when the LM also infers a shared latent concept between examples in a
prompt. We prove when this occurs despite a distribution mismatch between
prompts and pretraining data in a setting where the pretraining distribution is
a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs
capable of in-context learning, we generate a small-scale synthetic dataset
(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond
the theory, experiments on GINC exhibit large-scale real-world phenomena
including improved in-context performance with model scaling (despite the same
pretraining loss), sensitivity to example order, and instances where zero-shot
is better than few-shot in-context learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiT: Zero-Shot Transfer with Locked-image text Tuning. (arXiv:2111.07991v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07991">
<div class="article-summary-box-inner">
<span><p>This paper presents contrastive-tuning, a simple method employing contrastive
training to align image and text models while still taking advantage of their
pre-training. In our empirical study we find that locked pre-trained image
models with unlocked text models work best. We call this instance of
contrastive-tuning "Locked-image Tuning" (LiT), which just teaches a text model
to read out good representations from a pre-trained image model for new tasks.
A LiT model gains the capability of zero-shot transfer to new vision tasks,
such as image classification or retrieval. The proposed LiT is widely
applicable; it works reliably with multiple pre-training methods (supervised
and unsupervised) and across diverse architectures (ResNet, Vision Transformers
and MLP-Mixer) using three different image-text datasets. With the
transformer-based pre-trained ViT-g/14 model, the LiT model achieves 84.5%
zero-shot transfer accuracy on the ImageNet test set, and 81.1% on the
challenging out-of-distribution ObjectNet test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation. (arXiv:2112.01589v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01589">
<div class="article-summary-box-inner">
<span><p>Assessing the quality of natural language generation systems through human
annotation is very expensive. Additionally, human annotation campaigns are
time-consuming and include non-reusable human labour. In practice, researchers
rely on automatic metrics as a proxy of quality. In the last decade, many
string-based metrics (e.g., BLEU) have been introduced. However, such metrics
usually rely on exact matches and thus, do not robustly handle synonyms. In
this paper, we introduce InfoLM a family of untrained metrics that can be
viewed as a string-based metric that addresses the aforementioned flaws thanks
to a pre-trained masked language model. This family of metrics also makes use
of information measures allowing the adaptation of InfoLM to various evaluation
criteria. Using direct assessment, we demonstrate that InfoLM achieves
statistically significant improvement and over $10$ points of correlation gains
in many configurations on both summarization and data2text generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset Geography: Mapping Language Data to Language Users. (arXiv:2112.03497v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03497">
<div class="article-summary-box-inner">
<span><p>As language technologies become more ubiquitous, there are increasing efforts
towards expanding the language diversity and coverage of natural language
processing (NLP) systems. Arguably, the most important factor influencing the
quality of modern NLP systems is data availability. In this work, we study the
geographical representativeness of NLP datasets, aiming to quantify if and by
how much do NLP datasets match the expected needs of the language speakers. In
doing so, we use entity recognition and linking systems, also making important
observations about their cross-lingual consistency and giving suggestions for
more robust evaluation. Last, we explore some geographical and economic factors
that may explain the observed dataset distributions. Code and data are
available here: https://github.com/ffaisal93/dataset_geography. Additional
visualizations are available here: https://nlp.cs.gmu.edu/project/datasetmaps/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02113">
<div class="article-summary-box-inner">
<span><p>Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKGC/tree/main/GenKGC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification. (arXiv:2202.05932v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05932">
<div class="article-summary-box-inner">
<span><p>Large-scale multi-label text classification (LMTC) aims to associate a
document with its relevant labels from a large candidate set. Most existing
LMTC approaches rely on massive human-annotated training data, which are often
costly to obtain and suffer from a long-tailed label distribution (i.e., many
labels occur only a few times in the training set). In this paper, we study
LMTC under the zero-shot setting, which does not require any annotated
documents with labels and only relies on label surface names and descriptions.
To train a classifier that calculates the similarity score between a document
and a label, we propose a novel metadata-induced contrastive learning (MICoL)
method. Different from previous text-based contrastive learning techniques,
MICoL exploits document metadata (e.g., authors, venues, and references of
research papers), which are widely available on the Web, to derive similar
document-document pairs. Experimental results on two large-scale datasets show
that: (1) MICoL significantly outperforms strong zero-shot text classification
and contrastive learning baselines; (2) MICoL is on par with the
state-of-the-art supervised metadata-aware LMTC method trained on 10K-200K
labeled documents; and (3) MICoL tends to predict more infrequent labels than
supervised methods, thus alleviates the deteriorated performance on long-tailed
labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross Language Image Matching for Weakly Supervised Semantic Segmentation. (arXiv:2203.02668v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02668">
<div class="article-summary-box-inner">
<span><p>It has been widely known that CAM (Class Activation Map) usually only
activates discriminative object regions and falsely includes lots of
object-related backgrounds. As only a fixed set of image-level object labels
are available to the WSSS (weakly supervised semantic segmentation) model, it
could be very difficult to suppress those diverse background regions consisting
of open set objects. In this paper, we propose a novel Cross Language Image
Matching (CLIMS) framework, based on the recently introduced Contrastive
Language-Image Pre-training (CLIP) model, for WSSS. The core idea of our
framework is to introduce natural language supervision to activate more
complete object regions and suppress closely-related open background regions.
In particular, we design object, background region and text label matching
losses to guide the model to excite more reasonable object regions for CAM of
each category. In addition, we design a co-occurring background suppression
loss to prevent the model from activating closely-related background regions,
with a predefined set of class-related background text descriptions. These
designs enable the proposed CLIMS to generate a more complete and compact
activation map for the target objects. Extensive experiments on PASCAL VOC2012
dataset show that our CLIMS significantly outperforms the previous
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Variational Hierarchical Model for Neural Cross-Lingual Summarization. (arXiv:2203.03820v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03820">
<div class="article-summary-box-inner">
<span><p>The goal of the cross-lingual summarization (CLS) is to convert a document in
one language (e.g., English) to a summary in another one (e.g., Chinese).
Essentially, the CLS task is the combination of machine translation (MT) and
monolingual summarization (MS), and thus there exists the hierarchical
relationship between MT\&amp;MS and CLS. Existing studies on CLS mainly focus on
utilizing pipeline methods or jointly training an end-to-end model through an
auxiliary MT or MS objective. However, it is very challenging for the model to
directly conduct CLS as it requires both the abilities to translate and
summarize. To address this issue, we propose a hierarchical model for the CLS
task, based on the conditional variational auto-encoder. The hierarchical model
contains two kinds of latent variables at the local and global levels,
respectively. At the local level, there are two latent variables, one for
translation and the other for summarization. As for the global level, there is
another latent variable for cross-lingual summarization conditioned on the two
local-level variables. Experiments on two language directions (English-Chinese)
verify the effectiveness and superiority of the proposed approach. In addition,
we show that our model is able to generate better cross-lingual summaries than
comparison models in the few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Blind Denoising via Swin-Conv-UNet and Data Synthesis. (arXiv:2203.13278v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13278">
<div class="article-summary-box-inner">
<span><p>While recent years have witnessed a dramatic upsurge of exploiting deep
neural networks toward solving image denoising, existing methods mostly rely on
simple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG
compression noise and camera sensor noise, and a general-purpose blind
denoising method for real images remains unsolved. In this paper, we attempt to
solve this problem from the perspective of network architecture design and
training data synthesis. Specifically, for the network architecture design, we
propose a swin-conv block to incorporate the local modeling ability of residual
convolutional layer and non-local modeling ability of swin transformer block,
and then plug it as the main building block into the widely-used image-to-image
translation UNet architecture. For the training data synthesis, we design a
practical noise degradation model which takes into consideration different
kinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and
processed camera sensor noises) and resizing, and also involves a random
shuffle strategy and a double degradation strategy. Extensive experiments on
AGWN removal and real image denoising demonstrate that the new network
architecture design achieves state-of-the-art performance and the new
degradation model can help to significantly improve the practicability. We
believe our work can provide useful insights into current denoising research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effectively leveraging Multi-modal Features for Movie Genre Classification. (arXiv:2203.13281v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13281">
<div class="article-summary-box-inner">
<span><p>Movie genre classification has been widely studied in recent years due to its
various applications in video editing, summarization, and recommendation. Prior
work has typically addressed this task by predicting genres based solely on the
visual content. As a result, predictions from these methods often perform
poorly for genres such as documentary or musical, since non-visual modalities
like audio or language play an important role in correctly classifying these
genres. In addition, the analysis of long videos at frame level is always
associated with high computational cost and makes the prediction less
efficient. To address these two issues, we propose a Multi-Modal approach
leveraging shot information, MMShot, to classify video genres in an efficient
and effective way. We evaluate our method on MovieNet and Condensed Movies for
genre classification, achieving 17% ~ 21% improvement on mean Average Precision
(mAP) over the state-of-the-art. Extensive experiments are conducted to
demonstrate the ability of MMShot for long video analysis and uncover the
correlations between genres and multiple movie elements. We also demonstrate
our approach's ability to generalize by evaluating the scene boundary detection
task, achieving 1.1% improvement on Average Precision (AP) over the
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition. (arXiv:2203.13285v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13285">
<div class="article-summary-box-inner">
<span><p>In this paper, we present our submission to 3rd Affective Behavior Analysis
in-the-wild (ABAW) challenge. Learningcomplex interactions among multimodal
sequences is critical to recognise dimensional affect from in-the-wild
audiovisual data. Recurrence and attention are the two widely used sequence
modelling mechanisms in the literature. To clearly understand the performance
differences between recurrent and attention models in audiovisual affect
recognition, we present a comprehensive evaluation of fusion models based on
LSTM-RNNs, self-attention and cross-modal attention, trained for valence and
arousal estimation. Particularly, we study the impact of some key design
choices: the modelling complexity of CNN backbones that provide features to the
the temporal models, with and without end-to-end learning. We trained the
audiovisual affect recognition models on in-the-wild ABAW corpus by
systematically tuning the hyper-parameters involved in the network architecture
design and training optimisation. Our extensive evaluation of the audiovisual
fusion models shows that LSTM-RNNs can outperform the attention models when
coupled with low-complex CNN backbones and trained in an end-to-end fashion,
implying that attention models may not necessarily be the optimal choice for
continuous-time multimodal emotion recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for fingerspelled content in American Sign Language. (arXiv:2203.13291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13291">
<div class="article-summary-box-inner">
<span><p>Natural language processing for sign language video - including tasks like
recognition, translation, and search - is crucial for making artificial
intelligence technologies accessible to deaf individuals, and is gaining
research interest in recent years. In this paper, we address the problem of
searching for fingerspelled key-words or key phrases in raw sign language
videos. This is an important task since significant content in sign language is
often conveyed via fingerspelling, and to our knowledge the task has not been
studied before. We propose an end-to-end model for this task, FSS-Net, that
jointly detects fingerspelling and matches it to a text sequence. Our
experiments, done on a large public dataset of ASL fingerspelling in the wild,
show the importance of fingerspelling detection as a component of a search and
retrieval model. Our model significantly outperforms baseline methods adapted
from prior work on related tasks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RayTran: 3D pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers. (arXiv:2203.13296v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13296">
<div class="article-summary-box-inner">
<span><p>We propose a transformer-based neural network architecture for multi-object
3D reconstruction from RGB videos. It relies on two alternative ways to
represent its knowledge: as a global 3D grid of features and an array of
view-specific 2D grids. We progressively exchange information between the two
with a dedicated bidirectional attention mechanism. We exploit knowledge about
the image formation process to significantly sparsify the attention weight
matrix, making our architecture feasible on current hardware, both in terms of
memory and computation. We attach a DETR-style head on top of the 3D feature
grid in order to detect the objects in the scene and to predict their 3D pose
and 3D shape. Compared to previous methods, our architecture is single stage,
end-to-end trainable, and it can reason holistically about a scene from
multiple video frames without needing a brittle tracking step. We evaluate our
method on the challenging Scan2CAD dataset, where we outperform (1) recent
state-of-the-art methods for 3D object pose estimation from RGB videos; and (2)
a strong alternative method combining Multi-view Stereo with RGB-D CAD
alignment. We plan to release our source code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Multi-label Facial Action Unit Detection with Transformer. (arXiv:2203.13301v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13301">
<div class="article-summary-box-inner">
<span><p>Facial Action Coding System is an important approach of facial expression
analysis.This paper describes our submission to the third Affective Behavior
Analysis (ABAW) 2022 competition. We proposed a transfomer based model to
detect facial action unit (FAU) in video. To be specific, we firstly trained a
multi-modal model to extract both audio and visual feature. After that, we
proposed a action units correlation module to learn relationships between each
action unit labels and refine action unit detection result. Experimental
results on validation dataset shows that our method achieves better performance
than baseline model, which verifies that the effectiveness of proposed network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Online Action Segmentation in Multi-View Instructional Videos. (arXiv:2203.13309v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13309">
<div class="article-summary-box-inner">
<span><p>This paper addresses a new problem of weakly-supervised online action
segmentation in instructional videos. We present a framework to segment
streaming videos online at test time using Dynamic Programming and show its
advantages over greedy sliding window approach. We improve our framework by
introducing the Online-Offline Discrepancy Loss (OODL) to encourage the
segmentation results to have a higher temporal consistency. Furthermore, only
during training, we exploit frame-wise correspondence between multiple views as
supervision for training weakly-labeled instructional videos. In particular, we
investigate three different multi-view inference techniques to generate more
accurate frame-wise pseudo ground-truth with no additional annotation cost. We
present results and ablation studies on two benchmark multi-view datasets,
Breakfast and IKEA ASM. Experimental results show efficacy of the proposed
methods both qualitatively and quantitatively in two domains of cooking and
assembly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13310">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection has long been a challenging task in autonomous
driving, which requires to decode 3D predictions solely from a single 2D image.
Most existing methods follow conventional 2D object detectors to first localize
objects by their centers, and then predict 3D attributes using
center-neighboring local features. However, such center-based pipeline views 3D
prediction as a subordinate task and lacks inter-object depth interactions with
global spatial clues. In this paper, we introduce a simple framework for
Monocular DEtection with depth-aware TRansformer, named MonoDETR. We enable the
vanilla transformer to be depth-aware and enforce the whole detection process
guided by depth. Specifically, we represent 3D object candidates as a set of
queries and produce non-local depth embeddings of the input image by a
lightweight depth predictor and an attention-based depth encoder. Then, we
propose a depth-aware decoder to conduct both inter-query and query-scene depth
feature communication. In this way, each object estimates its 3D attributes
adaptively from the depth-informative regions on the image, not limited by
center-around features. With minimal handcrafted designs, MonoDETR is an
end-to-end framework without additional data, anchors or NMS and achieves
competitive performance on KITTI benchmark among state-of-the-art center-based
networks. Extensive ablation studies demonstrate the effectiveness of our
approach and its potential to serve as a transformer baseline for future
monocular research. Code is available at
https://github.com/ZrrSkywalker/MonoDETR.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SharpContour: A Contour-based Boundary Refinement Approach for Efficient and Accurate Instance Segmentation. (arXiv:2203.13312v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13312">
<div class="article-summary-box-inner">
<span><p>Excellent performance has been achieved on instance segmentation but the
quality on the boundary area remains unsatisfactory, which leads to a rising
attention on boundary refinement. For practical use, an ideal post-processing
refinement scheme are required to be accurate, generic and efficient. However,
most of existing approaches propose pixel-wise refinement, which either
introduce a massive computation cost or design specifically for different
backbone models. Contour-based models are efficient and generic to be
incorporated with any existing segmentation methods, but they often generate
over-smoothed contour and tend to fail on corner areas. In this paper, we
propose an efficient contour-based boundary refinement approach, named
SharpContour, to tackle the segmentation of boundary area. We design a novel
contour evolution process together with an Instance-aware Point Classifier. Our
method deforms the contour iteratively by updating offsets in a discrete
manner. Differing from existing contour evolution methods, SharpContour
estimates each offset more independently so that it predicts much sharper and
accurate contours. Notably, our method is generic to seamlessly work with
diverse existing models with a small computational cost. Experiments show that
SharpContour achieves competitive gains whilst preserving high efficiency
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning for laboratory earthquake prediction and autoregressive forecasting of fault zone stress. (arXiv:2203.13313v1 [physics.geo-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13313">
<div class="article-summary-box-inner">
<span><p>Earthquake forecasting and prediction have long and in some cases sordid
histories but recent work has rekindled interest based on advances in early
warning, hazard assessment for induced seismicity and successful prediction of
laboratory earthquakes. In the lab, frictional stick-slip events provide an
analog for earthquakes and the seismic cycle. Labquakes are ideal targets for
machine learning (ML) because they can be produced in long sequences under
controlled conditions. Recent works show that ML can predict several aspects of
labquakes using fault zone acoustic emissions. Here, we generalize these
results and explore deep learning (DL) methods for labquake prediction and
autoregressive (AR) forecasting. DL improves existing ML methods of labquake
prediction. AR methods allow forecasting at future horizons via iterative
predictions. We demonstrate that DL models based on Long-Short Term Memory
(LSTM) and Convolution Neural Networks predict labquakes under several
conditions, and that fault zone stress can be predicted with fidelity,
confirming that acoustic energy is a fingerprint of fault zone stress. We
predict also time to start of failure (TTsF) and time to the end of Failure
(TTeF) for labquakes. Interestingly, TTeF is successfully predicted in all
seismic cycles, while the TTsF prediction varies with the amount of preseismic
fault creep. We report AR methods to forecast the evolution of fault stress
using three sequence modeling frameworks: LSTM, Temporal Convolution Network
and Transformer Network. AR forecasting is distinct from existing predictive
models, which predict only a target variable at a specific time. The results
for forecasting beyond a single seismic cycle are limited but encouraging. Our
ML/DL models outperform the state-of-the-art and our autoregressive model
represents a novel framework that could enhance current methods of earthquake
forecasting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Gait Recognition Using Bag of Words Feature Representation Method. (arXiv:2203.13317v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13317">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel gait recognition method based on a
bag-of-words feature representation method. The algorithm is trained, tested
and evaluated on a unique human gait data consisting of 93 individuals who
walked with comfortable pace between two end points during two different
sessions. To evaluate the effectiveness of the proposed model, the results are
compared with the outputs of the classification using extracted features. As it
is presented, the proposed method results in significant improvement accuracy
compared to using common statistical features, in all the used classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NPBG++: Accelerating Neural Point-Based Graphics. (arXiv:2203.13318v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13318">
<div class="article-summary-box-inner">
<span><p>We present a new system (NPBG++) for the novel view synthesis (NVS) task that
achieves high rendering realism with low scene fitting time. Our method
efficiently leverages the multiview observations and the point cloud of a
static scene to predict a neural descriptor for each point, improving upon the
pipeline of Neural Point-Based Graphics in several important ways. By
predicting the descriptors with a single pass through the source images, we
lift the requirement of per-scene optimization while also making the neural
descriptors view-dependent and more suitable for scenes with strong
non-Lambertian effects. In our comparisons, the proposed system outperforms
previous NVS approaches in terms of fitting and rendering runtimes while
producing images of similar quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text to Mesh Without 3D Supervision Using Limit Subdivision. (arXiv:2203.13333v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13333">
<div class="article-summary-box-inner">
<span><p>We present a technique for zero-shot generation of a 3D model using only a
target text prompt. Without a generative model or any 3D supervision our method
deforms a control shape of a limit subdivided surface along with a texture map
and normal map to obtain a 3D model asset that matches the input text prompt
and can be deployed into games or modeling applications. We rely only on a
pre-trained CLIP model that compares the input text prompt with differentiably
rendered images of our 3D model. While previous works have focused on
stylization or required training of generative models we perform optimization
on mesh parameters directly to generate shape and texture. To improve the
quality of results we also introduce a set of techniques such as render
augmentations, primitive selection, prompt augmentation that guide the mesh
towards a suitable result.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Occluded Human Mesh Recovery. (arXiv:2203.13349v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13349">
<div class="article-summary-box-inner">
<span><p>Top-down methods for monocular human mesh recovery have two stages: (1)
detect human bounding boxes; (2) treat each bounding box as an independent
single-human mesh recovery task. Unfortunately, the single-human assumption
does not hold in images with multi-human occlusion and crowding. Consequently,
top-down methods have difficulties in recovering accurate 3D human meshes under
severe person-person occlusion. To address this, we present Occluded Human Mesh
Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates
image spatial context to overcome the limitations of the single-human
assumption. The approach is conceptually simple and can be applied to any
existing top-down architecture. Along with the input image, we condition the
top-down model on spatial context from the image in the form of body-center
heatmaps. To reason from the predicted body centermaps, we introduce Contextual
Normalization (CoNorm) blocks to adaptively modulate intermediate features of
the top-down model. The contextual conditioning helps our model disambiguate
between two severely overlapping human bounding-boxes, making it robust to
multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves
superior performance on challenging multi-person benchmarks like 3DPW,
CrowdPose and OCHuman. Specifically, our proposed contextual reasoning
architecture applied to the SPIN model with ResNet-50 backbone results in 75.2
PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a
significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the
baseline. Code and models will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FitCLIP: Refining Large-Scale Pretrained Image-Text Models for Zero-Shot Video Understanding Tasks. (arXiv:2203.13371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13371">
<div class="article-summary-box-inner">
<span><p>Large-scale pretrained image-text models have shown incredible zero-shot
performance in a handful of tasks, including video ones such as action
recognition and text-to-video retrieval. However, these models haven't been
adapted to video, mainly because they don't account for the time dimension but
also because video frames are different from the typical images (e.g.,
containing motion blur, less sharpness). In this paper, we present a
fine-tuning strategy to refine these large-scale pretrained image-text models
for zero-shot video understanding tasks. We show that by carefully adapting
these models we obtain considerable improvements on two zero-shot Action
Recognition tasks and three zero-shot Text-to-video Retrieval tasks. The code
is available at https://github.com/bryant1410/fitclip
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Representation Forgetting in Supervised and Unsupervised Continual Learning. (arXiv:2203.13381v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13381">
<div class="article-summary-box-inner">
<span><p>Continual Learning research typically focuses on tackling the phenomenon of
catastrophic forgetting in neural networks. Catastrophic forgetting is
associated with an abrupt loss of knowledge previously learned by a model when
the task, or more broadly the data distribution, being trained on changes. In
supervised learning problems this forgetting, resulting from a change in the
model's representation, is typically measured or observed by evaluating the
decrease in old task performance. However, a model's representation can change
without losing knowledge about prior tasks. In this work we consider the
concept of representation forgetting, observed by using the difference in
performance of an optimal linear classifier before and after a new task is
introduced. Using this tool we revisit a number of standard continual learning
benchmarks and observe that, through this lens, model representations trained
without any explicit control for forgetting often experience small
representation forgetting and can sometimes be comparable to methods which
explicitly control for forgetting, especially in longer task sequences. We also
show that representation forgetting can lead to new insights on the effect of
model capacity and loss function used in continual learning. Based on our
results, we show that a simple yet competitive approach is to learn
representations continually with standard supervised contrastive learning while
constructing prototypes of class samples when queried on old samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossFormer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation. (arXiv:2203.13387v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13387">
<div class="article-summary-box-inner">
<span><p>3D human pose estimation can be handled by encoding the geometric
dependencies between the body parts and enforcing the kinematic constraints.
Recently, Transformer has been adopted to encode the long-range dependencies
between the joints in the spatial and temporal domains. While they had shown
excellence in long-range dependencies, studies have noted the need for
improving the locality of vision Transformers. In this direction, we propose a
novel pose estimation Transformer featuring rich representations of body joints
critical for capturing subtle changes across frames (i.e., inter-feature
representation). Specifically, through two novel interaction modules;
Cross-Joint Interaction and Cross-Frame Interaction, the model explicitly
encodes the local and global dependencies between the body joints. The proposed
architecture achieved state-of-the-art performance on two popular 3D human pose
estimation datasets, Human3.6 and MPI-INF-3DHP. In particular, our proposed
CrossFormer method boosts performance by 0.9% and 0.3%, compared to the closest
counterpart, PoseFormer, using the detected 2D poses and ground-truth settings
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point2Seq: Detecting 3D Objects as Sequences. (arXiv:2203.13394v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13394">
<div class="article-summary-box-inner">
<span><p>We present a simple and effective framework, named Point2Seq, for 3D object
detection from point clouds. In contrast to previous methods that normally
{predict attributes of 3D objects all at once}, we expressively model the
interdependencies between attributes of 3D objects, which in turn enables a
better detection accuracy. Specifically, we view each 3D object as a sequence
of words and reformulate the 3D object detection task as decoding words from 3D
scenes in an auto-regressive manner. We further propose a lightweight
scene-to-sequence decoder that can auto-regressively generate words conditioned
on features from a 3D scene as well as cues from the preceding words. The
predicted words eventually constitute a set of sequences that completely
describe the 3D objects in the scene, and all the predicted sequences are then
automatically assigned to the respective ground truths through similarity-based
sequence matching. Our approach is conceptually intuitive and can be readily
plugged upon most existing 3D-detection backbones without adding too much
computational overhead; the sequential decoding paradigm we proposed, on the
other hand, can better exploit information from complex 3D scenes with the aid
of preceding predicted words. Without bells and whistles, our method
significantly outperforms previous anchor- and center-based 3D object detection
frameworks, yielding the new state of the art on the challenging ONCE dataset
as well as the Waymo Open Dataset. Code is available at
\url{https://github.com/ocNflag/point2seq}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale and Cross-scale Contrastive Learning for Semantic Segmentation. (arXiv:2203.13409v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13409">
<div class="article-summary-box-inner">
<span><p>This work considers supervised contrastive learning for semantic
segmentation. Our approach is model agnostic. We apply contrastive learning to
enhance the discriminative power of the multi-scale features extracted by
semantic segmentation networks. Our key methodological insight is to leverage
samples from the feature spaces emanating from multiple stages of a model's
encoder itself requiring neither data augmentation nor online memory banks to
obtain a diverse set of samples. To allow for such an extension we introduce an
efficient and effective sampling process, that enables applying contrastive
losses over the encoder's features at multiple scales. Furthermore, by first
mapping the encoder's multi-scale representations to a common feature space, we
instantiate a novel form of supervised local-global constraint by introducing
cross-scale contrastive learning linking high-resolution local features to
low-resolution global features. Combined, our multi-scale and cross-scale
contrastive losses boost performance of various models (DeepLabV3, HRNet,
OCRNet, UPerNet) with both CNN and Transformer backbones, when evaluated on 4
diverse datasets from natural (Cityscapes, PascalContext, ADE20K) but also
surgical (CaDIS) domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Predictive Learning: A Negative-Free Method for Sound Source Localization in Visual Scenes. (arXiv:2203.13412v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13412">
<div class="article-summary-box-inner">
<span><p>Sound source localization in visual scenes aims to localize objects emitting
the sound in a given image. Recent works showing impressive localization
performance typically rely on the contrastive learning framework. However, the
random sampling of negatives, as commonly adopted in these methods, can result
in misalignment between audio and visual features and thus inducing ambiguity
in localization. In this paper, instead of following previous literature, we
propose Self-Supervised Predictive Learning (SSPL), a negative-free method for
sound localization via explicit positive mining. Specifically, we first devise
a three-stream network to elegantly associate sound source with two augmented
views of one corresponding video frame, leading to semantically coherent
similarities between audio and visual features. Second, we introduce a novel
predictive coding module for audio-visual feature alignment. Such a module
assists SSPL to focus on target objects in a progressive manner and effectively
lowers the positive-pair learning difficulty. Experiments show surprising
results that SSPL outperforms the state-of-the-art approach on two standard
sound localization benchmarks. In particular, SSPL achieves significant
improvements of 8.6% cIoU and 3.4% AUC on SoundNet-Flickr compared to the
previous best. Code is available at: https://github.com/zjsong/SSPL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noisy Boundaries: Lemon or Lemonade for Semi-supervised Instance Segmentation?. (arXiv:2203.13427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13427">
<div class="article-summary-box-inner">
<span><p>Current instance segmentation methods rely heavily on pixel-level annotated
images. The huge cost to obtain such fully-annotated images restricts the
dataset scale and limits the performance. In this paper, we formally address
semi-supervised instance segmentation, where unlabeled images are employed to
boost the performance. We construct a framework for semi-supervised instance
segmentation by assigning pixel-level pseudo labels. Under this framework, we
point out that noisy boundaries associated with pseudo labels are double-edged.
We propose to exploit and resist them in a unified manner simultaneously: 1) To
combat the negative effects of noisy boundaries, we propose a noise-tolerant
mask head by leveraging low-resolution features. 2) To enhance the positive
impacts, we introduce a boundary-preserving map for learning detailed
information within boundary-relevant regions. We evaluate our approach by
extensive experiments. It behaves extraordinarily, outperforming the supervised
baseline by a large margin, more than 6% on Cityscapes, 7% on COCO and 4.5% on
BDD100k. On Cityscapes, our method achieves comparable performance by utilizing
only 30% labeled images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices. (arXiv:2203.13436v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13436">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the problem of real-time video-based facial
emotion analytics, namely, facial expression recognition, prediction of valence
and arousal and detection of action unit points. We propose the novel
frame-level emotion recognition algorithm by extracting facial features with
the single EfficientNet model pre-trained on AffectNet. As a result, our
approach may be implemented even for video analytics on mobile devices.
Experimental results for the large scale Aff-Wild2 database from the third
Affective Behavior Analysis in-the-wild (ABAW) Competition demonstrate that our
simple model is significantly better when compared to the VggFace baseline. In
particular, our method is characterized by 0.15-0.2 higher performance measures
for validation sets in uni-task Expression Classification, Valence-Arousal
Estimation and Expression Classification. Due to simplicity, our approach may
be considered as a new baseline for all four sub-challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BCOT: A Markerless High-Precision 3D Object Tracking Benchmark. (arXiv:2203.13437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13437">
<div class="article-summary-box-inner">
<span><p>Template-based 3D object tracking still lacks a high-precision benchmark of
real scenes due to the difficulty of annotating the accurate 3D poses of real
moving video objects without using markers. In this paper, we present a
multi-view approach to estimate the accurate 3D poses of real moving objects,
and then use binocular data to construct a new benchmark for monocular
textureless 3D object tracking. The proposed method requires no markers, and
the cameras only need to be synchronous, relatively fixed as cross-view and
calibrated. Based on our object-centered model, we jointly optimize the object
pose by minimizing shape re-projection constraints in all views, which greatly
improves the accuracy compared with the single-view approach, and is even more
accurate than the depth-based method. Our new benchmark dataset contains 20
textureless objects, 22 scenes, 404 video sequences and 126K images captured in
real scenes. The annotation error is guaranteed to be less than 2mm, according
to both theoretical analysis and validation experiments. We re-evaluate the
state-of-the-art 3D object tracking methods with our dataset, reporting their
performance ranking in real scenes. Our BCOT benchmark and code can be found at
https://ar3dv.github.io/BCOT-Benchmark/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Microstructure Surface Reconstruction from SEM Images: An Alternative to Digital Image Correlation (DIC). (arXiv:2203.13438v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13438">
<div class="article-summary-box-inner">
<span><p>We reconstruct a 3D model of the surface of a material undergoing fatigue
testing and experiencing cracking. Specifically we reconstruct the surface
depth (out of plane intrusions and extrusions) and lateral (in-plane) motion
from multiple views of the sample at the end of the experiment, combined with a
reverse optical flow propagation backwards in time that utilizes interim single
view images. These measurements can be mapped to a material strain tensor which
helps to understand material life and predict failure. This approach offers an
alternative to the commonly used Digital Image Correlation (DIC) technique
which relies on tracking a speckle pattern applied to the material surface. DIC
only produces in-plane (2D) measurements whereas our approach is 3D and
non-invasive (requires no pattern being applied to the material).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D GAN Inversion for Controllable Portrait Image Animation. (arXiv:2203.13441v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13441">
<div class="article-summary-box-inner">
<span><p>Millions of images of human faces are captured every single day; but these
photographs portray the likeness of an individual with a fixed pose,
expression, and appearance. Portrait image animation enables the post-capture
adjustment of these attributes from a single image while maintaining a
photorealistic reconstruction of the subject's likeness or identity. Still,
current methods for portrait image animation are typically based on 2D warping
operations or manipulations of a 2D generative adversarial network (GAN) and
lack explicit mechanisms to enforce multi-view consistency. Thus these methods
may significantly alter the identity of the subject, especially when the
viewpoint relative to the camera is changed. In this work, we leverage newly
developed 3D GANs, which allow explicit control over the pose of the image
subject with multi-view consistency. We propose a supervision strategy to
flexibly manipulate expressions with 3D morphable models, and we show that the
proposed method also supports editing appearance attributes, such as age or
hairstyle, by interpolating within the latent space of the GAN. The proposed
technique for portrait image animation outperforms previous methods in terms of
image quality, identity preservation, and pose transfer while also supporting
attribute editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDAN: Multi-level Dependent Attention Network for Visual Emotion Analysis. (arXiv:2203.13443v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13443">
<div class="article-summary-box-inner">
<span><p>Visual Emotion Analysis (VEA) is attracting increasing attention. One of the
biggest challenges of VEA is to bridge the affective gap between visual clues
in a picture and the emotion expressed by the picture. As the granularity of
emotions increases, the affective gap increases as well. Existing deep
approaches try to bridge the gap by directly learning discrimination among
emotions globally in one shot without considering the hierarchical relationship
among emotions at different affective levels and the affective level of
emotions to be classified. In this paper, we present the Multi-level Dependent
Attention Network (MDAN) with two branches, to leverage the emotion hierarchy
and the correlation between different affective levels and semantic levels. The
bottom-up branch directly learns emotions at the highest affective level and
strictly follows the emotion hierarchy while predicting emotions at lower
affective levels. In contrast, the top-down branch attempt to disentangle the
affective gap by one-to-one mapping between semantic levels and affective
levels, namely, Affective Semantic Mapping. At each semantic level, a local
classifier learns discrimination among emotions at the corresponding affective
level. Finally, We integrate global learning and local learning into a unified
deep framework and optimize the network simultaneously. Moreover, to properly
extract and leverage channel dependencies and spatial attention while
disentangling the affective gap, we carefully designed two attention modules:
the Multi-head Cross Channel Attention module and the Level-dependent Class
Activation Map module. Finally, the proposed deep framework obtains new
state-of-the-art performance on six VEA benchmarks, where it outperforms
existing state-of-the-art methods by a large margin, e.g., +3.85% on the WEBEmo
dataset at 25 classes classification accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer Compression with Structured Pruning and Low Rank Approximation. (arXiv:2203.13444v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13444">
<div class="article-summary-box-inner">
<span><p>Transformer architecture has gained popularity due to its ability to scale
with large dataset. Consequently, there is a need to reduce the model size and
latency, especially for on-device deployment. We focus on vision transformer
proposed for image recognition task (Dosovitskiy et al., 2021), and explore the
application of different compression techniques such as low rank approximation
and pruning for this purpose. Specifically, we investigate a structured pruning
method proposed recently in Zhu et al. (2021) and find that mostly feedforward
blocks are pruned with this approach, that too, with severe degradation in
accuracy. We propose a hybrid compression approach to mitigate this where we
compress the attention blocks using low rank approximation and use the
previously mentioned pruning with a lower rate for feedforward blocks in each
transformer layer. Our technique results in 50% compression with 14% relative
increase in classification error whereas we obtain 44% compression with 20%
relative increase in error when only pruning is applied. We propose further
enhancements to bridge the accuracy gap but leave it as a future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PCA-Based Knowledge Distillation Towards Lightweight and Content-Style Balanced Photorealistic Style Transfer Models. (arXiv:2203.13452v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13452">
<div class="article-summary-box-inner">
<span><p>Photorealistic style transfer entails transferring the style of a reference
image to another image so the result seems like a plausible photo. Our work is
inspired by the observation that existing models are slow due to their large
sizes. We introduce PCA-based knowledge distillation to distill lightweight
models and show it is motivated by theory. To our knowledge, this is the first
knowledge distillation method for photorealistic style transfer. Our
experiments demonstrate its versatility for use with different backbone
architectures, VGG and MobileNet, across six image resolutions. Compared to
existing models, our top-performing model runs at speeds 5-20x faster using at
most 1\% of the parameters. Additionally, our distilled models achieve a better
balance between stylization strength and content preservation than existing
models. To support reproducing our method and models, we share the code at
\textit{https://github.com/chiutaiyin/PCA-Knowledge-Distillation}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN LEGO: Disassembling and Assembling Convolutional Neural Network. (arXiv:2203.13453v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13453">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Network (CNN), which mimics human visual perception
mechanism, has been successfully used in many computer vision areas. Some
psychophysical studies show that the visual perception mechanism synchronously
processes the form, color, movement, depth, etc., in the initial stage [7,20]
and then integrates all information for final recognition [38]. What's more,
the human visual system [20] contains different subdivisions or different
tasks. Inspired by the above visual perception mechanism, we investigate a new
task, termed as Model Disassembling and Assembling (MDA-Task), which can
disassemble the deep models into independent parts and assemble those parts
into a new deep model without performance cost like playing LEGO toys. To this
end, we propose a feature route attribution technique (FRAT) for disassembling
CNN classifiers in this paper. In FRAT, the positive derivatives of predicted
class probability w.r.t. the feature maps are adopted to locate the critical
features in each layer. Then, relevance analysis between the critical features
and preceding/subsequent parameter layers is adopted to bridge the route
between two adjacent parameter layers. In the assembling phase, class-wise
components of each layer are assembled into a new deep model for a specific
task. Extensive experiments demonstrate that the assembled CNN classifier can
achieve close accuracy with the original classifier without any fine-tune, and
excess original performance with one-epoch fine-tune. What's more, we also
conduct massive experiments to verify the broad application of MDA-Task on
model decision route visualization, model compression, knowledge distillation,
transfer learning, incremental learning, and so on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training. (arXiv:2203.13455v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13455">
<div class="article-summary-box-inner">
<span><p>Adversarial Training (AT) is known as an effective approach to enhance the
robustness of deep neural networks. Recently researchers notice that robust
models with AT have good generative ability and can synthesize realistic
images, while the reason behind it is yet under-explored. In this paper, we
demystify this phenomenon by developing a unified probabilistic framework,
called Contrastive Energy-based Models (CEM). On the one hand, we provide the
first probabilistic characterization of AT through a unified understanding of
robustness and generative ability. On the other hand, our unified framework can
be extended to the unsupervised scenario, which interprets unsupervised
contrastive learning as an important sampling of CEM. Based on these, we
propose a principled method to develop adversarial learning and sampling
methods. Experiments show that the sampling methods derived from our framework
improve the sample quality in both supervised and unsupervised learning.
Notably, our unsupervised adversarial sampling method achieves an Inception
score of 9.61 on CIFAR-10, which is superior to previous energy-based models
and comparable to state-of-the-art generative models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap. (arXiv:2203.13457v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13457">
<div class="article-summary-box-inner">
<span><p>Recently, contrastive learning has risen to be a promising approach for
large-scale self-supervised learning. However, theoretical understanding of how
it works is still unclear. In this paper, we propose a new guarantee on the
downstream performance without resorting to the conditional independence
assumption that is widely adopted in previous work but hardly holds in
practice. Our new theory hinges on the insight that the support of different
intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Based on this augmentation overlap perspective, theoretically, we
obtain asymptotically closed bounds for downstream performance under weaker
assumptions, and empirically, we propose an unsupervised model selection metric
ARC that aligns well with downstream accuracy. Our theory suggests an
alternative understanding of contrastive learning: the role of aligning
positive samples is more like a surrogate task than an ultimate goal, and the
overlapped augmented views (i.e., the chaos) create a ladder for contrastive
learning to gradually learn class-separated representations. The code for
computing ARC is available at https://github.com/zhangq327/ARC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PANDORA: Polarization-Aided Neural Decomposition Of Radiance. (arXiv:2203.13458v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13458">
<div class="article-summary-box-inner">
<span><p>Reconstructing an object's geometry and appearance from multiple images, also
known as inverse rendering, is a fundamental problem in computer graphics and
vision. Inverse rendering is inherently ill-posed because the captured image is
an intricate function of unknown lighting conditions, material properties and
scene geometry. Recent progress in representing scene properties as
coordinate-based neural networks have facilitated neural inverse rendering
resulting in impressive geometry reconstruction and novel-view synthesis. Our
key insight is that polarization is a useful cue for neural inverse rendering
as polarization strongly depends on surface normals and is distinct for diffuse
and specular reflectance. With the advent of commodity, on-chip, polarization
sensors, capturing polarization has become practical. Thus, we propose PANDORA,
a polarimetric inverse rendering approach based on implicit neural
representations. From multi-view polarization images of an object, PANDORA
jointly extracts the object's 3D geometry, separates the outgoing radiance into
diffuse and specular and estimates the illumination incident on the object. We
show that PANDORA outperforms state-of-the-art radiance decomposition
techniques. PANDORA outputs clean surface reconstructions free from texture
artefacts, models strong specularities accurately and estimates illumination
under practical unstructured scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised and Deep learning Frameworks for Video Classification and Key-frame Identification. (arXiv:2203.13459v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13459">
<div class="article-summary-box-inner">
<span><p>Automating video-based data and machine learning pipelines poses several
challenges including metadata generation for efficient storage and retrieval
and isolation of key-frames for scene understanding tasks. In this work, we
present two semi-supervised approaches that automate this process of manual
frame sifting in video streams by automatically classifying scenes for content
and filtering frames for fine-tuning scene understanding tasks. The first
rule-based method starts from a pre-trained object detector and it assigns
scene type, uncertainty and lighting categories to each frame based on
probability distributions of foreground objects. Next, frames with the highest
uncertainty and structural dissimilarity are isolated as key-frames. The second
method relies on the simCLR model for frame encoding followed by
label-spreading from 20% of frame samples to label the remaining frames for
scene and lighting categories. Also, clustering the video frames in the encoded
feature space further isolates key-frames at cluster boundaries. The proposed
methods achieve 64-93% accuracy for automated scene categorization for outdoor
image videos from public domain datasets of JAAD and KITTI. Also, less than 10%
of all input frames can be filtered as key-frames that can then be sent for
annotation and fine tuning of machine vision algorithms. Thus, the proposed
framework can be scaled to additional video data streams for automated training
of perception-driven systems with minimal training images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretation of Chest x-rays affected by bullets using deep transfer learning. (arXiv:2203.13461v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13461">
<div class="article-summary-box-inner">
<span><p>The potential of deep learning, especially in medical imaging, initiated
astonishing results and improved the methodologies after every passing day.
Deep learning in radiology provides the opportunity to classify, detect and
segment different diseases automatically. In the proposed study, we worked on a
non-trivial aspect of medical imaging where we classified and localized the
X-Rays affected by bullets. We tested Images on different classification and
localization models to get considerable accuracy. The replicated data set used
in the study was replicated on different images of chest X-Rays. The proposed
model worked not only on chest radiographs but other body organs X-rays like
leg, abdomen, head, even the training dataset based on chest radiographs.
Custom models have been used for classification and localization purposes after
tuning parameters. Finally, the results of our findings manifested using
different frameworks. This might assist the research enlightening towards this
field. To the best of our knowledge, this is the first study on the detection
and classification of radiographs affected by bullets using deep learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAD: Co-Adapting Discriminative Features for Improved Few-Shot Classification. (arXiv:2203.13465v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13465">
<div class="article-summary-box-inner">
<span><p>Few-shot classification is a challenging problem that aims to learn a model
that can adapt to unseen classes given a few labeled samples. Recent approaches
pre-train a feature extractor, and then fine-tune for episodic meta-learning.
Other methods leverage spatial features to learn pixel-level correspondence
while jointly training a classifier. However, results using such approaches
show marginal improvements. In this paper, inspired by the transformer style
self-attention mechanism, we propose a strategy to cross-attend and re-weight
discriminative features for few-shot classification. Given a base
representation of support and query images after global pooling, we introduce a
single shared module that projects features and cross-attends in two aspects:
(i) query to support, and (ii) support to query. The module computes attention
scores between features to produce an attention pooled representation of
features in the same class that is later added to the original representation
followed by a projection head. This effectively re-weights features in both
aspects (i &amp; ii) to produce features that better facilitate improved
metric-based meta-learning. Extensive experiments on public benchmarks show our
approach outperforms state-of-the-art methods by 3%~5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RD-Optimized Trit-Plane Coding of Deep Compressed Image Latent Tensors. (arXiv:2203.13467v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13467">
<div class="article-summary-box-inner">
<span><p>DPICT is the first learning-based image codec supporting fine granular
scalability. In this paper, we describe how to implement two key components of
DPICT efficiently: trit-plane slicing and RD-prioritized transmission. In
DPICT, we transform an image into a latent tensor, represent the tensor in
ternary digits (trits), and encode the trits in the decreasing order of
significance. For entropy encoding, we should compute the probability of each
trit, which demands high time complexity in both the encoder and the decoder.
To reduce the complexity, we develop a parallel computing scheme for the
probabilities and describe it in detail with pseudo-codes. Moreover, in this
paper, we compare the trit-plane slicing in DPICT with the alternative
bit-plane slicing. Experimental results show that the time complexity is
reduced significantly by the parallel computing and that the trit-plane slicing
provides better rate-distortion performances than the bit-plane slicing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Style Transfer: All is Your Palette. (arXiv:2203.13470v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13470">
<div class="article-summary-box-inner">
<span><p>Neural style transfer (NST) can create impressive artworks by transferring
reference style to content image. Current image-to-image NST methods are short
of fine-grained controls, which are often demanded by artistic editing. To
mitigate this limitation, we propose a drawing-like interactive style transfer
(IST) method, by which users can interactively create a harmonious-style image.
Our IST method can serve as a brush, dip style from anywhere, and then paint to
any region of the target content image. To determine the action scope, we
formulate a fluid simulation algorithm, which takes styles as pigments around
the position of brush interaction, and diffusion in style or content images
according to the similarity maps. Our IST method expands the creative dimension
of NST. By dipping and painting, even employing one style image can produce
thousands of eye-catching works. The demo video is available in supplementary
files or in <a href="http://mmcheng.net/ist.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Probability Sampling Network for Stochastic Human Trajectory Prediction. (arXiv:2203.13471v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13471">
<div class="article-summary-box-inner">
<span><p>Capturing multimodal natures is essential for stochastic pedestrian
trajectory prediction, to infer a finite set of future trajectories. The
inferred trajectories are based on observation paths and the latent vectors of
potential decisions of pedestrians in the inference step. However, stochastic
approaches provide varying results for the same data and parameter settings,
due to the random sampling of the latent vector. In this paper, we analyze the
problem by reconstructing and comparing probabilistic distributions from
prediction samples and socially-acceptable paths, respectively. Through this
analysis, we observe that the inferences of all stochastic models are biased
toward the random sampling, and fail to generate a set of realistic paths from
finite samples. The problem cannot be resolved unless an infinite number of
samples is available, which is infeasible in practice. We introduce that the
Quasi-Monte Carlo (QMC) method, ensuring uniform coverage on the sampling
space, as an alternative to the conventional random sampling. With the same
finite number of samples, the QMC improves all the multimodal prediction
results. We take an additional step ahead by incorporating a learnable sampling
network into the existing networks for trajectory prediction. For this purpose,
we propose the Non-Probability Sampling Network (NPSN), a very small network
(~5K parameters) that generates purposive sample sequences using the past paths
of pedestrians and their social interactions. Extensive experiments confirm
that NPSN can significantly improve both the prediction accuracy (up to 60%)
and reliability of the public pedestrian trajectory prediction benchmark. Code
is publicly available at https://github.com/inhwanbae/NPSN .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial Expression Recognition with Swin Transformer. (arXiv:2203.13472v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13472">
<div class="article-summary-box-inner">
<span><p>The task of recognizing human facial expressions plays a vital role in
various human-related systems, including health care and medical fields. With
the recent success of deep learning and the accessibility of a large amount of
annotated data, facial expression recognition research has been mature enough
to be utilized in real-world scenarios with audio-visual datasets. In this
paper, we introduce Swin transformer-based facial expression approach for an
in-the-wild audio-visual dataset of the Aff-Wild2 Expression dataset.
Specifically, we employ a three-stream network (i.e., Visual stream, Temporal
stream, and Audio stream) for the audio-visual videos to fuse the multi-modal
information into facial expression recognition. Experimental results on the
Aff-Wild2 dataset show the effectiveness of our proposed multi-modal
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Adversarial Transferability with Spatial Momentum. (arXiv:2203.13479v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13479">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNN) are vulnerable to adversarial examples. Although
many adversarial attack methods achieve satisfactory attack success rates under
the white-box setting, they usually show poor transferability when attacking
other DNN models. Momentum-based attack (MI-FGSM) is one effective method to
improve transferability. It integrates the momentum term into the iterative
process, which can stabilize the update directions by adding the gradients'
temporal correlation for each pixel. We argue that only this temporal momentum
is not enough, the gradients from the spatial domain within an image, i.e.
gradients from the context pixels centered on the target pixel are also
important to the stabilization. For that, in this paper, we propose a novel
method named Spatial Momentum Iterative FGSM Attack (SMI-FGSM), which
introduces the mechanism of momentum accumulation from temporal domain to
spatial domain by considering the context gradient information from different
regions within the image. SMI-FGSM is then integrated with MI-FGSM to
simultaneously stabilize the gradients' update direction from both the temporal
and spatial domain. The final method is called SM$^2$I-FGSM. Extensive
experiments are conducted on the ImageNet dataset and results show that
SM$^2$I-FGSM indeed further enhances the transferability. It achieves the best
transferability success rate for multiple mainstream undefended and defended
models, which outperforms the state-of-the-art methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polarization Multiplexed Diffractive Computing: All-Optical Implementation of a Group of Linear Transformations Through a Polarization-Encoded Diffractive Network. (arXiv:2203.13482v1 [physics.optics])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13482">
<div class="article-summary-box-inner">
<span><p>Research on optical computing has recently attracted significant attention
due to the transformative advances in machine learning. Among different
approaches, diffractive optical networks composed of spatially-engineered
transmissive surfaces have been demonstrated for all-optical statistical
inference and performing arbitrary linear transformations using passive,
free-space optical layers. Here, we introduce a polarization multiplexed
diffractive processor to all-optically perform multiple, arbitrarily-selected
linear transformations through a single diffractive network trained using deep
learning. In this framework, an array of pre-selected linear polarizers is
positioned between trainable transmissive diffractive materials that are
isotropic, and different target linear transformations (complex-valued) are
uniquely assigned to different combinations of input/output polarization
states. The transmission layers of this polarization multiplexed diffractive
network are trained and optimized via deep learning and error-backpropagation
by using thousands of examples of the input/output fields corresponding to each
one of the complex-valued linear transformations assigned to different
input/output polarization combinations. Our results and analysis reveal that a
single diffractive network can successfully approximate and all-optically
implement a group of arbitrarily-selected target transformations with a
negligible error when the number of trainable diffractive features/neurons (N)
approaches N_p x N_i x N_o, where N_i and N_o represent the number of pixels at
the input and output fields-of-view, respectively, and N_p refers to the number
of unique linear transformations assigned to different input/output
polarization combinations. This polarization-multiplexed all-optical
diffractive processor can find various applications in optical computing and
polarization-based machine vision tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compare learning: bi-attention network for few-shot learning. (arXiv:2203.13487v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13487">
<div class="article-summary-box-inner">
<span><p>Learning with few labeled data is a key challenge for visual recognition, as
deep neural networks tend to overfit using a few samples only. One of the
Few-shot learning methods called metric learning addresses this challenge by
first learning a deep distance metric to determine whether a pair of images
belong to the same category, then applying the trained metric to instances from
other test set with limited labels. This method makes the most of the few
samples and limits the overfitting effectively. However, extant metric networks
usually employ Linear classifiers or Convolutional neural networks (CNN) that
are not precise enough to globally capture the subtle differences between
vectors. In this paper, we propose a novel approach named Bi-attention network
to compare the instances, which can measure the similarity between embeddings
of instances precisely, globally and efficiently. We verify the effectiveness
of our model on two benchmarks. Experiments show that our approach achieved
improved accuracy and convergence speed over baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive learning of Class-agnostic Activation Map for Weakly Supervised Object Localization and Semantic Segmentation. (arXiv:2203.13505v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13505">
<div class="article-summary-box-inner">
<span><p>While class activation map (CAM) generated by image classification network
has been widely used for weakly supervised object localization (WSOL) and
semantic segmentation (WSSS), such classifiers usually focus on discriminative
object regions. In this paper, we propose Contrastive learning for
Class-agnostic Activation Map (C$^2$AM) generation only using unlabeled image
data, without the involvement of image-level supervision. The core idea comes
from the observation that i) semantic information of foreground objects usually
differs from their backgrounds; ii) foreground objects with similar appearance
or background with similar color/texture have similar representations in the
feature space. We form the positive and negative pairs based on the above
relations and force the network to disentangle foreground and background with a
class-agnostic activation map using a novel contrastive loss. As the network is
guided to discriminate cross-image foreground-background, the class-agnostic
activation maps learned by our approach generate more complete object regions.
We successfully extracted from C$^2$AM class-agnostic object bounding boxes for
object localization and background cues to refine CAM generated by
classification network for semantic segmentation. Extensive experiments on
CUB-200-2011, ImageNet-1K, and PASCAL VOC2012 datasets show that both WSOL and
WSSS can benefit from the proposed C$^2$AM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of the Production Strategy of Mask Types in the COVID-19 Environment. (arXiv:2203.13506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13506">
<div class="article-summary-box-inner">
<span><p>Since the outbreak of the COVID-19 in December 2019, medical protective
equipment such as disposable medical masks and KN95 masks have become essential
resources for the public. Enterprises in all sectors of society have also
transformed the production of medical masks. After the outbreak, how to choose
the right time to produce medical protective masks, and what type of medical
masks to produce will play a positive role in preventing and controlling the
epidemic in a short time. In this regard, the evolutionary game competition
analysis will be conducted through the relevant data of disposable medical
masks and KN95 masks to determine the appropriate nodes for the production of
corresponding mask types. After the research and analysis of the production
strategy of mask types, it has a positive effect on how to guide the resumption
of work and production.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Pre-training Based on Graph Attention Network for Document Understanding. (arXiv:2203.13530v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13530">
<div class="article-summary-box-inner">
<span><p>Document intelligence as a relatively new research topic supports many
business applications. Its main task is to automatically read, understand, and
analyze documents. However, due to the diversity of formats (invoices, reports,
forms, etc.) and layouts in documents, it is difficult to make machines
understand documents. In this paper, we present the GraphDoc, a multimodal
graph attention-based model for various document understanding tasks. GraphDoc
is pre-trained in a multimodal framework by utilizing text, layout, and image
information simultaneously. In a document, a text block relies heavily on its
surrounding contexts, so we inject the graph structure into the attention
mechanism to form a graph attention layer so that each input node can only
attend to its neighborhoods. The input nodes of each graph attention layer are
composed of textual, visual, and positional features from semantically
meaningful regions in a document image. We do the multimodal feature fusion of
each node by the gate fusion layer. The contextualization between each node is
modeled by the graph attention layer. GraphDoc learns a generic representation
from only 320k unlabeled documents via the Masked Sentence Modeling task.
Extensive experimental results on the publicly available datasets show that
GraphDoc achieves state-of-the-art performance, which demonstrates the
effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Performance Transformer Tracking. (arXiv:2203.13533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13533">
<div class="article-summary-box-inner">
<span><p>Correlation has a critical role in the tracking field, especially in recent
popular Siamese-based trackers. The correlation operation is a simple fusion
manner to consider the similarity between the template and the search region.
However, the correlation operation is a local linear matching process, losing
semantic information and falling into local optimum easily, which may be the
bottleneck of designing high-accuracy tracking algorithms. In this work, to
determine whether a better feature fusion method exists than correlation, a
novel attention-based feature fusion network, inspired by Transformer, is
presented. This network effectively combines the template and the search region
features using attention. Specifically, the proposed method includes an
ego-context augment module based on self-attention and a cross-feature augment
module based on cross-attention. First, we present a Transformer tracking
(named TransT) method based on the Siamese-like feature extraction backbone,
the designed attention-based fusion mechanism, and the classification and
regression head. Based on the TransT baseline, we further design a segmentation
branch to generate an accurate mask. Finally, we propose a stronger version of
TransT by extending TransT with a multi-template design and an IoU prediction
head, named TransT-M. Experiments show that our TransT and TransT-M methods
achieve promising results on seven popular datasets. Code and models are
available at https://github.com/chenxin-dlut/TransT-M.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeCo: Separating Unknown Musical Visual Sounds with Consistency Guidance. (arXiv:2203.13535v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13535">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the success of deep learning on the visual sound
separation task. However, existing works follow similar settings where the
training and testing datasets share the same musical instrument categories,
which to some extent limits the versatility of this task. In this work, we
focus on a more general and challenging scenario, namely the separation of
unknown musical instruments, where the categories in training and testing
phases have no overlap with each other. To tackle this new setting, we propose
the Separation-with-Consistency (SeCo) framework, which can accomplish the
separation on unknown categories by exploiting the consistency constraints.
Furthermore, to capture richer characteristics of the novel melodies, we devise
an online matching strategy, which can bring stable enhancements with no cost
of extra parameters. Experiments demonstrate that our SeCo framework exhibits
strong adaptation ability on the novel musical categories and outperforms the
baseline methods by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Visual Tracking via Hierarchical Cross-Attention Transformer. (arXiv:2203.13537v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13537">
<div class="article-summary-box-inner">
<span><p>In recent years, target tracking has made great progress in accuracy. This
development is mainly attributed to powerful networks (such as transformers)
and additional modules (such as online update and refinement modules). However,
less attention has been paid to tracking speed. Most state-of-the-art trackers
are satisfied with the real-time speed on powerful GPUs. However, practical
applications necessitate higher requirements for tracking speed, especially
when edge platforms with limited resources are used. In this work, we present
an efficient tracking method via a hierarchical cross-attention transformer
named HCAT. Our model runs about 195 fps on GPU, 45 fps on CPU, and 55 fps on
the edge AI platform of NVidia Jetson AGX Xavier. Experiments show that our
HCAT achieves promising results on LaSOT, GOT-10k, TrackingNet, NFS, OTB100,
UAV123, and VOT2020. Code and models are available at
https://github.com/chenxin-dlut/HCAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformable Butterfly: A Highly Structured and Sparse Linear Transform. (arXiv:2203.13556v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13556">
<div class="article-summary-box-inner">
<span><p>We introduce a new kind of linear transform named Deformable Butterfly
(DeBut) that generalizes the conventional butterfly matrices and can be adapted
to various input-output dimensions. It inherits the fine-to-coarse-grained
learnable hierarchy of traditional butterflies and when deployed to neural
networks, the prominent structures and sparsity in a DeBut layer constitutes a
new way for network compression. We apply DeBut as a drop-in replacement of
standard fully connected and convolutional layers, and demonstrate its
superiority in homogenizing a neural network and rendering it favorable
properties such as light weight and low inference complexity, without
compromising accuracy. The natural complexity-accuracy tradeoff arising from
the myriad deformations of a DeBut layer also opens up new rooms for analytical
and practical research. The codes and Appendix are publicly available at:
https://github.com/ruilin0212/DeBut.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Networks with Divisive normalization for image segmentation with application in cityscapes dataset. (arXiv:2203.13558v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13558">
<div class="article-summary-box-inner">
<span><p>One of the key problems in computer vision is adaptation: models are too
rigid to follow the variability of the inputs. The canonical computation that
explains adaptation in sensory neuroscience is divisive normalization, and it
has appealing effects on image manifolds. In this work we show that including
divisive normalization in current deep networks makes them more invariant to
non-informative changes in the images. In particular, we focus on U-Net
architectures for image segmentation. Experiments show that the inclusion of
divisive normalization in the U-Net architecture leads to better segmentation
results with respect to conventional U-Net. The gain increases steadily when
dealing with images acquired in bad weather conditions. In addition to the
results on the Cityscapes and Foggy Cityscapes datasets, we explain these
advantages through visualization of the responses: the equalization induced by
the divisive normalization leads to more invariant features to local changes in
contrast and illumination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Visual Navigation Perspective for Category-Level Object Pose Estimation. (arXiv:2203.13572v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13572">
<div class="article-summary-box-inner">
<span><p>This paper studies category-level object pose estimation based on a single
monocular image. Recent advances in pose-aware generative models have paved the
way for addressing this challenging task using analysis-by-synthesis. The idea
is to sequentially update a set of latent variables, e.g., pose, shape, and
appearance, of the generative model until the generated image best agrees with
the observation. However, convergence and efficiency are two challenges of this
inference procedure. In this paper, we take a deeper look at the inference of
analysis-by-synthesis from the perspective of visual navigation, and
investigate what is a good navigation policy for this specific task. We
evaluate three different strategies, including gradient descent, reinforcement
learning and imitation learning, via thorough comparisons in terms of
convergence, robustness and efficiency. Moreover, we show that a simple hybrid
approach leads to an effective and efficient solution. We further compare these
strategies to state-of-the-art methods, and demonstrate superior performance on
synthetic and real-world datasets leveraging off-the-shelf pose-aware
generative models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Test-Time Domain Adaptation. (arXiv:2203.13591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13591">
<div class="article-summary-box-inner">
<span><p>Test-time domain adaptation aims to adapt a source pre-trained model to a
target domain without using any source data. Existing works mainly consider the
case where the target domain is static. However, real-world machine perception
systems are running in non-stationary and continually changing environments
where the target domain distribution can change over time. Existing methods,
which are mostly based on self-training and entropy regularization, can suffer
from these non-stationary environments. Due to the distribution shift over time
in the target domain, pseudo-labels become unreliable. The noisy pseudo-labels
can further lead to error accumulation and catastrophic forgetting. To tackle
these issues, we propose a continual test-time adaptation approach~(CoTTA)
which comprises two parts. Firstly, we propose to reduce the error accumulation
by using weight-averaged and augmentation-averaged predictions which are often
more accurate. On the other hand, to avoid catastrophic forgetting, we propose
to stochastically restore a small part of the neurons to the source pre-trained
weights during each iteration to help preserve source knowledge in the
long-term. The proposed method enables the long-term adaptation for all
parameters in the network. CoTTA is easy to implement and can be readily
incorporated in off-the-shelf pre-trained models. We demonstrate the
effectiveness of our approach on four classification tasks and a segmentation
task for continual test-time adaptation, on which we outperform existing
methods. Our code is available at \url{https://qin.ee/cotta}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Hybrid Image Retargeting. (arXiv:2203.13595v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13595">
<div class="article-summary-box-inner">
<span><p>Image retargeting changes the aspect ratio of images while aiming to preserve
content and minimise noticeable distortion. Fast and high-quality methods are
particularly relevant at present, due to the large variety of image and display
aspect ratios. We propose a retargeting method that quantifies and limits
warping distortions with the use of content-aware cropping. The pipeline of the
proposed approach consists of the following steps. First, an importance map of
a source image is generated using deep semantic segmentation and saliency
detection models. Then, a preliminary warping mesh is computed using axis
aligned deformations, enhanced with the use of a distortion measure to ensure
low warping deformations. Finally, the retargeted image is produced using a
content-aware cropping algorithm. In order to evaluate our method, we perform a
user study based on the RetargetMe benchmark. Experimental analyses show that
our method outperforms recent approaches, while running in a fraction of their
execution time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Navigable Proximity Graph-Driven Native Hybrid Queries with Structured and Unstructured Constraints. (arXiv:2203.13601v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13601">
<div class="article-summary-box-inner">
<span><p>As research interest surges, vector similarity search is applied in multiple
fields, including data mining, computer vision, and information retrieval.
{Given a set of objects (e.g., a set of images) and a query object, we can
easily transform each object into a feature vector and apply the vector
similarity search to retrieve the most similar objects. However, the original
vector similarity search cannot well support \textit{hybrid queries}, where
users not only input unstructured query constraint (i.e., the feature vector of
query object) but also structured query constraint (i.e., the desired
attributes of interest). Hybrid query processing aims at identifying these
objects with similar feature vectors to query object and satisfying the given
attribute constraints. Recent efforts have attempted to answer a hybrid query
by performing attribute filtering and vector similarity search separately and
then merging the results later, which limits efficiency and accuracy because
they are not purpose-built for hybrid queries.} In this paper, we propose a
native hybrid query (NHQ) framework based on proximity graph (PG), which
provides the specialized \textit{composite index and joint pruning} modules for
hybrid queries. We easily deploy existing various PGs on this framework to
process hybrid queries efficiently. Moreover, we present two novel navigable
PGs (NPGs) with optimized edge selection and routing strategies, which obtain
better overall performance than existing PGs. After that, we deploy the
proposed NPGs in NHQ to form two hybrid query methods, which significantly
outperform the state-of-the-art competitors on all experimental datasets
(10$\times$ faster under the same \textit{Recall}), including eight public and
one in-house real-world datasets. Our code and datasets have been released at
\url{https://github.com/AshenOn3/NHQ}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rope3D: TheRoadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task. (arXiv:2203.13608v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13608">
<div class="article-summary-box-inner">
<span><p>Concurrent perception datasets for autonomous driving are mainly limited to
frontal view with sensors mounted on the vehicle. None of them is designed for
the overlooked roadside perception tasks. On the other hand, the data captured
from roadside cameras have strengths over frontal-view data, which is believed
to facilitate a safer and more intelligent autonomous driving system. To
accelerate the progress of roadside perception, we present the first
high-diversity challenging Roadside Perception 3D dataset- Rope3D from a novel
view. The dataset consists of 50k images and over 1.5M 3D objects in various
scenes, which are captured under different settings including various cameras
with ambiguous mounting positions, camera specifications, viewpoints, and
different environmental conditions. We conduct strict 2D-3D joint annotation
and comprehensive data analysis, as well as set up a new 3D roadside perception
benchmark with metrics and evaluation devkit. Furthermore, we tailor the
existing frontal-view monocular 3D object detection approaches and propose to
leverage the geometry constraint to solve the inherent ambiguities caused by
various sensors, viewpoints. Our dataset is available on
https://thudair.baai.ac.cn/rope.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Pre-training for Temporal Action Localization Tasks. (arXiv:2203.13609v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13609">
<div class="article-summary-box-inner">
<span><p>Unsupervised video representation learning has made remarkable achievements
in recent years. However, most existing methods are designed and optimized for
video classification. These pre-trained models can be sub-optimal for temporal
localization tasks due to the inherent discrepancy between video-level
classification and clip-level localization. To bridge this gap, we make the
first attempt to propose a self-supervised pretext task, coined as Pseudo
Action Localization (PAL) to Unsupervisedly Pre-train feature encoders for
Temporal Action Localization tasks (UP-TAL). Specifically, we first randomly
select temporal regions, each of which contains multiple clips, from one video
as pseudo actions and then paste them onto different temporal positions of the
other two videos. The pretext task is to align the features of pasted pseudo
action regions from two synthetic videos and maximize the agreement between
them. Compared to the existing unsupervised video representation learning
approaches, our PAL adapts better to downstream TAL tasks by introducing a
temporal equivariant contrastive learning paradigm in a temporally dense and
scale-aware manner. Extensive experiments show that PAL can utilize large-scale
unlabeled video data to significantly boost the performance of existing TAL
methods. Our codes and models will be made publicly available at
https://github.com/zhang-can/UP-TAL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Adapt to Unseen Abnormal Activities under Weak Supervision. (arXiv:2203.13610v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13610">
<div class="article-summary-box-inner">
<span><p>We present a meta-learning framework for weakly supervised anomaly detection
in videos, where the detector learns to adapt to unseen types of abnormal
activities effectively when only video-level annotations of binary labels are
available. Our work is motivated by the fact that existing methods suffer from
poor generalization to diverse unseen examples. We claim that an anomaly
detector equipped with a meta-learning scheme alleviates the limitation by
leading the model to an initialization point for better optimization. We
evaluate the performance of our framework on two challenging datasets,
UCF-Crime and ShanghaiTech. The experimental results demonstrate that our
algorithm boosts the capability to localize unseen abnormal events in a weakly
supervised setting. Besides the technical contributions, we perform the
annotation of missing labels in the UCF-Crime dataset and make our task
evaluated effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Incremental Learning for Action Recognition in Videos. (arXiv:2203.13611v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13611">
<div class="article-summary-box-inner">
<span><p>We tackle catastrophic forgetting problem in the context of class-incremental
learning for video recognition, which has not been explored actively despite
the popularity of continual learning. Our framework addresses this challenging
task by introducing time-channel importance maps and exploiting the importance
maps for learning the representations of incoming examples via knowledge
distillation. We also incorporate a regularization scheme in our objective
function, which encourages individual features obtained from different time
steps in a video to be uncorrelated and eventually improves accuracy by
alleviating catastrophic forgetting. We evaluate the proposed approach on
brand-new splits of class-incremental action recognition benchmarks constructed
upon the UCF101, HMDB51, and Something-Something V2 datasets, and demonstrate
the effectiveness of our algorithm in comparison to the existing continual
learning methods that are originally designed for image data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Repairing Group-Level Errors for DNNs Using Weighted Regularization. (arXiv:2203.13612v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13612">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) have been widely used in software making
decisions impacting people's lives. However, they have been found to exhibit
severe erroneous behaviors that may lead to unfortunate outcomes. Previous work
shows that such misbehaviors often occur due to class property violations
rather than errors on a single image. Although methods for detecting such
errors have been proposed, fixing them has not been studied so far. Here, we
propose a generic method called Weighted Regularization (WR) consisting of five
concrete methods targeting the error-producing classes to fix the DNNs. In
particular, it can repair confusion error and bias error of DNN models for both
single-label and multi-label image classifications. A confusion error happens
when a given DNN model tends to confuse between two classes. Each method in WR
assigns more weights at a stage of DNN retraining or inference to mitigate the
confusion between target pair. A bias error can be fixed similarly. We evaluate
and compare the proposed methods along with baselines on six widely-used
datasets and architecture combinations. The results suggest that WR methods
have different trade-offs but under each setting at least one WR method can
greatly reduce confusion/bias errors at a very limited cost of the overall
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Graph Convolutional Networks with Topologically Consistent Magnitude Pruning. (arXiv:2203.13616v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13616">
<div class="article-summary-box-inner">
<span><p>Graph convolution networks (GCNs) are currently mainstream in learning with
irregular data. These models rely on message passing and attention mechanisms
that capture context and node-to-node relationships. With multi-head attention,
GCNs become highly accurate but oversized, and their deployment on cheap
devices requires their pruning. However, pruning at high regimes usually leads
to topologically inconsistent networks with weak generalization. In this paper,
we devise a novel method for lightweight GCN design. Our proposed approach
parses and selects subnetworks with the highest magnitudes while guaranteeing
their topological consistency. The latter is obtained by selecting only
accessible and co-accessible connections which actually contribute in the
evaluation of the selected subnetworks. Experiments conducted on the
challenging FPHA dataset show the substantial gain of our topologically
consistent pruning method especially at very high pruning regimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness. (arXiv:2203.13639v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13639">
<div class="article-summary-box-inner">
<span><p>Neural architectures based on attention such as vision transformers are
revolutionizing image recognition. Their main benefit is that attention allows
reasoning about all parts of a scene jointly. In this paper, we show how the
global reasoning of (scaled) dot-product attention can be the source of a major
vulnerability when confronted with adversarial patch attacks. We provide a
theoretical understanding of this vulnerability and relate it to an adversary's
ability to misdirect the attention of all queries to a single key token under
the control of the adversarial patch. We propose novel adversarial objectives
for crafting adversarial patches which target this vulnerability explicitly. We
show the effectiveness of the proposed patch attacks on popular image
classification (ViTs and DeiTs) and object detection models (DETR). We find
that adversarial patches occupying 0.5% of the input can lead to robust
accuracies as low as 0% for ViT on ImageNet, and reduce the mAP of DETR on MS
COCO to less than 3%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StretchBEV: Stretching Future Instance Prediction Spatially and Temporally. (arXiv:2203.13641v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13641">
<div class="article-summary-box-inner">
<span><p>In self-driving, predicting future in terms of location and motion of all the
agents around the vehicle is a crucial requirement for planning. Recently, a
new joint formulation of perception and prediction has emerged by fusing rich
sensory information perceived from multiple cameras into a compact bird's-eye
view representation to perform prediction. However, the quality of future
predictions degrades over time while extending to longer time horizons due to
multiple plausible predictions. In this work, we address this inherent
uncertainty in future predictions with a stochastic temporal model. Our model
learns temporal dynamics in a latent space through stochastic residual updates
at each time step. By sampling from a learned distribution at each time step,
we obtain more diverse future predictions that are also more accurate compared
to previous work, especially stretching both spatially further regions in the
scene and temporally over longer time horizons. Despite separate processing of
each time step, our model is still efficient through decoupling of the learning
of dynamics and the generation of future predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDsrv -- visual sharing and analysis of molecular dynamics simulations. (arXiv:2203.13658v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13658">
<div class="article-summary-box-inner">
<span><p>Molecular dynamics simulation is a proven technique for computing and
visualizing the time-resolved motion of macromolecules at atomic resolution.
The MDsrv is a tool that streams MD trajectories and displays them
interactively in web browsers without requiring advanced skills, facilitating
interactive exploration and collaborative visual analysis. We have now enhanced
the MDsrv to further simplify the upload and sharing of MD trajectories and
improve their online viewing and analysis. With the new instance, the MDsrv
simplifies the creation of sessions, which allows the exchange of MD
trajectories with preset representations and perspectives. An important
innovation is that the MDsrv can now access and visualize trajectories from
remote datasets, which greatly expands its applicability and use, as the data
no longer needs to be accessible on a local server. In addition, initial
analyses such as sequence or structure alignments, distance measurements, or
RMSD calculations have been implemented, which optionally support visual
analysis. Finally, the MDsrv now offers a faster and more efficient
visualization of even large trajectories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adjacent Context Coordination Network for Salient Object Detection in Optical Remote Sensing Images. (arXiv:2203.13664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13664">
<div class="article-summary-box-inner">
<span><p>Salient object detection (SOD) in optical remote sensing images (RSIs), or
RSI-SOD, is an emerging topic in understanding optical RSIs. However, due to
the difference between optical RSIs and natural scene images (NSIs), directly
applying NSI-SOD methods to optical RSIs fails to achieve satisfactory results.
In this paper, we propose a novel Adjacent Context Coordination Network
(ACCoNet) to explore the coordination of adjacent features in an
encoder-decoder architecture for RSI-SOD. Specifically, ACCoNet consists of
three parts: an encoder, Adjacent Context Coordination Modules (ACCoMs), and a
decoder. As the key component of ACCoNet, ACCoM activates the salient regions
of output features of the encoder and transmits them to the decoder. ACCoM
contains a local branch and two adjacent branches to coordinate the multi-level
features simultaneously. The local branch highlights the salient regions in an
adaptive way, while the adjacent branches introduce global information of
adjacent levels to enhance salient regions. Additionally, to extend the
capabilities of the classic decoder block (i.e., several cascaded convolutional
layers), we extend it with two bifurcations and propose a
Bifurcation-Aggregation Block to capture the contextual information in the
decoder. Extensive experiments on two benchmark datasets demonstrate that the
proposed ACCoNet outperforms 22 state-of-the-art methods under nine evaluation
metrics, and runs up to 81 fps on a single NVIDIA Titan X GPU. The code and
results of our method are available at https://github.com/MathLee/ACCoNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Continuous-Time Optical Flow from Events and Frames. (arXiv:2203.13674v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13674">
<div class="article-summary-box-inner">
<span><p>We present a method for estimating dense continuous-time optical flow.
Traditional dense optical flow methods compute the pixel displacement between
two images. Due to missing information, these approaches cannot recover the
pixel trajectories in the blind time between two images. In this work, we show
that it is possible to compute per-pixel, continuous-time optical flow by
additionally using events from an event camera. Events provide temporally
fine-grained information about movement in image space due to their
asynchronous nature and microsecond response time. We leverage these benefits
to predict pixel trajectories densely in continuous-time via parameterized
B\'ezier curves. To achieve this, we introduce multiple innovations to build a
neural network with strong inductive biases for this task: First, we build
multiple sequential correlation volumes in time using event data. Second, we
use B\'ezier curves to index these correlation volumes at multiple timestamps
along the trajectory. Third, we use the retrieved correlation to update the
B\'ezier curve representations iteratively. Our method can optionally include
image pairs to boost performance further. The proposed approach outperforms
existing image-based and event-based methods by 11.5 % lower EPE on DSEC-Flow.
Finally, we introduce a novel synthetic dataset MultiFlow for pixel trajectory
regression on which our method is currently the only successful approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the performance of preconditioned methods to solve \(L^p\)-norm phase unwrapping. (arXiv:2203.13675v1 [math.NA])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13675">
<div class="article-summary-box-inner">
<span><p>In this paper, we analyze and evaluate suitable preconditioning techniques to
improve the performance of the $L^p$-norm phase unwrapping method. We consider
five preconditioning techniques commonly found in the literature, and analyze
their performance with different sizes of wrapped-phase maps. Keywords.- Phase
unwrapping, $L^p$-norm based method, Preconditioning techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ST-FL: Style Transfer Preprocessing in Federated Learning for COVID-19 Segmentation. (arXiv:2203.13680v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13680">
<div class="article-summary-box-inner">
<span><p>Chest Computational Tomography (CT) scans present low cost, speed and
objectivity for COVID-19 diagnosis and deep learning methods have shown great
promise in assisting the analysis and interpretation of these images. Most
hospitals or countries can train their own models using in-house data, however
empirical evidence shows that those models perform poorly when tested on new
unseen cases, surfacing the need for coordinated global collaboration. Due to
privacy regulations, medical data sharing between hospitals and nations is
extremely difficult. We propose a GAN-augmented federated learning model,
dubbed ST-FL (Style Transfer Federated Learning), for COVID-19 image
segmentation. Federated learning (FL) permits a centralised model to be learned
in a secure manner from heterogeneous datasets located in disparate private
data silos. We demonstrate that the widely varying data quality on FL client
nodes leads to a sub-optimal centralised FL model for COVID-19 chest CT image
segmentation. ST-FL is a novel FL framework that is robust in the face of
highly variable data quality at client nodes. The robustness is achieved by a
denoising CycleGAN model at each client of the federation that maps arbitrary
quality images into the same target quality, counteracting the severe data
variability evident in real-world FL use-cases. Each client is provided with
the target style, which is the same for all clients, and trains their own
denoiser. Our qualitative and quantitative results suggest that this FL model
performs comparably to, and in some cases better than, a model that has
centralised access to all the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Satellite Infrastructure/Mission Tradeoffs. (arXiv:2203.13686v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13686">
<div class="article-summary-box-inner">
<span><p>If a unit cannot receive intelligence from a source due to external factors,
we consider them disadvantaged users. We categorize this as a preoccupied unit
working on a low connectivity device on the edge. This case requires that we
use a different approach to deliver intelligence, particularly satellite
imagery information, than normally employed. To address this, we propose a
survey of information reduction techniques to deliver the information from a
satellite image in a smaller package. We investigate four techniques to aid in
the reduction of delivered information: traditional image compression, neural
network image compression, object detection image cutout, and image to caption.
Each of these mechanisms have their benefits and tradeoffs when considered for
a disadvantaged user.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The TerraByte Client: providing access to terabytes of plant data. (arXiv:2203.13691v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13691">
<div class="article-summary-box-inner">
<span><p>In this paper we demonstrate the TerraByte Client, a software to download
user-defined plant datasets from a data portal hosted at Compute Canada. To
that end the client offers two key functionalities: (1) It allows the user to
get an overview on what data is available and a quick way to visually check
samples of that data. For this the client receives the results of queries to a
database and displays the number of images that fulfill the search criteria.
Furthermore, a sample can be downloaded within seconds to confirm that the data
suits the user's needs. (2) The user can then download the specified data to
their own drive. This data is prepared into chunks server-side and sent to the
user's end-system, where it is automatically extracted into individual files.
The first chunks of data are available for inspection after a brief waiting
period of a minute or less depending on available bandwidth and type of data.
The TerraByte Client has a full graphical user interface for easy usage and
uses end-to-end encryption. The user interface is built on top of a low-level
client. This architecture in combination of offering the client program
open-source makes it possible for the user to develop their own user interface
or use the client's functionality directly. An example for direct usage could
be to download specific data on demand within a larger application, such as
training machine learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Neural Representations for Variable Length Human Motion Generation. (arXiv:2203.13694v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13694">
<div class="article-summary-box-inner">
<span><p>We propose an action-conditional human motion generation method using
variational implicit neural representations (INR). The variational formalism
enables action-conditional distributions of INRs, from which one can easily
sample representations to generate novel human motion sequences. Our method
offers variable-length sequence generation by construction because a part of
INR is optimized for a whole sequence of arbitrary length with temporal
embeddings. In contrast, previous works reported difficulties with modeling
variable-length sequences. We confirm that our method with a Transformer
decoder outperforms all relevant methods on HumanAct12, NTU-RGBD, and UESTC
datasets in terms of realism and diversity of generated motions. Surprisingly,
even our method with an MLP decoder consistently outperforms the
state-of-the-art Transformer-based auto-encoder. In particular, we show that
variable-length motions generated by our method are better than fixed-length
motions generated by the state-of-the-art method in terms of realism and
diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Image Deraining: Optimization Model Driven Deep CNN. (arXiv:2203.13699v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13699">
<div class="article-summary-box-inner">
<span><p>The deep convolutional neural network has achieved significant progress for
single image rain streak removal. However, most of the data-driven learning
methods are full-supervised or semi-supervised, unexpectedly suffering from
significant performance drops when dealing with real rain. These data-driven
learning methods are representative yet generalize poor for real rain. The
opposite holds true for the model-driven unsupervised optimization methods. To
overcome these problems, we propose a unified unsupervised learning framework
which inherits the generalization and representation merits for real rain
removal. Specifically, we first discover a simple yet important domain
knowledge that directional rain streak is anisotropic while the natural clean
image is isotropic, and formulate the structural discrepancy into the energy
function of the optimization model. Consequently, we design an optimization
model-driven deep CNN in which the unsupervised loss function of the
optimization model is enforced on the proposed network for better
generalization. In addition, the architecture of the network mimics the main
role of the optimization models with better feature representation. On one
hand, we take advantage of the deep network to improve the representation. On
the other hand, we utilize the unsupervised loss of the optimization model for
better generalization. Overall, the unsupervised learning framework achieves
good generalization and representation: unsupervised training (loss) with only
a few real rainy images (input) and physical meaning network (architecture).
Extensive experiments on synthetic and real-world rain datasets show the
superiority of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clustering Aided Weakly Supervised Training to Detect Anomalous Events in Surveillance Videos. (arXiv:2203.13704v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13704">
<div class="article-summary-box-inner">
<span><p>Formulating learning systems for the detection of real-world anomalous events
using only video-level labels is a challenging task mainly due to the presence
of noisy labels as well as the rare occurrence of anomalous events in the
training data. We propose a weakly supervised anomaly detection system which
has multiple contributions including a random batch selection mechanism to
reduce inter-batch correlation and a normalcy suppression block which learns to
minimize anomaly scores over normal regions of a video by utilizing the overall
information available in a training batch. In addition, a clustering loss block
is proposed to mitigate the label noise and to improve the representation
learning for the anomalous and normal regions. This block encourages the
backbone network to produce two distinct feature clusters representing normal
and anomalous events. Extensive analysis of the proposed approach is provided
using three popular anomaly detection datasets including UCF-Crime,
ShanghaiTech, and UCSD Ped2. The experiments demonstrate a superior anomaly
detection capability of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for Network Width with Bilaterally Coupled Network. (arXiv:2203.13714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13714">
<div class="article-summary-box-inner">
<span><p>Searching for a more compact network width recently serves as an effective
way of channel pruning for the deployment of convolutional neural networks
(CNNs) under hardware constraints. To fulfill the searching, a one-shot
supernet is usually leveraged to efficiently evaluate the performance
\wrt~different network widths. However, current methods mainly follow a
\textit{unilaterally augmented} (UA) principle for the evaluation of each
width, which induces the training unfairness of channels in supernet. In this
paper, we introduce a new supernet called Bilaterally Coupled Network (BCNet)
to address this issue. In BCNet, each channel is fairly trained and responsible
for the same amount of network widths, thus each network width can be evaluated
more accurately. Besides, we propose to reduce the redundant search space and
present the BCNetV2 as the enhanced supernet to ensure rigorous training
fairness over channels. Furthermore, we leverage a stochastic complementary
strategy for training the BCNet, and propose a prior initial population
sampling method to boost the performance of the evolutionary search. We also
propose the first open-source width benchmark on macro structures named
Channel-Bench-Macro for the better comparison of width search algorithms.
Extensive experiments on benchmark CIFAR-10 and ImageNet datasets indicate that
our method can achieve state-of-the-art or competing performance over other
baseline methods. Moreover, our method turns out to further boost the
performance of NAS models by refining their network widths. For example, with
the same FLOPs budget, our obtained EfficientNet-B0 achieves 77.53\% Top-1
accuracy on ImageNet dataset, surpassing the performance of original setting by
0.65\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stabilizing Adversarially Learned One-Class Novelty Detection Using Pseudo Anomalies. (arXiv:2203.13716v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13716">
<div class="article-summary-box-inner">
<span><p>Recently, anomaly scores have been formulated using reconstruction loss of
the adversarially learned generators and/or classification loss of
discriminators. Unavailability of anomaly examples in the training data makes
optimization of such networks challenging. Attributed to the adversarial
training, performance of such models fluctuates drastically with each training
step, making it difficult to halt the training at an optimal point. In the
current study, we propose a robust anomaly detection framework that overcomes
such instability by transforming the fundamental role of the discriminator from
identifying real vs. fake data to distinguishing good vs. bad quality
reconstructions. For this purpose, we propose a method that utilizes the
current state as well as an old state of the same generator to create good and
bad quality reconstruction examples. The discriminator is trained on these
examples to detect the subtle distortions that are often present in the
reconstructions of anomalous data. In addition, we propose an efficient generic
criterion to stop the training of our model, ensuring elevated performance.
Extensive experiments performed on six datasets across multiple domains
including image and video based anomaly detection, medical diagnosis, and
network security, have demonstrated excellent performance of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Digital Fingerprinting of Microstructures. (arXiv:2203.13718v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13718">
<div class="article-summary-box-inner">
<span><p>Finding efficient means of fingerprinting microstructural information is a
critical step towards harnessing data-centric machine learning approaches. A
statistical framework is systematically developed for compressed
characterisation of a population of images, which includes some classical
computer vision methods as special cases. The focus is on materials
microstructure. The ultimate purpose is to rapidly fingerprint sample images in
the context of various high-throughput design/make/test scenarios. This
includes, but is not limited to, quantification of the disparity between
microstructures for quality control, classifying microstructures, predicting
materials properties from image data and identifying potential processing
routes to engineer new materials with specific properties. Here, we consider
microstructure classification and utilise the resulting features over a range
of related machine learning tasks, namely supervised, semi-supervised, and
unsupervised learning.
</p>
<p>The approach is applied to two distinct datasets to illustrate various
aspects and some recommendations are made based on the findings. In particular,
methods that leverage transfer learning with convolutional neural networks
(CNNs), pretrained on the ImageNet dataset, are generally shown to outperform
other methods. Additionally, dimensionality reduction of these CNN-based
fingerprints is shown to have negligible impact on classification accuracy for
the supervised learning approaches considered. In situations where there is a
large dataset with only a handful of images labelled, graph-based label
propagation to unlabelled data is shown to be favourable over discarding
unlabelled data and performing supervised learning. In particular, label
propagation by Poisson learning is shown to be highly effective at low label
rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salt Detection Using Segmentation of Seismic Image. (arXiv:2203.13721v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13721">
<div class="article-summary-box-inner">
<span><p>In this project, a state-of-the-art deep convolution neural network (DCNN) is
presented to segment seismic images for salt detection below the earth's
surface. Detection of salt location is very important for starting mining.
Hence, a seismic image is used to detect the exact salt location under the
earth's surface. However, precisely detecting the exact location of salt
deposits is difficult. Therefore, professional seismic imaging still requires
expert human interpretation of salt bodies. This leads to very subjective,
highly variable renderings. Hence, to create the most accurate seismic images
and 3D renderings, we need a robust algorithm that automatically and accurately
identifies if a surface target is a salt or not. Since the performance of DCNN
is well-known and well-established for object recognition in images, DCNN is a
very good choice for this particular problem and being successfully applied to
a dataset of seismic images in which each pixel is labeled as salt or not. The
result of this algorithm is promising.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FReSCO: Flow Reconstruction and Segmentation for low latency Cardiac Output monitoring using deep artifact suppression and segmentation. (arXiv:2203.13729v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13729">
<div class="article-summary-box-inner">
<span><p>Purpose: Real-time monitoring of cardiac output (CO) requires low latency
reconstruction and segmentation of real-time phase contrast MR (PCMR), which
has previously been difficult to perform. Here we propose a deep learning
framework for 'Flow Reconstruction and Segmentation for low latency Cardiac
Output monitoring' (FReSCO).
</p>
<p>Methods: Deep artifact suppression and segmentation U-Nets were independently
trained. Breath hold spiral PCMR data (n=516) was synthetically undersampled
using a variable density spiral sampling pattern and gridded to create aliased
data for training of the artifact suppression U-net. A subset of the data
(n=96) was segmented and used to train the segmentation U-net. Real-time spiral
PCMR was prospectively acquired and then reconstructed and segmented using the
trained models (FReSCO) at low latency at the scanner in 10 healthy subjects
during rest, exercise and recovery periods. CO obtained via FReSCO was compared
to a reference rest CO and rest and exercise Compressed Sensing (CS) CO.
</p>
<p>Results: FReSCO was demonstrated prospectively at the scanner. Beat-to-beat
heartrate, stroke volume and CO could be visualized with a mean latency of
622ms. No significant differences were noted when compared to reference at rest
(Bias = -0.21+-0.50 L/min, p=0.246) or CS at peak exercise (Bias=0.12+-0.48
L/min, p=0.458).
</p>
<p>Conclusion: FReSCO was successfully demonstrated for real-time monitoring of
CO during exercise and could provide a convenient tool for assessment of the
hemodynamic response to a range of stressors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient-VDVAE: Less is more. (arXiv:2203.13751v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13751">
<div class="article-summary-box-inner">
<span><p>Hierarchical VAEs have emerged in recent years as a reliable option for
maximum likelihood estimation. However, instability issues and demanding
computational requirements have hindered research progress in the area. We
present simple modifications to the Very Deep VAE to make it converge up to
$2.6\times$ faster, save up to $20\times$ in memory load and improve stability
during training. Despite these changes, our models achieve comparable or better
negative log-likelihood performance than current state-of-the-art models on all
$7$ commonly used image datasets we evaluated on. We also make an argument
against using 5-bit benchmarks as a way to measure hierarchical VAE's
performance due to undesirable biases caused by the 5-bit quantization.
Additionally, we empirically demonstrate that roughly $3\%$ of the hierarchical
VAE's latent space dimensions is sufficient to encode most of the image
information, without loss of performance, opening up the doors to efficiently
leverage the hierarchical VAEs' latent space in downstream tasks. We release
our source code and models at https://github.com/Rayhane-mamah/Efficient-VDVAE .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of the use of color and its emotional relationship in visual creations based on experiences during the context of the COVID-19 pandemic. (arXiv:2203.13770v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13770">
<div class="article-summary-box-inner">
<span><p>Color is a complex communicative element that helps us understand and
evaluate our environment. At the level of artistic creation, this component
influences both the formal aspects of the composition and the symbolic weight,
directly affecting the construction and transmission of the message that you
want to communicate, creating a specific emotional reaction. During the
COVID-19 pandemic, people generated countless images transmitting this event's
subjective experiences. Using the repository of images created in the Instagram
account CAM (The COVID Art Museum), we propose a methodology to understand the
use of color and its emotional relationship in this context. The process
considers two stages in parallel that are then combined. First, emotions are
extracted and classified from the CAM dataset images through a convolutional
neural network. Second, we extract the colors and their harmonies through a
clustering process. Once both processes are completed, we combine the results
generating an expanded discussion on the usage of color, harmonies, and
emotion. The results indicate that warm colors are prevalent in the sample,
with a preference for analog compositions over complementary ones. The
relationship between emotions and these compositions shows a trend in positive
emotions, reinforced by the results of the algorithm a priori and the emotional
relationship analysis of the attributes of color (hue, chroma, and lighting).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion. (arXiv:2203.13777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13777">
<div class="article-summary-box-inner">
<span><p>Human behavior has the nature of indeterminacy, which requires the pedestrian
trajectory prediction system to model the multi-modality of future motion
states. Unlike existing stochastic trajectory prediction methods which usually
use a latent variable to represent multi-modality, we explicitly simulate the
process of human motion variation from indeterminate to determinate. In this
paper, we present a new framework to formulate the trajectory prediction task
as a reverse process of motion indeterminacy diffusion (MID), in which we
progressively discard indeterminacy from all the walkable areas until reaching
the desired trajectory. This process is learned with a parameterized Markov
chain conditioned by the observed trajectories. We can adjust the length of the
chain to control the degree of indeterminacy and balance the diversity and
determinacy of the predictions. Specifically, we encode the history behavior
information and the social interactions as a state embedding and devise a
Transformer-based diffusion model to capture the temporal dependencies of
trajectories. Extensive experiments on the human trajectory prediction
benchmarks including the Stanford Drone and ETH/UCY datasets demonstrate the
superiority of our method. Code is available at
https://github.com/gutianpei/MID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual-based Safe Landing for UAVs in Populated Areas: Real-time Validation in Virtual Environments. (arXiv:2203.13792v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13792">
<div class="article-summary-box-inner">
<span><p>Safe autonomous landing for Unmanned Aerial Vehicles (UAVs) in populated
areas is a crucial aspect for successful urban deployment, particularly in
emergency landing situations. Nonetheless, validating autonomous landing in
real scenarios is a challenging task involving a high risk of injuring people.
In this work, we propose a framework for real-time safe and thorough evaluation
of vision-based autonomous landing in populated scenarios, using
photo-realistic virtual environments. We propose to use the Unreal graphics
engine coupled with the AirSim plugin for drone's simulation, and evaluate
autonomous landing strategies based on visual detection of Safe Landing Zones
(SLZ) in populated scenarios. Then, we study two different criteria for
selecting the "best" SLZ, and evaluate them during autonomous landing of a
virtual drone in different scenarios and conditions, under different
distributions of people in urban scenes, including moving people. We evaluate
different metrics to quantify the performance of the landing strategies,
establishing a baseline for comparison with future works in this challenging
task, and analyze them through an important number of randomized iterations.
The study suggests that the use of the autonomous landing algorithms
considerably helps to prevent accidents involving humans, which may allow to
unleash the full potential of drones in urban environments near to people.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Dynamic-NeRF: Spline-NeRF. (arXiv:2203.13800v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13800">
<div class="article-summary-box-inner">
<span><p>The problem of reconstructing continuous functions over time is important for
problems such as reconstructing moving scenes, and interpolating between time
steps. Previous approaches that use deep-learning rely on regularization to
ensure that reconstructions are approximately continuous, which works well on
short sequences. As sequence length grows, though, it becomes more difficult to
regularize, and it becomes less feasible to learn only through regularization.
We propose a new architecture for function reconstruction based on classical
Bezier splines, which ensures $C^0$ and $C^1$-continuity, where $C^0$
continuity is that $\forall c:\lim\limits_{x\to c} f(x)
</p>
<p>= f(c)$, or more intuitively that there are no breaks at any point in the
function. In order to demonstrate our architecture, we reconstruct dynamic
scenes using Neural Radiance Fields, but hope it is clear that our approach is
general and can be applied to a variety of problems. We recover a Bezier spline
$B(\beta, t\in[0,1])$, parametrized by the control points $\beta$. Using Bezier
splines ensures reconstructions have $C^0$ and $C^1$ continuity, allowing for
guaranteed interpolation over time. We reconstruct $\beta$ with a multi-layer
perceptron (MLP), blending machine learning with classical animation
techniques. All code is available at https://github.com/JulianKnodt/nerf_atlas,
and datasets are from prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Playing Lottery Tickets in Style Transfer Models. (arXiv:2203.13802v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13802">
<div class="article-summary-box-inner">
<span><p>Style transfer has achieved great success and attracted a wide range of
attention from both academic and industrial communities due to its flexible
application scenarios. However, the dependence on pretty large VGG based
autoencoder leads to existing style transfer models have a high parameter
complexities which limits the application for resource-constrained devices.
Unfortunately, the compression of style transfer model has less been explored.
In parallel, study on the lottery ticket hypothesis (LTH) has shown great
potential in finding extremely sparse matching subnetworks which can achieve on
par or even better performance than original full networks when trained in
isolation. In this work, we perform the first empirical study to verify whether
such trainable networks also exist in style transfer models. From a wide range
of style transfer methods, we choose two of the most popular style transfer
models as the main testbeds, i.e., AdaIN and SANet, representing approaches of
global and local transformation based style transfer respectively. Through
extensive experiments and comprehensive analysis, we draw the following main
conclusions. (1) Compared with fixing VGG encoder, style transfer models can
benefit more from training the whole network together. (2) Using iterative
magnitude pruning, we find the most sparse matching subnetworks at 89.2% in
AdaIN and 73.7% in SANet, which suggests that style transfer models can play
lottery tickets too. (3) Feature transformation module should also be pruned to
get a sparser model without affecting the existence and quality of matching
subnetworks. (4) Besides AdaIN and SANet, other models such as LST, MANet,
AdaAttN and MCCNet can also play lottert tickets, which shows that LTH can be
generalized to various style transfer models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatially Multi-conditional Image Generation. (arXiv:2203.13812v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13812">
<div class="article-summary-box-inner">
<span><p>In most scenarios, conditional image generation can be thought of as an
inversion of the image understanding process. Since generic image understanding
involves the solving of multiple tasks, it is natural to aim at the generation
of images via multi-conditioning. However, multi-conditional image generation
is a very challenging problem due to the heterogeneity and the sparsity of the
(in practice) available conditioning labels. In this work, we propose a novel
neural architecture to address the problem of heterogeneity and sparsity of the
spatially multi-conditional labels. Our choice of spatial conditioning, such as
by semantics and depth, is driven by the promise it holds for better control of
the image generation process. The proposed method uses a transformer-like
architecture operating pixel-wise, which receives the available labels as input
tokens to merge them in a learned homogeneous space of labels. The merged
labels are then used for image generation via conditional generative
adversarial training. In this process, the sparsity of the labels is handled by
simply dropping the input tokens corresponding to the missing labels at the
desired locations, thanks to the proposed pixel-wise operating architecture.
Our experiments on three benchmark datasets demonstrate the clear superiority
of our method over the state-of-the-art and the compared baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Versatile Multi-Modal Pre-Training for Human-Centric Perception. (arXiv:2203.13815v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13815">
<div class="article-summary-box-inner">
<span><p>Human-centric perception plays a vital role in vision and graphics. But their
data annotations are prohibitively expensive. Therefore, it is desirable to
have a versatile pre-train model that serves as a foundation for data-efficient
downstream tasks transfer. To this end, we propose the Human-Centric
Multi-Modal Contrastive Learning framework HCMoCo that leverages the
multi-modal nature of human data (e.g. RGB, depth, 2D keypoints) for effective
representation learning. The objective comes with two main challenges: dense
pre-train for multi-modality data, efficient usage of sparse human priors. To
tackle the challenges, we design the novel Dense Intra-sample Contrastive
Learning and Sparse Structure-aware Contrastive Learning targets by
hierarchically learning a modal-invariant latent space featured with continuous
and ordinal feature distribution and structure-aware semantic consistency.
HCMoCo provides pre-train for different modalities by combining heterogeneous
datasets, which allows efficient usage of existing task-specific human data.
Extensive experiments on four downstream tasks of different modalities
demonstrate the effectiveness of HCMoCo, especially under data-efficient
settings (7.16% and 12% improvement on DensePose Estimation and Human Parsing).
Moreover, we demonstrate the versatility of HCMoCo by exploring cross-modality
supervision and missing-modality inference, validating its strong ability in
cross-modal association and reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling. (arXiv:2203.13817v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13817">
<div class="article-summary-box-inner">
<span><p>Neural fields such as implicit surfaces have recently enabled avatar modeling
from raw scans without explicit temporal correspondences. In this work, we
exploit autoregressive modeling to further extend this notion to capture
dynamic effects, such as soft-tissue deformations. Although autoregressive
models are naturally capable of handling dynamics, it is non-trivial to apply
them to implicit representations, as explicit state decoding is infeasible due
to prohibitive memory requirements. In this work, for the first time, we enable
autoregressive modeling of implicit avatars. To reduce the memory bottleneck
and efficiently model dynamic implicit surfaces, we introduce the notion of
articulated observer points, which relate implicit states to the explicit
surface of a parametric human body model. We demonstrate that encoding implicit
surfaces as a set of height fields defined on articulated observer points leads
to significantly better generalization compared to a latent representation. The
experiments show that our approach outperforms the state of the art, achieving
plausible dynamic deformations even for unseen motions.
https://zqbai-jeremy.github.io/autoavatar
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection and Tracking of Multiple Mice Using Part Proposal Networks. (arXiv:1906.02831v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.02831">
<div class="article-summary-box-inner">
<span><p>The study of mouse social behaviours has been increasingly undertaken in
neuroscience research. However, automated quantification of mouse behaviours
from the videos of interacting mice is still a challenging problem, where
object tracking plays a key role in locating mice in their living spaces.
Artificial markers are often applied for multiple mice tracking, which are
intrusive and consequently interfere with the movements of mice in a dynamic
environment. In this paper, we propose a novel method to continuously track
several mice and individual parts without requiring any specific tagging.
Firstly, we propose an efficient and robust deep learning based mouse part
detection scheme to generate part candidates. Subsequently, we propose a novel
Bayesian Integer Linear Programming Model that jointly assigns the part
candidates to individual targets with necessary geometric constraints whilst
establishing pair-wise association between the detected parts. There is no
publicly available dataset in the research community that provides a
quantitative test-bed for the part detection and tracking of multiple mice, and
we here introduce a new challenging Multi-Mice PartsTrack dataset that is made
of complex behaviours and actions. Finally, we evaluate our proposed approach
against several baselines on our new datasets, where the results show that our
method outperforms the other state-of-the-art approaches in terms of accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which Model to Transfer? Finding the Needle in the Growing Haystack. (arXiv:2010.06402v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06402">
<div class="article-summary-box-inner">
<span><p>Transfer learning has been recently popularized as a data-efficient
alternative to training models from scratch, in particular for computer vision
tasks where it provides a remarkably solid baseline. The emergence of rich
model repositories, such as TensorFlow Hub, enables the practitioners and
researchers to unleash the potential of these models across a wide range of
downstream tasks. As these repositories keep growing exponentially, efficiently
selecting a good model for the task at hand becomes paramount. We provide a
formalization of this problem through a familiar notion of regret and introduce
the predominant strategies, namely task-agnostic (e.g. ranking models by their
ImageNet performance) and task-aware search strategies (such as linear or kNN
evaluation). We conduct a large-scale empirical study and show that both
task-agnostic and task-aware methods can yield high regret. We then propose a
simple and computationally efficient hybrid search strategy which outperforms
the existing approaches. We highlight the practical benefits of the proposed
solution on a set of 19 diverse vision tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding and Increasing Efficiency of Frank-Wolfe Adversarial Training. (arXiv:2012.12368v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.12368">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are easily fooled by small perturbations known as
adversarial attacks. Adversarial Training (AT) is a technique that
approximately solves a robust optimization problem to minimize the worst-case
loss and is widely regarded as the most effective defense. Due to the high
computation time for generating strong adversarial examples in the AT process,
single-step approaches have been proposed to reduce training time. However,
these methods suffer from catastrophic overfitting where adversarial accuracy
drops during training, and although improvements have been proposed, they
increase training time and robustness is far from that of multi-step AT. We
develop a theoretical framework for adversarial training with FW optimization
(FW-AT) that reveals a geometric connection between the loss landscape and the
$\ell_2$ distortion of $\ell_\infty$ FW attacks. We analytically show that high
distortion of FW attacks is equivalent to small gradient variation along the
attack path. It is then experimentally demonstrated on various deep neural
network architectures that $\ell_\infty$ attacks against robust models achieve
near maximal distortion, while standard networks have lower distortion. It is
experimentally shown that catastrophic overfitting is strongly correlated with
low distortion of FW attacks. This mathematical transparency differentiates FW
from Projected Gradient Descent (PGD) optimization. To demonstrate the utility
of our theoretical framework we develop FW-AT-Adapt, a novel adversarial
training algorithm which uses a simple distortion measure to adapt the number
of attack steps during training to increase efficiency without compromising
robustness. FW-AT-Adapt provides training time on par with single-step fast AT
methods and closes the gap between fast AT methods and multi-step PGD-AT with
minimal loss in adversarial accuracy in white-box and black-box settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13450">
<div class="article-summary-box-inner">
<span><p>Digital watermarking is widely used for copyright protection. Traditional 3D
watermarking approaches or commercial software are typically designed to embed
messages into 3D meshes, and later retrieve the messages directly from
distorted/undistorted watermarked 3D meshes. However, in many cases, users only
have access to rendered 2D images instead of 3D meshes. Unfortunately,
retrieving messages from 2D renderings of 3D meshes is still challenging and
underexplored. We introduce a novel end-to-end learning framework to solve this
problem through: 1) an encoder to covertly embed messages in both mesh geometry
and textures; 2) a differentiable renderer to render watermarked 3D objects
from different camera angles and under varied lighting conditions; 3) a decoder
to recover the messages from 2D rendered images. From our experiments, we show
that our model can learn to embed information visually imperceptible to humans,
and to retrieve the embedded information from 2D renderings that undergo 3D
distortions. In addition, we demonstrate that our method can also work with
other renderers, such as ray tracers and real-time renderers with and without
fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens. (arXiv:2105.15168v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15168">
<div class="article-summary-box-inner">
<span><p>Transformers have offered a new methodology of designing neural networks for
visual recognition. Compared to convolutional networks, Transformers enjoy the
ability of referring to global features at each stage, yet the attention module
brings higher computational overhead that obstructs the application of
Transformers to process high-resolution visual data. This paper aims to
alleviate the conflict between efficiency and flexibility, for which we propose
a specialized token for each region that serves as a messenger (MSG). Hence, by
manipulating these MSG tokens, one can flexibly exchange visual information
across regions and the computational complexity is reduced. We then integrate
the MSG token into a multi-scale architecture named MSG-Transformer. In
standard image classification and object detection, MSG-Transformer achieves
competitive performance and the inference on both GPU and CPU is accelerated.
Code is available at https://github.com/hustvl/MSG-Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIFT Matching by Context Exposed. (arXiv:2106.09584v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09584">
<div class="article-summary-box-inner">
<span><p>This paper investigates how to step up local image descriptor matching by
exploiting matching context information. Two main contexts are identified,
originated respectively from the descriptor space and from the keypoint space.
The former is generally used to design the actual matching strategy while the
latter to filter matches according to the local spatial consistency. On this
basis, a new matching strategy and a novel local spatial filter, named
respectively blob matching and Delaunay Triangulation Matching (DTM) are
devised. Blob matching provides a general matching framework by merging
together several strategies, including rank-based pre-filtering as well as
many-to-many and symmetric matching, enabling to achieve a global improvement
upon each individual strategy. DTM alternates between Delaunay triangulation
contractions and expansions to figure out and adjust keypoint neighborhood
consistency. Experimental evaluation shows that DTM is comparable or better
than the state-of-the-art in terms of matching accuracy and robustness.
Evaluation is carried out according to a new benchmark devised for analyzing
the matching pipeline in terms of correct correspondences on both planar and
non-planar scenes, including several state-of-the-art methods as well as the
common SIFT matching approach for reference. This evaluation can be of
assistance for future research in this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-level Feature Learning for Contrastive Multi-view Clustering. (arXiv:2106.11193v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11193">
<div class="article-summary-box-inner">
<span><p>Multi-view clustering can explore common semantics from multiple views and
has attracted increasing attention. However, existing works punish multiple
objectives in the same feature space, where they ignore the conflict between
learning consistent common semantics and reconstructing inconsistent
view-private information. In this paper, we propose a new framework of
multi-level feature learning for contrastive multi-view clustering to address
the aforementioned issue. Our method learns different levels of features from
the raw features, including low-level features, high-level features, and
semantic labels/features in a fusion-free manner, so that it can effectively
achieve the reconstruction objective and the consistency objectives in
different feature spaces. Specifically, the reconstruction objective is
conducted on the low-level features. Two consistency objectives based on
contrastive learning are conducted on the high-level features and the semantic
labels, respectively. They make the high-level features effectively explore the
common semantics and the semantic labels achieve the multi-view clustering. As
a result, the proposed framework can reduce the adverse influence of
view-private information. Extensive experiments on public datasets demonstrate
that our method achieves state-of-the-art clustering effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recall@k Surrogate Loss with Large Batches and Similarity Mixup. (arXiv:2108.11179v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11179">
<div class="article-summary-box-inner">
<span><p>This work focuses on learning deep visual representation models for retrieval
by exploring the interplay between a new loss function, the batch size, and a
new regularization approach. Direct optimization, by gradient descent, of an
evaluation metric, is not possible when it is non-differentiable, which is the
case for recall in retrieval. A differentiable surrogate loss for the recall is
proposed in this work. Using an implementation that sidesteps the hardware
constraints of the GPU memory, the method trains with a very large batch size,
which is essential for metrics computed on the entire retrieval database. It is
assisted by an efficient mixup regularization approach that operates on
pairwise scalar similarities and virtually increases the batch size further.
The suggested method achieves state-of-the-art performance in several image
retrieval benchmarks when used for deep metric learning. For instance-level
recognition, the method outperforms similar approaches that train using an
approximation of average precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. (arXiv:2109.05488v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05488">
<div class="article-summary-box-inner">
<span><p>Estimating the articulated 3D hand-object pose from a single RGB image is a
highly ambiguous and challenging problem, requiring large-scale datasets that
contain diverse hand poses, object types, and camera viewpoints. Most
real-world datasets lack these diversities. In contrast, data synthesis can
easily ensure those diversities separately. However, constructing both valid
and diverse hand-object interactions and efficiently learning from the vast
synthetic data is still challenging. To address the above issues, we propose
ArtiBoost, a lightweight online data enhancement method. ArtiBoost can cover
diverse hand-object poses and camera viewpoints through sampling in a
Composited hand-object Configuration and Viewpoint space (CCV-space) and can
adaptively enrich the current hard-discernable items by loss-feedback and
sample re-weighting. ArtiBoost alternatively performs data exploration and
synthesis within a learning pipeline, and those synthetic data are blended into
real-world source data for training. We apply ArtiBoost on a simple learning
baseline network and witness the performance boost on several hand-object
benchmarks. Our models and code are available at
https://github.com/lixiny/ArtiBoost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Bone Length Attack on Action Recognition. (arXiv:2109.05830v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05830">
<div class="article-summary-box-inner">
<span><p>Skeleton-based action recognition models have recently been shown to be
vulnerable to adversarial attacks. Compared to adversarial attacks on images,
perturbations to skeletons are typically bounded to a lower dimension of
approximately 100 per frame. This lower-dimensional setting makes it more
difficult to generate imperceptible perturbations. Existing attacks resolve
this by exploiting the temporal structure of the skeleton motion so that the
perturbation dimension increases to thousands. In this paper, we show that
adversarial attacks can be performed on skeleton-based action recognition
models, even in a significantly low-dimensional setting without any temporal
manipulation. Specifically, we restrict the perturbations to the lengths of the
skeleton's bones, which allows an adversary to manipulate only approximately 30
effective dimensions. We conducted experiments on the NTU RGB+D and HDM05
datasets and demonstrate that the proposed attack successfully deceived models
with sometimes greater than 90% success rate by small perturbations.
Furthermore, we discovered an interesting phenomenon: in our low-dimensional
setting, the adversarial training with the bone length attack shares a similar
property with data augmentation, and it not only improves the adversarial
robustness but also improves the classification accuracy on the original data.
This is an interesting counterexample of the trade-off between adversarial
robustness and clean accuracy, which has been widely observed in studies on
adversarial training in the high-dimensional regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering. (arXiv:2109.08029v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08029">
<div class="article-summary-box-inner">
<span><p>Integrating outside knowledge for reasoning in visio-linguistic tasks such as
visual question answering (VQA) is an open problem. Given that pretrained
language models have been shown to include world knowledge, we propose to use a
unimodal (text-only) train and inference procedure based on automatic
off-the-shelf captioning of images and pretrained language models. Our results
on a visual question answering task which requires external knowledge (OK-VQA)
show that our text-only model outperforms pretrained multimodal (image-text)
models of comparable number of parameters. In contrast, our model is less
effective in a standard VQA task (VQA 2.0) confirming that our text-only method
is specially effective for tasks requiring external knowledge. In addition, we
show that increasing the language model's size improves notably its
performance, yielding results comparable to the state-of-the-art with our
largest model, significantly outperforming current multimodal systems, even
though augmented with external knowledge. Our qualitative analysis on OK-VQA
reveals that automatic captions often fail to capture relevant information in
the images, which seems to be balanced by the better inference ability of the
text-only language models. Our work opens up possibilities to further improve
inference in visio-linguistic tasks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QU-net++: Image Quality Detection Framework for Segmentation of 3D Medical Image Stacks. (arXiv:2110.14181v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14181">
<div class="article-summary-box-inner">
<span><p>Automated segmentation of pathological regions of interest aids medical image
diagnostics and follow-up care. However, accurate pathological segmentations
require high quality of annotated data that can be both cost and time intensive
to generate. In this work, we propose an automated two-step method that detects
a minimal image subset required to train segmentation models by evaluating the
quality of medical images from 3D image stacks using a U-net++ model. These
images that represent a lack of quality training can then be annotated and used
to fully train a U-net-based segmentation model. The proposed QU-net++ model
detects lack of quality training based on the disagreement in segmentations
produced from the final two output layers. The proposed model isolates around
10% of images per 3D stack and can scale across imaging modalities to segment
cysts in OCT images and ground glass opacity in Lung CT images with Dice scores
in the range 0.56-0.72. Thus, the proposed method can be applied for cost
effective multi-modal pathology segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiT: Zero-Shot Transfer with Locked-image text Tuning. (arXiv:2111.07991v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07991">
<div class="article-summary-box-inner">
<span><p>This paper presents contrastive-tuning, a simple method employing contrastive
training to align image and text models while still taking advantage of their
pre-training. In our empirical study we find that locked pre-trained image
models with unlocked text models work best. We call this instance of
contrastive-tuning "Locked-image Tuning" (LiT), which just teaches a text model
to read out good representations from a pre-trained image model for new tasks.
A LiT model gains the capability of zero-shot transfer to new vision tasks,
such as image classification or retrieval. The proposed LiT is widely
applicable; it works reliably with multiple pre-training methods (supervised
and unsupervised) and across diverse architectures (ResNet, Vision Transformers
and MLP-Mixer) using three different image-text datasets. With the
transformer-based pre-trained ViT-g/14 model, the LiT model achieves 84.5%
zero-shot transfer accuracy on the ImageNet test set, and 81.1% on the
challenging out-of-distribution ObjectNet test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08918">
<div class="article-summary-box-inner">
<span><p>Recent works with an implicit neural function shed light on representing
images in arbitrary resolution. However, a standalone multi-layer perceptron
shows limited performance in learning high-frequency components. In this paper,
we propose a Local Texture Estimator (LTE), a dominant-frequency estimator for
natural images, enabling an implicit function to capture fine details while
reconstructing images in a continuous manner. When jointly trained with a deep
super-resolution (SR) architecture, LTE is capable of characterizing image
textures in 2D Fourier space. We show that an LTE-based neural function
achieves favorable performance against existing deep SR methods within an
arbitrary-scale factor. Furthermore, we demonstrate that our implementation
takes the shortest running time compared to previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trustworthy Long-Tailed Classification. (arXiv:2111.09030v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09030">
<div class="article-summary-box-inner">
<span><p>Classification on long-tailed distributed data is a challenging problem,
which suffers from serious class-imbalance and accordingly unpromising
performance especially on tail classes. Recently, the ensembling based methods
achieve the state-of-the-art performance and show great potential. However,
there are two limitations for current methods. First, their predictions are not
trustworthy for failure-sensitive applications. This is especially harmful for
the tail classes where the wrong predictions is basically frequent. Second,
they assign unified numbers of experts to all samples, which is redundant for
easy samples with excessive computational cost. To address these issues, we
propose a Trustworthy Long-tailed Classification (TLC) method to jointly
conduct classification and uncertainty estimation to identify hard samples in a
multi-expert framework. Our TLC obtains the evidence-based uncertainty (EvU)
and evidence for each expert, and then combines these uncertainties and
evidences under the Dempster-Shafer Evidence Theory (DST). Moreover, we propose
a dynamic expert engagement to reduce the number of engaged experts for easy
samples and achieve efficiency while maintaining promising performances.
Finally, we conduct comprehensive experiments on the tasks of classification,
tail detection, OOD detection and failure prediction. The experimental results
show that the proposed TLC outperforms existing methods and is trustworthy with
reliable uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BoxeR: Box-Attention for 2D and 3D Transformers. (arXiv:2111.13087v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13087">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a simple attention mechanism, we call
box-attention. It enables spatial interaction between grid features, as sampled
from boxes of interest, and improves the learning capability of transformers
for several vision tasks. Specifically, we present BoxeR, short for Box
Transformer, which attends to a set of boxes by predicting their transformation
from a reference window on an input feature map. The BoxeR computes attention
weights on these boxes by considering its grid structure. Notably, BoxeR-2D
naturally reasons about box information within its attention module, making it
suitable for end-to-end instance detection and segmentation tasks. By learning
invariance to rotation in the box-attention module, BoxeR-3D is capable of
generating discriminative information from a bird's-eye view plane for 3D
end-to-end object detection. Our experiments demonstrate that the proposed
BoxeR-2D achieves state-of-the-art results on COCO detection and instance
segmentation. Besides, BoxeR-3D improves over the end-to-end 3D object
detection baseline and already obtains a compelling performance for the vehicle
category of Waymo Open, without any class-specific optimization. Code is
available at https://github.com/kienduynguyen/BoxeR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Inverse Transform Sampling For Efficient Vision Transformers. (arXiv:2111.15667v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15667">
<div class="article-summary-box-inner">
<span><p>While state-of-the-art vision transformer models achieve promising results
for image classification, they are computationally expensive and require many
GFLOPs. Although the GFLOPs of a vision transformer can be decreased by
reducing the number of tokens in the network, there is no setting that is
optimal for all input images. In this work, we, therefore, introduce a
differentiable parameter-free Adaptive Token Sampling (ATS) module, which can
be plugged into any existing vision transformer architecture. ATS empowers
vision transformers by scoring and adaptively sampling significant tokens. As a
result, the number of tokens is not constant anymore and varies for each input
image. By integrating ATS as an additional layer within current transformer
blocks, we can convert them into much more efficient vision transformers with
an adaptive number of tokens. Since ATS is a parameter-free module, it can be
added to off-the-shelf pre-trained vision transformers as a plug-and-play
module, thus reducing their GFLOPs without any additional training. Moreover,
due to its differentiable design, one can also train a vision transformer
equipped with ATS. We evaluate our module on both image and video
classification tasks by adding it to multiple SOTA vision transformers. Our
proposed module improves the SOTA by reducing the computational cost (GFLOPs)
by 2x while preserving the accuracy of SOTA models on ImageNet, Kinetics-400,
and Kinetics-600 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Structured Dictionary Perspective on Implicit Neural Representations. (arXiv:2112.01917v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01917">
<div class="article-summary-box-inner">
<span><p>Implicit neural representations (INRs) have recently emerged as a promising
alternative to classical discretized representations of signals. Nevertheless,
despite their practical success, we still do not understand how INRs represent
signals. We propose a novel unified perspective to theoretically analyse INRs.
Leveraging results from harmonic analysis and deep learning theory, we show
that most INR families are analogous to structured signal dictionaries whose
atoms are integer harmonics of the set of initial mapping frequencies. This
structure allows INRs to express signals with an exponentially increasing
frequency support using a number of parameters that only grows linearly with
depth. We also explore the inductive bias of INRs exploiting recent results
about the empirical neural tangent kernel (NTK). Specifically, we show that the
eigenfunctions of the NTK can be seen as dictionary atoms whose inner product
with the target signal determines the final performance of their
reconstruction. In this regard, we reveal that meta-learning has a reshaping
effect on the NTK analogous to dictionary learning, building dictionary atoms
as a combination of the examples seen during meta-training. Our results permit
to design and tune novel INR architectures, but can also be of interest for the
wider deep learning theory community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global-Local Context Network for Person Search. (arXiv:2112.02500v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02500">
<div class="article-summary-box-inner">
<span><p>Person search aims to jointly localize and identify a query person from
natural, uncropped images, which has been actively studied in the computer
vision community over the past few years. In this paper, we delve into the rich
context information globally and locally surrounding the target person, which
we refer to scene and group context, respectively. Unlike previous works that
treat the two types of context individually, we exploit them in a unified
global-local context network (GLCNet) with the intuitive aim of feature
enhancement. Specifically, re-ID embeddings and context features are enhanced
simultaneously in a multi-stage fashion, ultimately leading to enhanced,
discriminative features for person search. We conduct the experiments on two
person search benchmarks (i.e., CUHK-SYSU and PRW) as well as extend our
approach to a more challenging setting (i.e., character search on MovieNet).
Extensive experimental results demonstrate the consistent improvement of the
proposed GLCNet over the state-of-the-art methods on the three datasets. Our
source codes, pre-trained models, and the new setting for character search are
available at: https://github.com/ZhengPeng7/GLCNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs. (arXiv:2112.02789v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02789">
<div class="article-summary-box-inner">
<span><p>Recent neural human representations can produce high-quality multi-view
rendering but require using dense multi-view inputs and costly training. They
are hence largely limited to static models as training each frame is
infeasible. We present HumanNeRF - a generalizable neural representation - for
high-fidelity free-view synthesis of dynamic humans. Analogous to how IBRNet
assists NeRF by avoiding per-scene training, HumanNeRF employs an aggregated
pixel-alignment feature across multi-view inputs along with a pose embedded
non-rigid deformation field for tackling dynamic motions. The raw HumanNeRF can
already produce reasonable rendering on sparse video inputs of unseen subjects
and camera settings. To further improve the rendering quality, we augment our
solution with an appearance blending module for combining the benefits of both
neural volumetric rendering and neural texture blending. Extensive experiments
on various multi-view dynamic human datasets demonstrate the generalizability
and effectiveness of our approach in synthesizing photo-realistic free-view
humans under challenging motions and with very sparse camera view inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepFace-EMD: Re-ranking Using Patch-wise Earth Mover's Distance Improves Out-Of-Distribution Face Identification. (arXiv:2112.04016v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04016">
<div class="article-summary-box-inner">
<span><p>Face identification (FI) is ubiquitous and drives many high-stake decisions
made by law enforcement. State-of-the-art FI approaches compare two images by
taking the cosine similarity between their image embeddings. Yet, such an
approach suffers from poor out-of-distribution (OOD) generalization to new
types of images (e.g., when a query face is masked, cropped, or rotated) not
included in the training set or the gallery. Here, we propose a re-ranking
approach that compares two faces using the Earth Mover's Distance on the deep,
spatial features of image patches. Our extra comparison stage explicitly
examines image similarity at a fine-grained level (e.g., eyes to eyes) and is
more robust to OOD perturbations and occlusions than traditional FI.
Interestingly, without finetuning feature extractors, our method consistently
improves the accuracy on all tested OOD queries: masked, cropped, rotated, and
adversarial while obtaining similar results on in-distribution images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Statistics Mixing Regularization for Generative Adversarial Networks. (arXiv:2112.04120v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04120">
<div class="article-summary-box-inner">
<span><p>In generative adversarial networks, improving discriminators is one of the
key components for generation performance. As image classifiers are biased
toward texture and debiasing improves accuracy, we investigate 1) if the
discriminators are biased, and 2) if debiasing the discriminators will improve
generation performance. Indeed, we find empirical evidence that the
discriminators are sensitive to the style (e.g., texture and color) of images.
As a remedy, we propose feature statistics mixing regularization (FSMR) that
encourages the discriminator's prediction to be invariant to the styles of
input images. Specifically, we generate a mixed feature of an original and a
reference image in the discriminator's feature space and we apply
regularization so that the prediction for the mixed feature is consistent with
the prediction for the original image. We conduct extensive experiments to
demonstrate that our regularization leads to reduced sensitivity to style and
consistently improves the performance of various GAN architectures on nine
datasets. In addition, adding FSMR to recently-proposed augmentation-based GAN
methods further improves image quality. Our code is available at
https://github.com/naver-ai/FSMR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning. (arXiv:2112.04731v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04731">
<div class="article-summary-box-inner">
<span><p>Class Incremental Learning (CIL) aims at learning a multi-class classifier in
a phase-by-phase manner, in which only data of a subset of the classes are
provided at each phase. Previous works mainly focus on mitigating forgetting in
phases after the initial one. However, we find that improving CIL at its
initial phase is also a promising direction. Specifically, we experimentally
show that directly encouraging CIL Learner at the initial phase to output
similar representations as the model jointly trained on all classes can greatly
boost the CIL performance. Motivated by this, we study the difference between a
na\"ively-trained initial-phase model and the oracle model. Specifically, since
one major difference between these two models is the number of training
classes, we investigate how such difference affects the model representations.
We find that, with fewer training classes, the data representations of each
class lie in a long and narrow region; with more training classes, the
representations of each class scatter more uniformly. Inspired by this
observation, we propose Class-wise Decorrelation (CwD) that effectively
regularizes representations of each class to scatter more uniformly, thus
mimicking the model jointly trained with all classes (i.e., the oracle model).
Our CwD is simple to implement and easy to plug into existing methods.
Extensive experiments on various benchmark datasets show that CwD consistently
and significantly improves the performance of existing state-of-the-art methods
by around 1\% to 3\%. Code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Sketch for All: One-Shot Personalized Sketch Segmentation. (arXiv:2112.10838v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10838">
<div class="article-summary-box-inner">
<span><p>We present the first one-shot personalized sketch segmentation method. We aim
to segment all sketches belonging to the same category provisioned with a
single sketch with a given part annotation while (i) preserving the parts
semantics embedded in the exemplar, and (ii) being robust to input style and
abstraction. We refer to this scenario as personalized. With that, we
importantly enable a much-desired personalization capability for downstream
fine-grained sketch analysis tasks. To train a robust segmentation module, we
deform the exemplar sketch to each of the available sketches of the same
category. Our method generalizes to sketches not observed during training. Our
central contribution is a sketch-specific hierarchical deformation network.
Given a multi-level sketch-strokes encoding obtained via a graph convolutional
network, our method estimates rigid-body transformation from the target to the
exemplar, on the upper level. Finer deformation from the exemplar to the
globally warped target sketch is further obtained through stroke-wise
deformations, on the lower level. Both levels of deformation are guided by mean
squared distances between the keypoints learned without supervision, ensuring
that the stroke semantics are preserved. We evaluate our method against the
state-of-the-art segmentation and perceptual grouping baselines re-purposed for
the one-shot setting and against two few-shot 3D shape segmentation methods. We
show that our method outperforms all the alternatives by more than $10\%$ on
average. Ablation studies further demonstrate that our method is robust to
personalization: changes in input part semantics and style differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Amplitude SAR Imagery Splicing Localization. (arXiv:2201.02409v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02409">
<div class="article-summary-box-inner">
<span><p>Synthetic Aperture Radar (SAR) images are a valuable asset for a wide variety
of tasks. In the last few years, many websites have been offering them for free
in the form of easy to manage products, favoring their widespread diffusion and
research work in the SAR field. The drawback of these opportunities is that
such images might be exposed to forgeries and manipulations by malicious users,
raising new concerns about their integrity and trustworthiness. Up to now, the
multimedia forensics literature has proposed various techniques to localize
manipulations in natural photographs, but the integrity assessment of SAR
images was never investigated. This task poses new challenges, since SAR images
are generated with a processing chain completely different from that of natural
photographs. This implies that many forensics methods developed for natural
images are not guaranteed to succeed. In this paper, we investigate the problem
of amplitude SAR imagery splicing localization. Our goal is to localize regions
of an amplitude SAR image that have been copied and pasted from another image,
possibly undergoing some kind of editing in the process. To do so, we leverage
a Convolutional Neural Network (CNN) to extract a fingerprint highlighting
inconsistencies in the processing traces of the analyzed input. Then, we
examine this fingerprint to produce a binary tampering mask indicating the
pixel region under splicing attack. Results show that our proposed method,
tailored to the nature of SAR signals, provides better performances than
state-of-the-art forensic tools developed for natural images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Contrastive Learning is Provably (almost) Principal Component Analysis. (arXiv:2201.12680v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12680">
<div class="article-summary-box-inner">
<span><p>We show that Contrastive Learning (CL) under a family of loss functions
(including InfoNCE) has a game-theoretical formulation, where the \emph{max
player} finds representation to maximize contrastiveness, and the \emph{min
player} puts weights on pairs of samples with similar representation. We show
that the max player who does \emph{representation learning} reduces to
Principal Component Analysis for deep linear network, and almost all local
minima are global, recovering optimal PCA solutions. Experiments show that the
formulation yields comparable (or better) performance on CIFAR10 and STL-10
when extending beyond InfoNCE, yielding novel contrastive losses. Furthermore,
we extend our theoretical analysis to 2-layer ReLU networks, showing its
difference from linear ones, and proving that feature composition is preferred
over picking single dominant feature under strong augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Constrained Least Squares for Blind Image Super-Resolution. (arXiv:2202.07508v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07508">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the problem of blind image super-resolution(SR) with
a reformulated degradation model and two novel modules. Following the common
practices of blind SR, our method proposes to improve both the kernel
estimation as well as the kernel-based high-resolution image restoration. To be
more specific, we first reformulate the degradation model such that the
deblurring kernel estimation can be transferred into the low-resolution space.
On top of this, we introduce a dynamic deep linear filter module. Instead of
learning a fixed kernel for all images, it can adaptively generate deblurring
kernel weights conditional on the input and yield a more robust kernel
estimation. Subsequently, a deep constrained least square filtering module is
applied to generate clean features based on the reformulation and estimated
kernel. The deblurred feature and the low input image feature are then fed into
a dual-path structured SR network and restore the final high-resolution result.
To evaluate our method, we further conduct evaluations on several benchmarks,
including Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed
method achieves better accuracy and visual improvements against
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Object Localization as Domain Adaption. (arXiv:2203.01714v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01714">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object localization (WSOL) focuses on localizing objects
only with the supervision of image-level classification masks. Most previous
WSOL methods follow the classification activation map (CAM) that localizes
objects based on the classification structure with the multi-instance learning
(MIL) mechanism. However, the MIL mechanism makes CAM only activate
discriminative object parts rather than the whole object, weakening its
performance for localizing objects. To avoid this problem, this work provides a
novel perspective that models WSOL as a domain adaption (DA) task, where the
score estimator trained on the source/image domain is tested on the
target/pixel domain to locate objects. Under this perspective, a DA-WSOL
pipeline is designed to better engage DA approaches into WSOL to enhance
localization performance. It utilizes a proposed target sampling strategy to
select different types of target samples. Based on these types of target
samples, domain adaption localization (DAL) loss is elaborated. It aligns the
feature distribution between the two domains by DA and makes the estimator
perceive target domain cues by Universum regularization. Experiments show that
our pipeline outperforms SOTA methods on multi benchmarks. Code are released at
\url{https://github.com/zh460045050/DA-WSOL_CVPR2022}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Learning Contrastive Representations for Learning with Noisy Labels. (arXiv:2203.01785v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01785">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are able to memorize noisy labels easily with a softmax
cross-entropy (CE) loss. Previous studies attempted to address this issue focus
on incorporating a noise-robust loss function to the CE loss. However, the
memorization issue is alleviated but still remains due to the non-robust CE
loss. To address this issue, we focus on learning robust contrastive
representations of data on which the classifier is hard to memorize the label
noise under the CE loss. We propose a novel contrastive regularization function
to learn such representations over noisy data where label noise does not
dominate the representation learning. By theoretically investigating the
representations induced by the proposed regularization function, we reveal that
the learned representations keep information related to true labels and discard
information related to corrupted labels. Moreover, our theoretical results also
indicate that the learned representations are robust to the label noise. The
effectiveness of this method is demonstrated with experiments on benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network. (arXiv:2203.01824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01824">
<div class="article-summary-box-inner">
<span><p>3D room layout estimation by a single panorama using deep neural networks has
made great progress. However, previous approaches can not obtain efficient
geometry awareness of room layout with the only latitude of boundaries or
horizon-depth. We present that using horizon-depth along with room height can
obtain omnidirectional-geometry awareness of room layout in both horizontal and
vertical directions. In addition, we propose a planar-geometry aware loss
function with normals and gradients of normals to supervise the planeness of
walls and turning of corners. We propose an efficient network, LGT-Net, for
room layout estimation, which contains a novel Transformer architecture called
SWG-Transformer to model geometry relations. SWG-Transformer consists of
(Shifted) Window Blocks and Global Blocks to combine the local and global
geometry relations. Moreover, we design a novel relative position embedding of
Transformer to enhance the spatial identification ability for the panorama.
Experiments show that the proposed LGT-Net achieves better performance than
current state-of-the-arts (SOTA) on benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble Knowledge Guided Sub-network Search and Fine-tuning for Filter Pruning. (arXiv:2203.02651v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02651">
<div class="article-summary-box-inner">
<span><p>Conventional NAS-based pruning algorithms aim to find the sub-network with
the best validation performance. However, validation performance does not
successfully represent test performance, i.e., potential performance. Also,
although fine-tuning the pruned network to restore the performance drop is an
inevitable process, few studies have handled this issue. This paper proposes a
novel sub-network search and fine-tuning method that is named Ensemble
Knowledge Guidance (EKG). First, we experimentally prove that the fluctuation
of the loss landscape is an effective metric to evaluate the potential
performance. In order to search a sub-network with the smoothest loss landscape
at a low cost, we propose a pseudo-supernet built by an ensemble sub-network
knowledge distillation. Next, we propose a novel fine-tuning that re-uses the
information of the search phase. We store the interim sub-networks, that is,
the by-products of the search phase, and transfer their knowledge into the
pruned network. Note that EKG is easy to be plugged-in and computationally
efficient. For example, in the case of ResNet-50, about 45% of FLOPS is removed
without any performance drop in only 315 GPU hours. The implemented code is
available at https://github.com/sseung0703/EKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross Language Image Matching for Weakly Supervised Semantic Segmentation. (arXiv:2203.02668v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02668">
<div class="article-summary-box-inner">
<span><p>It has been widely known that CAM (Class Activation Map) usually only
activates discriminative object regions and falsely includes lots of
object-related backgrounds. As only a fixed set of image-level object labels
are available to the WSSS (weakly supervised semantic segmentation) model, it
could be very difficult to suppress those diverse background regions consisting
of open set objects. In this paper, we propose a novel Cross Language Image
Matching (CLIMS) framework, based on the recently introduced Contrastive
Language-Image Pre-training (CLIP) model, for WSSS. The core idea of our
framework is to introduce natural language supervision to activate more
complete object regions and suppress closely-related open background regions.
In particular, we design object, background region and text label matching
losses to guide the model to excite more reasonable object regions for CAM of
each category. In addition, we design a co-occurring background suppression
loss to prevent the model from activating closely-related background regions,
with a predefined set of class-related background text descriptions. These
designs enable the proposed CLIMS to generate a more complete and compact
activation map for the target objects. Extensive experiments on PASCAL VOC2012
dataset show that our CLIMS significantly outperforms the previous
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motron: Multimodal Probabilistic Human Motion Forecasting. (arXiv:2203.04132v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04132">
<div class="article-summary-box-inner">
<span><p>Autonomous systems and humans are increasingly sharing the same space. Robots
work side by side or even hand in hand with humans to balance each other's
limitations. Such cooperative interactions are ever more sophisticated. Thus,
the ability to reason not just about a human's center of gravity position, but
also its granular motion is an important prerequisite for human-robot
interaction. Though, many algorithms ignore the multimodal nature of humans or
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,
probabilistic, graph-structured model, that captures human's multimodality
using probabilistic methods while being able to output deterministic
maximum-likelihood motions and corresponding confidence values for each mode.
Our model aims to be tightly integrated with the robotic
planning-control-interaction loop; outputting physically feasible human motions
and being computationally efficient. We demonstrate the performance of our
model on several challenging real-world motion forecasting datasets,
outperforming a wide array of generative/variational methods while providing
state-of-the-art single-output motions if required. Both using significantly
less computational power than state-of-the art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack. (arXiv:2203.05154v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05154">
<div class="article-summary-box-inner">
<span><p>Defense models against adversarial attacks have grown significantly, but the
lack of practical evaluation methods has hindered progress. Evaluation can be
defined as looking for defense models' lower bound of robustness given a budget
number of iterations and a test dataset. A practical evaluation method should
be convenient (i.e., parameter-free), efficient (i.e., fewer iterations) and
reliable (i.e., approaching the lower bound of robustness). Towards this
target, we propose a parameter-free Adaptive Auto Attack (A$^3$) evaluation
method which addresses the efficiency and reliability in a test-time-training
fashion. Specifically, by observing that adversarial examples to a specific
defense model follow some regularities in their starting points, we design an
Adaptive Direction Initialization strategy to speed up the evaluation.
Furthermore, to approach the lower bound of robustness under the budget number
of iterations, we propose an online statistics-based discarding strategy that
automatically identifies and abandons hard-to-attack images. Extensive
experiments demonstrate the effectiveness of our A$^3$. Particularly, we apply
A$^3$ to nearly 50 widely-used defense models. By consuming much fewer
iterations than existing methods, i.e., $1/10$ on average (10$\times$ speed
up), we achieve lower robust accuracy in all cases. Notably, we won
$\textbf{first place}$ out of 1681 teams in CVPR 2021 White-box Adversarial
Attacks on Defense Models competitions with this method. Code is available at:
$\href{https://github.com/liuye6666/adaptive_auto_attack}{https://github.com/liuye6666/adaptive\_auto\_attack}$
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decontextualized I3D ConvNet for ultra-distance runners performance analysis at a glance. (arXiv:2203.06749v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06749">
<div class="article-summary-box-inner">
<span><p>In May 2021, the site runnersworld.com published that participation in
ultra-distance races has increased by 1,676% in the last 23 years. Moreover,
nearly 41% of those runners participate in more than one race per year. The
development of wearable devices has undoubtedly contributed to motivating
participants by providing performance measures in real-time. However, we
believe there is room for improvement, particularly from the organizers point
of view. This work aims to determine how the runners performance can be
quantified and predicted by considering a non-invasive technique focusing on
the ultra-running scenario. In this sense, participants are captured when they
pass through a set of locations placed along the race track. Each footage is
considered an input to an I3D ConvNet to extract the participant's running gait
in our work. Furthermore, weather and illumination capture conditions or
occlusions may affect these footages due to the race staff and other runners.
To address this challenging task, we have tracked and codified the
participant's running gait at some RPs and removed the context intending to
ensure a runner-of-interest proper evaluation. The evaluation suggests that the
features extracted by an I3D ConvNet provide enough information to estimate the
participant's performance along the different race tracks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization. (arXiv:2203.07740v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07740">
<div class="article-summary-box-inner">
<span><p>Arbitrary style transfer (AST) and domain generalization (DG) are important
yet challenging visual learning tasks, which can be cast as a feature
distribution matching problem. With the assumption of Gaussian feature
distribution, conventional feature distribution matching methods usually match
the mean and standard deviation of features. However, the feature distributions
of real-world data are usually much more complicated than Gaussian, which
cannot be accurately matched by using only the first-order and second-order
statistics, while it is computationally prohibitive to use high-order
statistics for distribution matching. In this work, we, for the first time to
our best knowledge, propose to perform Exact Feature Distribution Matching
(EFDM) by exactly matching the empirical Cumulative Distribution Functions
(eCDFs) of image features, which could be implemented by applying the Exact
Histogram Matching (EHM) in the image feature space. Particularly, a fast EHM
algorithm, named Sort-Matching, is employed to perform EFDM in a plug-and-play
manner with minimal cost. The effectiveness of our proposed EFDM method is
verified on a variety of AST and DG tasks, demonstrating new state-of-the-art
results. Codes are available at https://github.com/YBZh/EFDM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance. (arXiv:2203.08368v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08368">
<div class="article-summary-box-inner">
<span><p>The exponentially large discrete search space in mixed-precision quantization
(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous
works usually resort to iterative search methods on the training set, which
consume hundreds or even thousands of GPU-hours. In this study, we reveal that
some unique learnable parameters in quantization, namely the scale factors in
the quantizer, can serve as importance indicators of a layer, reflecting the
contribution of that layer to the final accuracy at certain bit-widths. These
importance indicators naturally perceive the numerical transformation during
quantization-aware training, which can precisely and correctly provide
quantization sensitivity metrics of layers. However, a deep network always
contains hundreds of such indicators, and training them one by one would lead
to an excessive time cost. To overcome this issue, we propose a joint training
scheme that can obtain all indicators at once. It considerably speeds up the
indicators training process by parallelizing the original sequential training
processes. With these learned importance indicators, we formulate the MPQ
search problem as a one-time integer linear programming (ILP) problem. That
avoids the iterative search and significantly reduces search time without
limiting the bit-width search space. For example, MPQ search on ResNet18 with
our indicators takes only 0.06 seconds. Also, extensive experiments show our
approach can achieve SOTA accuracy on ImageNet for far-ranging models with
various constraints (e.g., BitOps, compress rate).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DATA: Domain-Aware and Task-Aware Self-supervised Learning. (arXiv:2203.09041v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09041">
<div class="article-summary-box-inner">
<span><p>The paradigm of training models on massive data without label through
self-supervised learning (SSL) and finetuning on many downstream tasks has
become a trend recently. However, due to the high training costs and the
unconsciousness of downstream usages, most self-supervised learning methods
lack the capability to correspond to the diversities of downstream scenarios,
as there are various data domains, different vision tasks and latency
constraints on models. Neural architecture search (NAS) is one universally
acknowledged fashion to conquer the issues above, but applying NAS on SSL seems
impossible as there is no label or metric provided for judging model selection.
In this paper, we present DATA, a simple yet effective NAS approach specialized
for SSL that provides Domain-Aware and Task-Aware pre-training. Specifically,
we (i) train a supernet which could be deemed as a set of millions of networks
covering a wide range of model scales without any label, (ii) propose a
flexible searching mechanism compatible with SSL that enables finding networks
of different computation costs, for various downstream vision tasks and data
domains without explicit metric provided. Instantiated With MoCo v2, our method
achieves promising results across a wide range of computation costs on
downstream tasks, including image classification, object detection and semantic
segmentation. DATA is orthogonal to most existing SSL methods and endows them
the ability of customization on downstream needs. Extensive experiments on
other SSL methods demonstrate the generalizability of the proposed method. Code
is released at https://github.com/GAIA-vision/GAIA-ssl
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Perceptual Model for Estimating the Quality of Visual Speech. (arXiv:2203.10117v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10117">
<div class="article-summary-box-inner">
<span><p>Generating realistic lip motions to simulate speech production is key for
driving natural character animations from audio. Previous research has shown
that traditional metrics used to optimize and assess models for generating lip
motions from speech are not a good indicator of subjective opinion of animation
quality. Yet, running repetitive subjective studies for assessing the quality
of animations can be time-consuming and difficult to replicate. In this work,
we seek to understand the relationship between perturbed lip motion and
subjective opinion of lip motion quality. Specifically, we adjust the degree of
articulation for lip motion sequences and run a user-study to examine how this
adjustment impacts the perceived quality of lip motion. We then train a model
using the scores collected from our user-study to automatically predict the
subjective quality of an animated sequence. Our results show that (1) users
score lip motions with slight over-articulation the highest in terms of
perceptual quality; (2) under-articulation had a more detrimental effect on
perceived quality of lip motion compared to the effect of over-articulation;
and (3) we can automatically estimate the subjective perceptual score for a
given lip motion sequences with low error rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11692">
<div class="article-summary-box-inner">
<span><p>This manuscript describes the panoptic segmentation method we devised for our
submission to the CONIC challenge at ISBI 2022. Key features of our method are
a weighted loss that we specifically engineered for semantic segmentation of
highly imbalanced cell types, and an existing state-of-the art nuclei instance
segmentation model, which we combine in a Hovernet-like architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Portrait Delighting. (arXiv:2203.12088v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12088">
<div class="article-summary-box-inner">
<span><p>We present a deep neural network for removing undesirable shading features
from an unconstrained portrait image, recovering the underlying texture. Our
training scheme incorporates three regularization strategies: masked loss, to
emphasize high-frequency shading features; soft-shadow loss, which improves
sensitivity to subtle changes in lighting; and shading-offset estimation, to
supervise separation of shading and texture. Our method demonstrates improved
delighting quality and generalization when compared with the state-of-the-art.
We further demonstrate how our delighting method can enhance the performance of
light-sensitive computer vision tasks such as face relighting and semantic
parsing, allowing them to handle extreme lighting conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection. (arXiv:2203.12208v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12208">
<div class="article-summary-box-inner">
<span><p>Recent studies in deepfake detection have yielded promising results when the
training and testing face forgeries are from the same dataset. However, the
problem remains challenging when one tries to generalize the detector to
forgeries created by unseen methods in the training dataset. This work
addresses the generalizable deepfake detection from a simple principle: a
generalizable representation should be sensitive to diverse types of forgeries.
Following this principle, we propose to enrich the "diversity" of forgeries by
synthesizing augmented forgeries with a pool of forgery configurations and
strengthen the "sensitivity" to the forgeries by enforcing the model to predict
the forgery configurations. To effectively explore the large forgery
augmentation space, we further propose to use the adversarial training strategy
to dynamically synthesize the most challenging forgeries to the current model.
Through extensive experiments, we show that the proposed strategies are
surprisingly effective (see Figure 1), and they could achieve superior
performance than the current state-of-the-art methods. Code is available at
\url{https://github.com/liangchen527/SLADD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Random Forest Regression for continuous affect using Facial Action Units. (arXiv:2203.12818v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12818">
<div class="article-summary-box-inner">
<span><p>In this paper we describe our approach to the arousal and valence track of
the 3rd Workshop and Competition on Affective Behavior Analysis in-the-wild
(ABAW). We extracted facial features using OpenFace and used them to train a
multiple output random forest regressor. Our approach performed comparable to
the baseline approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization. (arXiv:2203.12870v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12870">
<div class="article-summary-box-inner">
<span><p>Direct estimating the 6-DoF object pose from a single color image is
challenging, and post-refinement is generally needed to achieve high-precision
estimation. In this paper, we propose a framework based on a recurrent neural
network (RNN) for object pose refinement, which is robust to erroneous initial
poses and occlusions. During the recurrent iterations, object pose refinement
is formulated as a non-linear least squares problem based on the estimated
correspondence field (between a rendered image and the observed image). The
problem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm
for end-toend training. The correspondence field estimation and pose refinement
are conducted alternatively in each iteration to recover accurate object poses.
Furthermore, to improve the robustness to occlusions, we introduce a
consistencycheck mechanism based on the learned descriptors of the 3D model and
observed 2D image, which downweights the unreliable correspondences during pose
optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and
YCB-Video datasets validate the effectiveness of our method and demonstrate
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13032">
<div class="article-summary-box-inner">
<span><p>In this paper, we briefly introduce our submission to the Valence-Arousal
Estimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW)
competition. Our method utilizes the multi-modal information, i.e., the visual
and audio information, and employs a temporal encoder to model the temporal
context in the videos. Besides, a smooth processor is applied to get more
reasonable predictions, and a model ensemble strategy is used to improve the
performance of our proposed method. The experiment results show that our method
achieves 65.55% ccc for valence and 70.88% ccc for arousal on the validation
set of the Aff-Wild2 dataset, which prove the effectiveness of our proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial Expression Recognition. (arXiv:2203.13052v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13052">
<div class="article-summary-box-inner">
<span><p>Facial expression recognition plays an important role in human-computer
interaction. In this paper, we propose the Coarse-to-Fine Cascaded network with
Smooth Predicting (CFC-SP) to improve the performance of facial expression
recognition. CFC-SP contains two core components, namely Coarse-to-Fine
Cascaded networks (CFC) and Smooth Predicting (SP). For CFC, it first groups
several similar emotions to form a rough category, and then employs a network
to conduct a coarse but accurate classification. Later, an additional network
for these grouped emotions is further used to obtain fine-grained predictions.
For SP, it improves the recognition capability of the model by capturing both
universal and unique expression features. To be specific, the universal
features denote the general characteristic of facial emotions within a period
and the unique features denote the specific characteristic at this moment.
Experiments on Aff-Wild2 show the effectiveness of the proposed CFSP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory. (arXiv:2203.13055v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13055">
<div class="article-summary-box-inner">
<span><p>Driving 3D characters to dance following a piece of music is highly
challenging due to the spatial constraints applied to poses by choreography
norms. In addition, the generated dance sequence also needs to maintain
temporal coherency with different music genres. To tackle these challenges, we
propose a novel music-to-dance framework, Bailando, with two powerful
components: 1) a choreographic memory that learns to summarize meaningful
dancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic
Generative Pre-trained Transformer (GPT) that composes these units to a fluent
dance coherent to the music. With the learned choreographic memory, dance
generation is realized on the quantized units that meet high choreography
standards, such that the generated dancing sequences are confined within the
spatial constraints. To achieve synchronized alignment between diverse motion
tempos and music beats, we introduce an actor-critic-based reinforcement
learning scheme to the GPT with a newly-designed beat-align reward function.
Extensive experiments on the standard benchmark demonstrate that our proposed
framework achieves state-of-the-art performance both qualitatively and
quantitatively. Notably, the learned choreographic memory is shown to discover
human-interpretable dancing-style poses in an unsupervised manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Preliminary Research on Space Situational Awareness Based on Event Cameras. (arXiv:2203.13093v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13093">
<div class="article-summary-box-inner">
<span><p>Event camera is a new type of sensor that is different from traditional
cameras. Each pixel is triggered asynchronously by an event. The trigger event
is the change of the brightness irradiated on the pixel. If the increment or
decrement is higher than a certain threshold, the event is output. Compared
with traditional cameras, event cameras have the advantages of high temporal
resolution, low latency, high dynamic range, low bandwidth and low power
consumption. We carried out a series of observation experiments in a simulated
space lighting environment. The experimental results show that the event camera
can give full play to the above advantages in space situational awareness. This
article first introduces the basic principles of the event camera, then
analyzes its advantages and disadvantages, then introduces the observation
experiment and analyzes the experimental results, and finally, a workflow of
space situational awareness based on event cameras is given.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IA-FaceS: A Bidirectional Method for Semantic Face Editing. (arXiv:2203.13097v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13097">
<div class="article-summary-box-inner">
<span><p>Semantic face editing has achieved substantial progress in recent years.
Known as a growingly popular method, latent space manipulation performs face
editing by changing the latent code of an input face to liberate users from
painting skills. However, previous latent space manipulation methods usually
encode an entire face into a single low-dimensional embedding, which constrains
the reconstruction capacity and the control flexibility of facial components,
such as eyes and nose. This paper proposes IA-FaceS as a bidirectional method
for disentangled face attribute manipulation as well as flexible, controllable
component editing without the need for segmentation masks or sketches in the
original image. To strike a balance between the reconstruction capacity and the
control flexibility, the encoder is designed as a multi-head structure to yield
embeddings for reconstruction and control, respectively: a high-dimensional
tensor with spatial properties for consistent reconstruction and four
low-dimensional facial component embeddings for semantic face editing.
Manipulating the separate component embeddings can help achieve disentangled
attribute manipulation and flexible control of facial components. To further
disentangle the highly-correlated components, a component adaptive modulation
(CAM) module is proposed for the decoder. The semantic single-eye editing is
developed for the first time without any input visual guidance, such as
segmentation masks or sketches. According to the experimental results, IA-FaceS
establishes a good balance between maintaining image details and performing
flexible face manipulation. Both quantitative and qualitative results indicate
that the proposed method outperforms the other techniques in reconstruction,
face attribute manipulation, and component transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation. (arXiv:2203.12917v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12917">
<div class="article-summary-box-inner">
<span><p>We propose WarpingGAN, an effective and efficient 3D point cloud generation
network. Unlike existing methods that generate point clouds by directly
learning the mapping functions between latent codes and 3D shapes, Warping-GAN
learns a unified local-warping function to warp multiple identical pre-defined
priors (i.e., sets of points uniformly distributed on regular 3D grids) into 3D
shapes driven by local structure-aware semantics. In addition, we also
ingeniously utilize the principle of the discriminator and tailor a stitching
loss to eliminate the gaps between different partitions of a generated shape
corresponding to different priors for boosting quality. Owing to the novel
generating mechanism, WarpingGAN, a single lightweight network after one-time
training, is capable of efficiently generating uniformly distributed 3D point
clouds with various resolutions. Extensive experimental results demonstrate the
superiority of our WarpingGAN over state-of-the-art methods in terms of
quantitative metrics, visual quality, and efficiency. The source code is
publicly available at https://github.com/yztang4/WarpingGAN.git.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-28 23:07:48.891726606 UTC">2022-03-28 23:07:48 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>