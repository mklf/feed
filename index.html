<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-18T01:30:00Z">04-18</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Brazilian Court Documents Clustered by Similarity Together Using Natural Language Processing Approaches with Transformers. (arXiv:2204.07182v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07182">
<div class="article-summary-box-inner">
<span><p>Recent advances in Artificial intelligence (AI) have leveraged promising
results in solving complex problems in the area of Natural Language Processing
(NLP), being an important tool to help in the expeditious resolution of
judicial proceedings in the legal area. In this context, this work targets the
problem of detecting the degree of similarity between judicial documents that
can be achieved in the inference group, by applying six NLP techniques based on
transformers, namely BERT, GPT-2 and RoBERTa pre-trained in the Brazilian
Portuguese language and the same specialized using 210,000 legal proceedings.
Documents were pre-processed and had their content transformed into a vector
representation using these NLP techniques. Unsupervised learning was used to
cluster the lawsuits, calculating the quality of the model based on the cosine
of the distance between the elements of the group to its centroid. We noticed
that models based on transformers present better performance when compared to
previous research, highlighting the RoBERTa model specialized in the Brazilian
Portuguese language, making it possible to advance in the current state of the
art in the area of NLP applied to the legal sector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying Feature Underspecified Lexicon Phonological Features in Multilingual Text-to-Speech. (arXiv:2204.07228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07228">
<div class="article-summary-box-inner">
<span><p>This study investigates whether the phonological features derived from the
Featurally Underspecified Lexicon model can be applied in text-to-speech
systems to generate native and non-native speech in English and Mandarin. We
present a mapping of ARPABET/pinyin to SAMPA/SAMPA-SC and then to phonological
features. This mapping was tested for whether it could lead to the successful
generation of native, non-native, and code-switched speech in the two
languages. We ran two experiments, one with a small dataset and one with a
larger dataset. The results supported that phonological features could be used
as a feasible input system for languages in or not in the train data, although
further investigation is needed to improve model performance. The results lend
support to FUL by presenting successfully synthesised output, and by having the
output carrying a source-language accent when synthesising a language not in
the training data. The TTS process stimulated human second language acquisition
process and thus also confirm FUL's ability to account for acquisition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Fake News Detection: Are current models "fact-checking" or "gut-checking"?. (arXiv:2204.07229v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07229">
<div class="article-summary-box-inner">
<span><p>Automatic fake news detection models are ostensibly based on logic, where the
truth of a claim made in a headline can be determined by supporting or refuting
evidence found in a resulting web query. These models are believed to be
reasoning in some way; however, it has been shown that these same results, or
better, can be achieved without considering the claim at all -- only the
evidence. This implies that other signals are contained within the examined
evidence, and could be based on manipulable factors such as emotion, sentiment,
or part-of-speech (POS) frequencies, which are vulnerable to adversarial
inputs. We neutralize some of these signals through multiple forms of both
neural and non-neural pre-processing and style transfer, and find that this
flattening of extraneous indicators can induce the models to actually require
both claims and evidence to perform well. We conclude with the construction of
a model using emotion vectors built off a lexicon and passed through an
"emotional attention" mechanism to appropriately weight certain emotions. We
provide quantifiable results that prove our hypothesis that manipulable
features are being used for fact-checking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Different are Pre-trained Transformers for Text Ranking?. (arXiv:2204.07233v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07233">
<div class="article-summary-box-inner">
<span><p>In recent years, large pre-trained transformers have led to substantial gains
in performance over traditional retrieval models and feedback approaches.
However, these results are primarily based on the MS Marco/TREC Deep Learning
Track setup, with its very particular setup, and our understanding of why and
how these models work better is fragmented at best. We analyze effective
BERT-based cross-encoders versus traditional BM25 ranking for the passage
retrieval task where the largest gains have been observed, and investigate two
main questions. On the one hand, what is similar? To what extent does the
neural ranker already encompass the capacity of traditional rankers? Is the
gain in performance due to a better ranking of the same documents (prioritizing
precision)? On the other hand, what is different? Can it retrieve effectively
documents missed by traditional systems (prioritizing recall)? We discover
substantial differences in the notion of relevance identifying strengths and
weaknesses of BERT that may inspire research for future improvement. Our
results contribute to our understanding of (black-box) neural rankers relative
to (well-understood) traditional rankers, help understand the particular
experimental setting of MS-Marco-based test collections.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A* shortest string decoding for non-idempotent semirings. (arXiv:2204.07236v1 [cs.FL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07236">
<div class="article-summary-box-inner">
<span><p>The single shortest path algorithm is undefined for weighted finite-state
automata over non-idempotent semirings because such semirings do not guarantee
the existence of a shortest path. However, in non-idempotent semirings
admitting an order satisfying a monotonicity condition (such as the plus-times
or log semirings), the notion of shortest string is well-defined. We describe
an algorithm which finds the shortest string for a weighted non-deterministic
automaton over such semirings using the backwards shortest distance of an
equivalent deterministic automaton (DFA) as a heuristic for A* search performed
over a companion idempotent semiring, which is proven to return the shortest
string. While there may be exponentially more states in the DFA, this algorithm
needs to visit only a small fraction of them if determinization is performed
"on the fly".
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers. (arXiv:2204.07237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07237">
<div class="article-summary-box-inner">
<span><p>This paper presents the first multi-objective transformer model for
constructing open cloze tests that exploits generation and discrimination
capabilities to improve performance. Our model is further enhanced by tweaking
its loss function and applying a post-processing re-ranking algorithm that
improves overall test structure. Experiments using automatic and human
evaluation show that our approach can achieve up to 82% accuracy according to
experts, outperforming previous work and baselines. We also release a
collection of high-quality open cloze tests along with sample system output and
human annotations that can serve as a future benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Art of Prompting: Event Detection based on Type Specific Prompts. (arXiv:2204.07241v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07241">
<div class="article-summary-box-inner">
<span><p>We compare various forms of prompts to represent event types and develop a
unified framework to incorporate the event type specific prompts for
supervised, few-shot, and zero-shot event detection. The experimental results
demonstrate that a well-defined and comprehensive event type prompt can
significantly improve the performance of event detection, especially when the
annotated data is scarce (few-shot event detection) or not available (zero-shot
event detection). By leveraging the semantics of event types, our unified
framework shows up to 24.3\% F-score gain over the previous state-of-the-art
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated speech tools for helping communities process restricted-access corpora for language revival efforts. (arXiv:2204.07272v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07272">
<div class="article-summary-box-inner">
<span><p>Many archival recordings of speech from endangered languages remain
unannotated and inaccessible to community members and language learning
programs. One bottleneck is the time-intensive nature of annotation. An even
narrower bottleneck occurs for recordings with access constraints, such as
language that must be vetted or filtered by authorised community members before
annotation can begin. We propose a privacy-preserving workflow to widen both
bottlenecks for recordings where speech in the endangered language is
intermixed with a more widely-used language such as English for meta-linguistic
commentary and questions (e.g. What is the word for 'tree'?). We integrate
voice activity detection (VAD), spoken language identification (SLI), and
automatic speech recognition (ASR) to transcribe the metalinguistic content,
which an authorised person can quickly scan to triage recordings that can be
annotated by people with lower levels of access. We report work-in-progress
processing 136 hours archival audio containing a mix of English and Muruwari.
Our collaborative work with the Muruwari custodian of the archival materials
show that this workflow reduces metalanguage transcription time by 20% even
given only minimal amounts of annotated training data: 10 utterances per
language for SLI and 39 minutes of the English for ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Prompting: Episodic Memory Prompt for Lifelong Event Detection. (arXiv:2204.07275v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07275">
<div class="article-summary-box-inner">
<span><p>Lifelong event detection aims to incrementally update a model with new event
types and data while retaining the capability on previously learned old types.
One critical challenge is that the model would catastrophically forget old
types when continually trained on new data. In this paper, we introduce
Episodic Memory Prompts (EMP) to explicitly preserve the learned task-specific
knowledge. Our method adopts continuous prompt for each task and they are
optimized to instruct the model prediction and learn event-specific
representation. The EMPs learned in previous tasks are carried along with the
model in subsequent tasks, and can serve as a memory module that keeps the old
knowledge and transferring to new tasks. Experiment results demonstrate the
effectiveness of our method. Furthermore, we also conduct a comprehensive
analysis of the new and old event types in lifelong learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models. (arXiv:2204.07288v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07288">
<div class="article-summary-box-inner">
<span><p>With many real-world applications of Natural Language Processing (NLP)
comprising of long texts, there has been a rise in NLP benchmarks that measure
the accuracy of models that can handle longer input sequences. However, these
benchmarks do not consider the trade-offs between accuracy, speed, and power
consumption as input sizes or model sizes are varied. In this work, we perform
a systematic study of this accuracy vs. efficiency trade-off on two widely used
long-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during
fine-tuning and inference on four datasets from the SCROLLS benchmark. To study
how this trade-off differs across hyperparameter settings, we compare the
models across four sequence lengths (1024, 2048, 3072, 4096) and two model
sizes (base and large) under a fixed resource budget. We find that LED
consistently achieves better accuracy at lower energy costs than Big Bird. For
summarization, we find that increasing model size is more energy efficient than
increasing sequence length for higher accuracy. However, this comes at the cost
of a large drop in inference speed. For question answering, we find that
smaller models are both more efficient and more accurate due to the larger
training batch sizes possible under a fixed resource budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying and Measuring Token-Level Sentiment Bias in Pre-trained Language Models with Prompts. (arXiv:2204.07289v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07289">
<div class="article-summary-box-inner">
<span><p>Due to the superior performance, large-scale pre-trained language models
(PLMs) have been widely adopted in many aspects of human society. However, we
still lack effective tools to understand the potential bias embedded in the
black-box models. Recent advances in prompt tuning show the possibility to
explore the internal mechanism of the PLMs. In this work, we propose two
token-level sentiment tests: Sentiment Association Test (SAT) and Sentiment
Shift Test (SST) which utilize the prompt as a probe to detect the latent bias
in the PLMs. Our experiments on the collection of sentiment datasets show that
both SAT and SST can identify sentiment bias in PLMs and SST is able to
quantify the bias. The results also suggest that fine-tuning can possibly
augment the existing bias in PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where to Go for the Holidays: Towards Mixed-Type Dialogs for Clarification of User Goals. (arXiv:2204.07299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07299">
<div class="article-summary-box-inner">
<span><p>Most dialog systems posit that users have figured out clear and specific
goals before starting an interaction. For example, users have determined the
departure, the destination, and the travel time for booking a flight. However,
in many scenarios, limited by experience and knowledge, users may know what
they need, but still struggle to figure out clear and specific goals by
determining all the necessary slots.
</p>
<p>In this paper, we identify this challenge and make a step forward by
collecting a new human-to-human mixed-type dialog corpus. It contains 5k dialog
sessions and 168k utterances for 4 dialog types and 5 domains. Within each
session, an agent first provides user-goal-related knowledge to help figure out
clear and specific goals, and then help achieve them.
</p>
<p>Furthermore, we propose a mixed-type dialog model with a novel Prompt-based
continual learning mechanism. Specifically, the mechanism enables the model to
continually strengthen its ability on any specific type by utilizing existing
dialog corpora effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Cross-Modal Understanding in Visual Dialog via Contrastive Learning. (arXiv:2204.07302v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07302">
<div class="article-summary-box-inner">
<span><p>Visual Dialog is a challenging vision-language task since the visual dialog
agent needs to answer a series of questions after reasoning over both the image
content and dialog history. Though existing methods try to deal with the
cross-modal understanding in visual dialog, they are still not enough in
ranking candidate answers based on their understanding of visual and textual
contexts. In this paper, we analyze the cross-modal understanding in visual
dialog based on the vision-language pre-training model VD-BERT and propose a
novel approach to improve the cross-modal understanding for visual dialog,
named ICMU. ICMU enhances cross-modal understanding by distinguishing different
pulled inputs (i.e. pulled images, questions or answers) based on four-way
contrastive learning. In addition, ICMU exploits the single-turn visual
question answering to enhance the visual dialog model's cross-modal
understanding to handle a multi-turn visually-grounded conversation.
Experiments show that the proposed approach improves the visual dialog model's
cross-modal understanding and brings satisfactory gain to the VisDial dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saga: A Platform for Continuous Construction and Serving of Knowledge At Scale. (arXiv:2204.07309v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07309">
<div class="article-summary-box-inner">
<span><p>We introduce Saga, a next-generation knowledge construction and serving
platform for powering knowledge-based applications at industrial scale. Saga
follows a hybrid batch-incremental design to continuously integrate billions of
facts about real-world entities and construct a central knowledge graph that
supports multiple production use cases with diverse requirements around data
freshness, accuracy, and availability. In this paper, we discuss the unique
challenges associated with knowledge graph construction at industrial scale,
and review the main components of Saga and how they address these challenges.
Finally, we share lessons-learned from a wide array of production use cases
powered by Saga.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding. (arXiv:2204.07316v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07316">
<div class="article-summary-box-inner">
<span><p>Transformer-based models are widely used in natural language understanding
(NLU) tasks, and multimodal transformers have been effective in visual-language
tasks. This study explores distilling visual information from pretrained
multimodal transformers to pretrained language encoders. Our framework is
inspired by cross-modal encoders' success in visual-language tasks while we
alter the learning objective to cater to the language-heavy characteristics of
NLU. After training with a small number of extra adapting steps and finetuned,
the proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in
general language understanding evaluation (GLUE), situations with adversarial
generations (SWAG) benchmarks, and readability benchmarks. We analyze the
performance of XDBERT on GLUE to show that the improvement is likely visually
grounded.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Entire-Space Models for Target-oriented Opinion Words Extraction. (arXiv:2204.07337v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07337">
<div class="article-summary-box-inner">
<span><p>Target-oriented opinion words extraction (TOWE) is a subtask of aspect-based
sentiment analysis (ABSA). Given a sentence and an aspect term occurring in the
sentence, TOWE extracts the corresponding opinion words for the aspect term.
TOWE has two types of instance. In the first type, aspect terms are associated
with at least one opinion word, while in the second type, aspect terms do not
have corresponding opinion words. However, previous researches trained and
evaluated their models with only the first type of instance, resulting in a
sample selection bias problem. Specifically, TOWE models were trained with only
the first type of instance, while these models would be utilized to make
inference on the entire space with both the first type of instance and the
second type of instance. Thus, the generalization performance will be hurt.
Moreover, the performance of these models on the first type of instance cannot
reflect their performance on entire space. To validate the sample selection
bias problem, four popular TOWE datasets containing only aspect terms
associated with at least one opinion word are extended and additionally include
aspect terms without corresponding opinion words. Experimental results on these
datasets show that training TOWE models on entire space will significantly
improve model performance and evaluating TOWE models only on the first type of
instance will overestimate model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaMemo: Language Modeling with Look-Ahead Memory. (arXiv:2204.07341v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07341">
<div class="article-summary-box-inner">
<span><p>Although Transformers with fully connected self-attentions are powerful to
model long-term dependencies, they are struggling to scale to long texts with
thousands of words in language modeling. One of the solutions is to equip the
model with a recurrence memory. However, existing approaches directly reuse
hidden states from the previous segment that encodes contexts in a
uni-directional way. As a result, this prohibits the memory to dynamically
interact with the current context that provides up-to-date information for
token prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo)
that enhances the recurrence memory by incrementally attending to the
right-side tokens, and interpolating with the old memory states to maintain
long-term information in the history. LaMemo embraces bi-directional attention
and segment recurrence with an additional computation overhead only linearly
proportional to the memory length. Experiments on widely used language modeling
benchmarks demonstrate its superiority over the baselines equipped with
different types of memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07356">
<div class="article-summary-box-inner">
<span><p>Pretrained models have produced great success in both Computer Vision (CV)
and Natural Language Processing (NLP). This progress leads to learning joint
representations of vision and language pretraining by feeding visual and
linguistic contents into a multi-layer transformer, Visual-Language Pretrained
Models (VLPMs). In this paper, we present an overview of the major advances
achieved in VLPMs for producing joint representations of vision and language.
As the preliminaries, we briefly describe the general task definition and
genetic architecture of VLPMs. We first discuss the language and vision data
encoding methods and then present the mainstream VLPM structure as the core
content. We further summarise several essential pretraining and fine-tuning
strategies. Finally, we highlight three future directions for both CV and NLP
researchers to provide insightful guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Revision by On-the-Fly Representation Optimization. (arXiv:2204.07359v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07359">
<div class="article-summary-box-inner">
<span><p>Text revision refers to a family of natural language generation tasks, where
the source and target sequences share moderate resemblance in surface form but
differentiate in attributes, such as text formality and simplicity. Current
state-of-the-art methods formulate these tasks as sequence-to-sequence learning
problems, which rely on large-scale parallel training corpus. In this paper, we
present an iterative in-place editing approach for text revision, which
requires no parallel data. In this approach, we simply fine-tune a pre-trained
Transformer with masked language modeling and attribute classification. During
inference, the editing at each iteration is realized by two-step span
replacement. At the first step, the distributed representation of the text
optimizes on the fly towards an attribute function. At the second step, a text
span is masked and another new one is proposed conditioned on the optimized
representation. The empirical experiments on two typical and important text
revision tasks, text formalization and text simplification, show the
effectiveness of our approach. It achieves competitive and even better
performance than state-of-the-art supervised methods on text simplification,
and gains better performance than strong unsupervised methods on text
formalization \footnote{Code and model are available at
\url{https://github.com/jingjingli01/OREO}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Surprisal in Issue Trackers Actionable?. (arXiv:2204.07363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07363">
<div class="article-summary-box-inner">
<span><p>Background. From information theory, surprisal is a measurement of how
unexpected an event is. Statistical language models provide a probabilistic
approximation of natural languages, and because surprisal is constructed with
the probability of an event occuring, it is therefore possible to determine the
surprisal associated with English sentences. The issues and pull requests of
software repository issue trackers give insight into the development process
and likely contain the surprising events of this process.
</p>
<p>Objective. Prior works have identified that unusual events in software
repositories are of interest to developers, and use simple code metrics-based
methods for detecting them. In this study we will propose a new method for
unusual event detection in software repositories using surprisal. With the
ability to find surprising issues and pull requests, we intend to further
analyse them to determine if they actually hold importance in a repository, or
if they pose a significant challenge to address. If it is possible to find bad
surprises early, or before they cause additional troubles, it is plausible that
effort, cost and time will be saved as a result.
</p>
<p>Method. After extracting the issues and pull requests from 5000 of the most
popular software repositories on GitHub, we will train a language model to
represent these issues. We will measure their perceived importance in the
repository, measure their resolution difficulty using several analogues,
measure the surprisal of each, and finally generate inferential statistics to
describe any correlations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART. (arXiv:2204.07367v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07367">
<div class="article-summary-box-inner">
<span><p>Word ordering is a constrained language generation task taking unordered
words as input. Existing work uses linear models and neural networks for the
task, yet pre-trained language models have not been studied in word ordering,
let alone why they help. We use BART as an instance and show its effectiveness
in the task. To explain why BART helps word ordering, we extend analysis with
probing and empirically identify that syntactic dependency knowledge in BART is
a reliable explanation. We also report performance gains with BART in the
related partial tree linearization task, which readily extends our analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Building a Personalized Dialogue Generator via Implicit User Persona Detection. (arXiv:2204.07372v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07372">
<div class="article-summary-box-inner">
<span><p>Current works in the generation of personalized dialogue primarily contribute
to the agent avoiding contradictory persona and driving the response more
informative. However, we found that the generated responses from these models
are mostly self-centered with little care for the other party since they ignore
the user's persona. Moreover, we consider high-quality transmission is
essentially built based on apprehending the persona of the other party.
Motivated by this, we propose a novel personalized dialogue generator by
detecting implicit user persona. Because it's difficult to collect a large
number of personas for each user, we attempt to model the user's potential
persona and its representation from the dialogue absence of any external
information. Perception variable and fader variable are conceived utilizing
Conditional Variational Inference. The two latent variables simulate the
process of people being aware of the other party's persona and producing the
corresponding expression in conversation. Finally, Posterior-discriminated
Regularization is presented to enhance the training procedure. Empirical
studies demonstrate that compared with the state-of-the-art methods, ours is
more concerned with the user's persona and outperforms in evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Email Spam Detection Using Hierarchical Attention Hybrid Deep Learning Method. (arXiv:2204.07390v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07390">
<div class="article-summary-box-inner">
<span><p>Email is one of the most widely used ways to communicate, with millions of
people and businesses relying on it to communicate and share knowledge and
information on a daily basis. Nevertheless, the rise in email users has
occurred a dramatic increase in spam emails in recent years. Processing and
managing emails properly for individuals and companies are getting increasingly
difficult. This article proposes a novel technique for email spam detection
that is based on a combination of convolutional neural networks, gated
recurrent units, and attention mechanisms. During system training, the network
is selectively focused on necessary parts of the email text. The usage of
convolution layers to extract more meaningful, abstract, and generalizable
features by hierarchical representation is the major contribution of this
study. Additionally, this contribution incorporates cross-dataset evaluation,
which enables the generation of more independent performance results from the
model's training dataset. According to cross-dataset evaluation results, the
proposed technique advances the results of the present attention-based
techniques by utilizing temporal convolutions, which give us more flexible
receptive field sizes are utilized. The suggested technique's findings are
compared to those of state-of-the-art models and show that our approach
outperforms them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Fine-grained Causal Reasoning and QA. (arXiv:2204.07408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07408">
<div class="article-summary-box-inner">
<span><p>Understanding causality is key to the success of NLP applications, especially
in high-stakes domains. Causality comes in various perspectives such as enable
and prevent that, despite their importance, have been largely ignored in the
literature. This paper introduces a novel fine-grained causal reasoning dataset
and presents a series of novel predictive tasks in NLP, such as causality
detection, event causality extraction, and Causal QA. Our dataset contains
human annotations of 25K cause-effect event pairs and 24K question-answering
pairs within multi-sentence samples, where each can have multiple causal
relationships. Through extensive experiments and analysis, we show that the
complex relations in our dataset bring unique challenges to state-of-the-art
methods across all three tasks and highlight potential research opportunities,
especially in developing "causal-thinking" methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ML_LTU at SemEval-2022 Task 4: T5 Towards Identifying Patronizing and Condescending Language. (arXiv:2204.07432v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07432">
<div class="article-summary-box-inner">
<span><p>This paper describes the system used by the Machine Learning Group of LTU in
subtask 1 of the SemEval-2022 Task 4: Patronizing and Condescending Language
(PCL) Detection. Our system consists of finetuning a pretrained
Text-to-Text-Transfer Transformer (T5) and innovatively reducing its
out-of-class predictions. The main contributions of this paper are 1) the
description of the implementation details of the T5 model we used, 2) analysis
of the successes &amp; struggles of the model in this task, and 3) ablation studies
beyond the official submission to ascertain the relative importance of data
split. Our model achieves an F1 score of 0.5452 on the official test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification. (arXiv:2204.07434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07434">
<div class="article-summary-box-inner">
<span><p>Document-level Event Causality Identification (DECI) aims to identify causal
relations between event pairs in a document. It poses a great challenge of
across-sentence reasoning without clear causal indicators. In this paper, we
propose a novel Event Relational Graph TransfOrmer (ERGO) framework for DECI,
which improves existing state-of-the-art (SOTA) methods upon two aspects.
First, we formulate DECI as a node classification problem by constructing an
event relational graph, without the needs of prior knowledge or tools. Second,
ERGO seamlessly integrates event-pair relation classification and global
inference, which leverages a Relational Graph Transformer (RGT) to capture the
potential causal chain. Besides, we introduce edge-building strategies and
adaptive focal loss to deal with the massive false positives caused by common
spurious correlation. Extensive experiments on two benchmark datasets show that
ERGO significantly outperforms previous SOTA methods (13.1% F1 gains on
average). We have conducted extensive quantitative analysis and case studies to
provide insights for future research directions (Section 4.8).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Political Communities on Twitter: Case Study of the 2022 French Presidential Election. (arXiv:2204.07436v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07436">
<div class="article-summary-box-inner">
<span><p>With the significant increase in users on social media platforms, a new means
of political campaigning has appeared. Twitter and Facebook are now notable
campaigning tools during elections. Indeed, the candidates and their parties
now take to the internet to interact and spread their ideas. In this paper, we
aim to identify political communities formed on Twitter during the 2022 French
presidential election and analyze each respective community. We create a
large-scale Twitter dataset containing 1.2 million users and 62.6 million
tweets that mention keywords relevant to the election. We perform community
detection on a retweet graph of users and propose an in-depth analysis of the
stance of each community. Finally, we attempt to detect offensive tweets and
automatic bots, comparing across communities in order to gain insight into each
candidate's supporter demographics and online campaign strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval. (arXiv:2204.07441v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07441">
<div class="article-summary-box-inner">
<span><p>Large-scale single-stream pre-training has shown dramatic performance in
image-text retrieval. Regrettably, it faces low inference efficiency due to
heavy attention layers. Recently, two-stream methods like CLIP and ALIGN with
high inference efficiency have also shown promising performance, however, they
only consider instance-level alignment between the two streams (thus there is
still room for improvement). To overcome these limitations, we propose a novel
COllaborative Two-Stream vision-language pretraining model termed COTS for
image-text retrieval by enhancing cross-modal interaction. In addition to
instance level alignment via momentum contrastive learning, we leverage two
extra levels of cross-modal interactions in our COTS: (1) Token-level
interaction - a masked visionlanguage modeling (MVLM) learning objective is
devised without using a cross-stream network module, where variational
autoencoder is imposed on the visual encoder to generate visual tokens for each
image. (2) Task-level interaction - a KL-alignment learning objective is
devised between text-to-image and image-to-text retrieval tasks, where the
probability distribution per task is computed with the negative queues in
momentum contrastive learning. Under a fair comparison setting, our COTS
achieves the highest performance among all two-stream methods and comparable
performance (but with 10,800X faster in inference) w.r.t. the latest
single-stream methods. Importantly, our COTS is also applicable to
text-to-video retrieval, yielding new state-ofthe-art on the widely-used
MSR-VTT dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stretching Sentence-pair NLI Models to Reason over Long Documents and Clusters. (arXiv:2204.07447v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07447">
<div class="article-summary-box-inner">
<span><p>Natural Language Inference (NLI) has been extensively studied by the NLP
community as a framework for estimating the semantic relation between sentence
pairs. While early work identified certain biases in NLI models, recent
advancements in modeling and datasets demonstrated promising performance. In
this work, we further explore the direct zero-shot applicability of NLI models
to real applications, beyond the sentence-pair setting they were trained on.
First, we analyze the robustness of these models to longer and out-of-domain
inputs. Then, we develop new aggregation methods to allow operating over full
documents, reaching state-of-the-art performance on the ContractNLI dataset.
Interestingly, we find NLI scores to provide strong retrieval signals, leading
to more relevant evidence extractions compared to common similarity-based
methods. Finally, we go further and investigate whole document clusters to
identify both discrepancies and consensus among sources. In a test case, we
find real inconsistencies between Wikipedia pages in different languages about
the same topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Qtrade AI at SemEval-2022 Task 11: An Unified Framework for Multilingual NER Task. (arXiv:2204.07459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07459">
<div class="article-summary-box-inner">
<span><p>This paper describes our system, which placed third in the Multilingual Track
(subtask 11), fourth in the Code-Mixed Track (subtask 12), and seventh in the
Chinese Track (subtask 9) in the SemEval 2022 Task 11: MultiCoNER Multilingual
Complex Named Entity Recognition. Our system's key contributions are as
follows: 1) For multilingual NER tasks, we offer an unified framework with
which one can easily execute single-language or multilingual NER tasks, 2) for
low-resource code-mixed NER task, one can easily enhance his or her dataset
through implementing several simple data augmentation methods and 3) for
Chinese tasks, we propose a model that can capture Chinese lexical semantic,
lexical border, and lexical graph structural information. Finally, our system
achieves macro-f1 scores of 77.66, 84.35, and 74.00 on subtasks 11, 12, and 9,
respectively, during the testing phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Pre-trained Language Models with Syntactic Dependency Prediction Task for Chinese Semantic Error Recognition. (arXiv:2204.07464v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07464">
<div class="article-summary-box-inner">
<span><p>Existing Chinese text error detection mainly focuses on spelling and simple
grammatical errors. These errors have been studied extensively and are
relatively simple for humans. On the contrary, Chinese semantic errors are
understudied and more complex that humans cannot easily recognize. The task of
this paper is Chinese Semantic Error Recognition (CSER), a binary
classification task to determine whether a sentence contains semantic errors.
The current research has no effective method to solve this task. In this paper,
we inherit the model structure of BERT and design several syntax-related
pre-training tasks so that the model can learn syntactic knowledge. Our
pre-training tasks consider both the directionality of the dependency structure
and the diversity of the dependency relationship. Due to the lack of a
published dataset for CSER, we build a high-quality dataset for CSER for the
first time named Corpus of Chinese Linguistic Semantic Acceptability (CoCLSA).
The experimental results on the CoCLSA show that our methods outperform
universal pre-trained models and syntax-infused models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixture of Experts for Biomedical Question Answering. (arXiv:2204.07469v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07469">
<div class="article-summary-box-inner">
<span><p>Biomedical Question Answering (BQA) has attracted increasing attention in
recent years due to its promising application prospect. It is a challenging
task because the biomedical questions are professional and usually vary widely.
Existing question answering methods answer all questions with a homogeneous
model, leading to various types of questions competing for the shared
parameters, which will confuse the model decision for each single type of
questions. In this paper, in order to alleviate the parameter competition
problem, we propose a Mixture-of-Expert (MoE) based question answering method
called MoEBQA that decouples the computation for different types of questions
by sparse routing. To be specific, we split a pretrained Transformer model into
bottom and top blocks. The bottom blocks are shared by all the examples, aiming
to capture the general features. The top blocks are extended to an MoE version
that consists of a series of independent experts, where each example is
assigned to a few experts according to its underlying question type. MoEBQA
automatically learns the routing strategy in an end-to-end manner so that each
expert tends to deal with the question types it is expert in. We evaluate
MoEBQA on three BQA datasets constructed based on real examinations. The
results show that our MoE extension significantly boosts the performance of
question answering models and achieves new state-of-the-art performance. In
addition, we elaborately analyze our MoE modules to reveal how MoEBQA works and
find that it can automatically group the questions into human-readable
clusters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polling Latent Opinions: A Method for Computational Sociolinguistics Using Transformer Language Models. (arXiv:2204.07483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07483">
<div class="article-summary-box-inner">
<span><p>Text analysis of social media for sentiment, topic analysis, and other
analysis depends initially on the selection of keywords and phrases that will
be used to create the research corpora. However, keywords that researchers
choose may occur infrequently, leading to errors that arise from using small
samples. In this paper, we use the capacity for memorization, interpolation,
and extrapolation of Transformer Language Models such as the GPT series to
learn the linguistic behaviors of a subgroup within larger corpora of Yelp
reviews. We then use prompt-based queries to generate synthetic text that can
be analyzed to produce insights into specific opinions held by the populations
that the models were trained on. Once learned, more specific sentiment queries
can be made of the model with high levels of accuracy when compared to
traditional keyword searches. We show that even in cases where a specific
keyphrase is limited or not present at all in the training corpora, the GPT is
able to accurately generate large volumes of text that have the correct
sentiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Passage Retrieval with Zero-Shot Question Generation. (arXiv:2204.07496v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07496">
<div class="article-summary-box-inner">
<span><p>We propose a simple and effective re-ranking method for improving passage
retrieval in open question answering. The re-ranker re-scores retrieved
passages with a zero-shot question generation model, which uses a pre-trained
language model to compute the probability of the input question conditioned on
a retrieved passage. This approach can be applied on top of any retrieval
method (e.g. neural or keyword-based), does not require any domain- or
task-specific training (and therefore is expected to generalize better to data
distribution shifts), and provides rich cross-attention between query and
passage (i.e. it must explain every token in the question). When evaluated on a
number of open-domain retrieval datasets, our re-ranker improves strong
unsupervised retrieval models by 6%-18% absolute and strong supervised models
by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new
state-of-the-art results on full open-domain question answering by simply
adding the new re-ranker to existing models with no further changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Game-Playing Agents with Natural Language Annotations. (arXiv:2204.07531v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07531">
<div class="article-summary-box-inner">
<span><p>We present a new dataset containing 10K human-annotated games of Go and show
how these natural language annotations can be used as a tool for model
interpretability. Given a board state and its associated comment, our approach
uses linear probing to predict mentions of domain-specific terms (e.g., ko,
atari) from the intermediate state representations of game-playing agents like
AlphaGo Zero. We find these game concepts are nontrivially encoded in two
distinct policy networks, one trained via imitation learning and another
trained via reinforcement learning. Furthermore, mentions of domain-specific
terms are most easily predicted from the later layers of both models,
suggesting that these policy networks encode high-level abstractions similar to
those used in the natural language annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Judgement as a Compass to Navigate Automatic Metrics for Formality Transfer. (arXiv:2204.07549v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07549">
<div class="article-summary-box-inner">
<span><p>Although text style transfer has witnessed rapid development in recent years,
there is as yet no established standard for evaluation, which is performed
using several automatic metrics, lacking the possibility of always resorting to
human judgement. We focus on the task of formality transfer, and on the three
aspects that are usually evaluated: style strength, content preservation, and
fluency. To cast light on how such aspects are assessed by common and new
metrics, we run a human-based evaluation and perform a rich correlation
analysis. We are then able to offer some recommendations on the use of such
metrics in formality transfer, also with an eye to their generalisability (or
not) to related tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarization with Graphical Elements. (arXiv:2204.07551v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07551">
<div class="article-summary-box-inner">
<span><p>Automatic text summarization has experienced substantial progress in recent
years. With this progress, the question has arisen whether the types of
summaries that are typically generated by automatic summarization models align
with users' needs. Ter Hoeve et al (2020) answer this question negatively.
Amongst others, they recommend focusing on generating summaries with more
graphical elements. This is in line with what we know from the
psycholinguistics literature about how humans process text. Motivated from
these two angles, we propose a new task: summarization with graphical elements,
and we verify that these summaries are helpful for a critical mass of people.
We collect a high quality human labeled dataset to support research into the
task. We present a number of baseline methods that show that the task is
interesting and challenging. Hence, with this work we hope to inspire a new
line of research within the automatic summarization community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Rare Word Recognition with LM-aware MWER Training. (arXiv:2204.07553v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07553">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) significantly improve the recognition accuracy of
end-to-end (E2E) models on words rarely seen during training, when used in
either the shallow fusion or the rescoring setups. In this work, we introduce
LMs in the learning of hybrid autoregressive transducer (HAT) models in the
discriminative training framework, to mitigate the training versus inference
gap regarding the use of LMs. For the shallow fusion setup, we use LMs during
both hypotheses generation and loss computation, and the LM-aware MWER-trained
model achieves 10\% relative improvement over the model trained with standard
MWER on voice search test sets containing rare words. For the rescoring setup,
we learn a small neural module to generate per-token fusion weights in a
data-dependent manner. This model achieves the same rescoring WER as regular
MWER-trained model, but without the need for sweeping fusion weights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Idiom Paraphrasing. (arXiv:2204.07555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07555">
<div class="article-summary-box-inner">
<span><p>Idioms, are a kind of idiomatic expression in Chinese, most of which consist
of four Chinese characters. Due to the properties of non-compositionality and
metaphorical meaning, Chinese Idioms are hard to be understood by children and
non-native speakers. This study proposes a novel task, denoted as Chinese Idiom
Paraphrasing (CIP). CIP aims to rephrase idioms-included sentences to
non-idiomatic ones under the premise of preserving the original sentence's
meaning. Since the sentences without idioms are easier handled by Chinese NLP
systems, CIP can be used to pre-process Chinese datasets, thereby facilitating
and improving the performance of Chinese NLP tasks, e.g., machine translation
system, Chinese idiom cloze, and Chinese idiom embeddings. In this study, CIP
task is treated as a special paraphrase generation task. To circumvent
difficulties in acquiring annotations, we first establish a large-scale CIP
dataset based on human and machine collaboration, which consists of 115,530
sentence pairs. We further deploy three baselines and two novel CIP approaches
to deal with CIP problems. The results show that the proposed methods have
better performances than the baselines based on the established CIP dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streaming Align-Refine for Non-autoregressive Deliberation. (arXiv:2204.07556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07556">
<div class="article-summary-box-inner">
<span><p>We propose a streaming non-autoregressive (non-AR) decoding algorithm to
deliberate the hypothesis alignment of a streaming RNN-T model. Our algorithm
facilitates a simple greedy decoding procedure, and at the same time is capable
of producing the decoding result at each frame with limited right context, thus
enjoying both high efficiency and low latency. These advantages are achieved by
converting the offline Align-Refine algorithm to be streaming-compatible, with
a novel transformer decoder architecture that performs local self-attentions
for both text and audio, and a time-aligned cross-attention at each layer.
Furthermore, we perform discriminative training of our model with the minimum
word error rate (MWER) criterion, which has not been done in the non-AR
decoding literature. Experiments on voice search datasets and Librispeech show
that with reasonable right context, our streaming model performs as well as the
offline counterpart, and discriminative training leads to further WER gain when
the first-pass model has small capacity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Factuality in Text Simplification. (arXiv:2204.07562v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07562">
<div class="article-summary-box-inner">
<span><p>Automated simplification models aim to make input texts more readable. Such
methods have the potential to make complex information accessible to a wider
audience, e.g., providing access to recent medical literature which might
otherwise be impenetrable for a lay reader. However, such models risk
introducing errors into automatically simplified texts, for instance by
inserting statements unsupported by the corresponding original text, or by
omitting key information. Providing more readable but inaccurate versions of
texts may in many cases be worse than providing no such access at all. The
problem of factual accuracy (and the lack thereof) has received heightened
attention in the context of summarization models, but the factuality of
automatically simplified texts has not been investigated. We introduce a
taxonomy of errors that we use to analyze both references drawn from standard
simplification datasets and state-of-the-art model outputs. We find that errors
often appear in both that are not captured by existing evaluation metrics,
motivating a need for research into ensuring the factual accuracy of automated
simplification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation Benchmarks for Spanish Sentence Representations. (arXiv:2204.07571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07571">
<div class="article-summary-box-inner">
<span><p>Due to the success of pre-trained language models, versions of languages
other than English have been released in recent years. This fact implies the
need for resources to evaluate these models. In the case of Spanish, there are
few ways to systematically assess the models' quality. In this paper, we narrow
the gap by building two evaluation benchmarks. Inspired by previous work
(Conneau and Kiela, 2018; Chen et al., 2019), we introduce Spanish SentEval and
Spanish DiscoEval, aiming to assess the capabilities of stand-alone and
discourse-aware sentence representations, respectively. Our benchmarks include
considerable pre-existing and newly constructed datasets that address different
tasks from various domains. In addition, we evaluate and analyze the most
recent pre-trained Spanish language models to exhibit their capabilities and
limitations. As an example, we discover that for the case of discourse
evaluation tasks, mBERT, a language model trained on multiple languages,
usually provides a richer latent representation than models trained only with
documents in Spanish. We hope our contribution will motivate a fairer, more
comparable, and less cumbersome way to evaluate future Spanish language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consecutive Decoding for Speech-to-text Translation. (arXiv:2009.09737v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09737">
<div class="article-summary-box-inner">
<span><p>Speech-to-text translation (ST), which directly translates the source
language speech to the target language text, has attracted intensive attention
recently. However, the combination of speech recognition and machine
translation in a single model poses a heavy burden on the direct cross-modal
cross-lingual mapping. To reduce the learning difficulty, we propose
COnSecutive Transcription and Translation (COSTT), an integral approach for
speech-to-text translation. The key idea is to generate source transcript and
target translation text with a single decoder. It benefits the model training
so that additional large parallel text corpus can be fully exploited to enhance
the speech translation training. Our method is verified on three mainstream
datasets, including Augmented LibriSpeech English-French dataset, IWSLT2018
English-German dataset, and TED English-Chinese dataset. Experiments show that
our proposed COSTT outperforms or on par with the previous state-of-the-art
methods on the three datasets. We have released our code at
\url{https://github.com/dqqcasia/st}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QAConv: Question Answering on Informative Conversations. (arXiv:2105.06912v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06912">
<div class="article-summary-box-inner">
<span><p>This paper introduces QAConv, a new question answering (QA) dataset that uses
conversations as a knowledge source. We focus on informative conversations,
including business emails, panel discussions, and work channels. Unlike
open-domain and task-oriented dialogues, these conversations are usually long,
complex, asynchronous, and involve strong domain knowledge. In total, we
collect 34,608 QA pairs from 10,259 selected conversations with both
human-written and machine-generated questions. We use a question generator and
a dialogue summarizer as auxiliary tools to collect and recommend questions.
The dataset has two testing scenarios: chunk mode and full mode, depending on
whether the grounded partial conversation is provided or retrieved.
Experimental results show that state-of-the-art pretrained QA systems have
limited zero-shot performance and tend to predict our questions as
unanswerable. Our dataset provides a new training and evaluation testbed to
facilitate QA on conversations research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieve-then-extract Based Knowledge Graph Querying Using Graph Neural Networks. (arXiv:2111.10541v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10541">
<div class="article-summary-box-inner">
<span><p>The abstract of Retrieve-then-extract Based Knowledge Graph Querying Using
Graph Neural Networks will be updated here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08879">
<div class="article-summary-box-inner">
<span><p>Most language grounding models learn to select the referred object from a
pool of object proposals provided by a pre-trained detector. This object
proposal bottleneck is limiting because an utterance may refer to visual
entities at various levels of granularity, such as the chair, the leg of a
chair, or the tip of the front leg of a chair, which may be missed by the
detector. Recently, MDETR introduced a language grounding model for 2D images
that do not have such a box proposal bottleneck; instead of selecting objects
from a proposal pool, it instead decodes the referenced object boxes directly
from image and language features and achieves big leaps in performance. We
propose a language grounding model for 3D scenes built on MDETR, which we call
BEAUTY-DETR, from bottom-up and top-down DETR. BEAUTY-DETR attends on an
additional object proposal pool computed bottom-up from a pre-trained detector.
Yet it decodes referenced objects without selecting them from the pool. In this
way, it uses powerful object detectors to help ground language without being
restricted by their misses. Second, BEAUTY-DETR augments supervision from
language grounding annotations by configuring object detection annotations as
language prompts to be grounded in images. The proposed model sets a new
state-of-the-art across popular 3D language grounding benchmarks with
significant performance gains over previous 3D approaches (12.6% on SR3D, 11.6%
on NR3D and 6.3% on ScanRefer). It outperforms a straightforward MDETR for the
3D point clouds method we implemented by 6.7% on SR3D, 11.8% on NR3D and 5% on
the ScanRefer benchmark. When applied to language grounding in 2D images, it
performs on par with MDETR. We ablate each of the design choices of the model
and quantify their contribution to performance. Code and checkpoints are
available at https://github.com/nickgkan/beauty_detr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences. (arXiv:2201.11838v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11838">
<div class="article-summary-box-inner">
<span><p>Transformers-based models, such as BERT, have dramatically improved the
performance for various natural language processing tasks. The clinical
knowledge enriched model, namely ClinicalBERT, also achieved state-of-the-art
results when performed on clinical named entity recognition and natural
language inference tasks. One of the core limitations of these transformers is
the substantial memory consumption due to their full self-attention mechanism.
To overcome this, long sequence transformer models, e.g. Longformer and
BigBird, were proposed with the idea of sparse attention mechanism to reduce
the memory usage from quadratic to the sequence length to a linear scale. These
models extended the maximum input sequence length from 512 to 4096, which
enhanced the ability of modeling long-term dependency and consequently achieved
optimal results in a variety of tasks. Inspired by the success of these long
sequence transformer models, we introduce two domain enriched language models,
namely Clinical-Longformer and Clinical-BigBird, which are pre-trained from
large-scale clinical corpora. We evaluate both pre-trained models using 10
baseline tasks including named entity recognition, question answering, and
document classification tasks. The results demonstrate that Clinical-Longformer
and Clinical-BigBird consistently and significantly outperform ClinicalBERT as
well as other short-sequence transformers in all downstream tasks. We have made
our source code available at
[https://github.com/luoyuanlab/Clinical-Longformer] the pre-trained models
available for public download at:
[https://huggingface.co/yikuan8/Clinical-Longformer].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-constraint Optimal Transport for Entity Alignment with Dangling Cases. (arXiv:2203.05744v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05744">
<div class="article-summary-box-inner">
<span><p>Entity alignment (EA) merges knowledge graphs (KGs) by identifying the
equivalent entities in different graphs, which can effectively enrich knowledge
representations of KGs. However, in practice, different KGs often include
dangling entities whose counterparts cannot be found in the other graph, which
limits the performance of EA methods. To improve EA with dangling entities, we
propose an unsupervised method called Semi-constraint Optimal Transport for
Entity Alignment in Dangling cases (SoTead). Our main idea is to model the
entity alignment between two KGs as an optimal transport problem from one KG's
entities to the others. First, we set pseudo entity pairs between KGs based on
pretrained word embeddings. Then, we conduct contrastive metric learning to
obtain the transport cost between each entity pair. Finally, we introduce a
virtual entity for each KG to "align" the dangling entities from the other KGs,
which relaxes the optimization constraints and leads to a semi-constraint
optimal transport. In the experimental part, we first show the superiority of
SoTead on a commonly-used entity alignment dataset. Besides, to analyze the
ability for dangling entity detection with other baselines, we construct a
medical cross-lingual knowledge graph dataset, MedED, where our SoTead also
reaches state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Text-to-Program for Question Answering on Structured Electronic Health Records. (arXiv:2203.06918v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06918">
<div class="article-summary-box-inner">
<span><p>Question Answering on Electronic Health Records (EHR-QA) has a significant
impact on the healthcare domain, and it is being actively studied. Previous
research on structured EHR-QA focuses on converting natural language queries
into query language such as SQL or SPARQL (NLQ2Query), so the problem scope is
limited to pre-defined data types by the specific query language. In order to
expand the EHR-QA task beyond this limitation to handle multi-modal medical
data and solve complex inference in the future, more primitive systemic
language is needed. In this paper, we design the program-based model
(NLQ2Program) for EHR-QA as the first step towards the future direction. We
tackle MIMICSPARQL*, the graph-based EHR-QA dataset, via a program-based
approach in a semi-supervised manner in order to overcome the absence of gold
programs. Without the gold program, our proposed model shows comparable
performance to the previous state-of-the-art model, which is an NLQ2Query model
(0.9% gain). In addition, for a reliable EHR-QA model, we apply the uncertainty
decomposition method to measure the ambiguity in the input question. We
empirically confirmed data uncertainty is most indicative of the ambiguity in
the input question.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-scale Bilingual Language-Image Contrastive Learning. (arXiv:2203.14463v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14463">
<div class="article-summary-box-inner">
<span><p>This paper is a technical report to share our experience and findings
building a Korean and English bilingual multimodal model. While many of the
multimodal datasets focus on English and multilingual multimodal research uses
machine-translated texts, employing such machine-translated texts is limited to
describing unique expressions, cultural information, and proper noun in
languages other than English. In this work, we collect 1.1 billion image-text
pairs (708 million Korean and 476 million English) and train a bilingual
multimodal model named KELIP. We introduce simple yet effective training
schemes, including MAE pre-training and multi-crop augmentation. Extensive
experiments demonstrate that a model trained with such training schemes shows
competitive performance in both languages. Moreover, we discuss
multimodal-related research questions: 1) strong augmentation-based methods can
distract the model from learning proper multimodal relations; 2) training
multimodal model without cross-lingual relation can learn the relation via
visual semantics; 3) our bilingual KELIP can capture cultural differences of
visual semantics for the same meaning of words; 4) a large-scale multimodal
model can be used for multimodal feature analogy. We hope that this work will
provide helpful experience and findings for future research. We provide an
open-source pre-trained KELIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Attention through Gradient-Based Learned Runtime Pruning. (arXiv:2204.03227v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03227">
<div class="article-summary-box-inner">
<span><p>Self-attention is a key enabler of state-of-art accuracy for various
transformer-based Natural Language Processing models. This attention mechanism
calculates a correlation score for each word with respect to the other words in
a sentence. Commonly, only a small subset of words highly correlates with the
word under attention, which is only determined at runtime. As such, a
significant amount of computation is inconsequential due to low attention
scores and can potentially be pruned. The main challenge is finding the
threshold for the scores below which subsequent computation will be
inconsequential. Although such a threshold is discrete, this paper formulates
its search through a soft differentiable regularizer integrated into the loss
function of the training. This formulation piggy backs on the back-propagation
training to analytically co-optimize the threshold and the weights
simultaneously, striking a formally optimal balance between accuracy and
computation pruning. To best utilize this mathematical innovation, we devise a
bit-serial architecture, dubbed LeOPArd, for transformer language models with
bit-level early termination microarchitectural mechanism. We evaluate our
design across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision
transformer models. Post-layout results show that, on average, LeOPArd yields
1.9x and 3.9x speedup and energy reduction, respectively, while keeping the
average accuracy virtually intact (&lt;0.2% degradation)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Schema Graph Fusion Network for Multi-Domain Dialogue State Tracking. (arXiv:2204.06677v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06677">
<div class="article-summary-box-inner">
<span><p>Dialogue State Tracking (DST) aims to keep track of users' intentions during
the course of a conversation. In DST, modelling the relations among domains and
slots is still an under-studied problem. Existing approaches that have
considered such relations generally fall short in: (1) fusing prior slot-domain
membership relations and dialogue-aware dynamic slot relations explicitly, and
(2) generalizing to unseen domains. To address these issues, we propose a novel
\textbf{D}ynamic \textbf{S}chema \textbf{G}raph \textbf{F}usion
\textbf{Net}work (\textbf{DSGFNet}), which generates a dynamic schema graph to
explicitly fuse the prior slot-domain membership relations and dialogue-aware
dynamic slot relations. It also uses the schemata to facilitate knowledge
transfer to new domains. DSGFNet consists of a dialogue utterance encoder, a
schema graph encoder, a dialogue-aware schema graph evolving network, and a
schema graph enhanced dialogue state decoder. Empirical results on benchmark
datasets (i.e., SGD, MultiWOZ2.1, and MultiWOZ2.2), show that DSGFNet
outperforms existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges for Open-domain Targeted Sentiment Analysis. (arXiv:2204.06893v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06893">
<div class="article-summary-box-inner">
<span><p>Since previous studies on open-domain targeted sentiment analysis are limited
in dataset domain variety and sentence level, we propose a novel dataset
consisting of 6,013 human-labeled data to extend the data domains in topics of
interest and document level. Furthermore, we offer a nested target annotation
schema to extract the complete sentiment information in documents, boosting the
practicality and effectiveness of open-domain targeted sentiment analysis.
Moreover, we leverage the pre-trained model BART in a sequence-to-sequence
generation method for the task. Benchmark results show that there exists large
room for improvement of open-domain targeted sentiment analysis. Meanwhile,
experiments have shown that challenges remain in the effective use of
open-domain data, long documents, the complexity of target structure, and
domain variances.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Object Segmentation in 3D Point Clouds. (arXiv:2204.07183v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07183">
<div class="article-summary-box-inner">
<span><p>Deep learning depends on large amounts of labeled training data. Manual
labeling is expensive and represents a bottleneck, especially for tasks such as
segmentation, where labels must be assigned down to the level of individual
points. That challenge is even more daunting for 3D data: 3D point clouds
contain millions of points per scene, and their accurate annotation is markedly
more time-consuming. The situation is further aggravated by the added
complexity of user interfaces for 3D point clouds, which slows down annotation
even more. For the case of 2D image segmentation, interactive techniques have
become common, where user feedback in the form of a few clicks guides a
segmentation algorithm -- nowadays usually a neural network -- to achieve an
accurate labeling with minimal effort. Surprisingly, interactive segmentation
of 3D scenes has not been explored much. Previous work has attempted to obtain
accurate 3D segmentation masks using human feedback from the 2D domain, which
is only possible if correctly aligned images are available together with the 3D
point cloud, and it involves switching between the 2D and 3D domains. Here, we
present an interactive 3D object segmentation method in which the user
interacts directly with the 3D point cloud. Importantly, our model does not
require training data from the target domain: when trained on ScanNet, it
performs well on several other datasets with different data characteristics as
well as different object classes. Moreover, our method is orthogonal to
supervised (instance) segmentation methods and can be combined with them to
refine automatic segmentations with minimal human effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Compositional Consistency for Video Question Answering. (arXiv:2204.07190v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07190">
<div class="article-summary-box-inner">
<span><p>Recent video question answering benchmarks indicate that state-of-the-art
models struggle to answer compositional questions. However, it remains unclear
which types of compositional reasoning cause models to mispredict. Furthermore,
it is difficult to discern whether models arrive at answers using compositional
reasoning or by leveraging data biases. In this paper, we develop a question
decomposition engine that programmatically deconstructs a compositional
question into a directed acyclic graph of sub-questions. The graph is designed
such that each parent question is a composition of its children. We present
AGQA-Decomp, a benchmark containing $2.3M$ question graphs, with an average of
$11.49$ sub-questions per graph, and $4.55M$ total new sub-questions. Using
question graphs, we evaluate three state-of-the-art models with a suite of
novel compositional consistency metrics. We find that models either cannot
reason correctly through most compositions or are reliant on incorrect
reasoning to reach answers, frequently contradicting themselves or achieving
high accuracies when failing at intermediate reasoning steps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLGAN: Generative Adversarial Networks for Power-Line Segmentation in Aerial Images. (arXiv:2204.07243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07243">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation of power lines in various aerial images is very
important for UAV flight safety. The complex background and very thin
structures of power lines, however, make it an inherently difficult task in
computer vision. This paper presents PLGAN, a simple yet effective method based
on generative adversarial networks, to segment power lines from aerial images
with different backgrounds. Instead of directly using the adversarial networks
to generate the segmentation, we take their certain decoding features and embed
them into another semantic segmentation network by considering more context,
geometry, and appearance information of power lines. We further exploit the
appropriate form of the generated images for high-quality feature embedding and
define a new loss function in the Hough-transform parameter space to enhance
the segmentation of very thin power lines. Extensive experiments and
comprehensive analysis demonstrate that our proposed PLGAN outperforms the
prior state-of-the-art methods for semantic segmentation and line detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robotic and Generative Adversarial Attacks in Offline Writer-independent Signature Verification. (arXiv:2204.07246v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07246">
<div class="article-summary-box-inner">
<span><p>This study explores how robots and generative approaches can be used to mount
successful false-acceptance adversarial attacks on signature verification
systems. Initially, a convolutional neural network topology and data
augmentation strategy are explored and tuned, producing an 87.12% accurate
model for the verification of 2,640 human signatures. Two robots are then
tasked with forging 50 signatures, where 25 are used for the verification
attack, and the remaining 25 are used for tuning of the model to defend against
them. Adversarial attacks on the system show that there exists an information
security risk; the Line-us robotic arm can fool the system 24% of the time and
the iDraw 2.0 robot 32% of the time. A conditional GAN finds similar success,
with around 30% forged signatures misclassified as genuine. Following fine-tune
transfer learning of robotic and generative data, adversarial attacks are
reduced below the model threshold by both robots and the GAN. It is observed
that tuning the model reduces the risk of attack by robots to 8% and 12%, and
that conditional generative adversarial attacks can be reduced to 4% when 25
images are presented and 5% when 1000 images are presented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Early Myocardial Infarction Detection with One-Class Classification over Multi-view Echocardiography. (arXiv:2204.07253v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07253">
<div class="article-summary-box-inner">
<span><p>Myocardial infarction (MI) is the leading cause of mortality and morbidity in
the world. Early therapeutics of MI can ensure the prevention of further
myocardial necrosis. Echocardiography is the fundamental imaging technique that
can reveal the earliest sign of MI. However, the scarcity of echocardiographic
datasets for the MI detection is the major issue for training data-driven
classification algorithms. In this study, we propose a framework for early
detection of MI over multi-view echocardiography that leverages one-class
classification (OCC) techniques. The OCC techniques are used to train a model
for detecting a specific target class using instances from that particular
category only. We investigated the usage of uni-modal and multi-modal one-class
classification techniques in the proposed framework using the HMC-QU dataset
that includes apical 4-chamber (A4C) and apical 2-chamber (A2C) views in a
total of 260 echocardiography recordings. Experimental results show that the
multi-modal approach achieves a sensitivity level of 85.23% and F1-Score of
80.21%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imposing Consistency for Optical Flow Estimation. (arXiv:2204.07262v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07262">
<div class="article-summary-box-inner">
<span><p>Imposing consistency through proxy tasks has been shown to enhance
data-driven learning and enable self-supervision in various tasks. This paper
introduces novel and effective consistency strategies for optical flow
estimation, a problem where labels from real-world data are very challenging to
derive. More specifically, we propose occlusion consistency and zero forcing in
the forms of self-supervised learning and transformation consistency in the
form of semi-supervised learning. We apply these consistency techniques in a
way that the network model learns to describe pixel-level motions better while
requiring no additional annotations. We demonstrate that our consistency
strategies applied to a strong baseline network model using the original
datasets and labels provide further improvements, attaining the
state-of-the-art results on the KITTI-2015 scene flow benchmark in the
non-stereo category. Our method achieves the best foreground accuracy (4.33% in
Fl-all) over both the stereo and non-stereo categories, even though using only
monocular image inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Spatially Varying Pixel Exposures for Motion Deblurring. (arXiv:2204.07267v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07267">
<div class="article-summary-box-inner">
<span><p>Computationally removing the motion blur introduced by camera shake or object
motion in a captured image remains a challenging task in computational
photography. Deblurring methods are often limited by the fixed global exposure
time of the image capture process. The post-processing algorithm either must
deblur a longer exposure that contains relatively little noise or denoise a
short exposure that intentionally removes the opportunity for blur at the cost
of increased noise. We present a novel approach of leveraging spatially varying
pixel exposures for motion deblurring using next-generation focal-plane
sensor--processors along with an end-to-end design of these exposures and a
machine learning--based motion-deblurring framework. We demonstrate in
simulation and a physical prototype that learned spatially varying pixel
exposures (L-SVPE) can successfully deblur scenes while recovering high
frequency detail. Our work illustrates the promising role that focal-plane
sensor--processors can play in the future of computational imaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model-agnostic Multi-Domain Learning with Domain-Specific Adapters for Action Recognition. (arXiv:2204.07270v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07270">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a multi-domain learning model for action
recognition. The proposed method inserts domain-specific adapters between
layers of domain-independent layers of a backbone network. Unlike a multi-head
network that switches classification heads only, our model switches not only
the heads, but also the adapters for facilitating to learn feature
representations universal to multiple domains. Unlike prior works, the proposed
method is model-agnostic and doesn't assume model structures unlike prior
works. Experimental results on three popular action recognition datasets
(HMDB51, UCF101, and Kinetics-400) demonstrate that the proposed method is more
effective than a multi-head architecture and more efficient than separately
training models for each domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invisible-to-Visible: Privacy-Aware Human Instance Segmentation using Airborne Ultrasound via Collaborative Learning Variational Autoencoder. (arXiv:2204.07280v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07280">
<div class="article-summary-box-inner">
<span><p>In action understanding in indoor, we have to recognize human pose and action
considering privacy. Although camera images can be used for highly accurate
human action recognition, camera images do not preserve privacy. Therefore, we
propose a new task for human instance segmentation from invisible information,
especially airborne ultrasound, for action recognition. To perform instance
segmentation from invisible information, we first convert sound waves to
reflected sound directional images (sound images). Although the sound images
can roughly identify the location of a person, the detailed shape is ambiguous.
To address this problem, we propose a collaborative learning variational
autoencoder (CL-VAE) that simultaneously uses sound and RGB images during
training. In inference, it is possible to obtain instance segmentation results
only from sound images. As a result of performance verification, CL-VAE could
estimate human instance segmentations more accurately than conventional
variational autoencoder and some other models. Since this method can obtain
human segmentations individually, it could be applied to human action
recognition tasks with privacy protection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guided Co-Modulated GAN for 360{\deg} Field of View Extrapolation. (arXiv:2204.07286v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07286">
<div class="article-summary-box-inner">
<span><p>We propose a method to extrapolate a 360{\deg} field of view from a single
image that allows for user-controlled synthesis of the out-painted content. To
do so, we propose improvements to an existing GAN-based in-painting
architecture for out-painting panoramic image representation. Our method
obtains state-of-the-art results and outperforms previous methods on standard
image quality metrics. To allow controlled synthesis of out-painting, we
introduce a novel guided co-modulation framework, which drives the image
generation process with a common pretrained discriminative model. Doing so
maintains the high visual quality of generated panoramas while enabling
user-controlled semantic content in the extrapolated field of view. We
demonstrate the state-of-the-art results of our method on field of view
extrapolation both qualitatively and quantitatively, providing thorough
analysis of our novel editing capabilities. Finally, we demonstrate that our
approach benefits the photorealistic virtual insertion of highly glossy objects
in photographs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Learning based Semi-Supervised Object Detection. (arXiv:2204.07300v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07300">
<div class="article-summary-box-inner">
<span><p>Semi-supervised object detection (SSOD) aims to facilitate the training and
deployment of object detectors with the help of a large amount of unlabeled
data. Though various self-training based and consistency-regularization based
SSOD methods have been proposed, most of them are anchor-based detectors,
ignoring the fact that in many real-world applications anchor-free detectors
are more demanded. In this paper, we intend to bridge this gap and propose a
DenSe Learning (DSL) based anchor-free SSOD algorithm. Specifically, we achieve
this goal by introducing several novel techniques, including an Adaptive
Filtering strategy for assigning multi-level and accurate dense pixel-wise
pseudo-labels, an Aggregated Teacher for producing stable and precise
pseudo-labels, and an uncertainty-consistency-regularization term among scales
and shuffled patches for improving the generalization capability of the
detector. Extensive experiments are conducted on MS-COCO and PASCAL-VOC, and
the results show that our proposed DSL method records new state-of-the-art SSOD
performance, surpassing existing methods by a large margin. Codes can be found
at \textcolor{blue}{https://github.com/chenbinghui1/DSL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Cross-Modal Understanding in Visual Dialog via Contrastive Learning. (arXiv:2204.07302v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07302">
<div class="article-summary-box-inner">
<span><p>Visual Dialog is a challenging vision-language task since the visual dialog
agent needs to answer a series of questions after reasoning over both the image
content and dialog history. Though existing methods try to deal with the
cross-modal understanding in visual dialog, they are still not enough in
ranking candidate answers based on their understanding of visual and textual
contexts. In this paper, we analyze the cross-modal understanding in visual
dialog based on the vision-language pre-training model VD-BERT and propose a
novel approach to improve the cross-modal understanding for visual dialog,
named ICMU. ICMU enhances cross-modal understanding by distinguishing different
pulled inputs (i.e. pulled images, questions or answers) based on four-way
contrastive learning. In addition, ICMU exploits the single-turn visual
question answering to enhance the visual dialog model's cross-modal
understanding to handle a multi-turn visually-grounded conversation.
Experiments show that the proposed approach improves the visual dialog model's
cross-modal understanding and brings satisfactory gain to the VisDial dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference. (arXiv:2204.07305v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07305">
<div class="article-summary-box-inner">
<span><p>Few-shot learning (FSL) is an important and topical problem in computer
vision that has motivated extensive research into numerous methods spanning
from sophisticated meta-learning methods to simple transfer learning baselines.
We seek to push the limits of a simple-but-effective pipeline for more
realistic and practical settings of few-shot image classification. To this end,
we explore few-shot learning from the perspective of neural network
architecture, as well as a three stage pipeline of network updates under
different data supplies, where unsupervised external data is considered for
pre-training, base categories are used to simulate few-shot tasks for
meta-training, and the scarcely labelled data of an novel task is taken for
fine-tuning. We investigate questions such as: (1) How pre-training on external
data benefits FSL? (2) How state-of-the-art transformer architectures can be
exploited? and (3) How fine-tuning mitigates domain shift? Ultimately, we show
that a simple transformer-based pipeline yields surprisingly good performance
on standard benchmarks such as Mini-ImageNet, CIFAR-FS, CDFSL and Meta-Dataset.
Our code and demo are available at https://hushell.github.io/pmf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaSets: Meta-Learning on Point Sets for Generalizable Representations. (arXiv:2204.07311v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07311">
<div class="article-summary-box-inner">
<span><p>Deep learning techniques for point clouds have achieved strong performance on
a range of 3D vision tasks. However, it is costly to annotate large-scale point
sets, making it critical to learn generalizable representations that can
transfer well across different point sets. In this paper, we study a new
problem of 3D Domain Generalization (3DDG) with the goal to generalize the
model to other unseen domains of point clouds without any access to them in the
training process. It is a challenging problem due to the substantial geometry
shift from simulated to real data, such that most existing 3D models
underperform due to overfitting the complete geometries in the source domain.
We propose to tackle this problem via MetaSets, which meta-learns point cloud
representations from a group of classification tasks on carefully-designed
transformed point sets containing specific geometry priors. The learned
representations are more generalizable to various unseen domains of different
geometries. We design two benchmarks for Sim-to-Real transfer of 3D point
clouds. Experimental results show that MetaSets outperforms existing 3D deep
learning methods by large margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Compression for Rate Constrained Object Detection on the Edge. (arXiv:2204.07314v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07314">
<div class="article-summary-box-inner">
<span><p>Recent advances in computer vision has led to a growth of interest in
deploying visual analytics model on mobile devices. However, most mobile
devices have limited computing power, which prohibits them from running large
scale visual analytics neural networks. An emerging approach to solve this
problem is to offload the computation of these neural networks to computing
resources at an edge server. Efficient computation offloading requires
optimizing the trade-off between multiple objectives including compressed data
rate, analytics performance, and computation speed. In this work, we consider a
"split computation" system to offload a part of the computation of the YOLO
object detection model. We propose a learnable feature compression approach to
compress the intermediate YOLO features with light-weight computation. We train
the feature compression and decompression module together with the YOLO model
to optimize the object detection accuracy under a rate constraint. Compared to
baseline methods that apply either standard image compression or learned image
compression at the mobile and perform image decompression and YOLO at the edge,
the proposed system achieves higher detection accuracy at the low to medium
rate range. Furthermore, the proposed system requires substantially lower
computation time on the mobile device with CPU only.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Keypoint-based Global Association Network for Lane Detection. (arXiv:2204.07335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07335">
<div class="article-summary-box-inner">
<span><p>Lane detection is a challenging task that requires predicting complex
topology shapes of lane lines and distinguishing different types of lanes
simultaneously. Earlier works follow a top-down roadmap to regress predefined
anchors into various shapes of lane lines, which lacks enough flexibility to
fit complex shapes of lanes due to the fixed anchor shapes. Lately, some works
propose to formulate lane detection as a keypoint estimation problem to
describe the shapes of lane lines more flexibly and gradually group adjacent
keypoints belonging to the same lane line in a point-by-point manner, which is
inefficient and time-consuming during postprocessing. In this paper, we propose
a Global Association Network (GANet) to formulate the lane detection problem
from a new perspective, where each keypoint is directly regressed to the
starting point of the lane line instead of point-by-point extension.
Concretely, the association of keypoints to their belonged lane line is
conducted by predicting their offsets to the corresponding starting points of
lanes globally without dependence on each other, which could be done in
parallel to greatly improve efficiency. In addition, we further propose a
Lane-aware Feature Aggregator (LFA), which adaptively captures the local
correlations between adjacent keypoints to supplement local information to the
global association. Extensive experiments on two popular lane detection
benchmarks show that our method outperforms previous methods with F1 score of
79.63% on CULane and 97.71% on Tusimple dataset with high FPS. The code will be
released at https://github.com/Wolfwjs/GANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAiD: Context-Aware Instance Discrimination for Self-supervised Learning in Medical Imaging. (arXiv:2204.07344v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07344">
<div class="article-summary-box-inner">
<span><p>Recently, self-supervised instance discrimination methods have achieved
significant success in learning visual representations from unlabeled
photographic images. However, given the marked differences between photographic
and medical images, the efficacy of instance-based objectives, focusing on
learning the most discriminative global features in the image (i.e., wheels in
bicycle), remains unknown in medical imaging. Our preliminary analysis showed
that high global similarity of medical images in terms of anatomy hampers
instance discrimination methods for capturing a set of distinct features,
negatively impacting their performance on medical downstream tasks. To
alleviate this limitation, we have developed a simple yet effective
self-supervised framework, called Context-Aware instance Discrimination (CAiD).
CAiD aims to improve instance discrimination learning by providing finer and
more discriminative information encoded from a diverse local context of
unlabeled medical images. We conduct a systematic analysis to investigate the
utility of the learned features from a three-pronged perspective: (i)
generalizability and transferability, (ii) separability in the embedding space,
and (iii) reusability. Our extensive experiments demonstrate that CAiD (1)
enriches representations learned from existing instance discrimination methods;
(2) delivers more discriminative features by adequately capturing finer
contextual information from individual medial images; and (3) improves
reusability of low/mid-level features compared to standard instance
discriminative methods. As open science, all codes and pre-trained models are
available on our GitHub page: https://github.com/JLiangLab/CAiD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVSTER: Epipolar Transformer for Efficient Multi-View Stereo. (arXiv:2204.07346v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07346">
<div class="article-summary-box-inner">
<span><p>Learning-based Multi-View Stereo (MVS) methods warp source images into the
reference camera frustum to form 3D volumes, which are fused as a cost volume
to be regularized by subsequent networks. The fusing step plays a vital role in
bridging 2D semantics and 3D spatial associations. However, previous methods
utilize extra networks to learn 2D information as fusing cues, underusing 3D
spatial correlations and bringing additional computation costs. Therefore, we
present MVSTER, which leverages the proposed epipolar Transformer to learn both
2D semantics and 3D spatial associations efficiently. Specifically, the
epipolar Transformer utilizes a detachable monocular depth estimator to enhance
2D semantics and uses cross-attention to construct data-dependent 3D
associations along epipolar line. Additionally, MVSTER is built in a cascade
structure, where entropy-regularized optimal transport is leveraged to
propagate finer depth estimations in each stage. Extensive experiments show
MVSTER achieves state-of-the-art reconstruction performance with significantly
higher efficiency: Compared with MVSNet and CasMVSNet, our MVSTER achieves 34%
and 14% relative improvements on the DTU benchmark, with 80% and 51% relative
reductions in running time. MVSTER also ranks first on Tanks&amp;Temples-Advanced
among all published works. Code is released at https://github.com/JeffWang987.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crowd counting with crowd attention convolutional neural network. (arXiv:2204.07347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07347">
<div class="article-summary-box-inner">
<span><p>Crowd counting is a challenging problem due to the scene complexity and scale
variation. Although deep learning has achieved great improvement in crowd
counting, scene complexity affects the judgement of these methods and they
usually regard some objects as people mistakenly; causing potentially enormous
errors in the crowd counting result. To address the problem, we propose a novel
end-to-end model called Crowd Attention Convolutional Neural Network (CAT-CNN).
Our CAT-CNN can adaptively assess the importance of a human head at each pixel
location by automatically encoding a confidence map. With the guidance of the
confidence map, the position of human head in estimated density map gets more
attention to encode the final density map, which can avoid enormous
misjudgements effectively. The crowd count can be obtained by integrating the
final density map. To encode a highly refined density map, the total crowd
count of each image is classified in a designed classification task and we
first explicitly map the prior of the population-level category to feature
maps. To verify the efficiency of our proposed method, extensive experiments
are conducted on three highly challenging datasets. Results establish the
superiority of our method over many state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Condition-Invariant and Compact Visual Place Description by Convolutional Autoencoder. (arXiv:2204.07350v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07350">
<div class="article-summary-box-inner">
<span><p>Visual place recognition (VPR) in condition-varying environments is still an
open problem. Popular solutions are CNN-based image descriptors, which have
been shown to outperform traditional image descriptors based on hand-crafted
visual features. However, there are two drawbacks of current CNN-based
descriptors: a) their high dimension and b) lack of generalization, leading to
low efficiency and poor performance in applications. In this paper, we propose
to use a convolutional autoencoder (CAE) to tackle this problem. We employ a
high-level layer of a pre-trained CNN to generate features, and train a CAE to
map the features to a low-dimensional space to improve the condition invariance
property of the descriptor and reduce its dimension at the same time. We verify
our method in three challenging datasets involving significant illumination
changes, and our method is shown to be superior to the state-of-the-art. For
the benefit of the community, we make public the source code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07356">
<div class="article-summary-box-inner">
<span><p>Pretrained models have produced great success in both Computer Vision (CV)
and Natural Language Processing (NLP). This progress leads to learning joint
representations of vision and language pretraining by feeding visual and
linguistic contents into a multi-layer transformer, Visual-Language Pretrained
Models (VLPMs). In this paper, we present an overview of the major advances
achieved in VLPMs for producing joint representations of vision and language.
As the preliminaries, we briefly describe the general task definition and
genetic architecture of VLPMs. We first discuss the language and vision data
encoding methods and then present the mainstream VLPM structure as the core
content. We further summarise several essential pretraining and fine-tuning
strategies. Finally, we highlight three future directions for both CV and NLP
researchers to provide insightful guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResT V2: Simpler, Faster and Stronger. (arXiv:2204.07366v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07366">
<div class="article-summary-box-inner">
<span><p>This paper proposes ResTv2, a simpler, faster, and stronger multi-scale
vision Transformer for visual recognition. ResTv2 simplifies the EMSA structure
in ResTv1 (i.e., eliminating the multi-head interaction part) and employs an
upsample operation to reconstruct the lost medium- and high-frequency
information caused by the downsampling operation. In addition, we explore
different techniques for better apply ResTv2 backbones to downstream tasks. We
found that although combining EMSAv2 and window attention can greatly reduce
the theoretical matrix multiply FLOPs, it may significantly decrease the
computation density, thus causing lower actual speed. We comprehensively
validate ResTv2 on ImageNet classification, COCO detection, and ADE20K semantic
segmentation. Experimental results show that the proposed ResTv2 can outperform
the recently state-of-the-art backbones by a large margin, demonstrating the
potential of ResTv2 as solid backbones. The code and models will be made
publicly available at \url{https://github.com/wofmanaf/ResT}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">2D Human Pose Estimation: A Survey. (arXiv:2204.07370v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07370">
<div class="article-summary-box-inner">
<span><p>Human pose estimation aims at localizing human anatomical keypoints or body
parts in the input data (e.g., images, videos, or signals). It forms a crucial
component in enabling machines to have an insightful understanding of the
behaviors of humans, and has become a salient problem in computer vision and
related fields. Deep learning techniques allow learning feature representations
directly from the data, significantly pushing the performance boundary of human
pose estimation. In this paper, we reap the recent achievements of 2D human
pose estimation methods and present a comprehensive survey. Briefly, existing
approaches put their efforts in three directions, namely network architecture
design, network training refinement, and post processing. Network architecture
design looks at the architecture of human pose estimation models, extracting
more robust features for keypoint recognition and localization. Network
training refinement tap into the training of neural networks and aims to
improve the representational ability of models. Post processing further
incorporates model-agnostic polishing strategies to improve the performance of
keypoint detection. More than 200 research contributions are involved in this
survey, covering methodological frameworks, common benchmark datasets,
evaluation metrics, and performance comparisons. We seek to provide researchers
with a more comprehensive and systematic review on human pose estimation,
allowing them to acquire a grand panorama and better identify future
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Adversarial Robustness-Accuracy Tradeoff in Robot Learning. (arXiv:2204.07373v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07373">
<div class="article-summary-box-inner">
<span><p>Adversarial training (i.e., training on adversarially perturbed input data)
is a well-studied method for making neural networks robust to potential
adversarial attacks during inference. However, the improved robustness does not
come for free but rather is accompanied by a decrease in overall model accuracy
and performance. Recent work has shown that, in practical robot learning
applications, the effects of adversarial training do not pose a fair trade-off
but inflict a net loss when measured in holistic robot performance. This work
revisits the robustness-accuracy trade-off in robot learning by systematically
analyzing if recent advances in robust training methods and theory in
conjunction with adversarial robot learning can make adversarial training
suitable for real-world robot applications. We evaluate a wide variety of robot
learning tasks ranging from autonomous driving in a high-fidelity environment
amenable to sim-to-real deployment, to mobile robot gesture recognition. Our
results demonstrate that, while these techniques make incremental improvements
on the trade-off on a relative scale, the negative side-effects caused by
adversarial training still outweigh the improvements by an order of magnitude.
We conclude that more substantial advances in robust learning methods are
necessary before they can benefit robot learning tasks in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Captioning In the Transformer Age. (arXiv:2204.07374v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07374">
<div class="article-summary-box-inner">
<span><p>Image Captioning (IC) has achieved astonishing developments by incorporating
various techniques into the CNN-RNN encoder-decoder architecture. However,
since CNN and RNN do not share the basic network component, such a
heterogeneous pipeline is hard to be trained end-to-end where the visual
encoder will not learn anything from the caption supervision. This drawback
inspires the researchers to develop a homogeneous architecture that facilitates
end-to-end training, for which Transformer is the perfect one that has proven
its huge potential in both vision and language domains and thus can be used as
the basic component of the visual encoder and language decoder in an IC
pipeline. Meantime, self-supervised learning releases the power of the
Transformer architecture that a pre-trained large-scale one can be generalized
to various tasks including IC. The success of these large-scale models seems to
weaken the importance of the single IC task. However, we demonstrate that IC
still has its specific significance in this age by analyzing the connections
between IC with some popular self-supervised learning paradigms. Due to the
page limitation, we only refer to highly important papers in this short survey
and more related works can be found at
https://github.com/SjokerLily/awesome-image-captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crowd counting with segmentation attention convolutional neural network. (arXiv:2204.07380v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07380">
<div class="article-summary-box-inner">
<span><p>Deep learning occupies an undisputed dominance in crowd counting. In this
paper, we propose a novel convolutional neural network (CNN) architecture
called SegCrowdNet. Despite the complex background in crowd scenes, the
proposeSegCrowdNet still adaptively highlights the human head region and
suppresses the non-head region by segmentation. With the guidance of an
attention mechanism, the proposed SegCrowdNet pays more attention to the human
head region and automatically encodes the highly refined density map. The crowd
count can be obtained by integrating the density map. To adapt the variation of
crowd counts, SegCrowdNet intelligently classifies the crowd count of each
image into several groups. In addition, the multi-scale features are learned
and extracted in the proposed SegCrowdNet to overcome the scale variations of
the crowd. To verify the effectiveness of our proposed method, extensive
experiments are conducted on four challenging datasets. The results demonstrate
that our proposed SegCrowdNet achieves excellent performance compared with the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FasterVideo: Efficient Online Joint Object Detection And Tracking. (arXiv:2204.07394v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07394">
<div class="article-summary-box-inner">
<span><p>Object detection and tracking in videos represent essential and
computationally demanding building blocks for current and future visual
perception systems. In order to reduce the efficiency gap between available
methods and computational requirements of real-world applications, we propose
to re-think one of the most successful methods for image object detection,
Faster R-CNN, and extend it to the video domain. Specifically, we extend the
detection framework to learn instance-level embeddings which prove beneficial
for data association and re-identification purposes. Focusing on the
computational aspects of detection and tracking, our proposed method reaches a
very high computational efficiency necessary for relevant applications, while
still managing to compete with recent and state-of-the-art methods as shown in
the experiments we conduct on standard object tracking benchmarks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSR-HEF: Crowd Counting with Multi-Scale Semantic Refining and Hard Example Focusing. (arXiv:2204.07406v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07406">
<div class="article-summary-box-inner">
<span><p>Crowd counting based on density maps is generally regarded as a regression
task.Deep learning is used to learn the mapping between image content and crowd
density distribution. Although great success has been achieved, some
pedestrians far away from the camera are difficult to be detected. And the
number of hard examples is often larger. Existing methods with simple Euclidean
distance algorithm indiscriminately optimize the hard and easy examples so that
the densities of hard examples are usually incorrectly predicted to be lower or
even zero, which results in large counting errors. To address this problem, we
are the first to propose the Hard Example Focusing(HEF) algorithm for the
regression task of crowd counting. The HEF algorithm makes our model rapidly
focus on hard examples by attenuating the contribution of easy examples.Then
higher importance will be given to the hard examples with wrong estimations.
Moreover, the scale variations in crowd scenes are large, and the scale
annotations are labor-intensive and expensive. By proposing a multi-Scale
Semantic Refining (SSR) strategy, lower layers of our model can break through
the limitation of deep learning to capture semantic features of different
scales to sufficiently deal with the scale variation. We perform extensive
experiments on six benchmark datasets to verify the proposed method. Results
indicate the superiority of our proposed method over the state-of-the-art
methods. Moreover, our designed model is smaller and faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Sensitivity-Based Filter Pruning. (arXiv:2204.07412v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07412">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel sensitivity-based filter pruning algorithm
(SbF-Pruner) to learn the importance scores of filters of each layer
end-to-end. Our method learns the scores from the filter weights, enabling it
to account for the correlations between the filters of each layer. Moreover, by
training the pruning scores of all layers simultaneously our method can account
for layer interdependencies, which is essential to find a performant sparse
sub-network. Our proposed method can train and generate a pruned network from
scratch in a straightforward, one-stage training process without requiring a
pretrained network. Ultimately, we do not need layer-specific hyperparameters
and pre-defined layer budgets, since SbF-Pruner can implicitly determine the
appropriate number of channels in each layer. Our experimental results on
different network architectures suggest that SbF-Pruner outperforms advanced
pruning methods. Notably, on CIFAR-10, without requiring a pretrained baseline
network, we obtain 1.02% and 1.19% accuracy gain on ResNet56 and ResNet110,
compared to the baseline reported for state-of-the-art pruning algorithms. This
is while SbF-Pruner reduces parameter-count by 52.3% (for ResNet56) and 54%
(for ResNet101), which is better than the state-of-the-art pruning algorithms
with a high margin of 9.5% and 6.6%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOTVerse: A User-defined Task Space of Single Object Tracking. (arXiv:2204.07414v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07414">
<div class="article-summary-box-inner">
<span><p>Single object tracking (SOT) research falls into a cycle - trackers perform
well on most benchmarks but quickly fail in challenging scenarios, causing
researchers to doubt the insufficient data content and take more effort
constructing larger datasets with more challenging situations. However,
isolated experimental environments and limited evaluation methods more
seriously hinder the SOT research. The former causes existing datasets can not
be exploited comprehensively, while the latter neglects challenging factors in
the evaluation process. In this article, we systematize the representative
benchmarks and form a single object tracking metaverse (SOTVerse) - a
user-defined SOT task space to break through the bottleneck. We first propose a
3E Paradigm to describe tasks by three components (i.e., environment,
evaluation, and executor). Then, we summarize task characteristics, clarify the
organization standards, and construct SOTVerse with 12.56 million frames.
Specifically, SOTVerse automatically labels challenging factors per frame,
allowing users to generate user-defined spaces efficiently via construction
rules. Besides, SOTVerse provides two mechanisms with new indicators and
successfully evaluates trackers under various subtasks. Consequently, SOTVerse
firstly provides a strategy to improve resource utilization in the computer
vision area, making research more standardized and scientific. The SOTVerse,
toolkit, evaluation server, and results are available at
<a href="http://metaverse.aitestunion.com.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep CardioSound: An Ensembled Deep Learning Model for Heart Sound MultiLabelling. (arXiv:2204.07420v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07420">
<div class="article-summary-box-inner">
<span><p>Heart sound diagnosis and classification play an essential role in detecting
cardiovascular disorders, especially when the remote diagnosis becomes standard
clinical practice. Most of the current work is designed for single category
based heard sound classification tasks. To further extend the landscape of the
automatic heart sound diagnosis landscape, this work proposes a deep multilabel
learning model that can automatically annotate heart sound recordings with
labels from different label groups, including murmur's timing, pitch, grading,
quality, and shape. Our experiment results show that the proposed method has
achieved outstanding performance on the holdout data for the multi-labelling
task with sensitivity=0.990, specificity=0.999, F1=0.990 at the segments level,
and an overall accuracy=0.969 at the patient's recording level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Learning for Instance Segmentation of Waste Bottles using Mask R-CNN Algorithm. (arXiv:2204.07437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07437">
<div class="article-summary-box-inner">
<span><p>This paper proposes a methodological approach with a transfer learning scheme
for plastic waste bottle detection and instance segmentation using the
\textit{mask region proposal convolutional neural network} (Mask R-CNN).
Plastic bottles constitute one of the major pollutants posing a serious threat
to the environment both in oceans and on land. The automated identification and
segregation of bottles can facilitate plastic waste recycling. We prepare a
custom-made dataset of 192 bottle images with pixel-by pixel-polygon annotation
for the automatic segmentation task. The proposed transfer learning scheme
makes use of a Mask R-CNN model pre-trained on the Microsoft COCO dataset. We
present a comprehensive scheme for fine-tuning the base pre-trained Mask-RCNN
model on our custom dataset. Our final fine-tuned model has achieved 59.4
\textit{mean average precision} (mAP), which corresponds to the MS COCO metric.
The results indicate a promising application of deep learning for detecting
waste bottles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold. (arXiv:2204.07439v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07439">
<div class="article-summary-box-inner">
<span><p>Binary Neural Networks (BNNs) have emerged as a promising solution for
reducing the memory footprint and compute costs of deep neural networks. BNNs,
on the other hand, suffer from information loss because binary activations are
limited to only two values, resulting in reduced accuracy. To improve the
accuracy, previous studies have attempted to control the distribution of binary
activation by manually shifting the threshold of the activation function or
making the shift amount trainable. During the process, they usually depended on
statistical information computed from a batch. We argue that using statistical
data from a batch fails to capture the crucial information for each input
instance in BNN computations, and the differences between statistical
information computed from each instance need to be considered when determining
the binary activation threshold of each instance. Based on the concept, we
propose the Binary Neural Network with INSTAnce-aware threshold (INSTA-BNN),
which decides the activation threshold value considering the difference between
statistical data computed from a batch and each instance. The proposed
INSTA-BNN outperforms the baseline by 2.5% and 2.3% on the ImageNet
classification task with comparable computing cost, achieving 68.0% and 71.7%
top-1 accuracy on ResNet-18 and MobileNetV1 based models, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval. (arXiv:2204.07441v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07441">
<div class="article-summary-box-inner">
<span><p>Large-scale single-stream pre-training has shown dramatic performance in
image-text retrieval. Regrettably, it faces low inference efficiency due to
heavy attention layers. Recently, two-stream methods like CLIP and ALIGN with
high inference efficiency have also shown promising performance, however, they
only consider instance-level alignment between the two streams (thus there is
still room for improvement). To overcome these limitations, we propose a novel
COllaborative Two-Stream vision-language pretraining model termed COTS for
image-text retrieval by enhancing cross-modal interaction. In addition to
instance level alignment via momentum contrastive learning, we leverage two
extra levels of cross-modal interactions in our COTS: (1) Token-level
interaction - a masked visionlanguage modeling (MVLM) learning objective is
devised without using a cross-stream network module, where variational
autoencoder is imposed on the visual encoder to generate visual tokens for each
image. (2) Task-level interaction - a KL-alignment learning objective is
devised between text-to-image and image-to-text retrieval tasks, where the
probability distribution per task is computed with the negative queues in
momentum contrastive learning. Under a fair comparison setting, our COTS
achieves the highest performance among all two-stream methods and comparable
performance (but with 10,800X faster in inference) w.r.t. the latest
single-stream methods. Importantly, our COTS is also applicable to
text-to-video retrieval, yielding new state-ofthe-art on the widely-used
MSR-VTT dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable and Real-time Multi-Camera Vehicle Detection, Re-Identification, and Tracking. (arXiv:2204.07442v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07442">
<div class="article-summary-box-inner">
<span><p>Multi-camera vehicle tracking is one of the most complicated tasks in
Computer Vision as it involves distinct tasks including Vehicle Detection,
Tracking, and Re-identification. Despite the challenges, multi-camera vehicle
tracking has immense potential in transportation applications including speed,
volume, origin-destination (O-D), and routing data generation. Several recent
works have addressed the multi-camera tracking problem. However, most of the
effort has gone towards improving accuracy on high-quality benchmark datasets
while disregarding lower camera resolutions, compression artifacts and the
overwhelming amount of computational power and time needed to carry out this
task on its edge and thus making it prohibitive for large-scale and real-time
deployment. Therefore, in this work we shed light on practical issues that
should be addressed for the design of a multi-camera tracking system to provide
actionable and timely insights. Moreover, we propose a real-time city-scale
multi-camera vehicle tracking system that compares favorably to computationally
intensive alternatives and handles real-world, low-resolution CCTV instead of
idealized and curated video streams. To show its effectiveness, in addition to
integration into the Regional Integrated Transportation Information System
(RITIS), we participated in the 2021 NVIDIA AI City multi-camera tracking
challenge and our method is ranked among the top five performers on the public
leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Violence in Video Based on Deep Features Fusion Technique. (arXiv:2204.07443v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07443">
<div class="article-summary-box-inner">
<span><p>With the rapid growth of surveillance cameras in many public places to
mon-itor human activities such as in malls, streets, schools and, prisons,
there is a strong demand for such systems to detect violence events
automatically. Au-tomatic analysis of video to detect violence is significant
for law enforce-ment. Moreover, it helps to avoid any social, economic and
environmental damages. Mostly, all systems today require manual human
supervisors to de-tect violence scenes in the video which is inefficient and
inaccurate. in this work, we interest in physical violence that involved two
persons or more. This work proposed a novel method to detect violence using a
fusion tech-nique of two significantly different convolutional neural networks
(CNNs) which are AlexNet and SqueezeNet networks. Each network followed by
separate Convolution Long Short Term memory (ConvLSTM) to extract ro-bust and
richer features from a video in the final hidden state. Then, making a fusion
of these two obtained states and fed to the max-pooling layer. Final-ly,
features were classified using a series of fully connected layers and soft-max
classifier. The performance of the proposed method is evaluated using three
standard benchmark datasets in terms of detection accuracy: Hockey Fight
dataset, Movie dataset and Violent Flow dataset. The results show an accuracy
of 97%, 100%, and 96% respectively. A comparison of the results with the state
of the art techniques revealed the promising capability of the proposed method
in recognizing violent videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Aware Pretraining for Dense Video Captioning. (arXiv:2204.07449v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07449">
<div class="article-summary-box-inner">
<span><p>This report describes the details of our approach for the event
dense-captioning task in ActivityNet Challenge 2021. We present a
semantic-aware pretraining method for dense video captioning, which empowers
the learned features to recognize high-level semantic concepts. Diverse video
features of different modalities are fed into an event captioning module to
generate accurate and meaningful sentences. Our final ensemble model achieves a
10.00 METEOR score on the test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ORCNet: A context-based network to simultaneously segment the ocular region components. (arXiv:2204.07456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07456">
<div class="article-summary-box-inner">
<span><p>Accurate extraction of the Region of Interest is critical for successful
ocular region-based biometrics. In this direction, we propose a new
context-based segmentation approach, entitled Ocular Region Context Network
(ORCNet), introducing a specific loss function, i.e., he Punish Context Loss
(PC-Loss). The PC-Loss punishes the segmentation losses of a network by using a
percentage difference value between the ground truth and the segmented masks.
We obtain the percentage difference by taking into account Biederman's semantic
relationship concepts, in which we use three contexts (semantic, spatial, and
scale) to evaluate the relationships of the objects in an image. Our proposal
achieved promising results in the evaluated scenarios: iris, sclera, and ALL
(iris + sclera) segmentations, utperforming the literature baseline techniques.
The ORCNet with ResNet-152 outperforms the best baseline (EncNet with
ResNet-152) on average by 2.27%, 28.26% and 6.43% in terms of F-Score, Error
Rate and Intersection Over Union, respectively. We also provide (for research
purposes) 3,191 manually labeled masks for the MICHE-I database, as another
contribution of our work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensitivity of sparse codes to image distortions. (arXiv:2204.07466v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07466">
<div class="article-summary-box-inner">
<span><p>Sparse coding has been proposed as a theory of visual cortex and as an
unsupervised algorithm for learning representations. We show empirically with
the MNIST dataset that sparse codes can be very sensitive to image distortions,
a behavior that may hinder invariant object recognition. A locally linear
analysis suggests that the sensitivity is due to the existence of linear
combinations of active dictionary elements with high cancellation. A nearest
neighbor classifier is shown to perform worse on sparse codes than original
images. For a linear classifier with a sufficiently large number of labeled
examples, sparse codes are shown to yield higher accuracy than original images,
but no higher than a representation computed by a random feedforward net.
Sensitivity to distortions seems to be a basic property of sparse codes, and
one should be aware of this property when applying sparse codes to invariant
object recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Attention using Partial-Order Relationships for Image Captioning. (arXiv:2204.07476v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07476">
<div class="article-summary-box-inner">
<span><p>The use of attention models for automated image captioning has enabled many
systems to produce accurate and meaningful descriptions for images. Over the
years, many novel approaches have been proposed to enhance the attention
process using different feature representations. In this paper, we extend this
approach by creating a guided attention network mechanism, that exploits the
relationship between the visual scene and text-descriptions using spatial
features from the image, high-level information from the topics, and temporal
context from caption generation, which are embedded together in an ordered
embedding space. A pairwise ranking objective is used for training this
embedding space which allows similar images, topics and captions in the shared
semantic space to maintain a partial order in the visual-semantic hierarchy and
hence, helps the model to produce more visually accurate captions. The
experimental results based on MSCOCO dataset shows the competitiveness of our
approach, with many state-of-the-art models on various evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards PAC Multi-Object Detection and Tracking. (arXiv:2204.07482v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07482">
<div class="article-summary-box-inner">
<span><p>Accurately detecting and tracking multi-objects is important for
safety-critical applications such as autonomous navigation. However, it remains
challenging to provide guarantees on the performance of state-of-the-art
techniques based on deep learning. We consider a strategy known as conformal
prediction, which predicts sets of labels instead of a single label; in the
classification and regression settings, these algorithms can guarantee that the
true label lies within the prediction set with high probability. Building on
these ideas, we propose multi-object detection and tracking algorithms that
come with probably approximately correct (PAC) guarantees. They do so by
constructing both a prediction set around each object detection as well as
around the set of edge transitions; given an object, the detection prediction
set contains its true bounding box with high probability, and the edge
prediction set contains its true transition across frames with high
probability. We empirically demonstrate that our method can detect and track
objects with PAC guarantees on the COCO and MOT-17 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-wise Contrastive Style Learning for Instagram Filter Removal. (arXiv:2204.07486v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07486">
<div class="article-summary-box-inner">
<span><p>Image-level corruptions and perturbations degrade the performance of CNNs on
different downstream vision tasks. Social media filters are one of the most
common resources of various corruptions and perturbations for real-world visual
analysis applications. The negative effects of these distractive factors can be
alleviated by recovering the original images with their pure style for the
inference of the downstream vision tasks. Assuming these filters substantially
inject a piece of additional style information to the social media images, we
can formulate the problem of recovering the original versions as a reverse
style transfer problem. We introduce Contrastive Instagram Filter Removal
Network (CIFR), which enhances this idea for Instagram filter removal by
employing a novel multi-layer patch-wise contrastive style learning mechanism.
Experiments show our proposed strategy produces better qualitative and
quantitative results than the previous studies. Moreover, we present the
results of our additional experiments for proposed architecture within
different settings. Finally, we present the inference outputs and quantitative
comparison of filtered and recovered images on localization and segmentation
tasks to encourage the main motivation for this problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesizing Informative Training Samples with GAN. (arXiv:2204.07513v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07513">
<div class="article-summary-box-inner">
<span><p>Remarkable progress has been achieved in synthesizing photo-realistic images
with generative adversarial neural networks (GANs). Recently, GANs are utilized
as the training sample generator when obtaining or storing real training data
is expensive even infeasible. However, traditional GANs generated images are
not as informative as the real training samples when being used to train deep
neural networks. In this paper, we propose a novel method to synthesize
Informative Training samples with GAN (IT-GAN). Specifically, we freeze a
pre-trained GAN model and learn the informative latent vectors that corresponds
to informative training samples. The synthesized images are required to
preserve information for training deep neural networks rather than visual
reality or fidelity. Experiments verify that the deep neural networks can learn
faster and achieve better performance when being trained with our IT-GAN
generated images. We also show that our method is a promising solution to
dataset condensation problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unconditional Image-Text Pair Generation with Multimodal Cross Quantizer. (arXiv:2204.07537v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07537">
<div class="article-summary-box-inner">
<span><p>Though deep generative models have gained a lot of attention, most of the
existing works are designed for the unimodal generation task. In this paper, we
explore a new method for unconditional image-text pair generation. We propose
MXQ-VAE, a vector quantization method for multimodal image-text representation.
MXQ-VAE accepts a paired image and text as input, and learns a joint quantized
representation space, so that the image-text pair can be converted to a
sequence of unified indices. Then we can use autoregressive generative models
to model the joint image-text representation, and even perform unconditional
image-text pair generation. Extensive experimental results demonstrate that our
approach effectively generates semantically consistent image-text pair and also
enhances meaningful alignment between image and text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised atmospheric component learning in low-light image problem. (arXiv:2204.07546v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07546">
<div class="article-summary-box-inner">
<span><p>Ambient lighting conditions play a crucial role in determining the perceptual
quality of images from photographic devices. In general, inadequate
transmission light and undesired atmospheric conditions jointly degrade the
image quality. If we know the desired ambient factors associated with the given
low-light image, we can recover the enhanced image easily \cite{b1}. Typical
deep networks perform enhancement mappings without investigating the light
distribution and color formulation properties. This leads to a lack of image
instance-adaptive performance in practice. On the other hand, physical
model-driven schemes suffer from the need for inherent decompositions and
multiple objective minimizations. Moreover, the above approaches are rarely
data efficient or free of postprediction tuning. Influenced by the above
issues, this study presents a semisupervised training method using no-reference
image quality metrics for low-light image restoration. We incorporate the
classical haze distribution model \cite{b2} to explore the physical properties
of the given image in order to learn the effect of atmospheric components and
minimize a single objective for restoration. We validate the performance of our
network for six widely used low-light datasets. The experiments show that the
proposed study achieves state-of-the-art or comparable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation. (arXiv:2204.07548v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07548">
<div class="article-summary-box-inner">
<span><p>Recent works on 3D semantic segmentation propose to exploit the synergy
between images and point clouds by processing each modality with a dedicated
network and projecting learned 2D features onto 3D points. Merging large-scale
point clouds and images raises several challenges, such as constructing a
mapping between points and pixels, and aggregating features between multiple
views. Current methods require mesh reconstruction or specialized sensors to
recover occlusions, and use heuristics to select and aggregate available
images. In contrast, we propose an end-to-end trainable multi-view aggregation
model leveraging the viewing conditions of 3D points to merge features from
images taken at arbitrary positions. Our method can combine standard 2D and 3D
networks and outperforms both 3D models operating on colorized point clouds and
hybrid 2D/3D networks without requiring colorization, meshing, or true depth
maps. We set a new state-of-the-art for large-scale indoor/outdoor semantic
segmentation on S3DIS (74.7 mIoU 6-Fold) and on KITTI-360 (58.3 mIoU). Our full
pipeline is accessible at https://github.com/drprojects/DeepViewAgg, and only
requires raw 3D scans and a set of images and poses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PL-VINS: Real-Time Monocular Visual-Inertial SLAM with Point and Line Features. (arXiv:2009.07462v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.07462">
<div class="article-summary-box-inner">
<span><p>Leveraging line features to improve localization accuracy of point-based
visual-inertial SLAM (VINS) is gaining interest as they provide additional
constraints on scene structure. However, real-time performance when
incorporating line features in VINS has not been addressed. This paper presents
PL-VINS, a real-time optimization-based monocular VINS method with point and
line features, developed based on the state-of-the-art point-based VINS-Mono
\cite{vins}. We observe that current works use the LSD \cite{lsd} algorithm to
extract line features; however, LSD is designed for scene shape representation
instead of the pose estimation problem, which becomes the bottleneck for the
real-time performance due to its high computational cost. In this paper, a
modified LSD algorithm is presented by studying a hidden parameter tuning and
length rejection strategy. The modified LSD can run at least three times as
fast as LSD. Further, by representing space lines with the Pl\"{u}cker
coordinates, the residual error in line estimation is modeled in terms of the
point-to-line distance, which is then minimized by iteratively updating the
minimum four-parameter orthonormal representation of the Pl\"{u}cker
coordinates. Experiments in a public benchmark dataset show that the
localization error of our method is 12-16\% less than that of VINS-Mono at the
same pose update frequency. %For the benefit of the community, The source code
of our method is available at: https://github.com/cnqiangfu/PL-VINS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can You Spot the Chameleon? Adversarially Camouflaging Images from Co-Salient Object Detection. (arXiv:2009.09258v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09258">
<div class="article-summary-box-inner">
<span><p>Co-salient object detection (CoSOD) has recently achieved significant
progress and played a key role in retrieval-related tasks. However, it
inevitably poses an entirely new safety and security issue, i.e., highly
personal and sensitive content can potentially be extracting by powerful CoSOD
methods. In this paper, we address this problem from the perspective of
adversarial attacks and identify a novel task: adversarial co-saliency attack.
Specially, given an image selected from a group of images containing some
common and salient objects, we aim to generate an adversarial version that can
mislead CoSOD methods to predict incorrect co-salient regions. Note that,
compared with general white-box adversarial attacks for classification, this
new task faces two additional challenges: (1) low success rate due to the
diverse appearance of images in the group; (2) low transferability across CoSOD
methods due to the considerable difference between CoSOD pipelines. To address
these challenges, we propose the very first black-box joint adversarial
exposure and noise attack (Jadena), where we jointly and locally tune the
exposure and additive perturbations of the image according to a newly designed
high-feature-level contrast-sensitive loss function. Our method, without any
information on the state-of-the-art CoSOD methods, leads to significant
performance degradation on various co-saliency detection datasets and makes the
co-salient objects undetectable. This can have strong practical benefits in
properly securing the large number of personal photos currently shared on the
Internet. Moreover, our method is potential to be utilized as a metric for
evaluating the robustness of CoSOD methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TubeR: Tubelet Transformer for Video Action Detection. (arXiv:2104.00969v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00969">
<div class="article-summary-box-inner">
<span><p>We propose TubeR: a simple solution for spatio-temporal video action
detection. Different from existing methods that depend on either an off-line
actor detector or hand-designed actor-positional hypotheses like proposals or
anchors, we propose to directly detect an action tubelet in a video by
simultaneously performing action localization and recognition from a single
representation. TubeR learns a set of tubelet-queries and utilizes a
tubelet-attention module to model the dynamic spatio-temporal nature of a video
clip, which effectively reinforces the model capacity compared to using
actor-positional hypotheses in the spatio-temporal space. For videos containing
transitional states or scene changes, we propose a context aware classification
head to utilize short-term and long-term context to strengthen action
classification, and an action switch regression head for detecting the precise
temporal action extent. TubeR directly produces action tubelets with variable
lengths and even maintains good results for long video clips. TubeR outperforms
the previous state-of-the-art on commonly used action detection datasets AVA,
UCF101-24 and JHMDB51-21.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DHNet: Double MPEG-4 Compression Detection via Multiple DCT Histograms. (arXiv:2107.08939v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08939">
<div class="article-summary-box-inner">
<span><p>In this article, we aim to detect the double compression of MPEG-4, a
universal video codec that is built into surveillance systems and shooting
devices. Double compression is accompanied by various types of video
manipulation, and its traces can be exploited to determine whether a video is a
forgery. To this end, we present a neural network-based approach with
discriminant features for capturing peculiar artifacts in the discrete cosine
transform (DCT) domain caused by double MPEG-4 compression. By analyzing the
intra-coding process of MPEG-4, which performs block-DCT-based quantization, we
exploit multiple DCT histograms as features to focus on the statistical
properties of DCT coefficients on multiresolution blocks. Furthermore, we
improve detection performance using a vectorized feature of the quantization
table on dense layers as auxiliary information. Compared with neural
network-based approaches suitable for exploring subtle manipulations, the
experimental results reveal that this work achieves high performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigation of condominium building collapse in Surfside, Florida: A video feature tracking approach. (arXiv:2109.06629v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06629">
<div class="article-summary-box-inner">
<span><p>On June 24, 2021, a 12-story condominium building (Champlain Towers South) in
Surfside, Florida partially collapsed, resulting in one of the deadliest
building collapses in United States history with 98 people confirmed deceased.
In this work, we analyze the collapse event using a video clip that is publicly
available from social media. In our analysis, we apply computer vision
algorithms to corroborate new information from the video clip that may not be
readily interpreted by human eyes. By comparing the differential features
against different video frames, our proposed method is used to quantify the
falling structural components by mapping the directions and magnitudes of their
movements. We demonstrate the potential of this video processing methodology in
investigations of catastrophic structural failures and hope our approach may
serve as a basis for further investigations into structure collapse events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Coherence for Text-to-Image Retrieval. (arXiv:2109.11047v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11047">
<div class="article-summary-box-inner">
<span><p>Common image-text joint understanding techniques presume that images and the
associated text can universally be characterized by a single implicit model.
However, co-occurring images and text can be related in qualitatively different
ways, and explicitly modeling it could improve the performance of current joint
understanding models. In this paper, we train a Cross-Modal Coherence Modelfor
text-to-image retrieval task. Our analysis shows that models trained with
image--text coherence relations can retrieve images originally paired with
target text more often than coherence-agnostic models. We also show via human
evaluation that images retrieved by the proposed coherence-aware model are
preferred over a coherence-agnostic baseline by a huge margin. Our findings
provide insights into the ways that different modalities communicate and the
role of coherence relations in capturing commonsense inferences in text and
imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Instance Segmentation with Automotive Radar Detection Points. (arXiv:2110.01775v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01775">
<div class="article-summary-box-inner">
<span><p>Automotive radar provides reliable environmental perception in all-weather
conditions with affordable cost, but it hardly supplies semantic and geometry
information due to the sparsity of radar detection points. With the development
of automotive radar technologies in recent years, instance segmentation becomes
possible by using automotive radar. Its data contain contexts such as radar
cross section and micro-Doppler effects, and sometimes can provide detection
when the field of view is obscured. The outcome from instance segmentation
could be potentially used as the input of trackers for tracking targets. The
existing methods often utilize a clustering-based classification framework,
which fits the need of real-time processing but has limited performance due to
minimum information provided by sparse radar detection points. In this paper,
we propose an efficient method based on clustering of estimated semantic
information to achieve instance segmentation for the sparse radar detection
points. In addition, we show that the performance of the proposed approach can
be further enhanced by incorporating the visual multi-layer perceptron. The
effectiveness of the proposed method is verified by experimental results on the
popular RadarScenes dataset, achieving 89.53% mean coverage and 86.97% mean
average precision with the IoU threshold of 0.5, which is superior to other
approaches in the literature. More significantly, the consumed memory is around
1MB, and the inference time is less than 40ms, indicating that our proposed
algorithm is storage and time efficient. These two criteria ensure the
practicality of the proposed method in real-world systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Importance of Firth Bias Reduction in Few-Shot Classification. (arXiv:2110.02529v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02529">
<div class="article-summary-box-inner">
<span><p>Learning accurate classifiers for novel categories from very few examples,
known as few-shot image classification, is a challenging task in statistical
machine learning and computer vision. The performance in few-shot
classification suffers from the bias in the estimation of classifier
parameters; however, an effective underlying bias reduction technique that
could alleviate this issue in training few-shot classifiers has been
overlooked. In this work, we demonstrate the effectiveness of Firth bias
reduction in few-shot classification. Theoretically, Firth bias reduction
removes the $O(N^{-1})$ first order term from the small-sample bias of the
Maximum Likelihood Estimator. Here we show that the general Firth bias
reduction technique simplifies to encouraging uniform class assignment
probabilities for multinomial logistic classification, and almost has the same
effect in cosine classifiers. We derive an easy-to-implement optimization
objective for Firth penalized multinomial logistic and cosine classifiers,
which is equivalent to penalizing the cross-entropy loss with a KL-divergence
between the uniform label distribution and the predictions. Then, we
empirically evaluate that it is consistently effective across the board for
few-shot image classification, regardless of (1) the feature representations
from different backbones, (2) the number of samples per class, and (3) the
number of classes. Finally, we show the robustness of Firth bias reduction, in
the case of imbalanced data distribution. Our implementation is available at
https://github.com/ehsansaleh/firth_bias_reduction
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple but Effective: CLIP Embeddings for Embodied AI. (arXiv:2111.09888v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09888">
<div class="article-summary-box-inner">
<span><p>Contrastive language image pretraining (CLIP) encoders have been shown to be
beneficial for a range of visual tasks from classification and detection to
captioning and image manipulation. We investigate the effectiveness of CLIP
visual backbones for Embodied AI tasks. We build incredibly simple baselines,
named EmbCLIP, with no task specific architectures, inductive biases (such as
the use of semantic maps), auxiliary tasks during training, or depth maps --
yet we find that our improved baselines perform very well across a range of
tasks and simulators. EmbCLIP tops the RoboTHOR ObjectNav leaderboard by a huge
margin of 20 pts (Success Rate). It tops the iTHOR 1-Phase Rearrangement
leaderboard, beating the next best submission, which employs Active Neural
Mapping, and more than doubling the % Fixed Strict metric (0.08 to 0.17). It
also beats the winners of the 2021 Habitat ObjectNav Challenge, which employ
auxiliary tasks, depth maps, and human demonstrations, and those of the 2019
Habitat PointNav Challenge. We evaluate the ability of CLIP's visual
representations at capturing semantic information about input observations --
primitives that are useful for navigation-heavy embodied tasks -- and find that
CLIP's representations encode these primitives more effectively than
ImageNet-pretrained backbones. Finally, we extend one of our baselines,
producing an agent capable of zero-shot object navigation that can navigate to
objects that were not used as targets during training. Our code and models are
available at https://github.com/allenai/embodied-clip
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nanorobot queue: Cooperative treatment of cancer based on team member communication and image processing. (arXiv:2111.11236v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11236">
<div class="article-summary-box-inner">
<span><p>Although nanorobots have been used as clinical prescriptions for work such as
gastroscopy, and even photoacoustic tomography technology has been proposed to
control nanorobots to deliver drugs at designated delivery points in real time,
and there are cases of eliminating "superbacteria" in blood through nanorobots,
most technologies are immature, either with low efficiency or low accuracy,
Either it can not be mass produced, so the most effective way to treat cancer
diseases at this stage is through chemotherapy and radiotherapy. Patients are
suffering and can not be cured. Therefore, this paper proposes an ideal model
of a treatment method that can completely cure cancer, a cooperative treatment
method based on nano robot queue through team member communication and computer
vision image classification (target detection).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Compositional Zero-shot Learning with DeCompositional Consensus. (arXiv:2111.14673v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14673">
<div class="article-summary-box-inner">
<span><p>Parts represent a basic unit of geometric and semantic similarity across
different objects. We argue that part knowledge should be composable beyond the
observed object classes. Towards this, we present 3D Compositional Zero-shot
Learning as a problem of part generalization from seen to unseen object classes
for semantic segmentation. We provide a structured study through benchmarking
the task with the proposed Compositional-PartNet dataset. This dataset is
created by processing the original PartNet to maximize part overlap across
different objects. The existing point cloud part segmentation methods fail to
generalize to unseen object classes in this setting. As a solution, we propose
DeCompositional Consensus, which combines a part segmentation network with a
part scoring network. The key intuition to our approach is that a segmentation
mask over some parts should have a consensus with its part scores when each
part is taken apart. The two networks reason over different part combinations
defined in a per-object part prior to generate the most suitable segmentation
mask. We demonstrate that our method allows compositional zero-shot
segmentation and generalized zero-shot classification, and establishes the
state of the art on both tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-domain Integrative Swin Transformer network for Sparse-View Tomographic Reconstruction. (arXiv:2111.14831v7 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14831">
<div class="article-summary-box-inner">
<span><p>Decreasing projection views to lower X-ray radiation dose usually leads to
severe streak artifacts. To improve image quality from sparse-view data, a
Multi-domain Integrative Swin Transformer network (MIST-net) was developed in
this article. First, MIST-net incorporated lavish domain features from data,
residual-data, image, and residual-image using flexible network architectures,
where residual-data and residual-image sub-network was considered as data
consistency module to eliminate interpolation and reconstruction errors.
Second, a trainable edge enhancement filter was incorporated to detect and
protect image edges. Third, a high-quality reconstruction Swin transformer
(i.e., Recformer) was designed to capture image global features. The experiment
results on numerical and real cardiac clinical datasets with 48-views
demonstrated that our proposed MIST-net provided better image quality with more
small features and sharp edges than other competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIVE: Evaluating the Human Interpretability of Visual Explanations. (arXiv:2112.03184v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03184">
<div class="article-summary-box-inner">
<span><p>As machine learning is increasingly applied to high-impact, high-risk
domains, there have been a number of new methods aimed at making AI models more
human interpretable. Despite the recent growth of interpretability work, there
is a lack of systematic evaluation of proposed techniques. In this work, we
propose HIVE (Human Interpretability of Visual Explanations), a novel human
evaluation framework for visual interpretability methods that allows for
falsifiable hypothesis testing, cross-method comparison, and human-centered
evaluation. To the best of our knowledge, this is the first work of its kind.
Using HIVE, we conduct IRB-approved human studies with nearly 1000 participants
and evaluate four methods that represent the diversity of computer vision
interpretability works: GradCAM, BagNet, ProtoPNet, and ProtoTree. Our results
suggest that explanations engender human trust, even for incorrect predictions,
yet are not distinct enough for users to distinguish between correct and
incorrect predictions. We open-source HIVE to enable future studies and to
encourage more human-centered approaches to interpretability research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Control for Free! Image Synthesis with Semantic Diffusion Guidance. (arXiv:2112.05744v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05744">
<div class="article-summary-box-inner">
<span><p>Controllable image synthesis models allow creation of diverse images based on
text instructions or guidance from a reference image. Recently, denoising
diffusion probabilistic models have been shown to generate more realistic
imagery than prior methods, and have been successfully demonstrated in
unconditional and class-conditional settings. We investigate fine-grained,
continuous control of this model class, and introduce a novel unified framework
for semantic diffusion guidance, which allows either language or image
guidance, or both. Guidance is injected into a pretrained unconditional
diffusion model using the gradient of image-text or image matching scores. We
explore CLIP-based language guidance as well as both content and style-based
image guidance in a unified framework. Our text-guided synthesis approach can
be applied to datasets without associated text annotations. We conduct
experiments on FFHQ and LSUN datasets, and show results on fine-grained
text-guided image synthesis, synthesis of images related to a style or content
reference image, and examples with both textual and image guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video as Conditional Graph Hierarchy for Multi-Granular Question Answering. (arXiv:2112.06197v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06197">
<div class="article-summary-box-inner">
<span><p>Video question answering requires the models to understand and reason about
both the complex video and language data to correctly derive the answers.
Existing efforts have been focused on designing sophisticated cross-modal
interactions to fuse the information from two modalities, while encoding the
video and question holistically as frame and word sequences. Despite their
success, these methods are essentially revolving around the sequential nature
of video- and question-contents, providing little insight to the problem of
question-answering and lacking interpretability as well. In this work, we argue
that while video is presented in frame sequence, the visual elements (e.g.,
objects, actions, activities and events) are not sequential but rather
hierarchical in semantic space. To align with the multi-granular essence of
linguistic concepts in language queries, we propose to model video as a
conditional graph hierarchy which weaves together visual facts of different
granularity in a level-wise manner, with the guidance of corresponding textual
cues. Despite the simplicity, our extensive experiments demonstrate the
superiority of such conditional hierarchical graph architecture, with clear
performance improvements over prior methods and also better generalization
across different type of questions. Further analyses also demonstrate the
model's reliability as it shows meaningful visual-textual evidences for the
predicted answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupling Zero-Shot Semantic Segmentation. (arXiv:2112.07910v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07910">
<div class="article-summary-box-inner">
<span><p>Zero-shot semantic segmentation (ZS3) aims to segment the novel categories
that have not been seen in the training. Existing works formulate ZS3 as a
pixel-level zeroshot classification problem, and transfer semantic knowledge
from seen classes to unseen ones with the help of language models pre-trained
only with texts. While simple, the pixel-level ZS3 formulation shows the
limited capability to integrate vision-language models that are often
pre-trained with image-text pairs and currently demonstrate great potential for
vision tasks. Inspired by the observation that humans often perform
segment-level semantic labeling, we propose to decouple the ZS3 into two
sub-tasks: 1) a classagnostic grouping task to group the pixels into segments.
2) a zero-shot classification task on segments. The former task does not
involve category information and can be directly transferred to group pixels
for unseen classes. The latter task performs at segment-level and provides a
natural way to leverage large-scale vision-language models pre-trained with
image-text pairs (e.g. CLIP) for ZS3. Based on the decoupling formulation, we
propose a simple and effective zero-shot semantic segmentation model, called
ZegFormer, which outperforms the previous methods on ZS3 standard benchmarks by
large margins, e.g., 22 points on the PASCAL VOC and 3 points on the COCO-Stuff
in terms of mIoU for unseen classes. Code will be released at
https://github.com/dingjiansw101/ZegFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08879">
<div class="article-summary-box-inner">
<span><p>Most language grounding models learn to select the referred object from a
pool of object proposals provided by a pre-trained detector. This object
proposal bottleneck is limiting because an utterance may refer to visual
entities at various levels of granularity, such as the chair, the leg of a
chair, or the tip of the front leg of a chair, which may be missed by the
detector. Recently, MDETR introduced a language grounding model for 2D images
that do not have such a box proposal bottleneck; instead of selecting objects
from a proposal pool, it instead decodes the referenced object boxes directly
from image and language features and achieves big leaps in performance. We
propose a language grounding model for 3D scenes built on MDETR, which we call
BEAUTY-DETR, from bottom-up and top-down DETR. BEAUTY-DETR attends on an
additional object proposal pool computed bottom-up from a pre-trained detector.
Yet it decodes referenced objects without selecting them from the pool. In this
way, it uses powerful object detectors to help ground language without being
restricted by their misses. Second, BEAUTY-DETR augments supervision from
language grounding annotations by configuring object detection annotations as
language prompts to be grounded in images. The proposed model sets a new
state-of-the-art across popular 3D language grounding benchmarks with
significant performance gains over previous 3D approaches (12.6% on SR3D, 11.6%
on NR3D and 6.3% on ScanRefer). It outperforms a straightforward MDETR for the
3D point clouds method we implemented by 6.7% on SR3D, 11.8% on NR3D and 5% on
the ScanRefer benchmark. When applied to language grounding in 2D images, it
performs on par with MDETR. We ablate each of the design choices of the model
and quantify their contribution to performance. Code and checkpoints are
available at https://github.com/nickgkan/beauty_detr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Network-Aware 5G Edge Computing for Object Detection: Augmenting Wearables to "See" More, Farther and Faster. (arXiv:2112.13194v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13194">
<div class="article-summary-box-inner">
<span><p>Advanced wearable devices are increasingly incorporating high-resolution
multi-camera systems. As state-of-the-art neural networks for processing the
resulting image data are computationally demanding, there has been growing
interest in leveraging fifth generation (5G) wireless connectivity and mobile
edge computing for offloading this processing to the cloud. To assess this
possibility, this paper presents a detailed simulation and evaluation of 5G
wireless offloading for object detection within a powerful, new smart wearable
called VIS4ION, for the Blind-and-Visually Impaired (BVI). The current VIS4ION
system is an instrumented book-bag with high-resolution cameras, vision
processing and haptic and audio feedback. The paper considers uploading the
camera data to a mobile edge cloud to perform real-time object detection and
transmitting the detection results back to the wearable. To determine the video
requirements, the paper evaluates the impact of video bit rate and resolution
on object detection accuracy and range. A new street scene dataset with labeled
objects relevant to BVI navigation is leveraged for analysis. The vision
evaluation is combined with a detailed full-stack wireless network simulation
to determine the distribution of throughputs and delays with real navigation
paths and ray-tracing from new high-resolution 3D models in an urban
environment. For comparison, the wireless simulation considers both a standard
4G-Long Term Evolution (LTE) carrier and high-rate 5G millimeter-wave (mmWave)
carrier. The work thus provides a thorough and realistic assessment of edge
computing with mmWave connectivity in an application with both high bandwidth
and low latency requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation. (arXiv:2203.06558v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06558">
<div class="article-summary-box-inner">
<span><p>Training a generalizable 3D part segmentation network is quite challenging
but of great importance in real-world applications. To tackle this problem,
some works design task-specific solutions by translating human understanding of
the task to machine's learning process, which faces the risk of missing the
optimal strategy since machines do not necessarily understand in the exact
human way. Others try to use conventional task-agnostic approaches designed for
domain generalization problems with no task prior knowledge considered. To
solve the above issues, we propose AutoGPart, a generic method enabling
training generalizable 3D part segmentation networks with the task prior
considered. AutoGPart builds a supervision space with geometric prior knowledge
encoded, and lets the machine to search for the optimal supervisions from the
space for a specific segmentation task automatically. Extensive experiments on
three generalizable 3D part segmentation tasks are conducted to demonstrate the
effectiveness and versatility of AutoGPart. We demonstrate that the performance
of segmentation networks using simple backbones can be significantly improved
when trained with supervisions searched by our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-Gait: Automatic Ataxia Risk Assessment with Computer Vision on Gait Task Videos. (arXiv:2203.08215v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08215">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigated whether we can 1) detect participants with
ataxia-specific gait characteristics (risk-prediction), and 2) assess severity
of ataxia from gait (severity-assessment) using computer vision. We created a
dataset of 155 videos from 89 participants, 24 controls and 65 diagnosed with
(or are pre-manifest) spinocerebellar ataxias (SCAs), performing the gait task
of the Scale for the Assessment and Rating of Ataxia (SARA) from 11 medical
sites located in 8 different states across the United States. We develop a
computer vision pipeline to detect, track, and separate out the participants
from their surroundings and construct several features from their body pose
coordinates to capture gait characteristics like step width, step length,
swing, stability, speed, etc. Our risk-prediction model achieves 83.06%
accuracy and an 80.23% F1 score. Similarly, our severity-assessment model
achieves a mean absolute error (MAE) score of 0.6225 and a Pearson's
correlation coefficient score of 0.7268. Our models still performed
competitively when evaluated on data from sites not used during training.
Furthermore, through feature importance analysis, we found that our models
associate wider steps, decreased walking speed, and increased instability with
greater ataxia severity, which is consistent with previously established
clinical knowledge. Our models create possibilities for remote ataxia
assessment in non-clinical settings in the future, which could significantly
improve accessibility of ataxia care. Furthermore, our underlying dataset was
assembled from a geographically diverse cohort, highlighting its potential to
further increase equity. The code used in this study is open to the public, and
the anonymized body pose landmark dataset is also available upon request.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-scale Bilingual Language-Image Contrastive Learning. (arXiv:2203.14463v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14463">
<div class="article-summary-box-inner">
<span><p>This paper is a technical report to share our experience and findings
building a Korean and English bilingual multimodal model. While many of the
multimodal datasets focus on English and multilingual multimodal research uses
machine-translated texts, employing such machine-translated texts is limited to
describing unique expressions, cultural information, and proper noun in
languages other than English. In this work, we collect 1.1 billion image-text
pairs (708 million Korean and 476 million English) and train a bilingual
multimodal model named KELIP. We introduce simple yet effective training
schemes, including MAE pre-training and multi-crop augmentation. Extensive
experiments demonstrate that a model trained with such training schemes shows
competitive performance in both languages. Moreover, we discuss
multimodal-related research questions: 1) strong augmentation-based methods can
distract the model from learning proper multimodal relations; 2) training
multimodal model without cross-lingual relation can learn the relation via
visual semantics; 3) our bilingual KELIP can capture cultural differences of
visual semantics for the same meaning of words; 4) a large-scale multimodal
model can be used for multimodal feature analogy. We hope that this work will
provide helpful experience and findings for future research. We provide an
open-source pre-trained KELIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exemplar-based Pattern Synthesis with Implicit Periodic Field Network. (arXiv:2204.01671v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01671">
<div class="article-summary-box-inner">
<span><p>Synthesis of ergodic, stationary visual patterns is widely applicable in
texturing, shape modeling, and digital content creation. The wide applicability
of this technique thus requires the pattern synthesis approaches to be
scalable, diverse, and authentic. In this paper, we propose an exemplar-based
visual pattern synthesis framework that aims to model the inner statistics of
visual patterns and generate new, versatile patterns that meet the
aforementioned requirements. To this end, we propose an implicit network based
on generative adversarial network (GAN) and periodic encoding, thus calling our
network the Implicit Periodic Field Network (IPFN). The design of IPFN ensures
scalability: the implicit formulation directly maps the input coordinates to
features, which enables synthesis of arbitrary size and is computationally
efficient for 3D shape synthesis. Learning with a periodic encoding scheme
encourages diversity: the network is constrained to model the inner statistics
of the exemplar based on spatial latent codes in a periodic field. Coupled with
continuously designed GAN training procedures, IPFN is shown to synthesize
tileable patterns with smooth transitions and local variations. Last but not
least, thanks to both the adversarial training technique and the encoded
Fourier features, IPFN learns high-frequency functions that produce authentic,
high-quality results. To validate our approach, we present novel experimental
results on various applications in 2D texture synthesis and 3D shape synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Optical Flow-Based Line Feature Tracking. (arXiv:2204.03331v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03331">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a novel sparse optical flow (SOF)-based line feature
tracking method for the camera pose estimation problem. This method is inspired
by the point-based SOF algorithm and developed based on an observation that two
adjacent images in time-varying image sequences satisfy brightness invariant.
Based on this observation, we re-define the goal of line feature tracking:
track two endpoints of a line feature instead of the entire line based on gray
value matching instead of descriptor matching. To achieve this goal, an
efficient two endpoint tracking (TET) method is presented: first, describe a
given line feature with its two endpoints; next, track the two endpoints based
on SOF to obtain two new tracked endpoints by minimizing a pixel-level
grayscale residual function; finally, connect the two tracked endpoints to
generate a new line feature. The correspondence is established between the
given and the new line feature. Compared with current descriptor-based methods,
our TET method needs not to compute descriptors and detect line features
repeatedly. Naturally, it has an obvious advantage over computation.
Experiments in several public benchmark datasets show our method yields highly
competitive accuracy with an obvious advantage over speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmenting across places: The need for fair transfer learning with satellite imagery. (arXiv:2204.04358v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04358">
<div class="article-summary-box-inner">
<span><p>The increasing availability of high-resolution satellite imagery has enabled
the use of machine learning to support land-cover measurement and inform
policy-making. However, labelling satellite images is expensive and is
available for only some locations. This prompts the use of transfer learning to
adapt models from data-rich locations to others. Given the potential for
high-impact applications of satellite imagery across geographies, a systematic
assessment of transfer learning implications is warranted. In this work, we
consider the task of land-cover segmentation and study the fairness
implications of transferring models across locations. We leverage a large
satellite image segmentation benchmark with 5987 images from 18 districts (9
urban and 9 rural). Via fairness metrics we quantify disparities in model
performance along two axes -- across urban-rural locations and across
land-cover classes. Findings show that state-of-the-art models have better
overall accuracy in rural areas compared to urban areas, through unsupervised
domain adaptation methods transfer learning better to urban versus rural areas
and enlarge fairness gaps. In analysis of reasons for these findings, we show
that raw satellite images are overall more dissimilar between source and target
districts for rural than for urban locations. This work highlights the need to
conduct fairness analysis for satellite imagery segmentation models and
motivates the development of methods for fair transfer learning in order not to
introduce disparities between places, particularly urban and rural locations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Machine Learning Model Evaluation in Pathology. (arXiv:2204.05205v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05205">
<div class="article-summary-box-inner">
<span><p>Machine Learning has been applied to pathology images in research and
clinical practice with promising outcomes. However, standard ML models often
lack the rigorous evaluation required for clinical decisions. Machine learning
techniques for natural images are ill-equipped to deal with pathology images
that are significantly large and noisy, require expensive labeling, are hard to
interpret, and are susceptible to spurious correlations. We propose a set of
practical guidelines for ML evaluation in pathology that address the above
concerns. The paper includes measures for setting up the evaluation framework,
effectively dealing with variability in labels, and a recommended suite of
tests to address issues related to domain shift, robustness, and confounding
variables. We hope that the proposed framework will bridge the gap between ML
researchers and domain experts, leading to wider adoption of ML techniques in
pathology and improving patient outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06718">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) achieves impressive success in the field
of computer vision during the past few decades. As the core of CNNs, image
convolution operation helps CNNs to achieve good performance on image-related
tasks. However, image convolution is hard to be implemented and parallelized.
In this paper, we propose a novel neural network model, namely CEMNet, that can
be trained in frequency domain. The most important motivation of this research
is that we can use the very simple element-wise multiplication operation to
replace the image convolution in frequency domain based on Cross-Correlation
Theorem. We further introduce Weight Fixation Mechanism to alleviate
over-fitting, and analyze the working behavior of Batch Normalization, Leaky
ReLU and Dropout in frequency domain to design their counterparts for CEMNet.
Also, to deal with complex inputs brought by DFT, we design two branch network
structure for CEMNet. Experimental results imply that CEMNet works well in
frequency domain, and achieve good performance on MNIST and CIFAR-10 databases.
To our knowledge, CEMNet is the first model trained in Fourier Domain that
achieves more than 70\% validation accuracy on CIFAR-10 database.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-18 23:06:39.929916913 UTC">2022-04-18 23:06:39 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>