<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-25T01:30:00Z">10-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing. (arXiv:2110.11338v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11338">
<div class="article-summary-box-inner">
<span><p>Vision-language transformers (VL transformers) have shown impressive accuracy
in cross-modal retrieval. However, most of the existing VL transformers use
early-interaction dataflow that computes a joint representation for the
text-image input. In the retrieval stage, such models need to infer on all the
matched text-image combinations, which causes high computing costs. The goal of
this paper is to decompose the early-interaction dataflow inside the
pre-trained VL transformer to achieve acceleration while maintaining its
outstanding accuracy. To achieve this, we propose a novel Vision-language
Transformer Decomposing (VLDeformer) to modify the VL transformer as an
individual encoder for a single image or text through contrastive learning,
which accelerates retrieval speed by thousands of times. Meanwhile, we propose
to compose bi-modal hard negatives for the contrastive learning objective,
which enables the VLDeformer to maintain the outstanding accuracy of the
backbone VL transformer. Extensive experiments on COCO and Flickr30k datasets
demonstrate the superior performance of the proposed method. Considering both
effectiveness and efficiency, VLDeformer provides a superior selection for
cross-modal retrieval in the similar pre-training datascale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCENIC: A JAX Library for Computer Vision Research and Beyond. (arXiv:2110.11403v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11403">
<div class="article-summary-box-inner">
<span><p>Scenic is an open-source JAX library with a focus on Transformer-based models
for computer vision research and beyond. The goal of this toolkit is to
facilitate rapid experimentation, prototyping, and research of new vision
architectures and models. Scenic supports a diverse range of vision tasks
(e.g., classification, segmentation, detection)and facilitates working on
multi-modal problems, along with GPU/TPU support for multi-host, multi-device
large-scale training. Scenic also offers optimized implementations of
state-of-the-art research models spanning a wide range of modalities. Scenic
has been successfully used for numerous projects and published papers and
continues serving as the library of choice for quick prototyping and
publication of new research ideas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SYNERGY: Building Task Bots at Scale Using Symbolic Knowledge and Machine Teaching. (arXiv:2110.11514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11514">
<div class="article-summary-box-inner">
<span><p>In this paper we explore the use of symbolic knowledge and machine teaching
to reduce human data labeling efforts in building neural task bots. We propose
SYNERGY, a hybrid learning framework where a task bot is developed in two
steps: (i) Symbolic knowledge to neural networks: Large amounts of simulated
dialog sessions are generated based on task-specific symbolic knowledge which
is represented as a task schema consisting of dialog flows and task-oriented
databases. Then a pre-trained neural dialog model, SOLOIST, is fine-tuned on
the simulated dialogs to build a bot for the task. (ii) Neural learning: The
fine-tuned neural dialog model is continually refined with a handful of real
task-specific dialogs via machine teaching, where training samples are
generated by human teachers interacting with the task bot. We validate SYNERGY
on four dialog tasks. Experimental results show that SYNERGY maps task-specific
knowledge into neural dialog models achieving greater diversity and coverage of
dialog flows, and continually improves model performance with machine teaching,
thus demonstrating strong synergistic effects of symbolic knowledge and machine
teaching.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Bridge between Training and Inference for Dialogue. (arXiv:2110.11560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11560">
<div class="article-summary-box-inner">
<span><p>Although exposure bias has been widely studied in some NLP tasks, it faces
its unique challenges in dialogue response generation, the representative
one-to-various generation scenario. In real human dialogue, there are many
appropriate responses for the same context, not only with different
expressions, but also with different topics. Therefore, due to the much bigger
gap between various ground-truth responses and the generated synthetic
response, exposure bias is more challenging in dialogue generation task. What's
more, as MLE encourages the model to only learn the common words among
different ground-truth responses, but ignores the interesting and specific
parts, exposure bias may further lead to the common response generation
problem, such as "I don't know" and "HaHa?" In this paper, we propose a novel
adaptive switching mechanism, which learns to automatically transit between
ground-truth learning and generated learning regarding the word-level matching
score, such as the cosine similarity. Experimental results on both Chinese STC
dataset and English Reddit dataset, show that our adaptive method achieves a
significant improvement in terms of metric-based evaluation and human
evaluation, as compared with the state-of-the-art exposure bias approaches.
Further analysis on NMT task also shows that our model can achieve a
significant improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Counterfactuals via Latent Optimization and Shapley-Guided Search. (arXiv:2110.11589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11589">
<div class="article-summary-box-inner">
<span><p>We study the problem of generating counterfactual text for a classifier as a
means for understanding and debugging classification. Given a textual input and
a classification model, we aim to minimally alter the text to change the
model's prediction. White-box approaches have been successfully applied to
similar problems in vision where one can directly optimize the continuous
input. Optimization-based approaches become difficult in the language domain
due to the discrete nature of text. We bypass this issue by directly optimizing
in the latent space and leveraging a language model to generate candidate
modifications from optimized latent representations. We additionally use
Shapley values to estimate the combinatoric effect of multiple changes. We then
use these estimates to guide a beam search for the final counterfactual text.
We achieve favorable performance compared to recent white-box and black-box
baselines using human and automatic evaluations. Ablation studies show that
both latent optimization and the use of Shapley values improve success rate and
the quality of the generated counterfactuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCICAP: Generating Captions for Scientific Figures. (arXiv:2110.11624v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11624">
<div class="article-summary-box-inner">
<span><p>Researchers use figures to communicate rich, complex information in
scientific papers. The captions of these figures are critical to conveying
effective messages. However, low-quality figure captions commonly occur in
scientific articles and may decrease understanding. In this paper, we propose
an end-to-end neural framework to automatically generate informative,
high-quality captions for scientific figures. To this end, we introduce SCICAP,
a large-scale figure-caption dataset based on computer science arXiv papers
published between 2010 and 2020. After pre-processing - including figure-type
classification, sub-figure identification, text normalization, and caption text
selection - SCICAP contained more than two million figures extracted from over
290,000 papers. We then established baseline models that caption graph plots,
the dominant (19.2%) figure type. The experimental results showed both
opportunities and steep challenges of generating captions for scientific
figures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ListReader: Extracting List-form Answers for Opinion Questions. (arXiv:2110.11692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11692">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) is a high-level ability of natural language
processing. Most extractive ma-chine reading comprehension models focus on
factoid questions (e.g., who, when, where) and restrict the output answer as a
short and continuous span in the original passage. However, in real-world
scenarios, many questions are non-factoid (e.g., how, why) and their answers
are organized in the list format that contains multiple non-contiguous spans.
Naturally, existing extractive models are by design unable to answer such
questions. To address this issue, this paper proposes ListReader, a neural
ex-tractive QA model for list-form answer. In addition to learning the
alignment between the question and content, we introduce a heterogeneous graph
neural network to explicitly capture the associations among candidate segments.
Moreover, our model adopts a co-extraction setting that can extract either
span- or sentence-level answers, allowing better applicability. Two large-scale
datasets of different languages are constructed to support this study.
Experimental results show that our model considerably outperforms various
strong baselines. Further discussions provide an intuitive understanding of how
our model works and where the performance gain comes from.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Decoding Strategies for Increasing Specificity. (arXiv:2110.11850v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11850">
<div class="article-summary-box-inner">
<span><p>Language models are known to produce vague and generic outputs. We propose
two unsupervised decoding strategies based on either word-frequency or
point-wise mutual information to increase the specificity of any model that
outputs a probability distribution over its vocabulary at generation time. We
test the strategies in a prompt completion task; with human evaluations, we
find that both strategies increase the specificity of outputs with only modest
decreases in sensibility. We also briefly present a summarization use case,
where these strategies can produce more specific summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning-based NLP Data Pipeline for EHR Scanned Document Information Extraction. (arXiv:2110.11864v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11864">
<div class="article-summary-box-inner">
<span><p>Scanned documents in electronic health records (EHR) have been a challenge
for decades, and are expected to stay in the foreseeable future. Current
approaches for processing often include image preprocessing, optical character
recognition (OCR), and text mining. However, there is limited work that
evaluates the choice of image preprocessing methods, the selection of NLP
models, and the role of document layout. The impact of each element remains
unknown. We evaluated this method on a use case of two key indicators for sleep
apnea, Apnea hypopnea index (AHI) and oxygen saturation (SaO2) values, from
scanned sleep study reports. Our data that included 955 manually annotated
reports was secondarily utilized from a previous study in the University of
Texas Medical Branch. We performed image preprocessing: gray-scaling followed
by 1 iteration of dilating and erode, and 20% contrast increasing. The OCR was
implemented with the Tesseract OCR engine. A total of seven Bag-of-Words models
(Logistic Regression, Ridge Regression, Lasso Regression, Support Vector
Machine, k-Nearest Neighbor, Na\"ive Bayes, and Random Forest) and three deep
learning-based models (BiLSTM, BERT, and Clinical BERT) were evaluated. We also
evaluated the combinations of image preprocessing methods (gray-scaling, dilate
&amp; erode, increased contrast by 20%, increased contrast by 60%), and two deep
learning architectures (with and without structured input that provides
document layout information). Our proposed method using Clinical BERT reached
an AUROC of 0.9743 and document accuracy of 94.76% for AHI, and an AUROC of
0.9523, and document accuracy of 91.61% for SaO2. We demonstrated the proper
use of image preprocessing and document layout could be beneficial to scanned
document processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLiText: A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks. (arXiv:2110.11869v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11869">
<div class="article-summary-box-inner">
<span><p>In natural language processing (NLP), state-of-the-art (SOTA) semi-supervised
learning (SSL) frameworks have shown great performance on deep pre-trained
language models such as BERT, and are expected to significantly reduce the
demand for manual labeling. However, our empirical studies indicate that these
frameworks are not suitable for lightweight models such as TextCNN, LSTM and
etc. In this work, we develop a new SSL framework called FLiText, which stands
for Faster and Lighter semi-supervised Text classification. FLiText introduces
an inspirer network together with the consistency regularization framework,
which leverages a generalized regular constraint on the lightweight models for
efficient SSL. As a result, FLiText obtains new SOTA performance for
lightweight models across multiple SSL benchmarks on text classification.
Compared with existing SOTA SSL methods on TextCNN, FLiText improves the
accuracy of lightweight model TextCNN from 51.00% to 90.49% on IMDb, 39.8% to
58.06% on Yelp-5, and from 55.3% to 65.08% on Yahoo. In addition, compared with
the fully supervised method on the full dataset, FLiText just uses less than 1%
of labeled data to improve the accuracy by 6.59%, 3.94%, and 3.22% on the
datasets of IMDb, Yelp-5, and Yahoo respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biomedical text summarization using Conditional Generative Adversarial Network(CGAN). (arXiv:2110.11870v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11870">
<div class="article-summary-box-inner">
<span><p>Text summarization in medicine can help doctors for reducing the time to
access important information from countless documents. The paper offers a
supervised extractive summarization method based on conditional generative
adversarial networks using convolutional neural networks. Unlike previous
models, which often use greedy methods to select sentences, we use a new
approach for selecting sentences. Moreover, we provide a network for biomedical
word embedding, which improves summarization. An essential contribution of the
paper is introducing a new loss function for the discriminator, making the
discriminator perform better. The proposed model achieves results comparable to
the state-of-the-art approaches, as determined by the ROUGE metric. Experiments
on the medical dataset show that the proposed method works on average 5% better
than the competing models and is more similar to the reference summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An N-gram based approach to auto-extracting topics from research articles. (arXiv:2110.11879v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11879">
<div class="article-summary-box-inner">
<span><p>A lot of manual work goes into identifying a topic for an article. With a
large volume of articles, the manual process can be exhausting. Our approach
aims to address this issue by automatically extracting topics from the text of
large Numbers of articles. This approach takes into account the efficiency of
the process. Based on existing N-gram analysis, our research examines how often
certain words appear in documents in order to support automatic topic
extraction. In order to improve efficiency, we apply custom filtering standards
to our research. Additionally, delete as many noncritical or irrelevant phrases
as possible. In this way, we can ensure we are selecting unique keyphrases for
each article, which capture its core idea. For our research, we chose to center
on the autonomous vehicle domain, since the research is relevant to our daily
lives. We have to convert the PDF versions of most of the research papers into
editable types of files such as TXT. This is because most of the research
papers are only in PDF format. To test our proposed idea of automating,
numerous articles on robotics have been selected. Next, we evaluate our
approach by comparing the result with that obtained manually.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Dialogue System with AUDITED. (arXiv:2110.11881v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11881">
<div class="article-summary-box-inner">
<span><p>We devise a multimodal conversation system for dialogue utterances composed
of text, image or both modalities. We leverage Auxiliary UnsuperviseD vIsual
and TExtual Data (AUDITED). To improve the performance of text-based task, we
utilize translations of target sentences from English to French to form the
assisted supervision. For the image-based task, we employ the DeepFashion
dataset in which we seek nearest neighbor images of positive and negative
target images of the MMD data. These nearest neighbors form the nearest
neighbor embedding providing an external context for target images. We form two
methods to create neighbor embedding vectors, namely Neighbor Embedding by Hard
Assignment (NEHA) and Neighbor Embedding by Soft Assignment (NESA) which
generate context subspaces per target image. Subsequently, these subspaces are
learnt by our pipeline as a context for the target data. We also propose a
discriminator which switches between the image- and text-based tasks. We show
improvements over baselines on the large-scale Multimodal Dialogue Dataset
(MMD) and SIMMC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges in Procedural Multimodal Machine Comprehension:A Novel Way To Benchmark. (arXiv:2110.11899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11899">
<div class="article-summary-box-inner">
<span><p>We focus on Multimodal Machine Reading Comprehension (M3C) where a model is
expected to answer questions based on given passage (or context), and the
context and the questions can be in different modalities. Previous works such
as RecipeQA have proposed datasets and cloze-style tasks for evaluation.
However, we identify three critical biases stemming from the question-answer
generation process and memorization capabilities of large deep models. These
biases makes it easier for a model to overfit by relying on spurious
correlations or naive data patterns. We propose a systematic framework to
address these biases through three Control-Knobs that enable us to generate a
test bed of datasets of progressive difficulty levels. We believe that our
benchmark (referred to as Meta-RecipeQA) will provide, for the first time, a
fine grained estimate of a model's generalization capabilities. We also propose
a general M3C model that is used to realize several prior SOTA models and
motivate a novel hierarchical transformer based reasoning network (HTRN). We
perform a detailed evaluation of these models with different language and
visual features on our benchmark. We observe a consistent improvement with HTRN
over SOTA (~18% in Visual Cloze task and ~13% in average over all the tasks).
We also observe a drop in performance across all the models when testing on
RecipeQA and proposed Meta-RecipeQA (e.g. 83.6% versus 67.1% for HTRN), which
shows that the proposed dataset is relatively less biased. We conclude by
highlighting the impact of the control knobs with some quantitative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Double Trouble: How to not explain a text classifier's decisions using counterfactuals synthesized by masked language models?. (arXiv:2110.11929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11929">
<div class="article-summary-box-inner">
<span><p>Explaining how important each input feature is to a classifier's decision is
critical in high-stake applications. An underlying principle behind dozens of
explanation methods is to take the prediction difference between
before-and-after an input feature (here, a token) is removed as its attribution
- the individual treatment effect in causal inference. A recent method called
Input Marginalization (IM) (Kim et al., 2020) uses BERT to replace a token -
i.e. simulating the do(.) operator - yielding more plausible counterfactuals.
However, our rigorous evaluation using five metrics and on three datasets found
IM explanations to be consistently more biased, less accurate, and less
plausible than those derived from simply deleting a word.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cleaning Dirty Books: Post-OCR Processing for Previously Scanned Texts. (arXiv:2110.11934v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11934">
<div class="article-summary-box-inner">
<span><p>Substantial amounts of work are required to clean large collections of
digitized books for NLP analysis, both because of the presence of errors in the
scanned text and the presence of duplicate volumes in the corpora. In this
paper, we consider the issue of deduplication in the presence of optical
character recognition (OCR) errors. We present methods to handle these errors,
evaluated on a collection of 19,347 texts from the Project Gutenberg dataset
and 96,635 texts from the HathiTrust Library. We demonstrate that improvements
in language models now enable the detection and correction of OCR errors
without consideration of the scanning image itself. The inconsistencies found
by aligning pairs of scans of the same underlying work provides training data
to build models for detecting and correcting errors. We identify the canonical
version for each of 17,136 repeatedly-scanned books from 58,808 scans. Finally,
we investigate methods to detect and correct errors in single-copy texts. We
show that on average, our method corrects over six times as many errors as it
introduces. We also provide interesting analysis on the relation between
scanning quality and other factors such as location and publication year.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Learning Assessment through Multimodal Analysis of Reading Behaviour and Language Comprehension. (arXiv:2110.11938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11938">
<div class="article-summary-box-inner">
<span><p>Reading comprehension, which has been defined as gaining an understanding of
written text through a process of translating grapheme into meaning, is an
important academic skill. Other language learning skills - writing, speaking
and listening, all are connected to reading comprehension. There have been
several measures proposed by researchers to automate the assessment of
comprehension skills for second language (L2) learners, especially English as
Second Language (ESL) and English as Foreign Language (EFL) learners. However,
current methods measure particular skills without analysing the impact of
reading frequency on comprehension skills. In this dissertation, we show how
different skills could be measured and scored automatically. We also
demonstrate, using example experiments on multiple forms of learners'
responses, how frequent reading practices could impact on the variables of
multimodal skills (reading pattern, writing, and oral fluency).
</p>
<p>This thesis comprises of five studies. The first and second studies are based
on eye-tracking data collected from EFL readers in repeated reading (RR)
sessions. The third and fourth studies are to evaluate free-text summary
written by EFL readers in repeated reading sessions. The fifth and last study,
described in the sixth chapter of the thesis, is to evaluate recorded oral
summaries recited by EFL readers in repeated reading sessions.
</p>
<p>In a nutshell, through this dissertation, we show that multimodal skills of
learners could be assessed to measure their comprehension skills as well as to
measure the effect of repeated readings on these skills in the course of time,
by finding significant features and by applying machine learning techniques
with a combination of statistical models such as LMER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving BERT with Self-Supervised Attention. (arXiv:2004.03808v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.03808">
<div class="article-summary-box-inner">
<span><p>One of the most popular paradigms of applying large pre-trained NLP models
such as BERT is to fine-tune it on a smaller dataset. However, one challenge
remains as the fine-tuned model often overfits on smaller datasets. A symptom
of this phenomenon is that irrelevant or misleading words in the sentence,
which are easy to understand for human beings, can substantially degrade the
performance of these finetuned BERT models. In this paper, we propose a novel
technique, called Self-Supervised Attention (SSA) to help facilitate this
generalization challenge. Specifically, SSA automatically generates weak,
token-level attention labels iteratively by probing the fine-tuned model from
the previous iteration. We investigate two different ways of integrating SSA
into BERT and propose a hybrid approach to combine their benefits. Empirically,
through a variety of public datasets, we illustrate significant performance
improvement using our SSA-enhanced BERT model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions. (arXiv:2102.05379v3 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05379">
<div class="article-summary-box-inner">
<span><p>Generative flows and diffusion models have been predominantly trained on
ordinal data, for example natural images. This paper introduces two extensions
of flows and diffusion for categorical data such as language or image
segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined
by a composition of a continuous distribution (such as a normalizing flow), and
an argmax function. To optimize this model, we learn a probabilistic inverse
for the argmax that lifts the categorical data to a continuous space.
Multinomial Diffusion gradually adds categorical noise in a diffusion process,
for which the generative denoising process is learned. We demonstrate that our
method outperforms existing dequantization approaches on text modelling and
modelling on image segmentation maps in log-likelihood.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emotion, Physiological-Emotion, and Stress. (arXiv:2104.07123v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07123">
<div class="article-summary-box-inner">
<span><p>Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing on the
tasks of sentiment and emotion, as well as physiological-emotion and
emotion-based stress recognition through more comprehensively integrating the
audio-visual, language, and biological signal modalities. The purpose of MuSe
2021 is to bring together communities from different disciplines; mainly, the
audio-visual emotion recognition community (signal-based), the sentiment
analysis community (symbol-based), and the health informatics community. We
present four distinct sub-challenges: MuSe-Wilder and MuSe-Stress which focus
on continuous emotion (valence and arousal) prediction; MuSe-Sent, in which
participants recognise five classes each for valence and arousal; and
MuSe-Physio, in which the novel aspect of `physiological-emotion' is to be
predicted. For this years' challenge, we utilise the MuSe-CaR dataset focusing
on user-generated reviews and introduce the Ulm-TSST dataset, which displays
people in stressful depositions. This paper also provides detail on the
state-of-the-art feature sets extracted from these datasets for utilisation by
our baseline model, a Long Short-Term Memory-Recurrent Neural Network. For each
sub-challenge, a competitive baseline for participants is set; namely, on test,
we report a Concordance Correlation Coefficient (CCC) of .4616 CCC for
MuSe-Wilder; .4717 CCC for MuSe-Stress, and .4606 CCC for MuSe-Physio. For
MuSe-Sent an F1 score of 32.82 % is obtained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphTMT: Unsupervised Graph-based Topic Modeling from Video Transcripts. (arXiv:2105.01466v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01466">
<div class="article-summary-box-inner">
<span><p>To unfold the tremendous amount of multimedia data uploaded daily to social
media platforms, effective topic modeling techniques are needed. Existing work
tends to apply topic models on written text datasets. In this paper, we propose
a topic extractor on video transcripts. Exploiting neural word embeddings
through graph-based clustering, we aim to improve usability and semantic
coherence. Unlike most topic models, this approach works without knowing the
true number of topics, which is important when no such assumption can or should
be made. Experimental results on the real-life multimodal dataset MuSe-CaR
demonstrates that our approach GraphTMT extracts coherent and meaningful topics
and outperforms baseline methods. Furthermore, we successfully demonstrate the
applicability of our approach on the popular Citysearch corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Hierarchical Attention for Answering Complex Questions over Long Documents. (arXiv:2106.00200v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00200">
<div class="article-summary-box-inner">
<span><p>We propose a new model, DocHopper, that iteratively attends to different
parts of long, hierarchically structured documents to answer complex questions.
Similar to multi-hop question-answering (QA) systems, at each step, DocHopper
uses a query $q$ to attend to information from a document, combines this
``retrieved'' information with $q$ to produce the next query. However, in
contrast to most previous multi-hop QA systems, DocHopper is able to
``retrieve'' either short passages or long sections of the document, thus
emulating a multi-step process of ``navigating'' through a long document to
answer a question. To enable this novel behavior, DocHopper does not combine
document information with $q$ by concatenating text to the text of $q$, but by
combining a compact neural representation of $q$ with a compact neural
representation of a hierarchical part of the document, which can potentially be
quite large. We experiment with DocHopper on four different QA tasks that
require reading long and complex documents to answer multi-hop questions, and
show that DocHopper achieves state-of-the-art results on three of the datasets.
Additionally, DocHopper is efficient at inference time, being 3--10 times
faster than the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02636">
<div class="article-summary-box-inner">
<span><p>As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present,
and future. We introduce MERLOT, a model that learns multimodal script
knowledge by watching millions of YouTube videos with transcribed speech -- in
an entirely label-free, self-supervised manner. By pretraining with a mix of
both frame-level (spatial) and video-level (temporal) objectives, our model not
only learns to match images to temporally corresponding words, but also to
contextualize what is happening globally over time. As a result, MERLOT
exhibits strong out-of-the-box representations of temporal commonsense, and
achieves state-of-the-art performance on 12 different video QA datasets when
finetuned. It also transfers well to the world of static images, allowing
models to reason about the dynamic context behind visual scenes. On Visual
Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,
outperforming state-of-the-art models of similar size by over 3%, even those
that make heavy use of auxiliary supervised data (like object bounding boxes).
</p>
<p>Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the
pretraining video corpus; and 3) using diverse objectives that encourage
full-stack multimodal reasoning, from the recognition to cognition level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Large Scale Molecular Language Representations Capture Important Structural Information?. (arXiv:2106.09553v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09553">
<div class="article-summary-box-inner">
<span><p>Predicting the chemical properties of a molecule is of great importance in
many applications, including drug discovery and material design. Machine
learning based molecular property prediction holds the promise of enabling
accurate predictions at much less computationally complex cost when compared
to, for example, Density Functional Theory (DFT) calculations. Various
representation learning methods in a supervised setting, including the features
extracted using graph neural nets, have emerged for such tasks. However, the
vast chemical space and the limited availability of labels make supervised
learning challenging, calling for learning a general-purpose molecular
representation. Recently, pre-trained transformer-based language models on
large unlabeled corpus have produced state-of-the-art results in many
downstream natural language processing tasks. Inspired by this development, we
present molecular embeddings obtained by training an efficient transformer
encoder model, MoLFormer. This model employs a linear attention mechanism
coupled with highly parallelized training on SMILES sequences of 1.1 billion
unlabeled molecules from the PubChem and ZINC datasets. Experiments show that
the learned molecular representation outperforms supervised and unsupervised
graph neural net baselines on several regression and classification tasks from
10 benchmark datasets, while performing competitively on others. Further
analyses, specifically through the lens of attention, demonstrate that
MoLFormer indeed learns a molecule's local and global structural aspects. These
results provide encouraging evidence that large-scale molecular language models
can capture sufficient structural information to be able to predict diverse
molecular properties, including quantum-chemical properties
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages. (arXiv:2108.07127v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07127">
<div class="article-summary-box-inner">
<span><p>We translate a closed text that is known in advance and available in many
languages into a new and severely low resource language. Most human translation
efforts adopt a portion-based approach to translate consecutive pages/chapters
in order, which may not suit machine translation. We compare the portion-based
approach that optimizes coherence of the text locally with the random sampling
approach that increases coverage of the text globally. Our results show that
the random sampling approach performs better. When training on a seed corpus of
~1,000 lines from the Bible and testing on the rest of the Bible (~30,000
lines), random sampling gives a performance gain of +11.0 BLEU using English as
a simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a
Mayan language. Furthermore, we compare three ways of updating machine
translation models with increasing amount of human post-edited data through
iterations. We find that adding newly post-edited data to training after
vocabulary update without self-supervision performs the best. We propose an
algorithm for human and machine to work together seamlessly to translate a
closed text into a severely low resource language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Unreasonable Effectiveness of the Baseline: Discussing SVMs in Legal Text Classification. (arXiv:2109.07234v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07234">
<div class="article-summary-box-inner">
<span><p>We aim to highlight an interesting trend to contribute to the ongoing debate
around advances within legal Natural Language Processing. Recently, the focus
for most legal text classification tasks has shifted towards large pre-trained
deep learning models such as BERT. In this paper, we show that a more
traditional approach based on Support Vector Machine classifiers reaches
surprisingly competitive performance with BERT-based models on the
classification tasks in the LexGLUE benchmark. We also highlight that error
reduction obtained by using specialised BERT-based models over baselines is
noticeably smaller in the legal domain when compared to general language tasks.
We present and discuss three hypotheses as potential explanations for these
results to support future discussions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08722">
<div class="article-summary-box-inner">
<span><p>Prerequisite chain learning helps people acquire new knowledge efficiently.
While people may quickly determine learning paths over concepts in a domain,
finding such paths in other domains can be challenging. We introduce
Domain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this
cross-domain prerequisite chain learning task efficiently. Our novel model
consists of a variational graph autoencoder (VGAE) and a domain discriminator.
The VGAE is trained to predict concept relations through link prediction, while
the domain discriminator takes both source and target domain data as input and
is trained to predict domain labels. Most importantly, this method only needs
simple homogeneous graphs as input, compared with the current state-of-the-art
model. We evaluate our model on the LectureBankCD dataset, and results show
that our model outperforms recent graph-based benchmarks while using only 1/10
of graph scale and 1/3 computation time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually Grounded Reasoning across Languages and Cultures. (arXiv:2109.13238v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13238">
<div class="article-summary-box-inner">
<span><p>The design of widespread vision-and-language datasets and pre-trained
encoders directly adopts, or draws inspiration from, the concepts and images of
ImageNet. While one can hardly overestimate how much this benchmark contributed
to progress in computer vision, it is mostly derived from lexical databases and
image queries in English, resulting in source material with a North American or
Western European bias. Therefore, we devise a new protocol to construct an
ImageNet-style hierarchy representative of more languages and cultures. In
particular, we let the selection of both concepts and images be entirely driven
by native speakers, rather than scraping them automatically. Specifically, we
focus on a typologically diverse set of languages, namely, Indonesian, Mandarin
Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images
obtained through this new protocol, we create a multilingual dataset for
{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting
statements from native speaker annotators about pairs of images. The task
consists of discriminating whether each grounded statement is true or false. We
establish a series of baselines using state-of-the-art models and find that
their cross-lingual transfer performance lags dramatically behind supervised
performance in English. These results invite us to reassess the robustness and
accuracy of current state-of-the-art models beyond a narrow domain, but also
open up new exciting challenges for the development of truly multilingual and
multicultural systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Key Point Analysis via Contrastive Learning and Extractive Argument Summarization. (arXiv:2109.15086v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15086">
<div class="article-summary-box-inner">
<span><p>Key point analysis is the task of extracting a set of concise and high-level
statements from a given collection of arguments, representing the gist of these
arguments. This paper presents our proposed approach to the Key Point Analysis
shared task, collocated with the 8th Workshop on Argument Mining. The approach
integrates two complementary components. One component employs contrastive
learning via a siamese neural network for matching arguments to key points; the
other is a graph-based extractive summarization model for generating key
points. In both automatic and manual evaluation, our approach was ranked best
among all submissions to the shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Personality Detection Tools for Software Engineering Research: How Far Can We Go?. (arXiv:2110.05035v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05035">
<div class="article-summary-box-inner">
<span><p>Assessing the personality of software engineers may help to match individual
traits with the characteristics of development activities such as code review
and testing, as well as support managers in team composition. However,
self-assessment questionnaires are not a practical solution for collecting
multiple observations on a large scale. Instead, automatic personality
detection, while overcoming these limitations, is based on off-the-shelf
solutions trained on non-technical corpora, which might not be readily
applicable to technical domains like Software Engineering (SE). In this paper,
we first assess the performance of general-purpose personality detection tools
when applied to a technical corpus of developers' emails retrieved from the
public archives of the Apache Software Foundation. We observe a general low
accuracy of predictions and an overall disagreement among the tools. Second, we
replicate two previous research studies in SE by replacing the personality
detection tool used to infer developers' personalities from pull-request
discussions and emails. We observe that the original results are not confirmed,
i.e., changing the tool used in the original study leads to diverging
conclusions. Our results suggest a need for personality detection tools
specially targeted for the software engineering domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Importance Estimation from Multiple Perspectives for Keyphrase Extraction. (arXiv:2110.09749v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09749">
<div class="article-summary-box-inner">
<span><p>Keyphrase extraction is a fundamental task in Natural Language Processing,
which usually contains two main parts: candidate keyphrase extraction and
keyphrase importance estimation. From the view of human understanding
documents, we typically measure the importance of phrase according to its
syntactic accuracy, information saliency, and concept consistency
simultaneously. However, most existing keyphrase extraction approaches only
focus on the part of them, which leads to biased results. In this paper, we
propose a new approach to estimate the importance of keyphrase from multiple
perspectives (called as \textit{KIEMP}) and further improve the performance of
keyphrase extraction. Specifically, \textit{KIEMP} estimates the importance of
phrase with three modules: a chunking module to measure its syntactic accuracy,
a ranking module to check its information saliency, and a matching module to
judge the concept (i.e., topic) consistency between phrase and the whole
document. These three modules are seamlessly jointed together via an end-to-end
multi-task learning model, which is helpful for three parts to enhance each
other and balance the effects of three perspectives. Experimental results on
six benchmark datasets show that \textit{KIEMP} outperforms the existing
state-of-the-art keyphrase extraction approaches in most cases.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing. (arXiv:2110.11338v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11338">
<div class="article-summary-box-inner">
<span><p>Vision-language transformers (VL transformers) have shown impressive accuracy
in cross-modal retrieval. However, most of the existing VL transformers use
early-interaction dataflow that computes a joint representation for the
text-image input. In the retrieval stage, such models need to infer on all the
matched text-image combinations, which causes high computing costs. The goal of
this paper is to decompose the early-interaction dataflow inside the
pre-trained VL transformer to achieve acceleration while maintaining its
outstanding accuracy. To achieve this, we propose a novel Vision-language
Transformer Decomposing (VLDeformer) to modify the VL transformer as an
individual encoder for a single image or text through contrastive learning,
which accelerates retrieval speed by thousands of times. Meanwhile, we propose
to compose bi-modal hard negatives for the contrastive learning objective,
which enables the VLDeformer to maintain the outstanding accuracy of the
backbone VL transformer. Extensive experiments on COCO and Flickr30k datasets
demonstrate the superior performance of the proposed method. Considering both
effectiveness and efficiency, VLDeformer provides a superior selection for
cross-modal retrieval in the similar pre-training datascale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESOD:Edge-based Task Scheduling for Object Detection. (arXiv:2110.11342v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11342">
<div class="article-summary-box-inner">
<span><p>Object Detection on the mobile system is a challenge in terms of everything.
Nowadays, many object detection models have been designed, and most of them
concentrate on precision. However, the computation burden of those models on
mobile systems is unacceptable. Researchers have designed some lightweight
networks for mobiles by sacrificing precision. We present a novel edge-based
task scheduling framework for object detection (termed as ESOD). In detail, we
train a DNN model (termed as pre-model) to predict which object detection model
to use for the coming task and offloads to which edge servers by physical
characteristics of the image task (e.g., brightness, saturation). The results
show that ESOD can reduce latency and energy consumption by an average of
22.13% and 29.60% and improve the mAP to 45.8(with 0.9 mAP better),
respectively, compared with the SOTA DETR model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decentralised Person Re-Identification with Selective Knowledge Aggregation. (arXiv:2110.11384v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11384">
<div class="article-summary-box-inner">
<span><p>Existing person re-identification (Re-ID) methods mostly follow a centralised
learning paradigm which shares all training data to a collection for model
learning. This paradigm is limited when data from different sources cannot be
shared due to privacy concerns. To resolve this problem, two recent works have
introduced decentralised (federated) Re-ID learning for constructing a globally
generalised model (server)without any direct access to local training data nor
shared data across different source domains (clients). However, these methods
are poor on how to adapt the generalised model to maximise its performance on
individual client domain Re-ID tasks having different Re-ID label spaces, due
to a lack of understanding of data heterogeneity across domains. We call this
poor 'model personalisation'. In this work, we present a new Selective
Knowledge Aggregation approach to decentralised person Re-ID to optimise the
trade-off between model personalisation and generalisation. Specifically, we
incorporate attentive normalisation into the normalisation layers in a deep
ReID model and propose to learn local normalisation layers specific to each
domain, which are decoupled from the global model aggregation in federated
Re-ID learning. This helps to preserve model personalisation knowledge on each
local client domain and learn instance-specific information. Further, we
introduce a dual local normalisation mechanism to learn generalised
normalisation layers in each local model, which are then transmitted to the
global model for central aggregation. This facilitates selective knowledge
aggregation on the server to construct a global generalised model for
out-of-the-box deployment on unseen novel domains. Extensive experiments on
eight person Re-ID datasets show that the proposed approach to decentralised
Re-ID significantly outperforms the state-of-the-art decentralised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEX: Domain Embedding Expansion for Generalized Person Re-identification. (arXiv:2110.11391v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11391">
<div class="article-summary-box-inner">
<span><p>In recent years, supervised Person Re-identification (Person ReID) approaches
have demonstrated excellent performance. However, when these methods are
applied to inputs from a different camera network, they typically suffer from
significant performance degradation. Different from most domain adaptation (DA)
approaches addressing this issue, we focus on developing a domain
generalization (DG) Person ReID model that can be deployed without additional
fine-tuning or adaptation. In this paper, we propose the Domain Embedding
Expansion (DEX) module. DEX dynamically manipulates and augments deep features
based on person and domain labels during training, significantly improving the
generalization capability and robustness of Person ReID models to unseen
domains. We also developed a light version of DEX (DEXLite), applying negative
sampling techniques to scale to larger datasets and reduce memory usage for
multi-branch networks. Our proposed DEX and DEXLite can be combined with many
existing methods, Bag-of-Tricks (BagTricks), the Multi-Granularity Network
(MGN), and Part-Based Convolutional Baseline (PCB), in a plug-and-play manner.
With DEX and DEXLite, existing methods can gain significant improvements when
tested on other unseen datasets, thereby demonstrating the general
applicability of our method. Our solution outperforms the state-of-the-art DG
Person ReID methods in all large-scale benchmarks as well as in most the
small-scale benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning. (arXiv:2110.11395v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11395">
<div class="article-summary-box-inner">
<span><p>Pruning neural networks reduces inference time and memory costs. On standard
hardware, these benefits will be especially prominent if coarse-grained
structures, like feature maps, are pruned. We devise two novel saliency-based
methods for second-order structured pruning (SOSP) which include correlations
among all structures and layers. Our main method SOSP-H employs an innovative
second-order approximation, which enables saliency evaluations by fast
Hessian-vector products. SOSP-H thereby scales like a first-order method
despite taking into account the full Hessian. We validate SOSP-H by comparing
it to our second method SOSP-I that uses a well-established Hessian
approximation, and to numerous state-of-the-art methods. While SOSP-H performs
on par or better in terms of accuracy, it has clear advantages in terms of
scalability and efficiency. This allowed us to scale SOSP-H to large-scale
vision tasks, even though it captures correlations across all layers of the
network. To underscore the global nature of our pruning methods, we evaluate
their performance not only by removing structures from a pretrained network,
but also by detecting architectural bottlenecks. We show that our algorithms
allow to systematically reveal architectural bottlenecks, which we then remove
to further increase the accuracy of the networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-Driven Reconstruction Technique based on Newton's Method for Emission Tomography. (arXiv:2110.11396v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11396">
<div class="article-summary-box-inner">
<span><p>In this work, we present the Deep Newton Reconstruction Network (DNR-Net), a
hybrid data-driven reconstruction technique for emission tomography inspired by
Newton's method, a well-known iterative optimization algorithm. The DNR-Net
employs prior information about the tomographic problem provided by the
projection operator while utilizing deep learning approaches to a) imitate
Newton's method by approximating the Newton descent direction and b) provide
data-driven regularisation. We demonstrate that DNR-Net is capable of providing
high-quality image reconstructions using data from SPECT phantom simulations by
applying it to reconstruct images from noisy sinograms, each one containing 24
projections. The Structural Similarity Index (SSIM) and the Contrast-to-Noise
ratio (CNR) were used to quantify the image quality. We also compare our
results to those obtained by the OSEM method. According to the quantitative
results, the DNR-Net produces reconstructions comparable to the ones produced
by OSEM while featuring higher contrast and less noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Channel redundancy and overlap in convolutional neural networks with channel-wise NNK graphs. (arXiv:2110.11400v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11400">
<div class="article-summary-box-inner">
<span><p>Feature spaces in the deep layers of convolutional neural networks (CNNs) are
often very high-dimensional and difficult to interpret. However, convolutional
layers consist of multiple channels that are activated by different types of
inputs, which suggests that more insights may be gained by studying the
channels and how they relate to each other. In this paper, we first analyze
theoretically channel-wise non-negative kernel (CW-NNK) regression graphs,
which allow us to quantify the overlap between channels and, indirectly, the
intrinsic dimension of the data representation manifold. We find that
redundancy between channels is significant and varies with the layer depth and
the level of regularization during training. Additionally, we observe that
there is a correlation between channel overlap in the last convolutional layer
and generalization performance. Our experimental results demonstrate that these
techniques can lead to a better understanding of deep representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCENIC: A JAX Library for Computer Vision Research and Beyond. (arXiv:2110.11403v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11403">
<div class="article-summary-box-inner">
<span><p>Scenic is an open-source JAX library with a focus on Transformer-based models
for computer vision research and beyond. The goal of this toolkit is to
facilitate rapid experimentation, prototyping, and research of new vision
architectures and models. Scenic supports a diverse range of vision tasks
(e.g., classification, segmentation, detection)and facilitates working on
multi-modal problems, along with GPU/TPU support for multi-host, multi-device
large-scale training. Scenic also offers optimized implementations of
state-of-the-art research models spanning a wide range of modalities. Scenic
has been successfully used for numerous projects and published papers and
continues serving as the library of choice for quick prototyping and
publication of new research ideas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Illiterate DALL$\cdot$E Learns to Compose. (arXiv:2110.11405v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11405">
<div class="article-summary-box-inner">
<span><p>Although DALL$\cdot$E has shown an impressive ability of composition-based
systematic generalization in image generation, it requires the dataset of
text-image pairs and the compositionality is provided by the text. In contrast,
object-centric representation models like the Slot Attention model learn
composable representations without the text prompt. However, unlike
DALL$\cdot$E its ability to systematically generalize for zero-shot generation
is significantly limited. In this paper, we propose a simple but novel
slot-based autoencoding architecture, called SLATE, for combining the best of
both worlds: learning object-centric representations that allows systematic
generalization in zero-shot image generation without text. As such, this model
can also be seen as an illiterate DALL$\cdot$E model. Unlike the pixel-mixture
decoders of existing object-centric representation models, we propose to use
the Image GPT decoder conditioned on the slots for capturing complex
interactions among the slots and pixels. In experiments, we show that this
simple and easy-to-implement architecture not requiring a text prompt achieves
significant improvement in in-distribution and out-of-distribution (zero-shot)
image generation and qualitatively comparable or better slot-attention
structure than the models based on mixture decoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-Data Pipelines for Machine Learning Applications. (arXiv:2110.11407v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11407">
<div class="article-summary-box-inner">
<span><p>Data pipelines are an essential component for end-to-end solutions that take
machine learning algorithms to production. Engineering data pipelines for
video-sequences poses several challenges including isolation of key-frames from
video sequences that are high quality and represent significant variations in
the scene. Manual isolation of such quality key-frames can take hours of
sifting through hours worth of video data. In this work, we present a data
pipeline framework that can automate this process of manual frame sifting in
video sequences by controlling the fraction of frames that can be removed based
on image quality and content type. Additionally, the frames that are retained
can be automatically tagged per sequence, thereby simplifying the process of
automated data retrieval for future ML model deployments. We analyze the
performance of the proposed video-data pipeline for versioned deployment and
monitoring for object detection algorithms that are trained on outdoor
autonomous driving video sequences. The proposed video-data pipeline can retain
anywhere between 0.1-20% of the all input frames that are representative of
high image quality and high variations in content. This frame selection,
automated scene tagging followed by model verification can be completed in
under 30 seconds for 22 video-sequences under analysis in this work. Thus, the
proposed framework can be scaled to additional video-sequence data sets for
automating ML versioned deployments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PROVES: Establishing Image Provenance using Semantic Signatures. (arXiv:2110.11411v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11411">
<div class="article-summary-box-inner">
<span><p>Modern AI tools, such as generative adversarial networks, have transformed
our ability to create and modify visual data with photorealistic results.
However, one of the deleterious side-effects of these advances is the emergence
of nefarious uses in manipulating information in visual data, such as through
the use of deep fakes. We propose a novel architecture for preserving the
provenance of semantic information in images to make them less susceptible to
deep fake attacks. Our architecture includes semantic signing and verification
steps. We apply this architecture to verifying two types of semantic
information: individual identities (faces) and whether the photo was taken
indoors or outdoors. Verification accounts for a collection of common image
transformation, such as translation, scaling, cropping, and small rotations,
and rejects adversarial transformations, such as adversarially perturbed or, in
the case of face verification, swapped faces. Experiments demonstrate that in
the case of provenance of faces in an image, our approach is robust to
black-box adversarial transformations (which are rejected) as well as benign
transformations (which are accepted), with few false negatives and false
positives. Background verification, on the other hand, is susceptible to
black-box adversarial examples, but becomes significantly more robust after
adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time, low-cost multi-person 3D pose estimation. (arXiv:2110.11414v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11414">
<div class="article-summary-box-inner">
<span><p>The process of tracking human anatomy in computer vision is referred to pose
estimation, and it is used in fields ranging from gaming to surveillance.
Three-dimensional pose estimation traditionally requires advanced equipment,
such as multiple linked intensity cameras or high-resolution time-of-flight
cameras to produce depth images. However, there are applications, e.g.~consumer
electronics, where significant constraints are placed on the size, power
consumption, weight and cost of the usable technology. Here, we demonstrate
that computational imaging methods can achieve accurate pose estimation and
overcome the apparent limitations of time-of-flight sensors designed for much
simpler tasks. The sensor we use is already widely integrated in consumer-grade
mobile devices, and despite its low spatial resolution, only 4$\times$4 pixels,
our proposed Pixels2Pose system transforms its data into accurate depth maps
and 3D pose data of multiple people up to a distance of 3 m from the sensor. We
are able to generate depth maps at a resolution of 32$\times$32 and 3D
localization of a body parts with an error of only $\approx$10 cm at a frame
rate of 7 fps. This work opens up promising real-life applications in scenarios
that were previously restricted by the advanced hardware requirements and cost
of time-of-flight technology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIRE-SNN: Harnessing the Inherent Robustness of Energy-Efficient Deep Spiking Neural Networks by Training with Crafted Input Noise. (arXiv:2110.11417v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11417">
<div class="article-summary-box-inner">
<span><p>Low-latency deep spiking neural networks (SNNs) have become a promising
alternative to conventional artificial neural networks (ANNs) because of their
potential for increased energy efficiency on event-driven neuromorphic
hardware. Neural networks, including SNNs, however, are subject to various
adversarial attacks and must be trained to remain resilient against such
attacks for many applications. Nevertheless, due to prohibitively high training
costs associated with SNNs, analysis, and optimization of deep SNNs under
various adversarial attacks have been largely overlooked. In this paper, we
first present a detailed analysis of the inherent robustness of low-latency
SNNs against popular gradient-based attacks, namely fast gradient sign method
(FGSM) and projected gradient descent (PGD). Motivated by this analysis, to
harness the model robustness against these attacks we present an SNN training
algorithm that uses crafted input noise and incurs no additional training time.
To evaluate the merits of our algorithm, we conducted extensive experiments
with variants of VGG and ResNet on both CIFAR-10 and CIFAR-100 datasets.
Compared to standard trained direct input SNNs, our trained models yield
improved classification accuracy of up to 13.7% and 10.1% on FGSM and PGD
attack-generated images, respectively, with negligible loss in clean image
accuracy. Our models also outperform inherently robust SNNs trained on
rate-coded inputs with improved or similar classification performance on
attack-generated images while having up to 25x and 4.6x lower latency and
computation energy, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Graph Sampling for Short Video Summarization using Gershgorin Disc Alignment. (arXiv:2110.11420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11420">
<div class="article-summary-box-inner">
<span><p>We study the problem of efficiently summarizing a short video into several
keyframes, leveraging recent progress in fast graph sampling. Specifically, we
first construct a similarity path graph (SPG) $\mathcal{G}$, represented by
graph Laplacian matrix $\mathbf{L}$, where the similarities between adjacent
frames are encoded as positive edge weights. We show that maximizing the
smallest eigenvalue $\lambda_{\min}(\mathbf{B})$ of a coefficient matrix
$\mathbf{B} = \text{diag}(\mathbf{a}) + \mu \mathbf{L}$, where $\mathbf{a}$ is
the binary keyframe selection vector, is equivalent to minimizing a worst-case
signal reconstruction error. We prove that, after partitioning $\mathcal{G}$
into $Q$ sub-graphs $\{\mathcal{G}^q\}^Q_{q=1}$, the smallest Gershgorin circle
theorem (GCT) lower bound of $Q$ corresponding coefficient matrices -- $\min_q
\lambda^-_{\min}(\mathbf{B}^q)$ -- is a lower bound for
$\lambda_{\min}(\mathbf{B})$. This inspires a fast graph sampling algorithm to
iteratively partition $\mathcal{G}$ into $Q$ sub-graphs using $Q$ samples
(keyframes), while maximizing $\lambda^-_{\min}(\mathbf{B}^q)$ for each
sub-graph $\mathcal{G}^q$. Experimental results show that our algorithm
achieves comparable video summarization performance as state-of-the-art
methods, at a substantially reduced complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUGL: Large Scale Multi Person Conditional Action Generation with Locomotion. (arXiv:2110.11460v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11460">
<div class="article-summary-box-inner">
<span><p>We introduce MUGL, a novel deep neural model for large-scale, diverse
generation of single and multi-person pose-based action sequences with
locomotion. Our controllable approach enables variable-length generations
customizable by action category, across more than 100 categories. To enable
intra/inter-category diversity, we model the latent generative space using a
Conditional Gaussian Mixture Variational Autoencoder. To enable realistic
generation of actions involving locomotion, we decouple local pose and global
trajectory components of the action sequence. We incorporate duration-aware
feature representations to enable variable-length sequence generation. We use a
hybrid pose sequence representation with 3D pose sequences sourced from videos
and 3D Kinect-based sequences of NTU-RGBD-120. To enable principled comparison
of generation quality, we employ suitably modified strong baselines during
evaluation. Although smaller and simpler compared to baselines, MUGL provides
better quality generations, paving the way for practical and controllable
large-scale human action generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation. (arXiv:2110.11474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11474">
<div class="article-summary-box-inner">
<span><p>Humans typically perceive the establishment of an action in a video through
the interaction between an actor and the surrounding environment. An action
only starts when the main actor in the video begins to interact with the
environment, while it ends when the main actor stops the interaction. Despite
the great progress in temporal action proposal generation, most existing works
ignore the aforementioned fact and leave their model learning to propose
actions as a black-box. In this paper, we make an attempt to simulate that
ability of a human by proposing Actor Environment Interaction (AEI) network to
improve the video representation for temporal action proposals generation. AEI
contains two modules, i.e., perception-based visual representation (PVR) and
boundary-matching module (BMM). PVR represents each video snippet by taking
human-human relations and humans-environment relations into consideration using
the proposed adaptive attention mechanism. Then, the video representation is
taken by BMM to generate action proposals. AEI is comprehensively evaluated in
ActivityNet-1.3 and THUMOS-14 datasets, on temporal action proposal and
detection tasks, with two boundary-matching architectures (i.e., CNN-based and
GCN-based) and two classifiers (i.e., Unet and P-GCN). Our AEI robustly
outperforms the state-of-the-art methods with remarkable performance and
generalization for both temporal action proposal generation and temporal action
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixNorm: Test-Time Adaptation Through Online Normalization Estimation. (arXiv:2110.11478v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11478">
<div class="article-summary-box-inner">
<span><p>We present a simple and effective way to estimate the batch-norm statistics
during test time, to fast adapt a source model to target test samples. Known as
Test-Time Adaptation, most prior works studying this task follow two
assumptions in their evaluation where (1) test samples come together as a large
batch, and (2) all from a single test distribution. However, in practice, these
two assumptions may not stand, the reasons for which we propose two new
evaluation settings where batch sizes are arbitrary and multiple distributions
are considered. Unlike the previous methods that require a large batch of
single distribution during test time to calculate stable batch-norm statistics,
our method avoid any dependency on large online batches and is able to estimate
accurate batch-norm statistics with a single sample. The proposed method
significantly outperforms the State-Of-The-Art in the newly proposed settings
in Test-Time Adaptation Task, and also demonstrates improvements in various
other settings such as Source-Free Unsupervised Domain Adaptation and Zero-Shot
Classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SymbioLCD: Ensemble-Based Loop Closure Detection using CNN-Extracted Objects and Visual Bag-of-Words. (arXiv:2110.11491v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11491">
<div class="article-summary-box-inner">
<span><p>Loop closure detection is an essential tool of Simultaneous Localization and
Mapping (SLAM) to minimize drift in its localization. Many state-of-the-art
loop closure detection (LCD) algorithms use visual Bag-of-Words (vBoW), which
is robust against partial occlusions in a scene but cannot perceive the
semantics or spatial relationships between feature points. CNN object
extraction can address those issues, by providing semantic labels and spatial
relationships between objects in a scene. Previous work has mainly focused on
replacing vBoW with CNN-derived features. In this paper, we propose SymbioLCD,
a novel ensemble-based LCD that utilizes both CNN-extracted objects and vBoW
features for LCD candidate prediction. When used in tandem, the added elements
of object semantics and spatial-awareness create a more robust and symbiotic
loop closure detection system. The proposed SymbioLCD uses scale-invariant
spatial and semantic matching, Hausdorff distance with temporal constraints,
and a Random Forest that utilizes combined information from both CNN-extracted
objects and vBoW features for predicting accurate loop closure candidates.
Evaluation of the proposed method shows it outperforms other Machine Learning
(ML) algorithms - such as SVM, Decision Tree and Neural Network, and
demonstrates that there is a strong symbiosis between CNN-extracted object
information and vBoW features which assists accurate LCD candidate prediction.
Furthermore, it is able to perceive loop closure candidates earlier than
state-of-the-art SLAM algorithms, utilizing added spatial and semantic
information from CNN-extracted objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Decision-Making for Active Object Detection from Hand. (arXiv:2110.11524v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11524">
<div class="article-summary-box-inner">
<span><p>A key component of understanding hand-object interactions is the ability to
identify the active object -- the object that is being manipulated by the human
hand -- despite the occlusion induced by hand-object interactions. Based on the
observation that hand appearance is a strong indicator of the location and size
of the active object, we set up our active object detection method as a
sequential decision-making process that is conditioned on the location and
appearance of the hands. The key innovation of our approach is the design of
the active object detection policy that uses an internal representation called
the Relational Box Field, which allows for every pixel to regress an improved
location of an active object bounding box, essentially giving every pixel the
ability to vote for a better bounding box location. The policy is trained using
a hybrid imitation learning and reinforcement learning approach, and at test
time, the policy is used repeatedly to refine the bounding box location of the
active object. We perform experiments on two large-scale datasets: 100DOH and
MECCANO, improving AP50 performance by 8% and 30%, respectively, over the state
of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Digital and Physical-World Attacks on Remote Pulse Detection. (arXiv:2110.11525v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11525">
<div class="article-summary-box-inner">
<span><p>Remote photoplethysmography (rPPG) is a technique for estimating blood volume
changes from reflected light without the need for a contact sensor. We present
the first examples of presentation attacks in the digital and physical domains
on rPPG from face video. Digital attacks are easily performed by adding
imperceptible periodic noise to the input videos. Physical attacks are
performed with illumination from visible spectrum LEDs placed in close
proximity to the face, while still being difficult to perceive with the human
eye. We also show that our attacks extend beyond medical applications, since
the method can effectively generate a strong periodic pulse on 3D-printed face
masks, which presents difficulties for pulse-based face presentation attack
detection (PAD). The paper concludes with ideas for using this work to improve
robustness of rPPG methods and pulse-based face PAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wide Neural Networks Forget Less Catastrophically. (arXiv:2110.11526v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11526">
<div class="article-summary-box-inner">
<span><p>A growing body of research in continual learning is devoted to overcoming the
"Catastrophic Forgetting" of neural networks by designing new algorithms that
are more robust to the distribution shifts. While the recent progress in
continual learning literature is encouraging, our understanding of what
properties of neural networks contribute to catastrophic forgetting is still
limited. To address this, instead of focusing on continual learning algorithms,
in this work, we focus on the model itself and study the impact of "width" of
the neural network architecture on catastrophic forgetting, and show that width
has a surprisingly significant effect on forgetting. To explain this effect, we
study the learning dynamics of the network from various perspectives such as
gradient norm and sparsity, orthogonalization, and lazy training regime. We
provide potential explanations that are consistent with the empirical results
across different architectures and continual learning benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo Supervised Monocular Depth Estimation with Teacher-Student Network. (arXiv:2110.11545v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11545">
<div class="article-summary-box-inner">
<span><p>Despite recent improvement of supervised monocular depth estimation, the lack
of high quality pixel-wise ground truth annotations has become a major hurdle
for further progress. In this work, we propose a new unsupervised depth
estimation method based on pseudo supervision mechanism by training a
teacher-student network with knowledge distillation. It strategically
integrates the advantages of supervised and unsupervised monocular depth
estimation, as well as unsupervised binocular depth estimation. Specifically,
the teacher network takes advantage of the effectiveness of binocular depth
estimation to produce accurate disparity maps, which are then used as the
pseudo ground truth to train the student network for monocular depth
estimation. This effectively converts the problem of unsupervised learning to
supervised learning. Our extensive experimental results demonstrate that the
proposed method outperforms the state-of-the-art on the KITTI benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signature-Graph Networks. (arXiv:2110.11551v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11551">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach for visual representation learning called
Signature-Graph Neural Networks (SGN). SGN learns latent global structures that
augment the feature representation of Convolutional Neural Networks (CNN). SGN
constructs unique undirected graphs for each image based on the CNN feature
maps. The feature maps are partitioned into a set of equal and non-overlapping
patches. The graph nodes are located on high-contrast sharp convolution
features with the local maxima or minima in these patches. The node embeddings
are aggregated through novel Signature-Graphs based on horizontal and vertical
edge connections. The representation vectors are then computed based on the
spectral Laplacian eigenvalues of the graphs. SGN outperforms existing methods
of recent graph convolutional networks, generative adversarial networks, and
auto-encoders with image classification accuracy of 99.65% on ASIRRA, 99.91% on
MNIST, 98.55% on Fashion-MNIST, 96.18% on CIFAR-10, 84.71% on CIFAR-100, 94.36%
on STL10, and 95.86% on SVHN datasets. We also introduce a novel implementation
of the state-of-the-art multi-head attention (MHA) on top of the proposed SGN.
Adding SGN to MHA improved the image classification accuracy from 86.92% to
94.36% on the STL10 dataset
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototypical Classifier for Robust Class-Imbalanced Learning. (arXiv:2110.11553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11553">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have been shown to be very powerful methods for many
supervised learning tasks. However, they can also easily overfit to training
set biases, i.e., label noise and class imbalance. While both learning with
noisy labels and class-imbalanced learning have received tremendous attention,
existing works mainly focus on one of these two training set biases. To fill
the gap, we propose \textit{Prototypical Classifier}, which does not require
fitting additional parameters given the embedding network. Unlike conventional
classifiers that are biased towards head classes, Prototypical Classifier
produces balanced and comparable predictions for all classes even though the
training set is class-imbalanced. By leveraging this appealing property, we can
easily detect noisy labels by thresholding the confidence scores predicted by
Prototypical Classifier, where the threshold is dynamically adjusted through
the iteration. A sample reweghting strategy is then applied to mitigate the
influence of noisy labels. We test our method on CIFAR-10-LT, CIFAR-100-LT and
Webvision datasets, observing that Prototypical Classifier obtains substaintial
improvements compared with state of the arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHAttnSurv: Multi-Head Attention for Survival Prediction Using Whole-Slide Pathology Images. (arXiv:2110.11558v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11558">
<div class="article-summary-box-inner">
<span><p>In pathology, whole-slide images (WSI) based survival prediction has
attracted increasing interest. However, given the large size of WSIs and the
lack of pathologist annotations, extracting the prognostic information from
WSIs remains a challenging task. Previous studies have used multiple instance
learning approaches to combine the information from multiple randomly sampled
patches, but different visual patterns may contribute differently to prognosis
prediction. In this study, we developed a multi-head attention approach to
focus on various parts of a tumor slide, for more comprehensive information
extraction from WSIs. We evaluated our approach on four cancer types from The
Cancer Genome Atlas database. Our model achieved an average c-index of 0.640,
outperforming two existing state-of-the-art approaches for WSI-based survival
prediction, which have an average c-index of 0.603 and 0.619 on these datasets.
Visualization of our attention maps reveals each attention head focuses
synergistically on different morphological patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EvoGAN: An Evolutionary Computation Assisted GAN. (arXiv:2110.11583v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11583">
<div class="article-summary-box-inner">
<span><p>The image synthesis technique is relatively well established which can
generate facial images that are indistinguishable even by human beings.
However, all of these approaches uses gradients to condition the output,
resulting in the outputting the same image with the same input. Also, they can
only generate images with basic expression or mimic an expression instead of
generating compound expression. In real life, however, human expressions are of
great diversity and complexity. In this paper, we propose an evolutionary
algorithm (EA) assisted GAN, named EvoGAN, to generate various compound
expressions with any accurate target compound expression. EvoGAN uses an EA to
search target results in the data distribution learned by GAN. Specifically, we
use the Facial Action Coding System (FACS) as the encoding of an EA and use a
pre-trained GAN to generate human facial images, and then use a pre-trained
classifier to recognize the expression composition of the synthesized images as
the fitness function to guide the search of the EA. Combined random searching
algorithm, various images with the target expression can be easily sythesized.
Quantitative and Qualitative results are presented on several compound
expressions, and the experimental results demonstrate the feasibility and the
potential of EvoGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wide and Narrow: Video Prediction from Context and Motion. (arXiv:2110.11586v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11586">
<div class="article-summary-box-inner">
<span><p>Video prediction, forecasting the future frames from a sequence of input
frames, is a challenging task since the view changes are influenced by various
factors, such as the global context surrounding the scene and local motion
dynamics. In this paper, we propose a new framework to integrate these
complementary attributes to predict complex pixel dynamics through deep
networks. We present global context propagation networks that iteratively
aggregate the non-local neighboring representations to preserve the contextual
information over the past frames. To capture the local motion pattern of
objects, we also devise local filter memory networks that generate adaptive
filter kernels by storing the prototypical motion of moving objects in the
memory. The proposed framework, utilizing the outputs from both networks, can
address blurry predictions and color distortion. We conduct experiments on
Caltech pedestrian and UCF101 datasets, and demonstrate state-of-the-art
results. Especially for multi-step prediction, we obtain an outstanding
performance in quantitative and qualitative evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIML/CVL RGB-D Dataset: 2M RGB-D Images of Natural Indoor and Outdoor Scenes. (arXiv:2110.11590v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11590">
<div class="article-summary-box-inner">
<span><p>This manual is intended to provide a detailed description of the DIML/CVL
RGB-D dataset. This dataset is comprised of 2M color images and their
corresponding depth maps from a great variety of natural indoor and outdoor
scenes. The indoor dataset was constructed using the Microsoft Kinect v2, while
the outdoor dataset was built using the stereo cameras (ZED stereo camera and
built-in stereo camera). Table I summarizes the details of our dataset,
including acquisition, processing, format, and toolbox. Refer to Section II and
III for more details.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Inspired Autoencoder for Unsupervised Hyperspectral Image Super-Resolution. (arXiv:2110.11591v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11591">
<div class="article-summary-box-inner">
<span><p>This paper focuses on hyperspectral image (HSI) super-resolution that aims to
fuse a low-spatial-resolution HSI and a high-spatial-resolution multispectral
image to form a high-spatial-resolution HSI (HR-HSI). Existing deep
learning-based approaches are mostly supervised that rely on a large number of
labeled training samples, which is unrealistic. The commonly used model-based
approaches are unsupervised and flexible but rely on hand-craft priors.
Inspired by the specific properties of model, we make the first attempt to
design a model inspired deep network for HSI super-resolution in an
unsupervised manner. This approach consists of an implicit autoencoder network
built on the target HR-HSI that treats each pixel as an individual sample. The
nonnegative matrix factorization (NMF) of the target HR-HSI is integrated into
the autoencoder network, where the two NMF parts, spectral and spatial
matrices, are treated as decoder parameters and hidden outputs respectively. In
the encoding stage, we present a pixel-wise fusion model to estimate hidden
outputs directly, and then reformulate and unfold the model's algorithm to form
the encoder network. With the specific architecture, the proposed network is
similar to a manifold prior-based model, and can be trained patch by patch
rather than the entire image. Moreover, we propose an additional unsupervised
network to estimate the point spread function and spectral response function.
Experimental results conducted on both synthetic and real datasets demonstrate
the effectiveness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Text-Image Joint Embedding for Efficient Cross-Modal Retrieval with Deep Feature Engineering. (arXiv:2110.11592v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11592">
<div class="article-summary-box-inner">
<span><p>This paper introduces a two-phase deep feature engineering framework for
efficient learning of semantics enhanced joint embedding, which clearly
separates the deep feature engineering in data preprocessing from training the
text-image joint embedding model. We use the Recipe1M dataset for the technical
description and empirical validation. In preprocessing, we perform deep feature
engineering by combining deep feature engineering with semantic context
features derived from raw text-image input data. We leverage LSTM to identify
key terms, deep NLP models from the BERT family, TextRank, or TF-IDF to produce
ranking scores for key terms before generating the vector representation for
each key term by using word2vec. We leverage wideResNet50 and word2vec to
extract and encode the image category semantics of food images to help semantic
alignment of the learned recipe and image embeddings in the joint latent space.
In joint embedding learning, we perform deep feature engineering by optimizing
the batch-hard triplet loss function with soft-margin and double negative
sampling, taking into account also the category-based alignment loss and
discriminator-based alignment loss. Extensive experiments demonstrate that our
SEJE approach with deep feature engineering significantly outperforms the
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Detection of Injection and Press Mold Parts on 2D Drawing Using Deep Neural Network. (arXiv:2110.11593v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11593">
<div class="article-summary-box-inner">
<span><p>This paper proposes a method to automatically detect the key feature parts in
a CAD of commercial TV and monitor using a deep neural network. We developed a
deep learning pipeline that can detect the injection parts such as hook, boss,
undercut and press parts such as DPS, Embo-Screwless, Embo-Burring, and EMBO in
the 2D CAD drawing images. We first cropped the drawing to a specific size for
the training efficiency of a deep neural network. Then, we use Cascade R-CNN to
find the position of injection and press parts and use Resnet-50 to predict the
orientation of the parts. Finally, we convert the position of the parts found
through the cropped image to the position of the original image. As a result,
we obtained detection accuracy of injection and press parts with 84.1% in AP
(Average Precision), 91.2% in AR(Average Recall), 72.0% in AP, 87.0% in AR, and
orientation accuracy of injection and press parts with 94.4% and 92.0%, which
can facilitate the faster design in industrial product design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoShotXAI: Using Prototypical Few-Shot Architecture for Explainable AI. (arXiv:2110.11597v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11597">
<div class="article-summary-box-inner">
<span><p>Unexplainable black-box models create scenarios where anomalies cause
deleterious responses, thus creating unacceptable risks. These risks have
motivated the field of eXplainable Artificial Intelligence (XAI) to improve
trust by evaluating local interpretability in black-box neural networks.
Unfortunately, the ground truth is unavailable for the model's decision, so
evaluation is limited to qualitative assessment. Further, interpretability may
lead to inaccurate conclusions about the model or a false sense of trust. We
propose to improve XAI from the vantage point of the user's trust by exploring
a black-box model's latent feature space. We present an approach, ProtoShotXAI,
that uses a Prototypical few-shot network to explore the contrastive manifold
between nonlinear features of different classes. A user explores the manifold
by perturbing the input features of a query sample and recording the response
for a subset of exemplars from any class. Our approach is the first locally
interpretable XAI model that can be extended to, and demonstrated on, few-shot
networks. We compare ProtoShotXAI to the state-of-the-art XAI approaches on
MNIST, Omniglot, and ImageNet to demonstrate, both quantitatively and
qualitatively, that ProtoShotXAI provides more flexibility for model
exploration. Finally, ProtoShotXAI also demonstrates novel explainabilty and
detectabilty on adversarial samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High Fidelity 3D Reconstructions with Limited Physical Views. (arXiv:2110.11599v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11599">
<div class="article-summary-box-inner">
<span><p>Multi-view triangulation is the gold standard for 3D reconstruction from 2D
correspondences given known calibration and sufficient views. However in
practice, expensive multi-view setups -- involving tens sometimes hundreds of
cameras -- are required in order to obtain the high fidelity 3D reconstructions
necessary for many modern applications. In this paper we present a novel
approach that leverages recent advances in 2D-3D lifting using neural shape
priors while also enforcing multi-view equivariance. We show how our method can
achieve comparable fidelity to expensive calibrated multi-view rigs using a
limited (2-3) number of uncalibrated camera views.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Semi-Supervised Learning for3D Objects. (arXiv:2110.11601v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11601">
<div class="article-summary-box-inner">
<span><p>In recent years, semi-supervised learning has been widely explored and shows
excellent data efficiency for 2D data. There is an emerging need to improve
data efficiency for 3D tasks due to the scarcity of labeled 3D data. This paper
explores how the coherence of different modelities of 3D data (e.g. point
cloud, image, and mesh) can be used to improve data efficiency for both 3D
classification and retrieval tasks. We propose a novel multimodal
semi-supervised learning framework by introducing instance-level consistency
constraint and a novel multimodal contrastive prototype (M2CP) loss. The
instance-level consistency enforces the network to generate consistent
representations for multimodal data of the same object regardless of its
modality. The M2CP maintains a multimodal prototype for each class and learns
features with small intra-class variations by minimizing the feature distance
of each object to its prototype while maximizing the distance to the others.
Our proposed framework significantly outperforms all the state-of-the-art
counterparts for both classification and retrieval tasks by a large margin on
the modelNet10 and ModelNet40 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Stream Attention Learning for Monocular Vehicle Velocity and Inter-Vehicle Distance Estimation. (arXiv:2110.11608v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11608">
<div class="article-summary-box-inner">
<span><p>Vehicle velocity and inter-vehicle distance estimation are essential for ADAS
(Advanced driver-assistance systems) and autonomous vehicles. To save the cost
of expensive ranging sensors, recent studies focus on using a low-cost
monocular camera to perceive the environment around the vehicle in a
data-driven fashion. Existing approaches treat each vehicle independently for
perception and cause inconsistent estimation. Furthermore, important
information like context and spatial relation in 2D object detection is often
neglected in the velocity estimation pipeline. In this paper, we explore the
relationship between vehicles of the same frame with a
global-relative-constraint (GLC) loss to encourage consistent estimation. A
novel multi-stream attention network (MSANet) is proposed to extract different
aspects of features, e.g., spatial and contextual features, for joint vehicle
velocity and inter-vehicle distance estimation. Experiments show the
effectiveness and robustness of our proposed approach. MSANet outperforms
state-of-the-art algorithms on both the KITTI dataset and TuSimple velocity
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCICAP: Generating Captions for Scientific Figures. (arXiv:2110.11624v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11624">
<div class="article-summary-box-inner">
<span><p>Researchers use figures to communicate rich, complex information in
scientific papers. The captions of these figures are critical to conveying
effective messages. However, low-quality figure captions commonly occur in
scientific articles and may decrease understanding. In this paper, we propose
an end-to-end neural framework to automatically generate informative,
high-quality captions for scientific figures. To this end, we introduce SCICAP,
a large-scale figure-caption dataset based on computer science arXiv papers
published between 2010 and 2020. After pre-processing - including figure-type
classification, sub-figure identification, text normalization, and caption text
selection - SCICAP contained more than two million figures extracted from over
290,000 papers. We then established baseline models that caption graph plots,
the dominant (19.2%) figure type. The experimental results showed both
opportunities and steep challenges of generating captions for scientific
figures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Generalization Performance of Surgical Phase Recognition with Expert-Generated Annotations. (arXiv:2110.11626v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11626">
<div class="article-summary-box-inner">
<span><p>As the area of application of deep neural networks expands to areas requiring
expertise, e.g., in medicine and law, more exquisite annotation processes for
expert knowledge training are required. In particular, it is difficult to
guarantee generalization performance in the clinical field in the case of
expert knowledge training where opinions may differ even among experts on
annotations. To raise the issue of the annotation generation process for
expertise training of CNNs, we verified the annotations for surgical phase
recognition of laparoscopic cholecystectomy and subtotal gastrectomy for
gastric cancer. We produce calibrated annotations for the seven phases of
cholecystectomy by analyzing the discrepancies of previously annotated labels
and by discussing the criteria of surgical phases. For gastrectomy for gastric
cancer has more complex twenty-one surgical phases, we generate consensus
annotation by the revision process with five specialists. By training the
CNN-based surgical phase recognition networks with revised annotations, we
achieved improved generalization performance over models trained with original
annotation under the same cross-validation settings. We showed that the
expertise data annotation pipeline for deep neural networks should be more
rigorous based on the type of problem to apply clinical field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Face Recognition with Large Age Gaps by Learning to Distinguish Children. (arXiv:2110.11630v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11630">
<div class="article-summary-box-inner">
<span><p>Despite the unprecedented improvement of face recognition, existing face
recognition models still show considerably low performances in determining
whether a pair of child and adult images belong to the same identity. Previous
approaches mainly focused on increasing the similarity between child and adult
images of a given identity to overcome the discrepancy of facial appearances
due to aging. However, we observe that reducing the similarity between child
images of different identities is crucial for learning distinct features among
children and thus improving face recognition performance in child-adult pairs.
Based on this intuition, we propose a novel loss function called the
Inter-Prototype loss which minimizes the similarity between child images.
Unlike the previous studies, the Inter-Prototype loss does not require
additional child images or training additional learnable parameters. Our
extensive experiments and in-depth analyses show that our approach outperforms
existing baselines in face recognition with child-adult pairs. Our code and
newly-constructed test sets of child-adult pairs are available at
https://github.com/leebebeto/Inter-Prototype.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Occlusion-Robust Object Pose Estimation with Holistic Representation. (arXiv:2110.11636v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11636">
<div class="article-summary-box-inner">
<span><p>Practical object pose estimation demands robustness against occlusions to the
target object. State-of-the-art (SOTA) object pose estimators take a two-stage
approach, where the first stage predicts 2D landmarks using a deep network and
the second stage solves for 6DOF pose from 2D-3D correspondences. Albeit widely
adopted, such two-stage approaches could suffer from novel occlusions when
generalising and weak landmark coherence due to disrupted features. To address
these issues, we develop a novel occlude-and-blackout batch augmentation
technique to learn occlusion-robust deep features, and a multi-precision
supervision architecture to encourage holistic pose representation learning for
accurate and coherent landmark predictions. We perform careful ablation tests
to verify the impact of our innovations and compare our method to SOTA pose
estimators. Without the need of any post-processing or refinement, our method
exhibits superior performance on the LINEMOD dataset. On the YCB-Video dataset
our method outperforms all non-refinement methods in terms of the ADD(-S)
metric. We also demonstrate the high data-efficiency of our method. Our code is
available at <a href="http://github.com/BoChenYS/ROPE">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTP-Net For Cross-Domain Trajectory Prediction. (arXiv:2110.11645v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11645">
<div class="article-summary-box-inner">
<span><p>Deep learning based trajectory prediction methods rely on large amount of
annotated future trajectories, but may not generalize well to a new scenario
captured by another camera. Meanwhile, annotating trajectories for training a
network for this new scenario is time-consuming and expensive, therefore it is
desirable to adapt the model trained with the annotated source domain
trajectories to the target domain.
</p>
<p>To tackle domain adaptation for trajectory prediction, we propose a
Cross-domain Trajectory Prediction Network (CTP-Net), in which LSTMs are used
to encode the observed trajectories of both domain, and their features are
aligned by a cross-domain feature discriminator. Further, considering the
consistency between the observed trajectories and the predicted trajectories in
the target domain, a target domain offset discriminator is utilized to
adversarially regularize the future trajectory predictions to be consistent
with the observed trajectories. Extensive experiments demonstrate the
effectiveness of the proposed domain adaptation for trajectory prediction
setting as well as our method on domain adaptation for trajectory prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pixel-by-Pixel Cross-Domain Alignment for Few-Shot Semantic Segmentation. (arXiv:2110.11650v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11650">
<div class="article-summary-box-inner">
<span><p>In this paper we consider the task of semantic segmentation in autonomous
driving applications. Specifically, we consider the cross-domain few-shot
setting where training can use only few real-world annotated images and many
annotated synthetic images. In this context, aligning the domains is made more
challenging by the pixel-wise class imbalance that is intrinsic in the
segmentation and that leads to ignoring the underrepresented classes and
overfitting the well represented ones. We address this problem with a novel
framework called Pixel-By-Pixel Cross-Domain Alignment (PixDA). We propose a
novel pixel-by-pixel domain adversarial loss following three criteria: (i)
align the source and the target domain for each pixel, (ii) avoid negative
transfer on the correctly represented pixels, and (iii) regularize the training
of infrequent classes to avoid overfitting. The pixel-wise adversarial training
is assisted by a novel sample selection procedure, that handles the imbalance
between source and target data, and a knowledge distillation strategy, that
avoids overfitting towards the few target images. We demonstrate on standard
synthetic-to-real benchmarks that PixDA outperforms previous state-of-the-art
methods in (1-5)-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Projective Manifold Gradient Layer for Deep Rotation Regression. (arXiv:2110.11657v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11657">
<div class="article-summary-box-inner">
<span><p>Regressing rotations on SO(3) manifold using deep neural networks is an
important yet unsolved problem. The gap between Euclidean network output space
and the non-Euclidean SO(3) manifold imposes a severe challenge for neural
network learning in both forward and backward passes. While several works have
proposed different regression-friendly rotation representations, very few works
have been devoted to improving the gradient backpropagating in the backward
pass. In this paper, we propose a manifold-aware gradient that directly
backpropagates into deep network weights. Leveraging the Riemannian gradient
and a novel projective gradient, our proposed regularized projective manifold
gradient (RPMG) helps networks achieve new state-of-the-art performance in a
variety of rotation estimation tasks. The proposed gradient layer can also be
applied to other smooth manifolds such as the unit sphere.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">1st Place Solution for the UVO Challenge on Video-based Open-World Segmentation 2021. (arXiv:2110.11661v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11661">
<div class="article-summary-box-inner">
<span><p>In this report, we introduce our (pretty straightforard) two-step
"detect-then-match" video instance segmentation method. The first step performs
instance segmentation for each frame to get a large number of instance mask
proposals. The second step is to do inter-frame instance mask matching with the
help of optical flow. We demonstrate that with high quality mask proposals, a
simple matching mechanism is good enough for tracking. Our approach achieves
the first place in the UVO 2021 Video-based Open-World Segmentation Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reimagine BiSeNet for Real-Time Domain Adaptation in Semantic Segmentation. (arXiv:2110.11662v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11662">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation models have reached remarkable performance across
various tasks. However, this performance is achieved with extremely large
models, using powerful computational resources and without considering training
and inference time. Real-world applications, on the other hand, necessitate
models with minimal memory demands, efficient inference speed, and executable
with low-resources embedded devices, such as self-driving vehicles. In this
paper, we look at the challenge of real-time semantic segmentation across
domains, and we train a model to act appropriately on real-world data even
though it was trained on a synthetic realm. We employ a new lightweight and
shallow discriminator that was specifically created for this purpose. To the
best of our knowledge, we are the first to present a real-time adversarial
approach for assessing the domain adaption problem in semantic segmentation. We
tested our framework in the two standard protocol: GTA5 to Cityscapes and
SYNTHIA to Cityscapes. Code is available at:
https://github.com/taveraantonio/RTDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCCN: Global Context Convolutional Network. (arXiv:2110.11664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11664">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Global Context Convolutional Network (GCCN) for
visual recognition. GCCN computes global features representing contextual
information across image patches. These global contextual features are defined
as local maxima pixels with high visual sharpness in each patch. These features
are then concatenated and utilised to augment the convolutional features. The
learnt feature vector is normalised using the global context features using
Frobenius norm. This straightforward approach achieves high accuracy in
compassion to the state-of-the-art methods with 94.6% and 95.41% on CIFAR-10
and STL-10 datasets, respectively. To explore potential impact of GCCN on other
visual representation tasks, we implemented GCCN as a based model to few-shot
image classification. We learn metric distances between the augmented feature
vectors and their prototypes representations, similar to Prototypical and
Matching Networks. GCCN outperforms state-of-the-art few-shot learning methods
achieving 99.9%, 84.8% and 80.74% on Omniglot, MiniImageNet and CUB-200,
respectively. GCCN has significantly improved on the accuracy of
state-of-the-art prototypical and matching networks by up to 30% in different
few-shot learning scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable, automated urban interventions to improve pedestrian and vehicle safety. (arXiv:2110.11672v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11672">
<div class="article-summary-box-inner">
<span><p>At the moment, urban mobility research and governmental initiatives are
mostly focused on motor-related issues, e.g. the problems of congestion and
pollution. And yet, we can not disregard the most vulnerable elements in the
urban landscape: pedestrians, exposed to higher risks than other road users.
Indeed, safe, accessible, and sustainable transport systems in cities are a
core target of the UN's 2030 Agenda. Thus, there is an opportunity to apply
advanced computational tools to the problem of traffic safety, in regards
especially to pedestrians, who have been often overlooked in the past. This
paper combines public data sources, large-scale street imagery and computer
vision techniques to approach pedestrian and vehicle safety with an automated,
relatively simple, and universally-applicable data-processing scheme. The steps
involved in this pipeline include the adaptation and training of a Residual
Convolutional Neural Network to determine a hazard index for each given urban
scene, as well as an interpretability analysis based on image segmentation and
class activation mapping on those same images. Combined, the outcome of this
computational approach is a fine-grained map of hazard levels across a city,
and an heuristic to identify interventions that might simultaneously improve
pedestrian and vehicle safety. The proposed framework should be taken as a
complement to the work of urban planners and public authorities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-only Object Tracking. (arXiv:2110.11679v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11679">
<div class="article-summary-box-inner">
<span><p>Depth (D) indicates occlusion and is less sensitive to illumination changes,
which make depth attractive modality for Visual Object Tracking (VOT). Depth is
used in RGBD object tracking where the best trackers are deep RGB trackers with
additional heuristic using depth maps. There are two potential reasons for the
heuristics: 1) the lack of large RGBD tracking datasets to train deep RGBD
trackers and 2) the long-term evaluation protocol of VOT RGBD that benefits
from heuristics such as depth-based occlusion detection. In this work, we study
how far D-only tracking can go if trained with large amounts of depth data. To
compensate the lack of depth data, we generate depth maps for tracking. We
train a "Depth-DiMP" from the scratch with the generated data and fine-tune it
with the available small RGBD tracking datasets. The depth-only DiMP achieves
good accuracy in depth-only tracking and combined with the original RGB DiMP
the end-to-end trained RGBD-DiMP outperforms the recent VOT 2020 RGBD winners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation. (arXiv:2110.11680v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11680">
<div class="article-summary-box-inner">
<span><p>Several video-based 3D pose and shape estimation algorithms have been
proposed to resolve the temporal inconsistency of single-image-based methods.
However it still remains challenging to have stable and accurate
reconstruction. In this paper, we propose a new framework Deep Two-Stream Video
Inference for Human Body Pose and Shape Estimation (DTS-VIBE), to generate 3D
human pose and mesh from RGB videos. We reformulate the task as a
multi-modality problem that fuses RGB and optical flow for more reliable
estimation. In order to fully utilize both sensory modalities (RGB or optical
flow), we train a two-stream temporal network based on transformer to predict
SMPL parameters. The supplementary modality, optical flow, helps to maintain
temporal consistency by leveraging motion knowledge between two consecutive
frames. The proposed algorithm is extensively evaluated on the Human3.6 and
3DPW datasets. The experimental results show that it outperforms other
state-of-the-art methods by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Variational Autoencoder for Learned Image Reconstruction. (arXiv:2110.11681v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11681">
<div class="article-summary-box-inner">
<span><p>Learned image reconstruction techniques using deep neural networks have
recently gained popularity, and have delivered promising empirical results.
However, most approaches focus on one single recovery for each observation, and
thus neglect the uncertainty information. In this work, we develop a novel
computational framework that approximates the posterior distribution of the
unknown image at each query observation. The proposed framework is very
flexible: It handles implicit noise models and priors, it incorporates the data
formation process (i.e., the forward operator), and the learned reconstructive
properties are transferable between different datasets. Once the network is
trained using the conditional variational autoencoder loss, it provides a
computationally efficient sampler for the approximate posterior distribution
via feed-forward propagation, and the summarizing statistics of the generated
samples are used for both point-estimation and uncertainty quantification. We
illustrate the proposed framework with extensive numerical experiments on
positron emission tomography (with both moderate and low count levels) showing
that the framework generates high-quality samples when compared with
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal-Boost: Multimodal Medical Image Super-Resolution using Multi-Attention Network with Wavelet Transform. (arXiv:2110.11684v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11684">
<div class="article-summary-box-inner">
<span><p>Multimodal medical images are widely used by clinicians and physicians to
analyze and retrieve complementary information from high-resolution images in a
non-invasive manner. The loss of corresponding image resolution degrades the
overall performance of medical image diagnosis. Deep learning based single
image super resolution (SISR) algorithms has revolutionized the overall
diagnosis framework by continually improving the architectural components and
training strategies associated with convolutional neural networks (CNN) on
low-resolution images. However, existing work lacks in two ways: i) the SR
output produced exhibits poor texture details, and often produce blurred edges,
ii) most of the models have been developed for a single modality, hence,
require modification to adapt to a new one. This work addresses (i) by
proposing generative adversarial network (GAN) with deep multi-attention
modules to learn high-frequency information from low-frequency data. Existing
approaches based on the GAN have yielded good SR results; however, the texture
details of their SR output have been experimentally confirmed to be deficient
for medical images particularly. The integration of wavelet transform (WT) and
GANs in our proposed SR model addresses the aforementioned limitation
concerning textons. The WT divides the LR image into multiple frequency bands,
while the transferred GAN utilizes multiple attention and upsample blocks to
predict high-frequency components. Moreover, we present a learning technique
for training a domain-specific classifier as a perceptual loss function.
Combining multi-attention GAN loss with a perceptual loss function results in a
reliable and efficient performance. Applying the same model for medical images
from diverse modalities is challenging, our work addresses (ii) by training and
performing on several modalities via transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Fusion Affinity Graph with Noise-free Online Low-rank Representation for Natural Image Segmentation. (arXiv:2110.11685v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11685">
<div class="article-summary-box-inner">
<span><p>Affinity graph-based segmentation methods have become a major trend in
computer vision. The performance of these methods relies on the constructed
affinity graph, with particular emphasis on the neighborhood topology and
pairwise affinities among superpixels. Due to the advantages of assimilating
different graphs, a multi-scale fusion graph has a better performance than a
single graph with single-scale. However, these methods ignore the noise from
images which influences the accuracy of pairwise similarities. Multi-scale
combinatorial grouping and graph fusion also generate a higher computational
complexity. In this paper, we propose an adaptive fusion affinity graph
(AFA-graph) with noise-free low-rank representation in an online manner for
natural image segmentation. An input image is first over-segmented into
superpixels at different scales and then filtered by the proposed improved
kernel density estimation method. Moreover, we select global nodes of these
superpixels on the basis of their subspace-preserving presentation, which
reveals the feature distribution of superpixels exactly. To reduce time
complexity while improving performance, a sparse representation of global nodes
based on noise-free online low-rank representation is used to obtain a global
graph at each scale. The global graph is finally used to update a local graph
which is built upon all superpixels at each scale. Experimental results on the
BSD300, BSD500, MSRC, SBD, and PASCAL VOC show the effectiveness of AFA-graph
in comparison with state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation. (arXiv:2110.11728v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11728">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) have made a dramatic leap in
high-fidelity image synthesis and stylized face generation. Recently, a
layer-swapping mechanism has been developed to improve the stylization
performance. However, this method is incapable of fitting arbitrary styles in a
single model and requires hundreds of style-consistent training images for each
style. To address the above issues, we propose BlendGAN for arbitrary stylized
face generation by leveraging a flexible blending strategy and a generic
artistic dataset. Specifically, we first train a self-supervised style encoder
on the generic artistic dataset to extract the representations of arbitrary
styles. In addition, a weighted blending module (WBM) is proposed to blend face
and style representations implicitly and control the arbitrary stylization
effect. By doing so, BlendGAN can gracefully fit arbitrary styles in a unified
model while avoiding case-by-case preparation of style-consistent training
images. To this end, we also present a novel large-scale artistic face dataset
AAHQ. Extensive experiments demonstrate that BlendGAN outperforms
state-of-the-art methods in terms of visual quality and style diversity for
both latent-guided and reference-guided stylized face synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UBR$^2$S: Uncertainty-Based Resampling and Reweighting Strategy for Unsupervised Domain Adaptation. (arXiv:2110.11739v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11739">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) deals with the adaptation process of a
model to an unlabeled target domain while annotated data is only available for
a given source domain. This poses a challenging task, as the domain shift
between source and target instances deteriorates a model's performance when not
addressed. In this paper, we propose UBR$^2$S - the Uncertainty-Based
Resampling and Reweighting Strategy - to tackle this problem. UBR$^2$S employs
a Monte Carlo dropout-based uncertainty estimate to obtain per-class
probability distributions, which are then used for dynamic resampling of
pseudo-labels and reweighting based on their sample likelihood and the
accompanying decision error. Our proposed method achieves state-of-the-art
results on multiple UDA datasets with single and multi-source adaptation tasks
and can be applied to any off-the-shelf network architecture. Code for our
method is available at https://gitlab.com/tringwald/UBR2S.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Semantic Segmentation with Self-supervision from Pseudo-classes. (arXiv:2110.11742v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11742">
<div class="article-summary-box-inner">
<span><p>Despite the success of deep learning methods for semantic segmentation,
few-shot semantic segmentation remains a challenging task due to the limited
training data and the generalisation requirement for unseen classes. While
recent progress has been particularly encouraging, we discover that existing
methods tend to have poor performance in terms of meanIoU when query images
contain other semantic classes besides the target class. To address this issue,
we propose a novel self-supervised task that generates random pseudo-classes in
the background of the query images, providing extra training data that would
otherwise be unavailable when predicting individual target classes. To that
end, we adopted superpixel segmentation for generating the pseudo-classes. With
this extra supervision, we improved the meanIoU performance of the
state-of-the-art method by 2.5% and 5.1% on the one-shot tasks, as well as 6.7%
and 4.4% on the five-shot tasks, on the PASCAL-5i and COCO benchmarks,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Creating and Reenacting Controllable 3D Humans with Differentiable Rendering. (arXiv:2110.11746v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11746">
<div class="article-summary-box-inner">
<span><p>This paper proposes a new end-to-end neural rendering architecture to
transfer appearance and reenact human actors. Our method leverages a carefully
designed graph convolutional network (GCN) to model the human body manifold
structure, jointly with differentiable rendering, to synthesize new videos of
people in different contexts from where they were initially recorded. Unlike
recent appearance transferring methods, our approach can reconstruct a fully
controllable 3D texture-mapped model of a person, while taking into account the
manifold structure from body shape and texture appearance in the view
synthesis. Specifically, our approach models mesh deformations with a
three-stage GCN trained in a self-supervised manner on rendered silhouettes of
the human body. It also infers texture appearance with a convolutional network
in the texture domain, which is trained in an adversarial regime to reconstruct
human texture from rendered images of actors in different poses. Experiments on
different videos show that our method successfully infers specific body
deformations and avoid creating texture artifacts while achieving the best
values for appearance in terms of Structural Similarity (SSIM), Learned
Perceptual Image Patch Similarity (LPIPS), Mean Squared Error (MSE), and
Fr\'echet Video Distance (FVD). By taking advantages of both differentiable
rendering and the 3D parametric model, our method is fully controllable, which
allows controlling the human synthesis from both pose and rendering parameters.
The source code is available at
https://www.verlab.dcc.ufmg.br/retargeting-motion/wacv2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Cross-Modal Prediction and Relation Consistency for Semi-Supervised Image Captioning. (arXiv:2110.11767v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11767">
<div class="article-summary-box-inner">
<span><p>The task of image captioning aims to generate captions directly from images
via the automatically learned cross-modal generator. To build a well-performing
generator, existing approaches usually need a large number of described images,
which requires a huge effects on manual labeling. However, in real-world
applications, a more general scenario is that we only have limited amount of
described images and a large number of undescribed images. Therefore, a
resulting challenge is how to effectively combine the undescribed images into
the learning of cross-modal generator. To solve this problem, we propose a
novel image captioning method by exploiting the Cross-modal Prediction and
Relation Consistency (CPRC), which aims to utilize the raw image input to
constrain the generated sentence in the commonly semantic space. In detail,
considering that the heterogeneous gap between modalities always leads to the
supervision difficulty of using the global embedding directly, CPRC turns to
transform both the raw image and corresponding generated sentence into the
shared semantic space, and measure the generated sentence from two aspects: 1)
Prediction consistency. CPRC utilizes the prediction of raw image as soft label
to distill useful supervision for the generated sentence, rather than employing
the traditional pseudo labeling; 2) Relation consistency. CPRC develops a novel
relation consistency between augmented images and corresponding generated
sentences to retain the important relational knowledge. In result, CPRC
supervises the generated sentence from both the informativeness and
representativeness perspectives, and can reasonably use the undescribed images
to learn a more effective generator under the semi-supervised scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation and Active Learning for Fine-Grained Recognition in the Field of Biodiversity. (arXiv:2110.11778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11778">
<div class="article-summary-box-inner">
<span><p>Deep-learning methods offer unsurpassed recognition performance in a wide
range of domains, including fine-grained recognition tasks. However, in most
problem areas there are insufficient annotated training samples. Therefore, the
topic of transfer learning respectively domain adaptation is particularly
important. In this work, we investigate to what extent unsupervised domain
adaptation can be used for fine-grained recognition in a biodiversity context
to learn a real-world classifier based on idealized training data, e.g.
preserved butterflies and plants. Moreover, we investigate the influence of
different normalization layers, such as Group Normalization in combination with
Weight Standardization, on the classifier. We discovered that domain adaptation
works very well for fine-grained recognition and that the normalization methods
have a great influence on the results. Using domain adaptation and Transferable
Normalization, the accuracy of the classifier could be increased by up to 12.35
% compared to the baseline. Furthermore, the domain adaptation system is
combined with an active learning component to improve the results. We compare
different active learning strategies with each other. Surprisingly, we found
that more sophisticated strategies provide better results than the random
selection baseline for only one of the two datasets. In this case, the distance
and diversity strategy performed best. Finally, we present a problem analysis
of the datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwiftLane: Towards Fast and Efficient Lane Detection. (arXiv:2110.11779v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11779">
<div class="article-summary-box-inner">
<span><p>Recent work done on lane detection has been able to detect lanes accurately
in complex scenarios, yet many fail to deliver real-time performance
specifically with limited computational resources. In this work, we propose
SwiftLane: a simple and light-weight, end-to-end deep learning based framework,
coupled with the row-wise classification formulation for fast and efficient
lane detection. This framework is supplemented with a false positive
suppression algorithm and a curve fitting technique to further increase the
accuracy. Our method achieves an inference speed of 411 frames per second,
surpassing state-of-the-art in terms of speed while achieving comparable
results in terms of accuracy on the popular CULane benchmark dataset. In
addition, our proposed framework together with TensorRT optimization
facilitates real-time lane detection on a Nvidia Jetson AGX Xavier as an
embedded system while achieving a high inference speed of 56 frames per second.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Unlearning via Class-Discriminative Pruning. (arXiv:2110.11794v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11794">
<div class="article-summary-box-inner">
<span><p>We explore the problem of selectively forgetting categories from trained CNN
classification models in the federated learning (FL). Given that the data used
for training cannot be accessed globally in FL, our insights probe deep into
the internal influence of each channel. Through the visualization of feature
maps activated by different channels, we observe that different channels have a
varying contribution to different categories in image classification. Inspired
by this, we propose a method for scrubbing the model clean of information about
particular categories. The method does not require retraining from scratch, nor
global access to the data used for training. Instead, we introduce the concept
of Term Frequency Inverse Document Frequency (TF-IDF) to quantize the class
discrimination of channels. Channels with high TF-IDF scores have more
discrimination on the target categories and thus need to be pruned to unlearn.
The channel pruning is followed by a fine-tuning process to recover the
performance of the pruned model. Evaluated on CIFAR10 dataset, our method
accelerates the speed of unlearning by 8.9x for the ResNet model, and 7.9x for
the VGG model under no degradation in accuracy, compared to retraining from
scratch. For CIFAR100 dataset, the speedups are 9.9x and 8.4x, respectively. We
envision this work as a complementary block for FL towards compliance with
legal and ethical criteria.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDRVideo-GAN: Deep Generative HDR Video Reconstruction. (arXiv:2110.11795v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11795">
<div class="article-summary-box-inner">
<span><p>High dynamic range (HDR) videos provide a more visually realistic experience
than the standard low dynamic range (LDR) videos. Despite having significant
progress in HDR imaging, it is still a challenging task to capture high-quality
HDR video with a conventional off-the-shelf camera. Existing approaches rely
entirely on using dense optical flow between the neighboring LDR sequences to
reconstruct an HDR frame. However, they lead to inconsistencies in color and
exposure over time when applied to alternating exposures with noisy frames. In
this paper, we propose an end-to-end GAN-based framework for HDR video
reconstruction from LDR sequences with alternating exposures. We first extract
clean LDR frames from noisy LDR video with alternating exposures with a
denoising network trained in a self-supervised setting. Using optical flow, we
then align the neighboring alternating-exposure frames to a reference frame and
then reconstruct high-quality HDR frames in a complete adversarial setting. To
further improve the robustness and quality of generated frames, we incorporate
temporal stability-based regularization term along with content and style-based
losses in the cost function during the training procedure. Experimental results
demonstrate that our framework achieves state-of-the-art performance and
generates superior quality HDR frames of a video over the existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PropMix: Hard Sample Filtering and Proportional MixUp for Learning with Noisy Labels. (arXiv:2110.11809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11809">
<div class="article-summary-box-inner">
<span><p>The most competitive noisy label learning methods rely on an unsupervised
classification of clean and noisy samples, where samples classified as noisy
are re-labelled and "MixMatched" with the clean samples. These methods have two
issues in large noise rate problems: 1) the noisy set is more likely to contain
hard samples that are in-correctly re-labelled, and 2) the number of samples
produced by MixMatch tends to be reduced because it is constrained by the small
clean set size. In this paper, we introduce the learning algorithm PropMix to
handle the issues above. PropMix filters out hard noisy samples, with the goal
of increasing the likelihood of correctly re-labelling the easy noisy samples.
Also, PropMix places clean and re-labelled easy noisy samples in a training set
that is augmented with MixUp, removing the clean set size constraint and
including a large proportion of correctly re-labelled easy noisy samples. We
also include self-supervised pre-training to improve robustness to high noisy
label scenarios. Our experiments show that PropMix has state-of-the-art (SOTA)
results on CIFAR-10/-100(with symmetric, asymmetric and semantic label noise),
Red Mini-ImageNet (from the Controlled Noisy Web Labels), Clothing1M and
WebVision. In severe label noise bench-marks, our results are substantially
better than other methods. The code is available
athttps://github.com/filipe-research/PropMix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IVS3D: An Open Source Framework for Intelligent Video Sampling and Preprocessing to Facilitate 3D Reconstruction. (arXiv:2110.11810v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11810">
<div class="article-summary-box-inner">
<span><p>The creation of detailed 3D models is relevant for a wide range of
applications such as navigation in three-dimensional space, construction
planning or disaster assessment. However, the complex processing and long
execution time for detailed 3D reconstructions require the original database to
be reduced in order to obtain a result in reasonable time. In this paper we
therefore present our framework iVS3D for intelligent pre-processing of image
sequences. Our software is able to down sample entire videos to a specific
frame rate, as well as to resize and crop the individual images. Furthermore,
thanks to our modular architecture, it is easy to develop and integrate plugins
with additional algorithms. We provide three plugins as baseline methods that
enable an intelligent selection of suitable images and can enrich them with
additional information. To filter out images affected by motion blur, we
developed a plugin that detects these frames and also searches the spatial
neighbourhood for suitable images as replacements. The second plugin uses
optical flow to detect redundant images caused by a temporarily stationary
camera. In our experiments, we show how this approach leads to a more balanced
image sampling if the camera speed varies, and that excluding such redundant
images leads to a time saving of 8.1\percent for our sequences. A third plugin
makes it possible to exclude challenging image regions from the 3D
reconstruction by performing semantic segmentation. As we think that the
community can greatly benefit from such an approach, we will publish our
framework and the developed plugins open source using the MIT licence to allow
co-development and easy extension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN-based Omnidirectional Object Detection for HermesBot Autonomous Delivery Robot with Preliminary Frame Classification. (arXiv:2110.11829v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11829">
<div class="article-summary-box-inner">
<span><p>Mobile autonomous robots include numerous sensors for environment perception.
Cameras are an essential tool for robot's localization, navigation, and
obstacle avoidance. To process a large flow of data from the sensors, it is
necessary to optimize algorithms, or to utilize substantial computational
power. In our work, we propose an algorithm for optimizing a neural network for
object detection using preliminary binary frame classification. An autonomous
outdoor mobile robot with 6 rolling-shutter cameras on the perimeter providing
a 360-degree field of view was used as the experimental setup. The obtained
experimental results revealed that the proposed optimization accelerates the
inference time of the neural network in the cases with up to 5 out of 6 cameras
containing target objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-attribute Pizza Generator: Cross-domain Attribute Control with Conditional StyleGAN. (arXiv:2110.11830v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11830">
<div class="article-summary-box-inner">
<span><p>Multi-attribute conditional image generation is a challenging problem in
computervision. We propose Multi-attribute Pizza Generator (MPG), a conditional
Generative Neural Network (GAN) framework for synthesizing images from a
trichotomy of attributes: content, view-geometry, and implicit visual style. We
design MPG by extending the state-of-the-art StyleGAN2, using a new
conditioning technique that guides the intermediate feature maps to learn
multi-scale multi-attribute entangled representationsof controlling attributes.
Because of the complex nature of the multi-attribute image generation problem,
we regularize the image generation by predicting the explicit conditioning
attributes (ingredients and view). To synthesize a pizza image with view
attributesoutside the range of natural training images, we design a CGI pizza
dataset PizzaView using 3D pizza models and employ it to train a view attribute
regressor to regularize the generation process, bridging the real and CGI
training datasets. To verify the efficacy of MPG, we test it on Pizza10, a
carefully annotated multi-ingredient pizza image dataset. MPG can successfully
generate photo-realistic pizza images with desired ingredients and view
attributes, beyond the range of those observed in real-world training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-view Contrastive Graph Clustering. (arXiv:2110.11842v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11842">
<div class="article-summary-box-inner">
<span><p>With the explosive growth of information technology, multi-view graph data
have become increasingly prevalent and valuable. Most existing multi-view
clustering techniques either focus on the scenario of multiple graphs or
multi-view attributes. In this paper, we propose a generic framework to cluster
multi-view attributed graph data. Specifically, inspired by the success of
contrastive learning, we propose multi-view contrastive graph clustering (MCGC)
method to learn a consensus graph since the original graph could be noisy or
incomplete and is not directly applicable. Our method composes of two key
steps: we first filter out the undesirable high-frequency noise while
preserving the graph geometric features via graph filtering and obtain a smooth
representation of nodes; we then learn a consensus graph regularized by graph
contrastive loss. Results on several benchmark datasets show the superiority of
our method with respect to state-of-the-art approaches. In particular, our
simple approach outperforms existing deep learning-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrence along Depth: Deep Convolutional Neural Networks with Recurrent Layer Aggregation. (arXiv:2110.11852v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11852">
<div class="article-summary-box-inner">
<span><p>This paper introduces a concept of layer aggregation to describe how
information from previous layers can be reused to better extract features at
the current layer. While DenseNet is a typical example of the layer aggregation
mechanism, its redundancy has been commonly criticized in the literature. This
motivates us to propose a very light-weighted module, called recurrent layer
aggregation (RLA), by making use of the sequential structure of layers in a
deep CNN. Our RLA module is compatible with many mainstream deep CNNs,
including ResNets, Xception and MobileNetV2, and its effectiveness is verified
by our extensive experiments on image classification, object detection and
instance segmentation tasks. Specifically, improvements can be uniformly
observed on CIFAR, ImageNet and MS COCO datasets, and the corresponding
RLA-Nets can surprisingly boost the performances by 2-3% on the object
detection task. This evidences the power of our RLA module in helping main CNNs
better learn structural information in images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AIR-Nets: An Attention-Based Framework for Locally Conditioned Implicit Representations. (arXiv:2110.11860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11860">
<div class="article-summary-box-inner">
<span><p>This paper introduces Attentive Implicit Representation Networks (AIR-Nets),
a simple, but highly effective architecture for 3D reconstruction from point
clouds. Since representing 3D shapes in a local and modular fashion increases
generalization and reconstruction quality, AIR-Nets encode an input point cloud
into a set of local latent vectors anchored in 3D space, which locally describe
the object's geometry, as well as a global latent description, enforcing global
consistency. Our model is the first grid-free, encoder-based approach that
locally describes an implicit function. The vector attention mechanism from
[Zhao et al. 2020] serves as main point cloud processing module, and allows for
permutation invariance and translation equivariance. When queried with a 3D
coordinate, our decoder gathers information from the global and nearby local
latent vectors in order to predict an occupancy value. Experiments on the
ShapeNet dataset show that AIR-Nets significantly outperform previous
state-of-the-art encoder-based, implicit shape learning methods and especially
dominate in the sparse setting. Furthermore, our model generalizes well to the
FAUST dataset in a zero-shot setting. Finally, since AIR-Nets use a sparse
latent representation and follow a simple operating scheme, the model offers
several exiting avenues for future work. Our code is available at
https://github.com/SimonGiebenhain/AIR-Nets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning-based NLP Data Pipeline for EHR Scanned Document Information Extraction. (arXiv:2110.11864v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11864">
<div class="article-summary-box-inner">
<span><p>Scanned documents in electronic health records (EHR) have been a challenge
for decades, and are expected to stay in the foreseeable future. Current
approaches for processing often include image preprocessing, optical character
recognition (OCR), and text mining. However, there is limited work that
evaluates the choice of image preprocessing methods, the selection of NLP
models, and the role of document layout. The impact of each element remains
unknown. We evaluated this method on a use case of two key indicators for sleep
apnea, Apnea hypopnea index (AHI) and oxygen saturation (SaO2) values, from
scanned sleep study reports. Our data that included 955 manually annotated
reports was secondarily utilized from a previous study in the University of
Texas Medical Branch. We performed image preprocessing: gray-scaling followed
by 1 iteration of dilating and erode, and 20% contrast increasing. The OCR was
implemented with the Tesseract OCR engine. A total of seven Bag-of-Words models
(Logistic Regression, Ridge Regression, Lasso Regression, Support Vector
Machine, k-Nearest Neighbor, Na\"ive Bayes, and Random Forest) and three deep
learning-based models (BiLSTM, BERT, and Clinical BERT) were evaluated. We also
evaluated the combinations of image preprocessing methods (gray-scaling, dilate
&amp; erode, increased contrast by 20%, increased contrast by 60%), and two deep
learning architectures (with and without structured input that provides
document layout information). Our proposed method using Clinical BERT reached
an AUROC of 0.9743 and document accuracy of 94.76% for AHI, and an AUROC of
0.9523, and document accuracy of 91.61% for SaO2. We demonstrated the proper
use of image preprocessing and document layout could be beneficial to scanned
document processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CeyMo: See More on Roads -- A Novel Benchmark Dataset for Road Marking Detection. (arXiv:2110.11867v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11867">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel road marking benchmark dataset for road
marking detection, addressing the limitations in the existing publicly
available datasets such as lack of challenging scenarios, prominence given to
lane markings, unavailability of an evaluation script, lack of annotation
formats and lower resolutions. Our dataset consists of 2887 total images with
4706 road marking instances belonging to 11 classes. The images have a high
resolution of 1920 x 1080 and capture a wide range of traffic, lighting and
weather conditions. We provide road marking annotations in polygons, bounding
boxes and pixel-level segmentation masks to facilitate a diverse range of road
marking detection algorithms. The evaluation metrics and the evaluation script
we provide, will further promote direct comparison of novel approaches for road
marking detection with existing methods. Furthermore, we evaluate the
effectiveness of using both instance segmentation and object detection based
approaches for the road marking detection task. Speed and accuracy scores for
two instance segmentation models and two object detector models are provided as
a performance baseline for our benchmark dataset. The dataset and the
evaluation script will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Dialogue System with AUDITED. (arXiv:2110.11881v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11881">
<div class="article-summary-box-inner">
<span><p>We devise a multimodal conversation system for dialogue utterances composed
of text, image or both modalities. We leverage Auxiliary UnsuperviseD vIsual
and TExtual Data (AUDITED). To improve the performance of text-based task, we
utilize translations of target sentences from English to French to form the
assisted supervision. For the image-based task, we employ the DeepFashion
dataset in which we seek nearest neighbor images of positive and negative
target images of the MMD data. These nearest neighbors form the nearest
neighbor embedding providing an external context for target images. We form two
methods to create neighbor embedding vectors, namely Neighbor Embedding by Hard
Assignment (NEHA) and Neighbor Embedding by Soft Assignment (NESA) which
generate context subspaces per target image. Subsequently, these subspaces are
learnt by our pipeline as a context for the target data. We also propose a
discriminator which switches between the image- and text-based tasks. We show
improvements over baselines on the large-scale Multimodal Dialogue Dataset
(MMD) and SIMMC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C$^{4}$Net: Contextual Compression and Complementary Combination Network for Salient Object Detection. (arXiv:2110.11887v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11887">
<div class="article-summary-box-inner">
<span><p>Deep learning solutions of the salient object detection problem have achieved
great results in recent years. The majority of these models are based on
encoders and decoders, with a different multi-feature combination. In this
paper, we show that feature concatenation works better than other combination
methods like multiplication or addition. Also, joint feature learning gives
better results, because of the information sharing during their processing. We
designed a Complementary Extraction Module (CEM) to extract necessary features
with edge preservation. Our proposed Excessiveness Loss (EL) function helps to
reduce false-positive predictions and purifies the edges with other weighted
loss functions. Our designed Pyramid-Semantic Module (PSM) with Global guiding
flow (G) makes the prediction more accurate by providing high-level
complementary information to shallower layers. Experimental results show that
the proposed model outperforms the state-of-the-art methods on all benchmark
datasets under three evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Using Clothes Style Transfer for Scenario-aware Person Video Generation. (arXiv:2110.11894v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11894">
<div class="article-summary-box-inner">
<span><p>Clothes style transfer for person video generation is a challenging task, due
to drastic variations of intra-person appearance and video scenarios. To tackle
this problem, most recent AdaIN-based architectures are proposed to extract
clothes and scenario features for generation. However, these approaches suffer
from being short of fine-grained details and are prone to distort the origin
person. To further improve the generation performance, we propose a novel
framework with disentangled multi-branch encoders and a shared decoder.
Moreover, to pursue the strong video spatio-temporal consistency, an
inner-frame discriminator is delicately designed with input being cross-frame
difference. Besides, the proposed framework possesses the property of scenario
adaptation. Extensive experiments on the TEDXPeople benchmark demonstrate the
superiority of our method over state-of-the-art approaches in terms of image
quality and video coherence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges in Procedural Multimodal Machine Comprehension:A Novel Way To Benchmark. (arXiv:2110.11899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11899">
<div class="article-summary-box-inner">
<span><p>We focus on Multimodal Machine Reading Comprehension (M3C) where a model is
expected to answer questions based on given passage (or context), and the
context and the questions can be in different modalities. Previous works such
as RecipeQA have proposed datasets and cloze-style tasks for evaluation.
However, we identify three critical biases stemming from the question-answer
generation process and memorization capabilities of large deep models. These
biases makes it easier for a model to overfit by relying on spurious
correlations or naive data patterns. We propose a systematic framework to
address these biases through three Control-Knobs that enable us to generate a
test bed of datasets of progressive difficulty levels. We believe that our
benchmark (referred to as Meta-RecipeQA) will provide, for the first time, a
fine grained estimate of a model's generalization capabilities. We also propose
a general M3C model that is used to realize several prior SOTA models and
motivate a novel hierarchical transformer based reasoning network (HTRN). We
perform a detailed evaluation of these models with different language and
visual features on our benchmark. We observe a consistent improvement with HTRN
over SOTA (~18% in Visual Cloze task and ~13% in average over all the tasks).
We also observe a drop in performance across all the models when testing on
RecipeQA and proposed Meta-RecipeQA (e.g. 83.6% versus 67.1% for HTRN), which
shows that the proposed dataset is relatively less biased. We conclude by
highlighting the impact of the control knobs with some quantitative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised denoising for massive noisy images. (arXiv:2110.11911v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11911">
<div class="article-summary-box-inner">
<span><p>We propose an effective deep learning model for signal reconstruction, which
requires no signal prior, no noise model calibration, and no clean samples.
This model only assumes that the noise is independent of the measurement and
that the true signals share the same structured information. We demonstrate its
performance on a variety of real-world applications, from sub-\r{A}ngstr\"{o}m
resolution atomic images to sub-arcsecond resolution astronomy images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIGS: Meta Image Generation from Scene Graphs. (arXiv:2110.11918v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11918">
<div class="article-summary-box-inner">
<span><p>Generation of images from scene graphs is a promising direction towards
explicit scene generation and manipulation. However, the images generated from
the scene graphs lack quality, which in part comes due to high difficulty and
diversity in the data. We propose MIGS (Meta Image Generation from Scene
Graphs), a meta-learning based approach for few-shot image generation from
graphs that enables adapting the model to different scenes and increases the
image quality by training on diverse sets of tasks. By sampling the data in a
task-driven fashion, we train the generator using meta-learning on different
sets of tasks that are categorized based on the scene attributes. Our results
show that using this meta-learning approach for the generation of images from
scene graphs achieves state-of-the-art performance in terms of image quality
and capturing the semantic relationships in the scene. Project Website:
https://migs2021.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logical Activation Functions: Logit-space equivalents of Boolean Operators. (arXiv:2110.11940v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11940">
<div class="article-summary-box-inner">
<span><p>Neuronal representations within artificial neural networks are commonly
understood as logits, representing the log-odds score of presence (versus
absence) of features within the stimulus. Under this interpretation, we can
derive the probability $P(x_0 \land x_1)$ that a pair of independent features
are both present in the stimulus from their logits. By converting the resulting
probability back into a logit, we obtain a logit-space equivalent of the AND
operation. However, since this function involves taking multiple exponents and
logarithms, it is not well suited to be directly used within neural networks.
We thus constructed an efficient approximation named $\text{AND}_\text{AIL}$
(the AND operator Approximate for Independent Logits) utilizing only comparison
and addition operations, which can be deployed as an activation function in
neural networks. Like MaxOut, $\text{AND}_\text{AIL}$ is a generalization of
ReLU to two-dimensions. Additionally, we constructed efficient approximations
of the logit-space equivalents to the OR and XNOR operators. We deployed these
new activation functions, both in isolation and in conjunction, and
demonstrated their effectiveness on a variety of tasks including image
classification, transfer learning, abstract reasoning, and compositional
zero-shot learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOFT: Softmax-free Transformer with Linear Complexity. (arXiv:2110.11945v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11945">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have pushed the state-of-the-art for various
visual recognition tasks by patch-wise image tokenization followed by
self-attention. However, the employment of self-attention modules results in a
quadratic complexity in both computation and memory usage. Various attempts on
approximating the self-attention computation with linear complexity have been
made in Natural Language Processing. However, an in-depth analysis in this work
shows that they are either theoretically flawed or empirically ineffective for
visual recognition. We further identify that their limitations are rooted in
keeping the softmax self-attention during approximations. Specifically,
conventional self-attention is computed by normalizing the scaled dot-product
between token feature vectors. Keeping this softmax operation challenges any
subsequent linearization efforts. Based on this insight, for the first time, a
softmax-free transformer or SOFT is proposed. To remove softmax in
self-attention, Gaussian kernel function is used to replace the dot-product
similarity without further normalization. This enables a full self-attention
matrix to be approximated via a low-rank matrix decomposition. The robustness
of the approximation is achieved by calculating its Moore-Penrose inverse using
a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT
significantly improves the computational efficiency of existing ViT variants.
Crucially, with a linear complexity, much longer token sequences are permitted
in SOFT, resulting in superior trade-off between accuracy and complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Proposals for Practical Energy-Based Regression. (arXiv:2110.11948v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11948">
<div class="article-summary-box-inner">
<span><p>Energy-based models (EBMs) have experienced a resurgence within machine
learning in recent years, including as a promising alternative for
probabilistic regression. However, energy-based regression requires a proposal
distribution to be manually designed for training, and an initial estimate has
to be provided at test-time. We address both of these issues by introducing a
conceptually simple method to automatically learn an effective proposal
distribution, which is parameterized by a separate network head. To this end,
we derive a surprising result, leading to a unified training objective that
jointly minimizes the KL divergence from the proposal to the EBM, and the
negative log-likelihood of the EBM. At test-time, we can then employ importance
sampling with the trained proposal to efficiently evaluate the learned EBM and
produce stand-alone predictions. Furthermore, we utilize our derived training
objective to learn mixture density networks (MDNs) with a jointly trained
energy-based teacher, consistently outperforming conventional MDN training on
four real-world regression tasks within computer vision. Code is available at
https://github.com/fregu856/ebms_proposals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Approach Coloured Object Tracker with Adaptive Model and Bandwidth using Mean Shift Algorithm. (arXiv:1207.2602v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1207.2602">
<div class="article-summary-box-inner">
<span><p>The traditional color-based mean-shift tracking algorithm is popular among
tracking methods due to its simple and efficient procedure, however, the lack
of dynamism in its target model makes it unsuitable for tracking objects which
have changes in their sizes and shapes. In this paper, we propose a fast novel
threephase colored object tracker algorithm based on mean shift idea while
utilizing adaptive model. The proposed method can improve the mentioned
weaknesses of the original mean-shift algorithm. The experimental results show
that the new method is feasible, robust and has acceptable speed in comparison
with other algorithms.15 page,
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Large RGB-D Dataset for Semi-supervised Monocular Depth Estimation. (arXiv:1904.10230v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.10230">
<div class="article-summary-box-inner">
<span><p>Current self-supervised methods for monocular depth estimation are largely
based on deeply nested convolutional networks that leverage stereo image pairs
or monocular sequences during a training phase. However, they often exhibit
inaccurate results around occluded regions and depth boundaries. In this paper,
we present a simple yet effective approach for monocular depth estimation using
stereo image pairs. The study aims to propose a student-teacher strategy in
which a shallow student network is trained with the auxiliary information
obtained from a deeper and more accurate teacher network. Specifically, we
first train the stereo teacher network by fully utilizing the binocular
perception of 3-D geometry and then use the depth predictions of the teacher
network to train the student network for monocular depth inference. This
enables us to exploit all available depth data from massive unlabeled stereo
pairs. We propose a strategy that involves the use of a data ensemble to merge
the multiple depth predictions of the teacher network to improve the training
samples by collecting non-trivial knowledge beyond a single prediction. To
refine the inaccurate depth estimation that is used when training the student
network, we further propose stereo confidence-guided regression loss that
handles the unreliable pseudo depth values in occlusion, texture-less region,
and repetitive pattern. To complement the existing dataset comprising outdoor
driving scenes, we built a novel large-scale dataset consisting of one million
outdoor stereo images taken using hand-held stereo cameras. Finally, we
demonstrate that the monocular depth estimation network provides feature
representations that are suitable for high-level vision tasks. The experimental
results for various outdoor scenarios demonstrate the effectiveness and
flexibility of our approach, which outperforms state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Region-based Randers Geodesic Approach for Image Segmentation. (arXiv:1912.10122v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.10122">
<div class="article-summary-box-inner">
<span><p>The minimal path model based on the Eikonal partial differential equation has
served as a fundamental tool for the applications of image segmentation and
boundary detection in the passed two decades. However, the existing approaches
commonly only exploit the image edge-based features for computing minimal
paths, potentially limiting their performance in complicated segmentation
situations. In this paper, we introduce a new variational image segmentation
model based on the minimal path framework and the eikonal PDE, where the
region-based appearance term that defines then regional homogeneity features
can be taken into account for estimating the associated minimal paths. This is
done by constructing a Randers geodesic metric interpretation to the
region-based active contour energy. As a result, the minimization of the active
contour energy is transformed to finding the solution to the Randers eikonal
PDE.
</p>
<p>We also suggest a practical interactive image segmentation strategy, where
the target boundary can be delineated by the concatenation of the piecewise
geodesic paths. We invoke the Finsler variant of the fast marching method to
estimate the geodesic distance map, yielding an efficient implementation of the
proposed Eikonal region-based active contour model. Experimental results on
both synthetic and real images exhibit that our model indeed achieves
encouraging segmentation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation. (arXiv:2004.03686v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.03686">
<div class="article-summary-box-inner">
<span><p>Differently from 2D image datasets such as COCO, large-scale human datasets
with 3D ground-truth annotations are very difficult to obtain in the wild. In
this paper, we address this problem by augmenting existing 2D datasets with
high-quality 3D pose fits. Remarkably, the resulting annotations are sufficient
to train from scratch 3D pose regressor networks that outperform the current
state-of-the-art on in-the-wild benchmarks such as 3DPW. Additionally, training
on our augmented data is straightforward as it does not require to mix multiple
and incompatible 2D and 3D datasets or to use complicated network architectures
and training procedures. This simplified pipeline affords additional
improvements, including injecting extreme crop augmentations to better
reconstruct highly truncated people, and incorporating auxiliary inputs to
improve 3D pose estimation accuracy. It also reduces the dependency on 3D
datasets such as H36M that have restrictive licenses. We also use our method to
introduce new benchmarks for the study of real-world challenges such as
occlusions, truncations, and rare body poses. In order to obtain such high
quality 3D pseudo-annotations, inspired by progress in internal learning, we
introduce Exemplar Fine-Tuning (EFT). EFT combines the re-projection accuracy
of fitting methods like SMPLify with a 3D pose prior implicitly captured by a
pre-trained 3D pose regressor network. We show that EFT produces 3D annotations
that result in better downstream performance and are qualitatively preferable
in an extensive human-based assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IDDA: a large-scale multi-domain dataset for autonomous driving. (arXiv:2004.08298v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.08298">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is key in autonomous driving. Using deep visual
learning architectures is not trivial in this context, because of the
challenges in creating suitable large scale annotated datasets. This issue has
been traditionally circumvented through the use of synthetic datasets, that
have become a popular resource in this field. They have been released with the
need to develop semantic segmentation algorithms able to close the visual
domain shift between the training and test data. Although exacerbated by the
use of artificial data, the problem is extremely relevant in this field even
when training on real data. Indeed, weather conditions, viewpoint changes and
variations in the city appearances can vary considerably from car to car, and
even at test time for a single, specific vehicle. How to deal with domain
adaptation in semantic segmentation, and how to leverage effectively several
different data distributions (source domains) are important research questions
in this field. To support work in this direction, this paper contributes a new
large scale, synthetic dataset for semantic segmentation with more than 100
different source visual domains. The dataset has been created to explicitly
address the challenges of domain shift between training and test data in
various weather and view point conditions, in seven different city types.
Extensive benchmark experiments assess the dataset, showcasing open challenges
for the current state of the art. The dataset will be available at:
https://idda-dataset.github.io/home/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Retinex based GAN Pipeline to Utilize Paired and Unpaired Datasets for Enhancing Low Light Images. (arXiv:2006.15304v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.15304">
<div class="article-summary-box-inner">
<span><p>Low light image enhancement is an important challenge for the development of
robust computer vision algorithms. The machine learning approaches to this have
been either unsupervised, supervised based on paired dataset or supervised
based on unpaired dataset. This paper presents a novel deep learning pipeline
that can learn from both paired and unpaired datasets. Convolution Neural
Networks (CNNs) that are optimized to minimize standard loss, and Generative
Adversarial Networks (GANs) that are optimized to minimize the adversarial loss
are used to achieve different steps of the low light image enhancement process.
Cycle consistency loss and a patched discriminator are utilized to further
improve the performance. The paper also analyses the functionality and the
performance of different components, hidden layers, and the entire pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Future Urban Scenes Generation Through Vehicles Synthesis. (arXiv:2007.00323v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00323">
<div class="article-summary-box-inner">
<span><p>In this work we propose a deep learning pipeline to predict the visual future
appearance of an urban scene. Despite recent advances, generating the entire
scene in an end-to-end fashion is still far from being achieved. Instead, here
we follow a two stages approach, where interpretable information is included in
the loop and each actor is modelled independently. We leverage a per-object
novel view synthesis paradigm; i.e. generating a synthetic representation of an
object undergoing a geometrical roto-translation in the 3D space. Our model can
be easily conditioned with constraints (e.g. input trajectories) provided by
state-of-the-art tracking methods or by the user itself. This allows us to
generate a set of diverse realistic futures starting from the same input in a
multi-modal fashion. We visually and quantitatively show the superiority of
this approach over traditional end-to-end scene-generation methods on CityFlow,
a challenging real world dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shedding Light on Blind Spots: Developing a Reference Architecture to Leverage Video Data for Process Mining. (arXiv:2010.11289v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11289">
<div class="article-summary-box-inner">
<span><p>Process mining is one of the most active research streams in business process
management. In recent years, numerous methods have been proposed for analyzing
structured process data. Yet, in many cases, it is only the digitized parts of
processes that are directly captured from process-aware information systems,
and manual activities often result in blind spots. While the use of video
cameras to observe these activities could help to fill this gap, a standardized
approach to extracting event logs from unstructured video data remains lacking.
Here, we propose a reference architecture to bridge the gap between computer
vision and process mining. Various evaluation activities (i.e., competing
artifact analysis, prototyping, and real-world application) ensured that the
proposed reference architecture allows flexible, use-case-driven, and
context-specific instantiations. Our results also show that an exemplary
software prototype instantiation of the proposed reference architecture is
capable of automatically extracting most of the process-relevant events from
unstructured video data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Motion Blind Video Stabilization. (arXiv:2011.09697v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09697">
<div class="article-summary-box-inner">
<span><p>Despite the advances in the field of generative models in computer vision,
video stabilization still lacks a pure regressive deep-learning-based
formulation. Deep video stabilization is generally formulated with the help of
explicit motion estimation modules due to the lack of a dataset containing
pairs of videos with similar perspective but different motion. Therefore, the
deep learning approaches for this task have difficulties in the pixel-level
synthesis of latent stabilized frames, and resort to motion estimation modules
for indirect transformations of the unstable frames to stabilized frames,
leading to the loss of visual content near the frame boundaries. In this work,
we aim to declutter this over-complicated formulation of video stabilization
with the help of a novel dataset that contains pairs of training videos with
similar perspective but different motion, and verify its effectiveness by
successfully learning motion blind full-frame video stabilization through
employing strictly conventional generative techniques and further improve the
stability through a curriculum-learning inspired adversarial training strategy.
Through extensive experimentation, we show the quantitative and qualitative
advantages of the proposed approach to the state-of-the-art video stabilization
approaches. Moreover, our method achieves $\sim3\times$ speed-up over the
currently available fastest video stabilization methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DomainMix: Learning Generalizable Person Re-Identification Without Human Annotations. (arXiv:2011.11953v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11953">
<div class="article-summary-box-inner">
<span><p>Existing person re-identification models often have low generalizability,
which is mostly due to limited availability of large-scale labeled data in
training. However, labeling large-scale training data is very expensive and
time-consuming, while large-scale synthetic dataset shows promising value in
learning generalizable person re-identification models. Therefore, in this
paper a novel and practical person re-identification task is proposed,i.e. how
to use labeled synthetic dataset and unlabeled real-world dataset to train a
universal model. In this way, human annotations are no longer required, and it
is scalable to large and diverse real-world datasets. To address the task, we
introduce a framework with high generalizability, namely DomainMix.
Specifically, the proposed method firstly clusters the unlabeled real-world
images and selects the reliable clusters. During training, to address the large
domain gap between two domains, a domain-invariant feature learning method is
proposed, which introduces a new loss,i.e. domain balance loss, to conduct an
adversarial learning between domain-invariant feature learning and domain
discrimination, and meanwhile learns a discriminative feature for person
re-identification. This way, the domain gap between synthetic and real-world
data is much reduced, and the learned feature is generalizable thanks to the
large-scale and diverse training data. Experimental results show that the
proposed annotation-free method is more or less comparable to the counterpart
trained with full human annotations, which is quite promising. In addition, it
achieves the current state of the art on several person re-identification
datasets under direct cross-dataset evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching Distributions via Optimal Transport for Semi-Supervised Learning. (arXiv:2012.03790v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03790">
<div class="article-summary-box-inner">
<span><p>Semi-Supervised Learning (SSL) approaches have been an influential framework
for the usage of unlabeled data when there is not a sufficient amount of
labeled data available over the course of training. SSL methods based on
Convolutional Neural Networks (CNNs) have recently provided successful results
on standard benchmark tasks such as image classification. In this work, we
consider the general setting of SSL problem where the labeled and unlabeled
data come from the same underlying probability distribution. We propose a new
approach that adopts an Optimal Transport (OT) technique serving as a metric of
similarity between discrete empirical probability measures to provide
pseudo-labels for the unlabeled data, which can then be used in conjunction
with the initial labeled data to train the CNN model in an SSL manner. We have
evaluated and compared our proposed method with state-of-the-art SSL algorithms
on standard datasets to demonstrate the superiority and effectiveness of our
SSL algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding. (arXiv:2101.01881v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.01881">
<div class="article-summary-box-inner">
<span><p>To reduce a model size but retain performance, we often rely on knowledge
distillation (KD) which transfers knowledge from a large "teacher" model to a
smaller "student" model. However, KD on multimodal datasets such as
vision-language tasks is relatively unexplored, and digesting multimodal
information is challenging since different modalities present different types
of information. In this paper, we perform a large-scale empirical study to
investigate the importance and effects of each modality in knowledge
distillation. Furthermore, we introduce a multimodal knowledge distillation
framework, modality-specific distillation (MSD), to transfer knowledge from a
teacher on multimodal tasks by learning the teacher's behavior within each
modality. The idea aims at mimicking a teacher's modality-specific predictions
by introducing auxiliary loss terms for each modality. Furthermore, because
each modality has different saliency for predictions, we define saliency scores
for each modality and investigate saliency-based weighting schemes for the
auxiliary losses. We further study a weight learning approach to learn the
optimal weights on these loss terms. In our empirical analysis, we examine the
saliency of each modality in KD, demonstrate the effectiveness of the weighting
scheme in MSD, and show that it achieves better performance than KD on four
multimodal datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding and Achieving Efficient Robustness with Adversarial Supervised Contrastive Learning. (arXiv:2101.10027v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10027">
<div class="article-summary-box-inner">
<span><p>Contrastive learning (CL) has recently emerged as an effective approach to
learning representation in a range of downstream tasks. Central to this
approach is the selection of positive (similar) and negative (dissimilar) sets
to provide the model the opportunity to `contrast' between data and class
representation in the latent space. In this paper, we investigate CL for
improving model robustness using adversarial samples. We first designed and
performed a comprehensive study to understand how adversarial vulnerability
behaves in the latent space. Based on this empirical evidence, we propose an
effective and efficient supervised contrastive learning to achieve model
robustness against adversarial attacks. Moreover, we propose a new sample
selection strategy that optimizes the positive/negative sets by removing
redundancy and improving correlation with the anchor. Extensive experiments
show that our Adversarial Supervised Contrastive Learning (ASCL) approach
achieves comparable performance with the state-of-the-art defenses while
significantly outperforms other CL-based defense methods by using only $42.8\%$
positives and $6.3\%$ negatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Uncertainty Estimation of Learned Variational MRI Reconstruction. (arXiv:2102.06665v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06665">
<div class="article-summary-box-inner">
<span><p>Recent deep learning approaches focus on improving quantitative scores of
dedicated benchmarks, and therefore only reduce the observation-related
(aleatoric) uncertainty. However, the model-immanent (epistemic) uncertainty is
less frequently systematically analyzed. In this work, we introduce a Bayesian
variational framework to quantify the epistemic uncertainty. To this end, we
solve the linear inverse problem of undersampled MRI reconstruction in a
variational setting. The associated energy functional is composed of a data
fidelity term and the total deep variation (TDV) as a learned parametric
regularizer. To estimate the epistemic uncertainty we draw the parameters of
the TDV regularizer from a multivariate Gaussian distribution, whose mean and
covariance matrix are learned in a stochastic optimal control problem. In
several numerical experiments, we demonstrate that our approach yields
competitive results for undersampled MRI reconstruction. Moreover, we can
accurately quantify the pixelwise epistemic uncertainty, which can serve
radiologists as an additional resource to visualize reconstruction reliability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Branch Architecture Search for Unsupervised Domain Adaptation. (arXiv:2102.06679v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06679">
<div class="article-summary-box-inner">
<span><p>Unsupervised Domain Adaptation (UDA) is a key issue in visual recognition, as
it allows to bridge different visual domains enabling robust performances in
the real world. To date, all proposed approaches rely on human expertise to
manually adapt a given UDA method (e.g. DANN) to a specific backbone
architecture (e.g. ResNet). This dependency on handcrafted designs limits the
applicability of a given approach in time, as old methods need to be constantly
adapted to novel backbones.
</p>
<p>Existing Neural Architecture Search (NAS) approaches cannot be directly
applied to mitigate this issue, as they rely on labels that are not available
in the UDA setting. Furthermore, most NAS methods search for full
architectures, which precludes the use of pre-trained models, essential in a
vast range of UDA settings for reaching SOTA results. To the best of our
knowledge, no prior work has addressed these aspects in the context of NAS for
UDA. Here we tackle both aspects with an Adversarial Branch Architecture Search
for UDA (ABAS): i. we address the lack of target labels by a novel data-driven
ensemble approach for model selection; and ii. we search for an auxiliary
adversarial branch, attached to a pre-trained backbone, which drives the domain
alignment.
</p>
<p>We extensively validate ABAS to improve two modern UDA techniques, DANN and
ALDA, on three standard visual recognition datasets (Office31, Office-Home and
PACS). In all cases, ABAS robustly finds the adversarial branch architectures
and parameters which yield best performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HandTailor: Towards High-Precision Monocular 3D Hand Recovery. (arXiv:2102.09244v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09244">
<div class="article-summary-box-inner">
<span><p>3D hand pose estimation and shape recovery are challenging tasks in computer
vision. We introduce a novel framework HandTailor, which combines a
learning-based hand module and an optimization-based tailor module to achieve
high-precision hand mesh recovery from a monocular RGB image. The proposed hand
module unifies perspective projection and weak perspective projection in a
single network towards accuracy-oriented and in-the-wild scenarios. The
proposed tailor module then utilizes the coarsely reconstructed mesh model
provided by the hand module as initialization, and iteratively optimizes an
energy function to obtain better results. The tailor module is time-efficient,
costs only 8ms per frame on a modern CPU. We demonstrate that HandTailor can
get state-of-the-art performance on several public benchmarks, with impressive
qualitative results on in-the-wild experiments. Code and video are available on
our project webpage https://sites.google.com/view/handtailor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Training Enhances Online Continual Learning. (arXiv:2103.14010v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14010">
<div class="article-summary-box-inner">
<span><p>In continual learning, a system must incrementally learn from a
non-stationary data stream without catastrophic forgetting. Recently, multiple
methods have been devised for incrementally learning classes on large-scale
image classification tasks, such as ImageNet. State-of-the-art continual
learning methods use an initial supervised pre-training phase, in which the
first 10% - 50% of the classes in a dataset are used to learn representations
in an offline manner before continual learning of new classes begins. We
hypothesize that self-supervised pre-training could yield features that
generalize better than supervised learning, especially when the number of
samples used for pre-training is small. We test this hypothesis using the
self-supervised MoCo-V2, Barlow Twins, and SwAV algorithms. On ImageNet, we
find that these methods outperform supervised pre-training considerably for
online continual learning, and the gains are larger when fewer samples are
available. Our findings are consistent across three online continual learning
algorithms. Our best system achieves a 14.95% relative increase in top-1
accuracy on class incremental ImageNet over the prior state of the art for
online continual learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Uncertainty and Expected Gradient Length -- Regression: Two Sides Of The Same Coin?. (arXiv:2104.09493v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09493">
<div class="article-summary-box-inner">
<span><p>Active learning algorithms select a subset of data for annotation to maximize
the model performance on a budget. One such algorithm is Expected Gradient
Length, which as the name suggests uses the approximate gradient induced per
example in the sampling process. While Expected Gradient Length has been
successfully used for classification and regression, the formulation for
regression remains intuitively driven. Hence, our theoretical contribution
involves deriving this formulation, thereby supporting the experimental
evidence. Subsequently, we show that expected gradient length in regression is
equivalent to Bayesian uncertainty. If certain assumptions are infeasible, our
algorithmic contribution (EGL++) approximates the effect of ensembles with a
single deterministic network. Instead of computing multiple possible inferences
per input, we leverage previously annotated samples to quantify the probability
of previous labels being the true label. Such an approach allows us to extend
expected gradient length to a new task: human pose estimation. We perform
experimental validation on two human pose datasets (MPII and LSP/LSPET),
highlighting the interpretability and competitiveness of EGL++ with different
active learning algorithms for human pose estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation. (arXiv:2105.04447v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04447">
<div class="article-summary-box-inner">
<span><p>We propose a novel scene flow estimation approach to capture and infer 3D
motions from point clouds. Estimating 3D motions for point clouds is
challenging, since a point cloud is unordered and its density is significantly
non-uniform. Such unstructured data poses difficulties in matching
corresponding points between point clouds, leading to inaccurate flow
estimation. We propose a novel architecture named Sparse
Convolution-Transformer Network (SCTN) that equips the sparse convolution with
the transformer. Specifically, by leveraging the sparse convolution, SCTN
transfers irregular point cloud into locally consistent flow features for
estimating continuous and consistent motions within an object/local object
part. We further propose to explicitly learn point relations using a point
transformer module, different from exiting methods. We show that the learned
relation-based contextual information is rich and helpful for matching
corresponding points, benefiting scene flow estimation. In addition, a novel
loss function is proposed to adaptively encourage flow consistency according to
feature similarity. Extensive experiments demonstrate that our proposed
approach achieves a new state of the art in scene flow estimation. Our approach
achieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene
Flow respectively, which significantly outperforms previous methods by large
margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DFENet: A Novel Dimension Fusion Edge Guided Network for Brain MRI Segmentation. (arXiv:2105.07962v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07962">
<div class="article-summary-box-inner">
<span><p>The rapid increment of morbidity of brain stroke in the last few years have
been a driving force towards fast and accurate segmentation of stroke lesions
from brain MRI images. With the recent development of deep-learning,
computer-aided and segmentation methods of ischemic stroke lesions have been
useful for clinicians in early diagnosis and treatment planning. However, most
of these methods suffer from inaccurate and unreliable segmentation results
because of their inability to capture sufficient contextual features from the
MRI volumes. To meet these requirements, 3D convolutional neural networks have
been proposed, which, however, suffer from huge computational requirements. To
mitigate these problems, we propose a novel Dimension Fusion Edge-guided
network (DFENet) that can meet both of these requirements by fusing the
features of 2D and 3D CNNs. Unlike other methods, our proposed network uses a
parallel partial decoder (PPD) module for aggregating and upsampling selected
features, rich in important contextual information. Additionally, we use an
edge-guidance and enhanced mixing loss for constantly supervising and
improvising the learning process of the network. The proposed method is
evaluated on publicly available Anatomical Tracings of Lesions After Stroke
(ATLAS) dataset, resulting in mean DSC, IoU, Precision and Recall values of
0.5457, 0.4015, 0.6371, and 0.4969 respectively. The results, when compared to
other state-of-the-art methods, outperforms them by a significant margin.
Therefore, the proposed model is robust, accurate, superior to the existing
methods, and can be relied upon for biomedical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhance to Read Better: A Multi-Task Adversarial Network for Handwritten Document Image Enhancement. (arXiv:2105.12710v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12710">
<div class="article-summary-box-inner">
<span><p>Handwritten document images can be highly affected by degradation for
different reasons: Paper ageing, daily-life scenarios (wrinkles, dust, etc.),
bad scanning process and so on. These artifacts raise many readability issues
for current Handwritten Text Recognition (HTR) algorithms and severely devalue
their efficiency. In this paper, we propose an end to end architecture based on
Generative Adversarial Networks (GANs) to recover the degraded documents into a
clean and readable form. Unlike the most well-known document binarization
methods, which try to improve the visual quality of the degraded document, the
proposed architecture integrates a handwritten text recognizer that promotes
the generated document image to be more readable. To the best of our knowledge,
this is the first work to use the text information while binarizing handwritten
documents. Extensive experiments conducted on degraded Arabic and Latin
handwritten documents demonstrate the usefulness of integrating the recognizer
within the GAN architecture, which improves both the visual quality and the
readability of the degraded document images. Moreover, we outperform the state
of the art in H-DIBCO challenges, after fine tuning our pre-trained model with
synthetically degraded Latin handwritten images, on this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02636">
<div class="article-summary-box-inner">
<span><p>As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present,
and future. We introduce MERLOT, a model that learns multimodal script
knowledge by watching millions of YouTube videos with transcribed speech -- in
an entirely label-free, self-supervised manner. By pretraining with a mix of
both frame-level (spatial) and video-level (temporal) objectives, our model not
only learns to match images to temporally corresponding words, but also to
contextualize what is happening globally over time. As a result, MERLOT
exhibits strong out-of-the-box representations of temporal commonsense, and
achieves state-of-the-art performance on 12 different video QA datasets when
finetuned. It also transfers well to the world of static images, allowing
models to reason about the dynamic context behind visual scenes. On Visual
Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,
outperforming state-of-the-art models of similar size by over 3%, even those
that make heavy use of auxiliary supervised data (like object bounding boxes).
</p>
<p>Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the
pretraining video corpus; and 3) using diverse objectives that encourage
full-stack multimodal reasoning, from the recognition to cognition level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Label Learning from Single Positive Labels. (arXiv:2106.09708v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09708">
<div class="article-summary-box-inner">
<span><p>Predicting all applicable labels for a given image is known as multi-label
classification. Compared to the standard multi-class case (where each image has
only one label), it is considerably more challenging to annotate training data
for multi-label classification. When the number of potential labels is large,
human annotators find it difficult to mention all applicable labels for each
training image. Furthermore, in some settings detection is intrinsically
difficult e.g. finding small object instances in high resolution images. As a
result, multi-label training data is often plagued by false negatives. We
consider the hardest version of this problem, where annotators provide only one
relevant label for each image. As a result, training sets will have only one
positive label per image and no confirmed negatives. We explore this special
case of learning from missing labels across four different multi-label image
classification datasets for both linear classifiers and end-to-end fine-tuned
deep networks. We extend existing multi-label losses to this setting and
propose novel variants that constrain the number of expected positive labels
during training. Surprisingly, we show that in some cases it is possible to
approach the performance of fully labeled classifiers despite training with
significantly fewer confirmed labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Exit Vision Transformer for Dynamic Inference. (arXiv:2106.15183v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15183">
<div class="article-summary-box-inner">
<span><p>Deep neural networks can be converted to multi-exit architectures by
inserting early exit branches after some of their intermediate layers. This
allows their inference process to become dynamic, which is useful for time
critical IoT applications with stringent latency requirements, but with
time-variant communication and computation resources. In particular, in edge
computing systems and IoT networks where the exact computation time budget is
variable and not known beforehand. Vision Transformer is a recently proposed
architecture which has since found many applications across various domains of
computer vision. In this work, we propose seven different architectures for
early exit branches that can be used for dynamic inference in Vision
Transformer backbones. Through extensive experiments involving both
classification and regression problems, we show that each one of our proposed
architectures could prove useful in the trade-off between accuracy and speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ray-ONet: Efficient 3D Reconstruction From A Single RGB Image. (arXiv:2107.01899v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01899">
<div class="article-summary-box-inner">
<span><p>We propose Ray-ONet to reconstruct detailed 3D models from monocular images
efficiently. By predicting a series of occupancy probabilities along a ray that
is back-projected from a pixel in the camera coordinate, our method Ray-ONet
improves the reconstruction accuracy in comparison with Occupancy Networks
(ONet), while reducing the network inference complexity to O($N^2$). As a
result, Ray-ONet achieves state-of-the-art performance on the ShapeNet
benchmark with more than 20$\times$ speed-up at $128^3$ resolution and
maintains a similar memory footprint during inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09391">
<div class="article-summary-box-inner">
<span><p>We focus on building robustness in the convolutions of neural visual
classifiers, especially against natural perturbations like elastic
deformations, occlusions and Gaussian noise. Existing CNNs show outstanding
performance on clean images, but fail to tackle naturally occurring
perturbations. In this paper, we start from elastic perturbations, which
approximate (local) view-point changes of the object. We present
elastically-augmented convolutions (EAConv) by parameterizing filters as a
combination of fixed elastically-perturbed bases functions and trainable
weights for the purpose of integrating unseen viewpoints in the CNN. We show on
CIFAR-10 and STL-10 datasets that the general robustness of our method on
unseen occlusion, zoom, rotation, image cut and Gaussian perturbations
improves, while significantly improving the performance on clean images without
any data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Learning of Occlusion Aware Flow Guided 3D Geometry Perception with Adaptive Cross Weighted Loss from Monocular Videos. (arXiv:2108.03893v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03893">
<div class="article-summary-box-inner">
<span><p>Self-supervised deep learning-based 3D scene understanding methods can
overcome the difficulty of acquiring the densely labeled ground-truth and have
made a lot of advances. However, occlusions and moving objects are still some
of the major limitations. In this paper, we explore the learnable occlusion
aware optical flow guided self-supervised depth and camera pose estimation by
an adaptive cross weighted loss to address the above limitations. Firstly, we
explore to train the learnable occlusion mask fused optical flow network by an
occlusion-aware photometric loss with the temporally supplemental information
and backward-forward consistency of adjacent views. And then, we design an
adaptive cross-weighted loss between the depth-pose and optical flow loss of
the geometric and photometric error to distinguish the moving objects which
violate the static scene assumption. Our method shows promising results on
KITTI, Make3D, and Cityscapes datasets under multiple tasks. We also show good
generalization ability under a variety of challenging scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSP-SLAM: Object Oriented SLAM with Deep Shape Priors. (arXiv:2108.09481v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09481">
<div class="article-summary-box-inner">
<span><p>We propose DSP-SLAM, an object-oriented SLAM system that builds a rich and
accurate joint map of dense 3D models for foreground objects, and sparse
landmark points to represent the background. DSP-SLAM takes as input the 3D
point cloud reconstructed by a feature-based SLAM system and equips it with the
ability to enhance its sparse map with dense reconstructions of detected
objects. Objects are detected via semantic instance segmentation, and their
shape and pose is estimated using category-specific deep shape embeddings as
priors, via a novel second order optimization. Our object-aware bundle
adjustment builds a pose-graph to jointly optimize camera poses, object
locations and feature points. DSP-SLAM can operate at 10 frames per second on 3
different input modalities: monocular, stereo, or stereo+LiDAR. We demonstrate
DSP-SLAM operating at almost frame rate on monocular-RGB sequences from the
Friburg and Redwood-OS datasets, and on stereo+LiDAR sequences on the KITTI
odometry dataset showing that it achieves high-quality full object
reconstructions, even from partial observations, while maintaining a consistent
global map. Our evaluation shows improvements in object pose and shape
reconstruction with respect to recent deep prior-based reconstruction methods
and reductions in camera tracking drift on the KITTI dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised Compression for Resource-Constrained Edge Computing Systems. (arXiv:2108.11898v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11898">
<div class="article-summary-box-inner">
<span><p>There has been much interest in deploying deep learning algorithms on
low-powered devices, including smartphones, drones, and medical sensors.
However, full-scale deep neural networks are often too resource-intensive in
terms of energy and storage. As a result, the bulk part of the machine learning
operation is therefore often carried out on an edge server, where the data is
compressed and transmitted. However, compressing data (such as images) leads to
transmitting information irrelevant to the supervised task. Another popular
approach is to split the deep network between the device and the server while
compressing intermediate features. To date, however, such split computing
strategies have barely outperformed the aforementioned naive data compression
baselines due to their inefficient approaches to feature compression. This
paper adopts ideas from knowledge distillation and neural image compression to
compress intermediate feature representations more efficiently. Our supervised
compression approach uses a teacher model and a student model with a stochastic
bottleneck and learnable prior for entropy coding (Entropic Student). We
compare our approach to various neural image and feature compression baselines
in three vision tasks and found that it achieves better supervised
rate-distortion performance while maintaining smaller end-to-end latency. We
furthermore show that the learned feature representations can be tuned to serve
multiple downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformers For Weeds and Crops Classification Of High Resolution UAV Images. (arXiv:2109.02716v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02716">
<div class="article-summary-box-inner">
<span><p>Crop and weed monitoring is an important challenge for agriculture and food
production nowadays. Thanks to recent advances in data acquisition and
computation technologies, agriculture is evolving to a more smart and precision
farming to meet with the high yield and high quality crop production.
Classification and recognition in Unmanned Aerial Vehicles (UAV) images are
important phases for crop monitoring. Advances in deep learning models relying
on Convolutional Neural Network (CNN) have achieved high performances in image
classification in the agricultural domain. Despite the success of this
architecture, CNN still faces many challenges such as high computation cost,
the need of large labelled datasets, ... Natural language processing's
transformer architecture can be an alternative approach to deal with CNN's
limitations. Making use of the self-attention paradigm, Vision Transformer
(ViT) models can achieve competitive or better results without applying any
convolution operations. In this paper, we adopt the self-attention mechanism
via the ViT models for plant classification of weeds and crops: red beet,
off-type beet (green leaves), parsley and spinach. Our experiments show that
with small set of labelled training data, ViT models perform better compared to
state-of-the-art CNN-based models EfficientNet and ResNet, with a top accuracy
of 99.8\% achieved by the ViT model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invertible Frowns: Video-to-Video Facial Emotion Translation. (arXiv:2109.08061v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08061">
<div class="article-summary-box-inner">
<span><p>We present Wav2Lip-Emotion, a video-to-video translation architecture that
modifies facial expressions of emotion in videos of speakers. Previous work
modifies emotion in images, uses a single image to produce a video with
animated emotion, or puppets facial expressions in videos with landmarks from a
reference video. However, many use cases such as modifying an actor's
performance in post-production, coaching individuals to be more animated
speakers, or touching up emotion in a teleconference require a video-to-video
translation approach. We explore a method to maintain speakers' lip movements,
identity, and pose while translating their expressed emotion. Our approach
extends an existing multi-modal lip synchronization architecture to modify the
speaker's emotion using L1 reconstruction and pre-trained emotion objectives.
We also propose a novel automated emotion evaluation approach and corroborate
it with a user study. These find that we succeed in modifying emotion while
maintaining lip synchronization. Visual quality is somewhat diminished, with a
trade off between greater emotion modification and visual quality between model
variants. Nevertheless, we demonstrate (1) that facial expressions of emotion
can be modified with nothing other than L1 reconstruction and pre-trained
emotion objectives and (2) that our automated emotion evaluation approach
aligns with human judgements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceEraser: Removing Facial Parts for Augmented Reality. (arXiv:2109.10760v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10760">
<div class="article-summary-box-inner">
<span><p>Our task is to remove all facial parts (e.g., eyebrows, eyes, mouth and
nose), and then impose visual elements onto the ``blank'' face for augmented
reality. Conventional object removal methods rely on image inpainting
techniques (e.g., EdgeConnect, HiFill) that are trained in a self-supervised
manner with randomly manipulated image pairs. Specifically, given a set of
natural images, randomly masked images are used as inputs and the raw images
are treated as ground truths. Whereas, this technique does not satisfy the
requirements of facial parts removal, as it is hard to obtain ``ground-truth''
images with real ``blank'' faces. To address this issue, we propose a novel
data generation technique to produce paired training data that well mimic the
``blank'' faces. In the mean time, we propose a novel network architecture for
improved inpainting quality for our task. Finally, we demonstrate various
face-oriented augmented reality applications on top of our facial parts removal
model. The source codes are released at
\href{https://github.com/duxingren14/FaceEraser}{duxingren14/FaceEraser} on
github for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually Grounded Reasoning across Languages and Cultures. (arXiv:2109.13238v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13238">
<div class="article-summary-box-inner">
<span><p>The design of widespread vision-and-language datasets and pre-trained
encoders directly adopts, or draws inspiration from, the concepts and images of
ImageNet. While one can hardly overestimate how much this benchmark contributed
to progress in computer vision, it is mostly derived from lexical databases and
image queries in English, resulting in source material with a North American or
Western European bias. Therefore, we devise a new protocol to construct an
ImageNet-style hierarchy representative of more languages and cultures. In
particular, we let the selection of both concepts and images be entirely driven
by native speakers, rather than scraping them automatically. Specifically, we
focus on a typologically diverse set of languages, namely, Indonesian, Mandarin
Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images
obtained through this new protocol, we create a multilingual dataset for
{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting
statements from native speaker annotators about pairs of images. The task
consists of discriminating whether each grounded statement is true or false. We
establish a series of baselines using state-of-the-art models and find that
their cross-lingual transfer performance lags dramatically behind supervised
performance in English. These results invite us to reassess the robustness and
accuracy of current state-of-the-art models beyond a narrow domain, but also
open up new exciting challenges for the development of truly multilingual and
multicultural systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction. (arXiv:2110.03278v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03278">
<div class="article-summary-box-inner">
<span><p>Most existing human matting algorithms tried to separate pure human-only
foreground from the background. In this paper, we propose a Virtual
Multi-modality Foreground Matting (VMFM) method to learn human-object
interactive foreground (human and objects interacted with him or her) from a
raw RGB image. The VMFM method requires no additional inputs, e.g. trimap or
known background. We reformulate foreground matting as a self-supervised
multi-modality problem: factor each input image into estimated depth map,
segmentation mask, and interaction heatmap using three auto-encoders. In order
to fully utilize the characteristics of each modality, we first train a dual
encoder-to-decoder network to estimate the same alpha matte. Then we introduce
a self-supervised method: Complementary Learning(CL) to predict deviation
probability map and exchange reliable gradients across modalities without
label. We conducted extensive experiments to analyze the effectiveness of each
modality and the significance of different components in complementary
learning. We demonstrate that our model outperforms the state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning by Estimating Twin Class Distributions. (arXiv:2110.07402v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07402">
<div class="article-summary-box-inner">
<span><p>We present TWIST, a novel self-supervised representation learning method by
classifying large-scale unlabeled datasets in an end-to-end way. We employ a
siamese network terminated by a softmax operation to produce twin class
distributions of two augmented images. Without supervision, we enforce the
class distributions of different augmentations to be consistent. In the
meantime, we regularize the class distributions to make them sharp and diverse.
Specifically, we minimize the entropy of the distribution for each sample to
make the class prediction for each sample assertive and maximize the entropy of
the mean distribution to make the predictions of different samples diverse. In
this way, TWIST can naturally avoid the trivial solutions without specific
designs such as asymmetric network, stop-gradient operation, or momentum
encoder. Different from the clustering-based methods which alternate between
clustering and learning, our method is a single learning process guided by a
unified loss function. As a result, TWIST outperforms state-of-the-art methods
on a wide range of tasks, including unsupervised classification, linear
classification, semi-supervised learning, transfer learning, and some dense
prediction tasks such as detection and segmentation. Codes and pre-trained
models are given on: https://github.com/bytedance/TWIST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backpropagation with Biologically Plausible Spatio-Temporal Adjustment For Training Deep Spiking Neural Networks. (arXiv:2110.08858v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08858">
<div class="article-summary-box-inner">
<span><p>The spiking neural network (SNN) mimics the information processing operation
in the human brain, represents and transmits information in spike trains
containing wealthy spatial and temporal information, and shows superior
performance on many cognitive tasks. In addition, the event-driven information
processing enables the energy-efficient implementation on neuromorphic chips.
The success of deep learning is inseparable from backpropagation. Due to the
discrete information transmission, directly applying the backpropagation to the
training of the SNN still has a performance gap compared with the traditional
deep neural networks. Also, a large simulation time is required to achieve
better performance, which results in high latency. To address the problems, we
propose a biological plausible spatial adjustment, which rethinks the
relationship between membrane potential and spikes and realizes a reasonable
adjustment of gradients to different time steps. And it precisely controls the
backpropagation of the error along the spatial dimension. Secondly, we propose
a biologically plausible temporal adjustment making the error propagate across
the spikes in the temporal dimension, which overcomes the problem of the
temporal dependency within a single spike period of the traditional spiking
neurons. We have verified our algorithm on several datasets, and the
experimental results have shown that our algorithm greatly reduces the network
latency and energy consumption while also improving network performance. We
have achieved state-of-the-art performance on the neuromorphic datasets
N-MNIST, DVS-Gesture, and DVS-CIFAR10. For the static datasets MNIST and
CIFAR10, we have surpassed most of the traditional SNN backpropagation training
algorithm and achieved relatively superior performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No RL, No Simulation: Learning to Navigate without Navigating. (arXiv:2110.09470v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09470">
<div class="article-summary-box-inner">
<span><p>Most prior methods for learning navigation policies require access to
simulation environments, as they need online policy interaction and rely on
ground-truth maps for rewards. However, building simulators is expensive
(requires manual effort for each and every scene) and creates challenges in
transferring learned policies to robotic platforms in the real-world, due to
the sim-to-real domain gap. In this paper, we pose a simple question: Do we
really need active interaction, ground-truth maps or even
reinforcement-learning (RL) in order to solve the image-goal navigation task?
We propose a self-supervised approach to learn to navigate from only passive
videos of roaming. Our approach, No RL, No Simulator (NRNS), is simple and
scalable, yet highly effective. NRNS outperforms RL-based formulations by a
significant margin. We present NRNS as a strong baseline for any future
image-based navigation tasks that use RL or Simulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOS: A Low Latency and Lightweight Framework for Face Detection, Landmark Localization, and Head Pose Estimation. (arXiv:2110.10953v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10953">
<div class="article-summary-box-inner">
<span><p>With the emergence of service robots and surveillance cameras, dynamic face
recognition (DFR) in wild has received much attention in recent years. Face
detection and head pose estimation are two important steps for DFR. Very often,
the pose is estimated after the face detection. However, such sequential
computations lead to higher latency. In this paper, we propose a low latency
and lightweight network for simultaneous face detection, landmark localization
and head pose estimation. Inspired by the observation that it is more
challenging to locate the facial landmarks for faces with large angles, a pose
loss is proposed to constrain the learning. Moreover, we also propose an
uncertainty multi-task loss to learn the weights of individual tasks
automatically. Another challenge is that robots often use low computational
units like ARM based computing core and we often need to use lightweight
networks instead of the heavy ones, which lead to performance drop especially
for small and hard faces. In this paper, we propose online feedback sampling to
augment the training samples across different scales, which increases the
diversity of training data automatically. Through validation in commonly used
WIDER FACE, AFLW and AFLW2000 datasets, the results show that the proposed
method achieves the state-of-the-art performance in low computational
resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Knowledge Distillation With Peer-To-Peer Mutual Learning For Model Compression. (arXiv:2110.11023v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11023">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) is an effective model compression technique where
a compact student network is taught to mimic the behavior of a complex and
highly trained teacher network. In contrast, Mutual Learning (ML) provides an
alternative strategy where multiple simple student networks benefit from
sharing knowledge, even in the absence of a powerful but static teacher
network. Motivated by these findings, we propose a single-teacher,
multi-student framework that leverages both KD and ML to achieve better
performance. Furthermore, an online distillation strategy is utilized to train
the teacher and students simultaneously. To evaluate the performance of the
proposed approach, extensive experiments were conducted using three different
versions of teacher-student networks on benchmark biomedical classification
(MSI vs. MSS) and object detection (Polyp Detection) tasks. Ensemble of student
networks trained in the proposed manner achieved better results than the
ensemble of students trained using KD or ML individually, establishing the
benefit of augmenting knowledge transfer from teacher to students with
peer-to-peer learning between students.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Deployment of Recycling Classification through Efficient Hyper-Parameter Analysis. (arXiv:2110.11043v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11043">
<div class="article-summary-box-inner">
<span><p>The paradigm of automated waste classification has recently seen a shift in
the domain of interest from conventional image processing techniques to
powerful computer vision algorithms known as convolutional neural networks
(CNN). Historically, CNNs have demonstrated a strong dependency on powerful
hardware for real-time classification, yet the need for deployment on weaker
embedded devices is greater than ever. The work in this paper proposes a
methodology for reconstructing and tuning conventional image classification
models, using EfficientNets, to decrease their parameterisation with no
trade-off in model accuracy and develops a pipeline through TensorRT for
accelerating such models to run at real-time on an NVIDIA Jetson Nano embedded
device. The train-deployment discrepancy, relating how poor data augmentation
leads to a discrepancy in model accuracy between training and deployment, is
often neglected in many papers and thus the work is extended by analysing and
evaluating the impact real world perturbations had on model accuracy once
deployed. The scope of the work concerns developing a more efficient variant of
WasteNet, a collaborative recycling classification model. The newly developed
model scores a test-set accuracy of 95.8% with a real world accuracy of 95%, a
14% increase over the original. Our acceleration pipeline boosted model
throughput by 750% to 24 inferences per second on the Jetson Nano and real-time
latency of the system was verified through servomotor latency analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Insight into Measuring Face Image Utility with General and Face-specific Image Quality Metrics. (arXiv:2110.11111v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11111">
<div class="article-summary-box-inner">
<span><p>Quality scores provide a measure to evaluate the utility of biometric samples
for biometric recognition. Biometric recognition systems require high-quality
samples to achieve optimal performance. This paper focuses on face images and
the measurement of face image utility with general and face-specific image
quality metrics. While face-specific metrics rely on features of aligned face
images, general image quality metrics can be used on the global image and
relate to human perceptions. In this paper, we analyze the gap between the
general image quality metrics and the face image quality metrics. Our
contribution lies in a thorough examination of how different the image quality
assessment algorithms relate to the utility for the face recognition task. The
results of image quality assessment algorithms are further compared with those
of dedicated face image quality assessment algorithms. In total, 25 different
quality metrics are evaluated on three face image databases, BioSecure, LFW,
and VGGFace2 using three open-source face recognition solutions, SphereFace,
ArcFace, and FaceNet. Our results reveal a clear correlation between learned
image metrics to face image utility even without being specifically trained as
a face utility measure. Individual handcrafted features lack general stability
and perform significantly worse than general face-specific quality metrics. We
additionally provide a visual insight into the image areas contributing to the
quality score of a selected set of quality assessment methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HCV: Hierarchy-Consistency Verification for Incremental Implicitly-Refined Classification. (arXiv:2110.11148v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11148">
<div class="article-summary-box-inner">
<span><p>Human beings learn and accumulate hierarchical knowledge over their lifetime.
This knowledge is associated with previous concepts for consolidation and
hierarchical construction. However, current incremental learning methods lack
the ability to build a concept hierarchy by associating new concepts to old
ones. A more realistic setting tackling this problem is referred to as
Incremental Implicitly-Refined Classification (IIRC), which simulates the
recognition process from coarse-grained categories to fine-grained categories.
To overcome forgetting in this benchmark, we propose Hierarchy-Consistency
Verification (HCV) as an enhancement to existing continual learning methods.
Our method incrementally discovers the hierarchical relations between classes.
We then show how this knowledge can be exploited during both training and
inference. Experiments on three setups of varying difficulty demonstrate that
our HCV module improves performance of existing continual learning methods
under this IIRC setting by a large margin. Code is available in
https://github.com/wangkai930418/HCV_IIRC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Graph Convolutional Networks for Human Action Synthesis. (arXiv:2110.11191v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11191">
<div class="article-summary-box-inner">
<span><p>Synthesising the spatial and temporal dynamics of the human body skeleton
remains a challenging task, not only in terms of the quality of the generated
shapes, but also of their diversity, particularly to synthesise realistic body
movements of a specific action (action conditioning). In this paper, we propose
Kinetic-GAN, a novel architecture that leverages the benefits of Generative
Adversarial Networks and Graph Convolutional Networks to synthesise the
kinetics of the human body. The proposed adversarial architecture can condition
up to 120 different actions over local and global body movements while
improving sample quality and diversity through latent space disentanglement and
stochastic variations. Our experiments were carried out in three well-known
datasets, where Kinetic-GAN notably surpasses the state-of-the-art methods in
terms of distribution quality metrics while having the ability to synthesise
more than one order of magnitude regarding the number of different actions. Our
code and models are publicly available at
https://github.com/DegardinBruno/Kinetic-GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Super-resolution of multiphase materials by combining complementary 2D and 3D image data using generative adversarial networks. (arXiv:2110.11281v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11281">
<div class="article-summary-box-inner">
<span><p>Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effect of Wearing a Face Mask on Face Image Quality. (arXiv:2110.11283v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11283">
<div class="article-summary-box-inner">
<span><p>Due to the COVID-19 situation, face masks have become a main part of our
daily life. Wearing mouth-and-nose protection has been made a mandate in many
public places, to prevent the spread of the COVID-19 virus. However, face masks
affect the performance of face recognition, since a large area of the face is
covered. The effect of wearing a face mask on the different components of the
face recognition system in a collaborative environment is a problem that is
still to be fully studied. This work studies, for the first time, the effect of
wearing a face mask on face image quality by utilising state-of-the-art face
image quality assessment methods of different natures. This aims at providing
better understanding on the effect of face masks on the operation of face
recognition as a whole system. In addition, we further studied the effect of
simulated masks on face image utility in comparison to real face masks. We
discuss the correlation between the mask effect on face image quality and that
on the face verification performance by automatic systems and human experts,
indicating a consistent trend between both factors. The evaluation is conducted
on the database containing (1) no-masked faces, (2) real face masks, and (3)
simulated face masks, by synthetically generating digital facial masks on
no-masked faces according to the NIST protocols [1, 23]. Finally, a visual
interpretation of the face areas contributing to the quality score of a
selected set of quality assessment methods is provided to give a deeper insight
into the difference of network decisions in masked and non-masked faces, among
other variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on GANs with Margin Cosine Loss and Relativistic Discriminator. (arXiv:2110.11293v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11293">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) have emerged as useful generative
models, which are capable of implicitly learning data distributions of
arbitrarily complex dimensions. However, the training of GANs is empirically
well-known for being highly unstable and sensitive. The loss functions of both
the discriminator and generator concerning their parameters tend to oscillate
wildly during training. Different loss functions have been proposed to
stabilize the training and improve the quality of images generated. In this
paper, we perform an empirical study on the impact of several loss functions on
the performance of standard GAN models, Deep Convolutional Generative
Adversarial Networks (DCGANs). We introduce a new improvement that employs a
relativistic discriminator to replace the classical deterministic discriminator
in DCGANs and implement a margin cosine loss function for both the generator
and discriminator. This results in a novel loss function, namely Relativistic
Margin Cosine Loss (RMCosGAN). We carry out extensive experiments with four
datasets: CIFAR-$10$, MNIST, STL-$10$, and CAT. We compare RMCosGAN performance
with existing loss functions based on two metrics: Frechet inception distance
and inception score. The experimental results show that RMCosGAN outperforms
the existing ones and significantly improves the quality of images generated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLURP: Side Learning Uncertainty for Regression Problems. (arXiv:2110.11182v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11182">
<div class="article-summary-box-inner">
<span><p>It has become critical for deep learning algorithms to quantify their output
uncertainties to satisfy reliability constraints and provide accurate results.
Uncertainty estimation for regression has received less attention than
classification due to the more straightforward standardized output of the
latter class of tasks and their high importance. However, regression problems
are encountered in a wide range of applications in computer vision. We propose
SLURP, a generic approach for regression uncertainty estimation via a side
learner that exploits the output and the intermediate representations generated
by the main task model. We test SLURP on two critical regression tasks in
computer vision: monocular depth and optical flow estimation. In addition, we
conduct exhaustive benchmarks comprising transfer to different datasets and the
addition of aleatoric noise. The results show that our proposal is generic and
readily applicable to various regression problems and has a low computational
cost with respect to existing solutions.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-25 23:02:53.043887887 UTC">2021-10-25 23:02:53 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.4</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>